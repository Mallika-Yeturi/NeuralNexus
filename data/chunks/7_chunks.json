[
  {
    "content": "# 7\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n7Neural Networks\n\u201c[M]achines of this character can behave in a very complicated manner when\nthe number of units is large.\u201d\nAlan Turing (1948) \u201cIntelligent Machines\u201d, page 6\nNeural networks are a fundamental computational tool for language process-\ning, and a very old one. They are called neural because their origins lie in the\nMcCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simpli\ufb01ed model of the\nbiological neuron as a kind of computing element that could be described in terms\nof propositional logic. But the modern use in language processing no longer draws\non these early biological inspirations.\nInstead, a modern neural network is a network of small computing units, each\nof which takes a vector of input values and produces a single output value. In this\nchapter we introduce the neural net applied to classi\ufb01cation. The architecture we\nintroduce is called a feedforward network because the computation proceeds iter- feedforward\natively from one layer of units to the next. The use of modern neural nets is often\ncalled deep learning , because modern networks are often deep (have many layers). deep learning\nNeural networks share much of the same mathematics as logistic regression. But\nneural networks are a more powerful classi\ufb01er than logistic regression, and indeed a\nminimal neural network (technically one with a single \u2018hidden layer\u2019) can be shown\nto learn any function.\nNeural net classi\ufb01ers are different from logistic regression in another way. With\nlogistic regression, we applied the regression classi\ufb01er to many different tasks by\ndeveloping many rich kinds of feature templates based on domain knowledge. When\nworking with neural networks, it is more common to avoid most uses of rich hand-\nderived features, instead building neural networks that take raw words as inputs\nand learn to induce features as part of the process of learning to classify. We saw\nexamples of this kind of representation learning for embeddings in Chapter 6. Nets\nthat are very deep are particularly good at representation learning. For that reason\ndeep neural nets are the right tool for tasks that offer suf\ufb01cient data to learn features\nautomatically.\nIn this chapter we\u2019ll introduce feedforward networks as classi\ufb01ers, and also ap-\nply them to the simple task of language modeling: assigning probabilities to word\nsequences and predicting upcoming words. In subsequent chapters we\u2019ll introduce\nmany other aspects of neural models, such as recurrent neural networks (Chap-\nter 8), the Transformer (Chapter 9), and masked language modeling (Chapter 11).",
    "metadata": {
      "source": "7",
      "chunk_id": 0,
      "token_count": 593,
      "chapter_title": "7"
    }
  },
  {
    "content": "## Page 2\n\n2CHAPTER 7 \u2022 N EURAL NETWORKS\n7.1 Units\nThe building block of a neural network is a single computational unit. A unit takes\na set of real valued numbers as input, performs some computation on them, and\nproduces an output.\nAt its heart, a neural unit is taking a weighted sum of its inputs, with one addi-\ntional term in the sum called a bias term . Given a set of inputs x1:::xn, a unit has bias term\na set of corresponding weights w1:::wnand a bias b, so the weighted sum zcan be\nrepresented as:\nz=b+X\niwixi (7.1)\nOften it\u2019s more convenient to express this weighted sum using vector notation; recall\nfrom linear algebra that a vector is, at heart, just a list or array of numbers. Thus vector\nwe\u2019ll talk about zin terms of a weight vector w, a scalar bias b, and an input vector\nx, and we\u2019ll replace the sum with the convenient dot product :\nz=w\u0001x+b (7.2)\nAs de\ufb01ned in Eq. 7.2, zis just a real valued number.\nFinally, instead of using z, a linear function of x, as the output, neural units\napply a non-linear function ftoz. We will refer to the output of this function as\ntheactivation value for the unit, a. Since we are just modeling a single unit, the activation\nactivation for the node is in fact the \ufb01nal output of the network, which we\u2019ll generally\ncally. So the value yis de\ufb01ned as:\ny=a=f(z)\nWe\u2019ll discuss three popular non-linear functions fbelow (the sigmoid, the tanh, and\nthe recti\ufb01ed linear unit or ReLU) but it\u2019s pedagogically convenient to start with the\nsigmoid function since we saw it in Chapter 5: sigmoid\ny=s(z) =1\n1+e\u0000z(7.3)\nThe sigmoid (shown in Fig. 7.1) has a number of advantages; it maps the output\ninto the range (0;1), which is useful in squashing outliers toward 0 or 1. And it\u2019s\ndifferentiable, which as we saw in Section ??will be handy for learning.\nFigure 7.1 The sigmoid function takes a real value and maps it to the range (0;1). It is\nnearly linear around 0 but outlier values get squashed toward 0 or 1.\nSubstituting Eq. 7.2 into Eq. 7.3 gives us the output of a neural unit:\ny=s(w\u0001x+b) =1\n1+exp(\u0000(w\u0001x+b))(7.4)",
    "metadata": {
      "source": "7",
      "chunk_id": 1,
      "token_count": 589,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\n7.1 \u2022 U NITS 3\nFig. 7.2 shows a \ufb01nal schematic of a basic neural unit. In this example the unit\ntakes 3 input values x1;x2, and x3, and computes a weighted sum, multiplying each\nvalue by a weight ( w1,w2, and w3, respectively), adds them to a bias term b, and then\npasses the resulting sum through a sigmoid function to result in a number between 0\nand 1.\nx1x2x3\nyw1w2w3\u2211b\u03c3+1za\nFigure 7.2 A neural unit, taking 3 inputs x1,x2, and x3(and a bias bthat we represent as a\nweight for an input clamped at +1) and producing an output y. We include some convenient\nintermediate variables: the output of the summation, z, and the output of the sigmoid, a. In\nthis case the output of the unit yis the same as a, but in deeper networks we\u2019ll reserve yto\nmean the \ufb01nal output of the entire network, leaving aas the activation of an individual node.\nLet\u2019s walk through an example just to get an intuition. Let\u2019s suppose we have a\nunit with the following weight vector and bias:\nw= [0:2;0:3;0:9]\nb=0:5\nWhat would this unit do with the following input vector:\nx= [0:5;0:6;0:1]\nThe resulting output ywould be:\ny=s(w\u0001x+b) =1\n1+e\u0000(w\u0001x+b)=1\n1+e\u0000(:5\u0003:2+:6\u0003:3+:1\u0003:9+:5)=1\n1+e\u00000:87=:70\nIn practice, the sigmoid is not commonly used as an activation function. A function\nthat is very similar but almost always better is the tanh function shown in Fig. 7.3a; tanh\ntanh is a variant of the sigmoid that ranges from -1 to +1:\ny=tanh(z) =ez\u0000e\u0000z\nez+e\u0000z(7.5)\nThe simplest activation function, and perhaps the most commonly used, is the rec-\nti\ufb01ed linear unit, also called the ReLU , shown in Fig. 7.3b. It\u2019s just the same as z ReLU\nwhen zis positive, and 0 otherwise:\ny=ReLU(z) =max(z;0) (7.6)\nThese activation functions have different properties that make them useful for differ-\nent language applications or network architectures. For example, the tanh function\nhas the nice properties of being smoothly differentiable and mapping outlier values\ntoward the mean. The recti\ufb01er function, on the other hand, has nice properties that",
    "metadata": {
      "source": "7",
      "chunk_id": 2,
      "token_count": 611,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4CHAPTER 7 \u2022 N EURAL NETWORKS\n(a) (b)\nFigure 7.3 The tanh and ReLU activation functions.\nresult from it being very close to linear. In the sigmoid or tanh functions, very high\nvalues of zresult in values of ythat are saturated , i.e., extremely close to 1, and have saturated\nderivatives very close to 0. Zero derivatives cause problems for learning, because as\nwe\u2019ll see in Section 7.5, we\u2019ll train networks by propagating an error signal back-\nwards, multiplying gradients (partial derivatives) from each layer of the network;\ngradients that are almost 0 cause the error signal to get smaller and smaller until it is\ntoo small to be used for training, a problem called the vanishing gradient problem.vanishing\ngradient\nRecti\ufb01ers don\u2019t have this problem, since the derivative of ReLU for high values of z\nis 1 rather than very close to 0.\n7.2 The XOR problem\nEarly in the history of neural networks it was realized that the power of neural net-\nworks, as with the real neurons that inspired them, comes from combining these\nunits into larger networks.\nOne of the most clever demonstrations of the need for multi-layer networks was\nthe proof by Minsky and Papert (1969) that a single neural unit cannot compute\nsome very simple functions of its input. Consider the task of computing elementary\nlogical functions of two inputs, like AND, OR, and XOR. As a reminder, here are\nthe truth tables for those functions:\nAND OR XOR\nx1 x2 y x1 x2 y x1 x2 y\n0 0 0 0 0 0 0 0 0\n0 1 0 0 1 1 0 1 1\n1 0 0 1 0 1 1 0 1\n1 1 1 1 1 1 1 1 0\nThis example was \ufb01rst shown for the perceptron , which is a very simple neural perceptron\nunit that has a binary output and has a very simple step function as its non-linear\nactivation function. The output yof a perceptron is 0 or 1, and is computed as\nfollows (using the same weight w, input x, and bias bas in Eq. 7.2):\ny=\u001a0;ifw\u0001x+b\u00140\n1;ifw\u0001x+b>0(7.7)",
    "metadata": {
      "source": "7",
      "chunk_id": 3,
      "token_count": 535,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5",
    "metadata": {
      "source": "7",
      "chunk_id": 4,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "7.2 \u2022 T HEXOR PROBLEM 5\nIt\u2019s very easy to build a perceptron that can compute the logical AND and OR\nfunctions of its binary inputs; Fig. 7.4 shows the necessary weights.\nx1x2+1-111\nx1x2+1011\n(a) (b)\nFigure 7.4 The weights wand bias bfor perceptrons for computing logical functions. The\ninputs are shown as x1andx2and the bias as a special node with value +1 which is multiplied\nwith the bias weight b. (a) logical AND, with weights w1=1 and w2=1 and bias weight\nb=\u00001. (b) logical OR, with weights w1=1 and w2=1 and bias weight b=0. These\nweights/biases are just one from an in\ufb01nite number of possible sets of weights and biases that\nwould implement the functions.\nIt turns out, however, that it\u2019s not possible to build a perceptron to compute\nlogical XOR! (It\u2019s worth spending a moment to give it a try!)\nThe intuition behind this important result relies on understanding that a percep-\ntron is a linear classi\ufb01er. For a two-dimensional input x1andx2, the perceptron\nequation, w1x1+w2x2+b=0 is the equation of a line. (We can see this by putting\nit in the standard linear format: x2= (\u0000w1=w2)x1+ (\u0000b=w2).) This line acts as a\ndecision boundary in two-dimensional space in which the output 0 is assigned to alldecision\nboundary\ninputs lying on one side of the line, and the output 1 to all input points lying on the\nother side of the line. If we had more than 2 inputs, the decision boundary becomes\na hyperplane instead of a line, but the idea is the same, separating the space into two\ncategories.\nFig. 7.5 shows the possible logical inputs ( 00,01,10, and 11) and the line drawn\nby one possible set of parameters for an AND and an OR classi\ufb01er. Notice that there\nis simply no way to draw a line that separates the positive cases of XOR (01 and 10)\nfrom the negative cases (00 and 11). We say that XOR is not a linearly separablelinearly\nseparable\nfunction. Of course we could draw a boundary with a curve, or some other function,\nbut not a single line.\n7.2.1 The solution: neural networks\nWhile the XOR function cannot be calculated by a single perceptron, it can be cal-\nculated by a layered network of perceptron units. Rather than see this with networks\nof simple perceptrons, however, let\u2019s see how to compute XOR using two layers of\nReLU-based units following Goodfellow et al. (2016). Fig. 7.6 shows a \ufb01gure with\nthe input being processed by two layers of neural units. The middle layer (called\nh) has two units, and the output layer (called y) has one unit. A set of weights and\nbiases are shown that allows the network to correctly compute the XOR function.\nLet\u2019s walk through what happens with the input x= [0, 0]. If we multiply each\ninput value by the appropriate weight, sum, and then add the bias b, we get the vector\n[0, -1], and we then apply the recti\ufb01ed linear transformation to give the output of the\nhlayer as [0, 0]. Now we once again multiply by the weights, sum, and add the",
    "metadata": {
      "source": "7",
      "chunk_id": 5,
      "token_count": 783,
      "chapter_title": ""
    }
  },
  {
    "content": "from the negative cases (00 and 11). We say that XOR is not a linearly separablelinearly\nseparable\nfunction. Of course we could draw a boundary with a curve, or some other function,\nbut not a single line.\n7.2.1 The solution: neural networks\nWhile the XOR function cannot be calculated by a single perceptron, it can be cal-\nculated by a layered network of perceptron units. Rather than see this with networks\nof simple perceptrons, however, let\u2019s see how to compute XOR using two layers of\nReLU-based units following Goodfellow et al. (2016). Fig. 7.6 shows a \ufb01gure with\nthe input being processed by two layers of neural units. The middle layer (called\nh) has two units, and the output layer (called y) has one unit. A set of weights and\nbiases are shown that allows the network to correctly compute the XOR function.\nLet\u2019s walk through what happens with the input x= [0, 0]. If we multiply each\ninput value by the appropriate weight, sum, and then add the bias b, we get the vector\n[0, -1], and we then apply the recti\ufb01ed linear transformation to give the output of the\nhlayer as [0, 0]. Now we once again multiply by the weights, sum, and add the\nbias (0 in this case) resulting in the value 0. The reader should work through the\ncomputation of the remaining 3 possible input pairs to see that the resulting yvalues\nare 1 for the inputs [0, 1] and [1, 0] and 0 for [0, 0] and [1, 1].",
    "metadata": {
      "source": "7",
      "chunk_id": 6,
      "token_count": 368,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6CHAPTER 7 \u2022 N EURAL NETWORKS\n0011x1x20011x1x20011x1x2\na)  x1 AND x2b)  x1 OR x2c)  x1 XOR x2?\nFigure 7.5 The functions AND, OR, and XOR, represented with input x1on the x-axis and input x2on the\ny-axis. Filled circles represent perceptron outputs of 1, and white circles perceptron outputs of 0. There is no\nway to draw a line that correctly separates the two categories for XOR. Figure styled after Russell and Norvig\n(2002).\nx1x2h1h2y1+11-1111-201+10\nFigure 7.6 XOR solution after Goodfellow et al. (2016). There are three ReLU units, in\ntwo layers; we\u2019ve called them h1,h2(hfor \u201chidden layer\u201d) and y1. As before, the numbers\non the arrows represent the weights wfor each unit, and we represent the bias bas a weight\non a unit clamped to +1, with the bias weights/units in gray.\nIt\u2019s also instructive to look at the intermediate results, the outputs of the two\nhidden nodes h1andh2. We showed in the previous paragraph that the hvector for\nthe inputs x= [0, 0] was [0, 0]. Fig. 7.7b shows the values of the hlayer for all\n4 inputs. Notice that hidden representations of the two input points x= [0, 1] and\nx= [1, 0] (the two cases with XOR output = 1) are merged to the single point h=\n[1, 0]. The merger makes it easy to linearly separate the positive and negative cases\nof XOR. In other words, we can view the hidden layer of the network as forming a\nrepresentation of the input.\nIn this example we just stipulated the weights in Fig. 7.6. But for real examples\nthe weights for neural networks are learned automatically using the error backprop-\nagation algorithm to be introduced in Section 7.5. That means the hidden layers will\nlearn to form useful representations. This intuition, that neural networks can auto-\nmatically learn useful representations of the input, is one of their key advantages,\nand one that we will return to again and again in later chapters.",
    "metadata": {
      "source": "7",
      "chunk_id": 7,
      "token_count": 521,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\n7.3 \u2022 F EEDFORWARD NEURAL NETWORKS 7\n0011x1x2\na) The original x space0011h1h2\n2b) The new (linearly separable) h space\nFigure 7.7 The hidden layer forming a new representation of the input. (b) shows the\nrepresentation of the hidden layer, h, compared to the original input representation xin (a).\nNotice that the input point [0, 1] has been collapsed with the input point [1, 0], making it\npossible to linearly separate the positive and negative cases of XOR. After Goodfellow et al.\n(2016).\n7.3 Feedforward Neural Networks\nLet\u2019s now walk through a slightly more formal presentation of the simplest kind of\nneural network, the feedforward network . A feedforward network is a multilayerfeedforward\nnetwork\nnetwork in which the units are connected with no cycles; the outputs from units in\neach layer are passed to units in the next higher layer, and no outputs are passed\nback to lower layers. (In Chapter 8 we\u2019ll introduce networks with cycles, called\nrecurrent neural networks .)\nFor historical reasons multilayer networks, especially feedforward networks, are\nsometimes called multi-layer perceptrons (orMLP s); this is a technical misnomer,multi-layer\nperceptrons\nMLP since the units in modern multilayer networks aren\u2019t perceptrons (perceptrons have a\nsimple step-function as their activation function, but modern networks are made up\nof units with many kinds of non-linearities like ReLUs and sigmoids), but at some\npoint the name stuck.\nSimple feedforward networks have three kinds of nodes: input units, hidden\nunits, and output units.\nFig. 7.8 shows a picture. The input layer xis a vector of simple scalar values just\nas we saw in Fig. 7.2.\nThe core of the neural network is the hidden layer hformed of hidden units hi, hidden layer\neach of which is a neural unit as described in Section 7.1, taking a weighted sum of\nits inputs and then applying a non-linearity. In the standard architecture, each layer\nisfully-connected , meaning that each unit in each layer takes as input the outputs fully-connected\nfrom all the units in the previous layer, and there is a link between every pair of units\nfrom two adjacent layers. Thus each hidden unit sums over all the input units.\nRecall that a single hidden unit has as parameters a weight vector and a bias. We\nrepresent the parameters for the entire hidden layer by combining the weight vector\nand bias for each unit iinto a single weight matrix Wand a single bias vector bfor\nthe whole layer (see Fig. 7.8). Each element Wjiof the weight matrix Wrepresents\nthe weight of the connection from the ith input unit xito the jth hidden unit hj.\nThe advantage of using a single matrix Wfor the weights of the entire layer is\nthat now the hidden layer computation for a feedforward network can be done very",
    "metadata": {
      "source": "7",
      "chunk_id": 8,
      "token_count": 662,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8CHAPTER 7 \u2022 N EURAL NETWORKS\nx1x2xn0\u2026\u2026+1b\u2026UW\ninput layerhidden layeroutput layerh1y1y2yn2h2h3hn1\nFigure 7.8 A simple 2-layer feedforward network, with one hidden layer, one output layer,\nand one input layer (the input layer is usually not counted when enumerating layers).\nef\ufb01ciently with simple matrix operations. In fact, the computation only has three\nsteps: multiplying the weight matrix by the input vector x, adding the bias vector b,\nand applying the activation function g(such as the sigmoid, tanh, or ReLU activation\nfunction de\ufb01ned above).\nThe output of the hidden layer, the vector h, is thus the following (for this exam-\nple we\u2019ll use the sigmoid function sas our activation function):\nh=s(Wx+b) (7.8)\nNotice that we\u2019re applying the sfunction here to a vector, while in Eq. 7.3 it was\napplied to a scalar. We\u2019re thus allowing s(\u0001), and indeed any activation function\ng(\u0001), to apply to a vector element-wise, so g[z1;z2;z3] = [g(z1);g(z2);g(z3)].\nLet\u2019s introduce some constants to represent the dimensionalities of these vectors\nand matrices. We\u2019ll refer to the input layer as layer 0 of the network, and have n0\nrepresent the number of inputs, so xis a vector of real numbers of dimension n0,\nor more formally x2Rn0, a column vector of dimensionality [n0;1]. Let\u2019s call the\nhidden layer layer 1 and the output layer layer 2. The hidden layer has dimensional-\nityn1, soh2Rn1and also b2Rn1(since each hidden unit can take a different bias\nvalue). And the weight matrix Whas dimensionality W2Rn1\u0002n0, i.e.[n1;n0].\nTake a moment to convince yourself that the matrix multiplication in Eq. 7.8 will\ncompute the value of each hjass\u0000Pn0\ni=1Wjixi+bj\u0001\n.\nAs we saw in Section 7.2, the resulting value h(forhidden but also for hypoth-\nesis) forms a representation of the input. The role of the output layer is to take\nthis new representation hand compute a \ufb01nal output. This output could be a real-\nvalued number, but in many cases the goal of the network is to make some sort of\nclassi\ufb01cation decision, and so we will focus on the case of classi\ufb01cation.\nIf we are doing a binary task like sentiment classi\ufb01cation, we might have a sin-\ngle output node, and its scalar value yis the probability of positive versus negative\nsentiment. If we are doing multinomial classi\ufb01cation, such as assigning a part-of-\nspeech tag, we might have one output node for each potential part-of-speech, whose\noutput value is the probability of that part-of-speech, and the values of all the output\nnodes must sum to one. The output layer is thus a vector ythat gives a probability\ndistribution across the output nodes.",
    "metadata": {
      "source": "7",
      "chunk_id": 9,
      "token_count": 702,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9",
    "metadata": {
      "source": "7",
      "chunk_id": 10,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "7.3 \u2022 F EEDFORWARD NEURAL NETWORKS 9\nLet\u2019s see how this happens. Like the hidden layer, the output layer has a weight\nmatrix (let\u2019s call it U), but some models don\u2019t include a bias vector bin the output\nlayer, so we\u2019ll simplify by eliminating the bias vector in this example. The weight\nmatrix is multiplied by its input vector ( h) to produce the intermediate output z:\nz=Uh\nThere are n2output nodes, so z2Rn2, weight matrix Uhas dimensionality U2\nRn2\u0002n1, and element Ui jis the weight from unit jin the hidden layer to unit iin the\noutput layer.\nHowever, zcan\u2019t be the output of the classi\ufb01er, since it\u2019s a vector of real-valued\nnumbers, while what we need for classi\ufb01cation is a vector of probabilities. There is\na convenient function for normalizing a vector of real values, by which we mean normalizing\nconverting it to a vector that encodes a probability distribution (all the numbers lie\nbetween 0 and 1 and sum to 1): the softmax function that we saw on page ??of softmax\nChapter 5. More generally for any vector zof dimensionality d, the softmax is\nde\ufb01ned as:\nsoftmax (zi) =exp(zi)Pd\nj=1exp(zj)1\u0014i\u0014d (7.9)\nThus for example given a vector\nz= [0:6;1:1;\u00001:5;1:2;3:2;\u00001:1]; (7.10)\nthe softmax function will normalize it to a probability distribution (shown rounded):\nsoftmax (z) = [ 0:055;0:090;0:0067 ;0:10;0:74;0:010] (7.11)\nYou may recall that we used softmax to create a probability distribution from a\nvector of real-valued numbers (computed from summing weights times features) in\nthe multinomial version of logistic regression in Chapter 5.\nThat means we can think of a neural network classi\ufb01er with one hidden layer\nas building a vector hwhich is a hidden layer representation of the input, and then\nrunning standard multinomial logistic regression on the features that the network\ndevelops in h. By contrast, in Chapter 5 the features were mainly designed by hand\nvia feature templates. So a neural network is like multinomial logistic regression,\nbut (a) with many layers, since a deep neural network is like layer after layer of lo-\ngistic regression classi\ufb01ers; (b) with those intermediate layers having many possible\nactivation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we\u2019ll\ncontinue to use sfor convenience to mean any activation function); (c) rather than\nforming the features by feature templates, the prior layers of the network induce the\nfeature representations themselves.\nHere are the \ufb01nal equations for a feedforward network with a single hidden layer,\nwhich takes an input vector x, outputs a probability distribution y, and is parameter-\nized by weight matrices WandUand a bias vector b:\nh=s(Wx+b)\nz=Uh\ny=softmax (z) (7.12)\nAnd just to remember the shapes of all our variables, x2Rn0,h2Rn1,b2Rn1,\nW2Rn1\u0002n0,U2Rn2\u0002n1, and the output vector y2Rn2. We\u2019ll call this network a 2-\nlayer network (we traditionally don\u2019t count the input layer when numbering layers,\nbut do count the output layer). So by this terminology logistic regression is a 1-layer\nnetwork.",
    "metadata": {
      "source": "7",
      "chunk_id": 11,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 CHAPTER 7 \u2022 N EURAL NETWORKS\n7.3.1 More details on feedforward networks\nLet\u2019s now set up some notation to make it easier to talk about deeper networks of\ndepth more than 2. We\u2019ll use superscripts in square brackets to mean layer num-\nbers, starting at 0 for the input layer. So W[1]will mean the weight matrix for the\n(\ufb01rst) hidden layer, and b[1]will mean the bias vector for the (\ufb01rst) hidden layer. nj\nwill mean the number of units at layer j. We\u2019ll use g(\u0001)to stand for the activation\nfunction, which will tend to be ReLU or tanh for intermediate layers and softmax\nfor output layers. We\u2019ll use a[i]to mean the output from layer i, and z[i]to mean the\ncombination of previous layer output, weights and biases W[i]a[i\u00001]+b[i]. The 0th\nlayer is for inputs, so we\u2019ll refer to the inputs xmore generally as a[0].\nThus we can re-represent our 2-layer net from Eq. 7.12 as follows:\nz[1]=W[1]a[0]+b[1]\na[1]=g[1](z[1])\nz[2]=W[2]a[1]+b[2]\na[2]=g[2](z[2])\n\u02c6y=a[2](7.13)\nNote that with this notation, the equations for the computation done at each layer are\nthe same. The algorithm for computing the forward step in an n-layer feedforward\nnetwork, given the input vector a[0]is thus simply:\nforiin1,...,n\nz[i]=W[i]a[i\u00001]+b[i]\na[i]=g[i](z[i])\n\u02c6y=a[n]\nIt\u2019s often useful to have a name for the \ufb01nal set of activations right before the \ufb01nal\nsoftmax. So however many layers we have, we\u2019ll generally call the unnormalized\nvalues in the \ufb01nal vector z[n], the vector of scores right before the \ufb01nal softmax, the\nlogits (see Eq. ??). logits\nThe need for non-linear activation functions One of the reasons we use non-\nlinear activation functions for each layer in a neural network is that if we did not, the\nresulting network is exactly equivalent to a single-layer network. Let\u2019s see why this\nis true. Imagine the \ufb01rst two layers of such a network of purely linear layers:\nz[1]=W[1]x+b[1]\nz[2]=W[2]z[1]+b[2]\nWe can rewrite the function that the network is computing as:\nz[2]=W[2]z[1]+b[2]\n=W[2](W[1]x+b[1])+b[2]\n=W[2]W[1]x+W[2]b[1]+b[2]\n=W0x+b0(7.14)\nThis generalizes to any number of layers. So without non-linear activation functions,\na multilayer network is just a notational variant of a single layer network with a\ndifferent set of weights, and we lose all the representational power of multilayer\nnetworks.",
    "metadata": {
      "source": "7",
      "chunk_id": 12,
      "token_count": 710,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\n7.4 \u2022 F EEDFORWARD NETWORKS FOR NLP: C LASSIFICATION 11\nReplacing the bias unit In describing networks, we will often use a slightly sim-\npli\ufb01ed notation that represents exactly the same function without referring to an ex-\nplicit bias node b. Instead, we add a dummy node a0to each layer whose value will\nalways be 1. Thus layer 0, the input layer, will have a dummy node a[0]\n0=1, layer 1\nwill have a[1]\n0=1, and so on. This dummy node still has an associated weight, and\nthat weight represents the bias value b. For example instead of an equation like\nh=s(Wx+b) (7.15)\nwe\u2019ll use:\nh=s(Wx) (7.16)\nBut now instead of our vector xhaving n0values: x=x1;:::;xn0, it will have n0+\n1 values, with a new 0th dummy value x0=1:x=x0;:::;xn0. And instead of\ncomputing each hjas follows:\nhj=s n0X\ni=1Wjixi+bj!\n; (7.17)\nwe\u2019ll instead use:\nhj=s n0X\ni=0Wjixi!\n; (7.18)\nwhere the value Wj0replaces what had been bj. Fig. 7.9 shows a visualization.\nx1x2xn0\u2026\u2026+1b\u2026UWh1y1y2yn2h2h3hn1\nx1x2xn0\u2026\u2026x0=1\u2026UWh1y1y2yn2h2h3hn1\n(a) (b)\nFigure 7.9 Replacing the bias node (shown in a) with x0(b).\nWe\u2019ll continue showing the bias as bwhen we go over the learning algorithm\nin Section 7.5, but then we\u2019ll switch to this simpli\ufb01ed notation without explicit bias\nterms for the rest of the book.\n7.4 Feedforward networks for NLP: Classi\ufb01cation\nLet\u2019s see how to apply feedforward networks to NLP tasks! In this section we\u2019ll\nlook at classi\ufb01cation tasks like sentiment analysis; in the next section we\u2019ll introduce\nneural language modeling.",
    "metadata": {
      "source": "7",
      "chunk_id": 13,
      "token_count": 505,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 CHAPTER 7 \u2022 N EURAL NETWORKS\nLet\u2019s begin with a simple 2-layer sentiment classi\ufb01er. You might imagine tak-\ning our logistic regression classi\ufb01er from Chapter 5, which corresponds to a 1-layer\nnetwork, and just adding a hidden layer. The input element xicould be scalar fea-\ntures like those in Fig. ??, e.g., x1= count(words2doc), x2= count(positive lexicon\nwords2doc), x3= 1 if \u201cno\u201d2doc, and so on. And the output layer \u02c6ycould have\ntwo nodes (one each for positive and negative), or 3 nodes (positive, negative, neu-\ntral), in which case \u02c6y1would be the estimated probability of positive sentiment, \u02c6y2\nthe probability of negative and \u02c6y3the probability of neutral. The resulting equations\nwould be just what we saw above for a 2-layer network (as always, we\u2019ll continue\nto use the sto stand for any non-linearity, whether sigmoid, ReLU or other).\nx= [x1;x2;:::xN](each xiis a hand-designed feature)\nh=s(Wx+b)\nz=Uh\n\u02c6y=softmax (z) (7.19)\nFig. 7.10 shows a sketch of this architecture. As we mentioned earlier, adding this\nhidden layer to our logistic regression classi\ufb01er allows the network to represent the\nnon-linear interactions between features. This alone might give us a better sentiment\nclassi\ufb01er.\nUW[n\u2a091]Hidden layerOutput layersoftmax[dh\u2a09n][dh\u2a091][3\u2a09dh]Input wordsp(+)h1h2h3hdh\u2026y1^y2^y3^xhyInput layer n=3 features[3\u2a091]x1x2x3dessertwasgreatpositive lexiconwords = 1count of \u201cno\u201d = 0wordcount=3p(-)p(neut)\nFigure 7.10 Feedforward network sentiment analysis using traditional hand-built features\nof the input text.\nMost applications of neural networks for NLP do something different, however.\nInstead of using hand-built human-engineered features as the input to our classi\ufb01er,\nwe draw on deep learning\u2019s ability to learn features from the data by representing\nwords as embeddings, like the word2vec or GloVe embeddings we saw in Chapter 6.\nThere are various ways to represent an input for classi\ufb01cation. One simple baseline\nis to apply some sort of pooling function to the embeddings of all the words in the pooling\ninput. For example, for a text with ninput words/tokens w1;:::;wn, we can turn the\nnembeddings e(w1);:::;e(wn)(each of dimensionality d) into a single embedding\nalso of dimensionality dby just summing the embeddings, or by taking their mean\n(summing and then dividing by n):\nxmean=1\nnnX\ni=1e(wi) (7.20)",
    "metadata": {
      "source": "7",
      "chunk_id": 14,
      "token_count": 682,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\n7.4 \u2022 F EEDFORWARD NETWORKS FOR NLP: C LASSIFICATION 13\nThere are many other options, like taking the element-wise max. The element-wise\nmax of a set of nvectors is a new vector whose kth element is the max of the kth\nelements of all the nvectors. Here are the equations for this classi\ufb01er assuming\nmean pooling; the architecture is sketched in Fig. 7.11:\nx=mean(e(w1);e(w2);:::;e(wn))\nh=s(Wx+b)\nz=Uh\n\u02c6y=softmax (z) (7.21)\nUW[d\u2a091]Hidden layerOutput layersoftmax[dh\u2a09d][dh\u2a091][3\u2a09dh]Input wordsp(+)embedding for\u201cgreat\u201dembedding for\u201cdessert\u201dh1h2h3hdh\u2026y1^y2^y3^xhyInput layer pooled embedding[3\u2a091]pooling+dessertwasgreatembedding for\u201cwas\u201dp(-)p(neut)\nFigure 7.11 Feedforward network sentiment analysis using a pooled embedding of the in-\nput words.\nWhile Eq. 7.21 shows how to classify a single example x, in practice we want\nto ef\ufb01ciently classify an entire test set of mexamples. We do this by vectorizing\nthe process, just as we saw with logistic regression; instead of using for-loops to go\nthrough each example, we\u2019ll use matrix multiplication to do the entire computation\nof an entire test set at once. First, we pack all the input feature vectors for each input\nxinto a single input matrix X, with each row ia row vector consisting of the pooled\nembedding for input example x(i)(i.e., the vector x(i)). If the dimensionality of our\npooled input embedding is d,Xwill be a matrix of shape [m\u0002d].\nWe will then need to slightly modify Eq. 7.21. Xis of shape [m\u0002d]andWis of\nshape [dh\u0002d], so we\u2019ll have to reorder how we multiply XandWand transpose W\nso they correctly multiply to yield a matrix Hof shape [m\u0002dh].1The bias vector b\nfrom Eq. 7.21 of shape [1\u0002dh]will now have to be replicated into a matrix of shape\n[m\u0002dh]. We\u2019ll need to similarly reorder the next step and transpose U. Finally, our\noutput matrix \u02c6Ywill be of shape [m\u00023](or more generally [m\u0002do], where dois\nthe number of output classes), with each row iof our output matrix \u02c6Yconsisting of\nthe output vector \u02c6y(i).\u2018 Here are the \ufb01nal equations for computing the output class\n1Note that we could have kept the original order of our products if we had instead made our input\nmatrix Xrepresent each input as a column vector instead of a row vector, making it of shape [d\u0002m]. But\nrepresenting inputs as row vectors is convenient and common in neural network models.",
    "metadata": {
      "source": "7",
      "chunk_id": 15,
      "token_count": 672,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n14 CHAPTER 7 \u2022 N EURAL NETWORKS\ndistribution for an entire test set:\nH=s(XW|+b)\nZ=HU|\n\u02c6Y=softmax (Z) (7.22)\nThe idea of using word2vec or GloVe embeddings as our input representation\u2014\nand more generally the idea of relying on another algorithm to have already learned\nan embedding representation for our input words\u2014is called pretraining . Using pretraining\npretrained embedding representations, whether simple static word embeddings like\nword2vec or the much more powerful contextual embeddings we\u2019ll introduce in\nChapter 11, is one of the central ideas of deep learning. (It\u2019s also possible, how-\never, to train the word embeddings as part of an NLP task; we\u2019ll talk about how to\ndo this in Section 7.7 in the context of the neural language modeling task.)\n7.5 Training Neural Nets\nA feedforward neural net is an instance of supervised machine learning in which we\nknow the correct output yfor each observation x. What the system produces, via\nEq. 7.13, is \u02c6 y, the system\u2019s estimate of the true y. The goal of the training procedure\nis to learn parameters W[i]andb[i]for each layer ithat make \u02c6 yfor each training\nobservation as close as possible to the true y.\nIn general, we do all this by drawing on the methods we introduced in Chapter 5\nfor logistic regression, so the reader should be comfortable with that chapter before\nproceeding.\nFirst, we\u2019ll need a loss function that models the distance between the system\noutput and the gold output, and it\u2019s common to use the loss function used for logistic\nregression, the cross-entropy loss .\nSecond, to \ufb01nd the parameters that minimize this loss function, we\u2019ll use the\ngradient descent optimization algorithm introduced in Chapter 5.\nThird, gradient descent requires knowing the gradient of the loss function, the\nvector that contains the partial derivative of the loss function with respect to each\nof the parameters. In logistic regression, for each observation we could directly\ncompute the derivative of the loss function with respect to an individual worb. But\nfor neural networks, with millions of parameters in many layers, it\u2019s much harder to\nsee how to compute the partial derivative of some weight in layer 1 when the loss\nis attached to some much later layer. How do we partial out the loss over all those\nintermediate layers? The answer is the algorithm called error backpropagation or\nbackward differentiation .\n7.5.1 Loss function\nThecross-entropy loss that is used in neural networks is the same one we saw forcross-entropy\nloss\nlogistic regression. If the neural network is being used as a binary classi\ufb01er, with\nthe sigmoid at the \ufb01nal layer, the loss function is the same logistic regression loss\nwe saw in Eq. ??:\nLCE(\u02c6y;y) =\u0000logp(yjx) =\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)] (7.23)\nIf we are using the network to classify into 3 or more classes, the loss function is\nexactly the same as the loss for multinomial regression that we saw in Chapter 5 on",
    "metadata": {
      "source": "7",
      "chunk_id": 16,
      "token_count": 698,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\n7.5 \u2022 T RAINING NEURAL NETS 15\npage ??. Let\u2019s brie\ufb02y summarize the explanation here for convenience. First, when\nwe have more than 2 classes we\u2019ll need to represent both yand\u02c6yas vectors. Let\u2019s\nassume we\u2019re doing hard classi\ufb01cation , where only one class is the correct one.\nThe true label yis then a vector with Kelements, each corresponding to a class,\nwithyc=1 if the correct class is c, with all other elements of ybeing 0. Recall that\na vector like this, with one value equal to 1 and the rest 0, is called a one-hot vector .\nAnd our classi\ufb01er will produce an estimate vector with Kelements \u02c6y, each element\n\u02c6ykof which represents the estimated probability p(yk=1jx).\nThe loss function for a single example xis the negative sum of the logs of the K\noutput classes, each weighted by their probability yk:\nLCE(\u02c6y;y) =\u0000KX\nk=1yklog\u02c6yk (7.24)\nWe can simplify this equation further; let\u2019s \ufb01rst rewrite the equation using the func-\ntion 1fgwhich evaluates to 1 if the condition in the brackets is true and to 0 oth-\nerwise. This makes it more obvious that the terms in the sum in Eq. 7.24 will be 0\nexcept for the term corresponding to the true class for which yk=1:\nLCE(\u02c6y;y) =\u0000KX\nk=11fyk=1glog\u02c6yk\nIn other words, the cross-entropy loss is simply the negative log of the output proba-\nbility corresponding to the correct class, and we therefore also call this the negative\nlog likelihood loss :negative log\nlikelihood loss\nLCE(\u02c6y;y) =\u0000log\u02c6yc(where cis the correct class) (7.25)\nPlugging in the softmax formula from Eq. 7.9, and with Kthe number of classes:\nLCE(\u02c6y;y) =\u0000logexp(zc)PK\nj=1exp(zj)(where cis the correct class) (7.26)\n7.5.2 Computing the Gradient\nHow do we compute the gradient of this loss function? Computing the gradient\nrequires the partial derivative of the loss function with respect to each parameter.\nFor a network with one weight layer and sigmoid output (which is what logistic\nregression is), we could simply use the derivative of the loss that we used for logistic\nregression in Eq. 7.27 (and derived in Section ??):\n\u00b6LCE(\u02c6y;y)\n\u00b6wj= ( \u02c6y\u0000y)xj\n= (s(w\u0001x+b)\u0000y)xj (7.27)\nOr for a network with one weight layer and softmax output (=multinomial logistic\nregression), we could use the derivative of the softmax loss from Eq. ??, shown for\na particular weight wkand input xi\n\u00b6LCE(\u02c6y;y)\n\u00b6wk;i=\u0000(yk\u0000\u02c6yk)xi\n=\u0000(yk\u0000p(yk=1jx))xi\n=\u0000 \nyk\u0000exp(wk\u0001x+bk)PK\nj=1exp(wj\u0001x+bj)!\nxi (7.28)",
    "metadata": {
      "source": "7",
      "chunk_id": 17,
      "token_count": 731,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\n16 CHAPTER 7 \u2022 N EURAL NETWORKS\nBut these derivatives only give correct updates for one weight layer: the last one!\nFor deep networks, computing the gradients for each weight is much more complex,\nsince we are computing the derivative with respect to weight parameters that appear\nall the way back in the very early layers of the network, even though the loss is\ncomputed only at the very end of the network.\nThe solution to computing this gradient is an algorithm called error backprop-\nagation orbackprop (Rumelhart et al., 1986). While backprop was invented spe-error back-\npropagation\ncially for neural networks, it turns out to be the same as a more general procedure\ncalled backward differentiation , which depends on the notion of computation\ngraphs . Let\u2019s see how that works in the next subsection.\n7.5.3 Computation Graphs\nA computation graph is a representation of the process of computing a mathematical\nexpression, in which the computation is broken down into separate operations, each\nof which is modeled as a node in a graph.\nConsider computing the function L(a;b;c) =c(a+2b). If we make each of the\ncomponent addition and multiplication operations explicit, and add names ( dande)\nfor the intermediate outputs, the resulting series of computations is:\nd=2\u0003b\ne=a+d\nL=c\u0003e\nWe can now represent this as a graph, with nodes for each operation, and di-\nrected edges showing the outputs from each operation as the inputs to the next, as\nin Fig. 7.12. The simplest use of computation graphs is to compute the value of\nthe function with some given inputs. In the \ufb01gure, we\u2019ve assumed the inputs a=3,\nb=1,c=\u00002, and we\u2019ve shown the result of the forward pass to compute the re-\nsultL(3;1;\u00002) =\u000010. In the forward pass of a computation graph, we apply each\noperation left to right, passing the outputs of each computation as the input to the\nnext node.\ne=a+dd = 2bL=cea=3b=1c=-2e=5d=2L=-10forward passabc\nFigure 7.12 Computation graph for the function L(a;b;c) =c(a+2b), with values for input\nnodes a=3,b=1,c=\u00002, showing the forward pass computation of L.\n7.5.4 Backward differentiation on computation graphs\nThe importance of the computation graph comes from the backward pass , which\nis used to compute the derivatives that we\u2019ll need for the weight update. In this\nexample our goal is to compute the derivative of the output function Lwith respect",
    "metadata": {
      "source": "7",
      "chunk_id": 18,
      "token_count": 580,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\n7.5 \u2022 T RAINING NEURAL NETS 17\nto each of the input variables, i.e.,\u00b6L\n\u00b6a,\u00b6L\n\u00b6b, and\u00b6L\n\u00b6c. The derivative\u00b6L\n\u00b6atells us how\nmuch a small change in aaffects L.\nBackwards differentiation makes use of the chain rule in calculus, so let\u2019s re- chain rule\nmind ourselves of that. Suppose we are computing the derivative of a composite\nfunction f(x) =u(v(x)). The derivative of f(x)is the derivative of u(x)with respect\ntov(x)times the derivative of v(x)with respect to x:\nd f\ndx=du\ndv\u0001dv\ndx(7.29)\nThe chain rule extends to more than two functions. If computing the derivative of a\ncomposite function f(x) =u(v(w(x))), the derivative of f(x)is:\nd f\ndx=du\ndv\u0001dv\ndw\u0001dw\ndx(7.30)\nThe intuition of backward differentiation is to pass gradients back from the \ufb01nal\nnode to all the nodes in the graph. Fig. 7.13 shows part of the backward computation\nat one node e. Each node takes an upstream gradient that is passed in from its parent\nnode to the right, and for each of its inputs computes a local gradient (the gradient\nof its output with respect to its input), and uses the chain rule to multiply these two\nto compute a downstream gradient to be passed on to the next earlier node.\nedLed\u2202L\u2202d\u2202L\u2202e=\u2202e\u2202d\u2202L\u2202e\u2202e\u2202dupstream gradientdownstream gradientlocal gradient\nFigure 7.13 Each node (like ehere) takes an upstream gradient, multiplies it by the local\ngradient (the gradient of its output with respect to its input), and uses the chain rule to compute\na downstream gradient to be passed on to a prior node. A node may have multiple local\ngradients if it has multiple inputs.\nLet\u2019s now compute the 3 derivatives we need. Since in the computation graph\nL=ce, we can directly compute the derivative\u00b6L\n\u00b6c:\n\u00b6L\n\u00b6c=e (7.31)\nFor the other two, we\u2019ll need to use the chain rule:\n\u00b6L\n\u00b6a=\u00b6L\n\u00b6e\u00b6e\n\u00b6a\n\u00b6L\n\u00b6b=\u00b6L\n\u00b6e\u00b6e\n\u00b6d\u00b6d\n\u00b6b(7.32)\nEq. 7.32 and Eq. 7.31 thus require \ufb01ve intermediate derivatives:\u00b6L\n\u00b6e,\u00b6L\n\u00b6c,\u00b6e\n\u00b6a,\u00b6e\n\u00b6d, and\n\u00b6d\n\u00b6b, which are as follows (making use of the fact that the derivative of a sum is the",
    "metadata": {
      "source": "7",
      "chunk_id": 19,
      "token_count": 614,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\n18 CHAPTER 7 \u2022 N EURAL NETWORKS\nsum of the derivatives):\nL=ce:\u00b6L\n\u00b6e=c;\u00b6L\n\u00b6c=e\ne=a+d:\u00b6e\n\u00b6a=1;\u00b6e\n\u00b6d=1\nd=2b:\u00b6d\n\u00b6b=2\nIn the backward pass, we compute each of these partials along each edge of the\ngraph from right to left, using the chain rule just as we did above. Thus we begin by\ncomputing the downstream gradients from node L, which are\u00b6L\n\u00b6eand\u00b6L\n\u00b6c. For node e,\nwe then multiply this upstream gradient\u00b6L\n\u00b6eby the local gradient (the gradient of the\noutput with respect to the input),\u00b6e\n\u00b6dto get the output we send back to node d:\u00b6L\n\u00b6d.\nAnd so on, until we have annotated the graph all the way to all the input variables.\nThe forward pass conveniently already will have computed the values of the forward\nintermediate variables we need (like dande) to compute these derivatives. Fig. 7.14\nshows the backward pass.\ne=d+ad = 2bL=cea=3b=1e=5d=2L=-10 abc\u2202L=5\u2202c\u2202L=-2\u2202e\u2202e=1\u2202d\u2202d=2\u2202b\u2202e=1\u2202abackward passc=-2\u2202L=-2\u2202e\u2202L=5\u2202c\u2202L\u2202d=-2\u2202e\u2202d\u2202L\u2202e=\u2202L\u2202a=-2\u2202e\u2202a\u2202L\u2202e=\u2202L\u2202b=-4\u2202d\u2202b\u2202L\u2202d=\nFigure 7.14 Computation graph for the function L(a;b;c) =c(a+2b), showing the backward pass computa-\ntion of\u00b6L\n\u00b6a,\u00b6L\n\u00b6b, and\u00b6L\n\u00b6c.\nBackward differentiation for a neural network\nOf course computation graphs for real neural networks are much more complex.\nFig. 7.15 shows a sample computation graph for a 2-layer neural network with n0=\n2,n1=2, and n2=1, assuming binary classi\ufb01cation and hence using a sigmoid\noutput unit for simplicity. The function that the computation graph is computing is:\nz[1]=W[1]x+b[1]\na[1]=ReLU (z[1])\nz[2]=W[2]a[1]+b[2]\na[2]=s(z[2])\n\u02c6y=a[2](7.33)",
    "metadata": {
      "source": "7",
      "chunk_id": 20,
      "token_count": 591,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\n7.5 \u2022 T RAINING NEURAL NETS 19\nFor the backward pass we\u2019ll also need to compute the loss L. The loss function\nfor binary sigmoid output from Eq. 7.23 is\nLCE(\u02c6y;y) =\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)] (7.34)\nOur output \u02c6 y=a[2], so we can rephrase this as\nLCE(a[2];y) =\u0000h\nyloga[2]+(1\u0000y)log(1\u0000a[2])i\n(7.35)\nz[2] = +a[2] = \u03c3 a[1] = ReLUz[1] = +b[1]****x1x2a[1] = ReLUz[1] = +b[1]**w[2]11w[1]11w[1]12\nw[1]21w[1]22b[2]w[2]12L (a[2],y)1\n2111\n22\nFigure 7.15 Sample computation graph for a simple 2-layer neural net (= 1 hidden layer) with two input units\nand 2 hidden units. We\u2019ve adjusted the notation a bit to avoid long equations in the nodes by just mentioning\nthe function that is being computed, and the resulting variable name. Thus the * to the right of node w[1]\n11means\nthatw[1]\n11is to be multiplied by x1, and the node z[1]= + means that the value of z[1]is computed by summing\nthe three nodes that feed into it (the two products, and the bias term b[1]\ni).\nThe weights that need updating (those for which we need to know the partial\nderivative of the loss function) are shown in teal. In order to do the backward pass,\nwe\u2019ll need to know the derivatives of all the functions in the graph. We already saw\nin Section ??the derivative of the sigmoid s:\nds(z)\ndz=s(z)(1\u0000s(z)) (7.36)\nWe\u2019ll also need the derivatives of each of the other activation functions. The\nderivative of tanh is:\ndtanh(z)\ndz=1\u0000tanh2(z) (7.37)\nThe derivative of the ReLU is2\ndReLU (z)\ndz=\u001a0f or z <0\n1f or z\u00150(7.38)\n2The derivative is actually unde\ufb01ned at the point z=0, but by convention we treat it as 1.",
    "metadata": {
      "source": "7",
      "chunk_id": 21,
      "token_count": 565,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\n20 CHAPTER 7 \u2022 N EURAL NETWORKS\nWe\u2019ll give the start of the computation, computing the derivative of the loss function\nLwith respect to z, or\u00b6L\n\u00b6z(and leaving the rest of the computation as an exercise for\nthe reader). By the chain rule:\n\u00b6L\n\u00b6z=\u00b6L\n\u00b6a[2]\u00b6a[2]\n\u00b6z(7.39)\nSo let\u2019s \ufb01rst compute\u00b6L\n\u00b6a[2], taking the derivative of Eq. 7.35, repeated here:\nLCE(a[2];y) =\u0000h\nyloga[2]+(1\u0000y)log(1\u0000a[2])i\n\u00b6L\n\u00b6a[2]=\u0000  \ny\u00b6log(a[2])\n\u00b6a[2]!\n+(1\u0000y)\u00b6log(1\u0000a[2])\n\u00b6a[2]!\n=\u0000\u0012\u0012\ny1\na[2]\u0013\n+(1\u0000y)1\n1\u0000a[2](\u00001)\u0013\n=\u0000\u0012y\na[2]+y\u00001\n1\u0000a[2]\u0013\n(7.40)\nNext, by the derivative of the sigmoid:\n\u00b6a[2]\n\u00b6z=a[2](1\u0000a[2])\nFinally, we can use the chain rule:\n\u00b6L\n\u00b6z=\u00b6L\n\u00b6a[2]\u00b6a[2]\n\u00b6z\n=\u0000\u0012y\na[2]+y\u00001\n1\u0000a[2]\u0013\na[2](1\u0000a[2])\n=a[2]\u0000y (7.41)\nContinuing the backward computation of the gradients (next by passing the gra-\ndients over b[2]\n1and the two product nodes, and so on, back to all the teal nodes), is\nleft as an exercise for the reader.\n7.5.5 More details on learning\nOptimization in neural networks is a non-convex optimization problem, more com-\nplex than for logistic regression, and for that and other reasons there are many best\npractices for successful learning.\nFor logistic regression we can initialize gradient descent with all the weights and\nbiases having the value 0. In neural networks, by contrast, we need to initialize the\nweights with small random numbers. It\u2019s also helpful to normalize the input values\nto have 0 mean and unit variance.\nVarious forms of regularization are used to prevent over\ufb01tting. One of the most\nimportant is dropout : randomly dropping some units and their connections from dropout\nthe network during training (Hinton et al. 2012, Srivastava et al. 2014). At each\niteration of training (whenever we update parameters, i.e. each mini-batch if we are\nusing mini-batch gradient descent), we repeatedly choose a probability pand for\neach unit we replace its output with zero with probability p(and renormalize the\nrest of the outputs from that layer).",
    "metadata": {
      "source": "7",
      "chunk_id": 22,
      "token_count": 635,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21\n\n7.6 \u2022 F EEDFORWARD NEURAL LANGUAGE MODELING 21\nTuning of hyperparameters is also important. The parameters of a neural net- hyperparameter\nwork are the weights Wand biases b; those are learned by gradient descent. The\nhyperparameters are things that are chosen by the algorithm designer; optimal val-\nues are tuned on a devset rather than by gradient descent learning on the training\nset. Hyperparameters include the learning rate h, the mini-batch size, the model\narchitecture (the number of layers, the number of hidden nodes per layer, the choice\nof activation functions), how to regularize, and so on. Gradient descent itself also\nhas many architectural variants such as Adam (Kingma and Ba, 2015).\nFinally, most modern neural networks are built using computation graph for-\nmalisms that make it easy and natural to do gradient computation and parallelization\non vector-based GPUs (Graphic Processing Units). PyTorch (Paszke et al., 2017)\nand TensorFlow (Abadi et al., 2015) are two of the most popular. The interested\nreader should consult a neural network textbook for further details; some sugges-\ntions are at the end of the chapter.\n7.6 Feedforward Neural Language Modeling\nAs our second application of feedforward networks, let\u2019s consider language mod-\neling : predicting upcoming words from prior words. Neural language modeling\u2014\nbased on the transformer architecture that we will see in Chapter 9\u2014is the algorithm\nthat underlies all of modern NLP. In this section and the next we\u2019ll introduce a sim-\npler version of neural language models for feedforward networks, an algorithm \ufb01rst\nintroduced by Bengio et al. (2003). The feedforward language model introduces\nmany of the important concepts of neural language modeling, concepts we\u2019ll return\nto as we describe more powerful models in Chapter 8 and Chapter 9.\nNeural language models have many advantages over the n-gram language mod-\nels of Chapter 3. Compared to n-gram models, neural language models can handle\nmuch longer histories, can generalize better over contexts of similar words, and are\nmore accurate at word-prediction. On the other hand, neural net language models\nare much more complex, are slower and need more energy to train, and are less inter-\npretable than n-gram models, so for some smaller tasks an n-gram language model\nis still the right tool.\nA feedforward neural language model (LM) is a feedforward network that takes\nas input at time ta representation of some number of previous words ( wt\u00001;wt\u00002,\netc.) and outputs a probability distribution over possible next words. Thus\u2014like the\nn-gram LM\u2014the feedforward neural LM approximates the probability of a word\ngiven the entire prior context P(wtjw1:t\u00001)by approximating based on the N\u00001\nprevious words:\nP(wtjw1;:::; wt\u00001)\u0019P(wtjwt\u0000N+1;:::; wt\u00001) (7.42)\nIn the following examples we\u2019ll use a 4-gram example, so we\u2019ll show a neural net to\nestimate the probability P(wt=ijwt\u00003;wt\u00002;wt\u00001).\nNeural language models represent words in this prior context by their embed-\ndings , rather than just by their word identity as used in n-gram language models.\nUsing embeddings allows neural language models to generalize better to unseen\ndata. For example, suppose we\u2019ve seen this sentence in training:\nI have to make sure that the cat gets fed.",
    "metadata": {
      "source": "7",
      "chunk_id": 23,
      "token_count": 766,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22\n\n22 CHAPTER 7 \u2022 N EURAL NETWORKS\nbut have never seen the words \u201cgets fed\u201d after the word \u201cdog\u201d. Our test set has the\npre\ufb01x \u201cI forgot to make sure that the dog gets\u201d. What\u2019s the next word? An n-gram\nlanguage model will predict \u201cfed\u201d after \u201cthat the cat gets\u201d, but not after \u201cthat the dog\ngets\u201d. But a neural LM, knowing that \u201ccat\u201d and \u201cdog\u201d have similar embeddings, will\nbe able to generalize from the \u201ccat\u201d context to assign a high enough probability to\n\u201cfed\u201d even after seeing \u201cdog\u201d.\n7.6.1 Forward inference in the neural language model\nLet\u2019s walk through forward inference ordecoding for neural language models.forward\ninference\nForward inference is the task, given an input, of running a forward pass on the\nnetwork to produce a probability distribution over possible outputs, in this case next\nwords.\nWe \ufb01rst represent each of the Nprevious words as a one-hot vector of length\njVj, i.e., with one dimension for each word in the vocabulary. A one-hot vector is one-hot vector\na vector that has one element equal to 1\u2014in the dimension corresponding to that\nword\u2019s index in the vocabulary\u2014 while all the other elements are set to zero. Thus\nin a one-hot representation for the word \u201ctoothpaste\u201d, supposing it is V5, i.e., index\n5 in the vocabulary, x5=1, and xi=08i6=5, as shown here:\n[0 0 0 0 1 0 0 ... 0 0 0 0]\n1 2 3 4 5 6 7 ... ... |V|\nThe feedforward neural language model (sketched in Fig. 7.17) has a moving\nwindow that can see N words into the past. We\u2019ll let N equal 3, so the 3 words\nwt\u00001,wt\u00002, and wt\u00003are each represented as a one-hot vector. We then multiply\nthese one-hot vectors by the embedding matrix E. The embedding weight matrix E\nhas a column for each word, each a column vector of ddimensions, and hence has\ndimensionality d\u0002jVj. Multiplying by a one-hot vector that has only one non-zero\nelement xi=1 simply selects out the relevant column vector for word i, resulting in\nthe embedding for word i, as shown in Fig. 7.16.\nE|V|d1|V|d1=\u271555e5\nFigure 7.16 Selecting the embedding vector for word V5by multiplying the embedding\nmatrix Ewith a one-hot vector with a 1 in index 5.\nThe 3 resulting embedding vectors are concatenated to produce e, the embedding\nlayer. This is followed by a hidden layer and an output layer whose softmax produces\na probability distribution over words. For example y42, the value of output node 42,\nis the probability of the next word wtbeing V42, the vocabulary word with index 42\n(which is the word \u2018\ufb01sh\u2019 in our example).\nHere\u2019s the algorithm in detail for our mini example:\n1.Select three embeddings from E : Given the three previous words, we look\nup their indices, create 3 one-hot vectors, and then multiply each by the em-\nbedding matrix E. Consider wt\u00003. The one-hot vector for \u2018for\u2019 (index 35) is\nmultiplied by the embedding matrix E, to give the \ufb01rst part of the \ufb01rst hidden",
    "metadata": {
      "source": "7",
      "chunk_id": 24,
      "token_count": 763,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23\n\n7.6 \u2022 F EEDFORWARD NEURAL LANGUAGE MODELING 23\nUWembedding layer3d\u2a091hiddenlayeroutput layersoftmaxdh\u2a093ddh\u2a091|V|\u2a09dhinput layerone-hot vectorsE\n|V|\u2a093d\u2a09|V|p(do|\u2026)p(aardvark|\u2026)\np(zebra|\u2026)p(fish|\u2026)\n|V|\u2a091EEh1h2y1\nh3hdh\u2026\u2026y34\ny|V|\u2026001001|V|35\n001001|V|451001001|V|9920\n0\u2026\u2026y42y35102^^^\n^^hexyforallthe?thanksand\u2026wt-3wt-2wt-1wt\u2026\nFigure 7.17 Forward inference in a feedforward neural language model. At each timestep\ntthe network computes a d-dimensional embedding for each context word (by multiplying a\none-hot vector by the embedding matrix E), and concatenates the 3 resulting embeddings to\nget the embedding layer e. The embedding vector eis multiplied by a weight matrix Wand\nthen an activation function is applied element-wise to produce the hidden layer h, which is\nthen multiplied by another weight matrix U. Finally, a softmax output layer predicts at each\nnode ithe probability that the next word wtwill be vocabulary word Vi.\nlayer, the embedding layer . Since each column of the input matrix Eis anembedding\nlayer\nembedding for a word, and the input is a one-hot column vector xifor word\nVi, the embedding layer for input wwill be Exi=ei, the embedding for word\ni. We now concatenate the three embeddings for the three context words to\nproduce the embedding layer e.\n2.Multiply by W : We multiply by W(and add b) and pass through the ReLU\n(or other) activation function to get the hidden layer h.\n3.Multiply by U :his now multiplied by U\n4.Apply softmax : After the softmax, each node iin the output layer estimates\nthe probability P(wt=ijwt\u00001;wt\u00002;wt\u00003)\nIn summary, the equations for a neural language model with a window size of 3,\ngiven one-hot input vectors for each input context word, are:\ne= [Ext\u00003;Ext\u00002;Ext\u00001]\nh=s(We+b)\nz=Uh\n\u02c6y=softmax (z) (7.43)\nNote that we formed the embedding layer eby concatenating the 3 embeddings\nfor the three context vectors; we\u2019ll often use semicolons to mean concatenation of\nvectors.",
    "metadata": {
      "source": "7",
      "chunk_id": 25,
      "token_count": 571,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24",
    "metadata": {
      "source": "7",
      "chunk_id": 26,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "24 CHAPTER 7 \u2022 N EURAL NETWORKS\n7.7 Training the neural language model\nThe high-level intuition of training neural language models, whether the simple\nfeedforward language models we describe here or the more powerful transformer\nlanguage models of Chapter 9, is the idea of self-training orself-supervision that self-training\nwe saw in Chapter 6 for learning word representations. In self-training for language\nmodeling, we take a corpus of text as training material and at each time step task\nthe model to predict the next word. At \ufb01rst it will do poorly at this task, but since\nin each case we know the correct answer (it\u2019s the next word in the corpus!) we can\neasily train it to be better at predicting the correct next word. We call such a model\nself-supervised because we don\u2019t have to add any special gold labels to the data;\nthe natural sequence of words is its own supervision! We simply train the model to\nminimize the error in predicting the true next word in the training sequence.\nIn practice, training the model means setting the parameters q=E;W;U;b. For\nsome tasks, it\u2019s ok to freeze the embedding layer Ewith initial word2vec values. freeze\nFreezing means we use word2vec or some other pretraining algorithm to compute\nthe initial embedding matrix E, and then hold it constant while we only modify W,\nU, and b, i.e., we don\u2019t update Eduring language model training. However, often\nwe\u2019d like to learn the embeddings simultaneously with training the network. This is\nuseful when the task the network is designed for (like sentiment classi\ufb01cation, trans-\nlation, or parsing) places strong constraints on what makes a good representation for\nwords.\nLet\u2019s see how to train the entire model including E, i.e. to set all the parameters\nq=E;W;U;b. We\u2019ll do this via gradient descent (Fig. ??), using error backprop-\nagation on the computation graph to compute the gradient. Training thus not only\nsets the weights WandUof the network, but also as we\u2019re predicting upcoming\nwords, we\u2019re learning the embeddings Efor each word that best predict upcoming\nwords.\nFig. 7.18 shows the set up for a window size of N=3 context words. The input x\nconsists of 3 one-hot vectors, fully connected to the embedding layer via 3 instanti-\nations of the embedding matrix E. We don\u2019t want to learn separate weight matrices\nfor mapping each of the 3 previous words to the projection layer. We want one single\nembedding dictionary Ethat\u2019s shared among these three. That\u2019s because over time,\nmany different words will appear as wt\u00002orwt\u00001, and we\u2019d like to just represent\neach word with one vector, whichever context position it appears in. Recall that the\nembedding weight matrix Ehas a column for each word, each a column vector of d\ndimensions, and hence has dimensionality d\u0002jVj.\nGenerally training proceeds by taking as input a very long text, concatenating all\nthe sentences, starting with random weights, and then iteratively moving through the\ntext predicting each word wt. At each word wt, we use the cross-entropy (negative\nlog likelihood) loss. Recall that the general form for this (repeated from Eq. 7.25)\nis:\nLCE(\u02c6y;y) =\u0000log \u02c6yi;(where iis the correct class) (7.44)\nFor language modeling, the classes are the words in the vocabulary, so \u02c6 yihere means\nthe probability that the model assigns to the correct next word wt:",
    "metadata": {
      "source": "7",
      "chunk_id": 27,
      "token_count": 774,
      "chapter_title": ""
    }
  },
  {
    "content": "consists of 3 one-hot vectors, fully connected to the embedding layer via 3 instanti-\nations of the embedding matrix E. We don\u2019t want to learn separate weight matrices\nfor mapping each of the 3 previous words to the projection layer. We want one single\nembedding dictionary Ethat\u2019s shared among these three. That\u2019s because over time,\nmany different words will appear as wt\u00002orwt\u00001, and we\u2019d like to just represent\neach word with one vector, whichever context position it appears in. Recall that the\nembedding weight matrix Ehas a column for each word, each a column vector of d\ndimensions, and hence has dimensionality d\u0002jVj.\nGenerally training proceeds by taking as input a very long text, concatenating all\nthe sentences, starting with random weights, and then iteratively moving through the\ntext predicting each word wt. At each word wt, we use the cross-entropy (negative\nlog likelihood) loss. Recall that the general form for this (repeated from Eq. 7.25)\nis:\nLCE(\u02c6y;y) =\u0000log \u02c6yi;(where iis the correct class) (7.44)\nFor language modeling, the classes are the words in the vocabulary, so \u02c6 yihere means\nthe probability that the model assigns to the correct next word wt:\nLCE=\u0000logp(wtjwt\u00001;:::;wt\u0000n+1) (7.45)\nThe parameter update for stochastic gradient descent for this loss from step stos+1",
    "metadata": {
      "source": "7",
      "chunk_id": 28,
      "token_count": 323,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 25\n\n7.8 \u2022 S UMMARY 25\nUWembedding layer3d\u2a091hiddenlayeroutput layersoftmaxdh\u2a093ddh\u2a091|V|\u2a09dhinput layerone-hot vectorsE\n|V|\u2a093d\u2a09|V|p(do|\u2026)p(aardvark|\u2026)\np(zebra|\u2026)p(fish|\u2026)\n|V|\u2a091EEh1h2y1\nh3hdh\u2026\u2026y34\ny|V|\u2026001001|V|35\n001001|V|451001001|V|9920\n0\u2026\u2026y42y35102^^^\n^^hexyforallthefishthanksand\u2026wt-3wt-2wt-1wt\u2026L = \u2212log P(fish | for, all, the)wt=fish\nFigure 7.18 Learning all the way back to embeddings. Again, the embedding matrix Eis\nshared among the 3 context words.\nis then:\nqs+1=qs\u0000h\u00b6[\u0000logp(wtjwt\u00001;:::;wt\u0000n+1)]\n\u00b6q(7.46)\nThis gradient can be computed in any standard neural network framework which\nwill then backpropagate through q=E;W;U;b.\nTraining the parameters to minimize loss will result both in an algorithm for\nlanguage modeling (a word predictor) but also a new set of embeddings Ethat can\nbe used as word representations for other tasks.\n7.8 Summary\n\u2022 Neural networks are built out of neural units , originally inspired by biological\nneurons but now simply an abstract computational device.\n\u2022 Each neural unit multiplies input values by a weight vector, adds a bias, and\nthen applies a non-linear activation function like sigmoid, tanh, or recti\ufb01ed\nlinear unit.\n\u2022 In a fully-connected ,feedforward network, each unit in layer iis connected\nto each unit in layer i+1, and there are no cycles.\n\u2022 The power of neural networks comes from the ability of early layers to learn\nrepresentations that can be utilized by later layers in the network.\n\u2022 Neural networks are trained by optimization algorithms like gradient de-\nscent .\n\u2022Error backpropagation , backward differentiation on a computation graph ,\nis used to compute the gradients of the loss function for a network.",
    "metadata": {
      "source": "7",
      "chunk_id": 29,
      "token_count": 502,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 26\n\n26 CHAPTER 7 \u2022 N EURAL NETWORKS\n\u2022Neural language models use a neural network as a probabilistic classi\ufb01er, to\ncompute the probability of the next word given the previous nwords.\n\u2022 Neural language models can use pretrained embeddings , or can learn embed-\ndings from scratch in the process of language modeling.\nBibliographical and Historical Notes\nThe origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCul-\nloch and Pitts, 1943), a simpli\ufb01ed model of the biological neuron as a kind of com-\nputing element that could be described in terms of propositional logic. By the late\n1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and\nBernard Widrow at Stanford) developed research into neural networks; this phase\nsaw the development of the perceptron (Rosenblatt, 1958), and the transformation\nof the threshold into a bias, a notation we still use (Widrow and Hoff, 1960).\nThe \ufb01eld of neural networks declined after it was shown that a single perceptron\nunit was unable to model functions as simple as XOR (Minsky and Papert, 1969).\nWhile some small amount of work continued during the next two decades, a major\nrevival for the \ufb01eld didn\u2019t come until the 1980s, when practical tools for building\ndeeper networks like error backpropagation became widespread (Rumelhart et al.,\n1986). During the 1980s a wide variety of neural network and related architec-\ntures were developed, particularly for applications in psychology and cognitive sci-\nence (Rumelhart and McClelland 1986b, McClelland and Elman 1986, Rumelhart\nand McClelland 1986a, Elman 1990), for which the term connectionist orparal- connectionist\nlel distributed processing was often used (Feldman and Ballard 1982, Smolensky\n1988). Many of the principles and techniques developed in this period are foun-\ndational to modern work, including the ideas of distributed representations (Hinton,\n1986), recurrent networks (Elman, 1990), and the use of tensors for compositionality\n(Smolensky, 1990).\nBy the 1990s larger neural networks began to be applied to many practical lan-\nguage processing tasks as well, like handwriting recognition (LeCun et al. 1989) and\nspeech recognition (Morgan and Bourlard 1990). By the early 2000s, improvements\nin computer hardware and advances in optimization and training techniques made it\npossible to train even larger and deeper networks, leading to the modern term deep\nlearning (Hinton et al. 2006, Bengio et al. 2007). We cover more related history in\nChapter 8 and Chapter 16.\nThere are a number of excellent books on the subject. Goldberg (2017) has\nsuperb coverage of neural networks for natural language processing. For neural\nnetworks in general see Goodfellow et al. (2016) and Nielsen (2015).",
    "metadata": {
      "source": "7",
      "chunk_id": 30,
      "token_count": 688,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 27",
    "metadata": {
      "source": "7",
      "chunk_id": 31,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Bibliographical and Historical Notes 27\nAbadi, M., A. Agarwal, P. Barham, E. Brevdo, Z. Chen,\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Is-\nard, Y . Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Leven-\nberg, D. Man \u00b4e, R. Monga, S. Moore, D. Murray, C. Olah,\nM. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Tal-\nwar, P. Tucker, V . Vanhoucke, V . Vasudevan, F. Vi \u00b4egas,\nO. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y . Yu,\nand X. Zheng. 2015. TensorFlow: Large-scale machine\nlearning on heterogeneous systems. Software available\nfrom tensor\ufb02ow.org.\nBengio, Y ., R. Ducharme, P. Vincent, and C. Jauvin. 2003.\nA neural probabilistic language model. JMLR , 3:1137\u2013\n1155.\nBengio, Y ., P. Lamblin, D. Popovici, and H. Larochelle.\n2007. Greedy layer-wise training of deep networks.\nNeurIPS .\nElman, J. L. 1990. Finding structure in time. Cognitive sci-\nence, 14(2):179\u2013211.\nFeldman, J. A. and D. H. Ballard. 1982. Connectionist mod-\nels and their properties. Cognitive Science , 6:205\u2013254.\nGoldberg, Y . 2017. Neural Network Methods for Natural\nLanguage Processing , volume 10 of Synthesis Lectures\non Human Language Technologies . Morgan & Claypool.\nGoodfellow, I., Y . Bengio, and A. Courville. 2016. Deep\nLearning . MIT Press.\nHinton, G. E. 1986. Learning distributed representations of\nconcepts. COGSCI .\nHinton, G. E., S. Osindero, and Y .-W. Teh. 2006. A fast\nlearning algorithm for deep belief nets. Neural computa-\ntion, 18(7):1527\u20131554.\nHinton, G. E., N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov. 2012. Improving neural networks\nby preventing co-adaptation of feature detectors. ArXiv\npreprint arXiv:1207.0580.\nKingma, D. and J. Ba. 2015. Adam: A method for stochastic\noptimization. ICLR 2015 .\nLeCun, Y ., B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel. 1989. Backprop-\nagation applied to handwritten zip code recognition. Neu-\nral computation , 1(4):541\u2013551.\nMcClelland, J. L. and J. L. Elman. 1986. The TRACE model\nof speech perception. Cognitive Psychology , 18:1\u201386.",
    "metadata": {
      "source": "7",
      "chunk_id": 32,
      "token_count": 764,
      "chapter_title": ""
    }
  },
  {
    "content": "Hinton, G. E. 1986. Learning distributed representations of\nconcepts. COGSCI .\nHinton, G. E., S. Osindero, and Y .-W. Teh. 2006. A fast\nlearning algorithm for deep belief nets. Neural computa-\ntion, 18(7):1527\u20131554.\nHinton, G. E., N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov. 2012. Improving neural networks\nby preventing co-adaptation of feature detectors. ArXiv\npreprint arXiv:1207.0580.\nKingma, D. and J. Ba. 2015. Adam: A method for stochastic\noptimization. ICLR 2015 .\nLeCun, Y ., B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel. 1989. Backprop-\nagation applied to handwritten zip code recognition. Neu-\nral computation , 1(4):541\u2013551.\nMcClelland, J. L. and J. L. Elman. 1986. The TRACE model\nof speech perception. Cognitive Psychology , 18:1\u201386.\nMcCulloch, W. S. and W. Pitts. 1943. A logical calculus of\nideas immanent in nervous activity. Bulletin of Mathe-\nmatical Biophysics , 5:115\u2013133.\nMinsky, M. and S. Papert. 1969. Perceptrons . MIT Press.\nMorgan, N. and H. Bourlard. 1990. Continuous speech\nrecognition using multilayer perceptrons with hidden\nmarkov models. ICASSP .\nNielsen, M. A. 2015. Neural networks and Deep learning .\nDetermination Press USA.\nPaszke, A., S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-\nVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer.\n2017. Automatic differentiation in pytorch. NIPS-W .\nRosenblatt, F. 1958. The perceptron: A probabilistic model\nfor information storage and organization in the brain. Psy-\nchological review , 65(6):386\u2013408.Rumelhart, D. E., G. E. Hinton, and R. J. Williams. 1986.\nLearning internal representations by error propagation. In\nD. E. Rumelhart and J. L. McClelland, eds, Parallel Dis-\ntributed Processing , volume 2, 318\u2013362. MIT Press.\nRumelhart, D. E. and J. L. McClelland. 1986a. On learning\nthe past tense of English verbs. In D. E. Rumelhart and\nJ. L. McClelland, eds, Parallel Distributed Processing ,\nvolume 2, 216\u2013271. MIT Press.\nRumelhart, D. E. and J. L. McClelland, eds. 1986b. Parallel\nDistributed Processing . MIT Press.\nRussell, S. and P. Norvig. 2002. Arti\ufb01cial Intelligence: A\nModern Approach , 2nd edition. Prentice Hall.\nSmolensky, P. 1988. On the proper treatment of connection-",
    "metadata": {
      "source": "7",
      "chunk_id": 33,
      "token_count": 760,
      "chapter_title": ""
    }
  },
  {
    "content": "2017. Automatic differentiation in pytorch. NIPS-W .\nRosenblatt, F. 1958. The perceptron: A probabilistic model\nfor information storage and organization in the brain. Psy-\nchological review , 65(6):386\u2013408.Rumelhart, D. E., G. E. Hinton, and R. J. Williams. 1986.\nLearning internal representations by error propagation. In\nD. E. Rumelhart and J. L. McClelland, eds, Parallel Dis-\ntributed Processing , volume 2, 318\u2013362. MIT Press.\nRumelhart, D. E. and J. L. McClelland. 1986a. On learning\nthe past tense of English verbs. In D. E. Rumelhart and\nJ. L. McClelland, eds, Parallel Distributed Processing ,\nvolume 2, 216\u2013271. MIT Press.\nRumelhart, D. E. and J. L. McClelland, eds. 1986b. Parallel\nDistributed Processing . MIT Press.\nRussell, S. and P. Norvig. 2002. Arti\ufb01cial Intelligence: A\nModern Approach , 2nd edition. Prentice Hall.\nSmolensky, P. 1988. On the proper treatment of connection-\nism. Behavioral and brain sciences , 11(1):1\u201323.\nSmolensky, P. 1990. Tensor product variable binding and\nthe representation of symbolic structures in connectionist\nsystems. Arti\ufb01cial intelligence , 46(1-2):159\u2013216.\nSrivastava, N., G. E. Hinton, A. Krizhevsky, I. Sutskever,\nand R. R. Salakhutdinov. 2014. Dropout: a simple\nway to prevent neural networks from over\ufb01tting. JMLR ,\n15(1):1929\u20131958.\nWidrow, B. and M. E. Hoff. 1960. Adaptive switching cir-\ncuits. IRE WESCON Convention Record , volume 4.",
    "metadata": {
      "source": "7",
      "chunk_id": 34,
      "token_count": 459,
      "chapter_title": ""
    }
  }
]