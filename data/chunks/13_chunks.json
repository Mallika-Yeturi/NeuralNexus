[
  {
    "content": "# 13\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n13Machine Translation\n\u201cI want to talk the dialect of your people. It\u2019s no use of talking unless\npeople understand what you say.\u201d\nZora Neale Hurston, Moses, Man of the Mountain 1939, p. 121\nThis chapter introduces machine translation (MT), the use of computers to trans-machine\ntranslation\nMT late from one language to another.\nOf course translation, in its full generality, such as the translation of literature, or\npoetry, is a dif\ufb01cult, fascinating, and intensely human endeavor, as rich as any other\narea of human creativity.\nMachine translation in its present form therefore focuses on a number of very\npractical tasks. Perhaps the most common current use of machine translation is\nforinformation access . We might want to translate some instructions on the web,information\naccess\nperhaps the recipe for a favorite dish, or the steps for putting together some furniture.\nOr we might want to read an article in a newspaper, or get information from an\nonline resource like Wikipedia or a government webpage in some other language.\nMT for information\naccess is probably\none of the most com-\nmon uses of NLP\ntechnology, and Google\nTranslate alone (shown above) translates hundreds of billions of words a day be-\ntween over 100 languages. Improvements in machine translation can thus help re-\nduce what is often called the digital divide in information access: the fact that much digital divide\nmore information is available in English and other languages spoken in wealthy\ncountries. Web searches in English return much more information than searches in\nother languages, and online resources like Wikipedia are much larger in English and\nother higher-resourced languages. High-quality translation can help provide infor-\nmation to speakers of lower-resourced languages.\nAnother common use of machine translation is to aid human translators. MT sys-\ntems are routinely used to produce a draft translation that is \ufb01xed up in a post-editing post-editing\nphase by a human translator. This task is often called computer-aided translation\norCAT . CAT is commonly used as part of localization : the task of adapting content CAT\nlocalization or a product to a particular language community.\nFinally, a more recent application of MT is to in-the-moment human commu-\nnication needs. This includes incremental translation, translating speech on-the-\ufb02y\nbefore the entire sentence is complete, as is commonly used in simultaneous inter-\npretation. Image-centric translation can be used for example to use OCR of the text\non a phone camera image as input to an MT system to translate menus or street signs.\nThe standard algorithm for MT is the encoder-decoder network, an architectureencoder-\ndecoder\nthat we introduced in Chapter 8 for RNNs. Recall that encoder-decoder or sequence-\nto-sequence models are used for tasks in which we need to map an input sequence to\nan output sequence that is a complex function of the entire input sequence. Indeed,",
    "metadata": {
      "source": "13",
      "chunk_id": 0,
      "token_count": 656,
      "chapter_title": "13"
    }
  },
  {
    "content": "## Page 2\n\n2CHAPTER 13 \u2022 M ACHINE TRANSLATION\nin machine translation, the words of the target language don\u2019t necessarily agree with\nthe words of the source language in number or order. Consider translating the fol-\nlowing made-up English sentence into Japanese.\n(13.1) English: He wrote a letter to a friend\nJapanese: tomodachi\nfriendni\ntotegami-o\nletterkaita\nwrote\nNote that the elements of the sentences are in very different places in the different\nlanguages. In English, the verb is in the middle of the sentence, while in Japanese,\nthe verb kaita comes at the end. The Japanese sentence doesn\u2019t require the pronoun\nhe, while English does.\nSuch differences between languages can be quite complex. In the following ac-\ntual sentence from the United Nations, notice the many changes between the Chinese\nsentence (we\u2019ve given in red a word-by-word gloss of the Chinese characters) and\nits English equivalent produced by human translators.\n(13.2)\u5927\u4f1a/General Assembly \u5728/on 1982\u5e74/1982 12\u6708/December 10 \u65e5/10\u901a\u8fc7\n\u4e86/adopted\u7b2c37\u53f7/37th\u51b3\u8bae/resolution \uff0c\u6838\u51c6\u4e86/approved \u7b2c\u4e8c\n\u6b21/second\u63a2\u7d22/exploration \u53ca/and\u548c\u5e73peaceful\u5229\u7528/using\u5916\u5c42\u7a7a\n\u95f4/outer space \u4f1a\u8bae/conference \u7684/of\u5404\u9879/various\u5efa\u8bae/suggestions \u3002\nOn 10 December 1982 , the General Assembly adopted resolution 37 in\nwhich it endorsed the recommendations of the Second United Nations\nConference on the Exploration and Peaceful Uses of Outer Space .\nNote the many ways the English and Chinese differ. For example the order-\ning differs in major ways; the Chinese order of the noun phrase is \u201cpeaceful using\nouter space conference of suggestions\u201d while the English has \u201csuggestions of the ...\nconference on peaceful use of outer space\u201d). And the order differs in minor ways\n(the date is ordered differently). English requires thein many places that Chinese\ndoesn\u2019t, and adds some details (like \u201cin which\u201d and \u201cit\u201d) that aren\u2019t necessary in\nChinese. Chinese doesn\u2019t grammatically mark plurality on nouns (unlike English,\nwhich has the \u201c-s\u201d in \u201crecommendations\u201d), and so the Chinese must use the modi-\n\ufb01er\u5404\u9879/various to make it clear that there is not just one recommendation. English\ncapitalizes some words but not others. Encoder-decoder networks are very success-\nful at handling these sorts of complicated cases of sequence mappings.\nWe\u2019ll begin in the next section by considering the linguistic background about\nhow languages vary, and the implications this variance has for the task of MT. Then\nwe\u2019ll sketch out the standard algorithm, give details about things like input tokeniza-\ntion and creating training corpora of parallel sentences, give some more low-level\ndetails about the encoder-decoder network, and \ufb01nally discuss how MT is evaluated,\nintroducing the simple chrF metric.\n13.1 Language Divergences and Typology\nThere are about 7,000 languages in the world. Some aspects of human language\nseem to be universal , holding true for every one of these languages, or are statistical universal\nuniversals, holding true for most of these languages. Many universals arise from the\nfunctional role of language as a communicative system by humans. Every language,\nfor example, seems to have words for referring to people, for talking about eating\nand drinking, for being polite or not. There are also structural linguistic univer-\nsals; for example, every language seems to have nouns and verbs (Chapter 17), has",
    "metadata": {
      "source": "13",
      "chunk_id": 1,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\n13.1 \u2022 L ANGUAGE DIVERGENCES AND TYPOLOGY 3\nways to ask questions, or issue commands, has linguistic mechanisms for indicating\nagreement or disagreement.\nYet languages also differ in many ways (as has been pointed out since ancient\ntimes; see Fig. 13.1). Understanding what causes such translation divergencestranslation\ndivergence\n(Dorr, 1994) can help us build better MT models. We often distinguish the idiosyn-\ncratic and lexical differences that must be dealt with one by one (the word for \u201cdog\u201d\ndiffers wildly from language to language), from systematic differences that we can\nmodel in a general way (many languages put the verb before the grammatical ob-\nject; others put the verb after the grammatical object). The study of these systematic\ncross-linguistic similarities and differences is called linguistic typology . This sec- typology\ntion sketches some typological facts that impact machine translation; the interested\nreader should also look into WALS, the World Atlas of Language Structures, which\ngives many typological facts about languages (Dryer and Haspelmath, 2013).\nFigure 13.1 The Tower of Babel, Pieter Bruegel 1563. Wikimedia Commons, from the\nKunsthistorisches Museum, Vienna.\n13.1.1 Word Order Typology\nAs we hinted at in our example above comparing English and Japanese, languages\ndiffer in the basic word order of verbs, subjects, and objects in simple declara-\ntive clauses. German, French, English, and Mandarin, for example, are all SVO SVO\n(Subject-Verb-Object ) languages, meaning that the verb tends to come between\nthe subject and object. Hindi and Japanese, by contrast, are SOV languages, mean- SOV\ning that the verb tends to come at the end of basic clauses, and Irish and Arabic are\nVSO languages. Two languages that share their basic word order type often have VSO\nother similarities. For example, VOlanguages generally have prepositions , whereas\nOVlanguages generally have postpositions .\nLet\u2019s look in more detail at the example we saw above. In this SVO English\nsentence, the verb wrote is followed by its object a letter and the prepositional phrase",
    "metadata": {
      "source": "13",
      "chunk_id": 2,
      "token_count": 487,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4CHAPTER 13 \u2022 M ACHINE TRANSLATION\nto a friend , in which the preposition tois followed by its argument a friend . Arabic,\nwith a VSO order, also has the verb before the object and prepositions. By contrast,\nin the Japanese example that follows, each of these orderings is reversed; the verb is\npreceded by its arguments, and the postposition follows its argument.\n(13.3) English: He wrote a letter to a friend\nJapanese: tomodachi\nfriendni\ntotegami-o\nletterkaita\nwrote\nArabic: katabt\nwroteris\u00afala\nletterli\nto\u02d9sadq\nfriend\nOther kinds of ordering preferences vary idiosyncratically from language to lan-\nguage. In some SVO languages (like English and Mandarin) adjectives tend to ap-\npear before nouns, while in others languages like Spanish and Modern Hebrew, ad-\njectives appear after the noun:\n(13.4) Spanish bruja verde English green witch\n(a) (b)\nFigure 13.2 Examples of other word order differences: (a) In German, adverbs occur in\ninitial position that in English are more natural later, and tensed verbs occur in second posi-\ntion. (b) In Mandarin, preposition phrases expressing goals often occur pre-verbally, unlike\nin English.\nFig. 13.2 shows examples of other word order differences. All of these word\norder differences between languages can cause problems for translation, requiring\nthe system to do huge structural reorderings as it generates the output.\n13.1.2 Lexical Divergences\nOf course we also need to translate the individual words from one language to an-\nother. For any translation, the appropriate word can vary depending on the context.\nThe English source-language word bass, for example, can appear in Spanish as the\n\ufb01shlubina or the musical instrument bajo. German uses two distinct words for what\nin English would be called a wall:Wand for walls inside a building, and Mauer for\nwalls outside a building. Where English uses the word brother for any male sib-\nling, Chinese and many other languages have distinct words for older brother and\nyounger brother (Mandarin gege anddidi, respectively). In all these cases, trans-\nlating bass,wall, orbrother from English would require a kind of specialization,\ndisambiguating the different uses of a word. For this reason the \ufb01elds of MT and\nWord Sense Disambiguation (Appendix G) are closely linked.\nSometimes one language places more grammatical constraints on word choice\nthan another. We saw above that English marks nouns for whether they are singular\nor plural. Mandarin doesn\u2019t. Or French and Spanish, for example, mark grammat-\nical gender on adjectives, so an English translation into French requires specifying\nadjective gender.\nThe way that languages differ in lexically dividing up conceptual space may be\nmore complex than this one-to-many translation problem, leading to many-to-many",
    "metadata": {
      "source": "13",
      "chunk_id": 3,
      "token_count": 649,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\n13.1 \u2022 L ANGUAGE DIVERGENCES AND TYPOLOGY 5\nmappings. For example, Fig. 13.3 summarizes some of the complexities discussed\nby Hutchins and Somers (1992) in translating English leg, foot , and paw, to French.\nFor example, when legis used about an animal it\u2019s translated as French patte ; but\nabout the leg of a journey, as French etape ; if the leg is of a chair, we use French\npied.\nFurther, one language may have a lexical gap , where no word or phrase, short lexical gap\nof an explanatory footnote, can express the exact meaning of a word in the other\nlanguage. For example, English does not have a word that corresponds neatly to\nMandarin xi`aoor Japanese oyak \u00afok\u00afo(in English one has to make do with awkward\nphrases like \ufb01lial piety orloving child , orgood son/daughter for both).\netapepattejambepied   paw        footlegJOURNEYANIMALHUMANCHAIRANIMALBIRDHUMAN\nFigure 13.3 The complex overlap between English leg,foot, etc., and various French trans-\nlations as discussed by Hutchins and Somers (1992).\nFinally, languages differ systematically in how the conceptual properties of an\nevent are mapped onto speci\ufb01c words. Talmy (1985, 1991) noted that languages\ncan be characterized by whether direction of motion and manner of motion are\nmarked on the verb or on the \u201csatellites\u201d: particles, prepositional phrases, or ad-\nverbial phrases. For example, a bottle \ufb02oating out of a cave would be described in\nEnglish with the direction marked on the particle out, while in Spanish the direction\nwould be marked on the verb:\n(13.5) English: The bottle \ufb02oated out.\nSpanish: La\nThebotella\nbottlesali\u00b4o\nexited\ufb02otando .\n\ufb02oating.\nVerb-framed languages mark the direction of motion on the verb (leaving the verb-framed\nsatellites to mark the manner of motion), like Spanish acercarse \u2018approach\u2019, al-\ncanzar \u2018reach\u2019, entrar \u2018enter\u2019, salir \u2018exit\u2019. Satellite-framed languages mark the satellite-framed\ndirection of motion on the satellite (leaving the verb to mark the manner of motion),\nlike English crawl out ,\ufb02oat off ,jump down ,run after . Languages like Japanese,\nTamil, and the many languages in the Romance, Semitic, and Mayan languages fam-\nilies, are verb-framed; Chinese as well as non-Romance Indo-European languages\nlike English, Swedish, Russian, Hindi, and Farsi are satellite framed (Talmy 1991,\nSlobin 1996).\n13.1.3 Morphological Typology\nMorphologically , languages are often characterized along two dimensions of vari-\nation. The \ufb01rst is the number of morphemes per word, ranging from isolating isolating\nlanguages like Vietnamese and Cantonese, in which each word generally has one\nmorpheme, to polysynthetic languages like Siberian Yupik (\u201cEskimo\u201d), in which a polysynthetic\nsingle word may have very many morphemes, corresponding to a whole sentence in\nEnglish. The second dimension is the degree to which morphemes are segmentable,\nranging from agglutinative languages like Turkish, in which morphemes have rel- agglutinative\natively clean boundaries, to fusion languages like Russian, in which a single af\ufb01x fusion",
    "metadata": {
      "source": "13",
      "chunk_id": 4,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6CHAPTER 13 \u2022 M ACHINE TRANSLATION\nmay con\ufb02ate multiple morphemes, like -om in the word stolom (table- SG-INSTR -\nDECL 1), which fuses the distinct morphological categories instrumental, singular,\nand \ufb01rst declension.\nTranslating between languages with rich morphology requires dealing with struc-\nture below the word level, and for this reason modern systems generally use subword\nmodels like the wordpiece or BPE models of Section 13.2.1.\n13.1.4 Referential density\nFinally, languages vary along a typological dimension related to the things they tend\nto omit. Some languages, like English, require that we use an explicit pronoun when\ntalking about a referent that is given in the discourse. In other languages, however,\nwe can sometimes omit pronouns altogether, as the following example from Spanish\nshows1:\n(13.6) [El jefe] idio con un libro. / 0 iMostr \u00b4o su hallazgo a un descifrador ambulante.\n[The boss] came upon a book. [He] showed his \ufb01nd to a wandering decoder.\nLanguages that can omit pronouns are called pro-drop languages. Even among pro-drop\nthe pro-drop languages, there are marked differences in frequencies of omission.\nJapanese and Chinese, for example, tend to omit far more than does Spanish. This\ndimension of variation across languages is called the dimension of referential den-\nsity. We say that languages that tend to use more pronouns are more referentiallyreferential\ndensity\ndense than those that use more zeros. Referentially sparse languages, like Chinese or\nJapanese, that require the hearer to do more inferential work to recover antecedents\nare also called cold languages. Languages that are more explicit and make it easier cold language\nfor the hearer are called hotlanguages. The terms hotandcold are borrowed from hot language\nMarshall McLuhan\u2019s 1964 distinction between hot media like movies, which \ufb01ll in\nmany details for the viewer, versus cold media like comics, which require the reader\nto do more inferential work to \ufb01ll out the representation (Bickel, 2003).\nTranslating from languages with extensive pro-drop, like Chinese or Japanese, to\nnon-pro-drop languages like English can be dif\ufb01cult since the model must somehow\nidentify each zero and recover who or what is being talked about in order to insert\nthe proper pronoun.\n13.2 Machine Translation using Encoder-Decoder\nThe standard architecture for MT is the encoder-decoder transformer orsequence-\nto-sequence model, an architecture we saw for RNNs in Chapter 8. We\u2019ll see the\ndetails of how to apply this architecture to transformers in Section 13.3, but \ufb01rst let\u2019s\ntalk about the overall task.\nMost machine translation tasks make the simpli\ufb01cation that we can translate each\nsentence independently, so we\u2019ll just consider individual sentences for now. Given\na sentence in a source language, the MT task is then to generate a corresponding\nsentence in a target language. For example, an MT system is given an English\nsentence like\nThe green witch arrived\nand must translate it into the Spanish sentence:\n1Here we use the / 0-notation; we\u2019ll introduce this and discuss this issue further in Chapter 23",
    "metadata": {
      "source": "13",
      "chunk_id": 5,
      "token_count": 717,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\n13.2 \u2022 M ACHINE TRANSLATION USING ENCODER -DECODER 7\nLleg \u00b4o la bruja verde\nMT uses supervised machine learning: at training time the system is given a\nlarge set of parallel sentences (each sentence in a source language matched with\na sentence in the target language), and learns to map source sentences into target\nsentences. In practice, rather than using words (as in the example above), we split\nthe sentences into a sequence of subword tokens (tokens can be words, or subwords,\nor individual characters). The systems are then trained to maximize the probability\nof the sequence of tokens in the target language y1;:::;ymgiven the sequence of\ntokens in the source language x1;:::;xn:\nP(y1;:::; ymjx1;:::; xn) (13.7)\nRather than use the input tokens directly, the encoder-decoder architecture con-\nsists of two components, an encoder and a decoder . The encoder takes the input\nwords x= [x1;:::; xn]and produces an intermediate context h. At decoding time, the\nsystem takes hand, word by word, generates the output y:\nh=encoder (x) (13.8)\nyt+1=decoder (h;y1;:::; yt)8t2[1;:::; m] (13.9)\nIn the next two sections we\u2019ll talk about subword tokenization, and then how to get\nparallel corpora for training, and then we\u2019ll introduce the details of the encoder-\ndecoder architecture.\n13.2.1 Tokenization\nMachine translation systems use a vocabulary that is \ufb01xed in advance, and rather\nthan using space-separated words, this vocabulary is generated with subword to-\nkenization algorithms, like the BPE algorithm sketched in Chapter 2. A shared\nvocabulary is used for the source and target languages, which makes it easy to copy\ntokens (like names) from source to target. Using subword tokenization with tokens\nshared between languages makes it natural to translate between languages like En-\nglish or Hindi that use spaces to separate words, and languages like Chinese or Thai\nthat don\u2019t.\nWe build the vocabulary by running a subword tokenization algorithm on a cor-\npus that contains both source and target language data.\nRather than the simple BPE algorithm from Fig. ??, modern systems often use\nmore powerful tokenization algorithms. Some systems (like BERT) use a variant of\nBPE called the wordpiece algorithm, which instead of choosing the most frequent wordpiece\nset of tokens to merge, chooses merges based on which one most increases the lan-\nguage model probability of the tokenization. Wordpieces use a special symbol at the\nbeginning of each token; here\u2019s a resulting tokenization from the Google MT system\n(Wu et al., 2016):\nwords : Jet makers feud over seat width with big orders at stake\nwordpieces :J et makers fe ud over seat width with big orders atstake\nThe wordpiece algorithm is given a training corpus and a desired vocabulary size\nV , and proceeds as follows:\n1. Initialize the wordpiece lexicon with characters (for example a subset of Uni-\ncode characters, collapsing all the remaining characters to a special unknown\ncharacter token).",
    "metadata": {
      "source": "13",
      "chunk_id": 6,
      "token_count": 704,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8",
    "metadata": {
      "source": "13",
      "chunk_id": 7,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "8CHAPTER 13 \u2022 M ACHINE TRANSLATION\n2. Repeat until there are V wordpieces:\n(a) Train an n-gram language model on the training corpus, using the current\nset of wordpieces.\n(b) Consider the set of possible new wordpieces made by concatenating two\nwordpieces from the current lexicon. Choose the one new wordpiece that\nmost increases the language model probability of the training corpus.\nRecall that with BPE we had to specify the number of merges to perform; in\nwordpiece, by contrast, we specify the total vocabulary, which is a more intuitive\nparameter. A vocabulary of 8K to 32K word pieces is commonly used.\nAn even more commonly used tokenization algorithm is (somewhat ambigu-\nously) called the unigram algorithm (Kudo, 2018) or sometimes the SentencePiece unigram\nSentencePiece algorithm, and is used in systems like ALBERT (Lan et al., 2020) and T5 (Raf-\nfel et al., 2020). (Because unigram is the default tokenization algorithm used in\na library called SentencePiece that adds a useful wrapper around tokenization algo-\nrithms (Kudo and Richardson, 2018), authors often say they are using SentencePiece\ntokenization but really mean they are using the unigram algorithm).\nIn unigram tokenization, instead of building up a vocabulary by merging tokens,\nwe start with a huge vocabulary of every individual unicode character plus all fre-\nquent sequences of characters (including all space-separated words, for languages\nwith spaces), and iteratively remove some tokens to get to a desired \ufb01nal vocabulary\nsize. The algorithm is complex (involving suf\ufb01x-trees for ef\ufb01ciently storing many\ntokens, and the EM algorithm for iteratively assigning probabilities to tokens), so we\ndon\u2019t give it here, but see Kudo (2018) and Kudo and Richardson (2018). Roughly\nspeaking the algorithm proceeds iteratively by estimating the probability of each\ntoken, tokenizing the input data using various tokenizations, then removing a per-\ncentage of tokens that don\u2019t occur in high-probability tokenization, and then iterates\nuntil the vocabulary has been reduced down to the desired number of tokens.\nWhy does unigram tokenization work better than BPE? BPE tends to create lots\nof very small non-meaningful tokens (because BPE can only create larger words or\nmorphemes by merging characters one at a time), and it also tends to merge very\ncommon tokens, like the suf\ufb01x ed, onto their neighbors. We can see from these\nexamples from Bostrom and Durrett (2020) that unigram tends to produce tokens\nthat are more semantically meaningful:\nOriginal: corrupted Original: Completely preposterous suggestions\nBPE: cor rupted BPE: Comple t ely prep ost erous suggest ions\nUnigram: corrupt ed Unigram: Complete ly pre post er ous suggestion s\n13.2.2 Creating the Training data\nMachine translation models are trained on a parallel corpus , sometimes called a parallel corpus\nbitext , a text that appears in two (or more) languages. Large numbers of paral-\nlel corpora are available. Some are governmental; the Europarl corpus (Koehn, Europarl\n2005), extracted from the proceedings of the European Parliament, contains between\n400,000 and 2 million sentences each from 21 European languages. The United Na-\ntions Parallel Corpus contains on the order of 10 million sentences in the six of\ufb01cial\nlanguages of the United Nations (Arabic, Chinese, English, French, Russian, Span-",
    "metadata": {
      "source": "13",
      "chunk_id": 8,
      "token_count": 777,
      "chapter_title": ""
    }
  },
  {
    "content": "of very small non-meaningful tokens (because BPE can only create larger words or\nmorphemes by merging characters one at a time), and it also tends to merge very\ncommon tokens, like the suf\ufb01x ed, onto their neighbors. We can see from these\nexamples from Bostrom and Durrett (2020) that unigram tends to produce tokens\nthat are more semantically meaningful:\nOriginal: corrupted Original: Completely preposterous suggestions\nBPE: cor rupted BPE: Comple t ely prep ost erous suggest ions\nUnigram: corrupt ed Unigram: Complete ly pre post er ous suggestion s\n13.2.2 Creating the Training data\nMachine translation models are trained on a parallel corpus , sometimes called a parallel corpus\nbitext , a text that appears in two (or more) languages. Large numbers of paral-\nlel corpora are available. Some are governmental; the Europarl corpus (Koehn, Europarl\n2005), extracted from the proceedings of the European Parliament, contains between\n400,000 and 2 million sentences each from 21 European languages. The United Na-\ntions Parallel Corpus contains on the order of 10 million sentences in the six of\ufb01cial\nlanguages of the United Nations (Arabic, Chinese, English, French, Russian, Span-\nish) Ziemski et al. (2016). Other parallel corpora have been made from movie and\nTV subtitles, like the OpenSubtitles corpus (Lison and Tiedemann, 2016), or from\ngeneral web text, like the ParaCrawl corpus of 223 million sentence pairs between\n23 EU languages and English extracted from the CommonCrawl Ba \u02dcn\u00b4on et al. (2020).",
    "metadata": {
      "source": "13",
      "chunk_id": 9,
      "token_count": 370,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9",
    "metadata": {
      "source": "13",
      "chunk_id": 10,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "13.2 \u2022 M ACHINE TRANSLATION USING ENCODER -DECODER 9\nSentence alignment\nStandard training corpora for MT come as aligned pairs of sentences. When creat-\ning new corpora, for example for underresourced languages or new domains, these\nsentence alignments must be created. Fig. 13.4 gives a sample hypothetical sentence\nalignment.\nF1: -Bonjour, dit le petit prince.F2: -Bonjour, dit le marchand de pilules perfectionn\u00e9es qui apaisent la soif.F3: On en avale une par semaine et l'on n'\u00e9prouve plus le besoin de boire.F4: -C\u2019est une grosse \u00e9conomie de temps, dit le marchand.F5: Les experts ont fait des calculs.F6: On \u00e9pargne cinquante-trois minutes par semaine.F7: \u201cMoi, se dit le petit prince, si j'avais cinquante-trois minutes \u00e0 d\u00e9penser, je marcherais tout doucement vers une fontaine...\"E1: \u201cGood morning,\" said the little prince.E2: \u201cGood morning,\" said the merchant.E3: This was a merchant who sold pills that had been perfected to quench thirst.E4: You just swallow one pill a week and you won\u2019t feel the need for anything to drink.E5: \u201cThey save a huge amount of time,\" said the merchant.E6: \u201cFifty\u2212three minutes a week.\"E7: \u201cIf I had  fifty\u2212three minutes to spend?\" said the little prince to himself. E8: \u201cI would take a stroll to a spring of fresh water\u201d\nFigure 13.4 A sample alignment between sentences in English and French, with sentences extracted from\nAntoine de Saint-Exupery\u2019s Le Petit Prince and a hypothetical translation. Sentence alignment takes sentences\ne1;:::;en, and f1;:::;fmand \ufb01nds minimal sets of sentences that are translations of each other, including single\nsentence mappings like (e 1,f1), (e 4,f3), (e 5,f4), (e 6,f6) as well as 2-1 alignments (e 2/e3,f2), (e 7/e8,f7), and null\nalignments (f 5).\nGiven two documents that are translations of each other, we generally need two\nsteps to produce sentence alignments:\n\u2022 a cost function that takes a span of source sentences and a span of target sen-\ntences and returns a score measuring how likely these spans are to be transla-\ntions.\n\u2022 an alignment algorithm that takes these scores to \ufb01nd a good alignment be-\ntween the documents.\nTo score the similarity of sentences across languages, we need to make use of\namultilingual embedding space , in which sentences from different languages are\nin the same embedding space (Artetxe and Schwenk, 2019). Given such a space,\ncosine similarity of such embeddings provides a natural scoring function (Schwenk,\n2018). Thompson and Koehn (2019) give the following cost function between two\nsentences or spans x,yfrom the source and target documents respectively:\nc(x;y) =(1\u0000cos(x;y))nSents (x)nSents (y)PS\ns=11\u0000cos(x;ys)+PS\ns=11\u0000cos(xs;y)(13.10)\nwhere nSents ()gives the number of sentences (this biases the metric toward many\nalignments of single sentences instead of aligning very large spans). The denom-\ninator helps to normalize the similarities, and so x1;:::;xS;y1;:::;yS;are randomly",
    "metadata": {
      "source": "13",
      "chunk_id": 11,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "steps to produce sentence alignments:\n\u2022 a cost function that takes a span of source sentences and a span of target sen-\ntences and returns a score measuring how likely these spans are to be transla-\ntions.\n\u2022 an alignment algorithm that takes these scores to \ufb01nd a good alignment be-\ntween the documents.\nTo score the similarity of sentences across languages, we need to make use of\namultilingual embedding space , in which sentences from different languages are\nin the same embedding space (Artetxe and Schwenk, 2019). Given such a space,\ncosine similarity of such embeddings provides a natural scoring function (Schwenk,\n2018). Thompson and Koehn (2019) give the following cost function between two\nsentences or spans x,yfrom the source and target documents respectively:\nc(x;y) =(1\u0000cos(x;y))nSents (x)nSents (y)PS\ns=11\u0000cos(x;ys)+PS\ns=11\u0000cos(xs;y)(13.10)\nwhere nSents ()gives the number of sentences (this biases the metric toward many\nalignments of single sentences instead of aligning very large spans). The denom-\ninator helps to normalize the similarities, and so x1;:::;xS;y1;:::;yS;are randomly\nselected sentences sampled from the respective documents.\nUsually dynamic programming is used as the alignment algorithm (Gale and\nChurch, 1993), in a simple extension of the minimum edit distance algorithm we\nintroduced in Chapter 2.\nFinally, it\u2019s helpful to do some corpus cleanup by removing noisy sentence pairs.\nThis can involve handwritten rules to remove low-precision pairs (for example re-\nmoving sentences that are too long, too short, have different URLs, or even pairs",
    "metadata": {
      "source": "13",
      "chunk_id": 12,
      "token_count": 376,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 CHAPTER 13 \u2022 M ACHINE TRANSLATION\nthat are too similar, suggesting that they were copies rather than translations). Or\npairs can be ranked by their multilingual embedding cosine score and low-scoring\npairs discarded.\n13.3 Details of the Encoder-Decoder Model\nEncoderThegreenlleg\u00f3witcharrived<s>lleg\u00f3la\nlabruja\nbrujaverde\nverde</s>Decodercross-attentiontransformerblocks\nFigure 13.5 The encoder-decoder transformer architecture for machine translation. The encoder uses the\ntransformer blocks we saw in Chapter 8, while the decoder uses a more powerful block with an extra cross-\nattention layer that can attend to all the encoder words. We\u2019ll see this in more detail in the next section.\nThe standard architecture for MT is the encoder-decoder transformer. The encoder-\ndecoder architecture was introduced already for RNNs in Chapter 8, and the trans-\nformer version has the same idea. Fig. 13.5 shows the intuition of the architec-\nture at a high level. You\u2019ll see that the encoder-decoder architecture is made up of\ntwo transformers: an encoder , which is the same as the basic transformers from\nChapter 9, and a decoder , which is augmented with a special new layer called the\ncross-attention layer. The encoder takes the source language input word tokens\nX=x1;:::;xnand maps them to an output representation Henc=h1;:::;hn; via a\nstack of encoder blocks.\nThe decoder is essentially a conditional language model that attends to the en-\ncoder representation and generates the target words one by one, at each timestep\nconditioning on the source sentence and the previously generated target language\nwords to generate a token. Decoding can use any of the decoding methods discussed\nin Chapter 9 like greedy, or temperature or nucleus sampling. But the most com-\nmon decoding algorithm for MT is the beam search algorithm that we\u2019ll introduce\nin Section 13.4.\nBut the components of the architecture differ somewhat from the transformer\nblock we\u2019ve seen. First, in order to attend to the source language, the transformer\nblocks in the decoder have an extra cross-attention layer. Recall that the transformer\nblock of Chapter 9 consists of a self-attention layer that attends to the input from\nthe previous layer, followed by layer norm, a feed forward layer, and another layer\nnorm. The decoder transformer block includes an extra layer with a special kind\nof attention, cross-attention (also sometimes called encoder-decoder attention or cross-attention\nsource attention ). Cross-attention has the same form as the multi-head attention\nin a normal transformer block, except that while the queries as usual come from\nthe previous layer of the decoder, the keys and values come from the output of the\nencoder .",
    "metadata": {
      "source": "13",
      "chunk_id": 13,
      "token_count": 598,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\n13.3 \u2022 D ETAILS OF THE ENCODER -DECODER MODEL 11\nEncoderx1x2x3xn\u2026Decoderh3h2h1\u2026hn\nEncoderBlock 1Block 2Block Ky3y2y1\u2026\nDecoderBlock 1Block 2Block LUnembedding Matrixym\nMulti-Head AttentionLayer NormalizeLayer Normalize++\u2026Feedforward\nCausal Multi-Head AttentionLayer NormalizeLayer Normalize+\n+\u2026FeedforwardLayer Normalize+Cross-Attention\u2026\u2026\u2026\u2026\u2026\u2026LanguageModeling HeadHenc\nFigure 13.6 The transformer block for the encoder and the decoder. The \ufb01nal output of the encoder Henc=\nh1;:::;hnis the context used in the decoder. The decoder is a standard transformer except with one extra layer,\nthecross-attention layer, which takes that encoder output Hencand uses it to form its KandVinputs.\nThat is, where in standard multi-head attention the input to each attention layer is\nX, in cross attention the input is the the \ufb01nal output of the encoder Henc=h1;:::;hn.\nHencis of shape [n\u0002d], each row representing one input token. To link the keys\nand values from the encoder with the query from the prior layer of the decoder, we\nmultiply the encoder output Hencby the cross-attention layer\u2019s key weights WKand\nvalue weights WV. The query comes from the output from the prior decoder layer\nHdec[`\u00001], which is multiplied by the cross-attention layer\u2019s query weights WQ:\nQ=Hdec[`\u00001]WQ;K=HencWK;V=HencWV(13.11)\nCrossAttention (Q;K;V) = softmax\u0012QK|\npdk\u0013\nV (13.12)\nThe cross attention thus allows the decoder to attend to each of the source language\nwords as projected into the entire encoder \ufb01nal output representations. The other\nattention layer in each decoder block, the multi-head attention layer, is the same\ncausal (left-to-right) attention that we saw in Chapter 9. The multi-head attention in\nthe encoder, however, is allowed to look ahead at the entire source language text, so\nit is not masked.\nTo train an encoder-decoder model, we use the same self-supervision model we\nused for training encoder-decoders RNNs in Chapter 8. The network is given the\nsource text and then starting with the separator token is trained autoregressively to\npredict the next token using cross-entropy loss. Recall that cross-entropy loss for",
    "metadata": {
      "source": "13",
      "chunk_id": 14,
      "token_count": 555,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 CHAPTER 13 \u2022 M ACHINE TRANSLATION\nlanguage modeling is determined by the probability the model assigns to the correct\nnext word. So at time tthe CE loss is the negative log probability the model assigns\nto the next word in the training sequence:\nLCE(\u02c6yt;yt) =\u0000log\u02c6yt[wt+1] (13.13)\nAs in that case, we use teacher forcing in the decoder. Recall that in teacher forc- teacher forcing\ning, at each time step in decoding we force the system to use the gold target token\nfrom training as the next input xt+1, rather than allowing it to rely on the (possibly\nerroneous) decoder output \u02c6 yt.\n13.4 Decoding in MT: Beam Search\nRecall the greedy decoding algorithm from Chapter 9: at each time step tin gen-\neration, the output ytis chosen by computing the probability for each word in the\nvocabulary and then choosing the highest probability word (the argmax):\n\u02c6wt=argmaxw2VP(wjw<t) (13.14)\nA problem with greedy decoding is that what looks high probability at word tmight\nturn out to have been the wrong choice once we get to word t+1. The beam search\nalgorithm maintains multiple choices until later when we can see which one is best.\nIn beam search we model decoding as searching the space of possible genera-\ntions, represented as a search tree whose branches represent actions (generating a search tree\ntoken), and nodes represent states (having generated a particular pre\ufb01x). We search\nfor the best action sequence, i.e., the string with the highest probability.\nAn illustration of the problem\nFig. 13.7 shows a made-up example. The most probable sequence is ok ok EOS (its\nprobability is .4\u0002.7\u00021.0). But greedy search doesn\u2019t \ufb01nd it, incorrectly choosing\nyesas the \ufb01rst word since it has the highest local probability (0.5).\nstartokyesEOSokyesEOSokyesEOSEOSEOSEOSEOSt2t3p(t1|start)\nt1p(t2| t1)p(t3| t1,t2)\n.1.5.4.3.4.3.1.2.71.01.01.01.0\nFigure 13.7 A search tree for generating the target string T=t1;t2;:::from vocabulary\nV=fyes;ok;<s>g, showing the probability of generating each token from that state. Greedy\nsearch chooses yesfollowed by yes, instead of the globally most probable sequence ok ok .\nFor some problems, like part-of-speech tagging or parsing as we will see in\nChapter 17 or Chapter 18, we can use dynamic programming search (the Viterbi",
    "metadata": {
      "source": "13",
      "chunk_id": 15,
      "token_count": 601,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\n13.4 \u2022 D ECODING IN MT: B EAM SEARCH 13\nalgorithm) to address this problem. Unfortunately, dynamic programming is not ap-\nplicable to generation problems with long-distance dependencies between the output\ndecisions. The only method guaranteed to \ufb01nd the best solution is exhaustive search:\ncomputing the probability of every one of the VTpossible sentences (for some length\nvalue T) which is obviously too slow.\nThe solution: beam search\nInstead, MT systems generally decode using beam search , a heuristic search method beam search\n\ufb01rst proposed by Lowerre (1976). In beam search, instead of choosing the best token\nto generate at each timestep, we keep kpossible tokens at each step. This \ufb01xed-size\nmemory footprint kis called the beam width , on the metaphor of a \ufb02ashlight beam beam width\nthat can be parameterized to be wider or narrower.\nThus at the \ufb01rst step of decoding, we compute a softmax over the entire vocab-\nulary, assigning a probability to each word. We then select the k-best options from\nthis softmax output. These initial koutputs are the search frontier and these kinitial\nwords are called hypotheses . A hypothesis is an output sequence, a translation-so-\nfar, together with its probability.\na\u2026aardvark..arrived..the\u2026zebrastart\nt1a\u2026aardvark..the..witch\u2026zebraa\u2026aardvark..green..witch\u2026zebrat2hd1y1BOSy1y2\ny2hd1hd2thetheBOShd2greengreeny3hd1hd2arrivedarrivedBOSy2\nt3hd1hd2thetheBOSy2\nhd1hd2thetheBOShd2witchwitchy3a\u2026mage..the..witch\u2026zebraarrived\u2026aardvark..green..who\u2026zebray3\ny3\nFigure 13.8 Beam search decoding with a beam width of k=2. At each time step, we choose the kbest\nhypotheses, form the Vpossible extensions of each, score those k\u0002Vhypotheses and choose the best k=2\nto continue. At time 1, the frontier has the best 2 options from the initial decoder state: arrived andthe. We\nextend each, compute the probability of all the hypotheses so far ( arrived the ,arrived aardvark ,the green ,the\nwitch ) and again chose the best 2 ( the green andthe witch ) to be the search frontier. The images on the arcs\nschematically represent the decoders that must be run at each step to score the next words (for simplicity not\ndepicting cross-attention).\nAt subsequent steps, each of the kbest hypotheses is extended incrementally",
    "metadata": {
      "source": "13",
      "chunk_id": 16,
      "token_count": 602,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14",
    "metadata": {
      "source": "13",
      "chunk_id": 17,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "14 CHAPTER 13 \u2022 M ACHINE TRANSLATION\nby being passed to distinct decoders, which each generate a softmax over the entire\nvocabulary to extend the hypothesis to every possible next token. Each of these k\u0002V\nhypotheses is scored by P(yijx;y<i): the product of the probability of the current\nword choice multiplied by the probability of the path that led to it. We then prune\nthek\u0002Vhypotheses down to the kbest hypotheses, so there are never more than k\nhypotheses at the frontier of the search, and never more than kdecoders. Fig. 13.8\nillustrates this with a beam width of 2 for the beginning of The green witch arrived .\nThis process continues until an EOS is generated indicating that a complete can-\ndidate output has been found. At this point, the completed hypothesis is removed\nfrom the frontier and the size of the beam is reduced by one. The search continues\nuntil the beam has been reduced to 0. The result will be khypotheses.\nTo score each node by its log probability, we use the chain rule of probability to\nbreak down p(yjx)into the product of the probability of each word given its prior\ncontext, which we can turn into a sum of logs (for an output string of length t):\nscore(y) = logP(yjx)\n=log(P(y1jx)P(y2jy1;x)P(y3jy1;y2;x):::P(ytjy1;:::;yt\u00001;x))\n=tX\ni=1logP(yijy1;:::;yi\u00001;x) (13.15)\nThus at each step, to compute the probability of a partial sentence, we simply add the\nlog probability of the pre\ufb01x sentence so far to the log probability of generating the\nnext token. Fig. 13.9 shows the scoring for the example sentence shown in Fig. 13.8,\nusing some simple made-up probabilities. Log probabilities are negative or 0, and\nthe max of two log probabilities is the one that is greater (closer to 0).\nBOSarrivedthethewitchgreenwitchmage\nwhoy2y3log P(y1|x)y1log P(y2|y1,x)log P(y3|y2,y1,x)-.92-1.6-1.2-.69-2.3-.69-1.6\n-2.3arrived-.11-.51witch-.36-.22EOS\n-.51EOS-2.3at-1.61bylog P(y4|y3,y2,y1,x)log P(y5|y4,y3,y2,y1,x)arrivedcame-1.6\ny4y5log P(arrived|x) log P(arrived witch|x)log P(the|x)log P(the green|x)log P(the witch|x) =-1.6log P (arrived the|x) log P (\u201cthe green witch arrived\u201d|x) = log P (the|x) + log P(green|the,x) + log P(witch | the, green,x)+logP(arrived|the,green,witch,x)+log P(EOS|the,green,witch,arrived,x)= -2.3= -3.9= -1.6= -2.1=-.92-2.1-3.2\n-4.4-2.2-2.5-3.7-2.7-3.8-2.7-4.8\nFigure 13.9 Scoring for beam search decoding with a beam width of k=2. We maintain the log probability",
    "metadata": {
      "source": "13",
      "chunk_id": 18,
      "token_count": 789,
      "chapter_title": ""
    }
  },
  {
    "content": "-2.3arrived-.11-.51witch-.36-.22EOS\n-.51EOS-2.3at-1.61bylog P(y4|y3,y2,y1,x)log P(y5|y4,y3,y2,y1,x)arrivedcame-1.6\ny4y5log P(arrived|x) log P(arrived witch|x)log P(the|x)log P(the green|x)log P(the witch|x) =-1.6log P (arrived the|x) log P (\u201cthe green witch arrived\u201d|x) = log P (the|x) + log P(green|the,x) + log P(witch | the, green,x)+logP(arrived|the,green,witch,x)+log P(EOS|the,green,witch,arrived,x)= -2.3= -3.9= -1.6= -2.1=-.92-2.1-3.2\n-4.4-2.2-2.5-3.7-2.7-3.8-2.7-4.8\nFigure 13.9 Scoring for beam search decoding with a beam width of k=2. We maintain the log probability\nof each hypothesis in the beam by incrementally adding the logprob of generating each next token. Only the top\nkpaths are extended to the next step.\nFig. 13.10 gives the algorithm. One problem with this version of the algorithm is\nthat the completed hypotheses may have different lengths. Because language mod-",
    "metadata": {
      "source": "13",
      "chunk_id": 19,
      "token_count": 332,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\n13.4 \u2022 D ECODING IN MT: B EAM SEARCH 15\nfunction BEAM DECODE (c,beam width )returns best paths\ny0,h0 0\npath ()\ncomplete paths ()\nstate (c,y0,h0, path) ;initial state\nfrontier hstatei ;initial frontier\nwhile frontier contains incomplete paths andbeamwidth >0\nextended frontier hi\nfor each state2frontier do\ny DECODE (state )\nfor each word i2Vocabulary do\nsuccessor NEWSTATE (state ,i,yi)\nextended frontier ADDTOBEAM (successor ,extended frontier ,\nbeam width )\nfor each state inextended frontier do\nifstate is complete do\ncomplete paths APPEND (complete paths ,state )\nextended frontier REMOVE (extended frontier ,state )\nbeam width beam width - 1\nfrontier extended frontier\nreturn completed paths\nfunction NEWSTATE (state ,word ,word prob)returns new state\nfunction ADDTOBEAM (state ,frontier ,width )returns updated frontier\nifLENGTH (frontier )<width then\nfrontier INSERT (state ,frontier )\nelse if SCORE (state )>SCORE (WORST OF(frontier ))\nfrontier REMOVE (WORST OF(frontier ))\nfrontier INSERT (state ,frontier )\nreturn frontier\nFigure 13.10 Beam search decoding.\nels generally assign lower probabilities to longer strings, a naive algorithm would\nchoose shorter strings for y. (This is not an issue during the earlier steps of decod-\ning; since beam search is breadth-\ufb01rst, all the hypotheses being compared had the\nsame length.) For this reason we often apply length normalization methods, like\ndividing the logprob by the number of words:\nscore(y) =1\ntlogP(yjx) =1\nttX\ni=1logP(yijy1;:::;yi\u00001;x) (13.16)\nFor MT we generally use beam widths kbetween 5 and 10, giving us khypotheses at\nthe end. We can pass all kto the downstream application with their respective scores,\nor if we just need a single translation we can pass the most probable hypothesis.\n13.4.1 Minimum Bayes Risk Decoding\nMinimum Bayes risk orMBR decoding is an alternative decoding algorithm thatminimum\nBayes risk\nMBR",
    "metadata": {
      "source": "13",
      "chunk_id": 20,
      "token_count": 498,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\n16 CHAPTER 13 \u2022 M ACHINE TRANSLATION\ncan work even better than beam search and also tends to be better than the other\ndecoding algorithms like temperature sampling introduced in Section ??.\nThe intuition of minimum Bayes risk is that instead of trying to choose the trans-\nlation which is most probable, we choose the one that is likely to have the least error.\nFor example, we might want our decoding algorithm to \ufb01nd the translation which\nhas the highest score on some evaluation metric. For example in Section 13.6 we will\nintroduce metrics like chrF or BERTScore that measure the goodness-of-\ufb01t between\na candidate translation and a set of reference human translations. A translation that\nmaximizes this score, especially with a hypothetically huge set of perfect human\ntranslations is likely to be a good one (have minimum risk) even if it is not the most\nprobable translation by our particular probability estimator.\nIn practice, we don\u2019t know the perfect set of translations for a given sentence. So\nthe standard simpli\ufb01cation used in MBR decoding algorithms is to instead choose\nthe candidate translation which is most similar (by some measure of goodness-of-\ufb01t)\nwith some set of candidate translations. We\u2019re essentially approximating the enor-\nmous space of all possible translations Uwith a smaller set of possible candidate\ntranslations Y.\nGiven this set of possible candidate translations Y, and some similarity or align-\nment function util, we choose the best translation \u02c6 yas the translation which is most\nsimilar to all the other candidate translations:\n\u02c6y=argmax\ny2YX\nc2Yutil(y;c) (13.17)\nVarious util functions can be used, like chrF or BERTscore or BLEU. We can get the\nset of candidate translations by sampling using one of the basic sampling algorithms\nof Section ??like temperature sampling; good results can be obtained with as few\nas 32 or 64 candidates.\nMinimum Bayes risk decoding can also be used for other NLP tasks; indeed\nit was widely applied to speech recognition (Stolcke et al., 1997; Goel and Byrne,\n2000) before being applied to machine translation (Kumar and Byrne, 2004), and\nhas been shown to work well across many other generation tasks as well (e.g., sum-\nmarization, dialogue, and image captioning (Suzgun et al., 2023)).\n13.5 Translating in low-resource situations\nFor some languages, and especially for English, online resources are widely avail-\nable. There are many large parallel corpora that contain translations between En-\nglish and many languages. But the vast majority of the world\u2019s languages do not\nhave large parallel training texts available. An important ongoing research question\nis how to get good translation with lesser resourced languages. The resource prob-\nlem can even be true for high resource languages when we need to translate into low\nresource domains (for example in a particular genre that happens to have very little\nbitext).\nHere we brie\ufb02y introduce two commonly used approaches for dealing with this\ndata sparsity: backtranslation , which is a special case of the general statistical\ntechnique called data augmentation , and multilingual models , and also discuss\nsome socio-technical issues.",
    "metadata": {
      "source": "13",
      "chunk_id": 21,
      "token_count": 701,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\n13.5 \u2022 T RANSLATING IN LOW -RESOURCE SITUATIONS 17\n13.5.1 Data Augmentation\nData augmentation is a statistical technique for dealing with insuf\ufb01cient training\ndata, by adding new synthetic data that is generated from the current natural data.\nThe most common data augmentation technique for machine translation is called\nbacktranslation . Backtranslation relies on the intuition that while parallel corpora backtranslation\nmay be limited for particular languages or domains, we can often \ufb01nd a large (or\nat least larger) monolingual corpus, to add to the smaller parallel corpora that are\navailable. The algorithm makes use of monolingual corpora in the target language\nby creating synthetic bitexts.\nIn backtranslation, our goal is to improve source-to-target MT, given a small\nparallel text (a bitext) in the source/target languages, and some monolingual data in\nthe target language. We \ufb01rst use the bitext to train a MT system in the reverse di-\nrection: a target-to-source MT system . We then use it to translate the monolingual\ntarget data to the source language. Now we can add this synthetic bitext (natural\ntarget sentences, aligned with MT-produced source sentences) to our training data,\nand retrain our source-to-target MT model. For example suppose we want to trans-\nlate from Navajo to English but only have a small Navajo-English bitext, although of\ncourse we can \ufb01nd lots of monolingual English data. We use the small bitext to build\nan MT engine going the other way (from English to Navajo). Once we translate the\nmonolingual English text to Navajo, we can add this synthetic Navajo/English bitext\nto our training data.\nBacktranslation has various parameters. One is how we generate the backtrans-\nlated data; we can run the decoder in greedy inference, or use beam search. Or\nwe can do sampling, like the temperature sampling algorithm we saw in Chapter 9.\nAnother parameter is the ratio of backtranslated data to natural bitext data; we can\nchoose to upsample the bitext data (include multiple copies of each sentence). In\ngeneral backtranslation works surprisingly well; one estimate suggests that a system\ntrained on backtranslated text gets about 2/3 of the gain as would training on the\nsame amount of natural bitext (Edunov et al., 2018).\n13.5.2 Multilingual models\nThe models we\u2019ve described so far are for bilingual translation: one source language,\none target language. It\u2019s also possible to build a multilingual translator.\nIn a multilingual translator, we train the system by giving it parallel sentences\nin many different pairs of languages. That means we need to tell the system which\nlanguage to translate from and to! We tell the system which language is which\nby adding a special token lsto the encoder specifying the source language we\u2019re\ntranslating from, and a special token ltto the decoder telling it the target language\nwe\u2019d like to translate into.\nThus we slightly update Eq. 13.9 above to add these tokens in Eq. 13.19:\nh=encoder (x;ls) (13.18)\nyi+1=decoder (h;lt;y1;:::; yi)8i2[1;:::; m] (13.19)\nOne advantage of a multilingual model is that they can improve the translation\nof lower-resourced languages by drawing on information from a similar language\nin the training data that happens to have more resources. Perhaps we don\u2019t know\nthe meaning of a word in Galician, but the word appears in the similar and higher-\nresourced language Spanish.",
    "metadata": {
      "source": "13",
      "chunk_id": 22,
      "token_count": 787,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\n18 CHAPTER 13 \u2022 M ACHINE TRANSLATION\n13.5.3 Sociotechnical issues\nMany issues in dealing with low-resource languages go beyond the purely techni-\ncal. One problem is that for low-resource languages, especially from low-income\ncountries, native speakers are often not involved as the curators for content selec-\ntion, as the language technologists, or as the evaluators who measure performance\n(8et al., 2020). Indeed, one well-known study that manually audited a large set of\nparallel corpora and other major multilingual datasets found that for many of the\ncorpora, less than 50% of the sentences were of acceptable quality, with a lot of\ndata consisting of repeated sentences with web boilerplate or incorrect translations,\nsuggesting that native speakers may not have been suf\ufb01ciently involved in the data\nprocess (Kreutzer et al., 2022).\nOther issues, like the tendency of many MT approaches to focus on the case\nwhere one of the languages is English (Anastasopoulos and Neubig, 2020), have to\ndo with allocation of resources. Where most large multilingual systems were trained\non bitexts in which English was one of the two languages, recent huge corporate\nsystems like those of Fan et al. (2021) and Costa-juss `a et al. (2022) and datasets\nlike Schwenk et al. (2021) attempt to handle large numbers of languages (up to 200\nlanguages) and create bitexts between many more pairs of languages and not just\nthrough English.\nAt the smaller end, 8et al. (2020) propose a participatory design process to\nencourage content creators, curators, and language technologists who speak these\nlow-resourced languages to participate in developing MT algorithms. They provide\nonline groups, mentoring, and infrastructure, and report on a case study on devel-\noping MT algorithms for low-resource African languages. Among their conclusions\nwas to perform MT evaluation by post-editing rather than direct evaluation, since\nhaving labelers edit an MT system and then measure the distance between the MT\noutput and its post-edited version both was simpler to train evaluators and makes it\neasier to measure true errors in the MT output and not differences due to linguistic\nvariation (Bentivogli et al., 2018).\n13.6 MT Evaluation\nTranslations are evaluated along two dimensions:\n1.adequacy: how well the translation captures the exact meaning of the source adequacy\nsentence. Sometimes called faithfulness or\ufb01delity .\n2.\ufb02uency: how \ufb02uent the translation is in the target language (is it grammatical, \ufb02uency\nclear, readable, natural).\nUsing humans to evaluate is most accurate, but automatic metrics are also used for\nconvenience.\n13.6.1 Using Human Raters to Evaluate MT\nThe most accurate evaluations use human raters, such as online crowdworkers, to\nevaluate each translation along the two dimensions. For example, along the dimen-\nsion of \ufb02uency , we can ask how intelligible, how clear, how readable, or how natural\nthe MT output (the target text) is. We can give the raters a scale, for example, from\n1 (totally unintelligible) to 5 (totally intelligible), or 1 to 100, and ask them to rate\neach sentence or paragraph of the MT output.",
    "metadata": {
      "source": "13",
      "chunk_id": 23,
      "token_count": 743,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\n13.6 \u2022 MT E VALUATION 19\nWe can do the same thing to judge the second dimension, adequacy , using raters\nto assign scores on a scale. If we have bilingual raters, we can give them the source\nsentence and a proposed target sentence, and rate, on a 5-point or 100-point scale,\nhow much of the information in the source was preserved in the target. If we only\nhave monolingual raters but we have a good human translation of the source text, we\ncan give the monolingual raters the human reference translation and a target machine\ntranslation and again rate how much information is preserved. An alternative is to\ndoranking : give the raters a pair of candidate translations, and ask them which one ranking\nthey prefer.\nTraining of human raters (who are often online crowdworkers) is essential; raters\nwithout translation expertise \ufb01nd it dif\ufb01cult to separate \ufb02uency and adequacy, and\nso training includes examples carefully distinguishing these. Raters often disagree\n(source sentences may be ambiguous, raters will have different world knowledge,\nraters may apply scales differently). It is therefore common to remove outlier raters,\nand (if we use a \ufb01ne-grained enough scale) normalizing raters by subtracting the\nmean from their scores and dividing by the variance.\nAs discussed above, an alternative way of using human raters is to have them\npost-edit translations, taking the MT output and changing it minimally until they\nfeel it represents a correct translation. The difference between their post-edited\ntranslations and the original MT output can then be used as a measure of quality.\n13.6.2 Automatic Evaluation\nWhile humans produce the best evaluations of machine translation output, running a\nhuman evaluation can be time consuming and expensive. For this reason automatic\nmetrics are often used as temporary proxies. Automatic metrics are less accurate\nthan human evaluation, but can help test potential system improvements, and even\nbe used as an automatic loss function for training. In this section we introduce two\nfamilies of such metrics, those based on character- or word-overlap and those based\non embedding similarity.\nAutomatic Evaluation by Character Overlap: chrF\nThe simplest and most robust metric for MT evaluation is called chrF , which stands chrF\nforcharacter F-score (Popovi \u00b4c, 2015). chrF (along with many other earlier related\nmetrics like BLEU, METEOR, TER, and others) is based on a simple intuition de-\nrived from the pioneering work of Miller and Beebe-Center (1956): a good machine\ntranslation will tend to contain characters and words that occur in a human trans-\nlation of the same sentence. Consider a test set from a parallel corpus, in which\neach source sentence has both a gold human target translation and a candidate MT\ntranslation we\u2019d like to evaluate. The chrF metric ranks each MT target sentence by\na function of the number of character n-gram overlaps with the human translation.\nGiven the hypothesis and the reference, chrF is given a parameter kindicating\nthe length of character n-grams to be considered, and computes the average of the\nkprecisions (unigram precision, bigram, and so on) and the average of the krecalls\n(unigram recall, bigram recall, etc.):\nchrP percentage of character 1-grams, 2-grams, ..., k-grams in the hypothesis that\noccur in the reference, averaged.\nchrR percentage of character 1-grams, 2-grams,..., k-grams in the reference that\noccur in the hypothesis, averaged.\nThe metric then computes an F-score by combining chrP and chrR using a weighting",
    "metadata": {
      "source": "13",
      "chunk_id": 24,
      "token_count": 791,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\n20 CHAPTER 13 \u2022 M ACHINE TRANSLATION\nparameter b. It is common to set b=2, thus weighing recall twice as much as\nprecision:\nchrFb= (1+b2)chrP\u0001chrR\nb2\u0001chrP+chrR(13.20)\nForb=2, that would be:\nchrF2 =5\u0001chrP\u0001chrR\n4\u0001chrP+chrR\nFor example, consider two hypotheses that we\u2019d like to score against the refer-\nence translation witness for the past . Here are the hypotheses along with chrF values\ncomputed using parameters k=b=2 (in real examples, kwould be a higher number\nlike 6):\nREF: witness for the past,\nHYP1: witness of the past, chrF2,2 = .86\nHYP2: past witness chrF2,2 = .62\nLet\u2019s see how we computed that chrF value for HYP1 (we\u2019ll leave the compu-\ntation of the chrF value for HYP2 as an exercise for the reader). First, chrF ignores\nspaces, so we\u2019ll remove them from both the reference and hypothesis:\nREF: witnessforthepast, (18 unigrams, 17 bigrams)\nHYP1: witnessofthepast, (17 unigrams, 16 bigrams)\nNext let\u2019s see how many unigrams and bigrams match between the reference and\nhypothesis:\nunigrams that match: w i t n e s s f o t h e p a s t , (17 unigrams)\nbigrams that match: wi it tn ne es ss th he ep pa as st t, (13 bigrams)\nWe use that to compute the unigram and bigram precisions and recalls:\nunigram P: 17/17 = 1 unigram R: 17/18 = .944\nbigram P: 13/16 = .813 bigram R: 13/17 = .765\nFinally we average to get chrP and chrR, and compute the F-score:\nchrP = (17=17+13=16)=2=:906\nchrR = (17=18+13=17)=2=:855\nchrF2,2 =5chrP\u0003chrR\n4chrP +chrR=:86\nchrF is simple, robust, and correlates very well with human judgments in many\nlanguages (Kocmi et al., 2021).\nAlternative overlap metric: BLEU\nThere are various alternative overlap metrics. For example, before the development\nof chrF, it was common to use a word-based overlap metric called BLEU (for BiLin-\ngual Evaluation Understudy), that is purely precision-based rather than combining\nprecision and recall (Papineni et al., 2002). The BLEU score for a corpus of candi-\ndate translation sentences is a function of the n-gram word precision over all the\nsentences combined with a brevity penalty computed over the corpus as a whole.\nWhat do we mean by n-gram precision? Consider a corpus composed of a single\nsentence. The unigram precision for this corpus is the percentage of unigram tokens",
    "metadata": {
      "source": "13",
      "chunk_id": 25,
      "token_count": 676,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21",
    "metadata": {
      "source": "13",
      "chunk_id": 26,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "13.6 \u2022 MT E VALUATION 21\nin the candidate translation that also occur in the reference translation, and ditto for\nbigrams and so on, up to 4-grams. BLEU extends this unigram metric to the whole\ncorpus by computing the numerator as the sum over all sentences of the counts of all\nthe unigram types that also occur in the reference translation, and the denominator\nis the total of the counts of all unigrams in all candidate sentences. We compute\nthis n-gram precision for unigrams, bigrams, trigrams, and 4-grams and take the\ngeometric mean. BLEU has many further complications, including a brevity penalty\nfor penalizing candidate translations that are too short, and it also requires the n-\ngram counts be clipped in a particular way.\nBecause BLEU is a word-based metric, it is very sensitive to word tokenization,\nmaking it impossible to compare different systems if they rely on different tokeniza-\ntion standards, and doesn\u2019t work as well in languages with complex morphology.\nNonetheless, you will sometimes still see systems evaluated by BLEU, particularly\nfor translation into English. In such cases it\u2019s important to use packages that enforce\nstandardization for tokenization like S ACRE BLEU (Post, 2018).\nStatistical Signi\ufb01cance Testing for MT evals\nCharacter or word overlap-based metrics like chrF (or BLEU, or etc.) are mainly\nused to compare two systems, with the goal of answering questions like: did the\nnew algorithm we just invented improve our MT system? To know if the difference\nbetween the chrF scores of two MT systems is a signi\ufb01cant difference, we use the\npaired bootstrap test, or the similar randomization test.\nTo get a con\ufb01dence interval on a single chrF score using the bootstrap test, recall\nfrom Section ??that we take our test set (or devset) and create thousands of pseudo-\ntestsets by repeatedly sampling with replacement from the original test set. We now\ncompute the chrF score of each of the pseudo-testsets. If we drop the top 2.5% and\nbottom 2.5% of the scores, the remaining scores will give us the 95% con\ufb01dence\ninterval for the chrF score of our system.\nTo compare two MT systems A and B, we draw the same set of pseudo-testsets,\nand compute the chrF scores for each of them. We then compute the percentage of\npseudo-test-sets in which A has a higher chrF score than B.\nchrF: Limitations\nWhile automatic character and word-overlap metrics like chrF or BLEU are useful,\nthey have important limitations. chrF is very local: a large phrase that is moved\naround might barely change the chrF score at all, and chrF can\u2019t evaluate cross-\nsentence properties of a document like its discourse coherence (Chapter 24). chrF\nand similar automatic metrics also do poorly at comparing very different kinds of\nsystems, such as comparing human-aided translation against machine translation, or\ndifferent machine translation architectures against each other (Callison-Burch et al.,\n2006). Instead, automatic overlap metrics like chrF are most appropriate when eval-\nuating changes to a single system.\n13.6.3 Automatic Evaluation: Embedding-Based Methods\nThe chrF metric is based on measuring the exact character n-grams a human refer-\nence and candidate machine translation have in common. However, this criterion\nis overly strict, since a good translation may use alternate words or paraphrases. A\nsolution \ufb01rst pioneered in early metrics like METEOR (Banerjee and Lavie, 2005)\nwas to allow synonyms to match between the reference xand candidate \u02dc x. More",
    "metadata": {
      "source": "13",
      "chunk_id": 27,
      "token_count": 795,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22",
    "metadata": {
      "source": "13",
      "chunk_id": 28,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "22 CHAPTER 13 \u2022 M ACHINE TRANSLATION\nrecent metrics use BERT or other embeddings to implement this intuition.\nFor example, in some situations we might have datasets that have human as-\nsessments of translation quality. Such datasets consists of tuples (x;\u02dcx;r), where\nx= (x1;:::; xn)is a reference translation, \u02dc x= (\u02dcx1;:::; \u02dcxm)is a candidate machine\ntranslation, and r2Ris a human rating that expresses the quality of \u02dc xwith respect\ntox. Given such data, algorithms like COMET (Rei et al., 2020) BLEURT (Sellam\net al., 2020) train a predictor on the human-labeled datasets, for example by passing\nxand \u02dcxthrough a version of BERT (trained with extra pretraining, and then \ufb01netuned\non the human-labeled sentences), followed by a linear layer that is trained to predict\nr. The output of such models correlates highly with human labels.\nIn other cases, however, we don\u2019t have such human-labeled datasets. In that\ncase we can measure the similarity of xand \u02dcxby the similarity of their embeddings.\nThe BERTS CORE algorithm (Zhang et al., 2020) shown in Fig. 13.11, for example,\npasses the reference xand the candidate \u02dc xthrough BERT, computing a BERT em-\nbedding for each token xiand \u02dcxj. Each pair of tokens (xi;\u02dcxj)is scored by its cosine\nxi\u0001\u02dcxj\njxijj\u02dcxjj. Each token in xis matched to a token in \u02dc xto compute recall, and each token in\n\u02dcxis matched to a token in xto compute precision (with each token greedily matched\nto the most similar token in the corresponding sentence). BERTS CORE provides\nprecision and recall (and hence F 1):\nRBERT=1\njxjX\nxi2xmax\n\u02dcxj2\u02dcxxi\u0001\u02dcxjPBERT=1\nj\u02dcxjX\n\u02dcxj2\u02dcxmax\nxi2xxi\u0001\u02dcxj (13.21)\nPublished as a conference paper at ICLR 2020\nReferencethe weather is cold today\nCandidateit is freezing today\nCandidateContextualEmbeddingPairwise CosineSimilarityRBERT=(0.713\u00001.27)+(0.515\u00007.94)+...1.27+7.94+1.82+7.90+8.88",
    "metadata": {
      "source": "13",
      "chunk_id": 29,
      "token_count": 573,
      "chapter_title": ""
    }
  },
  {
    "content": "<latexit sha1_base64=\"OJyoKlmBAgUA0KDtUcsH/di5BlI=\">AAACSHicbZDLattAFIaPnLRJ3JvTLrsZYgoJAqFxGqwsCqal0FVJQ5wELCNG41EyZHRh5ijECL1EnqAv002X2eUZsumipXRR6Mj2Ipf+MPDznXM4Z/64UNKg7187raXlR49XVtfaT54+e/6is/7y0OSl5mLIc5Xr45gZoWQmhihRieNCC5bGShzFZx+a+tG50Ebm2QFOCzFO2UkmE8kZWhR1ov2oClFcYPX+4/5BXZN3JEw049Wm7/XpdogyFYZQr9ffci3aoTsL1Pd23265oZrkaOqqaXAb5FIv6DXOdwMvCOqo0/U9fyby0NCF6Q52/15+BYC9qHMVTnJepiJDrpgxI+oXOK6YRsmVqNthaUTB+Bk7ESNrM2aPGVezIGryxpIJSXJtX4ZkRm9PVCw1ZprGtjNleGru1xr4v9qoxCQYVzIrShQZny9KSkUwJ02qZCK14Kim1jCupb2V8FNmc0SbfduGQO9/+aE57HnU9+gX2h18hrlW4TVswCZQ6MMAPsEeDIHDN7iBn/DL+e78cH47f+atLWcx8wruqNX6B8dUrVw=</latexit><latexit",
    "metadata": {
      "source": "13",
      "chunk_id": 30,
      "token_count": 442,
      "chapter_title": ""
    }
  },
  {
    "content": "sha1_base64=\"RInTcZkWiVBnf/ncBstCvatCtG4=\">AAACSHicbZDPShxBEMZ7Nproxugaj14al4AyMEyvyoyHwGIQPImKq8LOMvT09mhjzx+6a0KWYV4iL5EnySXH3HwGLx4U8SDYs7sHo/mg4eNXVVT1F+VSaHDda6vxbmb2/Ye5+ebHhU+LS63lz6c6KxTjPZbJTJ1HVHMpUt4DAZKf54rTJJL8LLr6VtfPvnOlRZaewCjng4RepCIWjIJBYSs8DssA+A8od/eOT6oKf8VBrCgr113HI5sBiIRrTJyOt2EbtE22p8hzdrY27EAOM9BVWTfYNbKJ43dq59q+4/tV2Gq7jjsWfmvI1LS7O08/f3nLi4dh628wzFiR8BSYpFr3iZvDoKQKBJO8agaF5jllV/SC941NqTlmUI6DqPAXQ4Y4zpR5KeAxfTlR0kTrURKZzoTCpX5dq+H/av0CYn9QijQvgKdssiguJIYM16nioVCcgRwZQ5kS5lbMLqnJEUz2TRMCef3lt+a04xDXIUek3T1AE82hVbSG1hFBHuqifXSIeoih3+gG3aF76491az1Yj5PWhjWdWUH/qNF4BkPYrbk=</latexit><latexit",
    "metadata": {
      "source": "13",
      "chunk_id": 31,
      "token_count": 427,
      "chapter_title": ""
    }
  },
  {
    "content": "sha1_base64=\"RInTcZkWiVBnf/ncBstCvatCtG4=\">AAACSHicbZDPShxBEMZ7Nproxugaj14al4AyMEyvyoyHwGIQPImKq8LOMvT09mhjzx+6a0KWYV4iL5EnySXH3HwGLx4U8SDYs7sHo/mg4eNXVVT1F+VSaHDda6vxbmb2/Ye5+ebHhU+LS63lz6c6KxTjPZbJTJ1HVHMpUt4DAZKf54rTJJL8LLr6VtfPvnOlRZaewCjng4RepCIWjIJBYSs8DssA+A8od/eOT6oKf8VBrCgr113HI5sBiIRrTJyOt2EbtE22p8hzdrY27EAOM9BVWTfYNbKJ43dq59q+4/tV2Gq7jjsWfmvI1LS7O08/f3nLi4dh628wzFiR8BSYpFr3iZvDoKQKBJO8agaF5jllV/SC941NqTlmUI6DqPAXQ4Y4zpR5KeAxfTlR0kTrURKZzoTCpX5dq+H/av0CYn9QijQvgKdssiguJIYM16nioVCcgRwZQ5kS5lbMLqnJEUz2TRMCef3lt+a04xDXIUek3T1AE82hVbSG1hFBHuqifXSIeoih3+gG3aF76491az1Yj5PWhjWdWUH/qNF4BkPYrbk=</latexit><latexit",
    "metadata": {
      "source": "13",
      "chunk_id": 32,
      "token_count": 427,
      "chapter_title": ""
    }
  },
  {
    "content": "sha1_base64=\"fGWl4NCvlvtMu17rjLtk25oWpdc=\">AAACSHicbZBLS+RAFIUrPT7bVzsu3RQ2ghIIqVbpuBgQRZiVqNgqdJpQqa5oYeVB1Y1ME/Lz3Lic3fwGNy6UwZ2VNgtfBwoO372Xe+uEmRQaXPef1fgxMTk1PTPbnJtfWFxqLf8812muGO+xVKbqMqSaS5HwHgiQ/DJTnMah5BfhzUFVv7jlSos0OYNRxgcxvUpEJBgFg4JWcBoUPvA/UOwfnp6VJf6F/UhRVmy4Tpds+SBirjFxOt1N26AdslOjrrO7vWn7cpiCLouqwa6QTRyvUznX9hzPK4NW23XcsfBXQ2rTRrWOg9Zff5iyPOYJMEm17hM3g0FBFQgmedn0c80zym7oFe8bm1BzzKAYB1HidUOGOEqVeQngMX0/UdBY61Ecms6YwrX+XKvgd7V+DpE3KESS5cAT9rYoyiWGFFep4qFQnIEcGUOZEuZWzK6pyRFM9k0TAvn85a/mvOMQ1yEnpL13VMcxg1bRGtpABHXRHvqNjlEPMXSHHtATerburUfrv/Xy1tqw6pkV9EGNxisxMKq0</latexit>1.27",
    "metadata": {
      "source": "13",
      "chunk_id": 33,
      "token_count": 409,
      "chapter_title": ""
    }
  },
  {
    "content": "7.941.827.908.88idf",
    "metadata": {
      "source": "13",
      "chunk_id": 34,
      "token_count": 10,
      "chapter_title": ""
    }
  },
  {
    "content": "weightsImportance Weighting(Optional)Maximum Similarityx<latexit sha1_base64=\"f2yzimwbR/Dgjzp6tZ360fHRqNI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64=\"f2yzimwbR/Dgjzp6tZ360fHRqNI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit",
    "metadata": {
      "source": "13",
      "chunk_id": 35,
      "token_count": 736,
      "chapter_title": ""
    }
  },
  {
    "content": "sha1_base64=\"f2yzimwbR/Dgjzp6tZ360fHRqNI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64=\"f2yzimwbR/Dgjzp6tZ360fHRqNI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit>\u02c6x<latexit",
    "metadata": {
      "source": "13",
      "chunk_id": 36,
      "token_count": 726,
      "chapter_title": ""
    }
  },
  {
    "content": "sha1_base64=\"5QTnVRVSrnyzznVU7d5bF5u03Iw=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit><latexit sha1_base64=\"5QTnVRVSrnyzznVU7d5bF5u03Iw=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit><latexit",
    "metadata": {
      "source": "13",
      "chunk_id": 37,
      "token_count": 696,
      "chapter_title": ""
    }
  },
  {
    "content": "sha1_base64=\"5QTnVRVSrnyzznVU7d5bF5u03Iw=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit><latexit sha1_base64=\"5QTnVRVSrnyzznVU7d5bF5u03Iw=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit>",
    "metadata": {
      "source": "13",
      "chunk_id": 38,
      "token_count": 694,
      "chapter_title": ""
    }
  },
  {
    "content": "ReferenceFigure 1: Illustration of the computation of the recall metric RBERT. Given the reference xand\ncandidate \u02c6x, we compute BERT embeddings and pairwise cosine similarity. We highlight the greedy\nmatching in red, and include the optional idfimportance weighting.\nWe experiment with different models (Section 4), using the tokenizer provided with each model.\nGiven a tokenized reference sentence x=hx1,...,x ki, the embedding model generates a se-\nquence of vectors hx1,...,xki. Similarly, the tokenized candidate \u02c6x=h\u02c6x1,..., \u02c6xmiis mapped\ntoh\u02c6x1,...,\u02c6xli. The main model we use is BERT, which tokenizes the input text into a sequence\nof word pieces (Wu et al., 2016), where unknown words are split into several commonly observed\nsequences of characters. The representation for each word piece is computed with a Transformer\nencoder (Vaswani et al., 2017) by repeatedly applying self-attention and nonlinear transformations\nin an alternating fashion. BERT embeddings have been shown to bene\ufb01t various NLP tasks (Devlin\net al., 2019; Liu, 2019; Huang et al., 2019; Yang et al., 2019a).\nSimilarity Measure The vector representation allows for a soft measure of similarity instead of\nexact-string (Papineni et al., 2002) or heuristic (Banerjee & Lavie, 2005) matching. The cosine\nsimilarity of a reference token xiand a candidate token \u02c6xjisx>\ni\u02c6xj\nkxikk\u02c6xjk. We use pre-normalized\nvectors, which reduces this calculation to the inner product x>\ni\u02c6xj. While this measure considers\ntokens in isolation, the contextual embeddings contain information from the rest of the sentence.\nBERTS CORE The complete score matches each token in xto a token in \u02c6xto compute recall,\nand each token in \u02c6xto a token in xto compute precision. We use greedy matching to maximize\nthe matching similarity score,2where each token is matched to the most similar token in the other\nsentence. We combine precision and recall to compute an F1 measure. For a reference xand\ncandidate \u02c6x, the recall, precision, and F1 scores are:\nRBERT =1\n|x|X\nxi2xmax\n\u02c6xj2\u02c6xx>\ni\u02c6xj,P BERT =1\n|\u02c6x|X\n\u02c6xj2\u02c6xmax\nxi2xx>\ni\u02c6xj,F BERT =2PBERT \u00b7RBERT\nPBERT +RBERT.\nImportance Weighting Previous work on similarity measures demonstrated that rare words can\nbe more indicative for sentence similarity than common words (Banerjee & Lavie, 2005; Vedantam\net al., 2015). BERTS CORE enables us to easily incorporate importance weighting. We experiment\nwith inverse document frequency ( idf) scores computed from the test corpus. Given Mreference\nsentences {x(i)}M\ni=1, the idfscore of a word-piece token wis\nidf(w)=\u0000log1\nMMX\ni=1I[w2x(i)],\nwhere I[\u00b7]is an indicator function. We do not use the full tf-idfmeasure because we process single\nsentences, where the term frequency ( tf) is likely 1. For example, recall with idfweighting is\nRBERT =P\nxi2xidf(xi) max \u02c6xj2\u02c6xx>",
    "metadata": {
      "source": "13",
      "chunk_id": 39,
      "token_count": 783,
      "chapter_title": ""
    }
  },
  {
    "content": "candidate \u02c6x, the recall, precision, and F1 scores are:\nRBERT =1\n|x|X\nxi2xmax\n\u02c6xj2\u02c6xx>\ni\u02c6xj,P BERT =1\n|\u02c6x|X\n\u02c6xj2\u02c6xmax\nxi2xx>\ni\u02c6xj,F BERT =2PBERT \u00b7RBERT\nPBERT +RBERT.\nImportance Weighting Previous work on similarity measures demonstrated that rare words can\nbe more indicative for sentence similarity than common words (Banerjee & Lavie, 2005; Vedantam\net al., 2015). BERTS CORE enables us to easily incorporate importance weighting. We experiment\nwith inverse document frequency ( idf) scores computed from the test corpus. Given Mreference\nsentences {x(i)}M\ni=1, the idfscore of a word-piece token wis\nidf(w)=\u0000log1\nMMX\ni=1I[w2x(i)],\nwhere I[\u00b7]is an indicator function. We do not use the full tf-idfmeasure because we process single\nsentences, where the term frequency ( tf) is likely 1. For example, recall with idfweighting is\nRBERT =P\nxi2xidf(xi) max \u02c6xj2\u02c6xx>\ni\u02c6xjP\nxi2xidf(xi).\nBecause we use reference sentences to compute idf, the idfscores remain the same for all systems\nevaluated on a speci\ufb01c test set. We apply plus-one smoothing to handle unknown word pieces.\n2We compare greedy matching with optimal assignment in Appendix C.\n4\nFigure 13.11 The computation of BERTS CORE recall from reference xand candidate \u02c6 x, from Figure 1 in\nZhang et al. (2020). This version shows an extended version of the metric in which tokens are also weighted by\ntheir idf values.\n13.7 Bias and Ethical Issues\nMachine translation raises many of the same ethical issues that we\u2019ve discussed in\nearlier chapters. For example, consider MT systems translating from Hungarian\n(which has the gender neutral pronoun \u02ddo) or Spanish (which often drops pronouns)\ninto English (in which pronouns are obligatory, and they have grammatical gender).\nWhen translating a reference to a person described without speci\ufb01ed gender, MT\nsystems often default to male gender (Schiebinger 2014, Prates et al. 2019). And\nMT systems often assign gender according to culture stereotypes of the sort we saw\nin Section ??. Fig. 13.12 shows examples from Prates et al. (2019), in which Hun-\ngarian gender-neutral \u02ddo is a nurse is translated with she, but gender-neutral \u02ddo is a\nCEO is translated with he. Prates et al. (2019) \ufb01nd that these stereotypes can\u2019t com-\npletely be accounted for by gender bias in US labor statistics, because the biases are",
    "metadata": {
      "source": "13",
      "chunk_id": 40,
      "token_count": 647,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23\n\n13.8 \u2022 S UMMARY 23\nampli\ufb01ed by MT systems, with pronouns being mapped to male or female gender\nwith a probability higher than if the mapping was based on actual labor employment\nstatistics.\nHungarian (gender neutral) source English MT output\n\u02ddo egy \u00b4apol\u00b4o she is a nurse\n\u02ddo egy tud \u00b4os he is a scientist\n\u02ddo egy m \u00b4ern\u00a8ok he is an engineer\n\u02ddo egy p \u00b4ek he is a baker\n\u02ddo egy tan \u00b4ar she is a teacher\n\u02ddo egy esk \u00a8uv\u02ddoszervez \u02ddo she is a wedding organizer\n\u02ddo egy vez \u00b4erigazgat \u00b4o he is a CEO\nFigure 13.12 When translating from gender-neutral languages like Hungarian into English,\ncurrent MT systems interpret people from traditionally male-dominated occupations as male,\nand traditionally female-dominated occupations as female (Prates et al., 2019).\nSimilarly, a recent challenge set, the WinoMT dataset (Stanovsky et al., 2019)\nshows that MT systems perform worse when they are asked to translate sentences\nthat describe people with non-stereotypical gender roles, like \u201cThe doctor asked the\nnurse to help her in the operation\u201d.\nMany ethical questions in MT require further research. One open problem is\ndeveloping metrics for knowing what our systems don\u2019t know. This is because MT\nsystems can be used in urgent situations where human translators may be unavailable\nor delayed: in medical domains, to help translate when patients and doctors don\u2019t\nspeak the same language, or in legal domains, to help judges or lawyers communi-\ncate with witnesses or defendants. In order to \u2018do no harm\u2019, systems need ways to\nassign con\ufb01dence values to candidate translations, so they can abstain from giving con\ufb01dence\nincorrect translations that may cause harm.\n13.8 Summary\nMachine translation is one of the most widely used applications of NLP, and the\nencoder-decoder model, \ufb01rst developed for MT is a key tool that has applications\nthroughout NLP.\n\u2022 Languages have divergences , both structural and lexical, that make translation\ndif\ufb01cult.\n\u2022 The linguistic \ufb01eld of typology investigates some of these differences; lan-\nguages can be classi\ufb01ed by their position along typological dimensions like\nwhether verbs precede their objects.\n\u2022Encoder-decoder networks (for transformers just as we saw in Chapter 8 for\nRNNs) are composed of an encoder network that takes an input sequence\nand creates a contextualized representation of it, the context . This context\nrepresentation is then passed to a decoder which generates a task-speci\ufb01c\noutput sequence.\n\u2022Cross-attention allows the transformer decoder to view information from all\nthe hidden states of the encoder.\n\u2022 Machine translation models are trained on a parallel corpus , sometimes called\nabitext , a text that appears in two (or more) languages.",
    "metadata": {
      "source": "13",
      "chunk_id": 41,
      "token_count": 646,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24",
    "metadata": {
      "source": "13",
      "chunk_id": 42,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "24 CHAPTER 13 \u2022 M ACHINE TRANSLATION\n\u2022Backtranslation is a way of making use of monolingual corpora in the target\nlanguage by running a pilot MT engine backwards to create synthetic bitexts.\n\u2022 MT is evaluated by measuring a translation\u2019s adequacy (how well it captures\nthe meaning of the source sentence) and \ufb02uency (how \ufb02uent or natural it is\nin the target language). Human evaluation is the gold standard, but automatic\nevaluation metrics like chrF , which measure character n-gram overlap with\nhuman translations, or more recent metrics based on embedding similarity,\nare also commonly used.\nBibliographical and Historical Notes\nMT was proposed seriously by the late 1940s, soon after the birth of the computer\n(Weaver, 1949/1955). In 1954, the \ufb01rst public demonstration of an MT system pro-\ntotype (Dostert, 1955) led to great excitement in the press (Hutchins, 1997). The\nnext decade saw a great \ufb02owering of ideas, pre\ufb01guring most subsequent develop-\nments. But this work was ahead of its time\u2014implementations were limited by, for\nexample, the fact that pending the development of disks there was no good way to\nstore dictionary information.\nAs high-quality MT proved elusive (Bar-Hillel, 1960), there grew a consensus\non the need for better evaluation and more basic research in the new \ufb01elds of for-\nmal and computational linguistics. This consensus culminated in the famously crit-\nical ALPAC (Automatic Language Processing Advisory Committee) report of 1966\n(Pierce et al., 1966) that led in the mid 1960s to a dramatic cut in funding for MT\nin the US. As MT research lost academic respectability, the Association for Ma-\nchine Translation and Computational Linguistics dropped MT from its name. Some\nMT developers, however, persevered, and there were early MT systems like M \u00b4et\u00b4eo,\nwhich translated weather forecasts from English to French (Chandioux, 1976), and\nindustrial systems like Systran.\nIn the early years, the space of MT architectures spanned three general mod-\nels. In direct translation , the system proceeds word-by-word through the source-\nlanguage text, translating each word incrementally. Direct translation uses a large\nbilingual dictionary, each of whose entries is a small program with the job of trans-\nlating one word. In transfer approaches, we \ufb01rst parse the input text and then ap-\nply rules to transform the source-language parse into a target language parse. We\nthen generate the target language sentence from the parse tree. In interlingua ap-\nproaches, we analyze the source language text into some abstract meaning repre-\nsentation, called an interlingua . We then generate into the target language from\nthis interlingual representation. A common way to visualize these three early ap-\nproaches was the Vauquois triangle shown in Fig. 13.13. The triangle shows theVauquois\ntriangle\nincreasing depth of analysis required (on both the analysis and generation end) as\nwe move from the direct approach through transfer approaches to interlingual ap-\nproaches. In addition, it shows the decreasing amount of transfer knowledge needed\nas we move up the triangle, from huge amounts of transfer at the direct level (al-\nmost all knowledge is transfer knowledge for each word) through transfer (transfer\nrules only for parse trees or thematic roles) through interlingua (no speci\ufb01c transfer",
    "metadata": {
      "source": "13",
      "chunk_id": 43,
      "token_count": 761,
      "chapter_title": ""
    }
  },
  {
    "content": "els. In direct translation , the system proceeds word-by-word through the source-\nlanguage text, translating each word incrementally. Direct translation uses a large\nbilingual dictionary, each of whose entries is a small program with the job of trans-\nlating one word. In transfer approaches, we \ufb01rst parse the input text and then ap-\nply rules to transform the source-language parse into a target language parse. We\nthen generate the target language sentence from the parse tree. In interlingua ap-\nproaches, we analyze the source language text into some abstract meaning repre-\nsentation, called an interlingua . We then generate into the target language from\nthis interlingual representation. A common way to visualize these three early ap-\nproaches was the Vauquois triangle shown in Fig. 13.13. The triangle shows theVauquois\ntriangle\nincreasing depth of analysis required (on both the analysis and generation end) as\nwe move from the direct approach through transfer approaches to interlingual ap-\nproaches. In addition, it shows the decreasing amount of transfer knowledge needed\nas we move up the triangle, from huge amounts of transfer at the direct level (al-\nmost all knowledge is transfer knowledge for each word) through transfer (transfer\nrules only for parse trees or thematic roles) through interlingua (no speci\ufb01c transfer\nknowledge). We can view the encoder-decoder network as an interlingual approach,\nwith attention acting as an integration of direct and transfer, allowing words or their\nrepresentations to be directly accessed by the decoder.",
    "metadata": {
      "source": "13",
      "chunk_id": 44,
      "token_count": 329,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 25",
    "metadata": {
      "source": "13",
      "chunk_id": 45,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 25\nsourcetexttarget textDirect TranslationTransferInterlinguaSource Text:Semantic/SyntacticStructureTarget Text:Semantic/SyntacticStructuresource languageanalysissource languageanalysistarget language generation\nFigure 13.13 The Vauquois (1968) triangle.\nStatistical methods began to be applied around 1990, enabled \ufb01rst by the devel-\nopment of large bilingual corpora like the Hansard corpus of the proceedings of the\nCanadian Parliament, which are kept in both French and English, and then by the\ngrowth of the Web. Early on, a number of researchers showed that it was possible\nto extract pairs of aligned sentences from bilingual corpora, using words or simple\ncues like sentence length (Kay and R \u00a8oscheisen 1988, Gale and Church 1991, Gale\nand Church 1993, Kay and R \u00a8oscheisen 1993).\nAt the same time, the IBM group, drawing directly on the noisy channel model\nfor speech recognition, proposed two related paradigms for statistical MT . These statistical MT\ninclude the generative algorithms that became known as IBM Models 1 through IBM Models\n5, implemented in the Candide system. The algorithms (except for the decoder) Candide\nwere published in full detail\u2014 encouraged by the US government who had par-\ntially funded the work\u2014 which gave them a huge impact on the research community\n(Brown et al. 1990, Brown et al. 1993).\nThe group also developed a discriminative approach, called MaxEnt (for maxi-\nmum entropy, an alternative formulation of logistic regression), which allowed many\nfeatures to be combined discriminatively rather than generatively (Berger et al.,\n1996), which was further developed by Och and Ney (2002).\nBy the turn of the century, most academic research on machine translation used\nstatistical MT, either in the generative or discriminative mode. An extended version\nof the generative approach, called phrase-based translation was developed, basedphrase-based\ntranslation\non inducing translations for phrase-pairs (Och 1998, Marcu and Wong 2002, Koehn\net al. (2003), Och and Ney 2004, Deng and Byrne 2005, inter alia).\nOnce automatic metrics like BLEU were developed (Papineni et al., 2002), the\ndiscriminative log linear formulation (Och and Ney, 2004), drawing from the IBM\nMaxEnt work (Berger et al., 1996), was used to directly optimize evaluation metrics\nlike BLEU in a method known as Minimum Error Rate Training , orMERT (Och, MERT\n2003), also drawing from speech recognition models (Chou et al., 1993). Toolkits\nlike GIZA (Och and Ney, 2003) and Moses (Koehn et al. 2006, Zens and Ney 2007) Moses\nwere widely used.\nThere were also approaches around the turn of the century that were based on\nsyntactic structure (Chapter 18). Models based on transduction grammars (alsotransduction\ngrammars\ncalled synchronous grammars ) assign a parallel syntactic tree structure to a pair\nof sentences in different languages, with the goal of translating the sentences by\napplying reordering operations on the trees. From a generative perspective, we can\nview a transduction grammar as generating pairs of aligned sentences in two lan-\nguages. Some of the most widely used models included the inversion transduction",
    "metadata": {
      "source": "13",
      "chunk_id": 46,
      "token_count": 770,
      "chapter_title": ""
    }
  },
  {
    "content": "Once automatic metrics like BLEU were developed (Papineni et al., 2002), the\ndiscriminative log linear formulation (Och and Ney, 2004), drawing from the IBM\nMaxEnt work (Berger et al., 1996), was used to directly optimize evaluation metrics\nlike BLEU in a method known as Minimum Error Rate Training , orMERT (Och, MERT\n2003), also drawing from speech recognition models (Chou et al., 1993). Toolkits\nlike GIZA (Och and Ney, 2003) and Moses (Koehn et al. 2006, Zens and Ney 2007) Moses\nwere widely used.\nThere were also approaches around the turn of the century that were based on\nsyntactic structure (Chapter 18). Models based on transduction grammars (alsotransduction\ngrammars\ncalled synchronous grammars ) assign a parallel syntactic tree structure to a pair\nof sentences in different languages, with the goal of translating the sentences by\napplying reordering operations on the trees. From a generative perspective, we can\nview a transduction grammar as generating pairs of aligned sentences in two lan-\nguages. Some of the most widely used models included the inversion transduction\ngrammar (Wu, 1996) and synchronous context-free grammars (Chiang, 2005),inversion\ntransduction\ngrammar",
    "metadata": {
      "source": "13",
      "chunk_id": 47,
      "token_count": 302,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 26\n\n26 CHAPTER 13 \u2022 M ACHINE TRANSLATION\nNeural networks had been applied at various times to various aspects of machine\ntranslation; for example Schwenk et al. (2006) showed how to use neural language\nmodels to replace n-gram language models in a Spanish-English system based on\nIBM Model 4. The modern neural encoder-decoder approach was pioneered by\nKalchbrenner and Blunsom (2013), who used a CNN encoder and an RNN decoder,\nand was \ufb01rst applied to MT by Bahdanau et al. (2015). The transformer encoder-\ndecoder was proposed by Vaswani et al. (2017) (see the History section of Chap-\nter 9).\nResearch on evaluation of machine translation began quite early. Miller and\nBeebe-Center (1956) proposed a number of methods drawing on work in psycholin-\nguistics. These included the use of cloze and Shannon tasks to measure intelligibility\nas well as a metric of edit distance from a human translation, the intuition that un-\nderlies all modern overlap-based automatic evaluation metrics. The ALPAC report\nincluded an early evaluation study conducted by John Carroll that was extremely in-\n\ufb02uential (Pierce et al., 1966, Appendix 10). Carroll proposed distinct measures for\n\ufb01delity and intelligibility, and had raters score them subjectively on 9-point scales.\nMuch early evaluation work focuses on automatic word-overlap metrics like BLEU\n(Papineni et al., 2002), NIST (Doddington, 2002), TER (Translation Error Rate)\n(Snover et al., 2006), Precision and Recall (Turian et al., 2003), and METEOR\n(Banerjee and Lavie, 2005); character n-gram overlap methods like chrF (Popovi \u00b4c,\n2015) came later. More recent evaluation work, echoing the ALPAC report, has\nemphasized the importance of careful statistical methodology and the use of human\nevaluation (Kocmi et al., 2021; Marie et al., 2021).\nThe early history of MT is surveyed in Hutchins 1986 and 1997; Nirenburg et al.\n(2002) collects early readings. See Croft (1990) or Comrie (1989) for introductions\nto linguistic typology.\nExercises\n13.1 Compute by hand the chrF2,2 score for HYP2 on page 20 (the answer should\nround to .62).",
    "metadata": {
      "source": "13",
      "chunk_id": 48,
      "token_count": 550,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 27",
    "metadata": {
      "source": "13",
      "chunk_id": 49,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Exercises 27\nAnastasopoulos, A. and G. Neubig. 2020. Should all cross-\nlingual embeddings speak English? ACL.\nArtetxe, M. and H. Schwenk. 2019. Massively multilingual\nsentence embeddings for zero-shot cross-lingual transfer\nand beyond. TACL , 7:597\u2013610.\nBahdanau, D., K. H. Cho, and Y . Bengio. 2015. Neural ma-\nchine translation by jointly learning to align and translate.\nICLR 2015 .\nBanerjee, S. and A. Lavie. 2005. METEOR: An automatic\nmetric for MT evaluation with improved correlation with\nhuman judgments. Proceedings of ACL Workshop on In-\ntrinsic and Extrinsic Evaluation Measures for MT and/or\nSummarization .\nBa\u02dcn\u00b4on, M., P. Chen, B. Haddow, K. Hea\ufb01eld, H. Hoang,\nM. Espl `a-Gomis, M. L. Forcada, A. Kamran, F. Kirefu,\nP. Koehn, S. Ortiz Rojas, L. Pla Sempere, G. Ram \u00b4\u0131rez-\nS\u00b4anchez, E. Sarr \u00b4\u0131as, M. Strelec, B. Thompson, W. Waites,\nD. Wiggins, and J. Zaragoza. 2020. ParaCrawl: Web-\nscale acquisition of parallel corpora. ACL.\nBar-Hillel, Y . 1960. The present status of automatic transla-\ntion of languages. In F. Alt, ed., Advances in Computers\n1, 91\u2013163. Academic Press.\nBentivogli, L., M. Cettolo, M. Federico, and C. Federmann.\n2018. Machine translation human evaluation: an investi-\ngation of evaluation based on post-editing and its relation\nwith direct assessment. ICSLT .\nBerger, A., S. A. Della Pietra, and V . J. Della Pietra. 1996. A\nmaximum entropy approach to natural language process-\ning. Computational Linguistics , 22(1):39\u201371.\nBickel, B. 2003. Referential density in discourse and syntac-\ntic typology. Language , 79(2):708\u2013736.\nBostrom, K. and G. Durrett. 2020. Byte pair encoding is\nsuboptimal for language model pretraining. EMNLP .\nBrown, P. F., J. Cocke, S. A. Della Pietra, V . J. Della Pietra,\nF. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin.\n1990. A statistical approach to machine translation. Com-\nputational Linguistics , 16(2):79\u201385.\nBrown, P. F., S. A. Della Pietra, V . J. Della Pietra, and R. L.\nMercer. 1993. The mathematics of statistical machine\ntranslation: Parameter estimation. Computational Lin-\nguistics , 19(2):263\u2013311.\nCallison-Burch, C., M. Osborne, and P. Koehn. 2006. Re-\nevaluating the role of BLEU in machine translation re-\nsearch. EACL .",
    "metadata": {
      "source": "13",
      "chunk_id": 50,
      "token_count": 738,
      "chapter_title": ""
    }
  },
  {
    "content": "maximum entropy approach to natural language process-\ning. Computational Linguistics , 22(1):39\u201371.\nBickel, B. 2003. Referential density in discourse and syntac-\ntic typology. Language , 79(2):708\u2013736.\nBostrom, K. and G. Durrett. 2020. Byte pair encoding is\nsuboptimal for language model pretraining. EMNLP .\nBrown, P. F., J. Cocke, S. A. Della Pietra, V . J. Della Pietra,\nF. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin.\n1990. A statistical approach to machine translation. Com-\nputational Linguistics , 16(2):79\u201385.\nBrown, P. F., S. A. Della Pietra, V . J. Della Pietra, and R. L.\nMercer. 1993. The mathematics of statistical machine\ntranslation: Parameter estimation. Computational Lin-\nguistics , 19(2):263\u2013311.\nCallison-Burch, C., M. Osborne, and P. Koehn. 2006. Re-\nevaluating the role of BLEU in machine translation re-\nsearch. EACL .\nChandioux, J. 1976. M \u00b4ET\u00b4EO: un syst `eme op \u00b4erationnel pour\nla traduction automatique des bulletins m \u00b4et\u00b4eorologiques\ndestin \u00b4es au grand public. Meta , 21:127\u2013133.\nChiang, D. 2005. A hierarchical phrase-based model for sta-\ntistical machine translation. ACL.\nChou, W., C.-H. Lee, and B. H. Juang. 1993. Minimum error\nrate training based on n-best string models. ICASSP .\nComrie, B. 1989. Language Universals and Linguistic Ty-\npology , 2nd edition. Blackwell.Costa-juss `a, M. R., J. Cross, O. C \u00b8 elebi, M. Elbayad,\nK. Hea\ufb01eld, K. Heffernan, E. Kalbassi, J. Lam, D. Licht,\nJ. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood,\nB. Akula, L. Barrault, G. M. Gonzalez, P. Hansanti,\nJ. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe,\nS. Spruit, C. Tran, P. Andrews, N. F. Ayan, S. Bhosale,\nS. Edunov, A. Fan, C. Gao, V . Goswami, F. Guzm \u00b4an,\nP. Koehn, A. Mourachko, C. Ropers, S. Saleem,\nH. Schwenk, J. Wang, and NLLB Team. 2022. No\nlanguage left behind: Scaling human-centered machine\ntranslation. ArXiv.\nCroft, W. 1990. Typology and Universals . Cambridge Uni-\nversity Press.\nDeng, Y . and W. Byrne. 2005. HMM word and phrase align-\nment for statistical machine translation. HLT-EMNLP .\nDoddington, G. 2002. Automatic evaluation of machine\ntranslation quality using n-gram co-occurrence statistics.\nHLT.",
    "metadata": {
      "source": "13",
      "chunk_id": 51,
      "token_count": 756,
      "chapter_title": ""
    }
  },
  {
    "content": "J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood,\nB. Akula, L. Barrault, G. M. Gonzalez, P. Hansanti,\nJ. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe,\nS. Spruit, C. Tran, P. Andrews, N. F. Ayan, S. Bhosale,\nS. Edunov, A. Fan, C. Gao, V . Goswami, F. Guzm \u00b4an,\nP. Koehn, A. Mourachko, C. Ropers, S. Saleem,\nH. Schwenk, J. Wang, and NLLB Team. 2022. No\nlanguage left behind: Scaling human-centered machine\ntranslation. ArXiv.\nCroft, W. 1990. Typology and Universals . Cambridge Uni-\nversity Press.\nDeng, Y . and W. Byrne. 2005. HMM word and phrase align-\nment for statistical machine translation. HLT-EMNLP .\nDoddington, G. 2002. Automatic evaluation of machine\ntranslation quality using n-gram co-occurrence statistics.\nHLT.\nDorr, B. 1994. Machine translation divergences: A formal\ndescription and proposed solution. Computational Lin-\nguistics , 20(4):597\u2013633.\nDostert, L. 1955. The Georgetown-I.B.M. experiment. In\nMachine Translation of Languages: Fourteen Essays ,\n124\u2013135. MIT Press.\nDryer, M. S. and M. Haspelmath, eds. 2013. The World Atlas\nof Language Structures Online . Max Planck Institute for\nEvolutionary Anthropology, Leipzig. Available online at\nhttp://wals.info .\nEdunov, S., M. Ott, M. Auli, and D. Grangier. 2018. Under-\nstanding back-translation at scale. EMNLP .\nFan, A., S. Bhosale, H. Schwenk, Z. Ma, A. El-Kishky,\nS. Goyal, M. Baines, O. Celebi, G. Wenzek, V . Chaud-\nhary, N. Goyal, T. Birch, V . Liptchinsky, S. Edunov,\nM. Auli, and A. Joulin. 2021. Beyond english-centric\nmultilingual machine translation. JMLR , 22(107):1\u201348.\n8, W. Nekoto, V . Marivate, T. Matsila, T. Fasubaa,\nT. Kolawole, T. Fagbohungbe, S. O. Akinola, S. H.\nMuhammad, S. Kabongo, S. Osei, S. Freshia, R. A.\nNiyongabo, R. M. P. Ogayo, O. Ahia, M. Meressa,\nM. Adeyemi, M. Mokgesi-Selinga, L. Okegbemi, L. J.\nMartinus, K. Tajudeen, K. Degila, K. Ogueji, K. Siminyu,\nJ. Kreutzer, J. Webster, J. T. Ali, J. A. I. Orife,\nI. Ezeani, I. A. Dangana, H. Kamper, H. Elsahar, G. Duru,",
    "metadata": {
      "source": "13",
      "chunk_id": 52,
      "token_count": 752,
      "chapter_title": ""
    }
  },
  {
    "content": "hary, N. Goyal, T. Birch, V . Liptchinsky, S. Edunov,\nM. Auli, and A. Joulin. 2021. Beyond english-centric\nmultilingual machine translation. JMLR , 22(107):1\u201348.\n8, W. Nekoto, V . Marivate, T. Matsila, T. Fasubaa,\nT. Kolawole, T. Fagbohungbe, S. O. Akinola, S. H.\nMuhammad, S. Kabongo, S. Osei, S. Freshia, R. A.\nNiyongabo, R. M. P. Ogayo, O. Ahia, M. Meressa,\nM. Adeyemi, M. Mokgesi-Selinga, L. Okegbemi, L. J.\nMartinus, K. Tajudeen, K. Degila, K. Ogueji, K. Siminyu,\nJ. Kreutzer, J. Webster, J. T. Ali, J. A. I. Orife,\nI. Ezeani, I. A. Dangana, H. Kamper, H. Elsahar, G. Duru,\nG. Kioko, E. Murhabazi, E. van Biljon, D. Whitenack,\nC. Onyefuluchi, C. Emezue, B. Dossou, B. Sibanda, B. I.\nBassey, A. Olabiyi, A. Ramkilowan, A. \u00a8Oktem, A. Ak-\ninfaderin, and A. Bashir. 2020. Participatory research\nfor low-resourced machine translation: A case study in\nAfrican languages. Findings of EMNLP .\nGale, W. A. and K. W. Church. 1991. A program for aligning\nsentences in bilingual corpora. ACL.\nGale, W. A. and K. W. Church. 1993. A program for aligning\nsentences in bilingual corpora. Computational Linguis-\ntics, 19:75\u2013102.\nGoel, V . and W. Byrne. 2000. Minimum bayes-risk auto-\nmatic speech recognition. Computer Speech & Language ,\n14(2):115\u2013135.\nHutchins, W. J. 1986. Machine Translation: Past, Present,\nFuture . Ellis Horwood, Chichester, England.",
    "metadata": {
      "source": "13",
      "chunk_id": 53,
      "token_count": 532,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 28",
    "metadata": {
      "source": "13",
      "chunk_id": 54,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "28 Chapter 13 \u2022 Machine Translation\nHutchins, W. J. 1997. From \ufb01rst conception to \ufb01rst demon-\nstration: The nascent years of machine translation, 1947\u2013\n1954. A chronology. Machine Translation , 12:192\u2013252.\nHutchins, W. J. and H. L. Somers. 1992. An Introduction to\nMachine Translation . Academic Press.\nKalchbrenner, N. and P. Blunsom. 2013. Recurrent continu-\nous translation models. EMNLP .\nKay, M. and M. R \u00a8oscheisen. 1988. Text-translation align-\nment. Technical Report P90-00143, Xerox Palo Alto Re-\nsearch Center, Palo Alto, CA.\nKay, M. and M. R \u00a8oscheisen. 1993. Text-translation align-\nment. Computational Linguistics , 19:121\u2013142.\nKocmi, T., C. Federmann, R. Grundkiewicz, M. Junczys-\nDowmunt, H. Matsushita, and A. Menezes. 2021. To ship\nor not to ship: An extensive evaluation of automatic met-\nrics for machine translation. ArXiv.\nKoehn, P. 2005. Europarl: A parallel corpus for statistical\nmachine translation. MT summit, vol. 5 .\nKoehn, P., H. Hoang, A. Birch, C. Callison-Burch, M. Fed-\nerico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,\nR. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst.\n2006. Moses: Open source toolkit for statistical machine\ntranslation. ACL.\nKoehn, P., F. J. Och, and D. Marcu. 2003. Statistical phrase-\nbased translation. HLT-NAACL .\nKreutzer, J., I. Caswell, L. Wang, A. Wahab, D. van Esch,\nN. Ulzii-Orshikh, A. Tapo, N. Subramani, A. Sokolov,\nC. Sikasote, M. Setyawan, S. Sarin, S. Samb, B. Sagot,\nC. Rivera, A. Rios, I. Papadimitriou, S. Osei, P. O.\nSuarez, I. Orife, K. Ogueji, A. N. Rubungo, T. Q. Nguyen,\nM. M \u00a8uller, A. M \u00a8uller, S. H. Muhammad, N. Muhammad,\nA. Mnyakeni, J. Mirzakhalov, T. Matangira, C. Leong,\nN. Lawson, S. Kudugunta, Y . Jernite, M. Jenny, O. Firat,\nB. F. P. Dossou, S. Dlamini, N. de Silva, S. C \u00b8 abuk Ball\u0131,\nS. Biderman, A. Battisti, A. Baruwa, A. Bapna, P. Bal-\njekar, I. A. Azime, A. Awokoya, D. Ataman, O. Ahia,\nO. Ahia, S. Agrawal, and M. Adeyemi. 2022. Quality at",
    "metadata": {
      "source": "13",
      "chunk_id": 55,
      "token_count": 756,
      "chapter_title": ""
    }
  },
  {
    "content": "C. Sikasote, M. Setyawan, S. Sarin, S. Samb, B. Sagot,\nC. Rivera, A. Rios, I. Papadimitriou, S. Osei, P. O.\nSuarez, I. Orife, K. Ogueji, A. N. Rubungo, T. Q. Nguyen,\nM. M \u00a8uller, A. M \u00a8uller, S. H. Muhammad, N. Muhammad,\nA. Mnyakeni, J. Mirzakhalov, T. Matangira, C. Leong,\nN. Lawson, S. Kudugunta, Y . Jernite, M. Jenny, O. Firat,\nB. F. P. Dossou, S. Dlamini, N. de Silva, S. C \u00b8 abuk Ball\u0131,\nS. Biderman, A. Battisti, A. Baruwa, A. Bapna, P. Bal-\njekar, I. A. Azime, A. Awokoya, D. Ataman, O. Ahia,\nO. Ahia, S. Agrawal, and M. Adeyemi. 2022. Quality at\na glance: An audit of web-crawled multilingual datasets.\nTACL , 10:50\u201372.\nKudo, T. 2018. Subword regularization: Improving neural\nnetwork translation models with multiple subword candi-\ndates. ACL.\nKudo, T. and J. Richardson. 2018. SentencePiece: A simple\nand language independent subword tokenizer and detok-\nenizer for neural text processing. EMNLP .\nKumar, S. and W. Byrne. 2004. Minimum Bayes-risk decod-\ning for statistical machine translation. HLT-NAACL .\nLan, Z., M. Chen, S. Goodman, K. Gimpel, P. Sharma,\nand R. Soricut. 2020. ALBERT: A lite BERT for self-\nsupervised learning of language representations. ICLR .\nLison, P. and J. Tiedemann. 2016. Opensubtitles2016: Ex-\ntracting large parallel corpora from movie and tv subti-\ntles. LREC .\nLowerre, B. T. 1976. The Harpy Speech Recognition System .\nPh.D. thesis, Carnegie Mellon University, Pittsburgh, PA.\nMarcu, D. and W. Wong. 2002. A phrase-based, joint proba-\nbility model for statistical machine translation. EMNLP .\nMarie, B., A. Fujita, and R. Rubino. 2021. Scienti\ufb01c credi-\nbility of machine translation research: A meta-evaluation\nof 769 papers. ACL.McLuhan, M. 1964. Understanding Media: The Extensions\nof Man . New American Library.\nMiller, G. A. and J. G. Beebe-Center. 1956. Some psycho-\nlogical methods for evaluating the quality of translations.\nMechanical Translation , 3:73\u201380.\nNirenburg, S., H. L. Somers, and Y . Wilks, eds. 2002. Read-\nings in Machine Translation . MIT Press.\nOch, F. J. 1998. Ein beispielsbasierter und statis-\ntischer Ansatz zum maschinellen Lernen von",
    "metadata": {
      "source": "13",
      "chunk_id": 56,
      "token_count": 742,
      "chapter_title": ""
    }
  },
  {
    "content": "tracting large parallel corpora from movie and tv subti-\ntles. LREC .\nLowerre, B. T. 1976. The Harpy Speech Recognition System .\nPh.D. thesis, Carnegie Mellon University, Pittsburgh, PA.\nMarcu, D. and W. Wong. 2002. A phrase-based, joint proba-\nbility model for statistical machine translation. EMNLP .\nMarie, B., A. Fujita, and R. Rubino. 2021. Scienti\ufb01c credi-\nbility of machine translation research: A meta-evaluation\nof 769 papers. ACL.McLuhan, M. 1964. Understanding Media: The Extensions\nof Man . New American Library.\nMiller, G. A. and J. G. Beebe-Center. 1956. Some psycho-\nlogical methods for evaluating the quality of translations.\nMechanical Translation , 3:73\u201380.\nNirenburg, S., H. L. Somers, and Y . Wilks, eds. 2002. Read-\nings in Machine Translation . MIT Press.\nOch, F. J. 1998. Ein beispielsbasierter und statis-\ntischer Ansatz zum maschinellen Lernen von\nnat\u00a8urlichsprachlicher \u00a8Ubersetzung . Ph.D. thesis, Uni-\nversit \u00a8at Erlangen-N \u00a8urnberg, Germany. Diplomarbeit\n(diploma thesis).\nOch, F. J. 2003. Minimum error rate training in statistical\nmachine translation. ACL.\nOch, F. J. and H. Ney. 2002. Discriminative training and\nmaximum entropy models for statistical machine transla-\ntion. ACL.\nOch, F. J. and H. Ney. 2003. A systematic comparison of\nvarious statistical alignment models. Computational Lin-\nguistics , 29(1):19\u201351.\nOch, F. J. and H. Ney. 2004. The alignment template ap-\nproach to statistical machine translation. Computational\nLinguistics , 30(4):417\u2013449.\nPapineni, K., S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu:\nA method for automatic evaluation of machine transla-\ntion. ACL.\nPierce, J. R., J. B. Carroll, E. P. Hamp, D. G. Hays, C. F.\nHockett, A. G. Oettinger, and A. J. Perlis. 1966. Lan-\nguage and Machines: Computers in Translation and Lin-\nguistics . ALPAC report. National Academy of Sciences,\nNational Research Council, Washington, DC.\nPopovi \u00b4c, M. 2015. chrF: character n-gram F-score for auto-\nmatic MT evaluation. Proceedings of the Tenth Workshop\non Statistical Machine Translation .\nPost, M. 2018. A call for clarity in reporting BLEU scores.\nWMT 2018 .\nPrates, M. O. R., P. H. Avelar, and L. C. Lamb. 2019. Assess-\ning gender bias in machine translation: a case study with\nGoogle Translate. Neural Computing and Applications ,\n32:6363\u20136381.\nRaffel, C., N. Shazeer, A. Roberts, K. Lee, S. Narang,",
    "metadata": {
      "source": "13",
      "chunk_id": 57,
      "token_count": 735,
      "chapter_title": ""
    }
  },
  {
    "content": "Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu:\nA method for automatic evaluation of machine transla-\ntion. ACL.\nPierce, J. R., J. B. Carroll, E. P. Hamp, D. G. Hays, C. F.\nHockett, A. G. Oettinger, and A. J. Perlis. 1966. Lan-\nguage and Machines: Computers in Translation and Lin-\nguistics . ALPAC report. National Academy of Sciences,\nNational Research Council, Washington, DC.\nPopovi \u00b4c, M. 2015. chrF: character n-gram F-score for auto-\nmatic MT evaluation. Proceedings of the Tenth Workshop\non Statistical Machine Translation .\nPost, M. 2018. A call for clarity in reporting BLEU scores.\nWMT 2018 .\nPrates, M. O. R., P. H. Avelar, and L. C. Lamb. 2019. Assess-\ning gender bias in machine translation: a case study with\nGoogle Translate. Neural Computing and Applications ,\n32:6363\u20136381.\nRaffel, C., N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y . Zhou, W. Li, and P. J. Liu. 2020. Exploring\nthe limits of transfer learning with a uni\ufb01ed text-to-text\ntransformer. JMLR , 21(140):1\u201367.\nRei, R., C. Stewart, A. C. Farinha, and A. Lavie.\n2020. COMET: A neural framework for MT evaluation.\nEMNLP .\nSchiebinger, L. 2014. Scienti\ufb01c research must take gender\ninto account. Nature , 507(7490):9.\nSchwenk, H. 2018. Filtering and mining parallel data in a\njoint multilingual space. ACL.\nSchwenk, H., D. Dechelotte, and J.-L. Gauvain. 2006. Con-\ntinuous space language models for statistical machine\ntranslation. COLING/ACL .\nSchwenk, H., G. Wenzek, S. Edunov, E. Grave, A. Joulin,\nand A. Fan. 2021. CCMatrix: Mining billions of high-\nquality parallel sentences on the web. ACL.\nSellam, T., D. Das, and A. Parikh. 2020. BLEURT: Learning\nrobust metrics for text generation. ACL.",
    "metadata": {
      "source": "13",
      "chunk_id": 58,
      "token_count": 566,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 29",
    "metadata": {
      "source": "13",
      "chunk_id": 59,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Exercises 29\nSlobin, D. I. 1996. Two ways to travel. In M. Shibatani and\nS. A. Thompson, eds, Grammatical Constructions: Their\nForm and Meaning , 195\u2013220. Clarendon Press.\nSnover, M., B. Dorr, R. Schwartz, L. Micciulla, and\nJ. Makhoul. 2006. A study of translation edit rate with\ntargeted human annotation. AMTA-2006 .\nStanovsky, G., N. A. Smith, and L. Zettlemoyer. 2019. Eval-\nuating gender bias in machine translation. ACL.\nStolcke, A., Y . Konig, and M. Weintraub. 1997. Explicit\nword error minimization in N-best list rescoring. EU-\nROSPEECH , volume 1.\nSuzgun, M., L. Melas-Kyriazi, and D. Jurafsky. 2023. Fol-\nlow the wisdom of the crowd: Effective text generation\nvia minimum Bayes risk decoding. Findings of ACL\n2023 .\nTalmy, L. 1985. Lexicalization patterns: Semantic structure\nin lexical forms. In T. Shopen, ed., Language Typology\nand Syntactic Description, Volume 3 . Cambridge Univer-\nsity Press. Originally appeared as UC Berkeley Cognitive\nScience Program Report No. 30, 1980.\nTalmy, L. 1991. Path to realization: A typology of event\ncon\ufb02ation. BLS-91 .\nThompson, B. and P. Koehn. 2019. Vecalign: Improved sen-\ntence alignment in linear time and space. EMNLP .\nTurian, J. P., L. Shen, and I. D. Melamed. 2003. Evaluation\nof machine translation and its evaluation. Proceedings of\nMT Summit IX .\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, \u0141. Kaiser, and I. Polosukhin. 2017. Atten-\ntion is all you need. NeurIPS .\nVauquois, B. 1968. A survey of formal grammars and al-\ngorithms for recognition and transformation in machine\ntranslation. IFIP Congress 1968 .\nWeaver, W. 1949/1955. Translation. In W. N. Locke and\nA. D. Boothe, eds, Machine Translation of Languages ,\n15\u201323. MIT Press. Reprinted from a memorandum writ-\nten by Weaver in 1949.\nWu, D. 1996. A polynomial-time algorithm for statistical\nmachine translation. ACL.\nWu, Y ., M. Schuster, Z. Chen, Q. V . Le, M. Norouzi,\nW. Macherey, M. Krikun, Y . Cao, Q. Gao, K. Macherey,\nJ. Klingner, A. Shah, M. Johnson, X. Liu, \u0141. Kaiser,\nS. Gouws, Y . Kato, T. Kudo, H. Kazawa, K. Stevens,\nG. Kurian, N. Patil, W. Wang, C. Young, J. Smith,\nJ. Riesa, A. Rudnick, O. Vinyals, G. S. Corrado,\nM. Hughes, and J. Dean. 2016. Google\u2019s neural machine",
    "metadata": {
      "source": "13",
      "chunk_id": 60,
      "token_count": 766,
      "chapter_title": ""
    }
  },
  {
    "content": "Vauquois, B. 1968. A survey of formal grammars and al-\ngorithms for recognition and transformation in machine\ntranslation. IFIP Congress 1968 .\nWeaver, W. 1949/1955. Translation. In W. N. Locke and\nA. D. Boothe, eds, Machine Translation of Languages ,\n15\u201323. MIT Press. Reprinted from a memorandum writ-\nten by Weaver in 1949.\nWu, D. 1996. A polynomial-time algorithm for statistical\nmachine translation. ACL.\nWu, Y ., M. Schuster, Z. Chen, Q. V . Le, M. Norouzi,\nW. Macherey, M. Krikun, Y . Cao, Q. Gao, K. Macherey,\nJ. Klingner, A. Shah, M. Johnson, X. Liu, \u0141. Kaiser,\nS. Gouws, Y . Kato, T. Kudo, H. Kazawa, K. Stevens,\nG. Kurian, N. Patil, W. Wang, C. Young, J. Smith,\nJ. Riesa, A. Rudnick, O. Vinyals, G. S. Corrado,\nM. Hughes, and J. Dean. 2016. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and\nmachine translation. ArXiv preprint arXiv:1609.08144.\nZens, R. and H. Ney. 2007. Ef\ufb01cient phrase-table represen-\ntation for machine translation with applications to online\nMT and speech translation. NAACL-HLT .\nZhang, T., V . Kishore, F. Wu, K. Q. Weinberger, and Y . Artzi.\n2020. BERTscore: Evaluating text generation with\nBERT. ICLR 2020 .\nZiemski, M., M. Junczys-Dowmunt, and B. Pouliquen. 2016.\nThe United Nations parallel corpus v1.0. LREC .",
    "metadata": {
      "source": "13",
      "chunk_id": 61,
      "token_count": 445,
      "chapter_title": ""
    }
  }
]