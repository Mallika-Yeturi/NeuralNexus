[
  {
    "content": "# 15\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n15Chatbots & Dialogue Systems\nLes lois de la conversation sont en g \u00b4en\u00b4eral de ne s\u2019y appesantir sur aucun ob-\njet, mais de passer l \u00b4eg`erement, sans effort et sans affectation, d\u2019un sujet `a un\nautre ; de savoir y parler de choses frivoles comme de choses s \u00b4erieuses\n[The rules of conversation are, in general, not to dwell on any one subject,\nbut to pass lightly from one to another without effort and without affectation;\nto know how to speak about trivial topics as well as serious ones;]\nThe 18th C. Encyclopedia of Diderot, start of the entry on conversation\nThe literature of the fantastic abounds in inanimate objects magically endowed with\nthe gift of speech. From Ovid\u2019s statue of Pygmalion to Mary Shelley\u2019s story about\nFrankenstein, we continually reinvent stories about creat-\ning something and then having a chat with it. Legend has\nit that after \ufb01nishing his sculpture Moses , Michelangelo\nthought it so lifelike that he tapped it on the knee and\ncommanded it to speak. Perhaps this shouldn\u2019t be sur-\nprising. Language is the mark of humanity and sentience,\nandconversation ordialogue is the most fundamental conversation\ndialogue arena of language. It is the \ufb01rst kind of language we\nlearn as children, and the kind we engage in constantly,\nwhether we are ordering lunch, buying train tickets, or\ntalking with our families, friends, or coworkers.\nThis chapter introduces the fundamental algorithms of programs that use con-\nversation to interact with users. We often distinguish between two kinds of archi-\ntectures. Task-oriented dialogue systems converse with users to accomplish \ufb01xed dialogue system\ntasks like controlling appliances or \ufb01nding restaurants, relying on a data structure\ncalled the frame , which represents the knowledge a system needs to acquire from frame\nthe user (like the time to set an alarm clock). Chatbots , by contrast, are designed chatbot\nto mimic the longer and more unstructured conversations or \u2018chats\u2019 characteristic of\nhuman-human interaction. Modern systems incorporate aspects of both; industrial\nchatbots like ChatGPT can carry on longer unstructured conversations; industrial\ndigital assistants like Siri or Alexa are generally frame-based dialogue systems.\nThe fact that chatbots and dialogue systems are designed for human-computer\ninteraction has strong implications for their design and use. Many of these impli-\ncations already became clear in one of the earliest chatbots, ELIZA (Weizenbaum,\n1966). ELIZA was designed to simulate a Rogerian psychologist, based on a branch\nof clinical psychology whose methods involve drawing the patient out by re\ufb02ecting\npatient\u2019s statements back at them. Rogerian interactions are the rare type of conver-\nsation in which, as Weizenbaum points out, one can \u201cassume the pose of knowing\nalmost nothing of the real world\u201d. If a patient says \u201cI went for a long boat ride\u201d and\nthe psychiatrist says \u201cTell me about boats\u201d, you don\u2019t assume she didn\u2019t know what",
    "metadata": {
      "source": "15",
      "chunk_id": 0,
      "token_count": 702,
      "chapter_title": "15"
    }
  },
  {
    "content": "## Page 2\n\n2CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\nMen are all alike.\nIN WHAT WAY\nThey\u2019re always bugging us about something or other.\nCAN YOU THINK OF A SPECIFIC EXAMPLE\nWell my boyfriend made me come here.\nYOUR BOYFRIEND MADE YOU COME HERE\nHe says I\u2019m depressed much of the time.\nI AM SORRY TO HEAR YOU ARE DEPRESSED\nFigure 15.1 Sample ELIZA dialogue from Weizenbaum (1966).\na boat is, but rather assume she had some conversational goal.1\nWeizenbaum made use of this property of Rogerian psychiatric conversations,\nalong with clever regular expressions, to allow ELIZA to interact in ways that seemed\ndeceptively human-like, as in the sample conversational fragment in Fig. 15.1.\nAs we foreshadowed in Chapter 2, ELIZA worked by simple rules roughly like:\n(.*) YOU (.*) ME -> WHAT MAKES YOU THINK I \\2 YOU\nto transform a user sentence like \u201c You hate me \u201d into a system response like\nWHAT MAKES YOU THINK I HATE YOU\nAmong Weizenbaum\u2019s clever tricks are the linking of each ELIZA pattern/rule\nto a keyword. Consider the following user sentence:\nI know everybody laughed at me\nBecause it has the word \u201cI\u201d, this sentence could match the following rule whose\nkeyword is I:\nI (.*) -> You say you \\1\nproducing:\nYOU SAY YOU KNOW EVERYBODY LAUGHED AT YOU\nWeizenbaum points out, however, that a more powerful response would rely on\nthe keyword \u201ceverybody\u201d, since someone using universals like everybody oralways\nis probably thinking about a speci\ufb01c person or situation. So the ELIZA algorithm\nprefers to respond using patterns associated more speci\ufb01c keywords like everybody :\nWHO IN PARTICULAR ARE YOU THINKING OF?\nIf no keyword matches, the algorithm chooses a non-committal response like\n\u201cPLEASE GO ON \u201d, \u201cTHAT'S VERY INTERESTING \u201d, or \u201c I SEE \u201d.\nELIZA illustrates a number of important issues with chatbots. First, people\nbecame deeply emotionally involved and conducted very personal conversations,\neven to the extent of asking Weizenbaum to leave the room while they were typ-\ning. Reeves and Nass (1996) show that people tend to assign human characteristics\nto computers and interact with them in ways that are typical of human-human in-\nteractions. They interpret an utterance in the way they would if it had spoken by a\nhuman, (even though they are aware they are talking to a computer). This means that\nchatbots can have signi\ufb01cant in\ufb02uences on people\u2019s cognitive and emotional state.\nA second related issue is privacy. When Weizenbaum suggested that he might\nwant to store the ELIZA conversations, people immediately pointed out that this\nwould violate people\u2019s privacy. Modern chatbots in the home are likely to overhear\n1This is due to the Gricean principle of relevance that we\u2019ll discuss in the next section..",
    "metadata": {
      "source": "15",
      "chunk_id": 1,
      "token_count": 660,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\n15.1 \u2022 P ROPERTIES OF HUMAN CONVERSATION 3\nprivate information, even if they aren\u2019t used for counseling as ELIZA was. Indeed,\nif a chatbot is human-like, users are more likely to disclose private information, and\nyet less likely to worry about the harm of this disclosure (Ischen et al., 2019).\nBoth of these issues (emotional engagement and privacy) mean we need to think\ncarefully about how we deploy chatbots and the people who are interacting with\nthem. Dialogue research that uses human participants often requires getting permis-\nsion from the Institutional Review Board (IRB) of your institution.\nIn the next section we introduce some basic properties of human conversation.\nWe then turn in the rest of the chapter to the two basic paradigms for conversational\ninteraction: frame-based dialogue systems and chatbots.\n15.1 Properties of Human Conversation\nConversation between humans is an intricate and complex joint activity. Before\nwe attempt to design a dialogue system to converse with humans, it is crucial to\nunderstand something about how humans converse with each other. Consider some\nof the phenomena that occur in the conversation between a human travel agent and\na human client excerpted in Fig. 15.2.\nC1: . . . I need to travel in May.\nA2: And, what day in May did you want to travel?\nC3: OK uh I need to be there for a meeting that\u2019s from the 12th to the 15th.\nA4: And you\u2019re \ufb02ying into what city?\nC5: Seattle.\nA6: And what time would you like to leave Pittsburgh?\nC7: Uh hmm I don\u2019t think there\u2019s many options for non-stop.\nA8: Right. There\u2019s three non-stops today.\nC9: What are they?\nA10: The \ufb01rst one departs PGH at 10:00am arrives Seattle at 12:05 their time.\nThe second \ufb02ight departs PGH at 5:55pm, arrives Seattle at 8pm. And the\nlast \ufb02ight departs PGH at 8:15pm arrives Seattle at 10:28pm.\nC11: OK I\u2019ll take the 5ish \ufb02ight on the night before on the 11th.\nA12: On the 11th? OK. Departing at 5:55pm arrives Seattle at 8pm, U.S. Air\n\ufb02ight 115.\nC13: OK.\nA14: And you said returning on May 15th?\nC15: Uh, yeah, at the end of the day.\nA16: OK. There\u2019s #two non-stops . . . #\nC17: #Act. . . actually #, what day of the week is the 15th?\nA18: It\u2019s a Friday.\nC19: Uh hmm. I would consider staying there an extra day til Sunday.\nA20: OK. . . OK. On Sunday I have . . .\nFigure 15.2 Part of a phone conversation between a human travel agent (A) and human\nclient (C). The passages framed by # in A 16and C 17indicate overlaps in speech.\nTurns\nA dialogue is a sequence of turns (C1, A2, C3, and so on), each a single contribution turn\nfrom one speaker to the dialogue (as if in a game: I take a turn, then you take a turn,",
    "metadata": {
      "source": "15",
      "chunk_id": 2,
      "token_count": 741,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4",
    "metadata": {
      "source": "15",
      "chunk_id": 3,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "4CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\nthen me, and so on). There are 20 turns in Fig. 15.2. A turn can consist of a sentence\n(like C 1), although it might be as short as a single word (C 13) or as long as multiple\nsentences (A 10).\nTurn structure has important implications for spoken dialogue. A human has\nto know when to stop talking; the client interrupts (in A 16and C 17), so a system\nthat was performing this role must know to stop talking (and that the user might be\nmaking a correction). A system also has to know when to start talking. For example,\nmost of the time in conversation, speakers start their turns almost immediately after\nthe other speaker \ufb01nishes, without a long pause, because people are can usually\npredict when the other person is about to \ufb01nish talking. Spoken dialogue systems\nmust also detect whether a user is done speaking, so they can process the utterance\nand respond. This task\u2014called endpointing orendpoint detection \u2014 can be quite endpointing\nchallenging because of noise and because people often pause in the middle of turns.\nSpeech Acts\nA key insight into conversation\u2014due originally to the philosopher Wittgenstein\n(1953) but worked out more fully by Austin (1962)\u2014is that each utterance in a\ndialogue is a kind of action being performed by the speaker. These actions are com-\nmonly called speech acts ordialogue acts : here\u2019s one taxonomy consisting of 4 speech acts\nmajor classes (Bach and Harnish, 1979):\nConstatives: committing the speaker to something\u2019s being the case ( answering ,claiming ,\ncon\ufb01rming ,denying ,disagreeing ,stating )\nDirectives: attempts by the speaker to get the addressee to do something ( advising ,ask-\ning,forbidding ,inviting ,ordering ,requesting )\nCommissives: committing the speaker to some future course of action ( promising, planning,\nvowing, betting, opposing )\nAcknowledgments: express the speaker\u2019s attitude regarding the hearer with respect to some so-\ncial action ( apologizing ,greeting ,thanking ,accepting an acknowledgment )\nA user asking a person or a dialogue system to do something (\u2018Turn up the mu-\nsic\u2019) is issuing a D IRECTIVE . Asking a question that requires an answer is also\na way of issuing a D IRECTIVE : in a sense when the system says (A 2) \u201cwhat day\nin May did you want to travel?\u201d it\u2019s as if the system is (very politely) command-\ning the user to answer. By contrast, a user stating a constraint (like C 1\u2018I need to\ntravel in May\u2019) is issuing a C ONSTATIVE . A user thanking the system is issuing\nan A CKNOWLEDGMENT . The speech act expresses an important component of the\nintention of the speaker (or writer) in saying what they said.\nGrounding\nA dialogue is not just a series of independent speech acts, but rather a collective act\nperformed by the speaker and the hearer. Like all collective acts, it\u2019s important for\nthe participants to establish what they both agree on, called the common groundcommon\nground\n(Stalnaker, 1978). Speakers do this by grounding each other\u2019s utterances. Ground- grounding\ning means acknowledging that the hearer has understood the speaker (Clark, 1996).\n(People need grounding for non-linguistic actions as well; the reason an elevator but-",
    "metadata": {
      "source": "15",
      "chunk_id": 4,
      "token_count": 771,
      "chapter_title": ""
    }
  },
  {
    "content": "A user asking a person or a dialogue system to do something (\u2018Turn up the mu-\nsic\u2019) is issuing a D IRECTIVE . Asking a question that requires an answer is also\na way of issuing a D IRECTIVE : in a sense when the system says (A 2) \u201cwhat day\nin May did you want to travel?\u201d it\u2019s as if the system is (very politely) command-\ning the user to answer. By contrast, a user stating a constraint (like C 1\u2018I need to\ntravel in May\u2019) is issuing a C ONSTATIVE . A user thanking the system is issuing\nan A CKNOWLEDGMENT . The speech act expresses an important component of the\nintention of the speaker (or writer) in saying what they said.\nGrounding\nA dialogue is not just a series of independent speech acts, but rather a collective act\nperformed by the speaker and the hearer. Like all collective acts, it\u2019s important for\nthe participants to establish what they both agree on, called the common groundcommon\nground\n(Stalnaker, 1978). Speakers do this by grounding each other\u2019s utterances. Ground- grounding\ning means acknowledging that the hearer has understood the speaker (Clark, 1996).\n(People need grounding for non-linguistic actions as well; the reason an elevator but-\nton lights up when it\u2019s pressed is to acknowledge that the elevator has indeed been\ncalled, essentially grounding your action of pushing the button (Norman, 1988).)\nHumans constantly ground each other\u2019s utterances. We can ground by explicitly\nsaying \u201cOK\u201d, as the agent does in A 8or A 10. Or we can ground by repeating what\nthe other person says; in utterance A 2the agent repeats \u201cin May\u201d, demonstrating her",
    "metadata": {
      "source": "15",
      "chunk_id": 5,
      "token_count": 384,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\n15.1 \u2022 P ROPERTIES OF HUMAN CONVERSATION 5\nunderstanding to the client. Or notice that when the client answers a question, the\nagent begins the next question with \u201cAnd\u201d. The \u201cAnd\u201d implies that the new question\nis \u2018in addition\u2019 to the old question, again indicating to the client that the agent has\nsuccessfully understood the answer to the last question.\nSubdialogues and Dialogue Structure\nConversations have structure. Consider, for example, the local structure between\nspeech acts discussed in the \ufb01eld of conversation analysis (Sacks et al., 1974).conversation\nanalysis\nQUESTIONS set up an expectation for an ANSWER . PROPOSALS are followed by\nACCEPTANCE (orREJECTION ). C OMPLIMENTS (\u201cNice jacket!\u201d) often give rise to\nDOWNPLAYERS (\u201cOh, this old thing?\u201d). These pairs, called adjacency pairs are adjacency pair\ncomposed of a \ufb01rst pair part and a second pair part (Schegloff, 1968), and these\nexpectations can help systems decide what actions to take.\nHowever, dialogue acts aren\u2019t always followed immediately by their second pair\npart. The two parts can be separated by a side sequence (Jefferson 1972) or sub- side sequence\ndialogue . For example utterances C 17to A 20constitute a correction subdialogue subdialogue\n(Litman 1985, Litman and Allen 1987, Chu-Carroll and Carberry 1998):\nC17: #Act. . . actually#, what day of the week is the 15th?\nA18: It\u2019s a Friday.\nC19: Uh hmm. I would consider staying there an extra day til Sunday.\nA20: OK. . . OK. On Sunday I have . . .\nThe question in C 17interrupts the prior discourse, in which the agent was looking\nfor a May 15 return \ufb02ight. The agent must answer the question and also realize that\n\u2018\u2019I would consider staying...til Sunday\u201d means that the client would probably like to\nchange their plan, and now go back to \ufb01nding return \ufb02ights, but for the 17th.\nAnother side sequence is the clari\ufb01cation question , which can form a subdia-\nlogue between a REQUEST and a RESPONSE . This is especially common in dialogue\nsystems where speech recognition errors causes the system to have to ask for clari-\n\ufb01cations or repetitions like the following:\nUser: What do you have going to UNKNOWN WORD on the 5th?\nSystem: Let\u2019s see, going where on the 5th?\nUser: Going to Hong Kong.\nSystem: OK, here are some \ufb02ights...\nIn addition to side-sequences, questions often have presequences , like the fol- presequence\nlowing example where a user starts with a question about the system\u2019s capabilities\n(\u201cCan you make train reservations\u201d) before making a request.\nUser: Can you make train reservations?\nSystem: Yes I can.\nUser: Great, I\u2019d like to reserve a seat on the 4pm train to New York.\nInitiative\nSometimes a conversation is completely controlled by one participant. For exam-\nple a reporter interviewing a chef might ask questions, and the chef responds. We\nsay that the reporter in this case has the conversational initiative (Carbonell, 1970; initiative\nNickerson, 1976). In normal human-human dialogue, however, it\u2019s more common\nfor initiative to shift back and forth between the participants, as they sometimes\nanswer questions, sometimes ask them, sometimes take the conversations in new di-\nrections, sometimes not. You may ask me a question, and then I respond asking you",
    "metadata": {
      "source": "15",
      "chunk_id": 6,
      "token_count": 788,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\nto clarify something you said, which leads the conversation in all sorts of ways. We\ncall such interactions mixed initiative (Carbonell, 1970).\nFull mixed initiative, while the norm for human-human conversations, can be\ndif\ufb01cult for dialogue systems. The most primitive dialogue systems tend to use\nsystem-initiative , where the system asks a question and the user can\u2019t do anything\nuntil they answer it, or user-initiative like simple search engines, where the user\nspeci\ufb01es a query and the system passively responds. Even modern large language\nmodel-based dialogue systems, which come much closer to using full mixed initia-\ntive, often don\u2019t have completely natural initiative switching. Getting this right is an\nimportant goal for modern systems.\nInference and Implicature\nInference is also important in dialogue understanding. Consider the client\u2019s response\nC2, repeated here:\nA2: And, what day in May did you want to travel?\nC3: OK uh I need to be there for a meeting that\u2019s from the 12th to the 15th.\nNotice that the client does not in fact answer the agent\u2019s question. The client\nmerely mentions a meeting at a certain time. What is it that licenses the agent to\ninfer that the client is mentioning this meeting so as to inform the agent of the travel\ndates?\nThe speaker seems to expect the hearer to draw certain inferences; in other\nwords, the speaker is communicating more information than seems to be present\nin the uttered words. This kind of example was pointed out by Grice (1975, 1978)\nas part of his theory of conversational implicature .Implicature means a particu- implicature\nlar class of licensed inferences. Grice proposed that what enables hearers to draw\nthese inferences is that conversation is guided by a set of maxims , general heuristics\nthat play a guiding role in the interpretation of conversational utterances. One such\nmaxim is the maxim of relevance which says that speakers attempt to be relevant, relevance\nthey don\u2019t just utter random speech acts. When the client mentions a meeting on the\n12th, the agent reasons \u2018There must be some relevance for mentioning this meeting.\nWhat could it be?\u2019. The agent knows that one precondition for having a meeting\n(at least before Web conferencing) is being at the place where the meeting is held,\nand therefore that maybe the meeting is a reason for the travel, and if so, then since\npeople like to arrive the day before a meeting, the agent should infer that the \ufb02ight\nshould be on the 11th.\nThese subtle characteristics of human conversations ( turns ,speech acts ,ground-\ning,dialogue structure ,initiative , and implicature ) are among the reasons it is dif-\n\ufb01cult to build dialogue systems that can carry on natural conversations with humans.\nMany of these challenges are active areas of dialogue systems research.\n15.2 Frame-Based Dialogue Systems\nAtask-based dialogue system has the goal of helping a user solve a speci\ufb01c task\nlike making a travel reservation or buying a product. Task-based dialogue systems\nare based around frames , \ufb01rst introduced in the early in\ufb02uential GUS system for frame\nGUS travel planning (Bobrow et al., 1977). Frames are knowledge structures representing\nthe details of the user\u2019s task speci\ufb01cation. Each frame consists of a collection of\nslots , each of which can take a set of possible values . Together a set of frames is slot",
    "metadata": {
      "source": "15",
      "chunk_id": 7,
      "token_count": 767,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\n15.2 \u2022 F RAME -BASED DIALOGUE SYSTEMS 7\nsometimes called a domain ontology .\nHere we\u2019ll describe the most well-studied frame-based architecture, the dialogue-\nstate architecture, made up of the six components shown in Fig. 15.3. In the next\nsections we\u2019ll introduce four of them, after introducing the idea of frames (deferring\nthe speech recognition and synthesis components to Chapter 16).\nFigure 15.3 Architecture of a dialogue-state system for task-oriented dialogue from Williams et al. (2016).\n15.2.1 Frames and Slot Filling\nThe frame and its slots in a task-based dialogue system specify what the system\nneeds to know to perform its task. A hotel reservation system needs dates and loca-\ntions. An alarm clock system needs a time. The system\u2019s goal is to \ufb01ll the slots in\nthe frame with the \ufb01llers the user intends, and then perform the relevant action for\nthe user (answering a question, or booking a \ufb02ight).\nFig. 15.4 shows a sample frame for booking air travel, with some sample ques-\ntions used for \ufb01lling slots. In the simplest frame-based systems (including most com-\nmercial assistants until quite recently), these questions are pre-written templates, but\nin more sophisticated systems, questions are generated on-the-\ufb02y. The slot \ufb01llers are\noften constrained to a particular semantic type, like type CITY (taking on values like\nSan Francisco , orHong Kong ) or DATE ,AIRLINE , or TIME .\nSlot Type Example Question\nORIGIN CITY city \u201cFrom what city are you leaving?\u201d\nDESTINATION CITY city \u201cWhere are you going?\u201d\nDEPARTURE TIME time \u201cWhen would you like to leave?\u201d\nDEPARTURE DATE date \u201cWhat day would you like to leave?\u201d\nARRIV AL TIME time \u201cWhen do you want to arrive?\u201d\nARRIV AL DATE date \u201cWhat day would you like to arrive?\u201d\nFigure 15.4 A frame in a frame-based dialogue system, showing the type of each slot and\na sample question used to \ufb01ll the slot.",
    "metadata": {
      "source": "15",
      "chunk_id": 8,
      "token_count": 460,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\nMany domains require multiple frames. Besides frames for car or hotel reser-\nvations, we might need other frames for things like general route information (for\nquestions like Which airlines \ufb02y from Boston to San Francisco? ), That means the\nsystem must be able to disambiguate which slot of which frame a given input is\nsupposed to \ufb01ll.\nThe task of slot-\ufb01lling is usually combined with two other tasks, to extract 3\nthings from each user utterance. The \ufb01rst is domain classi\ufb01cation : is this user for\nexample talking about airlines, programming an alarm clock, or dealing with their\ncalendar? The second is user intent determination : what general task or goal is theintent\ndetermination\nuser trying to accomplish? For example the task could be to Find a Movie, or Show\na Flight, or Remove a Calendar Appointment. Together, the domain classi\ufb01cation\nand intent determination tasks decide which frame we are \ufb01lling. Finally, we need\nto do slot \ufb01lling itself: extract the particular slots and \ufb01llers that the user intends the slot \ufb01lling\nsystem to understand from their utterance with respect to their intent. From a user\nutterance like this one:\nShow me morning flights from Boston to San Francisco on Tuesday\na system might want to build a representation like:\nDOMAIN: AIR-TRAVEL INTENT: SHOW-FLIGHTS\nORIGIN-CITY: Boston DEST-CITY: San Francisco\nORIGIN-DATE: Tuesday ORIGIN-TIME: morning\nSimilarly an utterance like this: should give an intent like this:\nWake me tomorrow at 6 DOMAIN: ALARM-CLOCK\nINTENT: SET-ALARM\nTIME: 2017-07-01 0600\nThe simplest dialogue systems use handwritten rules for slot-\ufb01lling, like this\nregular expression for recognizing the SET-ALARM intent:\nwake me (up) | set (the|an) alarm | get me up\nBut most systems use supervised machine-learning: each sentence in a training\nset is annotated with slots, domain, and intent, and a sequence model maps from\ninput words to slot \ufb01llers, domain and intent. For example we\u2019ll have pairs of sen-\ntences that are labeled for domain ( AIRLINE ) and intent ( SHOWFLIGHT ), and are also\nlabeled with BIO representations for the slots and \ufb01llers. (Recall from Chapter 17\nthat in BIO tagging we introduce a tag for the beginning (B) and inside (I) of each\nslot label, and one for tokens outside (O) any slot label.)\nO O O O O B-DES I-DES O B-DEPTIME I-DEPTIME O AIRLINE-SHOWFLIGHT\nI want to fly to San Francisco on Monday afternoon please EOS\nFig. 15.5 shows a typical architecture for inference. The input words w1:::wn\nare passed through a pretrained language model encoder, followed by a feedforward\nlayer and a softmax at each token position over possible BIO tags, with the output\na series of BIO tags s1:::sn. We generally combine the domain-classi\ufb01cation and\nintent-extraction tasks with slot-\ufb01lling by adding a domain concatenated with an\nintent as the desired output for the \ufb01nal EOS token.\nOnce the sequence labeler has tagged the user utterance, a \ufb01ller string can be ex-\ntracted for each slot from the tags (e.g., \u201cSan Francisco\u201d), and these word strings\ncan then be normalized to the correct form in the ontology (perhaps the airport",
    "metadata": {
      "source": "15",
      "chunk_id": 9,
      "token_count": 794,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\n15.3 \u2022 D IALOGUE ACTS AND DIALOGUE STATE 9\nSanFranciscoonMondayEncodingsClassi\ufb01er+softmaxB-DESI-DESOB-DTIME\n\u2026d+i\n<EOS>Encoder \nFigure 15.5 Slot \ufb01lling by passing input words through an encoder, and then using a linear\nor feedforward layer followed by a softmax to generate a series of BIO tags. Here we also\nshow a \ufb01nal state: a domain concatenated with an intent.\ncode \u2018SFO\u2019), for example with dictionaries that specify that SF, SFO, and San Fran-\ncisco are synonyms. Often in industrial contexts, combinations of rules and machine\nlearning are used for each of these components.\nWe can make a very simple frame-based dialogue system by wrapping a small\namount of code around this slot extractor. Mainly we just need to ask the user\nquestions until all the slots are full, do a database query, then report back to the user,\nusing hand-built templates for generating sentences.\n15.2.2 Evaluating Task-Based Dialogue\nWe evaluate task-based systems by computing the task error rate , ortask success task error rate\nrate: the percentage of times the system booked the right plane \ufb02ight, or put the\nright event on the calendar. A more \ufb01ne-grained, but less extrinsic metric is the slot\nerror rate , the percentage of slots \ufb01lled with the correct values: slot error rate\nSlot Error Rate for a Sentence =# of inserted/deleted/subsituted slots\n# of total reference slots for sentence(15.1)\nFor example a system that extracted the slot structure below from this sentence:\n(15.2) Make an appointment with Chris at 10:30 in Gates 104\nSlot Filler\nPERSON Chris\nTIME 11:30 a.m.\nROOM Gates 104\nhas a slot error rate of 1/3, since the TIME is wrong. Instead of error rate, slot\nprecision, recall, and F-score can also be used. We can also measure ef\ufb01ciency\ncosts like the length of the dialogue in seconds or turns. ef\ufb01ciency costs\n15.3 Dialogue Acts and Dialogue State\nWhile the naive slot-extractor system described above can handle simple dialogues,\noften we want more complex interactions. For example, we might want to con\ufb01rm\nthat we\u2019ve understand the user, or ask them to repeat themselves. We can build a\nmore sophisticated system using dialogue acts anddialogue state .",
    "metadata": {
      "source": "15",
      "chunk_id": 10,
      "token_count": 540,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10",
    "metadata": {
      "source": "15",
      "chunk_id": 11,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "10 CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\n15.3.1 Dialogue Acts\nDialogue acts are a generalization of speech acts that also represent grounding. The dialogue acts\nset of acts can be general, or can be designed for particular dialogue tasks.\nTag Sys User Description\nHELLO (a=x;b=y;:::)X X Open a dialogue and give info a=x;b=y;:::\nINFORM (a=x;b=y;:::)X X Give info a=x;b=y;:::\nREQUEST (a;b=x;:::)X X Request value for a given b=x;:::\nREQALTS (a=x;:::) cX Request alternative with a=x;:::\nCONFIRM (a=x;b=y;:::)X X Explicitly con\ufb01rm a=x;b=y;:::\nCONFREQ (a=x;:::;d)Xc Implicitly con\ufb01rm a=x;:::and request value of d\nSELECT (a=x;a=y)Xc Implicitly con\ufb01rm a=x;:::and request value of d\nAFFIRM (a=x;b=y;:::)X X Af\ufb01rm and give further info a=x;b=y;:::\nNEGATE (a=x) cX Negate and give corrected value a=x\nDENY (a=x) cX Deny that a=x\nBYE() X X Close a dialogue\nFigure 15.6 Dialogue acts used by the HIS restaurant recommendation system of Young\net al. (2010). The Sys and User columns indicate which acts are valid as system outputs and\nuser inputs, respectively.\nFigure 15.6 shows a tagset for a restaurant recommendation system, and Fig. 15.7\nshows these tags labeling a sample dialogue from the HIS system (Young et al.,\n2010). This example also shows the content of each dialogue act, which are the slot\n\ufb01llers being communicated. So the user might INFORM the system that they want\nItalian food near a museum, or CONFIRM with the system that the price is reasonable.\nUtterance Dialogue act\nU: Hi, I am looking for somewhere to eat. hello(task = find,type=restaurant)\nS: You are looking for a restaurant. What\ntype of food do you like?confreq(type = restaurant, food)\nU: I\u2019d like an Italian near the museum. inform(food = Italian, near=museum)\nS: Roma is a nice Italian restaurant near\nthe museum.inform(name = \"Roma\", type = restaurant,\nfood = Italian, near = museum)\nU: Is it reasonably priced? confirm(pricerange = moderate)\nS: Yes, Roma is in the moderate price\nrange.affirm(name = \"Roma\", pricerange =\nmoderate)\nU: What is the phone number? request(phone)\nS: The number of Roma is 385456. inform(name = \"Roma\", phone = \"385456\")\nU: Ok, thank you goodbye. bye()\nFigure 15.7 A dialogue from the HIS System of Young et al. (2010) using the dialogue acts in Fig. 15.6.\n15.3.2 Dialogue State Tracking\nThe job of the dialogue-state tracker is to determine the current state of the frame\n(the \ufb01llers of each slot), and the user\u2019s most recent dialogue act. The dialogue-state\nis not just the slot-\ufb01llers in the current sentence; it includes the entire state of the\nframe at this point, summarizing all of the user\u2019s constraints. Fig. 15.8 from Mrk \u02c7si\u00b4c",
    "metadata": {
      "source": "15",
      "chunk_id": 12,
      "token_count": 761,
      "chapter_title": ""
    }
  },
  {
    "content": "type of food do you like?confreq(type = restaurant, food)\nU: I\u2019d like an Italian near the museum. inform(food = Italian, near=museum)\nS: Roma is a nice Italian restaurant near\nthe museum.inform(name = \"Roma\", type = restaurant,\nfood = Italian, near = museum)\nU: Is it reasonably priced? confirm(pricerange = moderate)\nS: Yes, Roma is in the moderate price\nrange.affirm(name = \"Roma\", pricerange =\nmoderate)\nU: What is the phone number? request(phone)\nS: The number of Roma is 385456. inform(name = \"Roma\", phone = \"385456\")\nU: Ok, thank you goodbye. bye()\nFigure 15.7 A dialogue from the HIS System of Young et al. (2010) using the dialogue acts in Fig. 15.6.\n15.3.2 Dialogue State Tracking\nThe job of the dialogue-state tracker is to determine the current state of the frame\n(the \ufb01llers of each slot), and the user\u2019s most recent dialogue act. The dialogue-state\nis not just the slot-\ufb01llers in the current sentence; it includes the entire state of the\nframe at this point, summarizing all of the user\u2019s constraints. Fig. 15.8 from Mrk \u02c7si\u00b4c\net al. (2017) shows the dialogue state after each turn.\nDialogue act detection is done just like domain or intent classi\ufb01cation, by passing\nthe input sentence through an encoder and adding an act classi\ufb01er. Often passing in\nthe prior dialogue act as well can improve classi\ufb01cation. And since dialogue acts",
    "metadata": {
      "source": "15",
      "chunk_id": 13,
      "token_count": 359,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11",
    "metadata": {
      "source": "15",
      "chunk_id": 14,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "15.3 \u2022 D IALOGUE ACTS AND DIALOGUE STATE 11\nUser: I\u2019m looking for a cheaper restaurant\ninform(price=cheap)\nSystem: Sure. What kind - and where?\nUser: Thai food, somewhere downtown\ninform(price=cheap, food=Thai, area=centre)\nSystem: The House serves cheap Thai food\nUser: Where is it?\ninform(price=cheap, food=Thai, area=centre); request(address)\nSystem: The House is at 106 Regent Street\nFigure 15.8 The output of the dialogue state tracker after each turn (Mrk \u02c7si\u00b4c et al., 2017).\nplace some constraints on the slots and values, the tasks of dialogue-act detection and\nslot-\ufb01lling are often performed jointly. The state tracker can just take the output of\na slot-\ufb01lling sequence-model (Section 15.2.1) after each sentence, or do something\nmore complicated like training a classi\ufb01er to decide if a value has been changed.\nA special case: detecting correction acts. If a dialogue system misrecognizes\nor misunderstands an utterance, users will repeat or reformulate the utterance. De-\ntecting these user correction acts is quite important, especially for spoken lan-user correction\nacts\nguage. Ironically, corrections are actually harder to recognize than normal sentences\n(Swerts et al., 2000), because users who are frustrated adjust their speech in a way\nthat is dif\ufb01cult for speech recognizers (Goldberg et al., 2003). For example speak-\ners often use a prosodic style for corrections called hyperarticulation , in which thehyperarticula-\ntion\nutterance is louder or longer or exaggerated in pitch, such as I said BAL-TI-MORE,\nnot Boston (Wade et al. 1992, Levow 1998, Hirschberg et al. 2001). Detecting acts\ncan be part of the general dialogue act detection classi\ufb01er, or can make use of spe-\ncial features beyond the words, like those shown below (Levow 1998, Litman et al.\n1999, Hirschberg et al. 2001, Bulyko et al. 2005, Awadallah et al. 2015).\nfeatures examples\nsemantic embedding similarity between correction and user\u2019s prior utterance\nphonetic phonetic overlap between candidate correction act and user\u2019s prior utterance\n(i.e. \u201cWhatsApp\u201d may be incorrectly recognized as \u201cWhat\u2019s up\u201d)\nprosodic hyperarticulation, increases in F0 range, pause duration, and word duration\nASR ASR con\ufb01dence, language model probability\n15.3.3 Dialogue Policy: Which act to generate\nIn early commercial frame-based systems, the dialogue policy is simple: ask ques-\ntions until all the slots are full, do a database query, then report back to the user. A\nmore sophisticated dialogue policy can help a system decide when to answer the dialogue policy\nuser\u2019s questions, when to instead ask the user a clari\ufb01cation question, and so on. A\ndialogue policy thus decides what dialogue act to generate. Choosing a dialogue act\nto generate, along with its arguments, is sometimes called content planning .content\nplanning\nLet\u2019s see how to do this for some important dialogue acts. Dialogue systems, es-\npecially speech systems, often misrecognize the users\u2019 words or meaning. To ensure\nsystem and user share a common ground, systems must con\ufb01rm understandings with\nthe user or reject utterances that the system don\u2019t understand. A system might use",
    "metadata": {
      "source": "15",
      "chunk_id": 15,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "features examples\nsemantic embedding similarity between correction and user\u2019s prior utterance\nphonetic phonetic overlap between candidate correction act and user\u2019s prior utterance\n(i.e. \u201cWhatsApp\u201d may be incorrectly recognized as \u201cWhat\u2019s up\u201d)\nprosodic hyperarticulation, increases in F0 range, pause duration, and word duration\nASR ASR con\ufb01dence, language model probability\n15.3.3 Dialogue Policy: Which act to generate\nIn early commercial frame-based systems, the dialogue policy is simple: ask ques-\ntions until all the slots are full, do a database query, then report back to the user. A\nmore sophisticated dialogue policy can help a system decide when to answer the dialogue policy\nuser\u2019s questions, when to instead ask the user a clari\ufb01cation question, and so on. A\ndialogue policy thus decides what dialogue act to generate. Choosing a dialogue act\nto generate, along with its arguments, is sometimes called content planning .content\nplanning\nLet\u2019s see how to do this for some important dialogue acts. Dialogue systems, es-\npecially speech systems, often misrecognize the users\u2019 words or meaning. To ensure\nsystem and user share a common ground, systems must con\ufb01rm understandings with\nthe user or reject utterances that the system don\u2019t understand. A system might use\nanexplicit con\ufb01rmation act to con\ufb01rm with the user, like Is that correct? below:explicit\ncon\ufb01rmation",
    "metadata": {
      "source": "15",
      "chunk_id": 16,
      "token_count": 312,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12",
    "metadata": {
      "source": "15",
      "chunk_id": 17,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "12 CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\nU:I\u2019d like to \ufb02y from Denver Colorado to New York City on September\ntwenty \ufb01rst in the morning on United Airlines\nS:Let\u2019s see then. I have you going from Denver Colorado to New York\non September twenty \ufb01rst. Is that correct?\nWhen using an implicit con\ufb01rmation act, a system instead grounds more im-implicit\ncon\ufb01rmation\nplicitly, for example by repeating the system\u2019s understanding as part of asking the\nnext question, as Shanghai is con\ufb01rmed in passing in this example:\nU:I want to travel to to Shanghai\nS:When do you want to travel to Shanghai?\nThere\u2019s a tradeoff. Explicit con\ufb01rmation makes it easier for users to correct\nmisrecognitions by just answering \u201cno\u201d to the con\ufb01rmation question. But explicit\ncon\ufb01rmation is time-consuming and awkward (Danieli and Gerbino 1995, Walker\net al. 1998). We also might want an act that expresses lack of understanding: rejec-\ntion, for example with a prompt like I\u2019m sorry, I didn\u2019t understand that . To decide rejection\namong these acts, we can make use of the fact that ASR systems often compute\ntheir con\ufb01dence in their transcription (often based on the log-likelihood the system\nassigns the sentence). A system can thus choose to explicitly con\ufb01rm only low-\ncon\ufb01dence sentences. Or systems might have a four-tiered level of con\ufb01dence with\nthree thresholds a,b, and g:\n<alow con\ufb01dence reject\n\u0015aabove the threshold con\ufb01rm explicitly\n\u0015bhigh con\ufb01dence con\ufb01rm implictly\n\u0015gvery high con\ufb01dence don\u2019t con\ufb01rm at all\n15.3.4 Natural language generation: Sentence Realization\nrecommend(restaurant name= Au Midi, neighborhood = midtown,\ncuisine = french)\n1 Au Midi is in Midtown and serves French food.\n2 There is a French restaurant in Midtown called Au Midi.\nFigure 15.9 Sample inputs to the sentence realization phase of NLG, showing the dialogue\nact and attributes prespeci\ufb01ed by the content planner, and two distinct potential output sen-\ntences to be generated. From the restaurant recommendation system of Nayak et al. (2017).\nOnce a dialogue act has been chosen, we need to generate the text of the re-\nsponse to the user. This part of the generation process is called sentence realiza-\ntion. Fig. 15.9 shows a sample input/output for the sentence realization phase. Thesentence\nrealization\ncontent planner has chosen the dialogue act RECOMMEND and some slots (name,\nneighborhood, cuisine) and \ufb01llers. The sentence realizer generates a sentence like\nlines 1 or 2 (by training on examples of representation/sentence pairs from a corpus\nof labeled dialogues). Because we won\u2019t see every restaurant or attribute in every\npossible wording, we can delexicalize : generalize the training examples by replac- delexicalize\ning speci\ufb01c slot value words in the training set with a generic placeholder token\nrepresenting the slot. Fig. 15.10 shows the sentences in Fig. 15.9 delexicalized.\nWe can map from frames to delexicalized sentences with an encoder decoder\nmodel (Mrk \u02c7si\u00b4c et al. 2017, inter alia), trained on hand-labeled dialogue corpora like",
    "metadata": {
      "source": "15",
      "chunk_id": 18,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "act and attributes prespeci\ufb01ed by the content planner, and two distinct potential output sen-\ntences to be generated. From the restaurant recommendation system of Nayak et al. (2017).\nOnce a dialogue act has been chosen, we need to generate the text of the re-\nsponse to the user. This part of the generation process is called sentence realiza-\ntion. Fig. 15.9 shows a sample input/output for the sentence realization phase. Thesentence\nrealization\ncontent planner has chosen the dialogue act RECOMMEND and some slots (name,\nneighborhood, cuisine) and \ufb01llers. The sentence realizer generates a sentence like\nlines 1 or 2 (by training on examples of representation/sentence pairs from a corpus\nof labeled dialogues). Because we won\u2019t see every restaurant or attribute in every\npossible wording, we can delexicalize : generalize the training examples by replac- delexicalize\ning speci\ufb01c slot value words in the training set with a generic placeholder token\nrepresenting the slot. Fig. 15.10 shows the sentences in Fig. 15.9 delexicalized.\nWe can map from frames to delexicalized sentences with an encoder decoder\nmodel (Mrk \u02c7si\u00b4c et al. 2017, inter alia), trained on hand-labeled dialogue corpora like\nMultiWOZ (Budzianowski et al., 2018). The input to the encoder is a sequence of",
    "metadata": {
      "source": "15",
      "chunk_id": 19,
      "token_count": 312,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\n15.4 \u2022 C HATBOTS 13\nrecommend(restaurant name= Au Midi, neighborhood = midtown,\ncuisine = french)\n1restaurant name is inneighborhood and serves cuisine food.\n2 There is a cuisine restaurant in neighborhood called restaurant name .\nFigure 15.10 Delexicalized sentences that can be used for generating many different relex-\nicalized sentences. From the restaurant recommendation system of Nayak et al. (2017).\ndecentservice:RECOMMENDcuisine:null[name]hasdecentserviceENCODERDECODER\nFigure 15.11 An encoder decoder sentence realizer mapping slots/\ufb01llers to English.\ntokens xtthat represent the dialogue act (e.g., RECOMMEND ) and its arguments (e.g.,\nservice:decent, cuisine:null ) (Nayak et al., 2017), as in Fig. 15.11.\nThe decoder outputs the delexicalized English sentence \u201c name has decent ser-\nvice\u201d, which we can then relexicalize , i.e. \ufb01ll back in correct slot values, resulting relexicalize\nin \u201cAu Midi has decent service\u201d.\n15.4 Chatbots\nChatbots are systems that can carry on extended conversations with the goal of chatbot\nmimicking the unstructured conversations or \u2018chats\u2019 characteristic of informal human-\nhuman interaction. While early systems like ELIZA (Weizenbaum, 1966) or PARRY\n(Colby et al., 1971) had theoretical goals like testing theories of psychological coun-\nseling, for most of the last 50 years chatbots have been designed for entertainment.\nThat changed with the recent rise of neural chatbots like ChatGPT, which incor-\nporate solutions to NLP tasks like question answering, writing tools, or machine\ntranslation into a conversational interface. A conversation with ChatGPT is shown\nin Fig. 15.12. In this section we describe neural chatbot architectures and datasets.\n[TBD]\nFigure 15.12 A conversation with ChatGPT.\n15.4.1 Training chatbots\nData Chatbots are generally trained on a training set that includes standard large\nlanguage model training data of the type discussed in Section ??: versions of the\nweb from the Common Crawl, including news sites, Wikipedia, as well as books.\nFor training chatbots, it is common to additionally add lots of dialogue data.\nThis can include datasets created speci\ufb01cally for training chatbots by hiring\nspeakers of the language to have conversations, such as by having them take on\npersonas or talk about knowledge provided to them. For example the Topical-Chat\ndataset has 11K crowdsourced conversations spanning 8 broad topics (Gopalakrish-\nnan et al., 2019), the E MPATHETIC DIALOGUES includes 25K crowdsourced con-",
    "metadata": {
      "source": "15",
      "chunk_id": 20,
      "token_count": 601,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n14 CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\nversations grounded in a speci\ufb01c situation where a speaker was feeling a speci\ufb01c\nemotion (Rashkin et al., 2019), and the SaFeRDialogues dataset (Ung et al., 2022)\nhas 8k dialogues demonstrating graceful responses to conversational feedback about\nsafety failures.\nSuch datasets are far too small to train a language model alone, and so it\u2019s com-\nmon to also pretrain on large datasets of pseudo-conversations drawn from Twitter\n(Ritter et al., 2010), Reddit (Roller et al., 2021), Weibo ( \u5fae\u535a), and other social\nmedia platforms. To turn social media data into data that has the structure of a con-\nversation, we can treat any post on the platform as the \ufb01rst turn in a conversation,\nand the sequence of comments/replies as subsequent turns in that conversation.\nDatasets from the web can be enormously toxic, so it\u2019s crucial to \ufb01lter the di-\nalogues \ufb01rst. This can be done by using the same toxicity classi\ufb01ers we describe\nbelow in the \ufb01ne-tuning section.\nArchitecture For training chatbots, it\u2019s most common to use the standard causal\nlanguage model architecture, in which the model predicts each word given all the\nprior words, and the loss is the standard language modeling loss. Fig. 15.13 shows a\nstandard training setup; no different than language model training in Chapter 9. The\nonly difference is the data, which has the addition of signi\ufb01cant conversation and\npseudo-conversation data as described in the prior section. As usual, the left context\ncan include the entire prior conversation (or as much as \ufb01ts in the context window).\nTransformerBlocksLM head\ngotpromoted!<s>gotpromoted!<s>Next wordCongratsLM Loss\u2026LM headLM headLM headLM headLM head\nICongrats!\u2026LM headLM head!-log y!-log yCongrats-log y<s>-log y!-log ypromoted-log ygot\u2026\u2026\u2026\u2026\u2026\u2026\nFigure 15.13 Training a causal (decoder-only) language model for a chatbot.\nAn alternative is to use the encoder-decoder architecture of Chapter 13. In this\ncase the entire conversation up to the last turn (as much as \ufb01ts in the context) is\npresented to the encoder, and the decoder generates the next turn.\npromotedgot!<s>Congrats!ENCODERDECODERI\nFigure 15.14 An alternative: an encoder-decoder language model for a chatbot.\nIn practice, dialogue systems require additional customization beyond just pre-\ntraining on dialogue data. In the next few sections we\u2019ll discuss various stages of",
    "metadata": {
      "source": "15",
      "chunk_id": 21,
      "token_count": 604,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\n15.4 \u2022 C HATBOTS 15\n\ufb01ne-tuning that can be used for this customization.\n15.4.2 Fine Tuning for Quality and Safety\nIt is a common practice for dialogue systems to use further labeled data for \ufb01ne-\ntuning. One function of this \ufb01ne-tuning step is to improve the quality of the dialogue,\ntraining the system to produce responses that are sensible and interesting. Another\nfunction might be to improve safety, keeping a dialogue system from suggesting\nharmful actions (like \ufb01nancial fraud, medical harm, inciting hatred, or abusing the\nuser or other people).\nIn the simplest method for improving quality and safety, speakers of the lan-\nguage are given an initial prompt and instructions to have high-quality, safe dia-\nlogues. They then interact with an initial dialogue system and their responses are\nused to \ufb01netune the model, usually as part of the instruct tuning step we introduced\nin Chapter 12. Thus a dialogue system learns to answer questions, follow other\ninstructions, and also carry on high-quality, safe dialogues, in a single multi-task\nlearning format.\nWhile \ufb01ne-tuning on positive examples is helpful, it is generally insuf\ufb01cient and\nso it is common to add more discriminative data that speci\ufb01cally downweights low-\nquality or harmful responses. The simplest paradigm for this is to train a model to\npredict turn-level safety and quality values, by training on human-labeled ratings.\nSuch ratings might be collected by \ufb01rst having speakers of the language carry on\ndialogues with a system, and then a second set of people act as labelers to label\nevery system turn for its quality and safety, resulting in a binary label for quality and\nsafety for each turn.\nOnce a dataset has been created with these labels, a language model can be used\nin a classi\ufb01cation task to label the quality and safety of a turn. For example in the\nLamda system (Cohen et al., 2022), a single language model is used in two phases,\nroughly corresponding to generative and discriminative tasks: \ufb01rst generating a re-\nsponse, and then generating a label. In the generative phase, the model is given the\nprior turn and a special RESPONSE token and generates the blue response turn. (In\ntraining, the training loss is given only for the blue response):\n\u201cWhat\u2019s up? RESPONSE Not much.\nIn a second, discriminative phase, the model is \ufb01ne-tuned to see an attribute\n(SENSIBLE, INTERESTING, UNSAFE) and then to predict a 0 or 1 value, again\nwith training losses given only for the blue value.\nWhat\u2019s up? RESPONSE Not much. SENSIBLE 1\nWhat\u2019s up? RESPONSE Not much. INTERESTING 0\nWhat\u2019s up? RESPONSE Not much. UNSAFE 0\nTo use the system in inference, the model \ufb01rst generates a response given the context,\nand then it is given the attribute and asked to generate a rating. The result is a\ngenerated turn along with a label. This label isn\u2019t shown to the user but can be use\nfor \ufb01ltering, either at training time or at deployment time. For example, the system\ncan generate multiple potential responses, \ufb01lter out any response that is unsafe, and\nreturn to the user the highest ranking response.",
    "metadata": {
      "source": "15",
      "chunk_id": 22,
      "token_count": 728,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\n16 CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\n15.4.3 Learning to perform retrieval as part of responding\nModern chatbots are also trained to do retrieval (calls to Google or Bing or other\nsearch engines) as a component of generating a response. This is generally done\nby having a sort of fake dialogue participant called something like Search Query\nwhose utterances are passed to a search engine. The system can be encourged to do\nsearches either by prompting or by \ufb01ne-tuning.\nFor example in the Sparrow chatbot (Glaese et al., 2022), the prompt includes\na few sample conversations that include two special users, Search Query , and\nSearch Results :\n...[this is preceded by other parts of the prompt]\nUser: What color is the sky?\nSparrow: Blue.\nUser: Why?\nSearch Query: why is sky blue\nSearch Results: Page title: Sky Except for direct sunlight, most of the light in\nthe daytime sky is caused by scattering, which is dominated by\na small-particle limit called Rayleigh scattering. The scattering\ndue to molecule-sized particles (as in air) is greater in the di-\nrections both toward and away from the source of light than it\nis in directions perpendicular to the incident path. Scattering is\nsigni\ufb01cant for light at all visible wavelengths, but is stronger at\nthe shorter (bluer) end of the visible spectrum, meaning that the\nscattered light is bluer than its source: the Sun. The remain-\ning direct sunlight, having lost some of its shorter-wavelength\ncomponents, appears slightly less blue.\nSparrow: Because of the Rayleigh scattering. It causes short waves of\nblue light to be scattered more than the other colours in the spec-\ntrum.\nUser: Who was Raleigh?\nSearch Query: who was Rayleigh\n... ...\nFrom these prompts, the system learns to generate texts with Search Query\nturns for fact-based questions from the user, and these are passed to a search engine\nto generate the Search Results turns.\nAlternatively, systems can be \ufb01netuned to to know when to use a search en-\ngine. For example, labelers can interact with a system, fact check each of the re-\nsponses, and whenever the system emits an incorrect response, perform the web\nsearch queries that the system should have used to check its answer, and then the in-\nteration is recorded and used for \ufb01ne-tuning. Or labelers can look at a transcript of a\nlanguage model carrying on a dialogue, and similarly mark every place where a fact\nwas wrong (or out-of-date) and write the set of search queries that would have been\nappropriate. A system is then \ufb01ne-tuned to generate search query turns which\nare again passed to a search engine to generate the search responses . The set\nof pages or snippets returned by the search engine in the search response turn are\nthen treated as the context for generation, similarly to the retrieval-based question-\nanswering methods of Chapter 14.",
    "metadata": {
      "source": "15",
      "chunk_id": 23,
      "token_count": 650,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\n15.5 \u2022 D IALOGUE SYSTEM DESIGN 17\n15.4.4 RLHF\nA more sophisticated family of methods uses reinforcement learning to learn to\nmatch human preferences for generated turns. In this method, RLHF for Rein- RLHF\nforcement Learning from Human Feedback, we give a system a dialogue context\nand sample two possible turns from the language model. We then have humans la-\nbel which of the two is better, creating a large dataset of sentence pairs with human\npreferences. These pairs are used to train a dialogue policy, and reinforcement learn-\ning is used to train the language model to generate turns that have higher rewards\n(Christiano et al., 2017; Ouyang et al., 2022). While using RLHF is the current state\nof the art at the time of this writing, a number of alternatives have been recently\ndeveloped that don\u2019t require reinforcement learning (Rafailov et al., 2023, e.g.,) and\nso this aspect of the \ufb01eld is changing very quickly.\n15.4.5 Evaluating Chatbots\nChatbots are evaluated by humans, who assign a score. This can be the human who\ntalked to the chatbot ( participant evaluation ) or a third party who reads a transcript\nof a human/chatbot conversation ( observer evaluation ). In the participant evalua-\ntion of See et al. (2019), the human evaluator chats with the model for six turns and\nrates the chatbot on 8 dimensions capturing conversational quality: avoiding repe-\ntition, interestingness, making sense, \ufb02uency, listening, inquisitiveness, humanness\nand engagingness on Likert scales like these:\nEngagingness How much did you enjoy talking to this user?\n\u000fNot at all \u000fA little \u000fSomewhat \u000fA lot\nMaking sense How often did this user say something which did NOT make sense?\n\u000fNever made any sense \u000fMost responses didn\u2019t make sense \u000fSome re-\nsponses didn\u2019t make sense \u000fEverything made perfect sense\nObserver evaluations use third party annotators to look at the text of a complete\nconversation. Sometimes we\u2019re interested in having raters assign a score to each\nsystem turn; for example (Artstein et al., 2009) have raters mark how coherent each\nturn is. Often, however, we just want a single high-level score to know if system A\nis better than system B. The acute-eval metric (Li et al., 2019) is such an observer acute-eval\nevaluation in which annotators look at two separate human-computer conversations\nand choose the system which performed better on four metrics: engagingness, inter-\nestingness, humanness, and knowledgability.\n15.5 Dialogue System Design\nBecause of the important role of the user, the \ufb01eld of dialogue systems is closely\nlinked with Human-Computer Interaction (HCI). This is especially true for task-\noriented dialogue and assistants, where the design of dialogue strategies, sometimes\ncalled voice user interface design, generally follows user-centered design princi-voice user\ninterface\nples (Gould and Lewis, 1985):\n1. Study the user and task: Understand the users and the task by interviewing\nusers, investigating similar systems, and studying related human-human dialogues.\n2. Build simulations and prototypes: A crucial tool in building dialogue systems\nis the Wizard-of-Oz system . In wizard systems, the users interact with what theyWizard-of-Oz\nsystem",
    "metadata": {
      "source": "15",
      "chunk_id": 24,
      "token_count": 741,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\n18 CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\nthink is a program but is in fact a human \u201cwizard\u201d disguised by a software interface\n(Gould et al. 1983, Good et al. 1984, Fraser and\nGilbert 1991). The name comes from the chil-\ndren\u2019s book The Wizard of Oz (Baum, 1900),\nin which the wizard turned out to be a simu-\nlation controlled by a man behind a curtain or\nscreen. A wizard system can be used to test out\nan architecture before implementation; only the\ninterface software and databases need to be in\nplace. The wizard gets input from the user, uses\na database interface to run queries based on the\nuser utterance, and then outputs sentences, ei-\nther by typing them or speaking them.\nWizard-of-Oz systems are not a perfect\nsimulation, since the wizard doesn\u2019t exactly\nsimulate the errors or limitations of a real sys-\ntem; but wizard studies can still provide a useful \ufb01rst idea of the domain issues.\n3. Iteratively test the design on users: An iterative design cycle with embedded\nuser testing is essential in system design (Nielsen 1992, Cole et al. 1997, Yankelovich\net al. 1995, Landauer 1995). For example in a well-known incident, an early dia-\nlogue system required the user to press a key to interrupt the system (Stifelman et al.,\n1993). But user testing showed users barged in (interrupted, talking over the sys- barged in\ntem), which led to a redesign of the system to recognize overlapped speech. It\u2019s also\nimportant to incorporate value sensitive design , in which we carefully consider dur-value sensitive\ndesign\ning the design process the bene\ufb01ts, harms and possible stakeholders of the resulting\nsystem (Friedman et al. 2017, Friedman and Hendry 2019).\n15.5.1 Ethical Issues in Dialogue System Design\nEthical issues have been key to how we think about designing arti\ufb01cial agents since\nwell before we had dialogue systems. Mary Shelley (depicted below) centered her\nnovel Frankenstein around the problem of creating arti\ufb01cial agents without consider-\ning\nethical and humanistic concerns. One issue is the\nsafety of users. If users seek information from di-\nalogue systems in safety-critical situations like ask-\ning medical advice, or in emergency situations, or\nwhen indicating the intentions of self-harm, incorrect\nadvice can be dangerous and even life-threatening.\nFor example (Bickmore et al., 2018) gave participants\nmedical problems to pose to three commercial di-\nalogue systems (Siri, Alexa, Google Assistant) and\nasked them to determine an action to take based on\nthe system responses; many of the proposed actions,\nif actually taken, would have led to harm or death.\nA system can also harm users by verbally attacking them, or creating represen-\ntational harms (Blodgett et al., 2020) by generating abusive or harmful stereotypes\nthat demean particular groups of people. Both abuse and stereotypes can cause psy-\nchological harm to users. Microsoft\u2019s 2016 Tay chatbot, for example, was taken Tay\nof\ufb02ine 16 hours after it went live, when it began posting messages with racial slurs,",
    "metadata": {
      "source": "15",
      "chunk_id": 25,
      "token_count": 729,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\n15.6 \u2022 S UMMARY 19\nconspiracy theories, and personal attacks on its users. Tay had learned these biases\nand actions from its training data, including from users who seemed to be purposely\nteaching the system to repeat this kind of language (Neff and Nagy 2016). Hender-\nson et al. (2017) examined dialogue datasets used to train corpus-based chatbots and\nfound toxic and abusive language, especially in social media corpora like Twitter\nand Reddit, and indeed such language then appears in the text generated by lan-\nguage models and dialogue systems (Gehman et al. 2020; Xu et al. 2020) which\ncan even amplify the bias from the training data (Dinan et al., 2020). Liu et al.\n(2020) developed another method for investigating bias, testing how neural dialogue\nsystems responded to pairs of simulated user turns that are identical except for men-\ntioning different genders or race. They found, for example, that simple changes like\nusing the word \u2018she\u2019 instead of \u2018he\u2019 in a sentence caused systems to respond more\noffensively and with more negative sentiment.\nAnother important ethical issue is privacy . Already in the \ufb01rst days of ELIZA,\nWeizenbaum pointed out the privacy implications of people\u2019s revelations to the chat-\nbot. The ubiquity of in-home dialogue systems means they may often overhear\nprivate information (Henderson et al., 2017). If a chatbot is human-like, users are\nalso more likely to disclose private information, and less likely to worry about the\nharm of this disclosure (Ischen et al., 2019). In general, chatbots that are trained\non transcripts of human-human or human-machine conversation must anonymize\npersonally identi\ufb01able information.\nFinally, chatbots raise important issues of gender equality in addition to textual\nbias. Current chatbots are overwhelmingly given female names, likely perpetuating\nthe stereotype of a subservient female servant (Paolino, 2017). And when users\nuse sexually harassing language, most commercial chatbots evade or give positive\nresponses rather than responding in clear negative ways (Fessler, 2017).\nThese ethical issues are an important area of investigation, including \ufb01nding\nways to mitigate problems of abuse and toxicity, like detecting and responding ap-\npropriately to toxic contexts (Wolf et al. 2017, Dinan et al. 2020, Xu et al. 2020).\nValue sensitive design, carefully considering possible harms in advance (Friedman\net al. 2017, Friedman and Hendry 2019) is also important; (Dinan et al., 2021) give\na number of suggestions for best practices in dialogue system design. For exam-\nple getting informed consent from participants, whether they are used for training,\nor whether they are interacting with a deployed system is important. Because di-\nalogue systems by de\ufb01nition involve human participants, researchers also work on\nthese issues with the Institutional Review Boards ( IRB) at their institutions, who IRB\nhelp protect the safety of experimental subjects.\n15.6 Summary\nChatbots anddialogue systems are crucial speech and language processing appli-\ncations that are already widely used commercially.\n\u2022 In human dialogue, speaking is a kind of action; these acts are referred to\nas speech acts or dialogue acts. Speakers also attempt to achieve common\nground by acknowledging that they have understand each other. Conversation\nalso is characterized by turn structure and dialogue structure.\n\u2022 Chatbots are conversational systems designed to mimic the appearance of in-\nformal human conversation. Rule-based chatbots like ELIZA and its modern",
    "metadata": {
      "source": "15",
      "chunk_id": 26,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\n20 CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\ndescendants use rules to map user sentences into system responses. Corpus-\nbased chatbots mine logs of human conversation to learn to automatically map\nuser sentences into system responses.\n\u2022 For task-based dialogue, most commercial dialogue systems use the GUS or\nframe-based architecture, in which the designer speci\ufb01es frames consisting of\nslots that the system must \ufb01ll by asking the user.\n\u2022 The dialogue-state architecture augments the GUS frame-and-slot architec-\nture with richer representations and more sophisticated algorithms for keeping\ntrack of user\u2019s dialogue acts, policies for generating its own dialogue acts, and\na natural language component.\n\u2022 Dialogue systems are a kind of human-computer interaction, and general HCI\nprinciples apply in their design, including the role of the user, simulations such\nas Wizard-of-Oz systems, and the importance of iterative design and testing\non real users.\nBibliographical and Historical Notes\nThe linguistic, philosophical, and psychological literature on dialogue is quite ex-\ntensive. For example the idea that utterances in a conversation are a kind of action\nbeing performed by the speaker was due originally to the philosopher Wittgenstein\n(1953) but worked out more fully by Austin (1962) and his student John Searle.\nVarious sets of speech acts have been de\ufb01ned over the years, and a rich linguistic\nand philosophical literature developed, especially focused on explaining the use of\nindirect speech acts. The idea of dialogue acts draws also from a number of other\nsources, including the ideas of adjacency pairs, pre-sequences, and other aspects of\nthe interactional properties of human conversation developed in the \ufb01eld of conver-\nsation analysis (see Levinson (1983) for an introduction to the \ufb01eld). This idea thatconversation\nanalysis\nacts set up strong local dialogue expectations was also pre\ufb01gured by Firth (1935, p.\n70), in a famous quotation:\nMost of the give-and-take of conversation in our everyday life is stereotyped\nand very narrowly conditioned by our particular type of culture. It is a sort\nof roughly prescribed social ritual, in which you generally say what the other\nfellow expects you, one way or the other, to say.\nAnother important research thread modeled dialogue as a kind of collaborative\nbehavior, including the ideas of common ground (Clark and Marshall, 1981), ref-\nerence as a collaborative process (Clark and Wilkes-Gibbs, 1986), joint intention\n(Levesque et al., 1990), and shared plans (Grosz and Sidner, 1980).\nThe earliest conversational systems were simple pattern-action chatbots like ELIZA\n(Weizenbaum, 1966). ELIZA had a widespread in\ufb02uence on popular perceptions of\narti\ufb01cial intelligence, and brought up some of the \ufb01rst ethical questions in natural\nlanguage processing \u2014such as the issues of privacy we discussed above as well the\nrole of algorithms in decision-making\u2014 leading its creator Joseph Weizenbaum to\n\ufb01ght for social responsibility in AI and computer science in general.\nComputational-implemented theories of dialogue blossomed in the 1970. That\nperiod saw the very in\ufb02uential GUS system (Bobrow et al., 1977), which in the late\n1970s established the frame-based paradigm that became the dominant industrial\nparadigm for dialogue systems for over 30 years.",
    "metadata": {
      "source": "15",
      "chunk_id": 27,
      "token_count": 749,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21",
    "metadata": {
      "source": "15",
      "chunk_id": 28,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 21\nAnother in\ufb02uential line of research from that decade focused on modeling the\nhierarchical structure of dialogue. Grosz\u2019s pioneering 1977 dissertation \ufb01rst showed\nthat \u201ctask-oriented dialogues have a structure that closely parallels the structure of\nthe task being performed\u201d (p. 27), leading to her work with Sidner and others show-\ning how to use similar notions of intention and plans to model discourse structure\nand coherence in dialogue. See, e.g., Lochbaum et al. (2000) for a summary of the\nrole of intentional structure in dialogue.\nYet a third line, \ufb01rst suggested by Bruce (1975), suggested that since speech acts\nare actions, they should be planned like other actions, and drew on the AI planning\nliterature (Fikes and Nilsson, 1971). A system seeking to \ufb01nd out some information\ncan come up with the plan of asking the interlocutor for the information. A system\nhearing an utterance can interpret a speech act by running the planner \u201cin reverse\u201d,\nusing inference rules to infer from what the interlocutor said what the plan might\nhave been. Plan-based models of dialogue are referred to as BDI models because BDI\nsuch planners model the beliefs ,desires , and intentions (BDI) of the system and in-\nterlocutor. BDI models of dialogue were \ufb01rst introduced by Allen, Cohen, Perrault,\nand their colleagues in a number of in\ufb02uential papers showing how speech acts could\nbe generated (Cohen and Perrault, 1979) and interpreted (Perrault and Allen 1980,\nAllen and Perrault 1980). At the same time, Wilensky (1983) introduced plan-based\nmodels of understanding as part of the task of interpreting stories.\nIn the 1990s, machine learning models that had \ufb01rst been applied to natural\nlanguage processing began to be applied to dialogue tasks like slot \ufb01lling (Miller\net al. 1994, Pieraccini et al. 1991). This period also saw lots of analytic work on the\nlinguistic properties of dialogue acts and on machine-learning-based methods for\ntheir detection. (Sag and Liberman 1975, Hinkelman and Allen 1989, Nagata and\nMorimoto 1994, Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke\net al. 2000, Gravano et al. 2012. This work strongly informed the development\nof the dialogue-state model (Larsson and Traum, 2000). Dialogue state tracking\nquickly became an important problem for task-oriented dialogue, and there has been\nan in\ufb02uential annual evaluation of state-tracking algorithms (Williams et al., 2016).\nThe turn of the century saw a line of work on applying reinforcement learning\nto dialogue, which \ufb01rst came out of AT&T and Bell Laboratories with work on\nMDP dialogue systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) along\nwith work on cue phrases, prosody, and rejection and con\ufb01rmation. Reinforcement\nlearning research turned quickly to the more sophisticated POMDP models (Roy\net al. 2000, Lemon et al. 2006, Williams and Young 2007) applied to small slot-\n\ufb01lling dialogue tasks. Neural reinforcement learning models have been used both for\nchatbot systems, for example simulating dialogues between two dialogue systems,",
    "metadata": {
      "source": "15",
      "chunk_id": 29,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "Morimoto 1994, Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke\net al. 2000, Gravano et al. 2012. This work strongly informed the development\nof the dialogue-state model (Larsson and Traum, 2000). Dialogue state tracking\nquickly became an important problem for task-oriented dialogue, and there has been\nan in\ufb02uential annual evaluation of state-tracking algorithms (Williams et al., 2016).\nThe turn of the century saw a line of work on applying reinforcement learning\nto dialogue, which \ufb01rst came out of AT&T and Bell Laboratories with work on\nMDP dialogue systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) along\nwith work on cue phrases, prosody, and rejection and con\ufb01rmation. Reinforcement\nlearning research turned quickly to the more sophisticated POMDP models (Roy\net al. 2000, Lemon et al. 2006, Williams and Young 2007) applied to small slot-\n\ufb01lling dialogue tasks. Neural reinforcement learning models have been used both for\nchatbot systems, for example simulating dialogues between two dialogue systems,\nrewarding good conversational properties like coherence and ease of answering (Li\net al., 2016), and for task-oriented dialogue (Williams et al., 2017).\nBy around 2010 the GUS architecture \ufb01nally began to be widely used commer-\ncially in dialogue systems on phones like Apple\u2019s SIRI (Bellegarda, 2013) and other\ndigital assistants.\nThe rise of the web gave rise to corpus-based chatbot architectures around the\nturn of the century, \ufb01rst using information retrieval models and then in the 2010s,\nafter the rise of deep learning, with sequence-to-sequence models.\n[TBD: Modern history of neural chatbots]\nOther important dialogue areas include the study of affect in dialogue (Rashkin\net al. 2019, Lin et al. 2019) and conversational interface design (Cohen et al. 2004,\nHarris 2005, Pearl 2017, Deibel and Evanhoe 2021).",
    "metadata": {
      "source": "15",
      "chunk_id": 30,
      "token_count": 487,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22\n\n22 CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\nExercises\n15.1 Write a \ufb01nite-state automaton for a dialogue manager for checking your bank\nbalance and withdrawing money at an automated teller machine.\n15.2 Adispreferred response is a response that has the potential to make a persondispreferred\nresponse\nuncomfortable or embarrassed in the conversational context; the most com-\nmon example dispreferred responses is turning down a request. People signal\ntheir discomfort with having to say no with surface cues (like the word well),\nor via signi\ufb01cant silence. Try to notice the next time you or someone else\nutters a dispreferred response, and write down the utterance. What are some\nother cues in the response that a system might use to detect a dispreferred\nresponse? Consider non-verbal cues like eye gaze and body gestures.\n15.3 When asked a question to which they aren\u2019t sure they know the answer, peo-\nple display their lack of con\ufb01dence by cues that resemble other dispreferred\nresponses. Try to notice some unsure answers to questions. What are some\nof the cues? If you have trouble doing this, read Smith and Clark (1993) and\nlisten speci\ufb01cally for the cues they mention.\n15.4 Implement a small air-travel help system based on text input. Your system\nshould get constraints from users about a particular \ufb02ight that they want to\ntake, expressed in natural language, and display possible \ufb02ights on a screen.\nMake simplifying assumptions. You may build in a simple \ufb02ight database or\nyou may use a \ufb02ight information system on the Web as your backend.",
    "metadata": {
      "source": "15",
      "chunk_id": 31,
      "token_count": 370,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23",
    "metadata": {
      "source": "15",
      "chunk_id": 32,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Exercises 23\nAllen, J. and C. R. Perrault. 1980. Analyzing intention in\nutterances. Arti\ufb01cial Intelligence , 15:143\u2013178.\nArtstein, R., S. Gandhe, J. Gerten, A. Leuski, and D. Traum.\n2009. Semi-formal evaluation of conversational charac-\nters. In Languages: From Formal to Natural , 22\u201335.\nSpringer.\nAustin, J. L. 1962. How to Do Things with Words . Harvard\nUniversity Press.\nAwadallah, A. H., R. G. Kulkarni, U. Ozertem, and R. Jones.\n2015. Charaterizing and predicting voice query reformu-\nlation. CIKM-15 .\nBach, K. and R. Harnish. 1979. Linguistic communication\nand speech acts . MIT Press.\nBaum, L. F. 1900. The Wizard of Oz . Available at Project\nGutenberg.\nBellegarda, J. R. 2013. Natural language technology in mo-\nbile devices: Two grounding frameworks. In Mobile\nSpeech and Advanced Natural Language Solutions , 185\u2013\n196. Springer.\nBickmore, T. W., H. Trinh, S. Olafsson, T. K. O\u2019Leary,\nR. Asadi, N. M. Rickles, and R. Cruz. 2018. Patient and\nconsumer safety risks when using conversational assis-\ntants for medical information: An observational study of\nSiri, Alexa, and Google Assistant. Journal of Medical\nInternet Research , 20(9):e11510.\nBlodgett, S. L., S. Barocas, H. Daum \u00b4e III, and H. Wallach.\n2020. Language (technology) is power: A critical survey\nof \u201cbias\u201d in NLP. ACL.\nBobrow, D. G., R. M. Kaplan, M. Kay, D. A. Norman,\nH. Thompson, and T. Winograd. 1977. GUS, A frame\ndriven dialog system. Arti\ufb01cial Intelligence , 8:155\u2013173.\nBruce, B. C. 1975. Generation as a social action. Proceed-\nings of TINLAP-1 (Theoretical Issues in Natural Lan-\nguage Processing) .\nBudzianowski, P., T.-H. Wen, B.-H. Tseng, I. Casanueva,\nS. Ultes, O. Ramadan, and M. Ga \u02c7si\u00b4c. 2018. MultiWOZ -\na large-scale multi-domain wizard-of-Oz dataset for task-\noriented dialogue modelling. EMNLP .\nBulyko, I., K. Kirchhoff, M. Ostendorf, and J. Goldberg.\n2005. Error-sensitive response generation in a spo-\nken language dialogue system. Speech Communication ,\n45(3):271\u2013288.\nCarbonell, J. R. 1970. AI in CAI: An arti\ufb01cial-intelligence\napproach to computer-assisted instruction. IEEE transac-\ntions on man-machine systems , 11(4):190\u2013202.\nChristiano, P. F., J. Leike, T. Brown, M. Martic, S. Legg,\nand D. Amodei. 2017. Deep reinforcement learning from\nhuman preferences. NeurIPS , volume 30.",
    "metadata": {
      "source": "15",
      "chunk_id": 33,
      "token_count": 749,
      "chapter_title": ""
    }
  },
  {
    "content": "Bruce, B. C. 1975. Generation as a social action. Proceed-\nings of TINLAP-1 (Theoretical Issues in Natural Lan-\nguage Processing) .\nBudzianowski, P., T.-H. Wen, B.-H. Tseng, I. Casanueva,\nS. Ultes, O. Ramadan, and M. Ga \u02c7si\u00b4c. 2018. MultiWOZ -\na large-scale multi-domain wizard-of-Oz dataset for task-\noriented dialogue modelling. EMNLP .\nBulyko, I., K. Kirchhoff, M. Ostendorf, and J. Goldberg.\n2005. Error-sensitive response generation in a spo-\nken language dialogue system. Speech Communication ,\n45(3):271\u2013288.\nCarbonell, J. R. 1970. AI in CAI: An arti\ufb01cial-intelligence\napproach to computer-assisted instruction. IEEE transac-\ntions on man-machine systems , 11(4):190\u2013202.\nChristiano, P. F., J. Leike, T. Brown, M. Martic, S. Legg,\nand D. Amodei. 2017. Deep reinforcement learning from\nhuman preferences. NeurIPS , volume 30.\nChu-Carroll, J. 1998. A statistical model for discourse act\nrecognition in dialogue interactions. Applying Machine\nLearning to Discourse Processing. Papers from the 1998\nAAAI Spring Symposium. Tech. rep. SS-98-01. AAAI\nPress.\nChu-Carroll, J. and S. Carberry. 1998. Collaborative re-\nsponse generation in planning dialogues. Computational\nLinguistics , 24(3):355\u2013400.\nClark, H. H. 1996. Using Language . Cambridge University\nPress.Clark, H. H. and C. Marshall. 1981. De\ufb01nite reference and\nmutual knowledge. In A. K. Joshi, B. L. Webber, and I. A.\nSag, eds, Elements of Discourse Understanding , 10\u201363.\nCambridge.\nClark, H. H. and D. Wilkes-Gibbs. 1986. Referring as a col-\nlaborative process. Cognition , 22:1\u201339.\nCohen, A. D., A. Roberts, A. Molina, A. Butryna, A. Jin,\nA. Kulshreshtha, B. Hutchinson, B. Zevenbergen, B. H.\nAguera-Arcas, C. ching Chang, C. Cui, C. Du, D. D. F.\nAdiwardana, D. Chen, D. D. Lepikhin, E. H. Chi,\nE. Hoffman-John, H.-T. Cheng, H. Lee, I. Krivokon,\nJ. Qin, J. Hall, J. Fenton, J. Soraker, K. Meier-Hellstern,\nK. Olson, L. M. Aroyo, M. P. Bosma, M. J. Pickett, M. A.\nMenegali, M. Croak, M. D \u00b4\u0131az, M. Lamm, M. Krikun,\nM. R. Morris, N. Shazeer, Q. V . Le, R. Bernstein, R. Ra-\njakumar, R. Kurzweil, R. Thoppilan, S. Zheng, T. Bos,",
    "metadata": {
      "source": "15",
      "chunk_id": 34,
      "token_count": 759,
      "chapter_title": ""
    }
  },
  {
    "content": "laborative process. Cognition , 22:1\u201339.\nCohen, A. D., A. Roberts, A. Molina, A. Butryna, A. Jin,\nA. Kulshreshtha, B. Hutchinson, B. Zevenbergen, B. H.\nAguera-Arcas, C. ching Chang, C. Cui, C. Du, D. D. F.\nAdiwardana, D. Chen, D. D. Lepikhin, E. H. Chi,\nE. Hoffman-John, H.-T. Cheng, H. Lee, I. Krivokon,\nJ. Qin, J. Hall, J. Fenton, J. Soraker, K. Meier-Hellstern,\nK. Olson, L. M. Aroyo, M. P. Bosma, M. J. Pickett, M. A.\nMenegali, M. Croak, M. D \u00b4\u0131az, M. Lamm, M. Krikun,\nM. R. Morris, N. Shazeer, Q. V . Le, R. Bernstein, R. Ra-\njakumar, R. Kurzweil, R. Thoppilan, S. Zheng, T. Bos,\nT. Duke, T. Doshi, V . Y . Zhao, V . Prabhakaran, W. Rusch,\nY . Li, Y . Huang, Y . Zhou, Y . Xu, and Z. Chen. 2022.\nLamda: Language models for dialog applications. ArXiv\npreprint.\nCohen, M. H., J. P. Giangola, and J. Balogh. 2004. Voice\nUser Interface Design . Addison-Wesley.\nCohen, P. R. and C. R. Perrault. 1979. Elements of a plan-\nbased theory of speech acts. Cognitive Science , 3(3):177\u2013\n212.\nColby, K. M., S. Weber, and F. D. Hilf. 1971. Arti\ufb01cial para-\nnoia. Arti\ufb01cial Intelligence , 2(1):1\u201325.\nCole, R. A., D. G. Novick, P. J. E. Vermeulen, S. Sutton,\nM. Fanty, L. F. A. Wessels, J. H. de Villiers, J. Schalkwyk,\nB. Hansen, and D. Burnett. 1997. Experiments with a\nspoken dialogue system for taking the US census. Speech\nCommunication , 23:243\u2013260.\nDanieli, M. and E. Gerbino. 1995. Metrics for evaluating\ndialogue strategies in a spoken language system. AAAI\nSpring Symposium on Empirical Methods in Discourse\nInterpretation and Generation .\nDeibel, D. and R. Evanhoe. 2021. Conversations with\nThings: UX Design for Chat and Voice . Rosenfeld.\nDinan, E., G. Abercrombie, A. S. Bergman, S. Spruit,\nD. Hovy, Y .-L. Boureau, and V . Rieser. 2021. Antici-\npating safety issues in e2e conversational ai: Framework\nand tooling. ArXiv.\nDinan, E., A. Fan, A. Williams, J. Urbanek, D. Kiela, and\nJ. Weston. 2020. Queens are powerful too: Mitigating\ngender bias in dialogue generation. EMNLP .",
    "metadata": {
      "source": "15",
      "chunk_id": 35,
      "token_count": 771,
      "chapter_title": ""
    }
  },
  {
    "content": "M. Fanty, L. F. A. Wessels, J. H. de Villiers, J. Schalkwyk,\nB. Hansen, and D. Burnett. 1997. Experiments with a\nspoken dialogue system for taking the US census. Speech\nCommunication , 23:243\u2013260.\nDanieli, M. and E. Gerbino. 1995. Metrics for evaluating\ndialogue strategies in a spoken language system. AAAI\nSpring Symposium on Empirical Methods in Discourse\nInterpretation and Generation .\nDeibel, D. and R. Evanhoe. 2021. Conversations with\nThings: UX Design for Chat and Voice . Rosenfeld.\nDinan, E., G. Abercrombie, A. S. Bergman, S. Spruit,\nD. Hovy, Y .-L. Boureau, and V . Rieser. 2021. Antici-\npating safety issues in e2e conversational ai: Framework\nand tooling. ArXiv.\nDinan, E., A. Fan, A. Williams, J. Urbanek, D. Kiela, and\nJ. Weston. 2020. Queens are powerful too: Mitigating\ngender bias in dialogue generation. EMNLP .\nFessler, L. 2017. We tested bots like Siri and Alexa to see\nwho would stand up to sexual harassment. Quartz . Feb\n22, 2017. https://qz.com/911681/ .\nFikes, R. E. and N. J. Nilsson. 1971. STRIPS: A new ap-\nproach to the application of theorem proving to problem\nsolving. Arti\ufb01cial Intelligence , 2:189\u2013208.\nFirth, J. R. 1935. The technique of semantics. Transactions\nof the philological society , 34(1):36\u201373.\nFraser, N. M. and G. N. Gilbert. 1991. Simulating speech\nsystems. Computer Speech and Language , 5:81\u201399.\nFriedman, B. and D. G. Hendry. 2019. Value Sensitive De-\nsign: Shaping Technology with Moral Imagination . MIT\nPress.\nFriedman, B., D. G. Hendry, and A. Borning. 2017. A sur-\nvey of value sensitive design methods. Foundations and\nTrends in Human-Computer Interaction , 11(2):63\u2013125.",
    "metadata": {
      "source": "15",
      "chunk_id": 36,
      "token_count": 533,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24",
    "metadata": {
      "source": "15",
      "chunk_id": 37,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "24 Chapter 15 \u2022 Chatbots & Dialogue Systems\nGehman, S., S. Gururangan, M. Sap, Y . Choi, and N. A.\nSmith. 2020. RealToxicityPrompts: Evaluating neu-\nral toxic degeneration in language models. Findings of\nEMNLP .\nGlaese, A., N. McAleese, M. Trebacz, J. Aslanides,\nV . Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chad-\nwick, P. Thacker, L. Campbell-Gillingham, J. Uesato, P.-\nS. Huang, R. Comanescu, F. Yang, A. See, S. Dathathri,\nR. Greig, C. Chen, D. Fritz, J. Sanchez Elias,\nR. Green, S. Mokr \u00b4a, N. Fernando, B. Wu, R. Foley,\nS. Young, I. Gabriel, W. Isaac, J. Mellor, D. Hassabis,\nK. Kavukcuoglu, L. A. Hendricks, and G. Irving. 2022.\nImproving alignment of dialogue agents via targeted hu-\nman judgements. ArXiv preprint.\nGoldberg, J., M. Ostendorf, and K. Kirchhoff. 2003. The im-\npact of response wording in error correction subdialogs.\nISCA Tutorial and Research Workshop on Error Handling\nin Spoken Dialogue Systems .\nGood, M. D., J. A. Whiteside, D. R. Wixon, and S. J.\nJones. 1984. Building a user-derived interface. CACM ,\n27(10):1032\u20131043.\nGoodwin, C. 1996. Transparent vision. In E. Ochs, E. A.\nSchegloff, and S. A. Thompson, eds, Interaction and\nGrammar , 370\u2013404. Cambridge University Press.\nGopalakrishnan, K., B. Hedayatnia, Q. Chen, A. Gottardi,\nS. Kwatra, A. Venkatesh, R. Gabriel, and D. Hakkani-\nT\u00a8ur. 2019. Topical-chat: Towards knowledge-grounded\nopen-domain conversations. INTERSPEECH .\nGould, J. D., J. Conti, and T. Hovanyecz. 1983. Compos-\ning letters with a simulated listening typewriter. CACM ,\n26(4):295\u2013308.\nGould, J. D. and C. Lewis. 1985. Designing for usabil-\nity: Key principles and what designers think. CACM ,\n28(3):300\u2013311.\nGravano, A., J. Hirschberg, and \u02c7S. Be \u02c7nu\u02c7s. 2012. Af\ufb01rma-\ntive cue words in task-oriented dialogue. Computational\nLinguistics , 38(1):1\u201339.\nGrice, H. P. 1975. Logic and conversation. In P. Cole and\nJ. L. Morgan, eds, Speech Acts: Syntax and Semantics\nVolume 3 , 41\u201358. Academic Press.\nGrice, H. P. 1978. Further notes on logic and conversation. In\nP. Cole, ed., Pragmatics: Syntax and Semantics Volume 9 ,\n113\u2013127. Academic Press.",
    "metadata": {
      "source": "15",
      "chunk_id": 38,
      "token_count": 752,
      "chapter_title": ""
    }
  },
  {
    "content": "T\u00a8ur. 2019. Topical-chat: Towards knowledge-grounded\nopen-domain conversations. INTERSPEECH .\nGould, J. D., J. Conti, and T. Hovanyecz. 1983. Compos-\ning letters with a simulated listening typewriter. CACM ,\n26(4):295\u2013308.\nGould, J. D. and C. Lewis. 1985. Designing for usabil-\nity: Key principles and what designers think. CACM ,\n28(3):300\u2013311.\nGravano, A., J. Hirschberg, and \u02c7S. Be \u02c7nu\u02c7s. 2012. Af\ufb01rma-\ntive cue words in task-oriented dialogue. Computational\nLinguistics , 38(1):1\u201339.\nGrice, H. P. 1975. Logic and conversation. In P. Cole and\nJ. L. Morgan, eds, Speech Acts: Syntax and Semantics\nVolume 3 , 41\u201358. Academic Press.\nGrice, H. P. 1978. Further notes on logic and conversation. In\nP. Cole, ed., Pragmatics: Syntax and Semantics Volume 9 ,\n113\u2013127. Academic Press.\nGrosz, B. J. 1977. The Representation and Use of Focus\nin Dialogue Understanding . Ph.D. thesis, University of\nCalifornia, Berkeley.\nGrosz, B. J. and C. L. Sidner. 1980. Plans for discourse. In\nP. R. Cohen, J. Morgan, and M. E. Pollack, eds, Intentions\nin Communication , 417\u2013444. MIT Press.\nGuindon, R. 1988. A multidisciplinary perspective on dia-\nlogue structure in user-advisor dialogues. In R. Guindon,\ned.,Cognitive Science and Its Applications for Human-\nComputer Interaction , 163\u2013200. Lawrence Erlbaum.\nHarris, R. A. 2005. Voice Interaction Design: Crafting the\nNew Conversational Speech Systems . Morgan Kaufmann.\nHenderson, P., K. Sinha, N. Angelard-Gontier, N. R. Ke,\nG. Fried, R. Lowe, and J. Pineau. 2017. Ethical chal-\nlenges in data-driven dialogue systems. AAAI/ACM AI\nEthics and Society Conference .Hinkelman, E. A. and J. Allen. 1989. Two constraints on\nspeech act ambiguity. ACL.\nHirschberg, J., D. J. Litman, and M. Swerts. 2001. Identi-\nfying user corrections automatically in spoken dialogue\nsystems. NAACL .\nIschen, C., T. Araujo, H. V oorveld, G. van Noort, and\nE. Smit. 2019. Privacy concerns in chatbot interactions.\nInternational Workshop on Chatbot Research and De-\nsign.\nJefferson, G. 1972. Side sequences. In D. Sudnow, ed., Stud-\nies in social interaction , 294\u2013333. Free Press, New York.\nLandauer, T. K., ed. 1995. The Trouble with Computers:\nUsefulness, Usability, and Productivity . MIT Press.\nLarsson, S. and D. R. Traum. 2000. Information state and\ndialogue management in the trindi dialogue move engine\ntoolkit. Natural Language Engineering , 6(323-340):97\u2013\n114.",
    "metadata": {
      "source": "15",
      "chunk_id": 39,
      "token_count": 765,
      "chapter_title": ""
    }
  },
  {
    "content": "G. Fried, R. Lowe, and J. Pineau. 2017. Ethical chal-\nlenges in data-driven dialogue systems. AAAI/ACM AI\nEthics and Society Conference .Hinkelman, E. A. and J. Allen. 1989. Two constraints on\nspeech act ambiguity. ACL.\nHirschberg, J., D. J. Litman, and M. Swerts. 2001. Identi-\nfying user corrections automatically in spoken dialogue\nsystems. NAACL .\nIschen, C., T. Araujo, H. V oorveld, G. van Noort, and\nE. Smit. 2019. Privacy concerns in chatbot interactions.\nInternational Workshop on Chatbot Research and De-\nsign.\nJefferson, G. 1972. Side sequences. In D. Sudnow, ed., Stud-\nies in social interaction , 294\u2013333. Free Press, New York.\nLandauer, T. K., ed. 1995. The Trouble with Computers:\nUsefulness, Usability, and Productivity . MIT Press.\nLarsson, S. and D. R. Traum. 2000. Information state and\ndialogue management in the trindi dialogue move engine\ntoolkit. Natural Language Engineering , 6(323-340):97\u2013\n114.\nLemon, O., K. Georgila, J. Henderson, and M. Stuttle. 2006.\nAn ISU dialogue system exhibiting reinforcement learn-\ning of dialogue policies: Generic slot-\ufb01lling in the TALK\nin-car system. EACL .\nLevesque, H. J., P. R. Cohen, and J. H. T. Nunes. 1990. On\nacting together. AAAI . Morgan Kaufmann.\nLevin, E., R. Pieraccini, and W. Eckert. 2000. A stochastic\nmodel of human-machine interaction for learning dialog\nstrategies. IEEE Transactions on Speech and Audio Pro-\ncessing , 8:11\u201323.\nLevinson, S. C. 1983. Conversational Analysis , chapter 6.\nCambridge University Press.\nLevow, G.-A. 1998. Characterizing and recognizing spoken\ncorrections in human-computer dialogue. COLING/ACL .\nLi, J., W. Monroe, A. Ritter, D. Jurafsky, M. Galley, and\nJ. Gao. 2016. Deep reinforcement learning for dialogue\ngeneration. EMNLP .\nLi, M., J. Weston, and S. Roller. 2019. Acute-eval: Improved\ndialogue evaluation with optimized questions and multi-\nturn comparisons. NeurIPS19 Workshop on Conversa-\ntional AI .\nLin, Z., A. Madotto, J. Shin, P. Xu, and P. Fung. 2019.\nMoEL: Mixture of empathetic listeners. EMNLP .\nLitman, D. J. 1985. Plan Recognition and Discourse Anal-\nysis: An Integrated Approach for Understanding Dia-\nlogues . Ph.D. thesis, University of Rochester, Rochester,\nNY .\nLitman, D. J. and J. Allen. 1987. A plan recognition model\nfor subdialogues in conversation. Cognitive Science ,\n11:163\u2013200.\nLitman, D. J., M. A. Walker, and M. Kearns. 1999. Auto-\nmatic detection of poor speech recognition at the dialogue\nlevel. ACL.",
    "metadata": {
      "source": "15",
      "chunk_id": 40,
      "token_count": 749,
      "chapter_title": ""
    }
  },
  {
    "content": "corrections in human-computer dialogue. COLING/ACL .\nLi, J., W. Monroe, A. Ritter, D. Jurafsky, M. Galley, and\nJ. Gao. 2016. Deep reinforcement learning for dialogue\ngeneration. EMNLP .\nLi, M., J. Weston, and S. Roller. 2019. Acute-eval: Improved\ndialogue evaluation with optimized questions and multi-\nturn comparisons. NeurIPS19 Workshop on Conversa-\ntional AI .\nLin, Z., A. Madotto, J. Shin, P. Xu, and P. Fung. 2019.\nMoEL: Mixture of empathetic listeners. EMNLP .\nLitman, D. J. 1985. Plan Recognition and Discourse Anal-\nysis: An Integrated Approach for Understanding Dia-\nlogues . Ph.D. thesis, University of Rochester, Rochester,\nNY .\nLitman, D. J. and J. Allen. 1987. A plan recognition model\nfor subdialogues in conversation. Cognitive Science ,\n11:163\u2013200.\nLitman, D. J., M. A. Walker, and M. Kearns. 1999. Auto-\nmatic detection of poor speech recognition at the dialogue\nlevel. ACL.\nLiu, H., J. Dacon, W. Fan, H. Liu, Z. Liu, and J. Tang. 2020.\nDoes gender matter? Towards fairness in dialogue sys-\ntems. COLING .\nLochbaum, K. E., B. J. Grosz, and C. L. Sidner. 2000. Dis-\ncourse structure and intention recognition. In R. Dale,\nH. Moisl, and H. L. Somers, eds, Handbook of Natural\nLanguage Processing . Marcel Dekker.\nMiller, S., R. J. Bobrow, R. Ingria, and R. Schwartz. 1994.\nHidden understanding models of natural language. ACL.",
    "metadata": {
      "source": "15",
      "chunk_id": 41,
      "token_count": 415,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 25",
    "metadata": {
      "source": "15",
      "chunk_id": 42,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Exercises 25\nMrk\u02c7si\u00b4c, N., D. \u00b4O S\u00b4eaghdha, T.-H. Wen, B. Thomson, and\nS. Young. 2017. Neural belief tracker: Data-driven dia-\nlogue state tracking. ACL.\nNagata, M. and T. Morimoto. 1994. First steps toward statis-\ntical modeling of dialogue to predict the speech act type\nof the next utterance. Speech Communication , 15:193\u2013\n203.\nNayak, N., D. Hakkani-T \u00a8ur, M. A. Walker, and L. P. Heck.\n2017. To plan or not to plan? discourse planning in\nslot-value informed sequence to sequence models for lan-\nguage generation. INTERSPEECH .\nNeff, G. and P. Nagy. 2016. Talking to bots: Symbiotic\nagency and the case of Tay. International Journal of\nCommunication , 10:4915\u20134931.\nNickerson, R. S. 1976. On conversational interaction with\ncomputers. Proceedings of the ACM/SIGGRAPH work-\nshop on User-oriented design of interactive graphics sys-\ntems.\nNielsen, J. 1992. The usability engineering life cycle. IEEE\nComputer , 25(3):12\u201322.\nNorman, D. A. 1988. The Design of Everyday Things . Basic\nBooks.\nOuyang, L., J. Wu, X. Jiang, D. Almeida, C. Wainwright,\nP. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,\nJ. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens,\nA. Askell, P. Welinder, P. Christiano, J. Leike, and\nR. Lowe. 2022. Training language models to follow in-\nstructions with human feedback. NeurIPS , volume 35.\nPaolino, J. 2017. Google Home vs Alexa: Two\nsimple user experience design gestures that de-\nlighted a female user. Medium . Jan 4,\n2017. https://medium.com/startup-grind/\ngoogle-home-vs-alexa-56e26f69ac77 .\nPearl, C. 2017. Designing Voice User Interfaces: Principles\nof Conversational Experiences . O\u2019Reilly.\nPerrault, C. R. and J. Allen. 1980. A plan-based analysis of\nindirect speech acts. American Journal of Computational\nLinguistics , 6(3-4):167\u2013182.\nPieraccini, R., E. Levin, and C.-H. Lee. 1991. Stochastic\nrepresentation of conceptual structure in the ATIS task.\nSpeech and Natural Language Workshop .\nRafailov, R., A. Sharma, E. Mitchell, S. Ermon, C. D. Man-\nning, and C. Finn. 2023. Direct preference optimiza-\ntion: Your language model is secretly a reward model.\nNeurIPS .\nRashkin, H., E. M. Smith, M. Li, and Y .-L. Boureau. 2019.\nTowards empathetic open-domain conversation models:\nA new benchmark and dataset. ACL.\nReeves, B. and C. Nass. 1996. The Media Equation: How\nPeople Treat Computers, Television, and New Media Like\nReal People and Places . Cambridge University Press.",
    "metadata": {
      "source": "15",
      "chunk_id": 43,
      "token_count": 755,
      "chapter_title": ""
    }
  },
  {
    "content": "google-home-vs-alexa-56e26f69ac77 .\nPearl, C. 2017. Designing Voice User Interfaces: Principles\nof Conversational Experiences . O\u2019Reilly.\nPerrault, C. R. and J. Allen. 1980. A plan-based analysis of\nindirect speech acts. American Journal of Computational\nLinguistics , 6(3-4):167\u2013182.\nPieraccini, R., E. Levin, and C.-H. Lee. 1991. Stochastic\nrepresentation of conceptual structure in the ATIS task.\nSpeech and Natural Language Workshop .\nRafailov, R., A. Sharma, E. Mitchell, S. Ermon, C. D. Man-\nning, and C. Finn. 2023. Direct preference optimiza-\ntion: Your language model is secretly a reward model.\nNeurIPS .\nRashkin, H., E. M. Smith, M. Li, and Y .-L. Boureau. 2019.\nTowards empathetic open-domain conversation models:\nA new benchmark and dataset. ACL.\nReeves, B. and C. Nass. 1996. The Media Equation: How\nPeople Treat Computers, Television, and New Media Like\nReal People and Places . Cambridge University Press.\nRitter, A., C. Cherry, and B. Dolan. 2010. Unsupervised\nmodeling of twitter conversations. NAACL HLT .\nRoller, S., E. Dinan, N. Goyal, D. Ju, M. Williamson, Y . Liu,\nJ. Xu, M. Ott, E. M. Smith, Y .-L. Boureau, and J. We-\nston. 2021. Recipes for building an open-domain chatbot.\nEACL .\nRoy, N., J. Pineau, and S. Thrun. 2000. Spoken dialogue\nmanagement using probabilistic reasoning. ACL.Sacks, H., E. A. Schegloff, and G. Jefferson. 1974. A sim-\nplest systematics for the organization of turn-taking for\nconversation. Language , 50(4):696\u2013735.\nSag, I. A. and M. Y . Liberman. 1975. The intonational dis-\nambiguation of indirect speech acts. In CLS-75 , 487\u2013498.\nUniversity of Chicago.\nSchegloff, E. A. 1968. Sequencing in conversational open-\nings. American Anthropologist , 70:1075\u20131095.\nSee, A., S. Roller, D. Kiela, and J. Weston. 2019. What\nmakes a good conversation? how controllable attributes\naffect human judgments. NAACL HLT .\nShriberg, E., R. Bates, P. Taylor, A. Stolcke, D. Jurafsky,\nK. Ries, N. Coccaro, R. Martin, M. Meteer, and C. Van\nEss-Dykema. 1998. Can prosody aid the automatic clas-\nsi\ufb01cation of dialog acts in conversational speech? Lan-\nguage and Speech (Special Issue on Prosody and Conver-\nsation) , 41(3-4):439\u2013487.\nSingh, S. P., D. J. Litman, M. Kearns, and M. A. Walker.\n2002. Optimizing dialogue management with reinforce-\nment learning: Experiments with the NJFun system.\nJAIR , 16:105\u2013133.",
    "metadata": {
      "source": "15",
      "chunk_id": 44,
      "token_count": 755,
      "chapter_title": ""
    }
  },
  {
    "content": "ambiguation of indirect speech acts. In CLS-75 , 487\u2013498.\nUniversity of Chicago.\nSchegloff, E. A. 1968. Sequencing in conversational open-\nings. American Anthropologist , 70:1075\u20131095.\nSee, A., S. Roller, D. Kiela, and J. Weston. 2019. What\nmakes a good conversation? how controllable attributes\naffect human judgments. NAACL HLT .\nShriberg, E., R. Bates, P. Taylor, A. Stolcke, D. Jurafsky,\nK. Ries, N. Coccaro, R. Martin, M. Meteer, and C. Van\nEss-Dykema. 1998. Can prosody aid the automatic clas-\nsi\ufb01cation of dialog acts in conversational speech? Lan-\nguage and Speech (Special Issue on Prosody and Conver-\nsation) , 41(3-4):439\u2013487.\nSingh, S. P., D. J. Litman, M. Kearns, and M. A. Walker.\n2002. Optimizing dialogue management with reinforce-\nment learning: Experiments with the NJFun system.\nJAIR , 16:105\u2013133.\nSmith, V . L. and H. H. Clark. 1993. On the course of an-\nswering questions. Journal of Memory and Language ,\n32:25\u201338.\nStalnaker, R. C. 1978. Assertion. In P. Cole, ed., Pragmat-\nics: Syntax and Semantics Volume 9 , 315\u2013332. Academic\nPress.\nStifelman, L. J., B. Arons, C. Schmandt, and E. A. Hulteen.\n1993. V oiceNotes: A speech interface for a hand-held\nvoice notetaker. INTERCHI 1993 .\nStolcke, A., K. Ries, N. Coccaro, E. Shriberg, R. Bates,\nD. Jurafsky, P. Taylor, R. Martin, M. Meteer, and C. Van\nEss-Dykema. 2000. Dialogue act modeling for automatic\ntagging and recognition of conversational speech. Com-\nputational Linguistics , 26(3):339\u2013371.\nSwerts, M., D. J. Litman, and J. Hirschberg. 2000. Correc-\ntions in spoken dialogue systems. ICSLP .\nUng, M., J. Xu, and Y .-L. Boureau. 2022. SaFeRDialogues:\nTaking feedback gracefully after conversational safety\nfailures. ACL.\nWade, E., E. Shriberg, and P. J. Price. 1992. User behaviors\naffecting speech recognition. ICSLP .\nWalker, M. A. 2000. An application of reinforcement learn-\ning to dialogue strategy selection in a spoken dialogue\nsystem for email. JAIR , 12:387\u2013416.\nWalker, M. A., J. C. Fromer, and S. S. Narayanan. 1998.\nLearning optimal dialogue strategies: A case study of a\nspoken dialogue agent for email. COLING/ACL .\nWeizenbaum, J. 1966. ELIZA \u2013 A computer program for the\nstudy of natural language communication between man\nand machine. CACM , 9(1):36\u201345.\nWilensky, R. 1983. Planning and Understanding: A Com-",
    "metadata": {
      "source": "15",
      "chunk_id": 45,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "tagging and recognition of conversational speech. Com-\nputational Linguistics , 26(3):339\u2013371.\nSwerts, M., D. J. Litman, and J. Hirschberg. 2000. Correc-\ntions in spoken dialogue systems. ICSLP .\nUng, M., J. Xu, and Y .-L. Boureau. 2022. SaFeRDialogues:\nTaking feedback gracefully after conversational safety\nfailures. ACL.\nWade, E., E. Shriberg, and P. J. Price. 1992. User behaviors\naffecting speech recognition. ICSLP .\nWalker, M. A. 2000. An application of reinforcement learn-\ning to dialogue strategy selection in a spoken dialogue\nsystem for email. JAIR , 12:387\u2013416.\nWalker, M. A., J. C. Fromer, and S. S. Narayanan. 1998.\nLearning optimal dialogue strategies: A case study of a\nspoken dialogue agent for email. COLING/ACL .\nWeizenbaum, J. 1966. ELIZA \u2013 A computer program for the\nstudy of natural language communication between man\nand machine. CACM , 9(1):36\u201345.\nWilensky, R. 1983. Planning and Understanding: A Com-\nputational Approach to Human Reasoning . Addison-\nWesley.\nWilliams, J. D., K. Asadi, and G. Zweig. 2017. Hybrid code\nnetworks: practical and ef\ufb01cient end-to-end dialog con-\ntrol with supervised and reinforcement learning. ACL.\nWilliams, J. D., A. Raux, and M. Henderson. 2016. The di-\nalog state tracking challenge series: A review. Dialogue\n& Discourse , 7(3):4\u201333.",
    "metadata": {
      "source": "15",
      "chunk_id": 46,
      "token_count": 397,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 26\n\n26 Chapter 15 \u2022 Chatbots & Dialogue Systems\nWilliams, J. D. and S. J. Young. 2007. Partially observ-\nable markov decision processes for spoken dialog sys-\ntems. Computer Speech and Language , 21(1):393\u2013422.\nWittgenstein, L. 1953. Philosophical Investigations. (Trans-\nlated by Anscombe, G.E.M.) . Blackwell.\nWolf, M. J., K. W. Miller, and F. S. Grodzinsky. 2017. Why\nwe should have seen that coming: Comments on Mi-\ncrosoft\u2019s Tay \u201cexperiment,\u201d and wider implications. The\nORBIT Journal , 1(2):1\u201312.\nXu, J., D. Ju, M. Li, Y .-L. Boureau, J. Weston, and E. Dinan.\n2020. Recipes for safety in open-domain chatbots. ArXiv\npreprint arXiv:2010.07079.\nYankelovich, N., G.-A. Levow, and M. Marx. 1995. Design-\ning SpeechActs: Issues in speech user interfaces. CHI-95 .\nYoung, S. J., M. Ga \u02c7si\u00b4c, S. Keizer, F. Mairesse, J. Schatz-\nmann, B. Thomson, and K. Yu. 2010. The Hidden Infor-\nmation State model: A practical framework for POMDP-\nbased spoken dialogue management. Computer Speech &\nLanguage , 24(2):150\u2013174.",
    "metadata": {
      "source": "15",
      "chunk_id": 47,
      "token_count": 341,
      "chapter_title": ""
    }
  }
]