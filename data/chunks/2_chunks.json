[
  {
    "content": "# 2\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n2Regular Expressions, Tok-\nenization, Edit Distance\nUser: I am unhappy.\nELIZA: DO YOU THINK COMING HERE WILL HELP YOU NOT TO BE UNHAPPY\nUser: I need some help, that much seems certain.\nELIZA: WHAT WOULD IT MEAN TO YOU IF YOU GOT SOME HELP\nUser: Perhaps I could learn to get along with my mother.\nELIZA: TELL ME MORE ABOUT YOUR FAMILY\nUser: My mother takes care of me.\nELIZA: WHO ELSE IN YOU FAMILY TAKES CARE OF YOU\nUser: My father.\nELIZA: YOUR FATHER\nUser: You are like my father in some ways.\nWeizenbaum (1966)\nThe dialogue above is from ELIZA , an early natural language processing system ELIZA\nthat could carry on a limited conversation with a user by imitating the responses of\na Rogerian psychotherapist (Weizenbaum, 1966). ELIZA is a surprisingly simple\nprogram that uses pattern matching to recognize phrases like \u201cI need X\u201d and translate\nthem into suitable outputs like \u201cWhat would it mean to you if you got X?\u201d. This\nsimple technique succeeds in this domain because ELIZA doesn\u2019t actually need to\nknow anything to mimic a Rogerian psychotherapist. As Weizenbaum notes, this is\none of the few dialogue genres where listeners can act as if they know nothing of the\nworld. ELIZA\u2019s mimicry of human conversation was remarkably successful: many\npeople who interacted with ELIZA came to believe that it really understood them\nand their problems, many continued to believe in ELIZA\u2019s abilities even after the\nprogram\u2019s operation was explained to them (Weizenbaum, 1976), and even today\nsuch chatbots are a fun diversion. chatbots\nOf course modern conversational agents are much more than a diversion; they\ncan answer questions, book \ufb02ights, or \ufb01nd restaurants, functions for which they rely\non a much more sophisticated understanding of the user\u2019s intent, as we will see in\nChapter 15. Nonetheless, the simple pattern-based methods that powered ELIZA\nand other chatbots play a crucial role in natural language processing.\nWe\u2019ll begin with the most important tool for describing text patterns: the regular\nexpression . Regular expressions can be used to specify strings we might want to\nextract from a document, from transforming \u201cI need X\u201d in ELIZA above, to de\ufb01ning\nstrings like $199 or$24.99 for extracting tables of prices from a document.\nWe\u2019ll then turn to a set of tasks collectively called text normalization , in whichtext\nnormalization\nregular expressions play an important part. Normalizing text means converting it\nto a more convenient, standard form. For example, most of what we are going to\ndo with language relies on \ufb01rst separating out or tokenizing words or word parts\nfrom running text, the task of tokenization . English words are often separated from tokenization\neach other by whitespace, but whitespace is not always suf\ufb01cient. New York and\nrock \u2019n\u2019 roll are sometimes treated as large words despite the fact that they contain\nspaces, while sometimes we\u2019ll need to separate I\u2019minto the two words Iandam.\nFor processing tweets or texts we\u2019ll need to tokenize emoticons like:)orhashtags",
    "metadata": {
      "source": "2",
      "chunk_id": 0,
      "token_count": 752,
      "chapter_title": "2"
    }
  },
  {
    "content": "## Page 2\n\n2CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\nlike#nlproc . Some languages, like Japanese, don\u2019t have spaces between words,\nso word tokenization becomes more dif\ufb01cult. And as we\u2019ll see, for large language\nmodels we\u2019ll use tokens that range greatly in size, from letters to subwords (parts of\nwords) to words and even sometimes short phrases.\nAnother part of text normalization is lemmatization , the task of determining lemmatization\nthat two words have the same root, despite their surface differences. For example,\nthe words sang ,sung , and sings are forms of the verb sing. The word sing is the\ncommon lemma of these words, and a lemmatizer maps from all of these to sing.\nLemmatization is essential for processing morphologically complex languages like\nArabic. Stemming refers to a simpler version of lemmatization in which we mainly stemming\njust strip suf\ufb01xes from the end of the word. Text normalization also includes sen-\ntence segmentation : breaking up a text into individual sentences, using cues likesentence\nsegmentation\nperiods or exclamation points.\nFinally, we\u2019ll need to compare words and other strings. We\u2019ll introduce a metric\ncalled edit distance that measures how similar two strings are based on the number\nof edits (insertions, deletions, substitutions) it takes to change one string into the\nother. Edit distance is an algorithm with applications throughout language process-\ning, from spelling correction to speech recognition to coreference resolution.\n2.1 Regular Expressions\nOne of the most useful tools for text processing in computer science has been the\nregular expression (often shortened to regex ), a language for specifying text searchregular\nexpression\nstrings. This practical language is used in every computer language, in text process-\ning tools like the Unix tools grep, and in editors like vim or Emacs. Formally, a\nregular expression is an algebraic notation for characterizing a set of strings. Reg-\nular expressions are particularly useful for searching in texts, when we have a pat-\ntern to search for and a corpus of texts to search through. A regular expression corpus\nsearch function will search through the corpus, returning all texts that match the\npattern. The corpus can be a single document or a collection. For example, the\nUnix command-line tool grep takes a regular expression and returns every line of\nthe input document that matches the expression.\nA search can be designed to return every match on a line, if there are more than\none, or just the \ufb01rst match. In the following examples we generally underline the\nexact string that matches the regular expression and show only the \ufb01rst match. We\u2019ll\nshow regular expressions delimited by slashes but note that slashes are notpart of\nthe regular expressions.\nRegular expressions come in many variants. We\u2019ll be describing extended regu-\nlar expressions ; different regular expression parsers may only recognize subsets of\nthese, or treat some expressions slightly differently. Using an online regular expres-\nsion tester is a handy way to test out your expressions and explore these variations.\n2.1.1 Basic Regular Expression Patterns\nThe simplest kind of regular expression is a sequence of simple characters; putting\ncharacters in sequence is called concatenation . To search for woodchuck , we type concatenation\n/woodchuck/ . The expression /Buttercup/ matches any string containing the\nsubstring Buttercup ;grep with that expression would return the line I\u2019m called lit-\ntle Buttercup . The search string can consist of a single character (like /!/) or a",
    "metadata": {
      "source": "2",
      "chunk_id": 1,
      "token_count": 753,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\n2.1 \u2022 R EGULAR EXPRESSIONS 3\nsequence of characters (like /urgl/ ) (see Fig. 2.1).\nRegex Example Patterns Matched\n/woodchucks/ \u201cinteresting links to woodchucks and lemurs\u201d\n/a/ \u201cMary Ann stopped by Mona\u2019s\u201d\n/!/ \u201cYou\u2019ve left the burglar behind again! \u201d said Nori\nFigure 2.1 Some simple regex searches.\nRegular expressions are case sensitive ; lower case /s/ is distinct from upper\ncase/S/ (/s/ matches a lower case sbut not an upper case S). This means that\nthe pattern /woodchucks/ will not match the string Woodchucks . We can solve this\nproblem with the use of the square braces [and]. The string of characters inside the\nbraces speci\ufb01es a disjunction of characters to match. For example, Fig. 2.2 shows\nthat the pattern /[wW]/ matches patterns containing either worW.\nRegex Match Example Patterns\n/[wW]oodchuck/ Woodchuck or woodchuck \u201cWoodchuck \u201d\n/[abc]/ \u2018a\u2019, \u2018b\u2019, or\u2018c\u2019 \u201cIn uomini, in solda ti\u201d\n/[1234567890]/ any digit \u201cplenty of 7 to 5\u201d\nFigure 2.2 The use of the brackets []to specify a disjunction of characters.\nThe regular expression /[1234567890]/ speci\ufb01es any single digit. While such\nclasses of characters as digits or letters are important building blocks in expressions,\nthey can get awkward (e.g., it\u2019s inconvenient to specify\n/[ABCDEFGHIJKLMNOPQRSTUVWXYZ]/ (2.1)\nto mean \u201cany capital letter\u201d). In cases where there is a well-de\ufb01ned sequence asso-\nciated with a set of characters, the brackets can be used with the dash ( -) to specify\nany one character in a range . The pattern /[2-5]/ speci\ufb01es any one of the charac- range\nters2,3,4, or5. The pattern /[b-g]/ speci\ufb01es one of the characters b,c,d,e,f, or\ng. Some other examples are shown in Fig. 2.3.\nRegex Match Example Patterns Matched\n/[A-Z]/ an upper case letter \u201cwe should call it \u2018D renched Blossoms\u2019 \u201d\n/[a-z]/ a lower case letter \u201cmy beans were impatient to be hoed!\u201d\n/[0-9]/ a single digit \u201cChapter 1 : Down the Rabbit Hole\u201d\nFigure 2.3 The use of the brackets []plus the dash -to specify a range.\nThe square braces can also be used to specify what a single character cannot be,\nby use of the caret ^. If the caret ^is the \ufb01rst symbol after the open square brace [,\nthe resulting pattern is negated. For example, the pattern /[^a]/ matches any single\ncharacter (including special characters) except a. This is only true when the caret\nis the \ufb01rst symbol after the open square brace. If it occurs anywhere else, it usually\nstands for a caret; Fig. 2.4 shows some examples.\nHow can we talk about optional elements, like an optional sinwoodchuck and\nwoodchucks ? We can\u2019t use the square brackets, because while they allow us to say\n\u201cs or S\u201d, they don\u2019t allow us to say \u201cs or nothing\u201d. For this we use the question mark\n/?/, which means \u201cthe preceding character or nothing\u201d, as shown in Fig. 2.5.\nWe can think of the question mark as meaning \u201czero or one instances of the\nprevious character\u201d. That is, it\u2019s a way of specifying how many of something that",
    "metadata": {
      "source": "2",
      "chunk_id": 2,
      "token_count": 790,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\nRegex Match (single characters) Example Patterns Matched\n/[^A-Z]/ not an upper case letter \u201cOyfn pripetchik\u201d\n/[^Ss]/ neither \u2018S\u2019 nor \u2018s\u2019 \u201cIhave no exquisite reason for\u2019t\u201d\n/[^.]/ not a period \u201cour resident Djinn\u201d\n/[e^]/ either \u2018e\u2019 or \u2018 ^\u2019 \u201clook up \u02c6 now\u201d\n/a^b/ the pattern \u2018 a^b\u2019 \u201clook up a\u02c6 b now\u201d\nFigure 2.4 The caret ^for negation or just to mean ^. See below re: the backslash for escaping the period.\nRegex Match Example Patterns Matched\n/woodchucks?/ woodchuck or woodchucks \u201cwoodchuck \u201d\n/colou?r/ color or colour \u201ccolor \u201d\nFigure 2.5 The question mark ?marks optionality of the previous expression.\nwe want, something that is very important in regular expressions. For example,\nconsider the language of certain sheep, which consists of strings that look like the\nfollowing:\nbaa!\nbaaa!\nbaaaa!\n. . .\nThis language consists of strings with a b, followed by at least two a\u2019s, followed\nby an exclamation point. The set of operators that allows us to say things like \u201csome\nnumber of as\u201d are based on the asterisk or *, commonly called the Kleene * (gen- Kleene *\nerally pronounced \u201ccleany star\u201d). The Kleene star means \u201czero or more occurrences\nof the immediately previous character or regular expression\u201d. So /a*/ means \u201cany\nstring of zero or more as\u201d. This will match aoraaaaaa , but it will also match the\nempty string at the start of Off Minor since the string Off Minor starts with zero a\u2019s.\nSo the regular expression for matching one or more ais/aa*/ , meaning one afol-\nlowed by zero or more as. More complex patterns can also be repeated. So /[ab]*/\nmeans \u201czero or more a\u2019s or b\u2019s\u201d (not \u201czero or more right square braces\u201d). This will\nmatch strings like aaaa orababab orbbbb , as well as the empty string.\nFor specifying multiple digits (useful for \ufb01nding prices) we can extend /[0-9]/ ,\nthe regular expression for a single digit. An integer (a string of digits) is thus\n/[0-9][0-9]*/ . (Why isn\u2019t it just /[0-9]*/ ?)\nSometimes it\u2019s annoying to have to write the regular expression for digits twice,\nso there is a shorter way to specify \u201cat least one\u201d of some character. This is the\nKleene + , which means \u201cone or more occurrences of the immediately preceding Kleene +\ncharacter or regular expression\u201d. Thus, the expression /[0-9]+/ is the normal way\nto specify \u201ca sequence of digits\u201d. There are thus two ways to specify the sheep\nlanguage: /baaa*!/ or/baa+!/ .\nOne very important special character is the period ( /./), awildcard expression\nthat matches any single character ( except a carriage return), as shown in Fig. 2.6.\nRegex Match Example Matches\n/beg.n/ any character between begandn begin , beg\u2019n , begun\nFigure 2.6 The use of the period .to specify any character.\nThe wildcard is often used together with the Kleene star to mean \u201cany string of\ncharacters\u201d. For example, suppose we want to \ufb01nd any line in which a particular",
    "metadata": {
      "source": "2",
      "chunk_id": 3,
      "token_count": 766,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5",
    "metadata": {
      "source": "2",
      "chunk_id": 4,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "2.1 \u2022 R EGULAR EXPRESSIONS 5\nword, for example, aardvark , appears twice. We can specify this with the regular\nexpression /aardvark.*aardvark/ .\nAnchors are special characters that anchor regular expressions to particular places anchors\nin a string. The most common anchors are the caret ^and the dollar sign $. The caret\n^matches the start of a line. The pattern /^The/ matches the word Theonly at the\nstart of a line. Thus, the caret ^has three uses: to match the start of a line, to in-\ndicate a negation inside of square brackets, and just to mean a caret. (What are the\ncontexts that allow grep or Python to know which function a given caret is supposed\nto have?) The dollar sign $matches the end of a line. So the pattern  $is a useful\npattern for matching a space at the end of a line, and /^The dog\\.$/ matches a\nline that contains only the phrase The dog. (We have to use the backslash here since\nwe want the .to mean \u201cperiod\u201d and not the wildcard.)\nRegex Match\n^ start of line\n$ end of line\n\\b word boundary\n\\B non-word boundary\nFigure 2.7 Anchors in regular expressions.\nThere are also two other anchors: \\bmatches a word boundary, and \\Bmatches\na non word-boundary. Thus, /\\bthe\\b/ matches the word thebut not the word\nother . A \u201cword\u201d for the purposes of a regular expression is de\ufb01ned based on the\nde\ufb01nition of words in programming languages as a sequence of digits, underscores,\nor letters. Thus /\\b99\\b/ will match the string 99inThere are 99 bottles of beer on\nthe wall (because 99 follows a space) but not 99inThere are 299 bottles of beer on\nthe wall (since 99 follows a number). But it will match 99in$99(since 99follows\na dollar sign ($), which is not a digit, underscore, or letter).\n2.1.2 Disjunction, Grouping, and Precedence\nSuppose we need to search for texts about pets; perhaps we are particularly interested\nin cats and dogs. In such a case, we might want to search for either the string cator\nthe string dog. Since we can\u2019t use the square brackets to search for \u201ccat or dog\u201d (why\ncan\u2019t we say /[catdog]/ ?), we need a new operator, the disjunction operator, also disjunction\ncalled the pipe symbol|. The pattern /cat|dog/ matches either the string cator\nthe string dog.\nSometimes we need to use this disjunction operator in the midst of a larger se-\nquence. For example, suppose I want to search for information about pet \ufb01sh for\nmy cousin David. How can I specify both guppy andguppies ? We cannot simply\nsay/guppy|ies/ , because that would match only the strings guppy andies. This\nis because sequences like guppy take precedence over the disjunction operator |. precedence\nTo make the disjunction operator apply only to a speci\ufb01c pattern, we need to use the\nparenthesis operators (and). Enclosing a pattern in parentheses makes it act like\na single character for the purposes of neighboring operators like the pipe |and the\nKleene*. So the pattern /gupp(y|ies)/ would specify that we meant the disjunc-\ntion only to apply to the suf\ufb01xes yandies.\nThe parenthesis operator (is also useful when we are using counters like the",
    "metadata": {
      "source": "2",
      "chunk_id": 5,
      "token_count": 778,
      "chapter_title": ""
    }
  },
  {
    "content": "in cats and dogs. In such a case, we might want to search for either the string cator\nthe string dog. Since we can\u2019t use the square brackets to search for \u201ccat or dog\u201d (why\ncan\u2019t we say /[catdog]/ ?), we need a new operator, the disjunction operator, also disjunction\ncalled the pipe symbol|. The pattern /cat|dog/ matches either the string cator\nthe string dog.\nSometimes we need to use this disjunction operator in the midst of a larger se-\nquence. For example, suppose I want to search for information about pet \ufb01sh for\nmy cousin David. How can I specify both guppy andguppies ? We cannot simply\nsay/guppy|ies/ , because that would match only the strings guppy andies. This\nis because sequences like guppy take precedence over the disjunction operator |. precedence\nTo make the disjunction operator apply only to a speci\ufb01c pattern, we need to use the\nparenthesis operators (and). Enclosing a pattern in parentheses makes it act like\na single character for the purposes of neighboring operators like the pipe |and the\nKleene*. So the pattern /gupp(y|ies)/ would specify that we meant the disjunc-\ntion only to apply to the suf\ufb01xes yandies.\nThe parenthesis operator (is also useful when we are using counters like the\nKleene*. Unlike the |operator, the Kleene *operator applies by default only to\na single character, not to a whole sequence. Suppose we want to match repeated\ninstances of a string. Perhaps we have a line that has column labels of the form",
    "metadata": {
      "source": "2",
      "chunk_id": 6,
      "token_count": 349,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\nColumn 1 Column 2 Column 3 . The expression /Column [0-9]+ */ will not\nmatch any number of columns; instead, it will match a single column followed by\nany number of spaces! The star here applies only to the space  that precedes it,\nnot to the whole sequence. With the parentheses, we could write the expression\n/(Column [0-9]+ *)*/ to match the word Column , followed by a number and\noptional spaces, the whole pattern repeated zero or more times.\nThis idea that one operator may take precedence over another, requiring us to\nsometimes use parentheses to specify what we mean, is formalized by the operator\nprecedence hierarchy for regular expressions. The following table gives the orderoperator\nprecedence\nof RE operator precedence, from highest precedence to lowest precedence.\nParenthesis ()\nCounters * + ? {}\nSequences and anchors the ^my end$\nDisjunction |\nThus, because counters have a higher precedence than sequences,\n/the*/ matches theeeee but not thethe . Because sequences have a higher prece-\ndence than disjunction, /the|any/ matches theoranybut not thany ortheny .\nPatterns can be ambiguous in another way. Consider the expression /[a-z]*/\nwhen matching against the text once upon a time . Since/[a-z]*/ matches zero or\nmore letters, this expression could match nothing, or just the \ufb01rst letter o,on,onc,\noronce . In these cases regular expressions always match the largest string they can;\nwe say that patterns are greedy , expanding to cover as much of a string as they can. greedy\nThere are, however, ways to enforce non-greedy matching, using another mean- non-greedy\ning of the ?quali\ufb01er. The operator *?is a Kleene star that matches as little text as *?\npossible. The operator +?is a Kleene plus that matches as little text as possible. +?\n2.1.3 A Simple Example\nSuppose we wanted to write a RE to \ufb01nd cases of the English article the. A simple\n(but incorrect) pattern might be:\n/the/ (2.2)\nOne problem is that this pattern will miss the word when it begins a sentence and\nhence is capitalized (i.e., The). This might lead us to the following pattern:\n/[tT]he/ (2.3)\nBut we will still overgeneralize, incorrectly return texts with theembedded in other\nwords (e.g., other orthere ). So we need to specify that we want instances with a\nword boundary on both sides:\n/\\b[tT]he\\b/ (2.4)\nThe simple process we just went through was based on \ufb01xing two kinds of errors:\nfalse positives , strings that we incorrectly matched like other orthere , and false false positives\nnegatives , strings that we incorrectly missed, like The. Addressing these two kinds false negatives\nof errors comes up again and again in language processing. Reducing the overall\nerror rate for an application thus involves two antagonistic efforts:\n\u2022 Increasing precision (minimizing false positives)\n\u2022 Increasing recall (minimizing false negatives)\nWe\u2019ll come back to precision and recall with more precise de\ufb01nitions in Chapter 4.",
    "metadata": {
      "source": "2",
      "chunk_id": 7,
      "token_count": 705,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\n2.1 \u2022 R EGULAR EXPRESSIONS 7\n2.1.4 More Operators\nFigure 2.8 shows some aliases for common ranges, which can be used mainly to\nsave typing. Besides the Kleene * and Kleene + we can also use explicit numbers as\ncounters, by enclosing them in curly brackets. The operator /{3}/ means \u201cexactly\n3 occurrences of the previous character or expression\u201d. So /a\\.{24}z/ will match\nafollowed by 24 dots followed by z(but not afollowed by 23 or 25 dots followed\nby a z).\nRegex Expansion Match First Matches\n\\d [0-9] any digit Party of 5\n\\D [^0-9] any non-digit Blue moon\n\\w [a-zA-Z0-9_] any alphanumeric/underscore Daiyu\n\\W [^\\w] a non-alphanumeric !!!!\n\\s [ \\r\\t\\n\\f] whitespace (space, tab) inConcord\n\\S [^\\s] Non-whitespace in Concord\nFigure 2.8 Aliases for common sets of characters.\nA range of numbers can also be speci\ufb01ed. So /{n,m}/ speci\ufb01es from ntom\noccurrences of the previous char or expression, and /{n,}/ means at least noccur-\nrences of the previous expression. REs for counting are summarized in Fig. 2.9.\nRegex Match\n* zero or more occurrences of the previous char or expression\n+ one or more occurrences of the previous char or expression\n? zero or one occurrence of the previous char or expression\n{n} exactly noccurrences of the previous char or expression\n{n,m} from ntomoccurrences of the previous char or expression\n{n,} at least noccurrences of the previous char or expression\n{,m} up to moccurrences of the previous char or expression\nFigure 2.9 Regular expression operators for counting.\nFinally, certain special characters are referred to by special notation based on the\nbackslash ( \\) (see Fig. 2.10). The most common of these are the newline character newline\n\\nand the tabcharacter\\t. To refer to characters that are special themselves (like\n.,*,[, and\\), precede them with a backslash, (i.e., /\\./ ,/\\*/ ,/\\[/ , and/\\\\/ ).\nRegex Match First Patterns Matched\n\\* an asterisk \u201c*\u201d \u201cK* A*P*L*A*N\u201d\n\\. a period \u201c.\u201d \u201cDr. Livingston, I presume\u201d\n\\? a question mark \u201cWhy don\u2019t they come and lend a hand? \u201d\n\\n a newline\n\\t a tab\nFigure 2.10 Some characters that need to be escaped (via backslash).\n2.1.5 A More Complex Example\nLet\u2019s try out a more signi\ufb01cant example of the power of REs. Suppose our goal is\nhelp a user buy a computer on the Web who wants \u201cat least 6 GHz and 500 GB of\ndisk space for less than $1000\u201d. To do this kind of retrieval, we \ufb01rst need to be",
    "metadata": {
      "source": "2",
      "chunk_id": 8,
      "token_count": 668,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\nable to look for expressions like 6 GHz or500 GB or$999.99 . Let\u2019s work out some\nregular expressions for this task.\nFirst, let\u2019s complete our regular expression for prices. Here\u2019s a regular expres-\nsion for a dollar sign followed by a string of digits:\n/$[0-9]+/ (2.5)\nNote that the $character has a different function here than the end-of-line function\nwe discussed earlier. Most regular expression parsers are smart enough to realize\nthat$here doesn\u2019t mean end-of-line. (As a thought experiment, think about how\nregex parsers might \ufb01gure out the function of $from the context.)\nNow we just need to deal with fractions of dollars. We\u2019ll add a decimal point\nand two digits afterwards:\n/$[0-9]+\\.[0-9][0-9]/ (2.6)\nThis pattern only allows $199.99 but not $199 . We need to make the cents optional\nand to make sure we\u2019re at a word boundary:\n/(^|\\W)$[0-9]+(\\.[0-9][0-9])?\\b/ (2.7)\nOne last catch! This pattern allows prices like $199999.99 which would be far too\nexpensive! We need to limit the dollars:\n/(^|\\W)$[0-9]{0,3}(\\.[0-9][0-9])?\\b/ (2.8)\nFurther \ufb01xes (like avoiding matching a dollar sign with no price after it) are left as\nan exercise for the reader.\nHow about disk space? We\u2019ll need to allow for optional fractions again ( 5.5 GB );\nnote the use of ?for making the \ufb01nal soptional, and the use of / */ to mean \u201czero\nor more spaces\u201d since there might always be extra spaces lying around:\n/\\b[0-9]+(\\.[0-9]+)? *(GB|[Gg]igabytes?)\\b/ (2.9)\nModifying this regular expression so that it only matches more than 500 GB is left\nas an exercise for the reader.\n2.1.6 Substitution, Capture Groups, and ELIZA\nAn important use of regular expressions is in substitutions . For example, the substi- substitution\ntution operator s/regexp1/pattern/ used in Python and in Unix commands like\nvimorsedallows a string characterized by a regular expression to be replaced by\nanother string:\ns/colour/color/ (2.10)\nIt is often useful to be able to refer to a particular subpart of the string matching\nthe \ufb01rst pattern. For example, suppose we wanted to put angle brackets around all\nintegers in a text, for example, changing the 35 boxes tothe<35>boxes . We\u2019d\nlike a way to refer to the integer we\u2019ve found so that we can easily add the brackets.\nTo do this, we put parentheses (and)around the \ufb01rst pattern and use the number\noperator\\1in the second pattern to refer back. Here\u2019s how it looks:\ns/([0-9]+)/<\\1>/ (2.11)",
    "metadata": {
      "source": "2",
      "chunk_id": 9,
      "token_count": 691,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9",
    "metadata": {
      "source": "2",
      "chunk_id": 10,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "2.1 \u2022 R EGULAR EXPRESSIONS 9\nThe parenthesis and number operators can also specify that a certain string or ex-\npression must occur twice in the text. For example, suppose we are looking for the\npattern \u201cthe Xer they were, the Xer they will be\u201d, where we want to constrain the two\nX\u2019s to be the same string. We do this by surrounding the \ufb01rst X with the parenthesis\noperator, and replacing the second X with the number operator \\1, as follows:\n/the (.*)er they were, the \\1er they will be/ (2.12)\nHere the\\1will be replaced by whatever string matched the \ufb01rst item in parentheses.\nSo this will match the bigger they were, the bigger they will be but not the bigger\nthey were, the faster they will be .\nThis use of parentheses to store a pattern in memory is called a capture group . capture group\nEvery time a capture group is used (i.e., parentheses surround a pattern), the re-\nsulting match is stored in a numbered register . If you match two different sets of register\nparentheses, \\2means whatever matched the second capture group. Thus\n/the (.*)er they (.*), the \\1er we \\2/ (2.13)\nwill match the faster they ran, the faster we ran but not the faster they ran, the faster\nwe ate . Similarly, the third capture group is stored in \\3, the fourth is \\4, and so on.\nParentheses thus have a double function in regular expressions; they are used\nto group terms for specifying the order in which operators should apply, and they\nare used to capture something in a register. Occasionally we might want to use\nparentheses for grouping, but don\u2019t want to capture the resulting pattern in a register.\nIn that case we use a non-capturing group , which is speci\ufb01ed by putting the specialnon-capturing\ngroup\ncommands ?:after the open parenthesis, in the form (?: pattern ) .\n/(?:some|a few) (people|cats) like some \\1/ (2.14)\nwill match some cats like some cats but not some cats like some some .\nSubstitutions and capture groups are very useful in implementing simple chat-\nbots like ELIZA (Weizenbaum, 1966). Recall that ELIZA simulates a Rogerian\npsychologist by carrying on conversations like the following:\nUser 1: Men are all alike.\nELIZA 1:IN WHAT WAY\nUser 2: They\u2019re always bugging us about something or other.\nELIZA 2:CAN YOU THINK OF A SPECIFIC EXAMPLE\nUser 3: Well, my boyfriend made me come here.\nELIZA 3:YOUR BOYFRIEND MADE YOU COME HERE\nUser 4: He says I\u2019m depressed much of the time.\nELIZA 4:I AM SORRY TO HEAR YOU ARE DEPRESSED\nELIZA works by having a series or cascade of regular expression substitutions\neach of which matches and changes some part of the input lines. After the input\nis uppercased, substitutions change all instances of MYtoYOUR , and I\u2019MtoYOU\nARE, and so on. That way when ELIZA repeats back part of the user utterance, it\nwill seem to be referring correctly to the user. The next set of substitutions matches\nand replaces other patterns in the input. Here are some examples:\ns/.* YOU ARE (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE \\1/\ns/.* YOU ARE (depressed|sad) .*/WHY DO YOU THINK YOU ARE \\1/",
    "metadata": {
      "source": "2",
      "chunk_id": 11,
      "token_count": 776,
      "chapter_title": ""
    }
  },
  {
    "content": "psychologist by carrying on conversations like the following:\nUser 1: Men are all alike.\nELIZA 1:IN WHAT WAY\nUser 2: They\u2019re always bugging us about something or other.\nELIZA 2:CAN YOU THINK OF A SPECIFIC EXAMPLE\nUser 3: Well, my boyfriend made me come here.\nELIZA 3:YOUR BOYFRIEND MADE YOU COME HERE\nUser 4: He says I\u2019m depressed much of the time.\nELIZA 4:I AM SORRY TO HEAR YOU ARE DEPRESSED\nELIZA works by having a series or cascade of regular expression substitutions\neach of which matches and changes some part of the input lines. After the input\nis uppercased, substitutions change all instances of MYtoYOUR , and I\u2019MtoYOU\nARE, and so on. That way when ELIZA repeats back part of the user utterance, it\nwill seem to be referring correctly to the user. The next set of substitutions matches\nand replaces other patterns in the input. Here are some examples:\ns/.* YOU ARE (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE \\1/\ns/.* YOU ARE (depressed|sad) .*/WHY DO YOU THINK YOU ARE \\1/\ns/.* all .*/IN WHAT WAY/\ns/.* always .*/CAN YOU THINK OF A SPECIFIC EXAMPLE/",
    "metadata": {
      "source": "2",
      "chunk_id": 12,
      "token_count": 301,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\nSince multiple substitutions can apply to a given input, substitutions are assigned\na rank and applied in order. Creating patterns is the topic of Exercise 2.3, and we\nreturn to the details of the ELIZA architecture in Chapter 15.\n2.1.7 Lookahead Assertions\nFinally, there will be times when we need to predict the future: look ahead in the\ntext to see if some pattern matches, but not yet advance the pointer we always keep\nto where we are in the text, so that we can then deal with the pattern if it occurs, but\nif it doesn\u2019t we can check for something else instead.\nThese lookahead assertions make use of the (?syntax that we saw in the previ- lookahead\nous section for non-capture groups. The operator (?= pattern) is true ifpattern\noccurs, but is zero-width , i.e. the match pointer doesn\u2019t advance. The operator zero-width\n(?! pattern) only returns true if a pattern does not match, but again is zero-width\nand doesn\u2019t advance the pointer. Negative lookahead is commonly used when we\nare parsing some complex pattern but want to rule out a special case. For example\nsuppose we want to match, at the beginning of a line, any single word that doesn\u2019t\nstart with \u201cV olcano\u201d. We can use negative lookahead to do this:\n/^(?!Volcano)[A-Za-z]+/ (2.15)\n2.2 Words\nBefore we talk about processing words, we need to decide what counts as a word.\nLet\u2019s start by looking at one particular corpus (plural corpora ), a computer-readable corpus\ncorpora collection of text or speech. For example the Brown corpus is a million-word col-\nlection of samples from 500 written English texts from different genres (newspa-\nper, \ufb01ction, non-\ufb01ction, academic, etc.), assembled at Brown University in 1963\u201364\n(Ku\u02c7cera and Francis, 1967). How many words are in the following Brown sentence?\nHe stepped out into the hall, was delighted to encounter\na water brother.\nThis sentence has 13 words if we don\u2019t count punctuation marks as words, 15\nif we count punctuation. Whether we treat period (\u201c .\u201d), comma (\u201c ,\u201d), and so on as\nwords depends on the task. Punctuation is critical for \ufb01nding boundaries of things\n(commas, periods, colons) and for identifying some aspects of meaning (question\nmarks, exclamation marks, quotation marks). For some tasks, like part-of-speech\ntagging or parsing or speech synthesis, we sometimes treat punctuation marks as if\nthey were separate words.\nThe Switchboard corpus of American English telephone conversations between\nstrangers was collected in the early 1990s; it contains 2430 conversations averaging\n6 minutes each, totaling 240 hours of speech and about 3 million words (Godfrey\net al., 1992). Such corpora of spoken language introduce other complications with\nregard to de\ufb01ning words. Let\u2019s look at one utterance from Switchboard; an utter-\nance is the spoken correlate of a sentence: utterance\nI do uh main- mainly business data processing\nThis utterance has two kinds of dis\ufb02uencies . The broken-off word main- is dis\ufb02uency\ncalled a fragment . Words like uhandumare called \ufb01llers or\ufb01lled pauses . Should fragment\n\ufb01lled pause we consider these to be words? Again, it depends on the application. If we are\nbuilding a speech transcription system, we might want to eventually strip out the\ndis\ufb02uencies.",
    "metadata": {
      "source": "2",
      "chunk_id": 13,
      "token_count": 794,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11",
    "metadata": {
      "source": "2",
      "chunk_id": 14,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "2.2 \u2022 W ORDS 11\nBut we also sometimes keep dis\ufb02uencies around. Dis\ufb02uencies like uhorum\nare actually helpful in speech recognition in predicting the upcoming word, because\nthey may signal that the speaker is restarting the clause or idea, and so for speech\nrecognition they are treated as regular words. Because different people use differ-\nent dis\ufb02uencies they can also be a cue to speaker identi\ufb01cation. In fact Clark and\nFox Tree (2002) showed that uhandumhave different meanings. What do you think\nthey are?\nPerhaps most important, in thinking about what is a word, we need to distinguish\ntwo ways of talking about words that will be useful throughout the book. Word types word type\nare the number of distinct words in a corpus; if the set of words in the vocabulary is\nV, the number of types is the vocabulary size jVj. Word instances are the total num- word instance\nberNof running words.1If we ignore punctuation, the following Brown sentence\nhas 14 types and 16 instances:\nThey picnicked by the pool, then lay back on the grass and\nlooked at the stars.\nWe still have decisions to make! For example, should we consider a capitalized\nstring (like They ) and one that is uncapitalized (like they) to be the same word type?\nThe answer is that it depends on the task! They andthey might be lumped together\nas the same type in some tasks, like speech recognition, where we care more about\nthe sequence of words and less about the formatting, while for other tasks, such\nas deciding whether a particular word is a name of a person or location (named-\nentity tagging), capitalization is a useful feature and is retained. Sometimes we keep\naround two versions of a particular NLP model, one with capitalization and one\nwithout capitalization.\nCorpus Types =jVjInstances = N\nShakespeare 31 thousand 884 thousand\nBrown corpus 38 thousand 1 million\nSwitchboard telephone conversations 20 thousand 2.4 million\nCOCA 2 million 440 million\nGoogle n-grams 13 million 1 trillion\nFigure 2.11 Rough numbers of wordform types and instances for some English language\ncorpora. The largest, the Google n-grams corpus, contains 13 million types, but this count\nonly includes types appearing 40 or more times, so the true number would be much larger.\nHow many words are there in English? When we speak about the number of\nwords in the language, we are generally referring to word types. Fig. 2.11 shows\nthe rough numbers of types and instances computed from some English corpora.\nThe larger the corpora we look at, the more word types we \ufb01nd, and in fact this\nrelationship between the number of types jVjand number of instances Nis called\nHerdan\u2019s Law (Herdan, 1960) or Heaps\u2019 Law (Heaps, 1978) after its discoverers Herdan\u2019s Law\nHeaps\u2019 Law (in linguistics and information retrieval respectively). It is shown in Eq. 2.16, where\nkandbare positive constants, and 0 <b<1.\njVj=kNb(2.16)\nThe value of bdepends on the corpus size and the genre, but at least for the large\ncorpora in Fig. 2.11, branges from .67 to .75. Roughly then we can say that the\n1In earlier tradition, and occasionally still, you might see word instances referred to as word tokens , but\nwe now try to reserve the word token instead to mean the output of subword tokenization algorithms.",
    "metadata": {
      "source": "2",
      "chunk_id": 15,
      "token_count": 795,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12",
    "metadata": {
      "source": "2",
      "chunk_id": 16,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "12 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\nvocabulary size for a text goes up signi\ufb01cantly faster than the square root of its\nlength in words.\nIt\u2019s sometimes useful to make a further distinction. Consider in\ufb02ected forms like\ncatsversus cat. We say these two words are different wordforms but have the same\nlemma . Alemma is a set of lexical forms having the same stem, and usually the lemma\nsame major part-of-speech. The wordform is the full in\ufb02ected or derived form of wordform\nthe word. The two wordforms catandcatsthus have the same lemma, which we can\nrepresent as cat.\nFor morphologically complex languages like Arabic, we often need to deal with\nlemmatization. For most tasks in English, however, wordforms are suf\ufb01cient, and\nwhen we talk about words in this book we almost always mean wordforms (although\nwe will discuss basic algorithms for lemmatization and the related task of stemming\nbelow in Section 2.6). One of the situations even in English where we talk about\nlemmas is when we measure the number of words in a dictionary. Dictionary en-\ntries orboldface forms are a very rough approximation to (an upper bound on) the\nnumber of lemmas (since some lemmas have multiple boldface forms). The 1989\nedition of the Oxford English Dictionary had 615,000 entries.\nFinally, we should note that in practice, for many NLP applications (for example\nfor neural language modeling) we don\u2019t actually use words as our internal unit of\nrepresentation at all! We instead tokenize the input strings into tokens , which can\nbe words but can also be only parts of words. We\u2019ll return to this tokenization\nquestion when we introduce the BPE algorithm in Section 2.5.2.\n2.3 Corpora\nWords don\u2019t appear out of nowhere. Any particular piece of text that we study\nis produced by one or more speci\ufb01c speakers or writers, in a speci\ufb01c dialect of a\nspeci\ufb01c language, at a speci\ufb01c time, in a speci\ufb01c place, for a speci\ufb01c function.\nPerhaps the most important dimension of variation is the language. NLP algo-\nrithms are most useful when they apply across many languages. The world has 7097\nlanguages at the time of this writing, according to the online Ethnologue catalog\n(Simons and Fennig, 2018). It is important to test algorithms on more than one lan-\nguage, and particularly on languages with different properties; by contrast there is\nan unfortunate current tendency for NLP algorithms to be developed or tested just\non English (Bender, 2019). Even when algorithms are developed beyond English,\nthey tend to be developed for the of\ufb01cial languages of large industrialized nations\n(Chinese, Spanish, Japanese, German etc.), but we don\u2019t want to limit tools to just\nthese few languages. Furthermore, most languages also have multiple varieties, of-\nten spoken in different regions or by different social groups. Thus, for example,\nif we\u2019re processing text that uses features of African American English ( AAE ) or AAE\nAfrican American Vernacular English (AA VE)\u2014the variations of English used by\nmillions of people in African American communities (King 2020)\u2014we must use\nNLP tools that function with features of those varieties. Twitter posts might use fea-\ntures often used by speakers of African American English, such as constructions like",
    "metadata": {
      "source": "2",
      "chunk_id": 17,
      "token_count": 765,
      "chapter_title": ""
    }
  },
  {
    "content": "Perhaps the most important dimension of variation is the language. NLP algo-\nrithms are most useful when they apply across many languages. The world has 7097\nlanguages at the time of this writing, according to the online Ethnologue catalog\n(Simons and Fennig, 2018). It is important to test algorithms on more than one lan-\nguage, and particularly on languages with different properties; by contrast there is\nan unfortunate current tendency for NLP algorithms to be developed or tested just\non English (Bender, 2019). Even when algorithms are developed beyond English,\nthey tend to be developed for the of\ufb01cial languages of large industrialized nations\n(Chinese, Spanish, Japanese, German etc.), but we don\u2019t want to limit tools to just\nthese few languages. Furthermore, most languages also have multiple varieties, of-\nten spoken in different regions or by different social groups. Thus, for example,\nif we\u2019re processing text that uses features of African American English ( AAE ) or AAE\nAfrican American Vernacular English (AA VE)\u2014the variations of English used by\nmillions of people in African American communities (King 2020)\u2014we must use\nNLP tools that function with features of those varieties. Twitter posts might use fea-\ntures often used by speakers of African American English, such as constructions like\niont(I don\u2019t in Mainstream American English ( MAE )), or talmbout corresponding MAE\nto MAE talking about , both examples that in\ufb02uence word segmentation (Blodgett\net al. 2016, Jones 2015).\nIt\u2019s also quite common for speakers or writers to use multiple languages in a\nsingle communicative act, a phenomenon called code switching . Code switching code switching",
    "metadata": {
      "source": "2",
      "chunk_id": 18,
      "token_count": 371,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\n2.4 \u2022 S IMPLE UNIXTOOLS FOR WORD TOKENIZATION 13\nis enormously common across the world; here are examples showing Spanish and\n(transliterated) Hindi code switching with English (Solorio et al. 2014, Jurgens et al.\n2017):\n(2.17) Por primera vez veo a @username actually being hateful! it was beautiful:)\n[For the \ufb01rst time I get to see @username actually being hateful! it was\nbeautiful:) ]\n(2.18) dost tha or ra- hega ... dont wory ... but dherya rakhe\n[\u201che was and will remain a friend ... don\u2019t worry ... but have faith\u201d]\nAnother dimension of variation is the genre. The text that our algorithms must\nprocess might come from newswire, \ufb01ction or non-\ufb01ction books, scienti\ufb01c articles,\nWikipedia, or religious texts. It might come from spoken genres like telephone\nconversations, business meetings, police body-worn cameras, medical interviews,\nor transcripts of television shows or movies. It might come from work situations\nlike doctors\u2019 notes, legal text, or parliamentary or congressional proceedings.\nText also re\ufb02ects the demographic characteristics of the writer (or speaker): their\nage, gender, race, socioeconomic class can all in\ufb02uence the linguistic properties of\nthe text we are processing.\nAnd \ufb01nally, time matters too. Language changes over time, and for some lan-\nguages we have good corpora of texts from different historical periods.\nBecause language is so situated, when developing computational models for lan-\nguage processing from a corpus, it\u2019s important to consider who produced the lan-\nguage, in what context, for what purpose. How can a user of a dataset know all these\ndetails? The best way is for the corpus creator to build a datasheet (Gebru et al., datasheet\n2020) or data statement (Bender et al., 2021) for each corpus. A datasheet speci\ufb01es\nproperties of a dataset like:\nMotivation : Why was the corpus collected, by whom, and who funded it?\nSituation : When and in what situation was the text written/spoken? For example,\nwas there a task? Was the language originally spoken conversation, edited\ntext, social media communication, monologue vs. dialogue?\nLanguage variety : What language (including dialect/region) was the corpus in?\nSpeaker demographics : What was, e.g., the age or gender of the text\u2019s authors?\nCollection process : How big is the data? If it is a subsample how was it sampled?\nWas the data collected with consent? How was the data pre-processed, and\nwhat metadata is available?\nAnnotation process : What are the annotations, what are the demographics of the\nannotators, how were they trained, how was the data annotated?\nDistribution : Are there copyright or other intellectual property restrictions?\n2.4 Simple Unix Tools for Word Tokenization\nBefore almost any natural language processing of a text, the text has to be normal-\nized, a task called text normalization . At least three tasks are commonly applied astext\nnormalization\npart of any normalization process:\n1. Tokenizing (segmenting) words\n2. Normalizing word formats\n3. Segmenting sentences",
    "metadata": {
      "source": "2",
      "chunk_id": 19,
      "token_count": 698,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n14 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\nIn the next sections we walk through each of these tasks, but we\u2019ll \ufb01rst start with\nan easy, if somewhat naive version of word tokenization and normalization (and fre-\nquency computation) that can be accomplished for English solely in a single Unix\ncommand-line, inspired by Church (1994). We\u2019ll make use of some Unix com-\nmands:tr, used to systematically change particular characters in the input; sort ,\nwhich sorts input lines in alphabetical order; and uniq , which collapses and counts\nadjacent identical lines.\nFor example let\u2019s begin with the \u2018complete words\u2019 of Shakespeare in one \ufb01le,\nsh.txt . We can use trto tokenize the words by changing every sequence of non-\nalphabetic characters to a newline (\u2019A-Za-z\u2019 means alphabetic and the -c option\ncomplements to non-alphabet, so together they mean to change every non-alphabetic\ncharacter into a newline. The -s (\u2018squeeze\u2019) option is used to replace the result\nof multiple consecutive changes into a single output, so a series of non-alphabetic\ncharacters in a row would all be \u2018squeezed\u2019 into a single newline):\ntr -sc 'A-Za-z ' '\\n'< sh.txt\nThe output of this command will be:\nTHE\nSONNETS\nby\nWilliam\nShakespeare\nFrom\nfairest\ncreatures\nWe\n...\nNow that there is one word per line, we can sort the lines, and pass them to uniq\n-cwhich will collapse and count them:\ntr -sc 'A-Za-z' '\\n' < sh.txt | sort | uniq -c\nwith the following output:\n1945 A\n72 AARON\n19 ABBESS\n25 Aaron\n6 Abate\n1 Abates\n5 Abbess\n6 Abbey\n3 Abbot\n...\nAlternatively, we can collapse all the upper case to lower case:\ntr -sc 'A-Za-z ' '\\n'< sh.txt | tr A-Z a-z | sort | uniq -c\nwhose output is\n14725 a\n97 aaron\n1 abaissiez\n10 abandon",
    "metadata": {
      "source": "2",
      "chunk_id": 20,
      "token_count": 465,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\n2.5 \u2022 W ORD AND SUBWORD TOKENIZATION 15\n2 abandoned\n2 abase\n1 abash\n14 abate\n3 abated\n3 abatement\n...\nNow we can sort again to \ufb01nd the frequent words. The -noption tosort means\nto sort numerically rather than alphabetically, and the -roption means to sort in\nreverse order (highest-to-lowest):\ntr -sc 'A-Za-z ' '\\n'< sh.txt | tr A-Z a-z | sort | uniq -c | sort -n -r\nThe results show that the most frequent words in Shakespeare, as in any other\ncorpus, are the short function words like articles, pronouns, prepositions:\n27378 the\n26084 and\n22538 i\n19771 to\n17481 of\n14725 a\n13826 you\n12489 my\n11318 that\n11112 in\n...\nUnix tools of this sort can be very handy in building quick word count statistics\nfor any corpus in English. While in some versions of Unix these command-line tools\nalso correctly handle Unicode characters and so can be used for many languages,\nin general for handling most languages outside English we use more sophisticated\ntokenization algorithms.\n2.5 Word and Subword Tokenization\nThe simple Unix tools above were \ufb01ne for getting rough word statistics but more\nsophisticated algorithms are generally necessary for tokenization , the task of seg- tokenization\nmenting running text into words. There are roughly two classes of tokenization\nalgorithms. In top-down tokenization, we de\ufb01ne a standard and implement rules to\nimplement that kind of tokenization.\nBut more commonly instead of using words as the input to NLP algorithms we\nbreak up words into subword tokens , which can be words or parts of words or subword tokens\neven individual letters. These are derived via bottom-up tokenization, in which we\nuse simple statistics of letter sequences to come up with the vocabulary of subword\ntokens, and break up the input into those subwords.\n2.5.1 Top-down (rule-based) tokenization\nWhile the Unix command sequence just removed all the numbers and punctuation,\nfor most NLP applications we\u2019ll need to keep these in our tokenization. We often",
    "metadata": {
      "source": "2",
      "chunk_id": 21,
      "token_count": 479,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16",
    "metadata": {
      "source": "2",
      "chunk_id": 22,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "16 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\nwant to break off punctuation as a separate token; commas are a useful piece of infor-\nmation for parsers, and periods help indicate sentence boundaries. But we\u2019ll often\nwant to keep the punctuation that occurs word internally, in examples like m.p.h. ,\nPh.D. ,AT&T , and cap\u2019n . Special characters and numbers will need to be kept in\nprices ($45.55) and dates ( 01/02/06 ); we don\u2019t want to segment that price into sepa-\nrate tokens of \u201c45\u201d and \u201c55\u201d. And there are URLs ( https://www.stanford.edu ),\nTwitter hashtags ( #nlproc ), or email addresses ( someone@cs.colorado.edu ).\nNumber expressions introduce complications; in addition to appearing at word\nboundaries, commas appear inside numbers in English, every three digits: 555,500.50 .\nTokenization differs by language; languages like Spanish, French, and German, for\nexample, use a comma to mark the decimal point, and spaces (or sometimes periods)\nwhere English puts commas, for example, 555 500,50 .\nA tokenizer can also be used to expand clitic contractions that are marked by clitic\napostrophes, converting what're to the two tokens what are , andwe're towe\nare. A clitic is a part of a word that can\u2019t stand on its own, and can only occur\nwhen it is attached to another word. Such contractions occur in other alphabetic\nlanguages, including French pronouns ( j'ai and articles l'homme ).\nDepending on the application, tokenization algorithms may also tokenize mul-\ntiword expressions like New York orrock 'n' roll as a single token, which re-\nquires a multiword expression dictionary of some sort. Tokenization is thus inti-\nmately tied up with named entity recognition , the task of detecting names, dates,\nand organizations (Chapter 17).\nOne commonly used tokenization standard is known as the Penn Treebank to-\nkenization standard, used for the parsed corpora (treebanks) released by the Lin-Penn Treebank\ntokenization\nguistic Data Consortium (LDC), the source of many useful datasets. This standard\nseparates out clitics ( doesn\u2019t becomes does plus n\u2019t), keeps hyphenated words to-\ngether, and separates out all punctuation (to save space we\u2019re showing visible spaces\n\u2018\u2019 between tokens, although newlines is a more common output):\nInput :\"The San Francisco-based restaurant,\" they said,\n\"doesn't charge $10\".\nOutput :\"TheSanFrancisco-based restaurant ,\"theysaid,\n\"doesn'tcharge $10\".\nIn practice, since tokenization is run before any other language processing, it\nneeds to be very fast. For word tokenization we generally use deterministic algo-\nrithms based on regular expressions compiled into ef\ufb01cient \ufb01nite state automata.\nFor example, Fig. 2.12 shows a basic regular expression that can be used to tok-\nenize English with the nltk.regexp tokenize function of the Python-based Nat-\nural Language Toolkit (NLTK) (Bird et al. 2009; https://www.nltk.org ).\nCarefully designed deterministic algorithms can deal with the ambiguities that\narise, such as the fact that the apostrophe needs to be tokenized differently when used\nas a genitive marker (as in the book\u2019s cover ), a quotative as in \u2018The other class\u2019, she\nsaid, or in clitics like they\u2019re .\nWord tokenization is more complex in languages like written Chinese, Japanese,",
    "metadata": {
      "source": "2",
      "chunk_id": 23,
      "token_count": 760,
      "chapter_title": ""
    }
  },
  {
    "content": "separates out clitics ( doesn\u2019t becomes does plus n\u2019t), keeps hyphenated words to-\ngether, and separates out all punctuation (to save space we\u2019re showing visible spaces\n\u2018\u2019 between tokens, although newlines is a more common output):\nInput :\"The San Francisco-based restaurant,\" they said,\n\"doesn't charge $10\".\nOutput :\"TheSanFrancisco-based restaurant ,\"theysaid,\n\"doesn'tcharge $10\".\nIn practice, since tokenization is run before any other language processing, it\nneeds to be very fast. For word tokenization we generally use deterministic algo-\nrithms based on regular expressions compiled into ef\ufb01cient \ufb01nite state automata.\nFor example, Fig. 2.12 shows a basic regular expression that can be used to tok-\nenize English with the nltk.regexp tokenize function of the Python-based Nat-\nural Language Toolkit (NLTK) (Bird et al. 2009; https://www.nltk.org ).\nCarefully designed deterministic algorithms can deal with the ambiguities that\narise, such as the fact that the apostrophe needs to be tokenized differently when used\nas a genitive marker (as in the book\u2019s cover ), a quotative as in \u2018The other class\u2019, she\nsaid, or in clitics like they\u2019re .\nWord tokenization is more complex in languages like written Chinese, Japanese,\nand Thai, which do not use spaces to mark potential word-boundaries. In Chinese,\nfor example, words are composed of characters (called hanzi in Chinese). Each hanzi\ncharacter generally represents a single unit of meaning (called a morpheme ) and is\npronounceable as a single syllable. Words are about 2.4 characters long on average.\nBut deciding what counts as a word in Chinese is complex. For example, consider\nthe following sentence:",
    "metadata": {
      "source": "2",
      "chunk_id": 24,
      "token_count": 381,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\n2.5 \u2022 W ORD AND SUBWORD TOKENIZATION 17\n>>> text = 'That U.S.A. poster-print costs $12.40... '\n>>> pattern = r '''(?x) # set flag to allow verbose regexps\n... (?:[A-Z]\\.)+ # abbreviations, e.g. U.S.A.\n... | \\w+(?:-\\w+)* # words with optional internal hyphens\n... | \\$?\\d+(?:\\.\\d+)?%? # currency, percentages, e.g. $12.40, 82%\n... | \\.\\.\\. # ellipsis\n... | [][.,;\" '?():_ `-] # these are separate tokens; includes ], [\n... '''\n>>> nltk.regexp_tokenize(text, pattern)\n['That ','U.S.A. ','poster-print ','costs ','$12.40 ','...']\nFigure 2.12 A Python trace of regular expression tokenization in the NLTK Python-based\nnatural language processing toolkit (Bird et al., 2009), commented for readability; the (?x)\nverbose \ufb02ag tells Python to strip comments and whitespace. Figure from Chapter 3 of Bird\net al. (2009).\n(2.19)\u59da\u660e\u8fdb\u5165\u603b\u51b3\u8d5b y\u00b4ao m \u00b4\u0131ng j`\u0131n r`u z\u02c7ong ju \u00b4e s`ai\n\u201cYao Ming reaches the \ufb01nals\u201d\nAs Chen et al. (2017) point out, this could be treated as 3 words (\u2018Chinese Treebank\u2019\nsegmentation):\n(2.20)\u59da\u660e\nYaoMing\u8fdb\u5165\nreaches\u603b\u51b3\u8d5b\n\ufb01nals\nor as 5 words (\u2018Peking University\u2019 segmentation):\n(2.21)\u59da\nYao\u660e\nMing\u8fdb\u5165\nreaches\u603b\noverall\u51b3\u8d5b\n\ufb01nals\nFinally, it is possible in Chinese simply to ignore words altogether and use characters\nas the basic elements, treating the sentence as a series of 7 characters:\n(2.22)\u59da\nYao\u660e\nMing\u8fdb\nenter\u5165\nenter\u603b\noverall\u51b3\ndecision\u8d5b\ngame\nIn fact, for most Chinese NLP tasks it turns out to work better to take characters\nrather than words as input, since characters are at a reasonable semantic level for\nmost applications, and since most word standards, by contrast, result in a huge vo-\ncabulary with large numbers of very rare words (Li et al., 2019).\nHowever, for Japanese and Thai the character is too small a unit, and so algo-\nrithms for word segmentation are required. These can also be useful for Chineseword\nsegmentation\nin the rare situations where word rather than character boundaries are required. For\nthese situations we can use the subword tokenization algorithms introduced in the\nnext section.\n2.5.2 Byte-Pair Encoding: A Bottom-up Tokenization Algorithm\nThere is a third option to tokenizing text, one that is most commonly used by large\nlanguage models. Instead of de\ufb01ning tokens as words (whether delimited by spaces\nor more complex algorithms), or as characters (as in Chinese), we can use our data to\nautomatically tell us what the tokens should be. This is especially useful in dealing\nwith unknown words, an important problem in language processing. As we will\nsee in the next chapter, NLP algorithms often learn some facts about language from\none corpus (a training corpus) and then use these facts to make decisions about a\nseparate testcorpus and its language. Thus if our training corpus contains, say the",
    "metadata": {
      "source": "2",
      "chunk_id": 25,
      "token_count": 778,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18",
    "metadata": {
      "source": "2",
      "chunk_id": 26,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "18 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\nwords low,new,newer , but not lower , then if the word lower appears in our test\ncorpus, our system will not know what to do with it.\nTo deal with this unknown word problem, modern tokenizers automatically in-\nduce sets of tokens that include tokens smaller than words, called subwords . Sub- subwords\nwords can be arbitrary substrings, or they can be meaning-bearing units like the\nmorphemes -estor-er. (A morpheme is the smallest meaning-bearing unit of a lan-\nguage; for example the word unwashable has the morphemes un-,wash , and -able .)\nIn modern tokenization schemes, most tokens are words, but some tokens are fre-\nquently occurring morphemes or other subwords like -er. Every unseen word like\nlower can thus be represented by some sequence of known subword units, such as\nlowander, or even as a sequence of individual letters if necessary.\nMost tokenization schemes have two parts: a token learner , and a token seg-\nmenter . The token learner takes a raw training corpus (sometimes roughly pre-\nseparated into words, for example by whitespace) and induces a vocabulary, a set\nof tokens. The token segmenter takes a raw test sentence and segments it into the\ntokens in the vocabulary. Two algorithms are widely used: byte-pair encoding\n(Sennrich et al., 2016), and unigram language modeling (Kudo, 2018), There is\nalso a SentencePiece library that includes implementations of both of these (Kudo\nand Richardson, 2018), and people often use the name SentencePiece to simply\nmean unigram language modeling tokenization.\nIn this section we introduce the simplest of the three, the byte-pair encoding or\nBPE algorithm (Sennrich et al., 2016); see Fig. 2.13. The BPE token learner begins BPE\nwith a vocabulary that is just the set of all individual characters. It then examines the\ntraining corpus, chooses the two symbols that are most frequently adjacent (say \u2018A\u2019,\n\u2018B\u2019), adds a new merged symbol \u2018AB\u2019 to the vocabulary, and replaces every adjacent\n\u2019A\u2019 \u2019B\u2019 in the corpus with the new \u2018AB\u2019. It continues to count and merge, creating\nnew longer and longer character strings, until kmerges have been done creating\nknovel tokens; kis thus a parameter of the algorithm. The resulting vocabulary\nconsists of the original set of characters plus knew symbols.\nThe algorithm is usually run inside words (not merging across word boundaries),\nso the input corpus is \ufb01rst white-space-separated to give a set of strings, each corre-\nsponding to the characters of a word, plus a special end-of-word symbol , and its\ncounts. Let\u2019s see its operation on the following tiny input corpus of 18 word tokens\nwith counts for each word (the word lowappears 5 times, the word newer 6 times,\nand so on), which would have a starting vocabulary of 11 letters:\ncorpus vocabulary\n5l o w , d, e, i, l, n, o, r, s, t, w\n2l o w e s t\n6n e w e r\n3w i d e r\n2n e w\nThe BPE algorithm \ufb01rst counts all pairs of adjacent symbols: the most frequent\nis the pair e rbecause it occurs in newer (frequency of 6) and wider (frequency of\n3) for a total of 9 occurrences.2We then merge these symbols, treating eras one\nsymbol, and count again:",
    "metadata": {
      "source": "2",
      "chunk_id": 27,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "new longer and longer character strings, until kmerges have been done creating\nknovel tokens; kis thus a parameter of the algorithm. The resulting vocabulary\nconsists of the original set of characters plus knew symbols.\nThe algorithm is usually run inside words (not merging across word boundaries),\nso the input corpus is \ufb01rst white-space-separated to give a set of strings, each corre-\nsponding to the characters of a word, plus a special end-of-word symbol , and its\ncounts. Let\u2019s see its operation on the following tiny input corpus of 18 word tokens\nwith counts for each word (the word lowappears 5 times, the word newer 6 times,\nand so on), which would have a starting vocabulary of 11 letters:\ncorpus vocabulary\n5l o w , d, e, i, l, n, o, r, s, t, w\n2l o w e s t\n6n e w e r\n3w i d e r\n2n e w\nThe BPE algorithm \ufb01rst counts all pairs of adjacent symbols: the most frequent\nis the pair e rbecause it occurs in newer (frequency of 6) and wider (frequency of\n3) for a total of 9 occurrences.2We then merge these symbols, treating eras one\nsymbol, and count again:\n2Note that there can be ties; we could have instead chosen to merge r \ufb01rst, since that also has a\nfrequency of 9.",
    "metadata": {
      "source": "2",
      "chunk_id": 28,
      "token_count": 312,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\n2.5 \u2022 W ORD AND SUBWORD TOKENIZATION 19\ncorpus vocabulary\n5l o w , d, e, i, l, n, o, r, s, t, w, er\n2l o w e s t\n6n e w er\n3w i d er\n2n e w\nNow the most frequent pair is er , which we merge; our system has learned\nthat there should be a token for word-\ufb01nal er, represented as er:\ncorpus vocabulary\n5l o w ,d,e,i,l,n,o,r,s,t,w,er,er\n2l o w e s t\n6n e w er\n3w i d er\n2n e w\nNextn e(total count of 8) get merged to ne:\ncorpus vocabulary\n5l o w ,d,e,i,l,n,o,r,s,t,w,er,er,ne\n2l o w e s t\n6ne w er\n3w i d er\n2ne w\nIf we continue, the next merges are:\nmerge current vocabulary\n(ne, w) ,d,e,i,l,n,o,r,s,t,w,er,er,ne,new\n(l, o) ,d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo\n(lo, w) ,d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low\n(new, er ) ,d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer\n(low,) ,d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer ,low\nfunction BYTE-PAIR ENCODING (strings C, number of merges k)returns vocab V\nV all unique characters in C # initial set of tokens is characters\nfori= 1tokdo # merge tokens ktimes\ntL,tR Most frequent pair of adjacent tokens in C\ntNEW tL+tR # make new token by concatenating\nV V+tNEW # update the vocabulary\nReplace each occurrence of tL,tRinCwith tNEW # and update the corpus\nreturn V\nFigure 2.13 The token learner part of the BPE algorithm for taking a corpus broken up\ninto individual characters or bytes, and learning a vocabulary by iteratively merging tokens.\nFigure adapted from Bostrom and Durrett (2020).\nOnce we\u2019ve learned our vocabulary, the token segmenter is used to tokenize a\ntest sentence. The token segmenter just runs on the merges we have learned from\nthe training data on the test data. It runs them greedily, in the order we learned them.\n(Thus the frequencies in the test data don\u2019t play a role, just the frequencies in the\ntraining data). So \ufb01rst we segment each test sentence word into characters. Then\nwe apply the \ufb01rst rule: replace every instance of e rin the test corpus with er, and",
    "metadata": {
      "source": "2",
      "chunk_id": 29,
      "token_count": 638,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\n20 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\nthen the second rule: replace every instance of er in the test corpus with er,\nand so on. By the end, if the test corpus contained the character sequence n e w e\nr, it would be tokenized as a full word. But the characters of a new (unknown)\nword like l o w e r would be merged into the two tokens lower .\nOf course in real settings BPE is run with many thousands of merges on a very\nlarge input corpus. The result is that most words will be represented as full symbols,\nand only the very rare words (and unknown words) will have to be represented by\ntheir parts.\n2.6 Word Normalization, Lemmatization and Stemming\nWord normalization is the task of putting words or tokens in a standard format. The normalization\nsimplest case of word normalization is case folding . Mapping everything to lower case folding\ncase means that Woodchuck andwoodchuck are represented identically, which is\nvery helpful for generalization in many tasks, such as information retrieval or speech\nrecognition. For sentiment analysis and other text classi\ufb01cation tasks, information\nextraction, and machine translation, by contrast, case can be quite helpful and case\nfolding is generally not done. This is because maintaining the difference between,\nfor example, USthe country and usthe pronoun can outweigh the advantage in\ngeneralization that case folding would have provided for other words. Sometimes\nwe produce both cased (i.e. including both upper and lower case words or tokens)\nand uncased versions of language models.\nSystems that use BPE or other kinds of bottom-up tokenization may do no fur-\nther word normalization. In other NLP systems, we may want to do further nor-\nmalizations, like choosing a single normal form for words with multiple forms like\nUSA andUSoruh-huh anduhhuh . This standardization may be valuable, despite\nthe spelling information that is lost in the normalization process. For information\nretrieval or information extraction about the US, we might want to see information\nfrom documents whether they mention the USor theUSA.\n2.6.1 Lemmatization\nFor other natural language processing situations we also want two morphologically\ndifferent forms of a word to behave similarly. For example in web search, someone\nmay type the string woodchucks but a useful system might want to also return pages\nthat mention woodchuck with no s. This is especially common in morphologically\ncomplex languages like Polish, where for example the word Warsaw has different\nendings when it is the subject ( Warszawa ), or after a preposition like \u201cin Warsaw\u201d ( w\nWarszawie ), or \u201cto Warsaw\u201d ( do Warszawy ), and so on. Lemmatization is the task lemmatization\nof determining that two words have the same root, despite their surface differences.\nThe words am,are, and ishave the shared lemma be; the words dinner anddinners\nboth have the lemma dinner . Lemmatizing each of these forms to the same lemma\nwill let us \ufb01nd all mentions of words in Polish like Warsaw . The lemmatized form\nof a sentence like He is reading detective stories would thus be He be read detective\nstory .\nHow is lemmatization done? The most sophisticated methods for lemmatization\ninvolve complete morphological parsing of the word. Morphology is the study of\nthe way words are built up from smaller meaning-bearing units called morphemes . morpheme\nTwo broad classes of morphemes can be distinguished: stems \u2014the central mor- stem",
    "metadata": {
      "source": "2",
      "chunk_id": 30,
      "token_count": 771,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21\n\n2.7 \u2022 S ENTENCE SEGMENTATION 21\npheme of the word, supplying the main meaning\u2014and af\ufb01xes \u2014adding \u201cadditional\u201d af\ufb01x\nmeanings of various kinds. So, for example, the word foxconsists of one morpheme\n(the morpheme fox) and the word cats consists of two: the morpheme catand the\nmorpheme -s. A morphological parser takes a word like cats and parses it into the\ntwo morphemes catands, or parses a Spanish word like amaren (\u2018if in the future\nthey would love\u2019) into the morpheme amar \u2018to love\u2019, and the morphological features\n3PL (third person plural) and future subjunctive .\nStemming: The Porter Stemmer\nLemmatization algorithms can be complex. For this reason we sometimes make\nuse of a simpler but cruder method, which mainly consists of chopping off word-\n\ufb01nal af\ufb01xes. This naive version of morphological analysis is called stemming . For stemming\nexample, the classic Porter stemmer (Porter, 1980), when applied to the following Porter stemmer\nparagraph:\nThis was not the map we found in Billy Bones's chest, but\nan accurate copy, complete in all things-names and heights\nand soundings-with the single exception of the red crosses\nand the written notes.\nproduces the following stemmed output:\nThi wa not the map we found in Billi Bone s chest but an\naccur copi complet in all thing name and height and sound\nwith the singl except of the red cross and the written note\nThe algorithm is based on rewrite rules run in series, with the output of each pass\nfed as input to the next pass. Some sample rules (more at https://tartarus.org/\nmartin/PorterStemmer/ ):\nATIONAL!ATE (e.g., relational !relate)\nING!\u000fif the stem contains a vowel (e.g., motoring !motor)\nSSES!SS (e.g., grasses !grass)\nSimple stemmers can be useful in cases where we need to collapse across dif-\nferent variants of the same lemma. Nonetheless, they are less commonly used in\nmodern systems since they commit errors of both over-generalizing (lemmatizing\npolicy topolice ) and under-generalizing (not lemmatizing European toEurope )\n(Krovetz, 1993).\n2.7 Sentence Segmentation\nSentence segmentation is another important step in text processing. The most use-sentence\nsegmentation\nful cues for segmenting a text into sentences are punctuation, like periods, question\nmarks, and exclamation points. Question marks and exclamation points are rela-\ntively unambiguous markers of sentence boundaries. Periods, on the other hand, are\nmore ambiguous. The period character \u201c.\u201d is ambiguous between a sentence bound-\nary marker and a marker of abbreviations like Mr.orInc.The previous sentence that\nyou just read showed an even more complex case of this ambiguity, in which the \ufb01nal\nperiod of Inc. marked both an abbreviation and the sentence boundary marker. For\nthis reason, sentence tokenization and word tokenization may be addressed jointly.",
    "metadata": {
      "source": "2",
      "chunk_id": 31,
      "token_count": 674,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22\n\n22 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\nIn general, sentence tokenization methods work by \ufb01rst deciding (based on rules\nor machine learning) whether a period is part of the word or is a sentence-boundary\nmarker. An abbreviation dictionary can help determine whether the period is part\nof a commonly used abbreviation; the dictionaries can be hand-built or machine-\nlearned (Kiss and Strunk, 2006), as can the \ufb01nal sentence splitter. In the Stanford\nCoreNLP toolkit (Manning et al., 2014), for example sentence splitting is rule-based,\na deterministic consequence of tokenization; a sentence ends when a sentence-ending\npunctuation (., !, or ?) is not already grouped with other characters into a token (such\nas for an abbreviation or number), optionally followed by additional \ufb01nal quotes or\nbrackets.\n2.8 Minimum Edit Distance\nMuch of natural language processing is concerned with measuring how similar two\nstrings are. For example in spelling correction, the user typed some erroneous\nstring\u2014let\u2019s say graffe \u2013and we want to know what the user meant. The user prob-\nably intended a word that is similar to graffe . Among candidate similar words,\nthe wordgiraffe , which differs by only one letter from graffe , seems intuitively\nto be more similar than, say grail orgraf , which differ in more letters. Another\nexample comes from coreference , the task of deciding whether two strings such as\nthe following refer to the same entity:\nStanford Arizona Cactus Garden\nStanford University Arizona Cactus Garden\nAgain, the fact that these two strings are very similar (differing by only one word)\nseems like useful evidence for deciding that they might be coreferent. Finally, string\nsimilarity is commonly used to measure the quality of the transcription produced by\na speech recognition system, by asking how similar (in words) the transcript is to a\nreference transcript. A system whose transcript is off by many words is measurably\nworse than one which is only off by a few words.\nEdit distance gives us a way to quantify these intuitions about string similarity.\nMore formally, the minimum edit distance between two strings is de\ufb01ned as theminimum edit\ndistance\nminimum number of editing operations (operations like insertion, deletion, substitu-\ntion) needed to transform one string into another.\nThe gap between intention andexecution , for example, is 5 (delete an i, substi-\ntuteeforn, substitute xfort, insertc, substitute uforn). It\u2019s much easier to see\nthis by looking at the most important visualization for string distances, an alignment alignment\nbetween the two strings, shown in Fig. 2.14. Given two sequences, an alignment is\na correspondence between substrings of the two sequences. Thus, we say Ialigns\nwith the empty string, NwithE, and so on. Beneath the aligned strings is another\nrepresentation; a series of symbols expressing an operation list for converting the\ntop string into the bottom string: dfor deletion, sfor substitution, ifor insertion.\nWe can also assign a particular cost or weight to each of these operations. The\nLevenshtein distance between two sequences is the simplest weighting factor in\nwhich each of the three operations has a cost of 1 (Levenshtein, 1966)\u2014we assume\nthat the substitution of a letter for itself, for example, tfort, has zero cost. The Lev-\nenshtein distance between intention andexecution is 5. Levenshtein also proposed\nan alternative version of his metric in which each insertion or deletion has a cost of\n1 and substitutions are not allowed. (This is equivalent to allowing substitution, but",
    "metadata": {
      "source": "2",
      "chunk_id": 32,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23\n\n2.8 \u2022 M INIMUM EDITDISTANCE 23\nINTE*NTION\njjjjjjjjjj\n*EXECUTION\nd s s i s\nFigure 2.14 Representing the minimum edit distance between two strings as an alignment .\nThe \ufb01nal row gives the operation list for converting the top string into the bottom string: d for\ndeletion, s for substitution, i for insertion.\ngiving each substitution a cost of 2 since any substitution can be represented by one\ninsertion and one deletion). Using this version, the Levenshtein distance between\nintention andexecution is 8.\n2.8.1 The Minimum Edit Distance Algorithm\nHow do we \ufb01nd the minimum edit distance? We can think of this as a search task, in\nwhich we are searching for the shortest path\u2014a sequence of edits\u2014from one string\nto another.\nn t e n t i o ni n t e c n t i o ni n x e n t i o ndelinssubsti n t e n t i o n\nFigure 2.15 Finding the edit distance viewed as a search problem\nThe space of all possible edits is enormous, so we can\u2019t search naively. However,\nlots of distinct edit paths will end up in the same state (string), so rather than recom-\nputing all those paths, we could just remember the shortest path to a state each time\nwe saw it. We can do this by using dynamic programming . Dynamic programmingdynamic\nprogramming\nis the name for a class of algorithms, \ufb01rst introduced by Bellman (1957), that apply\na table-driven method to solve problems by combining solutions to subproblems.\nSome of the most commonly used algorithms in natural language processing make\nuse of dynamic programming, such as the Viterbi algorithm (Chapter 17) and the\nCKY algorithm for parsing (Chapter 18).\nThe intuition of a dynamic programming problem is that a large problem can\nbe solved by properly combining the solutions to various subproblems. Consider\nthe shortest path of transformed words that represents the minimum edit distance\nbetween the strings intention andexecution shown in Fig. 2.16.\nImagine some string (perhaps it is exention ) that is in this optimal path (whatever\nit is). The intuition of dynamic programming is that if exention is in the optimal\noperation list, then the optimal sequence must also include the optimal path from\nintention toexention . Why? If there were a shorter path from intention toexention ,\nthen we could use it instead, resulting in a shorter overall path, and the optimal\nsequence wouldn\u2019t be optimal, thus leading to a contradiction.\nThe minimum edit distance algorithm was named by Wagner and Fischerminimum edit\ndistance\nalgorithm(1974) but independently discovered by many people (see the Historical Notes sec-\ntion of Chapter 17).\nLet\u2019s \ufb01rst de\ufb01ne the minimum edit distance between two strings. Given two\nstrings, the source string Xof length n, and target string Yof length m, we\u2019ll de\ufb01ne",
    "metadata": {
      "source": "2",
      "chunk_id": 33,
      "token_count": 640,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24\n\n24 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\nn t e n t i o ni n t e n t i o n\ne t e n t i o n\ne x e n t i o n\ne x e n u t i o n\ne x e c u t i o ndelete i\nsubstitute n by e\nsubstitute t by x\ninsert u\nsubstitute n by c\nFigure 2.16 Path from intention toexecution .\nD[i;j]as the edit distance between X[1::i]andY[1::j], i.e., the \ufb01rst icharacters of X\nand the \ufb01rst jcharacters of Y. The edit distance between XandYis thus D[n;m].\nWe\u2019ll use dynamic programming to compute D[n;m]bottom up, combining so-\nlutions to subproblems. In the base case, with a source substring of length ibut an\nempty target string, going from icharacters to 0 requires ideletes. With a target\nsubstring of length jbut an empty source going from 0 characters to jcharacters\nrequires jinserts. Having computed D[i;j]for small i;jwe then compute larger\nD[i;j]based on previously computed smaller values. The value of D[i;j]is com-\nputed by taking the minimum of the three possible paths through the matrix which\narrive there:\nD[i;j] =min8\n<\n:D[i\u00001;j]+del-cost (source [i])\nD[i;j\u00001]+ins-cost (target [j])\nD[i\u00001;j\u00001]+sub-cost (source [i];target [j])(2.23)\nWe mentioned above two versions of Levenshtein distance, one in which substitu-\ntions cost 1 and one in which substitutions cost 2 (i.e., are equivalent to an insertion\nplus a deletion). Let\u2019s here use that second version of Levenshtein distance in which\nthe insertions and deletions each have a cost of 1 (ins-cost( \u0001) = del-cost(\u0001) = 1), and\nsubstitutions have a cost of 2 (except substitution of identical letters has zero cost).\nUnder this version of Levenshtein, the computation for D[i;j]becomes:\nD[i;j] =min8\n>><\n>>:D[i\u00001;j]+1\nD[i;j\u00001]+1\nD[i\u00001;j\u00001]+\u001a2; if source [i]6=target [j]\n0; if source [i] =target [j](2.24)\nThe algorithm is summarized in Fig. 2.17; Fig. 2.18 shows the results of applying\nthe algorithm to the distance between intention andexecution with the version of\nLevenshtein in Eq. 2.24.\nAlignment Knowing the minimum edit distance is useful for algorithms like \ufb01nd-\ning potential spelling error corrections. But the edit distance algorithm is important\nin another way; with a small change, it can also provide the minimum cost align-\nment between two strings. Aligning two strings is useful throughout speech and\nlanguage processing. In speech recognition, minimum edit distance alignment is\nused to compute the word error rate (Chapter 16). Alignment plays a role in ma-\nchine translation, in which sentences in a parallel corpus (a corpus with a text in two\nlanguages) need to be matched to each other.\nTo extend the edit distance algorithm to produce an alignment, we can start by\nvisualizing an alignment as a path through the edit distance matrix. Figure 2.19",
    "metadata": {
      "source": "2",
      "chunk_id": 34,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 25\n\n2.8 \u2022 M INIMUM EDITDISTANCE 25\nfunction MIN-EDIT-DISTANCE (source ,target )returns min-distance\nn LENGTH (source )\nm LENGTH (target )\nCreate a distance matrix D[n+1,m+1]\n#Initialization: the zeroth row and column is the distance from the empty string\nD[0,0] = 0\nforeach row ifrom 1tondo\nD[i,0] D[i-1,0] + del-cost (source [i])\nforeach column jfrom 1tomdo\nD[0,j] D[0,j-1] + ins-cost (target [j])\n#Recurrence relation:\nforeach row ifrom 1tondo\nforeach column jfrom 1tomdo\nD[i, j] MIN(D[i\u00001,j] +del-cost (source [i]),\nD[i\u00001,j\u00001] + sub-cost (source [i],target [j]),\nD[i,j\u00001] + ins-cost (target [j]))\n#Termination\nreturn D[n,m]\nFigure 2.17 The minimum edit distance algorithm, an example of the class of dynamic\nprogramming algorithms. The various costs can either be \ufb01xed (e.g., 8x;ins-cost (x) =1)\nor can be speci\ufb01c to the letter (to model the fact that some letters are more likely to be in-\nserted than others). We assume that there is no cost for substituting a letter for itself (i.e.,\nsub-cost (x;x) =0).\nSrcnTar # e x e c u t i o n\n# 0 1 2 3 4 5 6 7 8 9\ni 1 2 3 4 5 6 7 6 7 8\nn 2 3 4 5 6 7 8 7 8 7\nt 3 4 5 6 7 8 7 8 9 8\ne 4 3 4 5 6 7 8 9 10 9\nn 5 4 5 6 7 8 9 10 11 10\nt 6 5 6 7 8 9 8 9 10 11\ni 7 6 7 8 9 10 9 8 9 10\no 8 7 8 9 10 11 10 9 8 9\nn 9 8 9 10 11 12 11 10 9 8\nFigure 2.18 Computation of minimum edit distance between intention andexecution with\nthe algorithm of Fig. 2.17, using Levenshtein distance with cost of 1 for insertions or dele-\ntions, 2 for substitutions.\nshows this path with boldfaced cells. Each boldfaced cell represents an alignment\nof a pair of letters in the two strings. If two boldfaced cells occur in the same row,\nthere will be an insertion in going from the source to the target; two boldfaced cells\nin the same column indicate a deletion.\nFigure 2.19 also shows the intuition of how to compute this alignment path. The\ncomputation proceeds in two steps. In the \ufb01rst step, we augment the minimum edit\ndistance algorithm to store backpointers in each cell. The backpointer from a cell\npoints to the previous cell (or cells) that we came from in entering the current cell.\nWe\u2019ve shown a schematic of these backpointers in Fig. 2.19. Some cells have mul-",
    "metadata": {
      "source": "2",
      "chunk_id": 35,
      "token_count": 785,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 26",
    "metadata": {
      "source": "2",
      "chunk_id": 36,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "26 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\ntiple backpointers because the minimum extension could have come from multiple\nprevious cells. In the second step, we perform a backtrace . In a backtrace, we start backtrace\nfrom the last cell (at the \ufb01nal row and column), and follow the pointers back through\nthe dynamic programming matrix. Each complete path between the \ufb01nal cell and the\ninitial cell is a minimum distance alignment. Exercise 2.7 asks you to modify the\nminimum edit distance algorithm to store the pointers and compute the backtrace to\noutput an alignment.\n# e x e c u t i o n\n# 0 1 2 3 4 5 6 7 8 9\ni\"1- \" 2- \" 3- \" 4- \" 5- \" 6- \" 7-6 7 8\nn\"2- \" 3- \" 4- \" 5- \" 6- \" 7- \" 8\"7- \" 8-7\nt\"3- \" 4- \" 5- \" 6- \" 7- \" 8-7 \"8- \" 9\"8\ne\"4-3 4- 5 6 7 \"8- \" 9- \" 10\"9\nn\"5\"4- \" 5- \" 6- \" 7- \" 8- \" 9- \" 10- \" 11-\"10\nt\"6\"5- \" 6- \" 7- \" 8- \" 9-8 9 10 \"11\ni\"7\"6- \" 7- \" 8- \" 9- \" 10\"9-8 9 10\no\"8\"7- \" 8- \" 9- \" 10- \" 11\"10\"9-8 9\nn\"9\"8- \" 9- \" 10- \" 11- \" 12\"11\"10\"9-8\nFigure 2.19 When entering a value in each cell, we mark which of the three neighboring\ncells we came from with up to three arrows. After the table is full we compute an alignment\n(minimum edit path) by using a backtrace , starting at the 8in the lower-right corner and\nfollowing the arrows back. The sequence of bold cells represents one possible minimum\ncost alignment between the two strings, again using Levenshtein distance with cost of 1 for\ninsertions or deletions, 2 for substitutions. Diagram design after Gus\ufb01eld (1997).\nWhile we worked our example with simple Levenshtein distance, the algorithm\nin Fig. 2.17 allows arbitrary weights on the operations. For spelling correction, for\nexample, substitutions are more likely to happen between letters that are next to\neach other on the keyboard. The Viterbi algorithm is a probabilistic extension of\nminimum edit distance. Instead of computing the \u201cminimum edit distance\u201d between\ntwo strings, Viterbi computes the \u201cmaximum probability alignment\u201d of one string\nwith another. We\u2019ll discuss this more in Chapter 17.\n2.9 Summary\nThis chapter introduced a fundamental tool in language processing, the regular ex-\npression , and showed how to perform basic text normalization tasks including\nword segmentation andnormalization ,sentence segmentation , and stemming .\nWe also introduced the important minimum edit distance algorithm for comparing\nstrings. Here\u2019s a summary of the main points we covered about these ideas:\n\u2022 The regular expression language is a powerful tool for pattern-matching.\n\u2022 Basic operations in regular expressions include concatenation of symbols,",
    "metadata": {
      "source": "2",
      "chunk_id": 37,
      "token_count": 788,
      "chapter_title": ""
    }
  },
  {
    "content": "(minimum edit path) by using a backtrace , starting at the 8in the lower-right corner and\nfollowing the arrows back. The sequence of bold cells represents one possible minimum\ncost alignment between the two strings, again using Levenshtein distance with cost of 1 for\ninsertions or deletions, 2 for substitutions. Diagram design after Gus\ufb01eld (1997).\nWhile we worked our example with simple Levenshtein distance, the algorithm\nin Fig. 2.17 allows arbitrary weights on the operations. For spelling correction, for\nexample, substitutions are more likely to happen between letters that are next to\neach other on the keyboard. The Viterbi algorithm is a probabilistic extension of\nminimum edit distance. Instead of computing the \u201cminimum edit distance\u201d between\ntwo strings, Viterbi computes the \u201cmaximum probability alignment\u201d of one string\nwith another. We\u2019ll discuss this more in Chapter 17.\n2.9 Summary\nThis chapter introduced a fundamental tool in language processing, the regular ex-\npression , and showed how to perform basic text normalization tasks including\nword segmentation andnormalization ,sentence segmentation , and stemming .\nWe also introduced the important minimum edit distance algorithm for comparing\nstrings. Here\u2019s a summary of the main points we covered about these ideas:\n\u2022 The regular expression language is a powerful tool for pattern-matching.\n\u2022 Basic operations in regular expressions include concatenation of symbols,\ndisjunction of symbols ( [],|),counters (*,+, and{n,m} ),anchors (^,$)\nand precedence operators ( (,)).\n\u2022Word tokenization and normalization are generally done by cascades of\nsimple regular expression substitutions or \ufb01nite automata.\n\u2022 The Porter algorithm is a simple and ef\ufb01cient way to do stemming , stripping\noff af\ufb01xes. It does not have high accuracy but may be useful for some tasks.",
    "metadata": {
      "source": "2",
      "chunk_id": 38,
      "token_count": 386,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 27\n\nBIBLIOGRAPHICAL AND HISTORICAL NOTES 27\n\u2022 The minimum edit distance between two strings is the minimum number of\noperations it takes to edit one into the other. Minimum edit distance can be\ncomputed by dynamic programming , which also results in an alignment of\nthe two strings.\nBibliographical and Historical Notes\nKleene 1951; 1956 \ufb01rst de\ufb01ned regular expressions and the \ufb01nite automaton, based\non the McCulloch-Pitts neuron. Ken Thompson was one of the \ufb01rst to build regular\nexpressions compilers into editors for text searching (Thompson, 1968). His edi-\ntoredincluded a command \u201cg/regular expression/p\u201d, or Global Regular Expression\nPrint, which later became the Unix grep utility.\nText normalization algorithms have been applied since the beginning of the\n\ufb01eld. One of the earliest widely used stemmers was Lovins (1968). Stemming\nwas also applied early to the digital humanities, by Packard (1973), who built an\naf\ufb01x-stripping morphological parser for Ancient Greek. Currently a wide vari-\nety of code for tokenization and normalization is available, such as the Stanford\nTokenizer ( https://nlp.stanford.edu/software/tokenizer.shtml ) or spe-\ncialized tokenizers for Twitter (O\u2019Connor et al., 2010), or for sentiment ( http:\n//sentiment.christopherpotts.net/tokenizing.html ). See Palmer (2012)\nfor a survey of text preprocessing. NLTK is an essential tool that offers both useful\nPython libraries ( https://www.nltk.org ) and textbook descriptions (Bird et al.,\n2009) of many algorithms including text normalization and corpus interfaces.\nFor more on Herdan\u2019s law and Heaps\u2019 Law, see Herdan (1960, p. 28), Heaps\n(1978), Egghe (2007) and Baayen (2001); For more on edit distance, see Gus\ufb01eld\n(1997). Our example measuring the edit distance from \u2018intention\u2019 to \u2018execution\u2019\nwas adapted from Kruskal (1983). There are various publicly available packages to\ncompute edit distance, including Unix diff and the NIST sclite program (NIST,\n2005).\nIn his autobiography Bellman (1984) explains how he originally came up with\nthe term dynamic programming :\n\u201c...The 1950s were not good years for mathematical research. [the]\nSecretary of Defense ...had a pathological fear and hatred of the word,\nresearch... I decided therefore to use the word, \u201cprogramming\u201d. I\nwanted to get across the idea that this was dynamic, this was multi-\nstage... I thought, let\u2019s ... take a word that has an absolutely precise\nmeaning, namely dynamic... it\u2019s impossible to use the word, dynamic,\nin a pejorative sense. Try thinking of some combination that will pos-\nsibly give it a pejorative meaning. It\u2019s impossible. Thus, I thought\ndynamic programming was a good name. It was something not even a\nCongressman could object to.\u201d\nExercises\n2.1 Write regular expressions for the following languages.\n1. the set of all alphabetic strings;\n2. the set of all lower case alphabetic strings ending in a b;",
    "metadata": {
      "source": "2",
      "chunk_id": 39,
      "token_count": 697,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 28\n\n28 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\n3. the set of all strings from the alphabet a;bsuch that each ais immedi-\nately preceded by and immediately followed by a b;\n2.2 Write regular expressions for the following languages. By \u201cword\u201d, we mean\nan alphabetic string separated from other words by whitespace, any relevant\npunctuation, line breaks, and so forth.\n1. the set of all strings with two consecutive repeated words (e.g., \u201cHum-\nbert Humbert\u201d and \u201cthe the\u201d but not \u201cthe bug\u201d or \u201cthe big bug\u201d);\n2. all strings that start at the beginning of the line with an integer and that\nend at the end of the line with a word;\n3. all strings that have both the word grotto and the word raven in them\n(but not, e.g., words like grottos that merely contain the word grotto );\n4. write a pattern that places the \ufb01rst word of an English sentence in a\nregister. Deal with punctuation.\n2.3 Implement an ELIZA-like program, using substitutions such as those described\non page 9. You might want to choose a different domain than a Rogerian psy-\nchologist, although keep in mind that you would need a domain in which your\nprogram can legitimately engage in a lot of simple repetition.\n2.4 Compute the edit distance (using insertion cost 1, deletion cost 1, substitution\ncost 1) of \u201cleda\u201d to \u201cdeal\u201d. Show your work (using the edit distance grid).\n2.5 Figure out whether drive is closer to brief or to divers and what the edit dis-\ntance is to each. You may use any version of distance that you like.\n2.6 Now implement a minimum edit distance algorithm and use your hand-computed\nresults to check your code.\n2.7 Augment the minimum edit distance algorithm to output an alignment; you\nwill need to store pointers and add a stage to compute the backtrace.",
    "metadata": {
      "source": "2",
      "chunk_id": 40,
      "token_count": 432,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 29",
    "metadata": {
      "source": "2",
      "chunk_id": 41,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Exercises 29\nBaayen, R. H. 2001. Word frequency distributions . Springer.\nBellman, R. 1957. Dynamic Programming . Princeton Uni-\nversity Press.\nBellman, R. 1984. Eye of the Hurricane: an autobiography .\nWorld Scienti\ufb01c Singapore.\nBender, E. M. 2019. The #BenderRule: On naming the lan-\nguages we study and why it matters. Blog post.\nBender, E. M., B. Friedman, and A. McMillan-Major. 2021.\nA guide for writing data statements for natural lan-\nguage processing. http://techpolicylab.uw.edu/\ndata-statements/ .\nBird, S., E. Klein, and E. Loper. 2009. Natural Language\nProcessing with Python . O\u2019Reilly.\nBlodgett, S. L., L. Green, and B. O\u2019Connor. 2016. Demo-\ngraphic dialectal variation in social media: A case study\nof African-American English. EMNLP .\nBostrom, K. and G. Durrett. 2020. Byte pair encoding is\nsuboptimal for language model pretraining. EMNLP .\nChen, X., Z. Shi, X. Qiu, and X. Huang. 2017. Adversar-\nial multi-criteria learning for Chinese word segmentation.\nACL.\nChurch, K. W. 1994. Unix for Poets. Slides from 2nd EL-\nSNET Summer School and unpublished paper ms.\nClark, H. H. and J. E. Fox Tree. 2002. Using uh and um in\nspontaneous speaking. Cognition , 84:73\u2013111.\nEgghe, L. 2007. Untangling Herdan\u2019s law and Heaps\u2019\nlaw: Mathematical and informetric arguments. JASIST ,\n58(5):702\u2013709.\nGebru, T., J. Morgenstern, B. Vecchione, J. W. Vaughan,\nH. Wallach, H. Daum \u00b4e III, and K. Crawford. 2020.\nDatasheets for datasets. ArXiv.\nGodfrey, J., E. Holliman, and J. McDaniel. 1992. SWITCH-\nBOARD: Telephone speech corpus for research and de-\nvelopment. ICASSP .\nGus\ufb01eld, D. 1997. Algorithms on Strings, Trees, and Se-\nquences . Cambridge University Press.\nHeaps, H. S. 1978. Information retrieval. Computational and\ntheoretical aspects . Academic Press.\nHerdan, G. 1960. Type-token mathematics . Mouton.\nJones, T. 2015. Toward a description of African American\nVernacular English dialect regions using \u201cBlack Twitter\u201d.\nAmerican Speech , 90(4):403\u2013440.\nJurgens, D., Y . Tsvetkov, and D. Jurafsky. 2017. Incorpo-\nrating dialectal variability for socially equitable language\nidenti\ufb01cation. ACL.\nKing, S. 2020. From African American Vernacular English\nto African American Language: Rethinking the study of\nrace and language in African Americans\u2019 speech. Annual\nReview of Linguistics , 6:285\u2013300.\nKiss, T. and J. Strunk. 2006. Unsupervised multilingual\nsentence boundary detection. Computational Linguistics ,\n32(4):485\u2013525.",
    "metadata": {
      "source": "2",
      "chunk_id": 42,
      "token_count": 750,
      "chapter_title": ""
    }
  },
  {
    "content": "Godfrey, J., E. Holliman, and J. McDaniel. 1992. SWITCH-\nBOARD: Telephone speech corpus for research and de-\nvelopment. ICASSP .\nGus\ufb01eld, D. 1997. Algorithms on Strings, Trees, and Se-\nquences . Cambridge University Press.\nHeaps, H. S. 1978. Information retrieval. Computational and\ntheoretical aspects . Academic Press.\nHerdan, G. 1960. Type-token mathematics . Mouton.\nJones, T. 2015. Toward a description of African American\nVernacular English dialect regions using \u201cBlack Twitter\u201d.\nAmerican Speech , 90(4):403\u2013440.\nJurgens, D., Y . Tsvetkov, and D. Jurafsky. 2017. Incorpo-\nrating dialectal variability for socially equitable language\nidenti\ufb01cation. ACL.\nKing, S. 2020. From African American Vernacular English\nto African American Language: Rethinking the study of\nrace and language in African Americans\u2019 speech. Annual\nReview of Linguistics , 6:285\u2013300.\nKiss, T. and J. Strunk. 2006. Unsupervised multilingual\nsentence boundary detection. Computational Linguistics ,\n32(4):485\u2013525.\nKleene, S. C. 1951. Representation of events in nerve nets\nand \ufb01nite automata. Technical Report RM-704, RAND\nCorporation. RAND Research Memorandum.\nKleene, S. C. 1956. Representation of events in nerve nets\nand \ufb01nite automata. In C. Shannon and J. McCarthy, eds,\nAutomata Studies , 3\u201341. Princeton University Press.Krovetz, R. 1993. Viewing morphology as an inference pro-\ncess. SIGIR-93 .\nKruskal, J. B. 1983. An overview of sequence comparison.\nIn D. Sankoff and J. B. Kruskal, eds, Time Warps, String\nEdits, and Macromolecules: The Theory and Practice of\nSequence Comparison , 1\u201344. Addison-Wesley.\nKudo, T. 2018. Subword regularization: Improving neural\nnetwork translation models with multiple subword candi-\ndates. ACL.\nKudo, T. and J. Richardson. 2018. SentencePiece: A simple\nand language independent subword tokenizer and detok-\nenizer for neural text processing. EMNLP .\nKu\u02c7cera, H. and W. N. Francis. 1967. Computational Analysis\nof Present-Day American English . Brown Univ. Press.\nLevenshtein, V . I. 1966. Binary codes capable of correct-\ning deletions, insertions, and reversals. Cybernetics and\nControl Theory , 10(8):707\u2013710. Original in Doklady\nAkademii Nauk SSSR 163(4): 845\u2013848 (1965).\nLi, X., Y . Meng, X. Sun, Q. Han, A. Yuan, and J. Li. 2019.\nIs word segmentation necessary for deep learning of Chi-\nnese representations? ACL.\nLovins, J. B. 1968. Development of a stemming algorithm.\nMechanical Translation and Computational Linguistics ,\n11(1\u20132):9\u201313.\nManning, C. D., M. Surdeanu, J. Bauer, J. Finkel, S. Bethard,",
    "metadata": {
      "source": "2",
      "chunk_id": 43,
      "token_count": 753,
      "chapter_title": ""
    }
  },
  {
    "content": "network translation models with multiple subword candi-\ndates. ACL.\nKudo, T. and J. Richardson. 2018. SentencePiece: A simple\nand language independent subword tokenizer and detok-\nenizer for neural text processing. EMNLP .\nKu\u02c7cera, H. and W. N. Francis. 1967. Computational Analysis\nof Present-Day American English . Brown Univ. Press.\nLevenshtein, V . I. 1966. Binary codes capable of correct-\ning deletions, insertions, and reversals. Cybernetics and\nControl Theory , 10(8):707\u2013710. Original in Doklady\nAkademii Nauk SSSR 163(4): 845\u2013848 (1965).\nLi, X., Y . Meng, X. Sun, Q. Han, A. Yuan, and J. Li. 2019.\nIs word segmentation necessary for deep learning of Chi-\nnese representations? ACL.\nLovins, J. B. 1968. Development of a stemming algorithm.\nMechanical Translation and Computational Linguistics ,\n11(1\u20132):9\u201313.\nManning, C. D., M. Surdeanu, J. Bauer, J. Finkel, S. Bethard,\nand D. McClosky. 2014. The Stanford CoreNLP natural\nlanguage processing toolkit. ACL.\nNIST. 2005. Speech recognition scoring toolkit (sctk) ver-\nsion 2.1.http://www.nist.gov/speech/tools/ .\nO\u2019Connor, B., M. Krieger, and D. Ahn. 2010. Tweetmotif:\nExploratory search and topic summarization for twitter.\nICWSM .\nPackard, D. W. 1973. Computer-assisted morphological\nanalysis of ancient Greek. COLING .\nPalmer, D. 2012. Text preprocessing. In N. Indurkhya and\nF. J. Damerau, eds, Handbook of Natural Language Pro-\ncessing , 9\u201330. CRC Press.\nPorter, M. F. 1980. An algorithm for suf\ufb01x stripping. Pro-\ngram , 14(3):130\u2013137.\nSennrich, R., B. Haddow, and A. Birch. 2016. Neural ma-\nchine translation of rare words with subword units. ACL.\nSimons, G. F. and C. D. Fennig. 2018. Ethnologue: Lan-\nguages of the world, 21st edition. SIL International.\nSolorio, T., E. Blair, S. Maharjan, S. Bethard, M. Diab,\nM. Ghoneim, A. Hawwari, F. AlGhamdi, J. Hirschberg,\nA. Chang, and P. Fung. 2014. Overview for the \ufb01rst\nshared task on language identi\ufb01cation in code-switched\ndata. Workshop on Computational Approaches to Code\nSwitching .\nThompson, K. 1968. Regular expression search algorithm.\nCACM , 11(6):419\u2013422.\nWagner, R. A. and M. J. Fischer. 1974. The string-to-string\ncorrection problem. Journal of the ACM , 21:168\u2013173.\nWeizenbaum, J. 1966. ELIZA \u2013 A computer program for the\nstudy of natural language communication between man\nand machine. CACM , 9(1):36\u201345.",
    "metadata": {
      "source": "2",
      "chunk_id": 44,
      "token_count": 761,
      "chapter_title": ""
    }
  },
  {
    "content": "Sennrich, R., B. Haddow, and A. Birch. 2016. Neural ma-\nchine translation of rare words with subword units. ACL.\nSimons, G. F. and C. D. Fennig. 2018. Ethnologue: Lan-\nguages of the world, 21st edition. SIL International.\nSolorio, T., E. Blair, S. Maharjan, S. Bethard, M. Diab,\nM. Ghoneim, A. Hawwari, F. AlGhamdi, J. Hirschberg,\nA. Chang, and P. Fung. 2014. Overview for the \ufb01rst\nshared task on language identi\ufb01cation in code-switched\ndata. Workshop on Computational Approaches to Code\nSwitching .\nThompson, K. 1968. Regular expression search algorithm.\nCACM , 11(6):419\u2013422.\nWagner, R. A. and M. J. Fischer. 1974. The string-to-string\ncorrection problem. Journal of the ACM , 21:168\u2013173.\nWeizenbaum, J. 1966. ELIZA \u2013 A computer program for the\nstudy of natural language communication between man\nand machine. CACM , 9(1):36\u201345.\nWeizenbaum, J. 1976. Computer Power and Human Reason:\nFrom Judgement to Calculation . W.H. Freeman & Co.",
    "metadata": {
      "source": "2",
      "chunk_id": 45,
      "token_count": 312,
      "chapter_title": ""
    }
  }
]