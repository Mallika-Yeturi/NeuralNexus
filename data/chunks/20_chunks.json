[
  {
    "content": "# 20\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n20Information Extraction:\nRelations, Events, and Time\nTime will explain.\nJane Austen, Persuasion\nImagine that you are an analyst with an investment \ufb01rm that tracks airline stocks.\nYou\u2019re given the task of determining the relationship (if any) between airline an-\nnouncements of fare increases and the behavior of their stocks the next day. His-\ntorical data about stock prices is easy to come by, but what about the airline an-\nnouncements? You will need to know at least the name of the airline, the nature of\nthe proposed fare hike, the dates of the announcement, and possibly the response of\nother airlines. Fortunately, these can be all found in news articles like this one:\nCiting high fuel prices, United Airlines said Friday it has increased fares\nby $6 per round trip on \ufb02ights to some cities also served by lower-\ncost carriers. American Airlines, a unit of AMR Corp., immediately\nmatched the move, spokesman Tim Wagner said. United, a unit of UAL\nCorp., said the increase took effect Thursday and applies to most routes\nwhere it competes against discount carriers, such as Chicago to Dallas\nand Denver to San Francisco.\nThis chapter presents techniques for extracting limited kinds of semantic con-\ntent from text. This process of information extraction (IE) turns the unstructuredinformation\nextraction\ninformation embedded in texts into structured data, for example for populating a\nrelational database to enable further processing.\nWe begin with the task of relation extraction : \ufb01nding and classifying semanticrelation\nextraction\nrelations among entities mentioned in a text, like child-of (X is the child-of Y), or\npart-whole or geospatial relations. Relation extraction has close links to populat-\ning a relational database, and knowledge graphs , datasets of structured relationalknowledge\ngraphs\nknowledge, are a useful way for search engines to present information to users.\nNext, we discuss event extraction , the task of \ufb01nding events in which these en-event\nextraction\ntities participate, like, in our sample text, the fare increases by United andAmerican\nand the reporting events said andcite. Events are also situated in time , occurring at\na particular date or time, and events can be related temporally, happening before or\nafter or simultaneously with each other. We\u2019ll need to recognize temporal expres-\nsions like Friday ,Thursday ortwo days from now and times such as 3:30 P .M. , and\nnormalize them onto speci\ufb01c calendar dates or times. We\u2019ll need to link Friday to\nthe time of United\u2019s announcement, Thursday to the previous day\u2019s fare increase,\nand we\u2019ll need to produce a timeline in which United\u2019s announcement follows the\nfare increase and American\u2019s announcement follows both of those events.\nThe related task of template \ufb01lling is to \ufb01nd recurring stereotypical events or template \ufb01lling\nsituations in documents and \ufb01ll in the template slots. These slot-\ufb01llers may consist\nof text segments extracted directly from the text, or concepts like times, amounts, or\nontology entities that have been inferred through additional processing. Our airline",
    "metadata": {
      "source": "20",
      "chunk_id": 0,
      "token_count": 706,
      "chapter_title": "20"
    }
  },
  {
    "content": "## Page 2\n\n2CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\nARTIFACTGENERALAFFILIATIONORGAFFILIATIONPART-WHOLEPERSON-SOCIALPHYSICALLocatedNearBusinessFamilyLasting PersonalCitizen-Resident-Ethnicity-ReligionOrg-Location-OriginFounderEmploymentMembershipOwnershipStudent-AlumInvestorUser-Owner-Inventor-ManufacturerGeographicalSubsidiary\nSports-Affiliation\nFigure 20.1 The 17 relations used in the ACE relation extraction task.\ntext presents such a stereotypical situation since airlines often raise fares and then\nwait to see if competitors follow along. Here we can identify United as a lead air-\nline that initially raised its fares, $6 as the amount, Thursday as the increase date,\nandAmerican as an airline that followed along, leading to a \ufb01lled template like the\nfollowing:\nFARE-RAISE ATTEMPT :2\n6664LEAD AIRLINE : U NITED AIRLINES\nAMOUNT : $6\nEFFECTIVE DATE: 2006-10-26\nFOLLOWER : A MERICAN AIRLINES3\n7775\n20.1 Relation Extraction\nLet\u2019s assume that we have detected the named entities in our sample text (perhaps\nusing the techniques of Chapter 17), and would like to discern the relationships that\nexist among the detected entities:\nCiting high fuel prices, [ ORG United Airlines ] said [ TIME Friday ] it\nhas increased fares by [ MONEY $6] per round trip on \ufb02ights to some\ncities also served by lower-cost carriers. [ ORG American Airlines ], a\nunit of [ ORG AMR Corp.] , immediately matched the move, spokesman\n[PER Tim Wagner ] said. [ ORG United] , a unit of [ ORG UAL Corp.] ,\nsaid the increase took effect [ TIME Thursday] and applies to most\nroutes where it competes against discount carriers, such as [ LOC Chicago]\nto [LOC Dallas] and [ LOC Denver] to [LOC San Francisco] .\nThe text tells us, for example, that Tim Wagner is a spokesman for American\nAirlines , that United is a unit of UAL Corp. , and that American is a unit of AMR .\nThese binary relations are instances of more generic relations such as part-of or\nemploys that are fairly frequent in news-style texts. Figure 20.1 lists the 17 relations\nused in the ACE relation extraction evaluations and Fig. 20.2 shows some sample\nrelations. We might also extract more domain-speci\ufb01c relations such as the notion of\nan airline route. For example from this text we can conclude that United has routes\nto Chicago, Dallas, Denver, and San Francisco.",
    "metadata": {
      "source": "20",
      "chunk_id": 1,
      "token_count": 584,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\n20.1 \u2022 R ELATION EXTRACTION 3\nRelations Types Examples\nPhysical-Located PER-GPE Hewas in Tennessee\nPart-Whole-Subsidiary ORG-ORG XYZ , the parent company of ABC\nPerson-Social-Family PER-PER Yoko \u2019s husband John\nOrg-AFF-Founder PER-ORG Steve Jobs , co-founder of Apple ...\nFigure 20.2 Semantic relations with examples and the named entity types they involve.\nSets of relations have been de\ufb01ned for many other domains as well. For example\nUMLS, the Uni\ufb01ed Medical Language System from the US National Library of\nMedicine has a network that de\ufb01nes 134 broad subject categories, entity types, and\n54 relations between the entities, such as the following:\nEntity Relation Entity\nInjury disrupts Physiological Function\nBodily Location location-of Biologic Function\nAnatomical Structure part-of Organism\nPharmacologic Substance causes Pathological Function\nPharmacologic Substance treats Pathologic Function\nGiven a medical sentence like this one:\n(20.1) Doppler echocardiography can be used to diagnose left anterior descending\nartery stenosis in patients with type 2 diabetes\nWe could thus extract the UMLS relation:\nEchocardiography, Doppler Diagnoses Acquired stenosis\nWikipedia also offers a large supply of relations, drawn from infoboxes , struc- infoboxes\ntured tables associated with certain Wikipedia articles. For example, the Wikipedia\ninfobox for Stanford includes structured facts like state = \"California\" or\npresident = \"Marc Tessier-Lavigne\" . These facts can be turned into rela-\ntions like president-of orlocated-in . or into relations in a metalanguage called RDF RDF\n(Resource Description Framework). An RDF triple is a tuple of entity-relation- RDF triple\nentity, called a subject-predicate-object expression. Here\u2019s a sample RDF triple:\nsubject predicate object\nGolden Gate Park location San Francisco\nFor example the crowdsourced DBpedia (Bizer et al., 2009) is an ontology de-\nrived from Wikipedia containing over 2 billion RDF triples. Another dataset from\nWikipedia infoboxes, Freebase (Bollacker et al., 2008), now part of Wikidata (Vrande \u02c7ci\u00b4c Freebase\nand Kr \u00a8otzsch, 2014), has relations between people and their nationality, or locations,\nand other locations they are contained in.\nWordNet or other ontologies offer useful ontological relations that express hier-\narchical relations between words or concepts. For example WordNet has the is-aor is-a\nhypernym relation between classes, hypernym\nGiraffe is-a ruminant is-a ungulate is-a mammal is-a vertebrate ...\nWordNet also has Instance-of relation between individuals and classes, so that for\nexample San Francisco is in the Instance-of relation with city. Extracting these\nrelations is an important step in extending or building ontologies.\nFinally, there are large datasets that contain sentences hand-labeled with their\nrelations, designed for training and testing relation extractors. The TACRED dataset\n(Zhang et al., 2017) contains 106,264 examples of relation triples about particular\npeople or organizations, labeled in sentences from news and web text drawn from the",
    "metadata": {
      "source": "20",
      "chunk_id": 2,
      "token_count": 708,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\nannual TAC Knowledge Base Population (TAC KBP) challenges. TACRED contains\n41 relation types (like per:city of birth, org:subsidiaries, org:member of, per:spouse),\nplus a no relation tag; examples are shown in Fig. 20.3. About 80% of all examples\nare annotated as no relation; having suf\ufb01cient negative data is important for training\nsupervised classi\ufb01ers.\nExample Entity Types & Label\nCarey will succeed Cathleen P. Black, who held the position for 15\nyears and will take on a new role as chairwoman of Hearst Maga-\nzines, the company said.PERSON /TITLE\nRelation: per:title\nIrene Morgan Kirkaldy, who was born and reared in Baltimore, lived\non Long Island and ran a child-care center in Queens with her second\nhusband, Stanley Kirkaldy.PERSON /CITY\nRelation: per:city ofbirth\nBaldwin declined further comment, and said JetBlue chief executive\nDave Barger was unavailable.Types: PERSON /TITLE\nRelation: norelation\nFigure 20.3 Example sentences and labels from the TACRED dataset (Zhang et al., 2017).\nA standard dataset was also produced for the SemEval 2010 Task 8, detecting\nrelations between nominals (Hendrickx et al., 2009). The dataset has 10,717 exam-\nples, each with a pair of nominals (untyped) hand-labeled with one of 9 directed\nrelations like product-producer ( afactory manufactures suits) orcomponent-whole\n(myapartment has a large kitchen ).\n20.2 Relation Extraction Algorithms\nThere are \ufb01ve main classes of algorithms for relation extraction: handwritten pat-\nterns ,supervised machine learning ,semi-supervised (viabootstrapping ordis-\ntant supervision ), and unsupervised . We\u2019ll introduce each of these in the next\nsections.\n20.2.1 Using Patterns to Extract Relations\nThe earliest and still common algorithm for relation extraction is lexico-syntactic\npatterns, \ufb01rst developed by Hearst (1992a), and therefore often called Hearst pat-\nterns . Consider the following sentence: Hearst patterns\nAgar is a substance prepared from a mixture of red algae, such as Ge-\nlidium, for laboratory or industrial use.\nHearst points out that most human readers will not know what Gelidium is, but that\nthey can readily infer that it is a kind of (a hyponym of)red algae , whatever that is.\nShe suggests that the following lexico-syntactic pattern\nNP0such as NP 1f;NP2:::;(andjor)NPig;i\u00151 (20.2)\nimplies the following semantics\n8NPi;i\u00151;hyponym (NPi;NP0) (20.3)\nallowing us to infer\nhyponym (Gelidium ;red algae ) (20.4)",
    "metadata": {
      "source": "20",
      "chunk_id": 3,
      "token_count": 668,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\n20.2 \u2022 R ELATION EXTRACTION ALGORITHMS 5\nNPf, NPg*f,g(andjor) other NP H temples, treasuries, and other important civic buildings\nNPHsuch asfNP,g*f(orjand)gNP red algae such as Gelidium\nsuch NP HasfNP,g*f(orjand)gNP such authors as Herrick, Goldsmith, and Shakespeare\nNPHf,gincludingfNP,g*f(orjand)gNP common-law countries, including Canada and England\nNPHf,gespeciallyfNPg*f(orjand)gNP European countries, especially France, England, and Spain\nFigure 20.4 Hand-built lexico-syntactic patterns for \ufb01nding hypernyms, using fgto mark optionality (Hearst\n1992a, Hearst 1998).\nFigure 20.4 shows \ufb01ve patterns Hearst (1992a, 1998) suggested for inferring\nthe hyponym relation; we\u2019ve shown NPHas the parent/hyponym. Modern versions\nof the pattern-based approach extend it by adding named entity constraints. For\nexample if our goal is to answer questions about \u201cWho holds what of\ufb01ce in which\norganization?\u201d, we can use patterns like the following:\nPER, POSITION of ORG:\nGeorge Marshall, Secretary of State of the United States\nPER (namedjappointedjchosejetc.) PER Prep? POSITION\nTruman appointed Marshall Secretary of State\nPER [be]? (namedjappointedjetc.) Prep? ORG POSITION\nGeorge Marshall was named US Secretary of State\nHand-built patterns have the advantage of high-precision and they can be tailored\nto speci\ufb01c domains. On the other hand, they are often low-recall, and it\u2019s a lot of\nwork to create them for all possible patterns.\n20.2.2 Relation Extraction via Supervised Learning\nSupervised machine learning approaches to relation extraction follow a scheme that\nshould be familiar by now. A \ufb01xed set of relations and entities is chosen, a training\ncorpus is hand-annotated with the relations and entities, and the annotated texts are\nthen used to train classi\ufb01ers to annotate an unseen test set.\nThe most straightforward approach, illustrated in Fig. 20.5 is: (1) Find pairs of\nnamed entities (usually in the same sentence). (2): Apply a relation-classi\ufb01cation\non each pair. The classi\ufb01er can use any supervised technique (logistic regression,\nRNN, Transformer, random forest, etc.).\nAn optional intermediate \ufb01ltering classi\ufb01er can be used to speed up the process-\ning by making a binary decision on whether a given pair of named entities are related\n(by any relation). It\u2019s trained on positive examples extracted directly from all rela-\ntions in the annotated corpus, and negative examples generated from within-sentence\nentity pairs that are not annotated with a relation.\nFeature-based supervised relation classi\ufb01ers. Let\u2019s consider sample features for\na feature-based classi\ufb01er (like logistic regression or random forests), classifying the\nrelationship between American Airlines (Mention 1, or M1) and Tim Wagner (Men-\ntion 2, M2) from this sentence:\n(20.5) American Airlines , a unit of AMR, immediately matched the move,\nspokesman Tim Wagner said\nThese include word features (as embeddings, or 1-hot, stemmed or not):\n\u2022 The headwords of M1 and M2 and their concatenation\nAirlines Wagner Airlines-Wagner",
    "metadata": {
      "source": "20",
      "chunk_id": 4,
      "token_count": 770,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\nfunction FINDRELATIONS (words )returns relations\nrelations nil\nentities FINDENTITIES (words )\nforall entity pairs he1,e2iinentities do\nifRELATED ?(e1,e2)\nrelations relations +CLASSIFY RELATION (e1,e2)\nFigure 20.5 Finding and classifying the relations among entities in a text.\n\u2022 Bag-of-words and bigrams in M1 and M2\nAmerican, Airlines, Tim, Wagner, American Airlines, Tim Wagner\n\u2022 Words or bigrams in particular positions\nM2: -1 spokesman\nM2: +1 said\n\u2022 Bag of words or bigrams between M1 and M2:\na, AMR, of, immediately, matched, move, spokesman, the, unit\nNamed entity features:\n\u2022 Named-entity types and their concatenation\n(M1: ORG, M2: PER, M1M2: ORG-PER)\n\u2022 Entity Level of M1 and M2 (from the set NAME, NOMINAL, PRONOUN)\nM1: NAME [it or he would be PRONOUN]\nM2: NAME [the company would be NOMINAL]\n\u2022 Number of entities between the arguments (in this case 1, for AMR)\nSyntactic structure is a useful signal, often represented as the dependency or\nconstituency syntactic path traversed through the tree between the entities.\n\u2022 Constituent paths between M1 and M2\nNP\"NP\"S\"S#NP\n\u2022 Dependency-tree paths\nAirlines sub jmatched comp said!sub jWagner\nNeural supervised relation classi\ufb01ers Neural models for relation extraction sim-\nilarly treat the task as supervised classi\ufb01cation. Let\u2019s consider a typical system ap-\nplied to the TACRED relation extraction dataset and task (Zhang et al., 2017). In\nTACRED we are given a sentence and two spans within it: a subject, which is a\nperson or organization, and an object, which is any other entity. The task is to assign\na relation from the 42 TAC relations, or no relation.\nA typical Transformer-encoder algorithm, shown in Fig. 20.6, simply takes a\npretrained encoder like BERT and adds a linear layer on top of the sentence repre-\nsentation (for example the BERT [CLS] token), a linear layer that is \ufb01netuned as a\n1-of-N classi\ufb01er to assign one of the 43 labels. The input to the BERT encoder is\npartially de-lexi\ufb01ed; the subject and object entities are replaced in the input by their\nNER tags. This helps keep the system from over\ufb01tting to the individual lexical items\n(Zhang et al., 2017). When using BERT-type Transformers for relation extraction, it\nhelps to use versions of BERT like RoBERTa (Liu et al., 2019) or spanBERT (Joshi\net al., 2020) that don\u2019t have two sequences separated by a [SEP] token, but instead\nform the input from a single long sequence of sentences.\nIn general, if the test set is similar enough to the training set, and if there is\nenough hand-labeled data, supervised relation extraction systems can get high ac-",
    "metadata": {
      "source": "20",
      "chunk_id": 5,
      "token_count": 717,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\n20.2 \u2022 R ELATION EXTRACTION ALGORITHMS 7\nENCODER[CLS][SUBJ_PERSON]wasbornin[OBJ_LOC],MichiganLinearClassi\ufb01erp(relation|SUBJ,OBJ)\nFigure 20.6 Relation extraction as a linear layer on top of an encoder (in this case BERT),\nwith the subject and object entities replaced in the input by their NER tags (Zhang et al. 2017,\nJoshi et al. 2020).\ncuracies. But labeling a large training set is extremely expensive and supervised\nmodels are brittle: they don\u2019t generalize well to different text genres. For this rea-\nson, much research in relation extraction has focused on the semi-supervised and\nunsupervised approaches we turn to next.\n20.2.3 Semisupervised Relation Extraction via Bootstrapping\nSupervised machine learning assumes that we have lots of labeled data. Unfortu-\nnately, this is expensive. But suppose we just have a few high-precision seed pat-\nterns , like those in Section 20.2.1, or perhaps a few seed tuples . That\u2019s enough seed patterns\nseed tuples to bootstrap a classi\ufb01er! Bootstrapping proceeds by taking the entities in the seed\nbootstrapping pair, and then \ufb01nding sentences (on the web, or whatever dataset we are using) that\ncontain both entities. From all such sentences, we extract and generalize the context\naround the entities to learn new patterns. Fig. 20.7 sketches a basic algorithm.\nfunction BOOTSTRAP (Relation R )returns new relation tuples\ntuples Gather a set of seed tuples that have relation R\niterate\nsentences \ufb01nd sentences that contain entities in tuples\npatterns generalize the context between and around entities in sentences\nnewpairs usepatterns to identify more tuples\nnewpairs newpairs with high con\ufb01dence\ntuples tuples +newpairs\nreturn tuples\nFigure 20.7 Bootstrapping from seed entity pairs to learn relations.\nSuppose, for example, that we need to create a list of airline/hub pairs, and we\nknow only that Ryanair has a hub at Charleroi. We can use this seed fact to discover\nnew patterns by \ufb01nding other mentions of this relation in our corpus. We search\nfor the terms Ryanair ,Charleroi andhubin some proximity. Perhaps we \ufb01nd the\nfollowing set of sentences:\n(20.6) Budget airline Ryanair, which uses Charleroi as a hub, scrapped all\nweekend \ufb02ights out of the airport.\n(20.7) All \ufb02ights in and out of Ryanair\u2019s hub at Charleroi airport were grounded on\nFriday...\n(20.8) A spokesman at Charleroi, a main hub for Ryanair, estimated that 8000\npassengers had already been affected.",
    "metadata": {
      "source": "20",
      "chunk_id": 6,
      "token_count": 614,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\nFrom these results, we can use the context of words between the entity mentions,\nthe words before mention one, the word after mention two, and the named entity\ntypes of the two mentions, and perhaps other features, to extract general patterns\nsuch as the following:\n/ [ORG], which uses [LOC] as a hub /\n/ [ORG]'s hub at [LOC] /\n/ [LOC], a main hub for [ORG] /\nThese new patterns can then be used to search for additional tuples.\nBootstrapping systems also assign con\ufb01dence values to new tuples to avoid se-con\ufb01dence\nvalues\nmantic drift . In semantic drift, an erroneous pattern leads to the introduction of semantic drift\nerroneous tuples, which, in turn, lead to the creation of problematic patterns and the\nmeaning of the extracted relations \u2018drifts\u2019. Consider the following example:\n(20.9) Sydney has a ferry hub at Circular Quay.\nIf accepted as a positive example, this expression could lead to the incorrect in-\ntroduction of the tuple hSydney ;CircularQuayi. Patterns based on this tuple could\npropagate further errors into the database.\nCon\ufb01dence values for patterns are based on balancing two factors: the pattern\u2019s\nperformance with respect to the current set of tuples and the pattern\u2019s productivity\nin terms of the number of matches it produces in the document collection. More\nformally, given a document collection D, a current set of tuples T, and a proposed\npattern p, we need to track two factors:\n\u2022hits(p): the set of tuples in Tthatpmatches while looking in D\n\u2022\ufb01nds (p): The total set of tuples that p\ufb01nds in D\nThe following equation balances these considerations (Riloff and Jones, 1999).\nConfRlogF (p) =jhits(p)j\nj\ufb01nds (p)jlog(j\ufb01nds (p)j) (20.10)\nThis metric is generally normalized to produce a probability.\nWe can assess the con\ufb01dence in a proposed new tuple by combining the evidence\nsupporting it from all the patterns P0that match that tuple in D(Agichtein and Gra-\nvano, 2000). One way to combine such evidence is the noisy-or technique. Assume noisy-or\nthat a given tuple is supported by a subset of the patterns in P, each with its own\ncon\ufb01dence assessed as above. In the noisy-or model, we make two basic assump-\ntions. First, that for a proposed tuple to be false, allof its supporting patterns must\nhave been in error, and second, that the sources of their individual failures are all\nindependent. If we loosely treat our con\ufb01dence measures as probabilities, then the\nprobability of any individual pattern pfailing is 1\u0000Conf (p); the probability of all\nof the supporting patterns for a tuple being wrong is the product of their individual\nfailure probabilities, leaving us with the following equation for our con\ufb01dence in a\nnew tuple.\nConf (t) =1\u0000Y\np2P0(1\u0000Conf (p)) (20.11)\nSetting conservative con\ufb01dence thresholds for the acceptance of new patterns\nand tuples during the bootstrapping process helps prevent the system from drifting\naway from the targeted relation.",
    "metadata": {
      "source": "20",
      "chunk_id": 7,
      "token_count": 733,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\n20.2 \u2022 R ELATION EXTRACTION ALGORITHMS 9\n20.2.4 Distant Supervision for Relation Extraction\nAlthough hand-labeling text with relation labels is expensive to produce, there are\nways to \ufb01nd indirect sources of training data. The distant supervision methoddistant\nsupervision\n(Mintz et al., 2009) combines the advantages of bootstrapping with supervised learn-\ning. Instead of just a handful of seeds, distant supervision uses a large database to\nacquire a huge number of seed examples, creates lots of noisy pattern features from\nall these examples and then combines them in a supervised classi\ufb01er.\nFor example suppose we are trying to learn the place-of-birth relationship be-\ntween people and their birth cities. In the seed-based approach, we might have only\n5 examples to start with. But Wikipedia-based databases like DBPedia or Freebase\nhave tens of thousands of examples of many relations; including over 100,000 ex-\namples of place-of-birth , (<Edwin Hubble, Marshfield> ,<Albert Einstein,\nUlm> , etc.,). The next step is to run named entity taggers on large amounts of text\u2014\nMintz et al. (2009) used 800,000 articles from Wikipedia\u2014and extract all sentences\nthat have two named entities that match the tuple, like the following:\n...Hubble was born in Marsh\ufb01eld...\n...Einstein, born (1879), Ulm...\n...Hubble\u2019s birthplace in Marsh\ufb01eld...\nTraining instances can now be extracted from this data, one training instance\nfor each identical tuple <relation, entity1, entity2> . Thus there will be one\ntraining instance for each of:\n<born-in, Edwin Hubble, Marshfield>\n<born-in, Albert Einstein, Ulm>\n<born-year, Albert Einstein, 1879>\nand so on.\nWe can then apply feature-based or neural classi\ufb01cation. For feature-based\nclassi\ufb01cation, we can use standard supervised relation extraction features like the\nnamed entity labels of the two mentions, the words and dependency paths in be-\ntween the mentions, and neighboring words. Each tuple will have features col-\nlected from many training instances; the feature vector for a single training instance\nlike ( <born-in,Albert Einstein, Ulm> will have lexical and syntactic features\nfrom many different sentences that mention Einstein and Ulm.\nBecause distant supervision has very large training sets, it is also able to use very\nrich features that are conjunctions of these individual features. So we will extract\nthousands of patterns that conjoin the entity types with the intervening words or\ndependency paths like these:\nPER was born in LOC\nPER, born (XXXX), LOC\nPER\u2019s birthplace in LOC\nTo return to our running example, for this sentence:\n(20.12) American Airlines , a unit of AMR, immediately matched the move,\nspokesman Tim Wagner said\nwe would learn rich conjunction features like this one:\nM1 = ORG & M2 = PER & nextword=\u201csaid\u201d& path= NP\"NP\"S\"S#NP\nThe result is a supervised classi\ufb01er that has a huge rich set of features to use\nin detecting relations. Since not every test sentence will have one of the training",
    "metadata": {
      "source": "20",
      "chunk_id": 8,
      "token_count": 706,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\nrelations, the classi\ufb01er will also need to be able to label an example as no-relation .\nThis label is trained by randomly selecting entity pairs that do not appear in any\nFreebase relation, extracting features for them, and building a feature vector for\neach such tuple. The \ufb01nal algorithm is sketched in Fig. 20.8.\nfunction DISTANT SUPERVISION (Database D, Text T )returns relation classi\ufb01er C\nforeach relation R\nforeach tuple ( e1,e2 ) of entities with relation RinD\nsentences Sentences in Tthat contain e1ande2\nf Frequent features in sentences\nobservations observations + new training tuple ( e1, e2, f, R )\nC Train supervised classi\ufb01er on observations\nreturn C\nFigure 20.8 The distant supervision algorithm for relation extraction. A neural classi\ufb01er\nwould skip the feature set f.\nDistant supervision shares advantages with each of the methods we\u2019ve exam-\nined. Like supervised classi\ufb01cation, distant supervision uses a classi\ufb01er with lots\nof features, and supervised by detailed hand-created knowledge. Like pattern-based\nclassi\ufb01ers, it can make use of high-precision evidence for the relation between en-\ntities. Indeed, distance supervision systems learn patterns just like the hand-built\npatterns of early relation extractors. For example the is-aorhypernym extraction\nsystem of Snow et al. (2005) used hypernym/hyponym NP pairs from WordNet as\ndistant supervision, and then learned new patterns from large amounts of text. Their\nsystem induced exactly the original 5 template patterns of Hearst (1992a), but also\n70,000 additional patterns including these four:\nNPHlike NP Many hormones like leptin...\nNPHcalled NP ...using a markup language called XHTML\nNP is a NP H Ruby is a programming language...\nNP, a NP H IBM, a company with a long...\nThis ability to use a large number of features simultaneously means that, un-\nlike the iterative expansion of patterns in seed-based systems, there\u2019s no semantic\ndrift. Like unsupervised classi\ufb01cation, it doesn\u2019t use a labeled training corpus of\ntexts, so it isn\u2019t sensitive to genre issues in the training corpus, and relies on very\nlarge amounts of unlabeled data. Distant supervision also has the advantage that it\ncan create training tuples to be used with neural classi\ufb01ers, where features are not\nrequired.\nThe main problem with distant supervision is that it tends to produce low-precision\nresults, and so current research focuses on ways to improve precision. Furthermore,\ndistant supervision can only help in extracting relations for which a large enough\ndatabase already exists. To extract new relations without datasets, or relations for\nnew domains, purely unsupervised methods must be used.\n20.2.5 Unsupervised Relation Extraction\nThe goal of unsupervised relation extraction is to extract relations from the web\nwhen we have no labeled training data, and not even any list of relations. This task\nis often called open information extraction orOpen IE . In Open IE, the relationsopen\ninformation\nextraction",
    "metadata": {
      "source": "20",
      "chunk_id": 9,
      "token_count": 693,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\n20.2 \u2022 R ELATION EXTRACTION ALGORITHMS 11\nare simply strings of words (usually beginning with a verb).\nFor example, the ReVerb system (Fader et al., 2011) extracts a relation from a\nsentence sin 4 steps:\n1. Run a part-of-speech tagger and entity chunker over s\n2. For each verb in s, \ufb01nd the longest sequence of words wthat start with a verb\nand satisfy syntactic and lexical constraints, merging adjacent matches.\n3. For each phrase w, \ufb01nd the nearest noun phrase xto the left which is not a\nrelative pronoun, wh-word or existential \u201cthere\u201d. Find the nearest noun phrase\nyto the right.\n4. Assign con\ufb01dence cto the relation r= (x;w;y)using a con\ufb01dence classi\ufb01er\nand return it.\nA relation is only accepted if it meets syntactic and lexical constraints. The\nsyntactic constraints ensure that it is a verb-initial sequence that might also include\nnouns (relations that begin with light verbs like make ,have , ordooften express the\ncore of the relation with a noun, like have a hub in ):\nVjVPjVW*P\nV = verb particle? adv?\nW = (nounjadjjadvjpronjdet )\nP = (prepjparticlejin\ufb01nitive \u201cto\u201d)\nThe lexical constraints are based on a dictionary Dthat is used to prune very rare,\nlong relation strings. The intuition is to eliminate candidate relations that don\u2019t oc-\ncur with suf\ufb01cient number of distinct argument types and so are likely to be bad\nexamples. The system \ufb01rst runs the above relation extraction algorithm of\ufb02ine on\n500 million web sentences and extracts a list of all the relations that occur after nor-\nmalizing them (removing in\ufb02ection, auxiliary verbs, adjectives, and adverbs). Each\nrelation ris added to the dictionary if it occurs with at least 20 different arguments.\nFader et al. (2011) used a dictionary of 1.7 million normalized relations.\nFinally, a con\ufb01dence value is computed for each relation using a logistic re-\ngression classi\ufb01er. The classi\ufb01er is trained by taking 1000 random web sentences,\nrunning the extractor, and hand labeling each extracted relation as correct or incor-\nrect. A con\ufb01dence classi\ufb01er is then trained on this hand-labeled data, using features\nof the relation and the surrounding words. Fig. 20.9 shows some sample features\nused in the classi\ufb01cation.\n(x,r,y) covers all words in s\nthe last preposition in risfor\nthe last preposition in rison\nlen(s)\u001410\nthere is a coordinating conjunction to the left of rins\nrmatches a lone V in the syntactic constraints\nthere is preposition to the left of xins\nthere is an NP to the right of yins\nFigure 20.9 Features for the classi\ufb01er that assigns con\ufb01dence to relations extracted by the\nOpen Information Extraction system REVERB (Fader et al., 2011).\nFor example the following sentence:\n(20.13) United has a hub in Chicago, which is the headquarters of United\nContinental Holdings.",
    "metadata": {
      "source": "20",
      "chunk_id": 10,
      "token_count": 717,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\nhas the relation phrases has a hub in andis the headquarters of (it also has hasand\nis, but longer phrases are preferred). Step 3 \ufb01nds United to the left and Chicago to\nthe right of has a hub in , and skips over which to \ufb01nd Chicago to the left of is the\nheadquarters of . The \ufb01nal output is:\nr1: <United, has a hub in, Chicago>\nr2: <Chicago, is the headquarters of, United Continental Holdings>\nThe great advantage of unsupervised relation extraction is its ability to handle\na huge number of relations without having to specify them in advance. The dis-\nadvantage is the need to map all the strings into some canonical form for adding\nto databases or knowledge graphs. Current methods focus heavily on relations ex-\npressed with verbs, and so will miss many relations that are expressed nominally.\n20.2.6 Evaluation of Relation Extraction\nSupervised relation extraction systems are evaluated by using test sets with human-\nannotated, gold-standard relations and computing precision, recall, and F-measure.\nLabeled precision and recall require the system to classify the relation correctly,\nwhereas unlabeled methods simply measure a system\u2019s ability to detect entities that\nare related.\nSemi-supervised andunsupervised methods are much more dif\ufb01cult to evalu-\nate, since they extract totally new relations from the web or a large text. Because\nthese methods use very large amounts of text, it is generally not possible to run them\nsolely on a small labeled test set, and as a result it\u2019s not possible to pre-annotate a\ngold set of correct instances of relations.\nFor these methods it\u2019s possible to approximate (only) precision by drawing a\nrandom sample of relations from the output, and having a human check the accuracy\nof each of these relations. Usually this approach focuses on the tuples to be extracted\nfrom a body of text rather than on the relation mentions ; systems need not detect\nevery mention of a relation to be scored correctly. Instead, the evaluation is based\non the set of tuples occupying the database when the system is \ufb01nished. That is,\nwe want to know if the system can discover that Ryanair has a hub at Charleroi; we\ndon\u2019t really care how many times it discovers it. The estimated precision \u02c6Pis then\n\u02c6P=# of correctly extracted relation tuples in the sample\ntotal # of extracted relation tuples in the sample.(20.14)\nAnother approach that gives us a little bit of information about recall is to com-\npute precision at different levels of recall. Assuming that our system is able to\nrank the relations it produces (by probability, or con\ufb01dence) we can separately com-\npute precision for the top 1000 new relations, the top 10,000 new relations, the top\n100,000, and so on. In each case we take a random sample of that set. This will\nshow us how the precision curve behaves as we extract more and more tuples. But\nthere is no way to directly evaluate recall.\n20.3 Extracting Events\nThe task of event extraction is to identify mentions of events in texts. For theevent\nextraction\npurposes of this task, an event mention is any expression denoting an event or state\nthat can be assigned to a particular point, or interval, in time. The following markup\nof the sample text on page 1 shows all the events in this text.",
    "metadata": {
      "source": "20",
      "chunk_id": 11,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\n20.4 \u2022 R EPRESENTING TIME 13\n[EVENT Citing] high fuel prices, United Airlines [ EVENT said] Fri-\nday it has [ EVENT increased] fares by $6 per round trip on \ufb02ights to\nsome cities also served by lower-cost carriers. American Airlines, a unit\nof AMR Corp., immediately [ EVENT matched] [ EVENT the move],\nspokesman Tim Wagner [ EVENT said]. United, a unit of UAL Corp.,\n[EVENT said] [ EVENT the increase] took effect Thursday and [ EVENT\napplies] to most routes where it [ EVENT competes] against discount\ncarriers, such as Chicago to Dallas and Denver to San Francisco.\nIn English, most event mentions correspond to verbs, and most verbs introduce\nevents. However, as we can see from our example, this is not always the case. Events\ncan be introduced by noun phrases, as in the move andthe increase , and some verbs\nfail to introduce events, as in the phrasal verb took effect , which refers to when the\nevent began rather than to the event itself. Similarly, light verbs such as make ,take, light verbs\nandhave often fail to denote events. A light verb is a verb that has very little meaning\nitself, and the associated event is instead expressed by its direct object noun. In light\nverb examples like took a \ufb02ight , it\u2019s the word \ufb02ight that de\ufb01nes the event; these light\nverbs just provide a syntactic structure for the noun\u2019s arguments.\nVarious versions of the event extraction task exist, depending on the goal. For\nexample in the TempEval shared tasks (Verhagen et al. 2009) the goal is to extract\nevents and aspects like their aspectual and temporal properties. Events are to be\nclassi\ufb01ed as actions, states, reporting events (say, report, tell, explain ), perceptionreporting\nevents\nevents, and so on. The aspect, tense, and modality of each event also needs to be\nextracted. Thus for example the various said events in the sample text would be\nannotated as (class=REPORTING, tense=PAST, aspect=PERFECTIVE).\nEvent extraction is generally modeled via supervised learning, detecting events\nvia IOB sequence models and assigning event classes and attributes with multi-class\nclassi\ufb01ers. The input can be neural models starting from encoders; or classic feature-\nbased models using features like those in Fig. 20.10.\nFeature Explanation\nCharacter af\ufb01xes Character-level pre\ufb01xes and suf\ufb01xes of target word\nNominalization suf\ufb01x Character-level suf\ufb01xes for nominalizations (e.g., -tion )\nPart of speech Part of speech of the target word\nLight verb Binary feature indicating that the target is governed by a light verb\nSubject syntactic category Syntactic category of the subject of the sentence\nMorphological stem Stemmed version of the target word\nVerb root Root form of the verb basis for a nominalization\nWordNet hypernyms Hypernym set for the target\nFigure 20.10 Features commonly used in classic feature-based approaches to event detection.\n20.4 Representing Time\nLet\u2019s begin by introducing the basics of temporal logic and how human languages temporal logic\nconvey temporal information. The most straightforward theory of time holds that it\n\ufb02ows inexorably forward and that events are associated with either points or inter-\nvals in time, as on a timeline. We can order distinct events by situating them on the\ntimeline; one event precedes another if the \ufb02ow of time leads from the \ufb01rst event",
    "metadata": {
      "source": "20",
      "chunk_id": 12,
      "token_count": 774,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n14 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\nto the second. Accompanying these notions in most theories is the idea of the cur-\nrent moment in time. Combining this notion with the idea of a temporal ordering\nrelationship yields the familiar notions of past, present, and future.\nVarious kinds of temporal representation systems can be used to talk about tem-\nporal ordering relationship. One of the most commonly used in computational mod-\neling is the interval algebra of Allen (1984). Allen models all events and time interval algebra\nexpressions as intervals there is no representation for points (although intervals can\nbe very short). In order to deal with intervals without points, he identi\ufb01es 13 primi-\ntive relations that can hold between these temporal intervals. Fig. 20.11 shows these\n13Allen relations . Allen relations\nBABA\nBA\nAAB\nBAB\nTime A  before BB after  AA overlaps BB overlaps' AA meets BB meets' AA equals B(B equals A)A starts BB starts' AA finishes BB finishes' A\nBA during BB during' AA\nFigure 20.11 The 13 temporal relations from Allen (1984).\n20.4.1 Reichenbach\u2019s reference point\nThe relation between simple verb tenses and points in time is by no means straight-\nforward. The present tense can be used to refer to a future event, as in this example:\n(20.15) Ok, we \ufb02y from San Francisco to Boston at 10.\nOr consider the following examples:\n(20.16) Flight 1902 arrived late.\n(20.17) Flight 1902 had arrived late.\nAlthough both refer to events in the past, representing them in the same way seems\nwrong. The second example seems to have another unnamed event lurking in the\nbackground (e.g., Flight 1902 had already arrived late when something else hap-\npened).",
    "metadata": {
      "source": "20",
      "chunk_id": 13,
      "token_count": 411,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\n20.4 \u2022 R EPRESENTING TIME 15\nTo account for this phenomena, Reichenbach (1947) introduced the notion of\nareference point . In our simple temporal scheme, the current moment in time is reference point\nequated with the time of the utterance and is used as a reference point for when\nthe event occurred (before, at, or after). In Reichenbach\u2019s approach, the notion of\nthe reference point is separated from the utterance time and the event time. The\nfollowing examples illustrate the basics of this approach:\n(20.18) When Mary\u2019s \ufb02ight departed, I ate lunch.\n(20.19) When Mary\u2019s \ufb02ight departed, I had eaten lunch.\nIn both of these examples, the eating event has happened in the past, that is, prior\nto the utterance. However, the verb tense in the \ufb01rst example indicates that the eating\nevent began when the \ufb02ight departed, while the second example indicates that the\neating was accomplished prior to the \ufb02ight\u2019s departure. Therefore, in Reichenbach\u2019s\nterms the departure event speci\ufb01es the reference point. These facts can be accom-\nmodated by additional constraints relating the eating anddeparture events. In the\n\ufb01rst example, the reference point precedes the eating event, and in the second exam-\nple, the eating precedes the reference point. Figure 20.12 illustrates Reichenbach\u2019s\napproach with the primary English tenses. Exercise 20.4 asks you to represent these\nexamples in FOL.\nPast PerfectSimple PastPresent Perfect\nSimple FutureFuture PerfectPresentEE\nEER\nRUR,EUR,U\nU,R,EU,RU\nFigure 20.12 Reichenbach\u2019s approach applied to various English tenses. In these diagrams,\ntime \ufb02ows from left to right, Edenotes the time of the event, Rdenotes the reference time,\nandUdenotes the time of the utterance.\nLanguages have many other ways to convey temporal information besides tense.\nMost useful for our purposes will be temporal expressions like in the morning or\n6:45 orafterwards .\n(20.20) I\u2019d like to go at 6:45 in the morning.\n(20.21) Somewhere around noon, please.\n(20.22) I want to take the train back afterwards.\nIncidentally, temporal expressions display a fascinating metaphorical conceptual\norganization. Temporal expressions in English are frequently expressed in spatial\nterms, as is illustrated by the various uses of at,in,somewhere , and near in these\nexamples (Lakoff and Johnson 1980, Jackendoff 1983). Metaphorical organizations\nsuch as these, in which one domain is systematically expressed in terms of another,\nare very common in languages of the world.",
    "metadata": {
      "source": "20",
      "chunk_id": 14,
      "token_count": 598,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\n16 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\n20.5 Representing Aspect\nA related notion to time is aspect , which is what we call the way events can be aspect\ncategorized by their internal temporal structure or temporal contour. By this we\nmean questions like whether events are ongoing or have ended, or whether they are\nconceptualized as happening at a point in time or over some interval. Such notions\nof temporal contour have been used to divide event expressions into classes since\nAristotle, although the set of four classes we\u2019ll introduce here is due to Vendler\n(1967) (you may also see the German term aktionsart used to refer to these classes). aktionsart\nThe most basic aspectual distinction is between events (which involve change) events\nandstates (which do not involve change). Stative expressions represent the notion states\nstative of an event participant being in a state , or having a particular property, at a given\npoint in time. Stative expressions capture aspects of the world at a single point in\ntime, and conceptualize the participant as unchanging and continuous. Consider the\nfollowing ATIS examples.\n(20.23) I like express trains.\n(20.24) I need the cheapest fare.\n(20.25) I want to go \ufb01rst class.\nIn examples like these, the event participant denoted by the subject can be seen as\nexperiencing something at a speci\ufb01c point in time, and don\u2019t involve any kind of\ninternal change over time (the liking or needing is conceptualized as continuous and\nunchanging).\nNon-states (which we\u2019ll refer to as events ) are divided into subclasses; we\u2019ll\nintroduce three here. Activity expressions describe events undertaken by a partic- activity\nipant that occur over a span of time (rather than being conceptualized as a single\npoint in time like stative expressions), and have no particular end point. Of course\nin practice all things end, but the meaning of the expression doesn\u2019t represent this\nfact. Consider the following examples:\n(20.26) She drove a Mazda.\n(20.27) I live in Brooklyn.\nThese examples both specify that the subject is engaged in, or has engaged in, the\nactivity speci\ufb01ed by the verb for some period of time, but doesn\u2019t specify when the\ndriving or living might have stopped.\nTwo more classes of expressions, achievement expressions and accomplish-\nment expressions, describe events that take place over time, but also conceptualize\nthe event as having a particular kind of endpoint or goal. The Greek word telos\nmeans \u2018end\u2019 or \u2019goal\u2019 and so the events described by these kinds of expressions are\noften called telic events. telic\nAccomplishment expressions describe events that have a natural end point andaccomplishment\nexpressions\nresult in a particular state. Consider the following examples:\n(20.28) He booked me a reservation.\n(20.29) The 7:00 train got me to New York City.\nIn these examples, an event is seen as occurring over some period of time that ends\nwhen the intended state is accomplished (i.e., the state of me having a reservation,\nor me being in New York City).\nThe \ufb01nal aspectual class, achievement expressions , is only subtly different thanachievement\nexpressions\naccomplishments. Consider the following:",
    "metadata": {
      "source": "20",
      "chunk_id": 15,
      "token_count": 720,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\n20.6 \u2022 T EMPORALLY ANNOTATED DATASETS : TIMEBANK 17\n(20.30) She found her gate.\n(20.31) I reached New York.\nLike accomplishment expressions, achievement expressions result in a state. But\nunlike accomplishments, achievement events are \u2018punctual\u2019: they are thought of as\nhappening in an instant and the verb doesn\u2019t conceptualize the process or activ-\nity leading up the state. Thus the events in these examples may in fact have been\npreceded by extended searching ortraveling events, but the verb doesn\u2019t conceptu-\nalize these preceding processes, but rather conceptualizes the events corresponding\nto\ufb01nding andreaching as points, not intervals.\nIn summary, a standard way of categorizing event expressions by their temporal\ncontours is via these four general classes:\nStative: I know my departure gate.\nActivity: John is \ufb02ying.\nAccomplishment: Sally booked her \ufb02ight.\nAchievement: She found her gate.\nBefore moving on, note that event expressions can easily be shifted from one\nclass to another. Consider the following examples:\n(20.32) I \ufb02ew.\n(20.33) I \ufb02ew to New York.\nThe \ufb01rst example is a simple activity; it has no natural end point. The second ex-\nample is clearly an accomplishment event since it has an end point, and results in a\nparticular state. Clearly, the classi\ufb01cation of an event is not solely governed by the\nverb, but by the semantics of the entire expression in context.\n20.6 Temporally Annotated Datasets: TimeBank\nTheTimeBank corpus consists of American English text annotated with temporal TimeBank\ninformation (Pustejovsky et al., 2003). The annotations use TimeML (Saur \u00b4\u0131 et al.,\n2006), a markup language for time based on Allen\u2019s interval algebra discussed above\n(Allen, 1984). There are three types of TimeML objects: an E VENT represent events\nand states, a T IME represents time expressions like dates, and a L INK represents\nvarious relationships between events and times (event-event, event-time, and time-\ntime). The links include temporal links (TL INK) for the 13 Allen relations, aspec-\ntual links (AL INK) for aspectual relationships between events and subevents, and\nSLINKS which mark factuality.\nConsider the following sample sentence and its corresponding markup shown in\nFig. 20.13, selected from one of the TimeBank documents.\n(20.34) Delta Air Lines earnings soared 33% to a record in the \ufb01scal \ufb01rst quarter,\nbucking the industry trend toward declining pro\ufb01ts.\nThis text has three events and two temporal expressions (including the creation\ntime of the article, which serves as the document time), and four temporal links that\ncapture the using the Allen relations:\n\u2022 Soaring e1isincluded in the \ufb01scal \ufb01rst quarter t58\n\u2022 Soaring e1isbefore 1989-10-26 t57\n\u2022 Soaring e1issimultaneous with the bucking e3",
    "metadata": {
      "source": "20",
      "chunk_id": 16,
      "token_count": 684,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\n18 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\n<TIMEX3 tid=\"t57\" type=\"DATE\" value=\"1989-10-26\" functionInDocument=\"CREATION_TIME\">\n10/26/89 </TIMEX3>\nDelta Air Lines earnings <EVENT eid=\"e1\" class=\"OCCURRENCE\"> soared </EVENT> 33% to a\nrecord in <TIMEX3 tid=\"t58\" type=\"DATE\" value=\"1989-Q1\" anchorTimeID=\"t57\"> the\nfiscal first quarter </TIMEX3>, <EVENT eid=\"e3\" class=\"OCCURRENCE\">bucking</EVENT>\nthe industry trend toward <EVENT eid=\"e4\" class=\"OCCURRENCE\">declining</EVENT>\nprofits.\nFigure 20.13 Example from the TimeBank corpus.\n\u2022 Declining e4includes soaring e1\nWe can also visualize the links as a graph. The TimeBank snippet in Eq. 20.35\nwould be represented with a graph like Fig. 20.14.\n(20.35) [DCT:11/02/891] 1: Paci\ufb01c First Financial Corp. said 2shareholders\napproved 3itsacquisition 4by Royal Trustco Ltd. of Toronto for $27 a share,\nor $212 million. The thrift holding company said 5itexpects 6toobtain 7\nregulatory approval 8andcomplete 9thetransaction 10byyear-end 11.\n1234567811910BEFOREBEFOREAFTERSIMULTANEOUSENDSCULMINATESBEFOREEVIDENTIALMODALFACTIVEMODALEVIDENTIALMODAL\nFigure 20.14 A graph of the text in Eq. 20.35, adapted from (Ocal et al., 2022). TL INKS\nare shown in blue, AL INKS in red, and SL INKS in green.\n20.7 Automatic Temporal Analysis\nHere we introduce the three common steps used in analyzing time in text:\n1. Extracting temporal expressions\n2.Normalizing these expressions, by converting them to a standard format.\n3.Linking events to times and extracting time graphs and timelines\n20.7.1 Extracting Temporal Expressions\nTemporal expressions are phrases that refer to absolute points in time, relative times,\ndurations, and sets of these. Absolute temporal expressions are those that can be absolute\nmapped directly to calendar dates, times of day, or both. Relative temporal expres- relative\nsions map to particular times through some other reference point (as in a week from\nlast Tuesday ). Finally, durations denote spans of time at varying levels of granular- duration\nity (seconds, minutes, days, weeks, centuries, etc.). Figure 20.15 lists some sample\ntemporal expressions in each of these categories.\nTemporal expressions are grammatical constructions that often have temporal\nlexical triggers as their heads, making them easy to \ufb01nd. Lexical triggers might lexical triggers\nbe nouns, proper nouns, adjectives, and adverbs; full temporal expressions consist",
    "metadata": {
      "source": "20",
      "chunk_id": 17,
      "token_count": 664,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\n20.7 \u2022 A UTOMATIC TEMPORAL ANALYSIS 19\nAbsolute Relative Durations\nApril 24, 1916 yesterday four hours\nThe summer of \u201977 next semester three weeks\n10:15 AM two weeks from yesterday six days\nThe 3rd quarter of 2006 last quarter the last three quarters\nFigure 20.15 Examples of absolute, relational and durational temporal expressions.\nof their phrasal projections: noun phrases, adjective phrases, and adverbial phrases\n(Figure 20.16).\nCategory Examples\nNoun morning ,noon ,night ,winter ,dusk,dawn\nProper Noun January, Monday, Ides, Easter, Rosh Hashana, Ramadan, Tet\nAdjective recent, past, annual, former\nAdverb hourly, daily, monthly, yearly\nFigure 20.16 Examples of temporal lexical triggers.\nThe task is to detect temporal expressions in running text, like this examples,\nshown with TIMEX3 tags (Pustejovsky et al. 2005, Ferro et al. 2005).\nA fare increase initiated <TIMEX3 >last week </TIMEX3 >by UAL\nCorp\u2019s United Airlines was matched by competitors over <TIMEX3 >the\nweekend </TIMEX3 >, marking the second successful fare increase in\n<TIMEX3 >two weeks </TIMEX3 >.\nRule-based approaches use cascades of regular expressions to recognize larger\nand larger chunks from previous stages, based on patterns containing parts of speech,\ntrigger words (e.g., February ) or classes (e.g., MONTH ) (Chang and Manning, 2012;\nStr\u00a8otgen and Gertz, 2013; Chambers, 2013). Here\u2019s a rule from SUTime (Chang and\nManning, 2012) for detecting expressions like 3 years old :\n/(\\d+)[-\\s]($TEUnits)(s)?([-\\s]old)?/\nSequence-labeling approaches use the standard IOB scheme, marking words\nthat are either (I)nside, (O)utside or at the (B)eginning of a temporal expression:\nA\nOfare\nOincrease\nOinitiated\nOlast\nBweek\nIby\nOUAL\nOCorp\u2019s...\nO\nA statistical sequence labeler is trained, using either embeddings or a \ufb01ne-tuned\nencoder, or classic features extracted from the token and context including words,\nlexical triggers, and POS.\nTemporal expression recognizers are evaluated with the usual recall, precision,\nandF-measures. A major dif\ufb01culty for all of these very lexicalized approaches is\navoiding expressions that trigger false positives:\n(20.36) 1984 tells the story of Winston Smith...\n(20.37) ...U2\u2019s classic Sunday Bloody Sunday\n20.7.2 Temporal Normalization\nTemporal normalization is the task of mapping a temporal expression to a pointtemporal\nnormalization\nin time or to a duration. Points in time correspond to calendar dates, to times of\nday, or both. Durations primarily consist of lengths of time. Normalized times\nare represented via the ISO 8601 standard for encoding temporal values (ISO8601,\n2004). Fig. 20.17 reproduces our earlier example with these value attributes.",
    "metadata": {
      "source": "20",
      "chunk_id": 18,
      "token_count": 699,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20",
    "metadata": {
      "source": "20",
      "chunk_id": 19,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "20 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\n<TIMEX3 i d =\u201d t1 \u2019 \u2019 t y p e =\u201dDATE\u201d v a l u e =\u201d 2007 \u221207 \u221202 \u201d f u n c t i o n I n D o c u m e n t =\u201dCREATION TIME\u201d >\nJ u l y 2 , 2007 </TIMEX3 >A f a r e i n c r e a s e i n i t i a t e d <TIMEX3 i d =\u201d t 2 \u201d t y p e =\u201dDATE\u201d\nv a l u e =\u201d 2007 \u2212W26\u201d anchorTimeID=\u201d t 1 \u201d >l a s t week </TIMEX3 >by U n i t e d A i r l i n e s was\nmatched by c o m p e t i t o r s o ve r <TIMEX3 i d =\u201d t 3 \u201d t y p e =\u201dDURATION\u201d v a l u e =\u201dP1WE\u201d\nanchorTimeID=\u201d t 1 \u201d >t h e weekend </TIMEX3 >, marking t h e second s u c c e s s f u l f a r e\ni n c r e a s e i n <TIMEX3 i d =\u201d t 4 \u201d t y p e =\u201dDURATION\u201d v a l u e =\u201dP2W\u201d anchorTimeID=\u201d t 1 \u201d >two\nweeks </TIMEX3 >.\nFigure 20.17 TimeML markup including normalized values for temporal expressions.\nThe dateline, or document date, for this text was July 2, 2007 . The ISO repre-\nsentation for this kind of expression is YYYY-MM-DD, or in this case, 2007-07-02.\nThe encodings for the temporal expressions in our sample text all follow from this\ndate, and are shown here as values for the VALUE attribute.\nThe \ufb01rst temporal expression in the text proper refers to a particular week of the\nyear. In the ISO standard, weeks are numbered from 01 to 53, with the \ufb01rst week\nof the year being the one that has the \ufb01rst Thursday of the year. These weeks are\nrepresented with the template YYYY-Wnn. The ISO week for our document date is\nweek 27; thus the value for last week is represented as \u201c2007-W26\u201d.\nThe next temporal expression is the weekend . ISO weeks begin on Monday;\nthus, weekends occur at the end of a week and are fully contained within a single\nweek. Weekends are treated as durations, so the value of the VALUE attribute has\nto be a length. Durations are represented according to the pattern P nx, where nis\nan integer denoting the length and xrepresents the unit, as in P3Y for three years\nor P2D for two days . In this example, one weekend is captured as P1WE. In this\ncase, there is also suf\ufb01cient information to anchor this particular weekend as part of\na particular week. Such information is encoded in the ANCHOR TIMEID attribute.\nFinally, the phrase two weeks also denotes a duration captured as P2W. Figure 20.18\ngive some more examples, but there is a lot more to the various temporal annotation\nstandards; consult ISO8601 (2004), Ferro et al. (2005), and Pustejovsky et al. (2005)\nfor more details.\nUnit Pattern Sample Value\nFully speci\ufb01ed dates YYYY-MM-DD 1991-09-28",
    "metadata": {
      "source": "20",
      "chunk_id": 20,
      "token_count": 776,
      "chapter_title": ""
    }
  },
  {
    "content": "represented with the template YYYY-Wnn. The ISO week for our document date is\nweek 27; thus the value for last week is represented as \u201c2007-W26\u201d.\nThe next temporal expression is the weekend . ISO weeks begin on Monday;\nthus, weekends occur at the end of a week and are fully contained within a single\nweek. Weekends are treated as durations, so the value of the VALUE attribute has\nto be a length. Durations are represented according to the pattern P nx, where nis\nan integer denoting the length and xrepresents the unit, as in P3Y for three years\nor P2D for two days . In this example, one weekend is captured as P1WE. In this\ncase, there is also suf\ufb01cient information to anchor this particular weekend as part of\na particular week. Such information is encoded in the ANCHOR TIMEID attribute.\nFinally, the phrase two weeks also denotes a duration captured as P2W. Figure 20.18\ngive some more examples, but there is a lot more to the various temporal annotation\nstandards; consult ISO8601 (2004), Ferro et al. (2005), and Pustejovsky et al. (2005)\nfor more details.\nUnit Pattern Sample Value\nFully speci\ufb01ed dates YYYY-MM-DD 1991-09-28\nWeeks YYYY-Wnn 2007-W27\nWeekends PnWE P1WE\n24-hour clock times HH:MM:SS 11:13:45\nDates and times YYYY-MM-DDTHH:MM:SS 1991-09-28T11:00:00\nFinancial quarters Qn 1999-Q3\nFigure 20.18 Sample ISO patterns for representing various times and durations.\nMost current approaches to temporal normalization are rule-based (Chang and\nManning 2012, Str \u00a8otgen and Gertz 2013). Patterns that match temporal expressions\nare associated with semantic analysis procedures. For example, the pattern above for\nrecognizing phrases like 3 years old can be associated with the predicate Duration\nthat takes two arguments, the length and the unit of time:\npattern: /(\\d+)[-\\s]($TEUnits)(s)?([-\\s]old)?/\nresult: Duration($1, $2)\nThe task is dif\ufb01cult because fully quali\ufb01ed temporal expressions are fairly rare\nin real texts. Most temporal expressions in news articles are incomplete and are only\nimplicitly anchored, often with respect to the dateline of the article, which we refer\nto as the document\u2019s temporal anchor . The values of temporal expressions suchtemporal\nanchor\nastoday ,yesterday , ortomorrow can all be computed with respect to this temporal",
    "metadata": {
      "source": "20",
      "chunk_id": 21,
      "token_count": 578,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21\n\n20.7 \u2022 A UTOMATIC TEMPORAL ANALYSIS 21\nanchor. The semantic procedure for today simply assigns the anchor, and the attach-\nments for tomorrow andyesterday add a day and subtract a day from the anchor,\nrespectively. Of course, given the cyclic nature of our representations for months,\nweeks, days, and times of day, our temporal arithmetic procedures must use modulo\narithmetic appropriate to the time unit being used.\nUnfortunately, even simple expressions such as the weekend orWednesday in-\ntroduce a fair amount of complexity. In our current example, the weekend clearly\nrefers to the weekend of the week that immediately precedes the document date. But\nthis won\u2019t always be the case, as is illustrated in the following example.\n(20.38) Random security checks that began yesterday at Sky Harbor will continue\nat least through the weekend.\nIn this case, the expression the weekend refers to the weekend of the week that the\nanchoring date is part of (i.e., the coming weekend). The information that signals\nthis meaning comes from the tense of continue , the verb governing the weekend .\nRelative temporal expressions are handled with temporal arithmetic similar to\nthat used for today andyesterday . The document date indicates that our example\narticle is ISO week 27, so the expression last week normalizes to the current week\nminus 1. To resolve ambiguous next andlastexpressions we consider the distance\nfrom the anchoring date to the nearest unit. Next Friday can refer either to the\nimmediately next Friday or to the Friday following that, but the closer the document\ndate is to a Friday, the more likely it is that the phrase will skip the nearest one. Such\nambiguities are handled by encoding language and domain-speci\ufb01c heuristics into\nthe temporal attachments.\n20.7.3 Temporal Ordering of Events\nThe goal of temporal analysis, is to link times to events and then \ufb01t all these events\ninto a complete timeline. This ambitious task is the subject of considerable current\nresearch but solving it with a high level of accuracy is beyond the capabilities of\ncurrent systems. A somewhat simpler, but still useful, task is to impose a partial or-\ndering on the events and temporal expressions mentioned in a text. Such an ordering\ncan provide many of the same bene\ufb01ts as a true timeline. An example of such a par-\ntial ordering is the determination that the fare increase by American Airlines came\nafter the fare increase by United in our sample text. Determining such an ordering\ncan be viewed as a binary relation detection and classi\ufb01cation task.\nEven this partial ordering task assumes that in addition to the detecting and nor-\nmalizing time expressions steps described above, we have already detected all the\nevents in the text. Indeed, many temporal expressions are anchored to events men-\ntioned in a text and not directly to other temporal expressions. Consider the follow-\ning example:\n(20.39) One week after the storm, JetBlue issued its customer bill of rights.\nTo determine when JetBlue issued its customer bill of rights we need to determine\nthe time of the storm event, and then we need to modify that time by the temporal\nexpression one week after .\nThus once the events and times have been detected, our goal next is to assert links\nbetween all the times and events: i.e. creating event-event, event-time, time-time,\nDCT-event, and DCT-time TimeML TL INKS . This can be done by training time\nrelation classi\ufb01ers to predict the correct T: INKbetween each pair of times/events,\nsupervised by the gold labels in the TimeBank corpus with features like words/em-\nbeddings, parse paths, tense and aspect The sieve-based architecture using precision-",
    "metadata": {
      "source": "20",
      "chunk_id": 22,
      "token_count": 788,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22\n\n22 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\nranked sets of classi\ufb01ers, which we\u2019ll introduce in Chapter 23, is also commonly\nused.\nSystems that perform all 4 tasks (time extraction creation and normalization,\nevent extraction, and time/event linking) include TARSQI (Verhagen et al., 2005)\nCLEAR TK (Bethard, 2013), CAEVO (Chambers et al., 2014), and CATENA (Mirza\nand Tonelli, 2016).\n20.8 Template Filling\nMany texts contain reports of events, and possibly sequences of events, that often\ncorrespond to fairly common, stereotypical situations in the world. These abstract\nsituations or stories, related to what have been called scripts (Schank and Abel- scripts\nson, 1977), consist of prototypical sequences of sub-events, participants, and their\nroles. The strong expectations provided by these scripts can facilitate the proper\nclassi\ufb01cation of entities, the assignment of entities into roles and relations, and most\ncritically, the drawing of inferences that \ufb01ll in things that have been left unsaid. In\ntheir simplest form, such scripts can be represented as templates consisting of \ufb01xed templates\nsets of slots that take as values slot-\ufb01llers belonging to particular classes. The task\noftemplate \ufb01lling is to \ufb01nd documents that invoke particular scripts and then \ufb01ll the template \ufb01lling\nslots in the associated templates with \ufb01llers extracted from the text. These slot-\ufb01llers\nmay consist of text segments extracted directly from the text, or they may consist of\nconcepts that have been inferred from text elements through some additional pro-\ncessing.\nA \ufb01lled template from our original airline story might look like the following.\nFARE-RAISE ATTEMPT :2\n6664LEAD AIRLINE : U NITED AIRLINES\nAMOUNT : $6\nEFFECTIVE DATE: 2006-10-26\nFOLLOWER : A MERICAN AIRLINES3\n7775\nThis template has four slots ( LEAD AIRLINE ,AMOUNT ,EFFECTIVE DATE ,FOL-\nLOWER ). The next section describes a standard sequence-labeling approach to \ufb01lling\nslots. Section 20.8.2 then describes an older system based on the use of cascades of\n\ufb01nite-state transducers and designed to address a more complex template-\ufb01lling task\nthat current learning-based systems don\u2019t yet address.\n20.8.1 Machine Learning Approaches to Template Filling\nIn the standard paradigm for template \ufb01lling, we are given training documents with\ntext spans annotated with prede\ufb01ned templates and their slot \ufb01llers. Our goal is to\ncreate one template for each event in the input, \ufb01lling in the slots with text spans.\nThe task is generally modeled by training two separate supervised systems. The\n\ufb01rst system decides whether the template is present in a particular sentence. This\ntask is called template recognition or sometimes, in a perhaps confusing bit oftemplate\nrecognition\nterminology, event recognition . Template recognition can be treated as a text classi-\n\ufb01cation task, with features extracted from every sequence of words that was labeled\nin training documents as \ufb01lling any slot from the template being detected. The usual\nset of features can be used: tokens, embeddings, word shapes, part-of-speech tags,\nsyntactic chunk tags, and named entity tags.",
    "metadata": {
      "source": "20",
      "chunk_id": 23,
      "token_count": 767,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23",
    "metadata": {
      "source": "20",
      "chunk_id": 24,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "20.8 \u2022 T EMPLATE FILLING 23\nThe second system has the job of role-\ufb01ller extraction . A separate classi\ufb01er isrole-\ufb01ller\nextraction\ntrained to detect each role ( LEAD -AIRLINE ,AMOUNT , and so on). This can be a\nbinary classi\ufb01er that is run on every noun-phrase in the parsed input sentence, or a\nsequence model run over sequences of words. Each role classi\ufb01er is trained on the\nlabeled data in the training set. Again, the usual set of features can be used, but now\ntrained only on an individual noun phrase or the \ufb01llers of a single slot.\nMultiple non-identical text segments might be labeled with the same slot la-\nbel. For example in our sample text, the strings United orUnited Airlines might be\nlabeled as the L EAD AIRLINE . These are not incompatible choices and the corefer-\nence resolution techniques introduced in Chapter 23 can provide a path to a solution.\nA variety of annotated collections have been used to evaluate this style of ap-\nproach to template \ufb01lling, including sets of job announcements, conference calls for\npapers, restaurant guides, and biological texts. A key open question is extracting\ntemplates in cases where there is no training data or even prede\ufb01ned templates, by\ninducing templates as sets of linked events (Chambers and Jurafsky, 2011).\n20.8.2 Earlier Finite-State Template-Filling Systems\nThe templates above are relatively simple. But consider the task of producing a\ntemplate that contained all the information in a text like this one (Grishman and\nSundheim, 1995):\nBridgestone Sports Co. said Friday it has set up a joint venture in Taiwan\nwith a local concern and a Japanese trading house to produce golf clubs to be\nshipped to Japan. The joint venture, Bridgestone Sports Taiwan Co., capital-\nized at 20 million new Taiwan dollars, will start production in January 1990\nwith production of 20,000 iron and \u201cmetal wood\u201d clubs a month.\nThe MUC-5 \u2018joint venture\u2019 task (the Message Understanding Conferences were\na series of U.S. government-organized information-extraction evaluations) was to\nproduce hierarchically linked templates describing joint ventures. Figure 20.19\nshows a structure produced by the FASTUS system (Hobbs et al., 1997). Note how\nthe \ufb01ller of the ACTIVITY slot of the TIE-UPtemplate is itself a template with slots.\nTie-up-1 Activity-1 :\nRELATIONSHIP tie-up C OMPANY Bridgestone Sports Taiwan Co.\nENTITIES Bridgestone Sports Co. P RODUCT iron and \u201cmetal wood\u201d clubs\na local concern S TART DATE DURING: January 1990\na Japanese trading house\nJOINT VENTURE Bridgestone Sports Taiwan Co.\nACTIVITY Activity-1\nAMOUNT NT$20000000\nFigure 20.19 The templates produced by FASTUS given the input text on page 23.\nEarly systems for dealing with these complex templates were based on cascades\nof transducers based on handwritten rules, as sketched in Fig. 20.20.\nThe \ufb01rst four stages use handwritten regular expression and grammar rules to\ndo basic tokenization, chunking, and parsing. Stage 5 then recognizes entities and\nevents with a recognizer based on \ufb01nite-state transducers (FSTs), and inserts the rec-\nognized objects into the appropriate slots in templates. This FST recognizer is based",
    "metadata": {
      "source": "20",
      "chunk_id": 25,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "produce hierarchically linked templates describing joint ventures. Figure 20.19\nshows a structure produced by the FASTUS system (Hobbs et al., 1997). Note how\nthe \ufb01ller of the ACTIVITY slot of the TIE-UPtemplate is itself a template with slots.\nTie-up-1 Activity-1 :\nRELATIONSHIP tie-up C OMPANY Bridgestone Sports Taiwan Co.\nENTITIES Bridgestone Sports Co. P RODUCT iron and \u201cmetal wood\u201d clubs\na local concern S TART DATE DURING: January 1990\na Japanese trading house\nJOINT VENTURE Bridgestone Sports Taiwan Co.\nACTIVITY Activity-1\nAMOUNT NT$20000000\nFigure 20.19 The templates produced by FASTUS given the input text on page 23.\nEarly systems for dealing with these complex templates were based on cascades\nof transducers based on handwritten rules, as sketched in Fig. 20.20.\nThe \ufb01rst four stages use handwritten regular expression and grammar rules to\ndo basic tokenization, chunking, and parsing. Stage 5 then recognizes entities and\nevents with a recognizer based on \ufb01nite-state transducers (FSTs), and inserts the rec-\nognized objects into the appropriate slots in templates. This FST recognizer is based\non hand-built regular expressions like the following (NG indicates Noun-Group and\nVG Verb-Group), which matches the \ufb01rst sentence of the news story above.",
    "metadata": {
      "source": "20",
      "chunk_id": 26,
      "token_count": 314,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24\n\n24 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\nNo. Step Description\n1 Tokens Tokenize input stream of characters\n2 Complex Words Multiword phrases, numbers, and proper names.\n3 Basic phrases Segment sentences into noun and verb groups\n4 Complex phrases Identify complex noun groups and verb groups\n5 Semantic Patterns Identify entities and events, insert into templates.\n6 Merging Merge references to the same entity or event\nFigure 20.20 Levels of processing in FASTUS (Hobbs et al., 1997). Each level extracts a\nspeci\ufb01c type of information which is then passed on to the next higher level.\nNG(Company/ies) VG(Set-up) NG(Joint-Venture) with NG(Company/ies)\nVG(Produce) NG(Product)\nThe result of processing these two sentences is the \ufb01ve draft templates (Fig. 20.21)\nthat must then be merged into the single hierarchical structure shown in Fig. 20.19.\nThe merging algorithm, after performing coreference resolution, merges two activi-\nties that are likely to be describing the same events.\n#Template/Slot Value\n1RELATIONSHIP : TIE-UP\nENTITIES : Bridgestone Co., a local concern, a Japanese trading house\n2ACTIVITY : PRODUCTION\nPRODUCT : \u201cgolf clubs\u201d\n3RELATIONSHIP : TIE-UP\nJOINT VENTURE : \u201cBridgestone Sports Taiwan Co.\u201d\nAMOUNT : NT$20000000\n4ACTIVITY : PRODUCTION\nCOMPANY : \u201cBridgestone Sports Taiwan Co.\u201d\nSTART DATE: DURING : January 1990\n5ACTIVITY : PRODUCTION\nPRODUCT : \u201ciron and \u201cmetal wood\u201d clubs\u201d\nFigure 20.21 The \ufb01ve partial templates produced by stage 5 of FASTUS . These templates\nare merged in stage 6 to produce the \ufb01nal template shown in Fig. 20.19 on page 23.\n20.9 Summary\nThis chapter has explored techniques for extracting limited forms of semantic con-\ntent from texts.\n\u2022Relations among entities can be extracted by pattern-based approaches, su-\npervised learning methods when annotated training data is available, lightly\nsupervised bootstrapping methods when small numbers of seed tuples or\nseed patterns are available, distant supervision when a database of relations\nis available, and unsupervised orOpen IE methods.\n\u2022 Reasoning about time can be facilitated by detection and normalization of\ntemporal expressions .\n\u2022Events can be ordered in time using sequence models and classi\ufb01ers trained\non temporally- and event-labeled data like the TimeBank corpus .",
    "metadata": {
      "source": "20",
      "chunk_id": 27,
      "token_count": 559,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 25",
    "metadata": {
      "source": "20",
      "chunk_id": 28,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 25\n\u2022Template-\ufb01lling applications can recognize stereotypical situations in texts\nand assign elements from the text to roles represented as \ufb01xed sets of slots .\nBibliographical and Historical Notes\nThe earliest work on information extraction addressed the template-\ufb01lling task in the\ncontext of the Frump system (DeJong, 1982). Later work was stimulated by the U.S.\ngovernment-sponsored MUC conferences (Sundheim 1991, Sundheim 1992, Sund-\nheim 1993, Sundheim 1995). Early MUC systems like CIRCUS system (Lehnert\net al., 1991) and SCISOR (Jacobs and Rau, 1990) were quite in\ufb02uential and inspired\nlater systems like FASTUS (Hobbs et al., 1997). Chinchor et al. (1993) describe the\nMUC evaluation techniques.\nDue to the dif\ufb01culty of porting systems from one domain to another, attention\nshifted to machine learning approaches. Early supervised learning approaches to\nIE (Cardie 1993, Cardie 1994, Riloff 1993, Soderland et al. 1995, Huffman 1996)\nfocused on automating the knowledge acquisition process, mainly for \ufb01nite-state\nrule-based systems. Their success, and the earlier success of HMM-based speech\nrecognition, led to the use of sequence labeling (HMMs: Bikel et al. 1997; MEMMs\nMcCallum et al. 2000; CRFs: Lafferty et al. 2001), and a wide exploration of fea-\ntures (Zhou et al., 2005). Neural approaches followed from the pioneering results of\nCollobert et al. (2011), who applied a CRF on top of a convolutional net.\nProgress in this area continues to be stimulated by formal evaluations with shared\nbenchmark datasets, including the Automatic Content Extraction (ACE) evaluations\nof 2000-2007 on named entity recognition, relation extraction, and temporal ex-\npressions1, the KBP (Knowledge Base Population ) evaluations (Ji et al. 2010, Sur- KBP\ndeanu 2013) of relation extraction tasks like slot \ufb01lling (extracting attributes (\u2018slots\u2019) slot \ufb01lling\nlike age, birthplace, and spouse for a given entity) and a series of SemEval work-\nshops (Hendrickx et al., 2009).\nSemisupervised relation extraction was \ufb01rst proposed by Hearst (1992b), and\nextended by systems like AutoSlog-TS (Riloff, 1996), DIPRE (Brin, 1998), SNOW-\nBALL (Agichtein and Gravano, 2000), and Jones et al. (1999). The distant super-\nvision algorithm we describe was drawn from Mintz et al. (2009), who \ufb01rst used\nthe term \u2018distant supervision\u2019 (which was suggested to them by Chris Manning)\nbut similar ideas had occurred in earlier systems like Craven and Kumlien (1999)\nand Morgan et al. (2004) under the name weakly labeled data , as well as in Snow\net al. (2005) and Wu and Weld (2007). Among the many extensions are Wu and\nWeld (2010), Riedel et al. (2010), and Ritter et al. (2013). Open IE systems include",
    "metadata": {
      "source": "20",
      "chunk_id": 29,
      "token_count": 763,
      "chapter_title": ""
    }
  },
  {
    "content": "deanu 2013) of relation extraction tasks like slot \ufb01lling (extracting attributes (\u2018slots\u2019) slot \ufb01lling\nlike age, birthplace, and spouse for a given entity) and a series of SemEval work-\nshops (Hendrickx et al., 2009).\nSemisupervised relation extraction was \ufb01rst proposed by Hearst (1992b), and\nextended by systems like AutoSlog-TS (Riloff, 1996), DIPRE (Brin, 1998), SNOW-\nBALL (Agichtein and Gravano, 2000), and Jones et al. (1999). The distant super-\nvision algorithm we describe was drawn from Mintz et al. (2009), who \ufb01rst used\nthe term \u2018distant supervision\u2019 (which was suggested to them by Chris Manning)\nbut similar ideas had occurred in earlier systems like Craven and Kumlien (1999)\nand Morgan et al. (2004) under the name weakly labeled data , as well as in Snow\net al. (2005) and Wu and Weld (2007). Among the many extensions are Wu and\nWeld (2010), Riedel et al. (2010), and Ritter et al. (2013). Open IE systems include\nKNOW ITALLEtzioni et al. (2005), TextRunner (Banko et al., 2007), and R EVERB\n(Fader et al., 2011). See Riedel et al. (2013) for a universal schema that combines\nthe advantages of distant supervision and Open IE.\n1www.nist.gov/speech/tests/ace/",
    "metadata": {
      "source": "20",
      "chunk_id": 30,
      "token_count": 356,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 26\n\n26 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\nExercises\n20.1 Acronym expansion, the process of associating a phrase with an acronym, can\nbe accomplished by a simple form of relational analysis. Develop a system\nbased on the relation analysis approaches described in this chapter to populate\na database of acronym expansions. If you focus on English Three Letter\nAcronyms (TLAs) you can evaluate your system\u2019s performance by comparing\nit to Wikipedia\u2019s TLA page.\n20.2 Acquire the CMU seminar corpus and develop a template-\ufb01lling system by\nusing any of the techniques mentioned in Section 20.8. Analyze how well\nyour system performs as compared with state-of-the-art results on this corpus.\n20.3 A useful functionality in newer email and calendar applications is the ability\nto associate temporal expressions connected with events in email (doctor\u2019s\nappointments, meeting planning, party invitations, etc.) with speci\ufb01c calendar\nentries. Collect a corpus of email containing temporal expressions related to\nevent planning. How do these expressions compare to the kinds of expressions\ncommonly found in news text that we\u2019ve been discussing in this chapter?\n20.4 For the following sentences, give FOL translations that capture the temporal\nrelationships between the events.\n1. When Mary\u2019s \ufb02ight departed, I ate lunch.\n2. When Mary\u2019s \ufb02ight departed, I had eaten lunch.",
    "metadata": {
      "source": "20",
      "chunk_id": 31,
      "token_count": 311,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 27",
    "metadata": {
      "source": "20",
      "chunk_id": 32,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Exercises 27\nAgichtein, E. and L. Gravano. 2000. Snowball: Extracting\nrelations from large plain-text collections. Proceedings\nof the 5th ACM International Conference on Digital Li-\nbraries .\nAllen, J. 1984. Towards a general theory of action and time.\nArti\ufb01cial Intelligence , 23(2):123\u2013154.\nBanko, M., M. Cafarella, S. Soderland, M. Broadhead, and\nO. Etzioni. 2007. Open information extraction for the\nweb. IJCAI .\nBethard, S. 2013. ClearTK-TimeML: A minimalist approach\nto TempEval 2013. SemEval-13 .\nBikel, D. M., S. Miller, R. Schwartz, and R. Weischedel.\n1997. Nymble: A high-performance learning name-\n\ufb01nder. ANLP .\nBizer, C., J. Lehmann, G. Kobilarov, S. Auer, C. Becker,\nR. Cyganiak, and S. Hellmann. 2009. DBpedia\u2014A crys-\ntallization point for the Web of Data. Web Semantics:\nscience, services and agents on the world wide web ,\n7(3):154\u2013165.\nBollacker, K., C. Evans, P. Paritosh, T. Sturge, and J. Taylor.\n2008. Freebase: a collaboratively created graph database\nfor structuring human knowledge. SIGMOD 2008 .\nBrin, S. 1998. Extracting patterns and relations from\nthe World Wide Web. Proceedings World Wide Web\nand Databases International Workshop, Number 1590 in\nLNCS . Springer.\nCardie, C. 1993. A case-based approach to knowledge ac-\nquisition for domain speci\ufb01c sentence analysis. AAAI .\nCardie, C. 1994. Domain-Speci\ufb01c Knowledge Acquisition\nfor Conceptual Sentence Analysis . Ph.D. thesis, Univer-\nsity of Massachusetts, Amherst, MA. Available as CMP-\nSCI Technical Report 94-74.\nChambers, N. 2013. NavyTime: Event and time ordering\nfrom raw text. SemEval-13 .\nChambers, N., T. Cassidy, B. McDowell, and S. Bethard.\n2014. Dense event ordering with a multi-pass architec-\nture. TACL , 2:273\u2013284.\nChambers, N. and D. Jurafsky. 2011. Template-based infor-\nmation extraction without the templates. ACL.\nChang, A. X. and C. D. Manning. 2012. SUTime: A library\nfor recognizing and normalizing time expressions. LREC .\nChinchor, N., L. Hirschman, and D. L. Lewis. 1993. Eval-\nuating Message Understanding systems: An analysis of\nthe third Message Understanding Conference. Computa-\ntional Linguistics , 19(3):409\u2013449.\nCollobert, R., J. Weston, L. Bottou, M. Karlen,\nK. Kavukcuoglu, and P. Kuksa. 2011. Natural language\nprocessing (almost) from scratch. JMLR , 12:2493\u20132537.\nCraven, M. and J. Kumlien. 1999. Constructing biologi-\ncal knowledge bases by extracting information from text\nsources. ISMB-99 .",
    "metadata": {
      "source": "20",
      "chunk_id": 33,
      "token_count": 763,
      "chapter_title": ""
    }
  },
  {
    "content": "from raw text. SemEval-13 .\nChambers, N., T. Cassidy, B. McDowell, and S. Bethard.\n2014. Dense event ordering with a multi-pass architec-\nture. TACL , 2:273\u2013284.\nChambers, N. and D. Jurafsky. 2011. Template-based infor-\nmation extraction without the templates. ACL.\nChang, A. X. and C. D. Manning. 2012. SUTime: A library\nfor recognizing and normalizing time expressions. LREC .\nChinchor, N., L. Hirschman, and D. L. Lewis. 1993. Eval-\nuating Message Understanding systems: An analysis of\nthe third Message Understanding Conference. Computa-\ntional Linguistics , 19(3):409\u2013449.\nCollobert, R., J. Weston, L. Bottou, M. Karlen,\nK. Kavukcuoglu, and P. Kuksa. 2011. Natural language\nprocessing (almost) from scratch. JMLR , 12:2493\u20132537.\nCraven, M. and J. Kumlien. 1999. Constructing biologi-\ncal knowledge bases by extracting information from text\nsources. ISMB-99 .\nDeJong, G. F. 1982. An overview of the FRUMP system.\nIn W. G. Lehnert and M. H. Ringle, eds, Strategies for\nNatural Language Processing , 149\u2013176. LEA.\nEtzioni, O., M. Cafarella, D. Downey, A.-M. Popescu,\nT. Shaked, S. Soderland, D. S. Weld, and A. Yates. 2005.\nUnsupervised named-entity extraction from the web: An\nexperimental study. Arti\ufb01cial Intelligence , 165(1):91\u2013\n134.Fader, A., S. Soderland, and O. Etzioni. 2011. Identifying\nrelations for open information extraction. EMNLP .\nFerro, L., L. Gerber, I. Mani, B. Sundheim, and G. Wilson.\n2005. Tides 2005 standard for the annotation of temporal\nexpressions. Technical report, MITRE.\nGrishman, R. and B. Sundheim. 1995. Design of the MUC-6\nevaluation. MUC-6 .\nHearst, M. A. 1992a. Automatic acquisition of hyponyms\nfrom large text corpora. COLING .\nHearst, M. A. 1992b. Automatic acquisition of hyponyms\nfrom large text corpora. COLING .\nHearst, M. A. 1998. Automatic discovery of WordNet rela-\ntions. In C. Fellbaum, ed., WordNet: An Electronic Lexi-\ncal Database . MIT Press.\nHendrickx, I., S. N. Kim, Z. Kozareva, P. Nakov,\nD.\u00b4O S\u00b4eaghdha, S. Pad \u00b4o, M. Pennacchiotti, L. Romano,\nand S. Szpakowicz. 2009. Semeval-2010 task 8: Multi-\nway classi\ufb01cation of semantic relations between pairs of\nnominals. 5th International Workshop on Semantic Eval-\nuation .\nHobbs, J. R., D. E. Appelt, J. Bear, D. Israel, M. Kameyama,",
    "metadata": {
      "source": "20",
      "chunk_id": 34,
      "token_count": 765,
      "chapter_title": ""
    }
  },
  {
    "content": "expressions. Technical report, MITRE.\nGrishman, R. and B. Sundheim. 1995. Design of the MUC-6\nevaluation. MUC-6 .\nHearst, M. A. 1992a. Automatic acquisition of hyponyms\nfrom large text corpora. COLING .\nHearst, M. A. 1992b. Automatic acquisition of hyponyms\nfrom large text corpora. COLING .\nHearst, M. A. 1998. Automatic discovery of WordNet rela-\ntions. In C. Fellbaum, ed., WordNet: An Electronic Lexi-\ncal Database . MIT Press.\nHendrickx, I., S. N. Kim, Z. Kozareva, P. Nakov,\nD.\u00b4O S\u00b4eaghdha, S. Pad \u00b4o, M. Pennacchiotti, L. Romano,\nand S. Szpakowicz. 2009. Semeval-2010 task 8: Multi-\nway classi\ufb01cation of semantic relations between pairs of\nnominals. 5th International Workshop on Semantic Eval-\nuation .\nHobbs, J. R., D. E. Appelt, J. Bear, D. Israel, M. Kameyama,\nM. E. Stickel, and M. Tyson. 1997. FASTUS: A cas-\ncaded \ufb01nite-state transducer for extracting information\nfrom natural-language text. In E. Roche and Y . Sch-\nabes, eds, Finite-State Language Processing , 383\u2013406.\nMIT Press.\nHuffman, S. 1996. Learning information extraction pat-\nterns from examples. In S. Wertmer, E. Riloff, and\nG. Scheller, eds, Connectionist, Statistical, and Symbolic\nApproaches to Learning Natural Language Processing ,\n246\u2013260. Springer.\nISO8601. 2004. Data elements and interchange formats\u2014\ninformation interchange\u2014representation of dates and\ntimes. Technical report, International Organization for\nStandards (ISO).\nJackendoff, R. 1983. Semantics and Cognition . MIT Press.\nJacobs, P. S. and L. F. Rau. 1990. SCISOR: A system\nfor extracting information from on-line news. CACM ,\n33(11):88\u201397.\nJi, H., R. Grishman, and H. T. Dang. 2010. Overview of the\ntac 2011 knowledge base population track. TAC-11 .\nJones, R., A. McCallum, K. Nigam, and E. Riloff. 1999.\nBootstrapping for text learning tasks. IJCAI-99 Workshop\non Text Mining: Foundations, Techniques and Applica-\ntions .\nJoshi, M., D. Chen, Y . Liu, D. S. Weld, L. Zettlemoyer, and\nO. Levy. 2020. SpanBERT: Improving pre-training by\nrepresenting and predicting spans. TACL , 8:64\u201377.\nLafferty, J. D., A. McCallum, and F. C. N. Pereira. 2001.\nConditional random \ufb01elds: Probabilistic models for seg-\nmenting and labeling sequence data. ICML .\nLakoff, G. and M. Johnson. 1980. Metaphors We Live By .\nUniversity of Chicago Press, Chicago, IL.",
    "metadata": {
      "source": "20",
      "chunk_id": 35,
      "token_count": 754,
      "chapter_title": ""
    }
  },
  {
    "content": "for extracting information from on-line news. CACM ,\n33(11):88\u201397.\nJi, H., R. Grishman, and H. T. Dang. 2010. Overview of the\ntac 2011 knowledge base population track. TAC-11 .\nJones, R., A. McCallum, K. Nigam, and E. Riloff. 1999.\nBootstrapping for text learning tasks. IJCAI-99 Workshop\non Text Mining: Foundations, Techniques and Applica-\ntions .\nJoshi, M., D. Chen, Y . Liu, D. S. Weld, L. Zettlemoyer, and\nO. Levy. 2020. SpanBERT: Improving pre-training by\nrepresenting and predicting spans. TACL , 8:64\u201377.\nLafferty, J. D., A. McCallum, and F. C. N. Pereira. 2001.\nConditional random \ufb01elds: Probabilistic models for seg-\nmenting and labeling sequence data. ICML .\nLakoff, G. and M. Johnson. 1980. Metaphors We Live By .\nUniversity of Chicago Press, Chicago, IL.\nLehnert, W. G., C. Cardie, D. Fisher, E. Riloff, and\nR. Williams. 1991. Description of the CIRCUS system\nas used for MUC-3. MUC-3 .\nLiu, Y ., M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,\nO. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov.\n2019. RoBERTa: A robustly optimized BERT pretraining\napproach. ArXiv preprint arXiv:1907.11692.",
    "metadata": {
      "source": "20",
      "chunk_id": 36,
      "token_count": 402,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 28",
    "metadata": {
      "source": "20",
      "chunk_id": 37,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "28 Chapter 20 \u2022 Information Extraction: Relations, Events, and Time\nMcCallum, A., D. Freitag, and F. C. N. Pereira. 2000. Max-\nimum entropy Markov models for information extraction\nand segmentation. ICML .\nMintz, M., S. Bills, R. Snow, and D. Jurafsky. 2009. Distant\nsupervision for relation extraction without labeled data.\nACL IJCNLP .\nMirza, P. and S. Tonelli. 2016. CATENA: CAusal and TEm-\nporal relation extraction from NAtural language texts.\nCOLING .\nMorgan, A. A., L. Hirschman, M. Colosimo, A. S. Yeh, and\nJ. B. Colombe. 2004. Gene name identi\ufb01cation and nor-\nmalization using a model organism database. Journal of\nBiomedical Informatics , 37(6):396\u2013410.\nOcal, M., A. Perez, A. Radas, and M. Finlayson. 2022.\nHolistic evaluation of automatic TimeML annotators.\nLREC .\nPustejovsky, J., P. Hanks, R. Saur \u00b4\u0131, A. See, R. Gaizauskas,\nA. Setzer, D. Radev, B. Sundheim, D. S. Day, L. Ferro,\nand M. Lazo. 2003. The TIMEBANK corpus. Proceed-\nings of Corpus Linguistics 2003 Conference . UCREL\nTechnical Paper number 16.\nPustejovsky, J., R. Ingria, R. Saur \u00b4\u0131, J. Casta \u02dcno, J. Littman,\nR. Gaizauskas, A. Setzer, G. Katz, and I. Mani. 2005. The\nSpeci\ufb01cation Language TimeML , chapter 27. Oxford.\nReichenbach, H. 1947. Elements of Symbolic Logic . Macmil-\nlan, New York.\nRiedel, S., L. Yao, and A. McCallum. 2010. Modeling rela-\ntions and their mentions without labeled text. In Machine\nLearning and Knowledge Discovery in Databases , 148\u2013\n163. Springer.\nRiedel, S., L. Yao, A. McCallum, and B. M. Marlin. 2013.\nRelation extraction with matrix factorization and univer-\nsal schemas. NAACL HLT .\nRiloff, E. 1993. Automatically constructing a dictionary for\ninformation extraction tasks. AAAI .\nRiloff, E. 1996. Automatically generating extraction patterns\nfrom untagged text. AAAI .\nRiloff, E. and R. Jones. 1999. Learning dictionaries for in-\nformation extraction by multi-level bootstrapping. AAAI .\nRitter, A., L. Zettlemoyer, Mausam, and O. Etzioni. 2013.\nModeling missing data in distant supervision for infor-\nmation extraction. TACL , 1:367\u2013378.\nSaur\u00b4\u0131, R., J. Littman, B. Knippen, R. Gaizauskas, A. Setzer,\nand J. Pustejovsky. 2006. TimeML annotation guidelines\nversion 1.2.1. Manuscript.\nSchank, R. C. and R. P. Abelson. 1977. Scripts, Plans, Goals",
    "metadata": {
      "source": "20",
      "chunk_id": 38,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "Learning and Knowledge Discovery in Databases , 148\u2013\n163. Springer.\nRiedel, S., L. Yao, A. McCallum, and B. M. Marlin. 2013.\nRelation extraction with matrix factorization and univer-\nsal schemas. NAACL HLT .\nRiloff, E. 1993. Automatically constructing a dictionary for\ninformation extraction tasks. AAAI .\nRiloff, E. 1996. Automatically generating extraction patterns\nfrom untagged text. AAAI .\nRiloff, E. and R. Jones. 1999. Learning dictionaries for in-\nformation extraction by multi-level bootstrapping. AAAI .\nRitter, A., L. Zettlemoyer, Mausam, and O. Etzioni. 2013.\nModeling missing data in distant supervision for infor-\nmation extraction. TACL , 1:367\u2013378.\nSaur\u00b4\u0131, R., J. Littman, B. Knippen, R. Gaizauskas, A. Setzer,\nand J. Pustejovsky. 2006. TimeML annotation guidelines\nversion 1.2.1. Manuscript.\nSchank, R. C. and R. P. Abelson. 1977. Scripts, Plans, Goals\nand Understanding . Lawrence Erlbaum.\nSnow, R., D. Jurafsky, and A. Y . Ng. 2005. Learning syntac-\ntic patterns for automatic hypernym discovery. NeurIPS .\nSoderland, S., D. Fisher, J. Aseltine, and W. G. Lehn-\nert. 1995. CRYSTAL: Inducing a conceptual dictionary.\nIJCAI-95 .\nStr\u00a8otgen, J. and M. Gertz. 2013. Multilingual and cross-\ndomain temporal tagging. Language Resources and Eval-\nuation , 47(2):269\u2013298.\nSundheim, B., ed. 1991. Proceedings of MUC-3 .\nSundheim, B., ed. 1992. Proceedings of MUC-4 .Sundheim, B., ed. 1993. Proceedings of MUC-5 . Baltimore,\nMD.\nSundheim, B., ed. 1995. Proceedings of MUC-6 .\nSurdeanu, M. 2013. Overview of the TAC2013 Knowledge\nBase Population evaluation: English slot \ufb01lling and tem-\nporal slot \ufb01lling. TAC-13 .\nVendler, Z. 1967. Linguistics in Philosophy . Cornell Univer-\nsity Press.\nVerhagen, M., R. Gaizauskas, F. Schilder, M. Hepple,\nJ. Moszkowicz, and J. Pustejovsky. 2009. The TempE-\nval challenge: Identifying temporal relations in text. Lan-\nguage Resources and Evaluation , 43(2):161\u2013179.\nVerhagen, M., I. Mani, R. Sauri, R. Knippen, S. B. Jang,\nJ. Littman, A. Rumshisky, J. Phillips, and J. Pustejovsky.\n2005. Automating temporal annotation with TARSQI.\nACL.\nVrande \u02c7ci\u00b4c, D. and M. Kr \u00a8otzsch. 2014. Wikidata: a free col-\nlaborative knowledge base. CACM , 57(10):78\u201385.",
    "metadata": {
      "source": "20",
      "chunk_id": 39,
      "token_count": 754,
      "chapter_title": ""
    }
  },
  {
    "content": "Sundheim, B., ed. 1995. Proceedings of MUC-6 .\nSurdeanu, M. 2013. Overview of the TAC2013 Knowledge\nBase Population evaluation: English slot \ufb01lling and tem-\nporal slot \ufb01lling. TAC-13 .\nVendler, Z. 1967. Linguistics in Philosophy . Cornell Univer-\nsity Press.\nVerhagen, M., R. Gaizauskas, F. Schilder, M. Hepple,\nJ. Moszkowicz, and J. Pustejovsky. 2009. The TempE-\nval challenge: Identifying temporal relations in text. Lan-\nguage Resources and Evaluation , 43(2):161\u2013179.\nVerhagen, M., I. Mani, R. Sauri, R. Knippen, S. B. Jang,\nJ. Littman, A. Rumshisky, J. Phillips, and J. Pustejovsky.\n2005. Automating temporal annotation with TARSQI.\nACL.\nVrande \u02c7ci\u00b4c, D. and M. Kr \u00a8otzsch. 2014. Wikidata: a free col-\nlaborative knowledge base. CACM , 57(10):78\u201385.\nWu, F. and D. S. Weld. 2007. Autonomously semantifying\nWikipedia. CIKM-07 .\nWu, F. and D. S. Weld. 2010. Open information extraction\nusing Wikipedia. ACL.\nZhang, Y ., V . Zhong, D. Chen, G. Angeli, and C. D. Man-\nning. 2017. Position-aware attention and supervised data\nimprove slot \ufb01lling. EMNLP .\nZhou, G., J. Su, J. Zhang, and M. Zhang. 2005. Exploring\nvarious knowledge in relation extraction. ACL.",
    "metadata": {
      "source": "20",
      "chunk_id": 40,
      "token_count": 426,
      "chapter_title": ""
    }
  }
]