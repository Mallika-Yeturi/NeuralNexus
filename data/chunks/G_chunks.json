[
  {
    "content": "# G",
    "metadata": {
      "source": "G",
      "chunk_id": 0,
      "token_count": 2,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 1",
    "metadata": {
      "source": "G",
      "chunk_id": 1,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\nGWord Senses and WordNet\nLady Bracknell . Are your parents living?\nJack . I have lost both my parents.\nLady Bracknell. To lose one parent, Mr. Worthing, may be regarded as a\nmisfortune; to lose both looks like carelessness.\nOscar Wilde, The Importance of Being Earnest\nWords are ambiguous : the same word can be used to mean different things. In ambiguous\nChapter 6 we saw that the word \u201cmouse\u201d has (at least) two meanings: (1) a small\nrodent, or (2) a hand-operated device to control a cursor. The word \u201cbank\u201d can\nmean: (1) a \ufb01nancial institution or (2) a sloping mound. In the quote above from\nhis play The Importance of Being Earnest , Oscar Wilde plays with two meanings of\n\u201close\u201d (to misplace an object, and to suffer the death of a close person).\nWe say that the words \u2018mouse\u2019 or \u2018bank\u2019 are polysemous (from Greek \u2018having\nmany senses\u2019, poly- \u2018many\u2019 + sema , \u2018sign, mark\u2019).1Asense (orword sense ) is word sense\na discrete representation of one aspect of the meaning of a word. In this chapter\nwe discuss word senses in more detail and introduce WordNet , a large online the- WordNet\nsaurus \u2014a database that represents word senses\u2014with versions in many languages.\nWordNet also represents relations between senses. For example, there is an IS-A\nrelation between dogandmammal (a dog is a kind of mammal) and a part-whole\nrelation between engine andcar(an engine is a part of a car).\nKnowing the relation between two senses can play an important role in tasks\ninvolving meaning. Consider the antonymy relation. Two words are antonyms if\nthey have opposite meanings, like long andshort , orupanddown . Distinguishing\nthese is quite important; if a user asks a dialogue agent to turn up the music, it\nwould be unfortunate to instead turn it down. But in fact in embedding models like\nword2vec, antonyms are easily confused with each other, because often one of the\nclosest words in embedding space to a word (e.g., up) is its antonym (e.g., down ).\nThesauruses that represent this relationship can help!\nWe also introduce word sense disambiguation (WSD ), the task of determiningword sense\ndisambiguation\nwhich sense of a word is being used in a particular context. We\u2019ll give supervised\nand unsupervised algorithms for deciding which sense was intended in a particular\ncontext. This task has a very long history in computational linguistics and many ap-\nplications. In question answering, we can be more helpful to a user who asks about\n\u201cbat care\u201d if we know which sense of bat is relevant. (Is the user a vampire? or\njust wants to play baseball.) And the different senses of a word often have differ-\nent translations; in Spanish the animal bat is a murci \u00b4elago while the baseball bat is\nabate, and indeed word sense algorithms may help improve MT (Pu et al., 2018).\nFinally, WSD has long been used as a tool for evaluating language processing mod-\nels, and understanding how models represent different word senses is an important\n1The word polysemy itself is ambiguous; you may see it used in a different way, to refer only to cases",
    "metadata": {
      "source": "G",
      "chunk_id": 2,
      "token_count": 771,
      "chapter_title": ""
    }
  },
  {
    "content": "word2vec, antonyms are easily confused with each other, because often one of the\nclosest words in embedding space to a word (e.g., up) is its antonym (e.g., down ).\nThesauruses that represent this relationship can help!\nWe also introduce word sense disambiguation (WSD ), the task of determiningword sense\ndisambiguation\nwhich sense of a word is being used in a particular context. We\u2019ll give supervised\nand unsupervised algorithms for deciding which sense was intended in a particular\ncontext. This task has a very long history in computational linguistics and many ap-\nplications. In question answering, we can be more helpful to a user who asks about\n\u201cbat care\u201d if we know which sense of bat is relevant. (Is the user a vampire? or\njust wants to play baseball.) And the different senses of a word often have differ-\nent translations; in Spanish the animal bat is a murci \u00b4elago while the baseball bat is\nabate, and indeed word sense algorithms may help improve MT (Pu et al., 2018).\nFinally, WSD has long been used as a tool for evaluating language processing mod-\nels, and understanding how models represent different word senses is an important\n1The word polysemy itself is ambiguous; you may see it used in a different way, to refer only to cases\nwhere a word\u2019s senses are related in some structured way, reserving the word homonymy to mean sense\nambiguities with no relation between the senses (Haber and Poesio, 2020). Here we will use \u2018polysemy\u2019\nto mean any kind of sense ambiguity, and \u2018structured polysemy\u2019 for polysemy with sense relations.",
    "metadata": {
      "source": "G",
      "chunk_id": 3,
      "token_count": 365,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 2\n\n2APPENDIX G \u2022 W ORD SENSES AND WORDNET\nanalytic direction.\nG.1 Word Senses\nAsense (orword sense ) is a discrete representation of one aspect of the meaning of word sense\na word. Loosely following lexicographic tradition, we represent each sense with a\nsuperscript: bank1andbank2,mouse1andmouse2. In context, it\u2019s easy to see the\ndifferent meanings:\nmouse1: .... a mouse controlling a computer system in 1968.\nmouse2: .... a quiet animal like a mouse\nbank1: ...a bank can hold the investments in a custodial account ...\nbank2: ...as agriculture burgeons on the east bank , the river ...\nG.1.1 De\ufb01ning Word Senses\nHow can we de\ufb01ne the meaning of a word sense? We introduced in Chapter 6 the\nstandard computational approach of representing a word as an embedding , a point\nin semantic space. The intuition of embedding models like word2vec or GloVe is\nthat the meaning of a word can be de\ufb01ned by its co-occurrences, the counts of words\nthat often occur nearby. But that doesn\u2019t tell us how to de\ufb01ne the meaning of a word\nsense . As we saw in Chapter 11, contextual embeddings like BERT go further by\noffering an embedding that represents the meaning of a word in its textual context,\nand we\u2019ll see that contextual embeddings lie at the heart of modern algorithms for\nword sense disambiguation.\nBut \ufb01rst, we need to consider the alternative ways that dictionaries and the-\nsauruses offer for de\ufb01ning senses. One is based on the fact that dictionaries or the-\nsauruses give textual de\ufb01nitions for each sense called glosses . Here are the glosses gloss\nfor two senses of bank :\n1. financial institution that accepts deposits and channels\nthe money into lending activities\n2. sloping land (especially the slope beside a body of water)\nGlosses are not a formal meaning representation; they are just written for people.\nConsider the following fragments from the de\ufb01nitions of right ,left,red, and blood\nfrom the American Heritage Dictionary (Morris, 1985).\nright adj. located nearer the right hand esp. being on the right when\nfacing the same direction as the observer.\nleft adj.located nearer to this side of the body than the right.\nred n.the color of blood or a ruby.\nblood n.the red liquid that circulates in the heart, arteries and veins of\nanimals.\nNote the circularity in these de\ufb01nitions. The de\ufb01nition of right makes two direct\nreferences to itself, and the entry for leftcontains an implicit self-reference in the\nphrase this side of the body , which presumably means the leftside. The entries for\nredandblood reference each other in their de\ufb01nitions. For humans, such entries are\nuseful since the user of the dictionary has suf\ufb01cient grasp of these other terms.",
    "metadata": {
      "source": "G",
      "chunk_id": 4,
      "token_count": 649,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\nG.1 \u2022 W ORD SENSES 3\nYet despite their circularity and lack of formal representation, glosses can still\nbe useful for computational modeling of senses. This is because a gloss is just a sen-\ntence, and from sentences we can compute sentence embeddings that tell us some-\nthing about the meaning of the sense. Dictionaries often give example sentences\nalong with glosses, and these can again be used to help build a sense representation.\nThe second way that thesauruses offer for de\ufb01ning a sense is\u2014like the dictionary\nde\ufb01nitions\u2014de\ufb01ning a sense through its relationship with other senses. For exam-\nple, the above de\ufb01nitions make it clear that right andleftare similar kinds of lemmas\nthat stand in some kind of alternation, or opposition, to one another. Similarly, we\ncan glean that redis a color and that blood is aliquid .Sense relations of this sort\n(IS-A , orantonymy ) are explicitly listed in on-line databases like WordNet . Given\na suf\ufb01ciently large database of such relations, many applications are quite capable\nof performing sophisticated semantic tasks about word senses (even if they do not\nreally know their right from their left).\nG.1.2 How many senses do words have?\nDictionaries and thesauruses give discrete lists of senses. By contrast, embeddings\n(whether static or contextual) offer a continuous high-dimensional model of meaning\nthat doesn\u2019t divide up into discrete senses.\nTherefore creating a thesaurus depends on criteria for deciding when the differ-\ning uses of a word should be represented with discrete senses. We might consider\ntwo senses discrete if they have independent truth conditions, different syntactic be-\nhavior, and independent sense relations, or if they exhibit antagonistic meanings.\nConsider the following uses of the verb serve from the WSJ corpus:\n(G.1) They rarely serve red meat, preferring to prepare seafood.\n(G.2) He served as U.S. ambassador to Norway in 1976 and 1977.\n(G.3) He might have served his time, come out and led an upstanding life.\nTheserve ofserving red meat and that of serving time clearly have different truth\nconditions and presuppositions; the serve ofserve as ambassador has the distinct\nsubcategorization structure serve as NP . These heuristics suggest that these are prob-\nably three distinct senses of serve . One practical technique for determining if two\nsenses are distinct is to conjoin two uses of a word in a single sentence; this kind\nof conjunction of antagonistic readings is called zeugma . Consider the following zeugma\nexamples:\n(G.4) Which of those \ufb02ights serve breakfast?\n(G.5) Does Air France serve Philadelphia?\n(G.6) ?Does Air France serve breakfast and Philadelphia?\nWe use (?) to mark those examples that are semantically ill-formed. The oddness of\nthe invented third example (a case of zeugma) indicates there is no sensible way to\nmake a single sense of serve work for both breakfast and Philadelphia. We can use\nthis as evidence that serve has two different senses in this case.\nDictionaries tend to use many \ufb01ne-grained senses so as to capture subtle meaning\ndifferences, a reasonable approach given that the traditional role of dictionaries is\naiding word learners. For computational purposes, we often don\u2019t need these \ufb01ne\ndistinctions, so we often group or cluster the senses; we have already done this for\nsome of the examples in this chapter. Indeed, clustering examples into senses, or\nsenses into broader-grained categories, is an important computational task that we\u2019ll\ndiscuss in Section G.7.",
    "metadata": {
      "source": "G",
      "chunk_id": 5,
      "token_count": 783,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4APPENDIX G \u2022 W ORD SENSES AND WORDNET\nG.2 Relations Between Senses\nThis section explores the relations between word senses, especially those that have\nreceived signi\ufb01cant computational investigation like synonymy ,antonymy , and hy-\npernymy .\nSynonymy\nWe introduced in Chapter 6 the idea that when two senses of two different words\n(lemmas) are identical, or nearly identical, we say the two senses are synonyms . synonym\nSynonyms include such pairs as\ncouch/sofa vomit/throw up \ufb01lbert/hazelnut car/automobile\nAnd we mentioned that in practice, the word synonym is commonly used to\ndescribe a relationship of approximate or rough synonymy. But furthermore, syn-\nonymy is actually a relationship between senses rather than words. Considering the\nwords bigandlarge . These may seem to be synonyms in the following sentences,\nsince we could swap bigandlarge in either sentence and retain the same meaning:\n(G.7) How big is that plane?\n(G.8) Would I be \ufb02ying on a large or small plane?\nBut note the following sentence in which we cannot substitute large forbig:\n(G.9) Miss Nelson, for instance, became a kind of big sister to Benjamin.\n(G.10) ?Miss Nelson, for instance, became a kind of large sister to Benjamin.\nThis is because the word bighas a sense that means being older or grown up, while\nlarge lacks this sense. Thus, we say that some senses of bigandlarge are (nearly)\nsynonymous while other ones are not.\nAntonymy\nWhereas synonyms are words with identical or similar meanings, antonyms are antonym\nwords with an opposite meaning, like:\nlong/short big/little fast/slow cold/hot dark/light\nrise/fall up/down in/out\nTwo senses can be antonyms if they de\ufb01ne a binary opposition or are at opposite\nends of some scale. This is the case for long/short ,fast/slow , orbig/little, which are\nat opposite ends of the length orsizescale. Another group of antonyms, reversives , reversives\ndescribe change or movement in opposite directions, such as rise/fall orup/down .\nAntonyms thus differ completely with respect to one aspect of their meaning\u2014\ntheir position on a scale or their direction\u2014but are otherwise very similar, sharing\nalmost all other aspects of meaning. Thus, automatically distinguishing synonyms\nfrom antonyms can be dif\ufb01cult.\nTaxonomic Relations\nAnother way word senses can be related is taxonomically. A word (or sense) is a\nhyponym of another word or sense if the \ufb01rst is more speci\ufb01c, denoting a subclass hyponym\nof the other. For example, caris a hyponym of vehicle ,dogis a hyponym of animal ,\nandmango is a hyponym of fruit. Conversely, we say that vehicle is ahypernym of hypernym\ncar, and animal is a hypernym of dog. It is unfortunate that the two words (hypernym",
    "metadata": {
      "source": "G",
      "chunk_id": 6,
      "token_count": 660,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\nG.2 \u2022 R ELATIONS BETWEEN SENSES 5\nand hyponym) are very similar and hence easily confused; for this reason, the word\nsuperordinate is often used instead of hypernym . superordinate\nSuperordinate vehicle fruit furniture mammal\nSubordinate car mango chair dog\nWe can de\ufb01ne hypernymy more formally by saying that the class denoted by\nthe superordinate extensionally includes the class denoted by the hyponym. Thus,\nthe class of animals includes as members all dogs, and the class of moving actions\nincludes all walking actions. Hypernymy can also be de\ufb01ned in terms of entail-\nment . Under this de\ufb01nition, a sense Ais a hyponym of a sense Bif everything\nthat is Ais also B, and hence being an Aentails being a B, or8x A(x))B(x). Hy-\nponymy/hypernymy is usually a transitive relation; if A is a hyponym of B and B is a\nhyponym of C, then A is a hyponym of C. Another name for the hypernym/hyponym\nstructure is the IS-A hierarchy, in which we say A IS-A B, or B subsumes A . IS-A\nHypernymy is useful for tasks like textual entailment or question answering;\nknowing that leukemia is a type of cancer , for example, would certainly be useful in\nanswering questions about leukemia.\nMeronymy\nAnother common relation is meronymy , the part-whole relation. A legis part of a part-whole\nchair ; awheel is part of a car. We say that wheel is ameronym ofcar, and caris a\nholonym ofwheel .\nStructured Polysemy\nThe senses of a word can also be related semantically, in which case we call the\nrelationship between them structured polysemy . Consider this sense bank :structured\npolysemy\n(G.11) The bank is on the corner of Nassau and Witherspoon.\nThis sense, perhaps bank4, means something like \u201cthe building belonging to\na \ufb01nancial institution\u201d. These two kinds of senses (an organization and the build-\ning associated with an organization ) occur together for many other words as well\n(school ,university ,hospital , etc.). Thus, there is a systematic relationship between\nsenses that we might represent as\nBUILDING$ORGANIZATION\nThis particular subtype of polysemy relation is called metonymy . Metonymy is metonymy\nthe use of one aspect of a concept or entity to refer to other aspects of the entity or\nto the entity itself. We are performing metonymy when we use the phrase the White\nHouse to refer to the administration whose of\ufb01ce is in the White House. Other\ncommon examples of metonymy include the relation between the following pairings\nof senses:\nAUTHOR $WORKS OF AUTHOR\n(Jane Austen wrote Emma ) (I really love Jane Austen )\nFRUITTREE $FRUIT\n(Plums have beautiful blossoms ) (I ate a preserved plum yesterday )",
    "metadata": {
      "source": "G",
      "chunk_id": 7,
      "token_count": 664,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6",
    "metadata": {
      "source": "G",
      "chunk_id": 8,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "6APPENDIX G \u2022 W ORD SENSES AND WORDNET\nG.3 WordNet: A Database of Lexical Relations\nThe most commonly used resource for sense relations in English and many other\nlanguages is the WordNet lexical database (Fellbaum, 1998). English WordNet WordNet\nconsists of three separate databases, one each for nouns and verbs and a third for\nadjectives and adverbs; closed class words are not included. Each database contains\na set of lemmas, each one annotated with a set of senses. The WordNet 3.0 release\nhas 117,798 nouns, 11,529 verbs, 22,479 adjectives, and 4,481 adverbs. The aver-\nage noun has 1.23 senses, and the average verb has 2.16 senses. WordNet can be\naccessed on the Web or downloaded locally. Figure G.1 shows the lemma entry for\nthe noun bass.\nThe noun \u201cbass\u201d has 8 senses in WordNet.\n1. bass1- (the lowest part of the musical range)\n2. bass2, bass part1- (the lowest part in polyphonic music)\n3. bass3, basso1- (an adult male singer with the lowest voice)\n4. sea bass1, bass4- (the lean \ufb02esh of a saltwater \ufb01sh of the family Serranidae)\n5. freshwater bass1, bass5- (any of various North American freshwater \ufb01sh with\nlean \ufb02esh (especially of the genus Micropterus))\n6. bass6, bass voice1, basso2- (the lowest adult male singing voice)\n7. bass7- (the member with the lowest range of a family of musical instruments)\n8. bass8- (nontechnical name for any of numerous edible marine and\nfreshwater spiny-\ufb01nned \ufb01shes)\nFigure G.1 A portion of the WordNet 3.0 entry for the noun bass.\nNote that there are eight senses, each of which has a gloss (a dictionary-style gloss\nde\ufb01nition), a list of synonyms for the sense, and sometimes also usage examples.\nWordNet doesn\u2019t represent pronunciation, so doesn\u2019t distinguish the pronunciation\n[b ae s] in bass4,bass5, and bass8from the other senses pronounced [b ey s].\nThe set of near-synonyms for a WordNet sense is called a synset (forsynonym synset\nset); synsets are an important primitive in WordNet. The entry for bass includes\nsynsets likefbass1, deep6g, orfbass6, bass voice1, basso2g. We can think of a\nsynset as representing a concept of the type we discussed in Appendix F. Thus,\ninstead of representing concepts in logical terms, WordNet represents them as lists\nof the word senses that can be used to express the concept. Here\u2019s another synset\nexample:\nfchump1, fool2, gull1, mark9, patsy1, fall guy1,\nsucker1, soft touch1, mug2g\nThe gloss of this synset describes it as:\nGloss : a person who is gullible and easy to take advantage of.\nGlosses are properties of a synset, so that each sense included in the synset has the\nsame gloss and can express this concept. Because they share glosses, synsets like\nthis one are the fundamental unit associated with WordNet entries, and hence it is\nsynsets, not wordforms, lemmas, or individual senses, that participate in most of the",
    "metadata": {
      "source": "G",
      "chunk_id": 9,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "[b ae s] in bass4,bass5, and bass8from the other senses pronounced [b ey s].\nThe set of near-synonyms for a WordNet sense is called a synset (forsynonym synset\nset); synsets are an important primitive in WordNet. The entry for bass includes\nsynsets likefbass1, deep6g, orfbass6, bass voice1, basso2g. We can think of a\nsynset as representing a concept of the type we discussed in Appendix F. Thus,\ninstead of representing concepts in logical terms, WordNet represents them as lists\nof the word senses that can be used to express the concept. Here\u2019s another synset\nexample:\nfchump1, fool2, gull1, mark9, patsy1, fall guy1,\nsucker1, soft touch1, mug2g\nThe gloss of this synset describes it as:\nGloss : a person who is gullible and easy to take advantage of.\nGlosses are properties of a synset, so that each sense included in the synset has the\nsame gloss and can express this concept. Because they share glosses, synsets like\nthis one are the fundamental unit associated with WordNet entries, and hence it is\nsynsets, not wordforms, lemmas, or individual senses, that participate in most of the\nlexical sense relations in WordNet.\nWordNet also labels each synset with a lexicographic category drawn from a\nsemantic \ufb01eld for example the 26 categories for nouns shown in Fig. G.2, as well\nas 15 for verbs (plus 2 for adjectives and 1 for adverbs). These categories are often",
    "metadata": {
      "source": "G",
      "chunk_id": 10,
      "token_count": 362,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\nG.3 \u2022 W ORDNET: A D ATABASE OF LEXICAL RELATIONS 7\ncalled supersenses , because they act as coarse semantic categories or groupings of supersense\nsenses which can be useful when word senses are too \ufb01ne-grained (Ciaramita and\nJohnson 2003, Ciaramita and Altun 2006). Supersenses have also been de\ufb01ned for\nadjectives (Tsvetkov et al., 2014) and prepositions (Schneider et al., 2018).\nCategory Example Category Example Category Example\nACT service GROUP place PLANT tree\nANIMAL dog LOCATION area POSSESSION price\nARTIFACT car MOTIVE reason PROCESS process\nATTRIBUTE quality NATURAL EVENT experience QUANTITY amount\nBODY hair NATURAL OBJECT \ufb02ower RELATION portion\nCOGNITION way OTHER stuff SHAPE square\nCOMMUNICATION review PERSON people STATE pain\nFEELING discomfort PHENOMENON result SUBSTANCE oil\nFOOD food TIME day\nFigure G.2 Supersenses: 26 lexicographic categories for nouns in WordNet.\nG.3.1 Sense Relations in WordNet\nWordNet represents all the kinds of sense relations discussed in the previous section,\nas illustrated in Fig. G.3 and Fig. G.4.\nRelation Also Called De\ufb01nition Example\nHypernym Superordinate From concepts to superordinates breakfast1!meal1\nHyponym Subordinate From concepts to subtypes meal1!lunch1\nInstance Hypernym Instance From instances to their concepts Austen1!author1\nInstance Hyponym Has-Instance From concepts to their instances composer1!Bach1\nPart Meronym Has-Part From wholes to parts table2!leg3\nPart Holonym Part-Of From parts to wholes course7!meal1\nAntonym Semantic opposition between lemmas leader1() follower1\nDerivation Lemmas w/same morphological root destruction1() destroy1\nFigure G.3 Some of the noun relations in WordNet.\nRelation De\ufb01nition Example\nHypernym From events to superordinate events \ufb02y9!travel5\nTroponym From events to subordinate event walk1!stroll1\nEntails From verbs (events) to the verbs (events) they entail snore1!sleep1\nAntonym Semantic opposition between lemmas increase1() decrease1\nFigure G.4 Some verb relations in WordNet.\nFor example WordNet represents hyponymy (page 4) by relating each synset to\nits immediately more general and more speci\ufb01c synsets through direct hypernym\nand hyponym relations. These relations can be followed to produce longer chains of\nmore general or more speci\ufb01c synsets. Figure G.5 shows hypernym chains for bass3\nandbass7; more general synsets are shown on successively indented lines.\nWordNet has two kinds of taxonomic entities: classes and instances. An instance\nis an individual, a proper noun that is a unique entity. San Francisco is an instance\nofcity, for example. But cityis a class, a hyponym of municipality and eventually\noflocation . Fig. G.6 shows a subgraph of WordNet demonstrating many of the\nrelations.",
    "metadata": {
      "source": "G",
      "chunk_id": 11,
      "token_count": 691,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8APPENDIX G \u2022 W ORD SENSES AND WORDNET\nbass3, basso (an adult male singer with the lowest voice)\n=> singer, vocalist, vocalizer, vocaliser\n=> musician, instrumentalist, player\n=> performer, performing artist\n=> entertainer\n=> person, individual, someone...\n=> organism, being\n=> living thing, animate thing,\n=> whole, unit\n=> object, physical object\n=> physical entity\n=> entity\nbass7(member with the lowest range of a family of instruments)\n=> musical instrument, instrument\n=> device\n=> instrumentality, instrumentation\n=> artifact, artefact\n=> whole, unit\n=> object, physical object\n=> physical entity\n=> entity\nFigure G.5 Hyponymy chains for two separate senses of the lemma bass. Note that the\nchains are completely distinct, only converging at the very abstract level whole, unit .\nFigure G.6 WordNet viewed as a graph. Figure from Navigli (2016).\nG.4 Word Sense Disambiguation\nThe task of selecting the correct sense for a word is called word sense disambigua-\ntion, orWSD . WSD algorithms take as input a word in context and a \ufb01xed inventoryword sense\ndisambiguation\nWSD of potential word senses and outputs the correct word sense in context.",
    "metadata": {
      "source": "G",
      "chunk_id": 12,
      "token_count": 287,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9",
    "metadata": {
      "source": "G",
      "chunk_id": 13,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "G.4 \u2022 W ORD SENSE DISAMBIGUATION 9\nG.4.1 WSD: The Task and Datasets\nIn this section we introduce the task setup for WSD, and then turn to algorithms.\nThe inventory of sense tags depends on the task. For sense tagging in the context\nof translation from English to Spanish, the sense tag inventory for an English word\nmight be the set of different Spanish translations. For automatic indexing of med-\nical articles, the sense-tag inventory might be the set of MeSH (Medical Subject\nHeadings) thesaurus entries. Or we can use the set of senses from a resource like\nWordNet, or supersenses if we want a coarser-grain set. Figure G.4.1 shows some\nsuch examples for the word bass.\nWordNet Spanish WordNet\nSense Translation Supersense Target Word in Context\nbass4lubina FOOD . . . \ufb01sh as Paci\ufb01c salmon and striped bass and. . .\nbass7bajo ARTIFACT . . . play bass because he doesn\u2019t have to solo. . .\nFigure G.7 Some possible sense tag inventories for bass.\nIn some situations, we just need to disambiguate a small number of words. In\nsuch lexical sample tasks, we have a small pre-selected set of target words and an lexical sample\ninventory of senses for each word from some lexicon. Since the set of words and the\nset of senses are small, simple supervised classi\ufb01cation approaches work very well.\nMore commonly, however, we have a harder problem in which we have to dis-\nambiguate all the words in some text. In this all-words task, the system is given an all-words\nentire texts and a lexicon with an inventory of senses for each entry and we have to\ndisambiguate every word in the text (or sometimes just every content word). The\nall-words task is similar to part-of-speech tagging, except with a much larger set of\ntags since each lemma has its own set. A consequence of this larger set of tags is\ndata sparseness.\nSupervised all-word disambiguation tasks are generally trained from a semantic\nconcordance , a corpus in which each open-class word in each sentence is labeledsemantic\nconcordance\nwith its word sense from a speci\ufb01c dictionary or thesaurus, most often WordNet.\nThe SemCor corpus is a subset of the Brown Corpus consisting of over 226,036\nwords that were manually tagged with WordNet senses (Miller et al. 1993, Landes\net al. 1998). Other sense-tagged corpora have been built for the SENSEVAL andSe-\nmEval WSD tasks, such as the SENSEVAL -3 Task 1 English all-words test data with\n2282 annotations (Snyder and Palmer, 2004) or the SemEval-13 Task 12 datasets.\nLarge semantic concordances are also available in other languages including Dutch\n(V ossen et al., 2011) and German (Henrich et al., 2012).\nHere\u2019s an example from the SemCor corpus showing the WordNet sense num-\nbers of the tagged words; we\u2019ve used the standard WSD notation in which a subscript\nmarks the part of speech (Navigli, 2009):\n(G.12) You will \ufb01nd9\nvthat avocado1\nnis1\nvunlike1\njother1\njfruit1\nnyou have ever1\nrtasted2\nv\nGiven each noun, verb, adjective, or adverb word in the hand-labeled test set (say",
    "metadata": {
      "source": "G",
      "chunk_id": 14,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "concordance\nwith its word sense from a speci\ufb01c dictionary or thesaurus, most often WordNet.\nThe SemCor corpus is a subset of the Brown Corpus consisting of over 226,036\nwords that were manually tagged with WordNet senses (Miller et al. 1993, Landes\net al. 1998). Other sense-tagged corpora have been built for the SENSEVAL andSe-\nmEval WSD tasks, such as the SENSEVAL -3 Task 1 English all-words test data with\n2282 annotations (Snyder and Palmer, 2004) or the SemEval-13 Task 12 datasets.\nLarge semantic concordances are also available in other languages including Dutch\n(V ossen et al., 2011) and German (Henrich et al., 2012).\nHere\u2019s an example from the SemCor corpus showing the WordNet sense num-\nbers of the tagged words; we\u2019ve used the standard WSD notation in which a subscript\nmarks the part of speech (Navigli, 2009):\n(G.12) You will \ufb01nd9\nvthat avocado1\nnis1\nvunlike1\njother1\njfruit1\nnyou have ever1\nrtasted2\nv\nGiven each noun, verb, adjective, or adverb word in the hand-labeled test set (say\nfruit), the SemCor-based WSD task is to choose the correct sense from the possible\nsenses in WordNet. For fruit this would mean choosing between the correct answer\nfruit1\nn(the ripened reproductive body of a seed plant), and the other two senses fruit2\nn\n(yield; an amount of a product) and fruit3\nn(the consequence of some effort or action).\nFig. G.8 sketches the task.\nWSD systems are typically evaluated intrinsically, by computing F1 against\nhand-labeled sense tags in a held-out set, such as the SemCor corpus or SemEval\ncorpora discussed above.",
    "metadata": {
      "source": "G",
      "chunk_id": 15,
      "token_count": 422,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 APPENDIX G \u2022 W ORD SENSES AND WORDNET\nanelectricguitarandbassplayerstando\ufb00toonesideelectric1: using electricityelectric2:  tenseelectric3: thrillingguitar1 bass1: low range\u2026bass4: sea \ufb01sh\u2026 bass7: instrument\u2026player1: in gameplayer2: musician player3: actor\u2026stand1: upright\u2026stand5: bear\u2026 stand10: put upright\u2026side1: relative region\u2026side3: of body\u2026 side11: slope\u2026x1y1x2y2x3y3y4y5y6\nx4x5x6\nFigure G.8 The all-words WSD task, mapping from input words ( x) to WordNet senses\n(y). Only nouns, verbs, adjectives, and adverbs are mapped, and note that some words (like\nguitar in the example) only have one sense in WordNet. Figure inspired by Chaplot and\nSalakhutdinov (2018).\nA surprisingly strong baseline is simply to choose the most frequent sense formost frequent\nsense\neach word from the senses in a labeled corpus (Gale et al., 1992a). For WordNet, this\ncorresponds to the \ufb01rst sense, since senses in WordNet are generally ordered from\nmost frequent to least frequent based on their counts in the SemCor sense-tagged\ncorpus. The most frequent sense baseline can be quite accurate, and is therefore\noften used as a default, to supply a word sense when a supervised algorithm has\ninsuf\ufb01cient training data.\nA second heuristic, called one sense per discourse is based on the work ofone sense per\ndiscourse\nGale et al. (1992b), who noticed that a word appearing multiple times in a text or\ndiscourse often appears with the same sense. This heuristic seems to hold better for\ncoarse-grained senses and particularly when a word\u2019s senses are unrelated, so isn\u2019t\ngenerally used as a baseline. Nonetheless various kinds of disambiguation tasks\noften include some such bias toward resolving an ambiguity the same way inside a\ndiscourse segment.\nG.4.2 The WSD Algorithm: Contextual Embeddings\nThe best performing WSD algorithm is a simple 1-nearest-neighbor algorithm using\ncontextual word embeddings, due to Melamud et al. (2016) and Peters et al. (2018).\nAt training time we pass each sentence in the SemCore labeled dataset through any\ncontextual embedding (e.g., BERT) resulting in a contextual embedding for each\nlabeled token in SemCore. (There are various ways to compute this contextual em-\nbedding vifor a token i; for BERT it is common to pool multiple layers by summing\nthe vector representations of ifrom the last four BERT layers). Then for each sense\nsof any word in the corpus, for each of the ntokens of that sense, we average their\nncontextual representations vito produce a contextual sense embedding v sfors:\nvs=1\nnX\nivi8vi2tokens (s) (G.13)\nAt test time, given a token of a target word tin context, we compute its contextual\nembedding tand choose its nearest neighbor sense from the training set, i.e., the",
    "metadata": {
      "source": "G",
      "chunk_id": 16,
      "token_count": 708,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\nG.4 \u2022 W ORD SENSE DISAMBIGUATION 11\nsense whose sense embedding has the highest cosine with t:\nsense(t) =argmax\ns2senses (t)cosine (t;vs) (G.14)\nFig. G.9 illustrates the model.\nI  found  the  jar  emptycIcfoundfind1vcthecjarcemptyfind9vfind5vfind4vENCODER\nFigure G.9 The nearest-neighbor algorithm for WSD. In green are the contextual embed-\ndings precomputed for each sense of each word; here we just show a few of the senses for\n\ufb01nd. A contextual embedding is computed for the target word found , and then the nearest\nneighbor sense (in this case \ufb01nd9v) is chosen. Figure inspired by Loureiro and Jorge (2019).\nWhat do we do for words we haven\u2019t seen in the sense-labeled training data?\nAfter all, the number of senses that appear in SemCor is only a small fraction of the\nwords in WordNet. The simplest algorithm is to fall back to the Most Frequent Sense\nbaseline, i.e. taking the \ufb01rst sense in WordNet. But that\u2019s not very satisfactory.\nA more powerful approach, due to Loureiro and Jorge (2019), is to impute the\nmissing sense embeddings, bottom-up, by using the WordNet taxonomy and super-\nsenses. We get a sense embedding for any higher-level node in the WordNet taxon-\nomy by averaging the embeddings of its children, thus computing the embedding for\neach synset as the average of its sense embeddings, the embedding for a hypernym\nas the average of its synset embeddings, and the lexicographic category (supersense)\nembedding as the average of the large set of synset embeddings with that category.\nMore formally, for each missing sense in WordNet \u02c6 s2W, let the sense embeddings\nfor the other members of its synset be S\u02c6s, the hypernym-speci\ufb01c synset embeddings\nbeH\u02c6s, and the lexicographic (supersense-speci\ufb01c) synset embeddings be L\u02c6s. We can\nthen compute the sense embedding for \u02c6 sas follows:\nifjS\u02c6sj>0;v\u02c6s=1\njS\u02c6sjX\nvs;8vs2S\u02c6s (G.15)\nelse ifjH\u02c6sj>0;v\u02c6s=1\njH\u02c6sjX\nvsyn;8vsyn2H\u02c6s (G.16)\nelse ifjL\u02c6sj>0;v\u02c6s=1\njL\u02c6sjX\nvsyn;8vsyn2L\u02c6s (G.17)\nSince all of the supersenses have some labeled data in SemCor, the algorithm is\nguaranteed to have some representation for all possible senses by the time the al-\ngorithm backs off to the most general (supersense) information, although of course\nwith a very coarse model.",
    "metadata": {
      "source": "G",
      "chunk_id": 17,
      "token_count": 665,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 APPENDIX G \u2022 W ORD SENSES AND WORDNET\nG.5 Alternate WSD algorithms and Tasks\nG.5.1 Feature-Based WSD\nFeature-based algorithms for WSD are extremely simple and function almost as\nwell as contextual language model algorithms. The best performing IMS algorithm\n(Zhong and Ng, 2010), augmented by embeddings (Iacobacci et al. 2016, Raganato\net al. 2017b), uses an SVM classi\ufb01er to choose the sense for each input word with\nthe following simple features of the surrounding words:\n\u2022 part-of-speech tags (for a window of 3 words on each side, stopping at sen-\ntence boundaries)\n\u2022collocation features of words or n-grams of lengths 1, 2, 3 at a particular collocation\nlocation in a window of 3 words on each side (i.e., exactly one word to the\nright, or the two words starting 3 words to the left, and so on).\n\u2022 weighted average of embeddings (of all words in a window of 10 words on\neach side, weighted exponentially by distance)\nConsider the ambiguous word bass in the following WSJ sentence:\n(G.18) An electric guitar and bass player stand off to one side,\nIf we used a small 2-word window, a standard feature vector might include parts-of-\nspeech, unigram and bigram collocation features, and a weighted sum gof embed-\ndings, that is:\n[wi\u00002;POS i\u00002;wi\u00001;POS i\u00001;wi+1;POS i+1;wi+2;POS i+2;wi\u00001\ni\u00002;\nwi+2\ni+1;g(E(wi\u00002);E(wi\u00001);E(wi+1);E(wi+2)] (G.19)\nwould yield the following vector:\n[guitar, NN, and, CC, player, NN, stand, VB, guitar and,\nplayer stand, g(E(guitar),E(and),E(player),E(stand))]\nG.5.2 The Lesk Algorithm as WSD Baseline\nGenerating sense labeled corpora like SemCor is quite dif\ufb01cult and expensive. An\nalternative class of WSD algorithms, knowledge-based algorithms, rely solely onknowledge-\nbased\nWordNet or other such resources and don\u2019t require labeled data. While supervised\nalgorithms generally work better, knowledge-based methods can be used in lan-\nguages or domains where thesauruses or dictionaries but not sense labeled corpora\nare available.\nThe Lesk algorithm is the oldest and most powerful knowledge-based WSD Lesk algorithm\nmethod, and is a useful baseline. Lesk is really a family of algorithms that choose\nthe sense whose dictionary gloss or de\ufb01nition shares the most words with the target\nword\u2019s neighborhood. Figure G.10 shows the simplest version of the algorithm,\noften called the Simpli\ufb01ed Lesk algorithm (Kilgarriff and Rosenzweig, 2000). Simpli\ufb01ed Lesk\nAs an example of the Lesk algorithm at work, consider disambiguating the word\nbank in the following context:\n(G.20) The bank can guarantee deposits will eventually cover future tuition costs\nbecause it invests in adjustable-rate mortgage securities.\ngiven the following two WordNet senses:",
    "metadata": {
      "source": "G",
      "chunk_id": 18,
      "token_count": 715,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\nG.5 \u2022 A LTERNATE WSD ALGORITHMS AND TASKS 13\nfunction SIMPLIFIED LESK(word, sentence )returns best sense of word\nbest-sense most frequent sense for word\nmax-overlap 0\ncontext set of words in sentence\nfor each sense insenses of word do\nsignature set of words in the gloss and examples of sense\noverlap COMPUTE OVERLAP (signature ,context )\nifoverlap >max-overlap then\nmax-overlap overlap\nbest-sense sense\nend\nreturn (best-sense )\nFigure G.10 The Simpli\ufb01ed Lesk algorithm. The C OMPUTE OVERLAP function returns\nthe number of words in common between two sets, ignoring function words or other words\non a stop list. The original Lesk algorithm de\ufb01nes the context in a more complex way.\nbank1Gloss: a \ufb01nancial institution that accepts deposits and channels the\nmoney into lending activities\nExamples: \u201che cashed a check at the bank\u201d, \u201cthat bank holds the mortgage\non my home\u201d\nbank2Gloss: sloping land (especially the slope beside a body of water)\nExamples: \u201cthey pulled the canoe up on the bank\u201d, \u201che sat on the bank of\nthe river and watched the currents\u201d\nSense bank1has two non-stopwords overlapping with the context in (G.20):\ndeposits andmortgage , while sense bank2has zero words, so sense bank1is chosen.\nThere are many obvious extensions to Simpli\ufb01ed Lesk, such as weighing the\noverlapping words by IDF (inverse document frequency) (Chapter 6) to downweight\nfrequent words like function words; best performing is to use word embedding co-\nsine instead of word overlap to compute the similarity between the de\ufb01nition and the\ncontext (Basile et al., 2014). Modern neural extensions of Lesk use the de\ufb01nitions\nto compute sense embeddings that can be directly used instead of SemCor-training\nembeddings (Kumar et al. 2019, Luo et al. 2018a, Luo et al. 2018b).\nG.5.3 Word-in-Context Evaluation\nWord Sense Disambiguation is a much more \ufb01ne-grained evaluation of word mean-\ning than the context-free word similarity tasks we described in Chapter 6. Recall that\ntasks like LexSim-999 require systems to match human judgments on the context-\nfree similarity between two words (how similar is cuptomug?). We can think of\nWSD as a kind of contextualized similarity task, since our goal is to be able to distin-\nguish the meaning of a word like bass in one context (playing music) from another\ncontext (\ufb01shing).\nSomewhere in between lies the word-in-context task. Here the system is given word-in-context\ntwo sentences, each with the same target word but in a different sentential context.\nThe system must decide whether the target words are used in the same sense in the\ntwo sentences or in a different sense . Fig. G.11 shows sample pairs from the WiC WiC\ndataset of Pilehvar and Camacho-Collados (2019).\nThe WiC sentences are mainly taken from the example usages for senses in\nWordNet. But WordNet senses are very \ufb01ne-grained. For this reason tasks like",
    "metadata": {
      "source": "G",
      "chunk_id": 19,
      "token_count": 719,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n14 APPENDIX G \u2022 W ORD SENSES AND WORDNET\nF There\u2019s a lot of trash on the bedof the river \u2014\nI keep a glass of water next to my bedwhen I sleep\nFJustify the margins \u2014 The end justi\ufb01es the means\nTAirpollution \u2014 Open a window and let in some air\nT The expanded window will give us time to catch the thieves \u2014\nYou have a two-hour window of clear weather to \ufb01nish working on the lawn\nFigure G.11 Positive (T) and negative (F) pairs from the WiC dataset (Pilehvar and\nCamacho-Collados, 2019).\nword-in-context \ufb01rst cluster the word senses into coarser clusters, so that the two\nsentential contexts for the target word are marked as T if the two senses are in the\nsame cluster. WiC clusters all pairs of senses if they are \ufb01rst degree connections in\nthe WordNet semantic graph, including sister senses, or if they belong to the same\nsupersense; we point to other sense clustering algorithms at the end of the chapter.\nThe baseline algorithm to solve the WiC task uses contextual embeddings like\nBERT with a simple thresholded cosine. We \ufb01rst compute the contextual embed-\ndings for the target word in each of the two sentences, and then compute the cosine\nbetween them. If it\u2019s above a threshold tuned on a devset we respond true (the two\nsenses are the same) else we respond false.\nG.5.4 Wikipedia as a source of training data\nDatasets other than SemCor have been used for all-words WSD. One important di-\nrection is to use Wikipedia as a source of sense-labeled data. When a concept is\nmentioned in a Wikipedia article, the article text may contain an explicit link to the\nconcept\u2019s Wikipedia page, which is named by a unique identi\ufb01er. This link can be\nused as a sense annotation. For example, the ambiguous word baris linked to a\ndifferent Wikipedia article depending on its meaning in context, including the page\nBAR(LAW), the page B AR(MUSIC ), and so on, as in the following Wikipedia\nexamples (Mihalcea, 2007).\nIn 1834, Sumner was admitted to the [[bar (law) jbar]] at the age of\ntwenty-three, and entered private practice in Boston.\nIt is danced in 3/4 time (like most waltzes), with the couple turning\napprox. 180 degrees every [[bar (music)jbar]] .\nJenga is a popular beer in the [[bar (establishment) jbar]] s of Thailand.\nThese sentences can then be added to the training data for a supervised system.\nIn order to use Wikipedia in this way, however, it is necessary to map from Wiki-\npedia concepts to whatever inventory of senses is relevant for the WSD application.\nAutomatic algorithms that map from Wikipedia to WordNet, for example, involve\n\ufb01nding the WordNet sense that has the greatest lexical overlap with the Wikipedia\nsense, by comparing the vector of words in the WordNet synset, gloss, and related\nsenses with the vector of words in the Wikipedia page title, outgoing links, and page\ncategory (Ponzetto and Navigli, 2010). The resulting mapping has been used to\ncreate BabelNet, a large sense-annotated resource (Navigli and Ponzetto, 2012).",
    "metadata": {
      "source": "G",
      "chunk_id": 20,
      "token_count": 736,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\nG.6 \u2022 U SING THESAURUSES TO IMPROVE EMBEDDINGS 15\nG.6 Using Thesauruses to Improve Embeddings\nThesauruses have also been used to improve both static and contextual word em-\nbeddings. For example, static word embeddings have a problem with antonyms .\nA word like expensive is often very similar in embedding cosine to its antonym\nlikecheap . Antonymy information from thesauruses can help solve this problem;\nFig. G.12 shows nearest neighbors to some target words in GloVe, and the improve-\nment after one such method.\nBefore counter\ufb01tting After counter\ufb01tting\neast west north south eastward eastern easterly\nexpensive pricey cheaper costly costly pricy overpriced\nBritish American Australian Britain Brits London BBC\nFigure G.12 The nearest neighbors in GloVe to east,expensive , and British include\nantonyms like west. The right side showing the improvement in GloVe nearest neighbors\nafter the counter\ufb01tting method (Mrk \u02c7si\u00b4c et al., 2016).\nThere are two families of solutions. The \ufb01rst requires retraining: we modify the\nembedding training to incorporate thesaurus relations like synonymy, antonym, or\nsupersenses. This can be done by modifying the static embedding loss function for\nword2vec (Yu and Dredze 2014, Nguyen et al. 2016) or by modifying contextual\nembedding training (Levine et al. 2020, Lauscher et al. 2019).\nThe second, for static embeddings, is more light-weight; after the embeddings\nhave been trained we learn a second mapping based on a thesaurus that shifts the\nembeddings of words in such a way that synonyms (according to the thesaurus) are\npushed closer and antonyms further apart. Such methods are called retro\ufb01tting retro\ufb01tting\n(Faruqui et al. 2015, Lengerich et al. 2018) or counter\ufb01tting (Mrk \u02c7si\u00b4c et al., 2016).\nG.7 Word Sense Induction\nIt is expensive and dif\ufb01cult to build large corpora in which each word is labeled for\nits word sense. For this reason, an unsupervised approach to sense disambiguation,\noften called word sense induction orWSI , is an important direction. In unsu-word sense\ninduction\npervised approaches, we don\u2019t use human-de\ufb01ned word senses. Instead, the set of\n\u201csenses\u201d of each word is created automatically from the instances of each word in\nthe training set.\nMost algorithms for word sense induction follow the early work of Sch \u00a8utze\n(Sch \u00a8utze 1992, Sch \u00a8utze 1998) in using some sort of clustering over word embed-\ndings. In training, we use three steps:\n1. For each token wiof word win a corpus, compute a context vector c.\n2. Use a clustering algorithm tocluster these word-token context vectors cinto\na prede\ufb01ned number of groups or clusters. Each cluster de\ufb01nes a sense of w.\n3. Compute the vector centroid of each cluster. Each vector centroid sjis a\nsense vector representing that sense of w.\nSince this is an unsupervised algorithm, we don\u2019t have names for each of these\n\u201csenses\u201d of w; we just refer to the jth sense of w.\nTo disambiguate a particular token tofwwe again have three steps:",
    "metadata": {
      "source": "G",
      "chunk_id": 21,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\n16 APPENDIX G \u2022 W ORD SENSES AND WORDNET\n1. Compute a context vector cfort.\n2. Retrieve all sense vectors sjforw.\n3. Assign tto the sense represented by the sense vector sjthat is closest to t.\nAll we need is a clustering algorithm and a distance metric between vectors.\nClustering is a well-studied problem with a wide number of standard algorithms that\ncan be applied to inputs structured as vectors of numerical values (Duda and Hart,\n1973). A frequently used technique in language applications is known as agglom-\nerative clustering . In this technique, each of the Ntraining instances is initiallyagglomerative\nclustering\nassigned to its own cluster. New clusters are then formed in a bottom-up fashion by\nthe successive merging of the two clusters that are most similar. This process con-\ntinues until either a speci\ufb01ed number of clusters is reached, or some global goodness\nmeasure among the clusters is achieved. In cases in which the number of training\ninstances makes this method too expensive, random sampling can be used on the\noriginal training set to achieve similar results.\nHow can we evaluate unsupervised sense disambiguation approaches? As usual,\nthe best way is to do extrinsic evaluation embedded in some end-to-end system; one\nexample used in a SemEval bakeoff is to improve search result clustering and di-\nversi\ufb01cation (Navigli and Vannella, 2013). Intrinsic evaluation requires a way to\nmap the automatically derived sense classes into a hand-labeled gold-standard set so\nthat we can compare a hand-labeled test set with a set labeled by our unsupervised\nclassi\ufb01er. Various such metrics have been tested, for example in the SemEval tasks\n(Manandhar et al. 2010, Navigli and Vannella 2013, Jurgens and Klapaftis 2013),\nincluding cluster overlap metrics, or methods that map each sense cluster to a pre-\nde\ufb01ned sense by choosing the sense that (in some training set) has the most overlap\nwith the cluster. However it is fair to say that no evaluation metric for this task has\nyet become standard.\nG.8 Summary\nThis chapter has covered a wide range of issues concerning the meanings associated\nwith lexical items. The following are among the highlights:\n\u2022 Aword sense is the locus of word meaning; de\ufb01nitions and meaning relations\nare de\ufb01ned at the level of the word sense rather than wordforms.\n\u2022 Many words are polysemous , having many senses.\n\u2022 Relations between senses include synonymy ,antonymy ,meronymy , and\ntaxonomic relations hyponymy andhypernymy .\n\u2022WordNet is a large database of lexical relations for English, and WordNets\nexist for a variety of languages.\n\u2022Word-sense disambiguation (WSD ) is the task of determining the correct\nsense of a word in context. Supervised approaches make use of a corpus\nof sentences in which individual words ( lexical sample task ) or all words\n(all-words task ) are hand-labeled with senses from a resource like WordNet.\nSemCor is the largest corpus with WordNet-labeled senses.\n\u2022 The standard supervised algorithm for WSD is nearest neighbors with contex-\ntual embeddings.\n\u2022 Feature-based algorithms using parts of speech and embeddings of words in\nthe context of the target word also work well.",
    "metadata": {
      "source": "G",
      "chunk_id": 22,
      "token_count": 730,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\nBIBLIOGRAPHICAL AND HISTORICAL NOTES 17\n\u2022 An important baseline for WSD is the most frequent sense , equivalent, in\nWordNet, to take the \ufb01rst sense .\n\u2022 Another baseline is a knowledge-based WSD algorithm called the Lesk al-\ngorithm which chooses the sense whose dictionary de\ufb01nition shares the most\nwords with the target word\u2019s neighborhood.\n\u2022Word sense induction is the task of learning word senses unsupervised.\nBibliographical and Historical Notes\nWord sense disambiguation traces its roots to some of the earliest applications of\ndigital computers. The insight that underlies modern algorithms for word sense dis-\nambiguation was \ufb01rst articulated by Weaver (1949/1955) in the context of machine\ntranslation:\nIf one examines the words in a book, one at a time as through an opaque\nmask with a hole in it one word wide, then it is obviously impossible\nto determine, one at a time, the meaning of the words. [. . . ] But if\none lengthens the slit in the opaque mask, until one can see not only\nthe central word in question but also say N words on either side, then\nif N is large enough one can unambiguously decide the meaning of the\ncentral word. [. . . ] The practical question is : \u201cWhat minimum value of\nN will, at least in a tolerable fraction of cases, lead to the correct choice\nof meaning for the central word?\u201d\nOther notions \ufb01rst proposed in this early period include the use of a thesaurus for dis-\nambiguation (Masterman, 1957), supervised training of Bayesian models for disam-\nbiguation (Madhu and Lytel, 1965), and the use of clustering in word sense analysis\n(Sparck Jones, 1986).\nMuch disambiguation work was conducted within the context of early AI-oriented\nnatural language processing systems. Quillian (1968) and Quillian (1969) proposed\na graph-based approach to language processing, in which the de\ufb01nition of a word\nwas represented by a network of word nodes connected by syntactic and semantic\nrelations, and sense disambiguation by \ufb01nding the shortest path between senses in\nthe graph. Simmons (1973) is another in\ufb02uential early semantic network approach.\nWilks proposed one of the earliest non-discrete models with his Preference Seman-\ntics(Wilks 1975c, Wilks 1975b, Wilks 1975a), and Small and Rieger (1982) and\nRiesbeck (1975) proposed understanding systems based on modeling rich procedu-\nral information for each word. Hirst\u2019s ABSITY system (Hirst and Charniak 1982,\nHirst 1987, Hirst 1988), which used a technique called marker passing based on se-\nmantic networks, represents the most advanced system of this type. As with these\nlargely symbolic approaches, early neural network (at the time called \u2018connection-\nist\u2019) approaches to word sense disambiguation relied on small lexicons with hand-\ncoded representations (Cottrell 1985, Kawamoto 1988).\nThe earliest implementation of a robust empirical approach to sense disambigua-\ntion is due to Kelly and Stone (1975), who directed a team that hand-crafted a set\nof disambiguation rules for 1790 ambiguous English words. Lesk (1986) was the\n\ufb01rst to use a machine-readable dictionary for word sense disambiguation. Fellbaum\n(1998) collects early work on WordNet. Early work using dictionaries as lexical",
    "metadata": {
      "source": "G",
      "chunk_id": 23,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18",
    "metadata": {
      "source": "G",
      "chunk_id": 24,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "18 APPENDIX G \u2022 W ORD SENSES AND WORDNET\nresources include Amsler\u2019s 1981 use of the Merriam Webster dictionary and Long-\nman\u2019s Dictionary of Contemporary English (Boguraev and Briscoe, 1989).\nSupervised approaches to disambiguation began with the use of decision trees\nby Black (1988). In addition to the IMS and contextual-embedding based methods\nfor supervised WSD, recent supervised algorithms includes encoder-decoder models\n(Raganato et al., 2017a).\nThe need for large amounts of annotated text in supervised methods led early\non to investigations into the use of bootstrapping methods (Hearst 1991, Yarowsky\n1995). For example the semi-supervised algorithm of Diab and Resnik (2002) is\nbased on aligned parallel corpora in two languages. For example, the fact that the\nFrench word catastrophe might be translated as English disaster in one instance\nandtragedy in another instance can be used to disambiguate the senses of the two\nEnglish words (i.e., to choose senses of disaster andtragedy that are similar).\nThe earliest use of clustering in the study of word senses was by Sparck Jones\n(1986); Pedersen and Bruce (1997), Sch \u00a8utze (1997), and Sch \u00a8utze (1998) applied dis-\ntributional methods. Clustering word senses into coarse senses has also been used coarse senses\nto address the problem of dictionary senses being too \ufb01ne-grained (Section G.5.3)\n(Dolan 1994, Chen and Chang 1998, Mihalcea and Moldovan 2001, Agirre and\nde Lacalle 2003, Palmer et al. 2004, Navigli 2006, Snow et al. 2007, Pilehvar et al.\n2013). Corpora with clustered word senses for training supervised clustering algo-\nrithms include Palmer et al. (2006) and OntoNotes (Hovy et al., 2006). OntoNotes\nSee Pustejovsky (1995), Pustejovsky and Boguraev (1996), Martin (1986), and\nCopestake and Briscoe (1995), inter alia, for computational approaches to the rep-\nresentation of polysemy. Pustejovsky\u2019s theory of the generative lexicon , and ingenerative\nlexicon\nparticular his theory of the qualia structure of words, is a way of accounting for thequalia\nstructure\ndynamic systematic polysemy of words in context.\nHistorical overviews of WSD include Agirre and Edmonds (2006) and Navigli\n(2009).\nExercises\nG.1 Collect a small corpus of example sentences of varying lengths from any\nnewspaper or magazine. Using WordNet or any standard dictionary, deter-\nmine how many senses there are for each of the open-class words in each sen-\ntence. How many distinct combinations of senses are there for each sentence?\nHow does this number seem to vary with sentence length?\nG.2 Using WordNet or a standard reference dictionary, tag each open-class word\nin your corpus with its correct tag. Was choosing the correct sense always a\nstraightforward task? Report on any dif\ufb01culties you encountered.\nG.3 Using your favorite dictionary, simulate the original Lesk word overlap dis-\nambiguation algorithm described on page 13 on the phrase Time \ufb02ies like an\narrow . Assume that the words are to be disambiguated one at a time, from",
    "metadata": {
      "source": "G",
      "chunk_id": 25,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "resentation of polysemy. Pustejovsky\u2019s theory of the generative lexicon , and ingenerative\nlexicon\nparticular his theory of the qualia structure of words, is a way of accounting for thequalia\nstructure\ndynamic systematic polysemy of words in context.\nHistorical overviews of WSD include Agirre and Edmonds (2006) and Navigli\n(2009).\nExercises\nG.1 Collect a small corpus of example sentences of varying lengths from any\nnewspaper or magazine. Using WordNet or any standard dictionary, deter-\nmine how many senses there are for each of the open-class words in each sen-\ntence. How many distinct combinations of senses are there for each sentence?\nHow does this number seem to vary with sentence length?\nG.2 Using WordNet or a standard reference dictionary, tag each open-class word\nin your corpus with its correct tag. Was choosing the correct sense always a\nstraightforward task? Report on any dif\ufb01culties you encountered.\nG.3 Using your favorite dictionary, simulate the original Lesk word overlap dis-\nambiguation algorithm described on page 13 on the phrase Time \ufb02ies like an\narrow . Assume that the words are to be disambiguated one at a time, from\nleft to right, and that the results from earlier decisions are used later in the\nprocess.\nG.4 Build an implementation of your solution to the previous exercise. Using\nWordNet, implement the original Lesk word overlap disambiguation algo-\nrithm described on page 13 on the phrase Time \ufb02ies like an arrow .",
    "metadata": {
      "source": "G",
      "chunk_id": 26,
      "token_count": 342,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19",
    "metadata": {
      "source": "G",
      "chunk_id": 27,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Exercises 19\nAgirre, E. and O. L. de Lacalle. 2003. Clustering WordNet\nword senses. RANLP 2003 .\nAgirre, E. and P. Edmonds, eds. 2006. Word Sense Disam-\nbiguation: Algorithms and Applications . Kluwer.\nAmsler, R. A. 1981. A taxonomy for English nouns and\nverbs. ACL.\nBasile, P., A. Caputo, and G. Semeraro. 2014. An enhanced\nLesk word sense disambiguation algorithm through a dis-\ntributional semantic model. COLING .\nBlack, E. 1988. An experiment in computational discrimi-\nnation of English word senses. IBM Journal of Research\nand Development , 32(2):185\u2013194.\nBoguraev, B. K. and T. Briscoe, eds. 1989. Computational\nLexicography for Natural Language Processing . Long-\nman.\nChaplot, D. S. and R. Salakhutdinov. 2018. Knowledge-\nbased word sense disambiguation using topic models.\nAAAI .\nChen, J. N. and J. S. Chang. 1998. Topical clustering of\nMRD senses based on information retrieval techniques.\nComputational Linguistics , 24(1):61\u201396.\nCiaramita, M. and Y . Altun. 2006. Broad-coverage sense\ndisambiguation and information extraction with a super-\nsense sequence tagger. EMNLP .\nCiaramita, M. and M. Johnson. 2003. Supersense tagging of\nunknown nouns in WordNet. EMNLP-2003 .\nCopestake, A. and T. Briscoe. 1995. Semi-productive\npolysemy and sense extension. Journal of Semantics ,\n12(1):15\u201368.\nCottrell, G. W. 1985. A Connectionist Approach to Word\nSense Disambiguation . Ph.D. thesis, University of\nRochester, Rochester, NY . Revised version published by\nPitman, 1989.\nDiab, M. and P. Resnik. 2002. An unsupervised method for\nword sense tagging using parallel corpora. ACL.\nDolan, B. 1994. Word sense ambiguation: Clustering related\nsenses. COLING .\nDuda, R. O. and P. E. Hart. 1973. Pattern Classi\ufb01cation and\nScene Analysis . John Wiley and Sons.\nFaruqui, M., J. Dodge, S. K. Jauhar, C. Dyer, E. Hovy, and\nN. A. Smith. 2015. Retro\ufb01tting word vectors to semantic\nlexicons. NAACL HLT .\nFellbaum, C., ed. 1998. WordNet: An Electronic Lexical\nDatabase . MIT Press.\nGale, W. A., K. W. Church, and D. Yarowsky. 1992a. Es-\ntimating upper and lower bounds on the performance of\nword-sense disambiguation programs. ACL.\nGale, W. A., K. W. Church, and D. Yarowsky. 1992b. One\nsense per discourse. HLT.\nHaber, J. and M. Poesio. 2020. Assessing polyseme sense\nsimilarity through co-predication acceptability and con-",
    "metadata": {
      "source": "G",
      "chunk_id": 28,
      "token_count": 763,
      "chapter_title": ""
    }
  },
  {
    "content": "word sense tagging using parallel corpora. ACL.\nDolan, B. 1994. Word sense ambiguation: Clustering related\nsenses. COLING .\nDuda, R. O. and P. E. Hart. 1973. Pattern Classi\ufb01cation and\nScene Analysis . John Wiley and Sons.\nFaruqui, M., J. Dodge, S. K. Jauhar, C. Dyer, E. Hovy, and\nN. A. Smith. 2015. Retro\ufb01tting word vectors to semantic\nlexicons. NAACL HLT .\nFellbaum, C., ed. 1998. WordNet: An Electronic Lexical\nDatabase . MIT Press.\nGale, W. A., K. W. Church, and D. Yarowsky. 1992a. Es-\ntimating upper and lower bounds on the performance of\nword-sense disambiguation programs. ACL.\nGale, W. A., K. W. Church, and D. Yarowsky. 1992b. One\nsense per discourse. HLT.\nHaber, J. and M. Poesio. 2020. Assessing polyseme sense\nsimilarity through co-predication acceptability and con-\ntextualised embedding distance. *SEM .\nHearst, M. A. 1991. Noun homograph disambiguation. Pro-\nceedings of the 7th Conference of the University of Wa-\nterloo Centre for the New OED and Text Research .\nHenrich, V ., E. Hinrichs, and T. V odolazova. 2012. We-\nbCAGe \u2013 a web-harvested corpus annotated with Ger-\nmaNet senses. EACL .Hirst, G. 1987. Semantic Interpretation and the Resolution\nof Ambiguity . Cambridge University Press.\nHirst, G. 1988. Resolving lexical ambiguity computationally\nwith spreading activation and polaroid words. In S. L.\nSmall, G. W. Cottrell, and M. K. Tanenhaus, eds, Lexical\nAmbiguity Resolution , 73\u2013108. Morgan Kaufmann.\nHirst, G. and E. Charniak. 1982. Word sense and case slot\ndisambiguation. AAAI .\nHovy, E. H., M. P. Marcus, M. Palmer, L. A. Ramshaw,\nand R. Weischedel. 2006. OntoNotes: The 90% solution.\nHLT-NAACL .\nIacobacci, I., M. T. Pilehvar, and R. Navigli. 2016. Em-\nbeddings for word sense disambiguation: An evaluation\nstudy. ACL.\nJurgens, D. and I. P. Klapaftis. 2013. SemEval-2013 task 13:\nWord sense induction for graded and non-graded senses.\n*SEM .\nKawamoto, A. H. 1988. Distributed representations of am-\nbiguous words and their resolution in connectionist net-\nworks. In S. L. Small, G. W. Cottrell, and M. Tanen-\nhaus, eds, Lexical Ambiguity Resolution , 195\u2013228. Mor-\ngan Kaufman.\nKelly, E. F. and P. J. Stone. 1975. Computer Recognition of\nEnglish Word Senses . North-Holland.",
    "metadata": {
      "source": "G",
      "chunk_id": 29,
      "token_count": 746,
      "chapter_title": ""
    }
  },
  {
    "content": "Hirst, G. and E. Charniak. 1982. Word sense and case slot\ndisambiguation. AAAI .\nHovy, E. H., M. P. Marcus, M. Palmer, L. A. Ramshaw,\nand R. Weischedel. 2006. OntoNotes: The 90% solution.\nHLT-NAACL .\nIacobacci, I., M. T. Pilehvar, and R. Navigli. 2016. Em-\nbeddings for word sense disambiguation: An evaluation\nstudy. ACL.\nJurgens, D. and I. P. Klapaftis. 2013. SemEval-2013 task 13:\nWord sense induction for graded and non-graded senses.\n*SEM .\nKawamoto, A. H. 1988. Distributed representations of am-\nbiguous words and their resolution in connectionist net-\nworks. In S. L. Small, G. W. Cottrell, and M. Tanen-\nhaus, eds, Lexical Ambiguity Resolution , 195\u2013228. Mor-\ngan Kaufman.\nKelly, E. F. and P. J. Stone. 1975. Computer Recognition of\nEnglish Word Senses . North-Holland.\nKilgarriff, A. and J. Rosenzweig. 2000. Framework and re-\nsults for English SENSEV AL. Computers and the Hu-\nmanities , 34:15\u201348.\nKumar, S., S. Jat, K. Saxena, and P. Talukdar. 2019. Zero-\nshot word sense disambiguation using sense de\ufb01nition\nembeddings. ACL.\nLandes, S., C. Leacock, and R. I. Tengi. 1998. Building se-\nmantic concordances. In C. Fellbaum, ed., WordNet: An\nElectronic Lexical Database , 199\u2013216. MIT Press.\nLauscher, A., I. Vuli \u00b4c, E. M. Ponti, A. Korhonen, and\nG. Glava \u02c7s. 2019. Informing unsupervised pretraining\nwith external linguistic knowledge. ArXiv preprint\narXiv:1909.02339.\nLengerich, B., A. Maas, and C. Potts. 2018. Retro\ufb01tting dis-\ntributional embeddings to knowledge graphs with func-\ntional relations. COLING .\nLesk, M. E. 1986. Automatic sense disambiguation using\nmachine readable dictionaries: How to tell a pine cone\nfrom an ice cream cone. Proceedings of the 5th Interna-\ntional Conference on Systems Documentation .\nLevine, Y ., B. Lenz, O. Dagan, O. Ram, D. Pad-\nnos, O. Sharir, S. Shalev-Shwartz, A. Shashua, and\nY . Shoham. 2020. SenseBERT: Driving some sense into\nBERT. ACL.\nLoureiro, D. and A. Jorge. 2019. Language modelling makes\nsense: Propagating representations through WordNet for\nfull-coverage word sense disambiguation. ACL.\nLuo, F., T. Liu, Z. He, Q. Xia, Z. Sui, and B. Chang. 2018a.\nLeveraging gloss knowledge in neural word sense disam-\nbiguation by hierarchical co-attention. EMNLP .",
    "metadata": {
      "source": "G",
      "chunk_id": 30,
      "token_count": 764,
      "chapter_title": ""
    }
  },
  {
    "content": "with external linguistic knowledge. ArXiv preprint\narXiv:1909.02339.\nLengerich, B., A. Maas, and C. Potts. 2018. Retro\ufb01tting dis-\ntributional embeddings to knowledge graphs with func-\ntional relations. COLING .\nLesk, M. E. 1986. Automatic sense disambiguation using\nmachine readable dictionaries: How to tell a pine cone\nfrom an ice cream cone. Proceedings of the 5th Interna-\ntional Conference on Systems Documentation .\nLevine, Y ., B. Lenz, O. Dagan, O. Ram, D. Pad-\nnos, O. Sharir, S. Shalev-Shwartz, A. Shashua, and\nY . Shoham. 2020. SenseBERT: Driving some sense into\nBERT. ACL.\nLoureiro, D. and A. Jorge. 2019. Language modelling makes\nsense: Propagating representations through WordNet for\nfull-coverage word sense disambiguation. ACL.\nLuo, F., T. Liu, Z. He, Q. Xia, Z. Sui, and B. Chang. 2018a.\nLeveraging gloss knowledge in neural word sense disam-\nbiguation by hierarchical co-attention. EMNLP .\nLuo, F., T. Liu, Q. Xia, B. Chang, and Z. Sui. 2018b. Incor-\nporating glosses into neural word sense disambiguation.\nACL.\nMadhu, S. and D. Lytel. 1965. A \ufb01gure of merit technique for\nthe resolution of non-grammatical ambiguity. Mechanical\nTranslation , 8(2):9\u201313.",
    "metadata": {
      "source": "G",
      "chunk_id": 31,
      "token_count": 379,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20",
    "metadata": {
      "source": "G",
      "chunk_id": 32,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "20 Appendix G \u2022 Word Senses and WordNet\nManandhar, S., I. P. Klapaftis, D. Dligach, and S. Pradhan.\n2010. SemEval-2010 task 14: Word sense induction &\ndisambiguation. SemEval .\nMartin, J. H. 1986. The acquisition of polysemy. ICML .\nMasterman, M. 1957. The thesaurus in syntax and semantics.\nMechanical Translation , 4(1):1\u20132.\nMelamud, O., J. Goldberger, and I. Dagan. 2016. con-\ntext2vec: Learning generic context embedding with bidi-\nrectional LSTM. CoNLL .\nMihalcea, R. 2007. Using Wikipedia for automatic word\nsense disambiguation. NAACL-HLT .\nMihalcea, R. and D. Moldovan. 2001. Automatic genera-\ntion of a coarse grained WordNet. NAACL Workshop on\nWordNet and Other Lexical Resources .\nMiller, G. A., C. Leacock, R. I. Tengi, and R. T. Bunker.\n1993. A semantic concordance. HLT.\nMorris, W., ed. 1985. American Heritage Dictionary , 2nd\ncollege edition edition. Houghton Mif\ufb02in.\nMrk\u02c7si\u00b4c, N., D. \u00b4O. S \u00b4eaghdha, B. Thomson, M. Ga \u02c7si\u00b4c, L. M.\nRojas-Barahona, P.-H. Su, D. Vandyke, T.-H. Wen, and\nS. Young. 2016. Counter-\ufb01tting word vectors to linguis-\ntic constraints. NAACL HLT .\nNavigli, R. 2006. Meaningful clustering of senses helps\nboost word sense disambiguation performance. COL-\nING/ACL .\nNavigli, R. 2009. Word sense disambiguation: A survey.\nACM Computing Surveys , 41(2).\nNavigli, R. 2016. Chapter 20. ontologies. In R. Mitkov, ed.,\nThe Oxford handbook of computational linguistics . Ox-\nford University Press.\nNavigli, R. and S. P. Ponzetto. 2012. BabelNet: The auto-\nmatic construction, evaluation and application of a wide-\ncoverage multilingual semantic network. Arti\ufb01cial Intel-\nligence , 193:217\u2013250.\nNavigli, R. and D. Vannella. 2013. SemEval-2013 task 11:\nWord sense induction and disambiguation within an end-\nuser application. *SEM .\nNguyen, K. A., S. Schulte im Walde, and N. T. Vu. 2016.\nIntegrating distributional lexical contrast into word em-\nbeddings for antonym-synonym distinction. ACL.\nPalmer, M., O. Babko-Malaya, and H. T. Dang. 2004. Dif-\nferent sense granularities for different applications. HLT-\nNAACL Workshop on Scalable Natural Language Under-\nstanding .\nPalmer, M., H. T. Dang, and C. Fellbaum. 2006. Making\n\ufb01ne-grained and coarse-grained sense distinctions, both\nmanually and automatically. Natural Language Engineer-\ning, 13(2):137\u2013163.",
    "metadata": {
      "source": "G",
      "chunk_id": 33,
      "token_count": 757,
      "chapter_title": ""
    }
  },
  {
    "content": "The Oxford handbook of computational linguistics . Ox-\nford University Press.\nNavigli, R. and S. P. Ponzetto. 2012. BabelNet: The auto-\nmatic construction, evaluation and application of a wide-\ncoverage multilingual semantic network. Arti\ufb01cial Intel-\nligence , 193:217\u2013250.\nNavigli, R. and D. Vannella. 2013. SemEval-2013 task 11:\nWord sense induction and disambiguation within an end-\nuser application. *SEM .\nNguyen, K. A., S. Schulte im Walde, and N. T. Vu. 2016.\nIntegrating distributional lexical contrast into word em-\nbeddings for antonym-synonym distinction. ACL.\nPalmer, M., O. Babko-Malaya, and H. T. Dang. 2004. Dif-\nferent sense granularities for different applications. HLT-\nNAACL Workshop on Scalable Natural Language Under-\nstanding .\nPalmer, M., H. T. Dang, and C. Fellbaum. 2006. Making\n\ufb01ne-grained and coarse-grained sense distinctions, both\nmanually and automatically. Natural Language Engineer-\ning, 13(2):137\u2013163.\nPedersen, T. and R. Bruce. 1997. Distinguishing word senses\nin untagged text. EMNLP .\nPeters, M., M. Neumann, M. Iyyer, M. Gardner, C. Clark,\nK. Lee, and L. Zettlemoyer. 2018. Deep contextualized\nword representations. NAACL HLT .\nPilehvar, M. T. and J. Camacho-Collados. 2019. WiC: the\nword-in-context dataset for evaluating context-sensitive\nmeaning representations. NAACL HLT .\nPilehvar, M. T., D. Jurgens, and R. Navigli. 2013. Align,\ndisambiguate and walk: A uni\ufb01ed approach for measur-\ning semantic similarity. ACL.Ponzetto, S. P. and R. Navigli. 2010. Knowledge-rich word\nsense disambiguation rivaling supervised systems. ACL.\nPu, X., N. Pappas, J. Henderson, and A. Popescu-Belis.\n2018. Integrating weakly supervised word sense disam-\nbiguation into neural machine translation. TACL , 6:635\u2013\n649.\nPustejovsky, J. 1995. The Generative Lexicon . MIT Press.\nPustejovsky, J. and B. K. Boguraev, eds. 1996. Lexical Se-\nmantics: The Problem of Polysemy . Oxford University\nPress.\nQuillian, M. R. 1968. Semantic memory. In M. Minsky, ed.,\nSemantic Information Processing , 227\u2013270. MIT Press.\nQuillian, M. R. 1969. The teachable language compre-\nhender: A simulation program and theory of language.\nCACM , 12(8):459\u2013476.\nRaganato, A., C. D. Bovi, and R. Navigli. 2017a. Neural se-\nquence learning models for word sense disambiguation.\nEMNLP .\nRaganato, A., J. Camacho-Collados, and R. Navigli. 2017b.",
    "metadata": {
      "source": "G",
      "chunk_id": 34,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "sense disambiguation rivaling supervised systems. ACL.\nPu, X., N. Pappas, J. Henderson, and A. Popescu-Belis.\n2018. Integrating weakly supervised word sense disam-\nbiguation into neural machine translation. TACL , 6:635\u2013\n649.\nPustejovsky, J. 1995. The Generative Lexicon . MIT Press.\nPustejovsky, J. and B. K. Boguraev, eds. 1996. Lexical Se-\nmantics: The Problem of Polysemy . Oxford University\nPress.\nQuillian, M. R. 1968. Semantic memory. In M. Minsky, ed.,\nSemantic Information Processing , 227\u2013270. MIT Press.\nQuillian, M. R. 1969. The teachable language compre-\nhender: A simulation program and theory of language.\nCACM , 12(8):459\u2013476.\nRaganato, A., C. D. Bovi, and R. Navigli. 2017a. Neural se-\nquence learning models for word sense disambiguation.\nEMNLP .\nRaganato, A., J. Camacho-Collados, and R. Navigli. 2017b.\nWord sense disambiguation: A uni\ufb01ed evaluation frame-\nwork and empirical comparison. EACL .\nRiesbeck, C. K. 1975. Conceptual analysis. In R. C. Schank,\ned.,Conceptual Information Processing , 83\u2013156. Ameri-\ncan Elsevier, New York.\nSchneider, N., J. D. Hwang, V . Srikumar, J. Prange, A. Blod-\ngett, S. R. Moeller, A. Stern, A. Bitan, and O. Abend.\n2018. Comprehensive supersense disambiguation of En-\nglish prepositions and possessives. ACL.\nSch\u00a8utze, H. 1992. Dimensions of meaning. Proceedings of\nSupercomputing \u201992 . IEEE Press.\nSch\u00a8utze, H. 1997. Ambiguity Resolution in Language Learn-\ning: Computational and Cognitive Models . CSLI Publi-\ncations, Stanford, CA.\nSch\u00a8utze, H. 1998. Automatic word sense discrimination.\nComputational Linguistics , 24(1):97\u2013124.\nSimmons, R. F. 1973. Semantic networks: Their compu-\ntation and use for understanding English sentences. In\nR. C. Schank and K. M. Colby, eds, Computer Models of\nThought and Language , 61\u2013113. W.H. Freeman & Co.\nSmall, S. L. and C. Rieger. 1982. Parsing and comprehend-\ning with Word Experts. In W. G. Lehnert and M. H.\nRingle, eds, Strategies for Natural Language Processing ,\n89\u2013147. Lawrence Erlbaum.\nSnow, R., S. Prakash, D. Jurafsky, and A. Y . Ng. 2007.\nLearning to merge word senses. EMNLP/CoNLL .\nSnyder, B. and M. Palmer. 2004. The English all-words task.\nSENSEVAL-3 .\nSparck Jones, K. 1986. Synonymy and Semantic Classi\ufb01ca-\ntion. Edinburgh University Press, Edinburgh. Republica-\ntion of 1964 PhD Thesis.",
    "metadata": {
      "source": "G",
      "chunk_id": 35,
      "token_count": 742,
      "chapter_title": ""
    }
  },
  {
    "content": "ing: Computational and Cognitive Models . CSLI Publi-\ncations, Stanford, CA.\nSch\u00a8utze, H. 1998. Automatic word sense discrimination.\nComputational Linguistics , 24(1):97\u2013124.\nSimmons, R. F. 1973. Semantic networks: Their compu-\ntation and use for understanding English sentences. In\nR. C. Schank and K. M. Colby, eds, Computer Models of\nThought and Language , 61\u2013113. W.H. Freeman & Co.\nSmall, S. L. and C. Rieger. 1982. Parsing and comprehend-\ning with Word Experts. In W. G. Lehnert and M. H.\nRingle, eds, Strategies for Natural Language Processing ,\n89\u2013147. Lawrence Erlbaum.\nSnow, R., S. Prakash, D. Jurafsky, and A. Y . Ng. 2007.\nLearning to merge word senses. EMNLP/CoNLL .\nSnyder, B. and M. Palmer. 2004. The English all-words task.\nSENSEVAL-3 .\nSparck Jones, K. 1986. Synonymy and Semantic Classi\ufb01ca-\ntion. Edinburgh University Press, Edinburgh. Republica-\ntion of 1964 PhD Thesis.\nTsvetkov, Y ., N. Schneider, D. Hovy, A. Bhatia, M. Faruqui,\nand C. Dyer. 2014. Augmenting English adjective senses\nwith supersenses. LREC .\nV ossen, P., A. G \u00a8or\u00a8og, F. Laan, M. Van Gompel, R. Izquierdo,\nand A. Van Den Bosch. 2011. Dutch-semcor: building a\nsemantically annotated corpus for dutch. Proceedings of\neLex.",
    "metadata": {
      "source": "G",
      "chunk_id": 36,
      "token_count": 398,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21\n\nExercises 21\nWeaver, W. 1949/1955. Translation. In W. N. Locke and\nA. D. Boothe, eds, Machine Translation of Languages ,\n15\u201323. MIT Press. Reprinted from a memorandum writ-\nten by Weaver in 1949.\nWilks, Y . 1975a. An intelligent analyzer and understander of\nEnglish. CACM , 18(5):264\u2013274.\nWilks, Y . 1975b. Preference semantics. In E. L. Keenan,\ned.,The Formal Semantics of Natural Language , 329\u2013\n350. Cambridge Univ. Press.\nWilks, Y . 1975c. A preferential, pattern-seeking, seman-\ntics for natural language inference. Arti\ufb01cial Intelligence ,\n6(1):53\u201374.\nYarowsky, D. 1995. Unsupervised word sense disambigua-\ntion rivaling supervised methods. ACL.\nYu, M. and M. Dredze. 2014. Improving lexical embeddings\nwith semantic knowledge. ACL.\nZhong, Z. and H. T. Ng. 2010. It makes sense: A wide-\ncoverage word sense disambiguation system for free text.\nACL.",
    "metadata": {
      "source": "G",
      "chunk_id": 37,
      "token_count": 275,
      "chapter_title": ""
    }
  }
]