[
  {
    "content": "# 5\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n5Logistic Regression\n\u201cAnd how do you know that these \ufb01ne begonias are not of equal importance?\u201d\nHercule Poirot, in Agatha Christie\u2019s The Mysterious Affair at Styles\nDetective stories are as littered with clues as texts are with words. Yet for the\npoor reader it can be challenging to know how to weigh the author\u2019s clues in order\nto make the crucial classi\ufb01cation task: deciding whodunnit.\nIn this chapter we introduce an algorithm that is admirably suited for discovering\nthe link between features or clues and some particular outcome: logistic regression .logistic\nregression\nIndeed, logistic regression is one of the most important analytic tools in the social\nand natural sciences. In natural language processing, logistic regression is the base-\nline supervised machine learning algorithm for classi\ufb01cation, and also has a very\nclose relationship with neural networks. As we will see in Chapter 7, a neural net-\nwork can be viewed as a series of logistic regression classi\ufb01ers stacked on top of\neach other. Thus the classi\ufb01cation and machine learning techniques introduced here\nwill play an important role throughout the book.\nLogistic regression can be used to classify an observation into one of two classes\n(like \u2018positive sentiment\u2019 and \u2018negative sentiment\u2019), or into one of many classes.\nBecause the mathematics for the two-class case is simpler, we\u2019ll describe this special\ncase of logistic regression \ufb01rst in the next few sections, and then brie\ufb02y summarize\nthe use of multinomial logistic regression for more than two classes in Section 5.3.\nWe\u2019ll introduce the mathematics of logistic regression in the next few sections.\nBut let\u2019s begin with some high-level issues.\nGenerative and Discriminative Classi\ufb01ers: The most important difference be-\ntween naive Bayes and logistic regression is that logistic regression is a discrimina-\ntiveclassi\ufb01er while naive Bayes is a generative classi\ufb01er.\nThese are two very different frameworks for how\nto build a machine learning model. Consider a visual\nmetaphor: imagine we\u2019re trying to distinguish dog\nimages from cat images. A generative model would\nhave the goal of understanding what dogs look like\nand what cats look like. You might literally ask such\na model to \u2018generate\u2019, i.e., draw, a dog. Given a test\nimage, the system then asks whether it\u2019s the cat model or the dog model that better\n\ufb01ts (is less surprised by) the image, and chooses that as its label.\nA discriminative model, by contrast, is only try-\ning to learn to distinguish the classes (perhaps with-\nout learning much about them). So maybe all the\ndogs in the training data are wearing collars and the\ncats aren\u2019t. If that one feature neatly separates the\nclasses, the model is satis\ufb01ed. If you ask such a\nmodel what it knows about cats all it can say is that\nthey don\u2019t wear collars.",
    "metadata": {
      "source": "5",
      "chunk_id": 0,
      "token_count": 685,
      "chapter_title": "5"
    }
  },
  {
    "content": "## Page 2\n\n2CHAPTER 5 \u2022 L OGISTIC REGRESSION\nMore formally, recall that the naive Bayes assigns a class cto a document dnot\nby directly computing P(cjd)but by computing a likelihood and a prior\n\u02c6c=argmax\nc2Clikelihoodz}|{\nP(djc)priorz}|{\nP(c) (5.1)\nAgenerative model like naive Bayes makes use of this likelihood term, whichgenerative\nmodel\nexpresses how to generate the features of a document if we knew it was of class c .\nBy contrast a discriminative model in this text categorization scenario attemptsdiscriminative\nmodel\ntodirectly compute P(cjd). Perhaps it will learn to assign a high weight to document\nfeatures that directly improve its ability to discriminate between possible classes,\neven if it couldn\u2019t generate an example of one of the classes.\nComponents of a probabilistic machine learning classi\ufb01er: Like naive Bayes,\nlogistic regression is a probabilistic classi\ufb01er that makes use of supervised machine\nlearning. Machine learning classi\ufb01ers require a training corpus of minput/output\npairs(x(i);y(i)). (We\u2019ll use superscripts in parentheses to refer to individual instances\nin the training set\u2014for sentiment classi\ufb01cation each instance might be an individual\ndocument to be classi\ufb01ed.) A machine learning system for classi\ufb01cation then has\nfour components:\n1. A feature representation of the input. For each input observation x(i), this\nwill be a vector of features [x1;x2;:::;xn]. We will generally refer to feature\nifor input x(j)asx(j)\ni, sometimes simpli\ufb01ed as xi, but we will also see the\nnotation fi,fi(x), or, for multiclass classi\ufb01cation, fi(c;x).\n2. A classi\ufb01cation function that computes \u02c6 y, the estimated class, via p(yjx). In\nthe next section we will introduce the sigmoid andsoftmax tools for classi\ufb01-\ncation.\n3. An objective function that we want to optimize for learning, usually involving\nminimizing a loss function corresponding to error on training examples. We\nwill introduce the cross-entropy loss function .\n4. An algorithm for optimizing the objective function. We introduce the stochas-\ntic gradient descent algorithm.\nLogistic regression has two phases:\ntraining: We train the system (speci\ufb01cally the weights wandb, introduced be-\nlow) using stochastic gradient descent and the cross-entropy loss.\ntest: Given a test example xwe compute p(yjx)and return the higher probability\nlabel y=1 ory=0.\n5.1 The sigmoid function\nThe goal of binary logistic regression is to train a classi\ufb01er that can make a binary\ndecision about the class of a new input observation. Here we introduce the sigmoid\nclassi\ufb01er that will help us make this decision.\nConsider a single input observation x, which we will represent by a vector of\nfeatures [x1;x2;:::;xn]. (We\u2019ll show sample features in the next subsection.) The\nclassi\ufb01er output ycan be 1 (meaning the observation is a member of the class) or\n0 (the observation is not a member of the class). We want to know the probability",
    "metadata": {
      "source": "5",
      "chunk_id": 1,
      "token_count": 716,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\n5.1 \u2022 T HE SIGMOID FUNCTION 3\nP(y=1jx)that this observation is a member of the class. So perhaps the decision\nis \u201cpositive sentiment\u201d versus \u201cnegative sentiment\u201d, the features represent counts of\nwords in a document, P(y=1jx)is the probability that the document has positive\nsentiment, and P(y=0jx)is the probability that the document has negative senti-\nment.\nLogistic regression solves this task by learning, from a training set, a vector of\nweights and a bias term . Each weight wiis a real number, and is associated with one\nof the input features xi. The weight wirepresents how important that input feature\nis to the classi\ufb01cation decision, and can be positive (providing evidence that the in-\nstance being classi\ufb01ed belongs in the positive class) or negative (providing evidence\nthat the instance being classi\ufb01ed belongs in the negative class). Thus we might\nexpect in a sentiment task the word awesome to have a high positive weight, and\nabysmal to have a very negative weight. The bias term , also called the intercept , is bias term\nintercept another real number that\u2019s added to the weighted inputs.\nTo make a decision on a test instance\u2014after we\u2019ve learned the weights in training\u2014\nthe classi\ufb01er \ufb01rst multiplies each xiby its weight wi, sums up the weighted features,\nand adds the bias term b. The resulting single number zexpresses the weighted sum\nof the evidence for the class.\nz= nX\ni=1wixi!\n+b (5.2)\nIn the rest of the book we\u2019ll represent such sums using the dot product notation dot product\nfrom linear algebra. The dot product of two vectors aandb, written as a\u0001b, is the\nsum of the products of the corresponding elements of each vector. (Notice that we\nrepresent vectors using the boldface notation b). Thus the following is an equivalent\nformation to Eq. 5.2:\nz=w\u0001x+b (5.3)\nBut note that nothing in Eq. 5.3 forces zto be a legal probability, that is, to lie\nbetween 0 and 1. In fact, since weights are real-valued, the output might even be\nnegative; zranges from\u0000\u00a5to\u00a5.\nFigure 5.1 The sigmoid function s(z) =1\n1+e\u0000ztakes a real value and maps it to the range\n(0;1). It is nearly linear around 0 but outlier values get squashed toward 0 or 1.\nTo create a probability, we\u2019ll pass zthrough the sigmoid function, s(z). The sigmoid\nsigmoid function (named because it looks like an s) is also called the logistic func-\ntion, and gives logistic regression its name. The sigmoid has the following equation,logistic\nfunction\nshown graphically in Fig. 5.1:\ns(z) =1\n1+e\u0000z=1\n1+exp(\u0000z)(5.4)",
    "metadata": {
      "source": "5",
      "chunk_id": 2,
      "token_count": 661,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4CHAPTER 5 \u2022 L OGISTIC REGRESSION\n(For the rest of the book, we\u2019ll use the notation exp (x)to mean ex.) The sigmoid\nhas a number of advantages; it takes a real-valued number and maps it into the range\n(0;1), which is just what we want for a probability. Because it is nearly linear around\n0 but \ufb02attens toward the ends, it tends to squash outlier values toward 0 or 1. And\nit\u2019s differentiable, which as we\u2019ll see in Section 5.10 will be handy for learning.\nWe\u2019re almost there. If we apply the sigmoid to the sum of the weighted features,\nwe get a number between 0 and 1. To make it a probability, we just need to make\nsure that the two cases, p(y=1)andp(y=0), sum to 1. We can do this as follows:\nP(y=1) = s(w\u0001x+b)\n=1\n1+exp(\u0000(w\u0001x+b))\nP(y=0) = 1\u0000s(w\u0001x+b)\n=1\u00001\n1+exp(\u0000(w\u0001x+b))\n=exp(\u0000(w\u0001x+b))\n1+exp(\u0000(w\u0001x+b))(5.5)\nThe sigmoid function has the property\n1\u0000s(x) =s(\u0000x) (5.6)\nso we could also have expressed P(y=0)ass(\u0000(w\u0001x+b)).\nFinally, one terminological point. The input to the sigmoid function, the score\nz=w\u0001x+bfrom Eq. 5.3, is often called the logit . This is because the logit function logit\nis the inverse of the sigmoid. The logit function is the log of the odds ratiop\n1\u0000p:\nlogit(p) =s\u00001(p) =lnp\n1\u0000p(5.7)\nUsing the term logit forzis a way of reminding us that by using the sigmoid to turn\nz(which ranges from \u0000\u00a5to\u00a5) into a probability, we are implicitly interpreting zas\nnot just any real-valued number, but as speci\ufb01cally a log odds.\n5.2 Classi\ufb01cation with Logistic Regression\nThe sigmoid function from the prior section thus gives us a way to take an instance\nxand compute the probability P(y=1jx).\nHow do we make a decision about which class to apply to a test instance x? For\na given x, we say yes if the probability P(y=1jx)is more than .5, and no otherwise.\nWe call .5 the decision boundary :decision\nboundary\ndecision (x) =\u001a1 if P(y=1jx)>0:5\n0 otherwise\nLet\u2019s have some examples of applying logistic regression as a classi\ufb01er for language\ntasks.",
    "metadata": {
      "source": "5",
      "chunk_id": 3,
      "token_count": 623,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\n5.2 \u2022 C LASSIFICATION WITH LOGISTIC REGRESSION 5\n5.2.1 Sentiment Classi\ufb01cation\nSuppose we are doing binary sentiment classi\ufb01cation on movie review text, and\nwe would like to know whether to assign the sentiment class +or\u0000to a review\ndocument doc. We\u2019ll represent each input observation by the 6 features x1:::x6of\nthe input shown in the following table; Fig. 5.2 shows the features in a sample mini\ntest document.\nVar De\ufb01nition Value in Fig. 5.2\nx1 count(positive lexicon words 2doc) 3\nx2 count(negative lexicon words 2doc) 2\nx3\u001a1 if \u201cno\u201d2doc\n0 otherwise1\nx4 count (1st and 2nd pronouns 2doc) 3\nx5\u001a1 if \u201c!\u201d2doc\n0 otherwise0\nx6 ln(word count of doc ) ln(66) =4:19\n It's hokey . There are virtually no surprises , and the writing is second-rate . So why was it so enjoyable  ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing .  It sucked me in , and it'll do the same to you  .x1=3x6=4.19x3=1x4=3x5=0x2=2\nFigure 5.2 A sample mini test document showing the extracted features in the vector x.\nLet\u2019s assume for the moment that we\u2019ve already learned a real-valued weight\nfor each of these features, and that the 6 weights corresponding to the 6 features\nare[2:5;\u00005:0;\u00001:2;0:5;2:0;0:7], while b= 0.1. (We\u2019ll discuss in the next section\nhow the weights are learned.) The weight w1, for example indicates how important\na feature the number of positive lexicon words ( great ,nice,enjoyable , etc.) is to\na positive sentiment decision, while w2tells us the importance of negative lexicon\nwords. Note that w1=2:5 is positive, while w2=\u00005:0, meaning that negative words\nare negatively associated with a positive sentiment decision, and are about twice as\nimportant as positive words.\nGiven these 6 features and the input review x,P(+jx)andP(\u0000jx)can be com-\nputed using Eq. 5.5:\np(+jx) =P(y=1jx) = s(w\u0001x+b)\n=s([2:5;\u00005:0;\u00001:2;0:5;2:0;0:7]\u0001[3;2;1;3;0;4:19]+0:1)\n=s(:833)\n=0:70 (5.8)\np(\u0000jx) =P(y=0jx) = 1\u0000s(w\u0001x+b)\n=0:30",
    "metadata": {
      "source": "5",
      "chunk_id": 4,
      "token_count": 676,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6CHAPTER 5 \u2022 L OGISTIC REGRESSION\n5.2.2 Other classi\ufb01cation tasks and features\nLogistic regression is applied to all sorts of NLP tasks, and any property of the input\ncan be a feature. Consider the task of period disambiguation : deciding if a periodperiod\ndisambiguation\nis the end of a sentence or part of a word, by classifying each period into one of two\nclasses, EOS (end-of-sentence) and not-EOS. We might use features like x1below\nexpressing that the current word is lower case, perhaps with a positive weight. Or a\nfeature expressing that the current word is in our abbreviations dictionary (\u201cProf.\u201d),\nperhaps with a negative weight. A feature can also express a combination of proper-\nties. For example a period following an upper case word is likely to be an EOS, but\nif the word itself is St.and the previous word is capitalized then the period is likely\npart of a shortening of the word street following a street name.\nx1=\u001a\n1 if \u201c Case(wi) =Lower\u201d\n0 otherwise\nx2=\u001a1 if \u201c wi2AcronymDict\u201d\n0 otherwise\nx3=\u001a1 if \u201c wi=St. & Case(wi\u00001) =Upper\u201d\n0 otherwise\nDesigning versus learning features: In classic models, features are designed by\nhand by examining the training set with an eye to linguistic intuitions and literature,\nsupplemented by insights from error analysis on the training set of an early version\nof a system. We can also consider ( feature interactions ), complex features that arefeature\ninteractions\ncombinations of more primitive features. We saw such a feature for period disam-\nbiguation above, where a period on the word St.was less likely to be the end of the\nsentence if the previous word was capitalized. Features can be created automatically\nviafeature templates , abstract speci\ufb01cations of features. For example a bigramfeature\ntemplates\ntemplate for period disambiguation might create a feature for every pair of words\nthat occurs before a period in the training set. Thus the feature space is sparse, since\nwe only have to create a feature if that n-gram exists in that position in the training\nset. The feature is generally created as a hash from the string descriptions. A user\ndescription of a feature as, \u201cbigram(American breakfast)\u201d is hashed into a unique\ninteger ithat becomes the feature number fi.\nIt should be clear from the prior paragraph that designing features by hand re-\nquires extensive human effort. For this reason, recent NLP systems avoid hand-\ndesigned features and instead focus on representation learning : ways to learn fea-\ntures automatically in an unsupervised way from the input. We\u2019ll introduce methods\nfor representation learning in Chapter 6 and Chapter 7.\nScaling input features: When different input features have extremely different\nranges of values, it\u2019s common to rescale them so they have comparable ranges. We\nstandardize input values by centering them to result in a zero mean and a standard standardize\ndeviation of one (this transformation is sometimes called the z-score ). That is, if miz-score\nis the mean of the values of feature xiacross the mobservations in the input dataset,\nandsiis the standard deviation of the values of features xiacross the input dataset,\nwe can replace each feature xiby a new feature x0\nicomputed as follows:\nmi=1\nmmX\nj=1x(j)\ni si=vuut1\nmmX\nj=1\u0010\nx(j)\ni\u0000mi\u00112\nx0\ni=xi\u0000mi\nsi(5.9)",
    "metadata": {
      "source": "5",
      "chunk_id": 5,
      "token_count": 787,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\n5.2 \u2022 C LASSIFICATION WITH LOGISTIC REGRESSION 7\nAlternatively, we can normalize the input features values to lie between 0 and 1: normalize\nx0\ni=xi\u0000min(xi)\nmax(xi)\u0000min(xi)(5.10)\nHaving input data with comparable range is useful when comparing values across\nfeatures. Data scaling is especially important in large neural networks, since it helps\nspeed up gradient descent.\n5.2.3 Processing many examples at once\nWe\u2019ve shown the equations for logistic regression for a single example. But in prac-\ntice we\u2019ll of course want to process an entire test set with many examples. Let\u2019s\nsuppose we have a test set consisting of mtest examples each of which we\u2019d like to\nclassify. We\u2019ll continue to use the notation from page 2, in which a superscript value\nin parentheses refers to the example index in some set of data (either for training or\nfor test). So in this case each test example x(i)has a feature vector x(i), 1\u0014i\u0014m.\n(As usual, we\u2019ll represent vectors and matrices in bold.)\nOne way to compute each output value \u02c6 y(i)is just to have a for-loop, and compute\neach test example one at a time:\nforeach x(i)in input [x(1);x(2);:::;x(m)]\ny(i)=s(w\u0001x(i)+b) (5.11)\nFor the \ufb01rst 3 test examples, then, we would be separately computing the pre-\ndicted \u02c6 y(i)as follows:\nP(y(1)=1jx(1)) = s(w\u0001x(1)+b)\nP(y(2)=1jx(2)) = s(w\u0001x(2)+b)\nP(y(3)=1jx(3)) = s(w\u0001x(3)+b)\nBut it turns out that we can slightly modify our original equation Eq. 5.5 to do\nthis much more ef\ufb01ciently. We\u2019ll use matrix arithmetic to assign a class to all the\nexamples with one matrix operation!\nFirst, we\u2019ll pack all the input feature vectors for each input xinto a single input\nmatrix X, where each row iis a row vector consisting of the feature vector for in-\nput example x(i)(i.e., the vector x(i)). Assuming each example has ffeatures and\nweights, Xwill therefore be a matrix of shape [m\u0002f], as follows:\nX=2\n66664x(1)\n1x(1)\n2:::x(1)\nf\nx(2)\n1x(2)\n2:::x(2)\nf\nx(3)\n1x(3)\n2:::x(3)\nf\n:::3\n77775(5.12)\nNow if we introduce bas a vector of length mwhich consists of the scalar bias\nterm brepeated mtimes, b= [b;b;:::;b], and ^ y= [\u02c6y(1);\u02c6y(2):::;\u02c6y(m)]as the vector of\noutputs (one scalar \u02c6 y(i)for each input x(i)and its feature vector x(i)), and represent\nthe weight vector was a column vector, we can compute all the outputs with a single\nmatrix multiplication and one addition:\ny=Xw+b (5.13)",
    "metadata": {
      "source": "5",
      "chunk_id": 6,
      "token_count": 725,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8CHAPTER 5 \u2022 L OGISTIC REGRESSION\nYou should convince yourself that Eq. 5.13 computes the same thing as our for-loop\nin Eq. 5.11. For example \u02c6 y(1), the \ufb01rst entry of the output vector y, will correctly be:\n\u02c6y(1)= [x(1)\n1;x(1)\n2;:::;x(1)\nf]\u0001[w1;w2;:::;wf]+b (5.14)\nNote that we had to reorder Xandwfrom the order they appeared in in Eq. 5.5 to\nmake the multiplications come out properly. Here is Eq. 5.13 again with the shapes\nshown:\ny=X w +b\n(m\u00021) ( m\u0002f)(f\u00021) (m\u00021) (5.15)\nModern compilers and compute hardware can compute this matrix operation very\nef\ufb01ciently, making the computation much faster, which becomes important when\ntraining or testing on very large datasets.\nNote by the way that we could have kept Xandwin the original order ( y=\nXw+b) if we had chosen to de\ufb01ne Xdifferently as a matrix of column vectors, one\nvector for each input example, instead of row vectors, and then it would have shape\n[f\u0002m]. But we conventionally represent inputs as rows.\n5.2.4 Choosing a classi\ufb01er\nLogistic regression has a number of advantages over naive Bayes. Naive Bayes has\noverly strong conditional independence assumptions. Consider two features which\nare strongly correlated; in fact, imagine that we just add the same feature f1twice.\nNaive Bayes will treat both copies of f1as if they were separate, multiplying them\nboth in, overestimating the evidence. By contrast, logistic regression is much more\nrobust to correlated features; if two features f1and f2are perfectly correlated, re-\ngression will simply assign part of the weight to w1and part to w2. Thus when\nthere are many correlated features, logistic regression will assign a more accurate\nprobability than naive Bayes. So logistic regression generally works better on larger\ndocuments or datasets and is a common default.\nDespite the less accurate probabilities, naive Bayes still often makes the correct\nclassi\ufb01cation decision. Furthermore, naive Bayes can work extremely well (some-\ntimes even better than logistic regression) on very small datasets (Ng and Jordan,\n2002) or short documents (Wang and Manning, 2012). Furthermore, naive Bayes is\neasy to implement and very fast to train (there\u2019s no optimization step). So it\u2019s still a\nreasonable approach to use in some situations.\n5.3 Multinomial logistic regression\nSometimes we need more than two classes. Perhaps we might want to do 3-way\nsentiment classi\ufb01cation (positive, negative, or neutral). Or we could be assigning\nsome of the labels we will introduce in Chapter 17, like the part of speech of a word\n(choosing from 10, 30, or even 50 different parts of speech), or the named entity\ntype of a phrase (choosing from tags like person, location, organization).\nIn such cases we use multinomial logistic regression , also called softmax re-multinomial\nlogistic\nregressiongression (in older NLP literature you will sometimes see the name maxent classi-\n\ufb01er). In multinomial logistic regression we want to label each observation with a\nclass kfrom a set of Kclasses, under the stipulation that only one of these classes is",
    "metadata": {
      "source": "5",
      "chunk_id": 7,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\n5.3 \u2022 M ULTINOMIAL LOGISTIC REGRESSION 9\nthe correct one (sometimes called hard classi\ufb01cation ; an observation can not be in\nmultiple classes). Let\u2019s use the following representation: the output yfor each input\nxwill be a vector of length K. If class cis the correct class, we\u2019ll set yc=1, and\nset all the other elements of yto be 0, i.e., yc=1 and yj=08j6=c. A vector like\nthisy, with one value=1 and the rest 0, is called a one-hot vector . The job of the\nclassi\ufb01er is to produce an estimate vector ^ y. For each class k, the value \u02c6 ykwill be\nthe classi\ufb01er\u2019s estimate of the probability p(yk=1jx).\n5.3.1 Softmax\nThe multinomial logistic classi\ufb01er uses a generalization of the sigmoid, called the\nsoftmax function, to compute p(yk=1jx). The softmax function takes a vector softmax\nz= [z1;z2;:::;zK]ofKarbitrary values and maps them to a probability distribution,\nwith each value in the range [0,1], and all the values summing to 1. Like the sigmoid,\nit is an exponential function.\nFor a vector zof dimensionality K, the softmax is de\ufb01ned as:\nsoftmax (zi) =exp(zi)PK\nj=1exp(zj)1\u0014i\u0014K (5.16)\nThe softmax of an input vector z= [z1;z2;:::;zK]is thus a vector itself:\nsoftmax (z) =\"\nexp(z1)PK\ni=1exp(zi);exp(z2)PK\ni=1exp(zi);:::;exp(zK)PK\ni=1exp(zi)#\n(5.17)\nThe denominatorPK\ni=1exp(zi)is used to normalize all the values into probabilities.\nThus for example given a vector:\nz= [0:6;1:1;\u00001:5;1:2;3:2;\u00001:1]\nthe resulting (rounded) softmax( z) is\n[0:05;0:09;0:01;0:1;0:74;0:01]\nLike the sigmoid, the softmax has the property of squashing values toward 0 or 1.\nThus if one of the inputs is larger than the others, it will tend to push its probability\ntoward 1, and suppress the probabilities of the smaller inputs.\nFinally, note that, just as for the sigmoid, we refer to z, the vector of scores that\nis the input to the softmax, as logits (see Eq. 5.7).\n5.3.2 Applying softmax in logistic regression\nWhen we apply softmax for logistic regression, the input will (just as for the sig-\nmoid) be the dot product between a weight vector wand an input vector x(plus a\nbias). But now we\u2019ll need separate weight vectors wkand bias bkfor each of the K\nclasses. The probability of each of our output classes \u02c6 ykcan thus be computed as:\np(yk=1jx) =exp(wk\u0001x+bk)\nKX\nj=1exp(wj\u0001x+bj)(5.18)",
    "metadata": {
      "source": "5",
      "chunk_id": 8,
      "token_count": 732,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 CHAPTER 5 \u2022 L OGISTIC REGRESSION\nThe form of Eq. 5.18 makes it seem that we would compute each output sep-\narately. Instead, it\u2019s more common to set up the equation for more ef\ufb01cient com-\nputation by modern vector processing hardware. We\u2019ll do this by representing the\nset of Kweight vectors as a weight matrix Wand a bias vector b. Each row kof\nWcorresponds to the vector of weights wk.Wthus has shape [K\u0002f], for Kthe\nnumber of output classes and fthe number of input features. The bias vector bhas\none value for each of the Koutput classes. If we represent the weights in this way,\nwe can compute \u02c6y, the vector of output probabilities for each of the Kclasses, by a\nsingle elegant equation:\n\u02c6y=softmax (Wx+b) (5.19)\nIf you work out the matrix arithmetic, you can see that the estimated score of\nthe \ufb01rst output class \u02c6 y1(before we take the softmax) will correctly turn out to be\nw1\u0001x+b1.\nOne helpful interpretation of the weight matrix Wis to see each row wkas a\nprototype of class k. The weight vector wkthat is learned represents the class as prototype\na kind of template. Since two vectors that are more similar to each other have a\nhigher dot product with each other, the dot product acts as a similarity function.\nLogistic regression is thus learning an exemplar representation for each class, such\nthat incoming vectors are assigned the class kthey are most similar to from the K\nclasses.\nFig. 5.3 shows the difference between binary and multinomial logistic regression\nby illustrating the weight vector versus weight matrix in the computation of the\noutput class probabilities.\n5.3.3 Features in Multinomial Logistic Regression\nFeatures in multinomial logistic regression act like features in binary logistic regres-\nsion, with the difference mentioned above that we\u2019ll need separate weight vectors\nand biases for each of the Kclasses. Recall our binary exclamation point feature x5\nfrom page 5:\nx5=\u001a\n1 if \u201c!\u201d2doc\n0 otherwise\nIn binary classi\ufb01cation a positive weight w5on a feature in\ufb02uences the classi\ufb01er\ntoward y=1 (positive sentiment) and a negative weight in\ufb02uences it toward y=0\n(negative sentiment) with the absolute value indicating how important the feature\nis. For multinomial logistic regression, by contrast, with separate weights for each\nclass, a feature can be evidence for or against each individual class.\nIn 3-way multiclass sentiment classi\ufb01cation, for example, we must assign each\ndocument one of the 3 classes +,\u0000, or 0 (neutral). Now a feature related to excla-\nmation marks might have a negative weight for 0 documents, and a positive weight\nfor+or\u0000documents:\nFeature De\ufb01nition w5;+w5;\u0000w5;0\nf5(x)\u001a1 if \u201c!\u201d2doc\n0 otherwise3:5 3 :1\u00005:3\nBecause these feature weights are dependent both on the input text and the output\nclass, we sometimes make this dependence explicit and represent the features them-\nselves as f(x;y): a function of both the input and the class. Using such a notation",
    "metadata": {
      "source": "5",
      "chunk_id": 9,
      "token_count": 727,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\n5.4 \u2022 L EARNING IN LOGISTIC REGRESSION 11\nBinary Logistic Regression\nw[f \u2a091]Outputsigmoid[1\u2a09f]Input wordsp(+) = 1- p(-)\u2026y^xyInput featurevector [scalar]positive lexiconwords = 1count of \u201cno\u201d = 0wordcount=3x1x2x3xfdessert   was    greatWeight vector\nMultinomial Logistic Regression\nW[f\u2a091]Outputsoftmax[K\u2a09f]Input wordsp(+)\u2026y1^y2^y3^xyInput featurevector [K\u2a091]positive lexiconwords = 1count of \u201cno\u201d = 0wordcount=3x1x2x3xfdessert   was    greatp(-)p(neut)Weight matrixThese f red weightsare a row of W correspondingto weight vector w3,(= weights for class 3,= a prototype of class 3)\nFigure 5.3 Binary versus multinomial logistic regression. Binary logistic regression uses a\nsingle weight vector w, and has a scalar output \u02c6 y. In multinomial logistic regression we have\nKseparate weight vectors corresponding to the Kclasses, all packed into a single weight\nmatrix W, and a vector output \u02c6y. We omit the biases from both \ufb01gures for clarity.\nf5(x)above could be represented as three features f5(x;+),f5(x;\u0000), and f5(x;0),\neach of which has a single weight. We\u2019ll use this kind of notation in our description\nof the CRF in Chapter 17.\n5.4 Learning in Logistic Regression\nHow are the parameters of the model, the weights wand bias b, learned? Logistic\nregression is an instance of supervised classi\ufb01cation in which we know the correct\nlabel y(either 0 or 1) for each observation x. What the system produces via Eq. 5.5\nis \u02c6y, the system\u2019s estimate of the true y. We want to learn parameters (meaning w\nandb) that make \u02c6 yfor each training observation as close as possible to the true y.\nThis requires two components that we foreshadowed in the introduction to the\nchapter. The \ufb01rst is a metric for how close the current label ( \u02c6 y) is to the true gold",
    "metadata": {
      "source": "5",
      "chunk_id": 10,
      "token_count": 518,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 CHAPTER 5 \u2022 L OGISTIC REGRESSION\nlabel y. Rather than measure similarity, we usually talk about the opposite of this:\nthedistance between the system output and the gold output, and we call this distance\nthelossfunction or the cost function . In the next section we\u2019ll introduce the loss loss\nfunction that is commonly used for logistic regression and also for neural networks,\nthecross-entropy loss .\nThe second thing we need is an optimization algorithm for iteratively updating\nthe weights so as to minimize this loss function. The standard algorithm for this is\ngradient descent ; we\u2019ll introduce the stochastic gradient descent algorithm in the\nfollowing section.\nWe\u2019ll describe these algorithms for the simpler case of binary logistic regres-\nsion in the next two sections, and then turn to multinomial logistic regression in\nSection 5.8.\n5.5 The cross-entropy loss function\nWe need a loss function that expresses, for an observation x, how close the classi\ufb01er\noutput ( \u02c6 y=s(w\u0001x+b)) is to the correct output ( y, which is 0 or 1). We\u2019ll call this:\nL(\u02c6y;y) = How much \u02c6 ydiffers from the true y (5.20)\nWe do this via a loss function that prefers the correct class labels of the train-\ning examples to be more likely . This is called conditional maximum likelihood\nestimation : we choose the parameters w;bthatmaximize the log probability of\nthe true ylabels in the training data given the observations x. The resulting loss\nfunction is the negative log likelihood loss , generally called the cross-entropy loss .cross-entropy\nloss\nLet\u2019s derive this loss function, applied to a single observation x. We\u2019d like to\nlearn weights that maximize the probability of the correct label p(yjx). Since there\nare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we can\nexpress the probability p(yjx)that our classi\ufb01er produces for one observation as the\nfollowing (keeping in mind that if y=1, Eq. 5.21 simpli\ufb01es to \u02c6 y; ify=0, Eq. 5.21\nsimpli\ufb01es to 1\u0000\u02c6y):\np(yjx) = \u02c6yy(1\u0000\u02c6y)1\u0000y(5.21)\nNow we take the log of both sides. This will turn out to be handy mathematically,\nand doesn\u2019t hurt us; whatever values maximize a probability will also maximize the\nlog of the probability:\nlogp(yjx) = log\u0002\n\u02c6yy(1\u0000\u02c6y)1\u0000y\u0003\n=ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y) (5.22)\nEq. 5.22 describes a log likelihood that should be maximized. In order to turn this\ninto a loss function (something that we need to minimize), we\u2019ll just \ufb02ip the sign on\nEq. 5.22. The result is the cross-entropy loss LCE:\nLCE(\u02c6y;y) =\u0000logp(yjx) =\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)] (5.23)\nFinally, we can plug in the de\ufb01nition of \u02c6 y=s(w\u0001x+b):\nLCE(\u02c6y;y) =\u0000[ylogs(w\u0001x+b)+(1\u0000y)log(1\u0000s(w\u0001x+b))] (5.24)",
    "metadata": {
      "source": "5",
      "chunk_id": 11,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13",
    "metadata": {
      "source": "5",
      "chunk_id": 12,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "5.6 \u2022 G RADIENT DESCENT 13\nLet\u2019s see if this loss function does the right thing for our example from Fig. 5.2. We\nwant the loss to be smaller if the model\u2019s estimate is close to correct, and bigger if\nthe model is confused. So \ufb01rst let\u2019s suppose the correct gold label for the sentiment\nexample in Fig. 5.2 is positive, i.e., y=1. In this case our model is doing well, since\nfrom Eq. 5.8 it indeed gave the example a higher probability of being positive (.70)\nthan negative (.30). If we plug s(w\u0001x+b) =:70 and y=1 into Eq. 5.24, the right\nside of the equation drops out, leading to the following loss (we\u2019ll use log to mean\nnatural log when the base is not speci\ufb01ed):\nLCE(\u02c6y;y) =\u0000[ylogs(w\u0001x+b)+(1\u0000y)log(1\u0000s(w\u0001x+b))]\n=\u0000[logs(w\u0001x+b)]\n=\u0000log(:70)\n= :36\nBy contrast, let\u2019s pretend instead that the example in Fig. 5.2 was actually negative,\ni.e., y=0 (perhaps the reviewer went on to say \u201cBut bottom line, the movie is\nterrible! I beg you not to see it!\u201d). In this case our model is confused and we\u2019d want\nthe loss to be higher. Now if we plug y=0 and 1\u0000s(w\u0001x+b) =:30 from Eq. 5.8\ninto Eq. 5.24, the left side of the equation drops out:\nLCE(\u02c6y;y) =\u0000[ylogs(w\u0001x+b)+(1\u0000y)log(1\u0000s(w\u0001x+b))]\n= \u0000[log(1\u0000s(w\u0001x+b))]\n= \u0000log(:30)\n= 1:2\nSure enough, the loss for the \ufb01rst classi\ufb01er (.36) is less than the loss for the second\nclassi\ufb01er (1.2).\nWhy does minimizing this negative log probability do what we want? A perfect\nclassi\ufb01er would assign probability 1 to the correct outcome ( y=1 or y=0) and\nprobability 0 to the incorrect outcome. That means if yequals 1, the higher \u02c6 yis (the\ncloser it is to 1), the better the classi\ufb01er; the lower \u02c6 yis (the closer it is to 0), the\nworse the classi\ufb01er. If yequals 0, instead, the higher 1 \u0000\u02c6yis (closer to 1), the better\nthe classi\ufb01er. The negative log of \u02c6 y(if the true yequals 1) or 1\u0000\u02c6y(if the true y\nequals 0) is a convenient loss metric since it goes from 0 (negative log of 1, no loss)\nto in\ufb01nity (negative log of 0, in\ufb01nite loss). This loss function also ensures that as\nthe probability of the correct answer is maximized, the probability of the incorrect\nanswer is minimized; since the two sum to one, any increase in the probability of the\ncorrect answer is coming at the expense of the incorrect answer. It\u2019s called the cross-\nentropy loss, because Eq. 5.22 is also the formula for the cross-entropy between the\ntrue probability distribution yand our estimated distribution \u02c6 y.",
    "metadata": {
      "source": "5",
      "chunk_id": 13,
      "token_count": 770,
      "chapter_title": ""
    }
  },
  {
    "content": "probability 0 to the incorrect outcome. That means if yequals 1, the higher \u02c6 yis (the\ncloser it is to 1), the better the classi\ufb01er; the lower \u02c6 yis (the closer it is to 0), the\nworse the classi\ufb01er. If yequals 0, instead, the higher 1 \u0000\u02c6yis (closer to 1), the better\nthe classi\ufb01er. The negative log of \u02c6 y(if the true yequals 1) or 1\u0000\u02c6y(if the true y\nequals 0) is a convenient loss metric since it goes from 0 (negative log of 1, no loss)\nto in\ufb01nity (negative log of 0, in\ufb01nite loss). This loss function also ensures that as\nthe probability of the correct answer is maximized, the probability of the incorrect\nanswer is minimized; since the two sum to one, any increase in the probability of the\ncorrect answer is coming at the expense of the incorrect answer. It\u2019s called the cross-\nentropy loss, because Eq. 5.22 is also the formula for the cross-entropy between the\ntrue probability distribution yand our estimated distribution \u02c6 y.\nNow we know what we want to minimize; in the next section, we\u2019ll see how to\n\ufb01nd the minimum.\n5.6 Gradient Descent\nOur goal with gradient descent is to \ufb01nd the optimal weights: minimize the loss\nfunction we\u2019ve de\ufb01ned for the model. In Eq. 5.25 below, we\u2019ll explicitly represent\nthe fact that the cross-entropy loss function LCEis parameterized by the weights. In",
    "metadata": {
      "source": "5",
      "chunk_id": 14,
      "token_count": 368,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n14 CHAPTER 5 \u2022 L OGISTIC REGRESSION\nmachine learning in general we refer to the parameters being learned as q; in the\ncase of logistic regression q=fw;bg. So the goal is to \ufb01nd the set of weights which\nminimizes the loss function, averaged over all examples:\n\u02c6q=argmin\nq1\nmmX\ni=1LCE(f(x(i);q);y(i)) (5.25)\nHow shall we \ufb01nd the minimum of this (or any) loss function? Gradient descent is\na method that \ufb01nds a minimum of a function by \ufb01guring out in which direction (in\nthe space of the parameters q) the function\u2019s slope is rising the most steeply, and\nmoving in the opposite direction. The intuition is that if you are hiking in a canyon\nand trying to descend most quickly down to the river at the bottom, you might look\naround yourself in all directions, \ufb01nd the direction where the ground is sloping the\nsteepest, and walk downhill in that direction.\nFor logistic regression, this loss function is conveniently convex . A convex func- convex\ntion has at most one minimum; there are no local minima to get stuck in, so gradient\ndescent starting from any point is guaranteed to \ufb01nd the minimum. (By contrast,\nthe loss for multi-layer neural networks is non-convex, and gradient descent may\nget stuck in local minima for neural network training and never \ufb01nd the global opti-\nmum.)\nAlthough the algorithm (and the concept of gradient) are designed for direction\nvectors , let\u2019s \ufb01rst consider a visualization of the case where the parameter of our\nsystem is just a single scalar w, shown in Fig. 5.4.\nGiven a random initialization of wat some value w1, and assuming the loss\nfunction Lhappened to have the shape in Fig. 5.4, we need the algorithm to tell us\nwhether at the next iteration we should move left (making w2smaller than w1) or\nright (making w2bigger than w1) to reach the minimum.\nwLoss\n0w1wminslope of loss at w1 is negative(goal)one stepof gradientdescent\nFigure 5.4 The \ufb01rst step in iteratively \ufb01nding the minimum of this loss function, by moving\nwin the reverse direction from the slope of the function. Since the slope is negative, we need\nto move win a positive direction, to the right. Here superscripts are used for learning steps,\nsow1means the initial value of w(which is 0), w2the value at the second step, and so on.\nThe gradient descent algorithm answers this question by \ufb01nding the gradient gradient\nof the loss function at the current point and moving in the opposite direction. The\ngradient of a function of many variables is a vector pointing in the direction of the\ngreatest increase in a function. The gradient is a multi-variable generalization of the",
    "metadata": {
      "source": "5",
      "chunk_id": 15,
      "token_count": 645,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\n5.6 \u2022 G RADIENT DESCENT 15\nslope, so for a function of one variable like the one in Fig. 5.4, we can informally\nthink of the gradient as the slope. The dotted line in Fig. 5.4 shows the slope of this\nhypothetical loss function at point w=w1. You can see that the slope of this dotted\nline is negative. Thus to \ufb01nd the minimum, gradient descent tells us to go in the\nopposite direction: moving win a positive direction.\nThe magnitude of the amount to move in gradient descent is the value of the\nsloped\ndwL(f(x;w);y)weighted by a learning rate h. A higher (faster) learning learning rate\nrate means that we should move wmore on each step. The change we make in our\nparameter is the learning rate times the gradient (or the slope, in our single-variable\nexample):\nwt+1=wt\u0000hd\ndwL(f(x;w);y) (5.26)\nNow let\u2019s extend the intuition from a function of one scalar variable wto many\nvariables, because we don\u2019t just want to move left or right, we want to know where\nin the N-dimensional space (of the Nparameters that make up q) we should move.\nThe gradient is just such a vector; it expresses the directional components of the\nsharpest slope along each of those Ndimensions. If we\u2019re just imagining two weight\ndimensions (say for one weight wand one bias b), the gradient might be a vector with\ntwo orthogonal components, each of which tells us how much the ground slopes in\nthewdimension and in the bdimension. Fig. 5.5 shows a visualization of the value\nof a 2-dimensional gradient vector taken at the red point.\nIn an actual logistic regression, the parameter vector wis much longer than 1 or\n2, since the input feature vector xcan be quite long, and we need a weight wifor\neach xi. For each dimension/variable wiinw(plus the bias b), the gradient will have\na component that tells us the slope with respect to that variable. In each dimension\nwi, we express the slope as a partial derivative\u00b6\n\u00b6wiof the loss function. Essentially\nwe\u2019re asking: \u201cHow much would a small change in that variable wiin\ufb02uence the\ntotal loss function L?\u201d\nFormally, then, the gradient of a multi-variable function fis a vector in which\neach component expresses the partial derivative of fwith respect to one of the vari-\nables. We\u2019ll use the inverted Greek delta symbol \u00d1to refer to the gradient, and\nCost(w,b)\nwb\nFigure 5.5 Visualization of the gradient vector at the red point in two dimensions wand\nb, showing a red arrow in the x-y plane pointing in the direction we will go to look for the\nminimum: the opposite direction of the gradient (recall that the gradient points in the direction\nof increase not decrease).",
    "metadata": {
      "source": "5",
      "chunk_id": 16,
      "token_count": 639,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\n16 CHAPTER 5 \u2022 L OGISTIC REGRESSION\nrepresent \u02c6 yasf(x;q)to make the dependence on qmore obvious:\n\u00d1L(f(x;q);y) =2\n6666664\u00b6\n\u00b6w1L(f(x;q);y)\n\u00b6\n\u00b6w2L(f(x;q);y)\n...\n\u00b6\n\u00b6wnL(f(x;q);y)\n\u00b6\n\u00b6bL(f(x;q);y)3\n7777775(5.27)\nThe \ufb01nal equation for updating qbased on the gradient is thus\nqt+1=qt\u0000h\u00d1L(f(x;q);y) (5.28)\n5.6.1 The Gradient for Logistic Regression\nIn order to update q, we need a de\ufb01nition for the gradient \u00d1L(f(x;q);y). Recall that\nfor logistic regression, the cross-entropy loss function is:\nLCE(\u02c6y;y) =\u0000[ylogs(w\u0001x+b)+(1\u0000y)log(1\u0000s(w\u0001x+b))] (5.29)\nIt turns out that the derivative of this function for one observation vector xis Eq. 5.30\n(the interested reader can see Section 5.10 for the derivation of this equation):\n\u00b6LCE(\u02c6y;y)\n\u00b6wj= [s(w\u0001x+b)\u0000y]xj\n= ( \u02c6y\u0000y)xj (5.30)\nYou\u2019ll also sometimes see this equation in the equivalent form:\n\u00b6LCE(\u02c6y;y)\n\u00b6wj=\u0000(y\u0000\u02c6y)xj (5.31)\nNote in these equations that the gradient with respect to a single weight wjrep-\nresents a very intuitive value: the difference between the true yand our estimated\n\u02c6y=s(w\u0001x+b)for that observation, multiplied by the corresponding input value\nxj.\n5.6.2 The Stochastic Gradient Descent Algorithm\nStochastic gradient descent is an online algorithm that minimizes the loss function\nby computing its gradient after each training example, and nudging qin the right\ndirection (the opposite direction of the gradient). (An \u201conline algorithm\u201d is one that\nprocesses its input example by example, rather than waiting until it sees the entire\ninput.) Stochastic gradient descent is called stochastic because it chooses a single\nrandom example at a time; in Section 5.6.4 we\u2019ll discuss other versions of gradient\ndescent that batch many examples at once. Fig. 5.6 shows the algorithm.\nThe learning rate his ahyperparameter that must be adjusted. If it\u2019s too high, hyperparameter\nthe learner will take steps that are too large, overshooting the minimum of the loss\nfunction. If it\u2019s too low, the learner will take steps that are too small, and take too\nlong to get to the minimum. It is common to start with a higher learning rate and then\nslowly decrease it, so that it is a function of the iteration kof training; the notation\nhkcan be used to mean the value of the learning rate at iteration k.",
    "metadata": {
      "source": "5",
      "chunk_id": 17,
      "token_count": 658,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\n5.6 \u2022 G RADIENT DESCENT 17\nfunction STOCHASTIC GRADIENT DESCENT (L(),f(),x,y)returns q\n# where: L is the loss function\n# f is a function parameterized by q\n# x is the set of training inputs x(1);x(2);:::;x(m)\n# y is the set of training outputs (labels) y(1);y(2);:::;y(m)\nq 0 # (or small random values)\nrepeat til done # see caption\nFor each training tuple (x(i);y(i))(in random order)\n1. Optional (for reporting): # How are we doing on this tuple?\nCompute \u02c6 y(i)=f(x(i);q)# What is our estimated output \u02c6 y?\nCompute the loss L(\u02c6y(i);y(i))# How far off is \u02c6 y(i)from the true output y(i)?\n2.g \u00d1qL(f(x(i);q);y(i)) # How should we move qto maximize loss?\n3.q q\u0000hg # Go the other way instead\nreturn q\nFigure 5.6 The stochastic gradient descent algorithm. Step 1 (computing the loss) is used\nmainly to report how well we are doing on the current tuple; we don\u2019t need to compute the\nloss in order to compute the gradient. The algorithm can terminate when it converges (when\nthe gradient norm <\u000f), or when progress halts (for example when the loss starts going up on\na held-out set). Weights are initialized to 0 for logistic regression, but to small random values\nfor neural networks, as we\u2019ll see in Chapter 7.\nWe\u2019ll discuss hyperparameters in more detail in Chapter 7, but in short, they are\na special kind of parameter for any machine learning model. Unlike regular param-\neters of a model (weights like wandb), which are learned by the algorithm from\nthe training set, hyperparameters are special parameters chosen by the algorithm\ndesigner that affect how the algorithm works.\n5.6.3 Working through an example\nLet\u2019s walk through a single step of the gradient descent algorithm. We\u2019ll use a\nsimpli\ufb01ed version of the example in Fig. 5.2 as it sees a single observation x, whose\ncorrect value is y=1 (this is a positive review), and with a feature vector x= [x1;x2]\nconsisting of these two features:\nx1=3 (count of positive lexicon words)\nx2=2 (count of negative lexicon words)\nLet\u2019s assume the initial weights and bias in q0are all set to 0, and the initial learning\nratehis 0.1:\nw1=w2=b=0\nh=0:1\nThe single update step requires that we compute the gradient, multiplied by the\nlearning rate\nqt+1=qt\u0000h\u00d1qL(f(x(i);q);y(i))",
    "metadata": {
      "source": "5",
      "chunk_id": 18,
      "token_count": 637,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18",
    "metadata": {
      "source": "5",
      "chunk_id": 19,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "18 CHAPTER 5 \u2022 L OGISTIC REGRESSION\nIn our mini example there are three parameters, so the gradient vector has 3 dimen-\nsions, for w1,w2, and b. We can compute the \ufb01rst gradient as follows:\n\u00d1w;bL=2\n64\u00b6LCE(\u02c6y;y)\n\u00b6w1\u00b6LCE(\u02c6y;y)\n\u00b6w2\u00b6LCE(\u02c6y;y)\n\u00b6b3\n75=2\n4(s(w\u0001x+b)\u0000y)x1\n(s(w\u0001x+b)\u0000y)x2\ns(w\u0001x+b)\u0000y3\n5=2\n4(s(0)\u00001)x1\n(s(0)\u00001)x2\ns(0)\u000013\n5=2\n4\u00000:5x1\n\u00000:5x2\n\u00000:53\n5=2\n4\u00001:5\n\u00001:0\n\u00000:53\n5\nNow that we have a gradient, we compute the new parameter vector q1by moving\nq0in the opposite direction from the gradient:\nq1=2\n4w1\nw2\nb3\n5\u0000h2\n4\u00001:5\n\u00001:0\n\u00000:53\n5=2\n4:15\n:1\n:053\n5\nSo after one step of gradient descent, the weights have shifted to be: w1=:15,\nw2=:1, and b=:05.\nNote that this observation xhappened to be a positive example. We would expect\nthat after seeing more negative examples with high counts of negative words, that\nthe weight w2would shift to have a negative value.\n5.6.4 Mini-batch training\nStochastic gradient descent is called stochastic because it chooses a single random\nexample at a time, moving the weights so as to improve performance on that single\nexample. That can result in very choppy movements, so it\u2019s common to compute the\ngradient over batches of training instances rather than a single instance.\nFor example in batch training we compute the gradient over the entire dataset. batch training\nBy seeing so many examples, batch training offers a superb estimate of which di-\nrection to move the weights, at the cost of spending a lot of time processing every\nsingle example in the training set to compute this perfect direction.\nA compromise is mini-batch training: we train on a group of mexamples (per- mini-batch\nhaps 512, or 1024) that is less than the whole dataset. (If mis the size of the dataset,\nthen we are doing batch gradient descent; if m=1, we are back to doing stochas-\ntic gradient descent.) Mini-batch training also has the advantage of computational\nef\ufb01ciency. The mini-batches can easily be vectorized, choosing the size of the mini-\nbatch based on the computational resources. This allows us to process all the exam-\nples in one mini-batch in parallel and then accumulate the loss, something that\u2019s not\npossible with individual or batch training.\nWe just need to de\ufb01ne mini-batch versions of the cross-entropy loss function\nwe de\ufb01ned in Section 5.5 and the gradient in Section 5.6.1. Let\u2019s extend the cross-\nentropy loss for one example from Eq. 5.23 to mini-batches of size m. We\u2019ll continue\nto use the notation that x(i)andy(i)mean the ith training features and training label,\nrespectively. We make the assumption that the training examples are independent:\nlogp(training labels ) = logmY",
    "metadata": {
      "source": "5",
      "chunk_id": 20,
      "token_count": 769,
      "chapter_title": ""
    }
  },
  {
    "content": "single example in the training set to compute this perfect direction.\nA compromise is mini-batch training: we train on a group of mexamples (per- mini-batch\nhaps 512, or 1024) that is less than the whole dataset. (If mis the size of the dataset,\nthen we are doing batch gradient descent; if m=1, we are back to doing stochas-\ntic gradient descent.) Mini-batch training also has the advantage of computational\nef\ufb01ciency. The mini-batches can easily be vectorized, choosing the size of the mini-\nbatch based on the computational resources. This allows us to process all the exam-\nples in one mini-batch in parallel and then accumulate the loss, something that\u2019s not\npossible with individual or batch training.\nWe just need to de\ufb01ne mini-batch versions of the cross-entropy loss function\nwe de\ufb01ned in Section 5.5 and the gradient in Section 5.6.1. Let\u2019s extend the cross-\nentropy loss for one example from Eq. 5.23 to mini-batches of size m. We\u2019ll continue\nto use the notation that x(i)andy(i)mean the ith training features and training label,\nrespectively. We make the assumption that the training examples are independent:\nlogp(training labels ) = logmY\ni=1p(y(i)jx(i))\n=mX\ni=1logp(y(i)jx(i))\n=\u0000mX\ni=1LCE(\u02c6y(i);y(i)) (5.32)",
    "metadata": {
      "source": "5",
      "chunk_id": 21,
      "token_count": 331,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\n5.7 \u2022 R EGULARIZATION 19\nNow the cost function for the mini-batch of mexamples is the average loss for each\nexample:\nCost(\u02c6y;y) =1\nmmX\ni=1LCE(\u02c6y(i);y(i))\n=\u00001\nmmX\ni=1y(i)logs(w\u0001x(i)+b)+(1\u0000y(i))log\u0010\n1\u0000s(w\u0001x(i)+b)\u0011\n(5.33)\nThe mini-batch gradient is the average of the individual gradients from Eq. 5.30:\n\u00b6Cost(\u02c6y;y)\n\u00b6wj=1\nmmX\ni=1h\ns(w\u0001x(i)+b)\u0000y(i)i\nx(i)\nj(5.34)\nInstead of using the sum notation, we can more ef\ufb01ciently compute the gradient\nin its matrix form, following the vectorization we saw on page 7, where we have a\nmatrix Xof size [m\u0002f]representing the minputs in the batch, and a vector yof size\n[m\u00021]representing the correct outputs:\n\u00b6Cost(\u02c6y;y)\n\u00b6w=1\nm(\u02c6y\u0000y)|X\n=1\nm(s(Xw+b)\u0000y)|X (5.35)\n5.7 Regularization\nNumquam ponenda est pluralitas sine necessitate\n\u2018Plurality should never be proposed unless needed\u2019\nWilliam of Occam\nThere is a problem with learning weights that make the model perfectly match the\ntraining data. If a feature is perfectly predictive of the outcome because it happens\nto only occur in one class, it will be assigned a very high weight. The weights for\nfeatures will attempt to perfectly \ufb01t details of the training set, in fact too perfectly,\nmodeling noisy factors that just accidentally correlate with the class. This problem is\ncalled over\ufb01tting . A good model should be able to generalize well from the training over\ufb01tting\ngeneralize data to the unseen test set, but a model that over\ufb01ts will have poor generalization.\nTo avoid over\ufb01tting, a new regularization term R(q)is added to the loss func- regularization\ntion in Eq. 5.25, resulting in the following loss for a batch of mexamples (slightly\nrewritten from Eq. 5.25 to be maximizing log probability rather than minimizing\nloss, and removing the1\nmterm which doesn\u2019t affect the argmax):\n\u02c6q=argmax\nqmX\ni=1logP(y(i)jx(i))\u0000aR(q) (5.36)\nThe new regularization term R(q)is used to penalize large weights. Thus a setting\nof the weights that matches the training data perfectly\u2014 but uses many weights with",
    "metadata": {
      "source": "5",
      "chunk_id": 22,
      "token_count": 606,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\n20 CHAPTER 5 \u2022 L OGISTIC REGRESSION\nhigh values to do so\u2014will be penalized more than a setting that matches the data a\nlittle less well, but does so using smaller weights. There are two common ways to\ncompute this regularization term R(q).L2 regularization is a quadratic function ofL2\nregularization\nthe weight values, named because it uses the (square of the) L2 norm of the weight\nvalues. The L2 norm, jjqjj2, is the same as the Euclidean distance of the vector q\nfrom the origin. If qconsists of nweights, then:\nR(q) =jjqjj2\n2=nX\nj=1q2\nj (5.37)\nThe L2 regularized loss function becomes:\n\u02c6q=argmax\nq\"mX\ni=1logP(y(i)jx(i))#\n\u0000anX\nj=1q2\nj (5.38)\nL1 regularization is a linear function of the weight values, named after the L1 normL1\nregularization\njjWjj1, the sum of the absolute values of the weights, or Manhattan distance (the\nManhattan distance is the distance you\u2019d have to walk between two points in a city\nwith a street grid like New York):\nR(q) =jjqjj1=nX\ni=1jqij (5.39)\nThe L1 regularized loss function becomes:\n\u02c6q=argmax\nq\"mX\ni=1logP(y(i)jx(i))#\n\u0000anX\nj=1jqjj (5.40)\nThese kinds of regularization come from statistics, where L1 regularization is called\nlasso regression (Tibshirani, 1996) and L2 regularization is called ridge regression , lasso\nridge and both are commonly used in language processing. L2 regularization is easier to\noptimize because of its simple derivative (the derivative of q2is just 2 q), while\nL1 regularization is more complex (the derivative of jqjis non-continuous at zero).\nBut while L2 prefers weight vectors with many small weights, L1 prefers sparse\nsolutions with some larger weights but many more weights set to zero. Thus L1\nregularization leads to much sparser weight vectors, that is, far fewer features.\nBoth L1 and L2 regularization have Bayesian interpretations as constraints on\nthe prior of how weights should look. L1 regularization can be viewed as a Laplace\nprior on the weights. L2 regularization corresponds to assuming that weights are\ndistributed according to a Gaussian distribution with mean m=0. In a Gaussian\nor normal distribution, the further away a value is from the mean, the lower its\nprobability (scaled by the variance s). By using a Gaussian prior on the weights, we\nare saying that weights prefer to have the value 0. A Gaussian for a weight qjis\n1q\n2ps2\njexp \n\u0000(qj\u0000mj)2\n2s2\nj!\n(5.41)\nIf we multiply each weight by a Gaussian prior on the weight, we are thus maximiz-\ning the following constraint:\n\u02c6q=argmax\nqmY\ni=1P(y(i)jx(i))\u0002nY\nj=11q\n2ps2\njexp \n\u0000(qj\u0000mj)2\n2s2\nj!\n(5.42)",
    "metadata": {
      "source": "5",
      "chunk_id": 23,
      "token_count": 730,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21",
    "metadata": {
      "source": "5",
      "chunk_id": 24,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "5.8 \u2022 L EARNING IN MULTINOMIAL LOGISTIC REGRESSION 21\nwhich in log space, with m=0, and assuming 2 s2=1, corresponds to\n\u02c6q=argmax\nqmX\ni=1logP(y(i)jx(i))\u0000anX\nj=1q2\nj (5.43)\nwhich is in the same form as Eq. 5.38.\n5.8 Learning in Multinomial Logistic Regression\nThe loss function for multinomial logistic regression generalizes the loss function\nfor binary logistic regression from 2 to Kclasses. Recall that that the cross-entropy\nloss for binary logistic regression (repeated from Eq. 5.23) is:\nLCE(\u02c6y;y) =\u0000logp(yjx) =\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)] (5.44)\nThe loss function for multinomial logistic regression generalizes the two terms in\nEq. 5.44 (one that is non-zero when y=1 and one that is non-zero when y=0) to\nKterms. As we mentioned above, for multinomial regression we\u2019ll represent both y\nand\u02c6yas vectors. The true label yis a vector with Kelements, each corresponding\nto a class, with yc=1 if the correct class is c, with all other elements of ybeing 0.\nAnd our classi\ufb01er will produce an estimate vector with Kelements \u02c6y, each element\n\u02c6ykof which represents the estimated probability p(yk=1jx).\nThe loss function for a single example x, generalizing from binary logistic re-\ngression, is the sum of the logs of the Koutput classes, each weighted by the indi-\ncator function yk(Eq. 5.45). This turns out to be just the negative log probability of\nthe correct class c(Eq. 5.46):\nLCE(\u02c6y;y) =\u0000KX\nk=1yklog \u02c6yk (5.45)\n=\u0000log \u02c6yc;(where cis the correct class) (5.46)\n=\u0000log \u02c6p(yc=1jx)(where cis the correct class)\n=\u0000logexp(wc\u0001x+bc)PK\nj=1exp(wj\u0001x+bj)(cis the correct class) (5.47)\nHow did we get from Eq. 5.45 to Eq. 5.46? Because only one class (let\u2019s call it c) is\nthe correct one, the vector ytakes the value 1 only for this value of k, i.e., has yc=1\nandyj=08j6=c. That means the terms in the sum in Eq. 5.45 will all be 0 except\nfor the term corresponding to the true class c. Hence the cross-entropy loss is simply\nthe log of the output probability corresponding to the correct class, and we therefore\nalso call Eq. 5.46 the negative log likelihood loss .negative log\nlikelihood loss\nOf course for gradient descent we don\u2019t need the loss, we need its gradient. The\ngradient for a single example turns out to be very similar to the gradient for binary\nlogistic regression, (\u02c6y\u0000y)x, that we saw in Eq. 5.30. Let\u2019s consider one piece of the\ngradient, the derivative for a single weight. For each class k, the weight of the ith\nelement of input xiswk;i. What is the partial derivative of the loss with respect to",
    "metadata": {
      "source": "5",
      "chunk_id": 25,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "=\u0000logexp(wc\u0001x+bc)PK\nj=1exp(wj\u0001x+bj)(cis the correct class) (5.47)\nHow did we get from Eq. 5.45 to Eq. 5.46? Because only one class (let\u2019s call it c) is\nthe correct one, the vector ytakes the value 1 only for this value of k, i.e., has yc=1\nandyj=08j6=c. That means the terms in the sum in Eq. 5.45 will all be 0 except\nfor the term corresponding to the true class c. Hence the cross-entropy loss is simply\nthe log of the output probability corresponding to the correct class, and we therefore\nalso call Eq. 5.46 the negative log likelihood loss .negative log\nlikelihood loss\nOf course for gradient descent we don\u2019t need the loss, we need its gradient. The\ngradient for a single example turns out to be very similar to the gradient for binary\nlogistic regression, (\u02c6y\u0000y)x, that we saw in Eq. 5.30. Let\u2019s consider one piece of the\ngradient, the derivative for a single weight. For each class k, the weight of the ith\nelement of input xiswk;i. What is the partial derivative of the loss with respect to\nwk;i? This derivative turns out to be just the difference between the true value for the\nclass k(which is either 1 or 0) and the probability the classi\ufb01er outputs for class k,",
    "metadata": {
      "source": "5",
      "chunk_id": 26,
      "token_count": 328,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22\n\n22 CHAPTER 5 \u2022 L OGISTIC REGRESSION\nweighted by the value of the input xicorresponding to the ith element of the weight\nvector for class k:\n\u00b6LCE\n\u00b6wk;i=\u0000(yk\u0000\u02c6yk)xi\n=\u0000(yk\u0000p(yk=1jx))xi\n=\u0000 \nyk\u0000exp(wk\u0001x+bk)PK\nj=1exp(wj\u0001x+bj)!\nxi (5.48)\nWe\u2019ll return to this case of the gradient for softmax regression when we introduce\nneural networks in Chapter 7, and at that time we\u2019ll also discuss the derivation of\nthis gradient in equations Eq. ??\u2013Eq. ??.\n5.9 Interpreting models\nOften we want to know more than just the correct classi\ufb01cation of an observation.\nWe want to know why the classi\ufb01er made the decision it did. That is, we want our\ndecision to be interpretable . Interpretability can be hard to de\ufb01ne strictly, but the interpretable\ncore idea is that as humans we should know why our algorithms reach the conclu-\nsions they do. Because the features to logistic regression are often human-designed,\none way to understand a classi\ufb01er\u2019s decision is to understand the role each feature\nplays in the decision. Logistic regression can be combined with statistical tests (the\nlikelihood ratio test, or the Wald test); investigating whether a particular feature is\nsigni\ufb01cant by one of these tests, or inspecting its magnitude (how large is the weight\nwassociated with the feature?) can help us interpret why the classi\ufb01er made the\ndecision it makes. This is enormously important for building transparent models.\nFurthermore, in addition to its use as a classi\ufb01er, logistic regression in NLP and\nmany other \ufb01elds is widely used as an analytic tool for testing hypotheses about the\neffect of various explanatory variables (features). In text classi\ufb01cation, perhaps we\nwant to know if logically negative words ( no, not, never ) are more likely to be asso-\nciated with negative sentiment, or if negative reviews of movies are more likely to\ndiscuss the cinematography. However, in doing so it\u2019s necessary to control for po-\ntential confounds: other factors that might in\ufb02uence sentiment (the movie genre, the\nyear it was made, perhaps the length of the review in words). Or we might be study-\ning the relationship between NLP-extracted linguistic features and non-linguistic\noutcomes (hospital readmissions, political outcomes, or product sales), but need to\ncontrol for confounds (the age of the patient, the county of voting, the brand of the\nproduct). In such cases, logistic regression allows us to test whether some feature is\nassociated with some outcome above and beyond the effect of other features.\n5.10 Advanced: Deriving the Gradient Equation\nIn this section we give the derivation of the gradient of the cross-entropy loss func-\ntion LCEfor logistic regression. Let\u2019s start with some quick calculus refreshers.\nFirst, the derivative of ln (x):\nd\ndxln(x) =1\nx(5.49)",
    "metadata": {
      "source": "5",
      "chunk_id": 27,
      "token_count": 686,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23\n\n5.11 \u2022 S UMMARY 23\nSecond, the (very elegant) derivative of the sigmoid:\nds(z)\ndz=s(z)(1\u0000s(z)) (5.50)\nFinally, the chain rule of derivatives. Suppose we are computing the derivative chain rule\nof a composite function f(x) =u(v(x)). The derivative of f(x)is the derivative of\nu(x)with respect to v(x)times the derivative of v(x)with respect to x:\nd f\ndx=du\ndv\u0001dv\ndx(5.51)\nFirst, we want to know the derivative of the loss function with respect to a single\nweight wj(we\u2019ll need to compute it for each weight, and for the bias):\n\u00b6LCE\n\u00b6wj=\u00b6\n\u00b6wj\u0000[ylogs(w\u0001x+b)+(1\u0000y)log(1\u0000s(w\u0001x+b))]\n=\u0000\u0014\u00b6\n\u00b6wjylogs(w\u0001x+b)+\u00b6\n\u00b6wj(1\u0000y)log[1\u0000s(w\u0001x+b)]\u0015\n(5.52)\nNext, using the chain rule, and relying on the derivative of log:\n\u00b6LCE\n\u00b6wj=\u0000y\ns(w\u0001x+b)\u00b6\n\u00b6wjs(w\u0001x+b)\u00001\u0000y\n1\u0000s(w\u0001x+b)\u00b6\n\u00b6wj1\u0000s(w\u0001x+b)\n(5.53)\nRearranging terms:\n\u00b6LCE\n\u00b6wj=\u0000\u0014y\ns(w\u0001x+b)\u00001\u0000y\n1\u0000s(w\u0001x+b)\u0015\u00b6\n\u00b6wjs(w\u0001x+b)\n(5.54)\nAnd now plugging in the derivative of the sigmoid, and using the chain rule one\nmore time, we end up with Eq. 5.55:\n\u00b6LCE\n\u00b6wj=\u0000\u0014y\u0000s(w\u0001x+b)\ns(w\u0001x+b)[1\u0000s(w\u0001x+b)]\u0015\ns(w\u0001x+b)[1\u0000s(w\u0001x+b)]\u00b6(w\u0001x+b)\n\u00b6wj\n=\u0000\u0014y\u0000s(w\u0001x+b)\ns(w\u0001x+b)[1\u0000s(w\u0001x+b)]\u0015\ns(w\u0001x+b)[1\u0000s(w\u0001x+b)]xj\n=\u0000[y\u0000s(w\u0001x+b)]xj\n= [s(w\u0001x+b)\u0000y]xj (5.55)\n5.11 Summary\nThis chapter introduced the logistic regression model of classi\ufb01cation .\n\u2022 Logistic regression is a supervised machine learning classi\ufb01er that extracts\nreal-valued features from the input, multiplies each by a weight, sums them,\nand passes the sum through a sigmoid function to generate a probability. A\nthreshold is used to make a decision.",
    "metadata": {
      "source": "5",
      "chunk_id": 28,
      "token_count": 598,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24\n\n24 CHAPTER 5 \u2022 L OGISTIC REGRESSION\n\u2022 Logistic regression can be used with two classes (e.g., positive and negative\nsentiment) or with multiple classes ( multinomial logistic regression , for ex-\nample for n-ary text classi\ufb01cation, part-of-speech labeling, etc.).\n\u2022 Multinomial logistic regression uses the softmax function to compute proba-\nbilities.\n\u2022 The weights (vector wand bias b) are learned from a labeled training set via a\nloss function, such as the cross-entropy loss , that must be minimized.\n\u2022 Minimizing this loss function is a convex optimization problem, and iterative\nalgorithms like gradient descent are used to \ufb01nd the optimal weights.\n\u2022Regularization is used to avoid over\ufb01tting.\n\u2022 Logistic regression is also one of the most useful analytic tools, because of its\nability to transparently study the importance of individual features.\nBibliographical and Historical Notes\nLogistic regression was developed in the \ufb01eld of statistics, where it was used for\nthe analysis of binary data by the 1960s, and was particularly common in medicine\n(Cox, 1969). Starting in the late 1970s it became widely used in linguistics as one\nof the formal foundations of the study of linguistic variation (Sankoff and Labov,\n1979).\nNonetheless, logistic regression didn\u2019t become common in natural language pro-\ncessing until the 1990s, when it seems to have appeared simultaneously from two\ndirections. The \ufb01rst source was the neighboring \ufb01elds of information retrieval and\nspeech processing, both of which had made use of regression, and both of which\nlent many other statistical techniques to NLP. Indeed a very early use of logistic\nregression for document routing was one of the \ufb01rst NLP applications to use (LSI)\nembeddings as word representations (Sch \u00a8utze et al., 1995).\nAt the same time in the early 1990s logistic regression was developed and ap-\nplied to NLP at IBM Research under the name maximum entropy modeling ormaximum\nentropy\nmaxent (Berger et al., 1996), seemingly independent of the statistical literature. Un-\nder that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech\ntagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997), coreference resolution\n(Kehler, 1997), and text classi\ufb01cation (Nigam et al., 1999).\nMore on classi\ufb01cation can be found in machine learning textbooks (Hastie et al.\n2001, Witten and Frank 2005, Bishop 2006, Murphy 2012).\nExercises",
    "metadata": {
      "source": "5",
      "chunk_id": 29,
      "token_count": 591,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 25\n\nExercises 25\nBerger, A., S. A. Della Pietra, and V . J. Della Pietra. 1996. A\nmaximum entropy approach to natural language process-\ning. Computational Linguistics , 22(1):39\u201371.\nBishop, C. M. 2006. Pattern recognition and machine learn-\ning. Springer.\nCox, D. 1969. Analysis of Binary Data . Chapman and Hall,\nLondon.\nHastie, T., R. J. Tibshirani, and J. H. Friedman. 2001. The\nElements of Statistical Learning . Springer.\nKehler, A. 1997. Probabilistic coreference in information\nextraction. EMNLP .\nMurphy, K. P. 2012. Machine learning: A probabilistic per-\nspective . MIT Press.\nNg, A. Y . and M. I. Jordan. 2002. On discriminative vs.\ngenerative classi\ufb01ers: A comparison of logistic regres-\nsion and naive bayes. NeurIPS .\nNigam, K., J. D. Lafferty, and A. McCallum. 1999. Using\nmaximum entropy for text classi\ufb01cation. IJCAI-99 work-\nshop on machine learning for information \ufb01ltering .\nRatnaparkhi, A. 1996. A maximum entropy part-of-speech\ntagger. EMNLP .\nRatnaparkhi, A. 1997. A linear observed time statistical\nparser based on maximum entropy models. EMNLP .\nRosenfeld, R. 1996. A maximum entropy approach to adap-\ntive statistical language modeling. Computer Speech and\nLanguage , 10:187\u2013228.\nSankoff, D. and W. Labov. 1979. On the uses of variable\nrules. Language in society , 8(2-3):189\u2013222.\nSch\u00a8utze, H., D. A. Hull, and J. Pedersen. 1995. A compar-\nison of classi\ufb01ers and document representations for the\nrouting problem. SIGIR-95 .\nTibshirani, R. J. 1996. Regression shrinkage and selection\nvia the lasso. Journal of the Royal Statistical Society. Se-\nries B (Methodological) , 58(1):267\u2013288.\nWang, S. and C. D. Manning. 2012. Baselines and bigrams:\nSimple, good sentiment and topic classi\ufb01cation. ACL.\nWitten, I. H. and E. Frank. 2005. Data Mining: Practi-\ncal Machine Learning Tools and Techniques , 2nd edition.\nMorgan Kaufmann.",
    "metadata": {
      "source": "5",
      "chunk_id": 30,
      "token_count": 595,
      "chapter_title": ""
    }
  }
]