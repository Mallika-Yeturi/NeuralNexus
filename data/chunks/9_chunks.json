[
  {
    "content": "# 9\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n9The Transformer\n\u201cThe true art of memory is the art of attention \u201d\nSamuel Johnson, Idler #74 , September 1759\nIn this chapter we introduce the transformer , the standard architecture for build-\ninglarge language models . Transformer-based large language models have com-\npletely changed the \ufb01eld of speech and language processing. Indeed, every subse-\nquent chapter in this textbook will make use of them. We\u2019ll focus for now on left-\nto-right (sometimes called causal or autoregressive) language modeling, in which\nwe are given a sequence of input tokens and predict output tokens one by one by\nconditioning on the prior context.\nThe transformer is a neural network with a speci\ufb01c structure that includes a\nmechanism called self-attention ormulti-head attention .1Attention can be thought\nof as a way to build contextual representations of a token\u2019s meaning by attending to\nand integrating information from surrounding tokens, helping the model learn how\ntokens relate to each other over large spans.\nStackedTransformerBlocksSolongandthanksforlongandthanksforNext tokenall\u2026\u2026\n\u2026\nU\nInput tokensx1x2LanguageModelingHead\nx3x4x5InputEncoding\nE1+\nE2+\nE3+\nE4+\nE5+\u2026\u2026\u2026\u2026\u2026\nU\nU\nU\nU\u2026logitslogitslogitslogitslogits\nFigure 9.1 The architecture of a (left-to-right) transformer, showing how each input token\nget encoded, passed through a set of stacked transformer blocks, and then a language model\nhead that predicts the next token.\nFig. 9.1 sketches the transformer architecture. A transformer has three major\ncomponents. At the center are columns of transformer blocks . Each block is a\nmultilayer network (a multi-head attention layer, feedforward networks and layer\nnormalization steps) that maps an input vector xiin column i(corresponding to input\n1Although multi-head attention developed historically from the RNN attention mechanism (Chapter 8),\nwe\u2019ll de\ufb01ne attention from scratch here for readers who haven\u2019t yet read Chapter 8.",
    "metadata": {
      "source": "9",
      "chunk_id": 0,
      "token_count": 488,
      "chapter_title": "9"
    }
  },
  {
    "content": "## Page 2\n\n2CHAPTER 9 \u2022 T HETRANSFORMER\ntoken i) to an output vector hi. The set of nblocks maps an entire context window\nof input vectors (x1;:::;xn)to a window of output vectors (h1;:::;hn)of the same\nlength. A column might contain from 12 to 96 or more stacked blocks.\nThe column of blocks is preceded by the input encoding component, which pro-\ncesses an input token (like the word thanks ) into a contextual vector representation,\nusing an embedding matrix Eand a mechanism for encoding token position. Each\ncolumn is followed by a language modeling head , which takes the embedding out-\nput by the \ufb01nal transformer block, passes it through an unembedding matrix Uand\na softmax over the vocabulary to generate a single token for that column.\nTransformer-based language models are complex, and so the details will unfold\nover the next 5 chapters. In the next sections we\u2019ll introduce multi-head attention,\nthe rest of the transformer block, and the input encoding and language modeling\nhead components. Chapter 10 discusses how language models are pretrained , and\nhow tokens are generated via sampling . Chapter 11 introduces masked language\nmodeling and the BERT family of bidirectional transformer encoder models. Chap-\nter 12 shows how to prompt LLMs to perform NLP tasks by giving instructions and\ndemonstrations, and how to align the model with human preferences. Chapter 13\nwill introduce machine translation with the encoder-decoder architecture.\n9.1 Attention\nRecall from Chapter 6 that for word2vec and other static embeddings, the repre-\nsentation of a word\u2019s meaning is always the same vector irrespective of the context:\nthe word chicken , for example, is always represented by the same \ufb01xed vector. So\na static vector for the word itmight somehow encode that this is a pronoun used\nfor animals and inanimate entities. But in context ithas a much richer meaning.\nConsider itin one of these two sentences:\n(9.1) The chicken didn\u2019t cross the road because itwas too tired.\n(9.2) The chicken didn\u2019t cross the road because itwas too wide.\nIn (9.1) itis the chicken (i.e., the reader knows that the chicken was tired), while\nin (9.2) itis the road (and the reader knows that the road was wide).2That is, if\nwe are to compute the meaning of this sentence, we\u2019ll need the meaning of itto be\nassociated with the chicken in the \ufb01rst sentence and associated with the road in\nthe second one, sensitive to the context.\nFurthermore, consider reading left to right like a causal language model, pro-\ncessing the sentence up to the word it:\n(9.3) The chicken didn\u2019t cross the road because it\nAt this point we don\u2019t yet know which thing itis going to end up referring to! So a\nrepresentation of itat this point might have aspects of both chicken androad as\nthe reader is trying to guess what happens next.\nThis fact that words have rich linguistic relationships with other words that may\nbe far away pervades language. Consider two more examples:\n(9.4) The keys to the cabinet areon the table.\n(9.5) I walked along the pond , and noticed one of the trees along the bank .\n2We say that in the \ufb01rst example itcorefers with the chicken, and in the second itcorefers with the\nroad; we\u2019ll return to this in Chapter 23.",
    "metadata": {
      "source": "9",
      "chunk_id": 1,
      "token_count": 752,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\n9.1 \u2022 A TTENTION 3\nIn (9.4), the phrase The keys is the subject of the sentence, and in English and many\nlanguages, must agree in grammatical number with the verb are; in this case both are\nplural. In English we can\u2019t use a singular verb like iswith a plural subject like keys\n(we\u2019ll discuss agreement more in Chapter 18). In (9.5), we know that bank refers\nto the side of a pond or river and not a \ufb01nancial institution because of the context,\nincluding words like pond . (We\u2019ll discuss word senses more in Chapter 11.)\nThe point of all these examples is that these contextual words that help us com-\npute the meaning of words in context can be quite far away in the sentence or para-\ngraph. Transformers can build contextual representations of word meaning, contex-\ntual embeddings , by integrating the meaning of these helpful contextual words. In acontextual\nembeddings\ntransformer, layer by layer, we build up richer and richer contextualized representa-\ntions of the meanings of input tokens. At each layer, we compute the representation\nof a token iby combining information about ifrom the previous layer with infor-\nmation about the neighboring tokens to produce a contextualized representation for\neach word at each position.\nAttention is the mechanism in the transformer that weighs and combines the\nrepresentations from appropriate other tokens in the context from layer k\u00001 to build\nthe representation for tokens in layer k.\nThechickendidn\u2019tcrosstheroadbecauseitwastootiredThechickendidn\u2019tcrosstheroadbecauseitwastootiredLayer k+1Layer kself-attention distributioncolumns corresponding to input tokens\nFigure 9.2 The self-attention weight distribution athat is part of the computation of the\nrepresentation for the word itat layer k+1. In computing the representation for it, we attend\ndifferently to the various words at layer k, with darker shades indicating higher self-attention\nvalues. Note that the transformer is attending highly to the columns corresponding to the\ntokens chicken androad , a sensible result, since at the point where itoccurs, it could plausibly\ncorefer with the chicken or the road, and hence we\u2019d like the representation for itto draw on\nthe representation for these earlier words. Figure adapted from Uszkoreit (2017).\nFig. 9.2 shows a schematic example simpli\ufb01ed from a transformer (Uszkoreit,\n2017). The \ufb01gure describes the situation when the current token is itand we need\nto compute a contextual representation for this token at layer k+1 of the transformer,\ndrawing on the representations (from layer k) of every prior token. The \ufb01gure uses\ncolor to represent the attention distribution over the contextual words: the tokens\nchicken androad both have a high attention weight, meaning that as we are com-\nputing the representation for it, we will draw most heavily on the representation for\nchicken androad . This will be useful in building the \ufb01nal representation for it,\nsince itwill end up coreferring with either chicken orroad .\nLet\u2019s now turn to how this attention distribution is represented and computed.",
    "metadata": {
      "source": "9",
      "chunk_id": 2,
      "token_count": 689,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4CHAPTER 9 \u2022 T HETRANSFORMER\n9.1.1 Attention more formally\nAs we\u2019ve said, the attention computation is a way to compute a vector representation\nfor a token at a particular layer of a transformer, by selectively attending to and\nintegrating information from prior tokens at the previous layer. Attention takes an\ninput representation xicorresponding to the input token at position i, and a context\nwindow of prior inputs x1::xi\u00001, and produces an output ai.\nIn causal, left-to-right language models, the context is any of the prior words.\nThat is, when processing xi, the model has access to xias well as the representations\nof all the prior tokens in the context window (context windows consist of thousands\nof tokens) but no tokens after i. (By contrast, in Chapter 11 we\u2019ll generalize attention\nso it can also look ahead to future words.)\nFig. 9.3 illustrates this \ufb02ow of information in an entire causal self-attention layer,\nin which this same attention computation happens in parallel at each token position\ni. Thus a self-attention layer maps input sequences (x1;:::;xn)to output sequences\nof the same length (a1;:::;an).\nattentionattentionSelf-AttentionLayerattentionattentionattentiona1a2a3a4a5x3x4x5x1x2\nFigure 9.3 Information \ufb02ow in causal self-attention. When processing each input xi, the\nmodel attends to all the inputs up to, and including xi.\nSimpli\ufb01ed version of attention At its heart, attention is really just a weighted\nsum of context vectors, with a lot of complications added to how the weights are\ncomputed and what gets summed. For pedagogical purposes let\u2019s \ufb01rst describe a\nsimpli\ufb01ed intuition of attention, in which the attention output aiat token position i\nis simply the weighted sum of all the representations xj, for all j\u0014i; we\u2019ll use ai j\nto mean how much xjshould contribute to ai:\nSimpli\ufb01ed version: ai=X\nj\u0014iai jxj (9.6)\nEach ai jis a scalar used for weighing the value of input xjwhen summing up\nthe inputs to compute ai. How shall we compute this aweighting? In attention we\nweight each prior embedding proportionally to how similar it is to the current token\ni. So the output of attention is a sum of the embeddings of prior tokens weighted\nby their similarity with the current token embedding. We compute similarity scores\nviadot product , which maps two vectors into a scalar value ranging from \u0000\u00a5to\n\u00a5. The larger the score, the more similar the vectors that are being compared. We\u2019ll\nnormalize these scores with a softmax to create the vector of weights ai j;j\u0014i.\nSimpli\ufb01ed Version: score(xi;xj) = xi\u0001xj (9.7)\nai j=softmax (score(xi;xj))8j\u0014i(9.8)\nThus in Fig. 9.3 we compute a3by computing three scores: x3\u0001x1,x3\u0001x2andx3\u0001x3,\nnormalizing them by a softmax, and using the resulting probabilities as weights\nindicating each of their proportional relevance to the current position i. Of course,",
    "metadata": {
      "source": "9",
      "chunk_id": 3,
      "token_count": 725,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\n9.1 \u2022 A TTENTION 5\nthe softmax weight will likely be highest for xi, since xiis very similar to itself,\nresulting in a high dot product. But other context words may also be similar to i, and\nthe softmax will also assign some weight to those words. Then we use these weights\nas the avalues in Eq. 9.6 to compute the weighted sum that is our a3.\nThe simpli\ufb01ed attention in equations 9.6 \u2013 9.8 demonstrates the attention-based\napproach to computing ai: compare the xito prior vectors, normalize those scores\ninto a probability distribution used to weight the sum of the prior vector. But now\nwe\u2019re ready to remove the simpli\ufb01cations.\nA single attention head using query, key, and value matrices Now that we\u2019ve\nseen a simple intuition of attention, let\u2019s introduce the actual attention head , the attention head\nversion of attention that\u2019s used in transformers. (The word head is often used in head\ntransformers to refer to speci\ufb01c structured layers). The attention head allows us to\ndistinctly represent three different roles that each input embedding plays during the\ncourse of the attention process:\n\u2022 As the current element being compared to the preceding inputs. We\u2019ll refer to\nthis role as a query . query\n\u2022 In its role as a preceding input that is being compared to the current element\nto determine a similarity weight. We\u2019ll refer to this role as a key. key\n\u2022 And \ufb01nally, as a value of a preceding element that gets weighted and summed value\nup to compute the output for the current element.\nTo capture these three different roles, transformers introduce weight matrices\nWQ,WK, and WV. These weights will project each input vector xiinto a represen-\ntation of its role as a key, query, or value:\nqi=xiWQ;ki=xiWK;vi=xiWV(9.9)\nGiven these projections, when we are computing the similarity of the current ele-\nment xiwith some prior element xj, we\u2019ll use the dot product between the current\nelement\u2019s query vector qiand the preceding element\u2019s keyvector kj. Furthermore,\nthe result of a dot product can be an arbitrarily large (positive or negative) value, and\nexponentiating large values can lead to numerical issues and loss of gradients during\ntraining. To avoid this, we scale the dot product by a factor related to the size of the\nembeddings, via dividing by the square root of the dimensionality of the query and\nkey vectors ( dk). We thus replace the simpli\ufb01ed Eq. 9.7 with Eq. 9.11. The ensuing\nsoftmax calculation resulting in ai jremains the same, but the output calculation for\nhead iis now based on a weighted sum over the value vectors v(Eq. 9.13).\nHere\u2019s a \ufb01nal set of equations for computing self-attention for a single self-\nattention output vector aifrom a single input vector xi. This version of attention\ncomputes aiby summing the values of the prior elements, each weighted by the\nsimilarity of its keyto the query from the current element:\nqi=xiWQ;kj=xjWK;vj=xjWV(9.10)\nscore(xi;xj) =qi\u0001kjpdk(9.11)\nai j=softmax (score(xi;xj))8j\u0014i (9.12)\nhead i=X\nj\u0014iai jvj (9.13)\nai=head iWO(9.14)",
    "metadata": {
      "source": "9",
      "chunk_id": 4,
      "token_count": 761,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6CHAPTER 9 \u2022 T HETRANSFORMER\n6. Sum the weighted value vectors4. Turn into \ud835\udefci,j weights via softmaxa3\n1. Generate key, query, value vectors2. Compare x3\u2019s query withthe keys for x1, x2, and x38. Output of self-attention\u00d7\u00d7\nx1kqvWKWQWV5. Weigh each value vector\u00f7\u221adk3. Divide scalar score by \u221adk\u00f7\u221adk\u00f7\u221adk\ud835\udefc3,1\ud835\udefc3,2\ud835\udefc3,3\nx2kqvWKWQWVx3kqvWKWQWVWO\n[1 \u00d7 d][1 \u00d7 d][dv \u00d7 d][1 \u00d7 dv][1 \u00d7 dv][1 \u00d7 dv][1 \u00d7 dv]\n[1 \u00d7 dv][1 \u00d7 dv][1 x dv]7. Reshape to [1 x d] \n[1 \u00d7 d][1 \u00d7 d]\nFigure 9.4 Calculating the value of a3, the third element of a sequence using causal (left-\nto-right) self-attention.\nWe illustrate this in Fig. 9.4 for the case of calculating the value of the third output\na3in a sequence.\nNote that we\u2019ve also introduced one more matrix, WO, which is right-multiplied\nby the attention head. This is necessary to reshape the output of the head. The input\nto attention xiand the output from attention aiboth have the same dimensionality\n[1\u0002d]. We often call dthemodel dimensionality , and indeed as we\u2019ll discuss in\nSection 9.2 the output hiof each transformer block, as well as the intermediate vec-\ntors inside the transformer block also have the same dimensionality [1\u0002d]. Having\neverything be the same dimensionality makes the transformer very modular.\nSo let\u2019s talk shapes. How do we get from [1\u0002d]at the input to [1\u0002d]at the\noutput? Let\u2019s look at all the internal shapes. We\u2019ll have a dimension dkfor the key\nand query vectors. The query vector and the key vector are both dimensionality\n1\u0002dk, so we can take their dot product qi\u0001kjto produce a scalar. We\u2019ll have a\nseparate dimension dvfor the value vectors. The transform matrix WQhas shape\n[d\u0002dk],WKis[d\u0002dk], and WVis[d\u0002dv]. So the output of head iin equation\nEq. 9.13 is of shape [1\u0002dv]. To get the desired output shape [1\u0002d]we\u2019ll need to\nreshape the head output, and so WOis of shape [dv\u0002d]. In the original transformer\nwork (Vaswani et al., 2017), dwas 512, dkanddvwere both 64.\nMulti-head Attention Equations 9.11-9.13 describe a single attention head . But\nactually, transformers use multiple attention heads. The intuition is that each head\nmight be attending to the context for different purposes: heads might be special-\nized to represent different linguistic relationships between context elements and the\ncurrent token, or to look for particular kinds of patterns in the context.\nSo in multi-head attention we have Aseparate attention heads that reside inmulti-head\nattention\nparallel layers at the same depth in a model, each with its own set of parameters that\nallows the head to model different aspects of the relationships among inputs. Thus",
    "metadata": {
      "source": "9",
      "chunk_id": 5,
      "token_count": 748,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\n9.1 \u2022 A TTENTION 7\neach head iin a self-attention layer has its own set of key, query and value matrices:\nWKi,WQiandWVi. These are used to project the inputs into separate key, value,\nand query embeddings for each head.\nWhen using multiple heads the model dimension dis still used for the input\nand output, the key and query embeddings have dimensionality dk, and the value\nembeddings are of dimensionality dv(again, in the original transformer paper dk=\ndv=64,A=8, and d=512). Thus for each head i, we have weight layers WQiof\nshape [d\u0002dk],WKiof shape [d\u0002dk], and WViof shape [d\u0002dv].\nBelow are the equations for attention augmented with multiple heads; Fig. 9.5\nshows an intuition.\nqc\ni=xiWQc;kc\nj=xjWKc;vc\nj=xjWVc;8c1\u0014c\u0014A (9.15)\nscorec(xi;xj) =qc\ni\u0001kc\njpdk(9.16)\nac\ni j=softmax (scorec(xi;xj))8j\u0014i (9.17)\nheadc\ni=X\nj\u0014iac\ni jvc\nj (9.18)\nai= (head1\bhead2:::\bheadA)WO(9.19)\nMultiHeadAttention (xi;[x1;\u0001\u0001\u0001;xN]) = ai (9.20)\nThe output of each of the Aheads is of shape 1 \u0002dv, and so the output of the\nmulti-head layer with Aheads consists of Avectors of shape 1 \u0002dv. These are\nconcatenated to produce a single output with dimensionality 1 \u0002hdv. Then we use\nyet another linear projection WO2RAdv\u0002dto reshape it, resulting in the multi-head\nattention vector aiwith the correct output shape [1\u0002d]at each input i.\naixi-1xixi-2xi-3WK1Head 1WV1WQ1\u2026\u2026WK2Head 2WV2WQ2WK8Head 8WV8WQ8aiWO  [hdv x d][1 x dv ][1 x d]\n[1 x d][1 x hdv ]Project down to dConcatenate OutputsEach headattends di\ufb00erentlyto context\u2026[1 x dv ]\nFigure 9.5 The multi-head attention computation for input xi, producing output ai. A multi-head attention\nlayer has Aheads, each with its own key, query and value weight matrices. The outputs from each of the heads\nare concatenated and then projected down to d, thus producing an output of the same size as the input.",
    "metadata": {
      "source": "9",
      "chunk_id": 6,
      "token_count": 603,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8CHAPTER 9 \u2022 T HETRANSFORMER\n9.2 Transformer Blocks\nThe self-attention calculation lies at the core of what\u2019s called a transformer block,\nwhich, in addition to the self-attention layer, includes three other kinds of layers: (1)\na feedforward layer, (2) residual connections, and (3) normalizing layers (colloqui-\nally called \u201clayer norm\u201d).\nLayer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1\n+\u2026\u2026ResidualStream\nFigure 9.6 The architecture of a transformer block showing the residual stream . This\n\ufb01gure shows the prenorm version of the architecture, in which the layer norms happen before\nthe attention and feedforward layers rather than after.\nFig. 9.6 illustrates a transformer block, sketching a common way of thinking\nabout the block that is called the residual stream (Elhage et al., 2021). In the resid- residual stream\nual stream viewpoint, we consider the processing of an individual token ithrough\nthe transformer block as a single stream of d-dimensional representations for token\nposition i. This residual stream starts with the original input vector, and the various\ncomponents read their input from the residual stream and add their output back into\nthe stream.\nThe input at the bottom of the stream is an embedding for a token, which has\ndimensionality d. This initial embedding gets passed up (by residual connections ),\nand is progressively added to by the other components of the transformer: the at-\ntention layer that we have seen, and the feedforward layer that we will introduce.\nBefore the attention and feedforward layer is a computation called the layer norm .\nThus the initial vector is passed through a layer norm and attention layer, and\nthe result is added back into the stream, in this case to the original input vector\nxi. And then this summed vector is again passed through another layer norm and a\nfeedforward layer, and the output of those is added back into the residual, and we\u2019ll\nusehito refer to the resulting output of the transformer block for token i. (In earlier\ndescriptions the residual stream was often described using a different metaphor as\nresidual connections that add the input of a component to its output, but the residual\nstream is a more perspicuous way of visualizing the transformer.)\nWe\u2019ve already seen the attention layer, so let\u2019s now introduce the feedforward",
    "metadata": {
      "source": "9",
      "chunk_id": 7,
      "token_count": 520,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\n9.2 \u2022 T RANSFORMER BLOCKS 9\nand layer norm computations in the context of processing a single input xiat token\nposition i.\nFeedforward layer The feedforward layer is a fully-connected 2-layer network,\ni.e., one hidden layer, two weight matrices, as introduced in Chapter 7. The weights\nare the same for each token position i, but are different from layer to layer. It\nis common to make the dimensionality dffof the hidden layer of the feedforward\nnetwork be larger than the model dimensionality d. (For example in the original\ntransformer model, d=512 and dff=2048.)\nFFN(xi) =ReLU (xiW1+b1)W2+b2 (9.21)\nLayer Norm At two stages in the transformer block we normalize the vector (Ba\net al., 2016). This process, called layer norm (short for layer normalization), is one layer norm\nof many forms of normalization that can be used to improve training performance\nin deep neural networks by keeping the values of a hidden layer in a range that\nfacilitates gradient-based training.\nLayer norm is a variation of the z-score from statistics, applied to a single vec-\ntor in a hidden layer. That is, the term layer norm is a bit confusing; layer norm\nisnotapplied to an entire transformer layer, but just to the embedding vector of a\nsingle token. Thus the input to layer norm is a single vector of dimensionality d\nand the output is that vector normalized, again of dimensionality d. The \ufb01rst step in\nlayer normalization is to calculate the mean, m, and standard deviation, s, over the\nelements of the vector to be normalized. Given an embedding vector xof dimen-\nsionality d, these values are calculated as follows.\nm=1\nddX\ni=1xi (9.22)\ns=vuut1\nddX\ni=1(xi\u0000m)2(9.23)\nGiven these values, the vector components are normalized by subtracting the mean\nfrom each and dividing by the standard deviation. The result of this computation is\na new vector with zero mean and a standard deviation of one.\n^ x=(x\u0000m)\ns(9.24)\nFinally, in the standard implementation of layer normalization, two learnable param-\neters, gandb, representing gain and offset values, are introduced.\nLayerNorm (x) =g(x\u0000m)\ns+b (9.25)\nPutting it all together The function computed by a transformer block can be ex-\npressed by breaking it down with one equation for each component computation,\nusing t(of shape [1\u0002d]) to stand for transformer and superscripts to demarcate",
    "metadata": {
      "source": "9",
      "chunk_id": 8,
      "token_count": 579,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 CHAPTER 9 \u2022 T HETRANSFORMER\neach computation inside the block:\nt1\ni=LayerNorm (xi) (9.26)\nt2\ni=MultiHeadAttention (t1\ni;\u0002\nt1\n1;\u0001\u0001\u0001;t1\nN\u0003\n) (9.27)\nt3\ni=t2\ni+xi (9.28)\nt4\ni=LayerNorm (t3\ni) (9.29)\nt5\ni=FFN(t4\ni) (9.30)\nhi=t5\ni+t3\ni (9.31)\nNotice that the only component that takes as input information from other tokens\n(other residual streams) is multi-head attention, which (as we see from Eq. 9.28)\nlooks at all the neighboring tokens in the context. The output from attention, how-\never, is then added into this token\u2019s embedding stream. In fact, Elhage et al. (2021)\nshow that we can view attention heads as literally moving information from the\nresidual stream of a neighboring token into the current stream. The high-dimensional\nembedding space at each position thus contains information about the current to-\nken and about neighboring tokens, albeit in different subspaces of the vector space.\nFig. 9.7 shows a visualization of this movement.\nToken Aresidual streamToken Bresidual stream\nFigure 9.7 An attention head can move information from token A\u2019s residual stream into\ntoken B\u2019s residual stream.\nCrucially, the input and output dimensions of transformer blocks are matched so\nthey can be stacked. Each token vector xiat the input to the block has dimensionality\nd, and the output hialso has dimensionality d. Transformers for large language\nmodels stack many of these blocks, from 12 layers (used for the T5 or GPT-3-small\nlanguage models) to 96 layers (used for GPT-3 large), to even more for more recent\nmodels. We\u2019ll come back to this issue of stacking in a bit.\nEquation 9.28 and following are just the equation for a single transformer block,\nbut the residual stream metaphor goes through all the transformer layers, from the\n\ufb01rst transformer blocks to the 12th, in a 12-layer transformer. At the earlier trans-\nformer blocks, the residual stream is representing the current token. At the highest\ntransformer blocks, the residual stream is usually representing the following token,\nsince at the very end it\u2019s being trained to predict the next token.\nOnce we stack many blocks, there is one more requirement: at the very end of\nthe last (highest) transformer block, there is a single extra layer norm that is run on\nthe last hiof each token stream (just below the language model head layer that we\nwill de\ufb01ne soon).3\n3Note that we are using the most common current transformer architecture, which is called the prenorm",
    "metadata": {
      "source": "9",
      "chunk_id": 9,
      "token_count": 627,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\n9.3 \u2022 P ARALLELIZING COMPUTATION USING A SINGLE MATRIX X 11\n9.3 Parallelizing computation using a single matrix X\nThis description of multi-head attention and the rest of the transformer block has\nbeen from the perspective of computing a single output at a single time step iin\na single residual stream. But as we pointed out earlier, the attention computation\nperformed for each token to compute aiis independent of the computation for each\nother token, and that\u2019s also true for all the computation in the transformer block\ncomputing hifrom the input xi. That means we can easily parallelize the entire\ncomputation, taking advantage of ef\ufb01cient matrix multiplication routines.\nWe do this by packing the input embeddings for the Ntokens of the input se-\nquence into a single matrix Xof size [N\u0002d]. Each row of Xis the embedding of\none token of the input. Transformers for large language models commonly have an\ninput length Nfrom 1K to 32K; much longer contexts of 128K or even up to millions\nof tokens can also be achieved with architectural changes like special long-context\nmechanisms that we don\u2019t discuss here. So for vanilla transformers, we can think of\nXhaving between 1K and 32K rows, each of the dimensionality of the embedding\nd(the model dimension).\nParallelizing attention Let\u2019s \ufb01rst see this for a single attention head and then turn\nto multiple heads, and then add in the rest of the components in the transformer\nblock. For one head we multiply Xby the key, query, and value matrices WQof\nshape [d\u0002dk],WKof shape [d\u0002dk], andWVof shape [d\u0002dv], to produce matrices\nQof shape [N\u0002dk],Kof shape [N\u0002dk], and Vof shape [N\u0002dv], containing all the\nkey, query, and value vectors:\nQ=XWQ;K=XWK;V=XWV(9.32)\nGiven these matrices we can compute all the requisite query-key comparisons simul-\ntaneously by multiplying QandK|in a single matrix multiplication. The product is\nof shape N\u0002N, visualized in Fig. 9.8.\nq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NNq1\u2022k2q1\u2022k3q1\u2022k4q2\u2022k3q2\u2022k4q3\u2022k4\nFigure 9.8 The N\u0002NQK|matrix showing how it computes all qi\u0001kjcomparisons in a\nsingle matrix multiple.\nOnce we have this QK|matrix, we can very ef\ufb01ciently scale these scores, take\nthe softmax, and then multiply the result by Vresulting in a matrix of shape N\u0002d:\na vector embedding representation for each token in the input. We\u2019ve reduced the\nentire self-attention step for an entire sequence of Ntokens for one head to the\narchitecture. The original de\ufb01nition of the transformer in Vaswani et al. (2017) used an alternative archi-\ntecture called the postnorm transformer in which the layer norm happens after the attention and FFN\nlayers; it turns out moving the layer norm beforehand works better, but does require this one extra layer\nat the end.",
    "metadata": {
      "source": "9",
      "chunk_id": 10,
      "token_count": 743,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 CHAPTER 9 \u2022 T HETRANSFORMER\nfollowing computation:\nhead =softmax\u0012\nmask\u0012QK|\npdk\u0013\u0013\nV\nA=head WO(9.33)\nMasking out the future You may have noticed that we introduced a mask function\nin Eq. 9.33 above. This is because the self-attention computation as we\u2019ve described\nit has a problem: the calculation of QK|results in a score for each query value to\nevery key value, including those that follow the query . This is inappropriate in the\nsetting of language modeling: guessing the next word is pretty simple if you already\nknow it! To \ufb01x this, the elements in the upper-triangular portion of the matrix are set\nto\u0000\u00a5, which the softmax will turn to zero, thus eliminating any knowledge of words\nthat follow in the sequence. This is done in practice by adding a mask matrix Min\nwhich Mi j=\u0000\u00a58j>i(i.e. for the upper-triangular portion) and Mi j=0 otherwise.\nFig. 9.9 shows the resulting masked QK|matrix. (we\u2019ll see in Chapter 11 how to\nmake use of words in the future for tasks that need it).\nq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NN\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\nFigure 9.9 TheN\u0002NQK|matrix showing the qi\u0001kjvalues, with the upper-triangle por-\ntion of the comparisons matrix zeroed out (set to \u0000\u00a5, which the softmax will turn to zero).\nFig. 9.10 shows a schematic of all the computations for a single attention head\nparallelized in matrix form.\nFig. 9.8 and Fig. 9.9 also make it clear that attention is quadratic in the length\nof the input, since at each layer we need to compute dot products between each pair\nof tokens in the input. This makes it expensive to compute attention over very long\ndocuments (like entire novels). Nonetheless modern large language models manage\nto use quite long contexts of thousands or tens of thousands of tokens.\nParallelizing multi-head attention In multi-head attention, as with self-attention,\nthe input and output have the model dimension d, the key and query embeddings\nhave dimensionality dk, and the value embeddings are of dimensionality dv(again,\nin the original transformer paper dk=dv=64,A=8, and d=512). Thus for\neach head i, we have weight layers WQiof shape [d\u0002dk],WKiof shape [d\u0002dk],\nandWViof shape [d\u0002dv], and these get multiplied by the inputs packed into X\nto produce Qof shape [N\u0002dk],Kof shape [N\u0002dk], and Vof shape [N\u0002dv].\nThe output of each of the Aheads is of shape N\u0002dv, and so the output of the\nmulti-head layer with Aheads consists of Amatrices of shape N\u0002dv. To make use\nof these matrices in further processing, they are concatenated to produce a single\noutput with dimensionality N\u0002hdv. Finally, we use a \ufb01nal linear projection WO\nof shape [Adv\u0002d], that reshape it to the original output dimension for each token.\nMultiplying the concatenated N\u0002hdvmatrix output by WOof shape [Adv\u0002d]yields",
    "metadata": {
      "source": "9",
      "chunk_id": 11,
      "token_count": 766,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\n9.3 \u2022 P ARALLELIZING COMPUTATION USING A SINGLE MATRIX X 13\nq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2\u2022k2q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k2q3\u2022k3\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221eq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3q1\u2022k2q2\u2022k3q1\u2022k3q3\u2022k4q2\u2022k4q1\u2022k4x=QKT maskedmask=q1\u2022k1q2\u2022k1q4\u2022k1q3\u2022k1q1\u2022k1q1\u2022k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dv\nFigure 9.10 Schematic of the attention computation for a single attention head in parallel. The \ufb01rst row shows\nthe computation of the Q,K, and Vmatrices. The second row shows the computation of QKT, the masking\n(the softmax computation and the normalizing by dimensionality are not shown) and then the weighted sum of\nthe value vectors to get the \ufb01nal attention vectors.\nthe self-attention output Aof shape [ N\u0002d].\nQi=XWQi;Ki=XWKi;Vi=XWVi(9.34)\nhead i=SelfAttention (Qi;Ki;Vi) = softmax\u0012QiKi|\npdk\u0013\nVi(9.35)\nMultiHeadAttention (X) = ( head 1\bhead 2:::\bhead A)WO(9.36)\nPutting it all together with the parallel input matrix XThe function computed\nin parallel by an entire layer of Ntransformer block over the entire Ninput tokens\ncan be expressed as:\nO=X+MultiHeadAttention (LayerNorm (X)) (9.37)\nH=O+FFN(LayerNorm (O)) (9.38)\nNote that in Eq. 9.37 we are using Xto mean the input to the layer, wherever it\ncomes from. For the \ufb01rst layer, as we will see in the next section, that input is the\ninitital word + positional embedding vectors that we have been describing by X. But\nfor subsequent layers k, the input is the output from the previous layer Hk\u00001. We\ncan also break down the computation performed in a transformer layer, showing one\nequation for each component computation. We\u2019ll use T(of shape [N\u0002d]) to stand\nfor transformer and superscripts to demarcate each computation inside the block,\nand again use Xto mean the input to the block from the previous layer or the initial",
    "metadata": {
      "source": "9",
      "chunk_id": 12,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n14 CHAPTER 9 \u2022 T HETRANSFORMER\nembedding:\nT1=LayerNorm (X) (9.39)\nT2=MultiHeadAttention (T1) (9.40)\nT3=T2+X (9.41)\nT4=LayerNorm (T3) (9.42)\nT5=FFN(T4) (9.43)\nH=T5+T3(9.44)\nHere when we use a notation like FFN (T3)we mean that the same FFN is applied\nin parallel to each of the Nembedding vectors in the window. Similarly, each of the\nNtokens is normed in parallel in the LayerNorm. Crucially, the input and output\ndimensions of transformer blocks are matched so they can be stacked. Since each\ntoken xiat the input to the block is represented by an embedding of dimensionality\n[1\u0002d], that means the input Xand output Hare both of shape [N\u0002d].\n9.4 The input: embeddings for token and position\nLet\u2019s talk about where the input Xcomes from. Given a sequence of Ntokens ( Nis\nthe context length in tokens), the matrix Xof shape [N\u0002d]has an embedding for embedding\neach word in the context. The transformer does this by separately computing two\nembeddings: an input token embedding, and an input positional embedding.\nA token embedding, introduced in Chapter 7 and Chapter 8, is a vector of di-\nmension dthat will be our initial representation for the input token. (As we pass\nvectors up through the transformer layers in the residual stream, this embedding\nrepresentation will change and grow, incorporating context and playing a different\nrole depending on the kind of language model we are building.) The set of initial\nembeddings are stored in the embedding matrix E, which has a row for each of the\njVjtokens in the vocabulary. Thus each word is a row vector of ddimensions, and\nEhas shape [jVj\u0002d].\nGiven an input token string like Thanks for all the we \ufb01rst convert the tokens\ninto vocabulary indices (these were created when we \ufb01rst tokenized the input using\nBPE or SentencePiece). So the representation of thanks for all the might be w=\n[5;4000;10532;2224]. Next we use indexing to select the corresponding rows from\nE, (row 5, row 4000, row 10532, row 2224).\nAnother way to think about selecting token embeddings from the embedding\nmatrix is to represent tokens as one-hot vectors of shape [1\u0002jVj], i.e., with one\ndimension for each word in the vocabulary. Recall that in a one-hot vector all the one-hot vector\nelements are 0 except one, the element whose dimension is the word\u2019s index in the\nvocabulary, which has value 1. So if the word \u201cthanks\u201d has index 5 in the vocabulary,\nx5=1, and xi=08i6=5, as shown here:\n[0 0 0 0 1 0 0 ... 0 0 0 0]\n1 2 3 4 5 6 7 ... ... |V|\nMultiplying by a one-hot vector that has only one non-zero element xi=1 simply\nselects out the relevant row vector for word i, resulting in the embedding for word i,\nas depicted in Fig. 9.11.",
    "metadata": {
      "source": "9",
      "chunk_id": 13,
      "token_count": 738,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\n9.4 \u2022 T HE INPUT :EMBEDDINGS FOR TOKEN AND POSITION 15\nE|V|d1|V|d=\u2715550 0 0 0 1 0 0 \u2026 0 0 0 0 1\nFigure 9.11 Selecting the embedding vector for word V5by multiplying the embedding\nmatrix Ewith a one-hot vector with a 1 in index 5.\nWe can extend this idea to represent the entire token sequence as a matrix of one-\nhot vectors, one for each of the Npositions in the transformer\u2019s context window, as\nshown in Fig. 9.12.\nE|V|ddN=\u2715|V|N0 0 0 0 0 0 0 \u2026 0 0 1 0 0 0 0 0 1 0 0 \u2026 0 0 0 0 1 0 0 0 0 0 0 \u2026 0 0 0 0 0 0 0 0 1 0 0 \u2026 0 0 0 0 \u2026\nFigure 9.12 Selecting the embedding matrix for the input sequence of token ids Wby mul-\ntiplying a one-hot matrix corresponding to Wby the embedding matrix E.\nThese token embeddings are not position-dependent. To represent the position\nof each token in the sequence, we combine these token embeddings with positional\nembeddings speci\ufb01c to each position in an input sequence.positional\nembeddings\nWhere do we get these positional embeddings? The simplest method, called\nabsolute position , is to start with randomly initialized embeddings correspondingabsolute\nposition\nto each possible input position up to some maximum length. For example, just as\nwe have an embedding for the word \ufb01sh, we\u2019ll have an embedding for the position 3.\nAs with word embeddings, these positional embeddings are learned along with other\nparameters during training. We can store them in a matrix Eposof shape [N\u0002d].\nTo produce an input embedding that captures positional information, we just\nadd the word embedding for each input to its corresponding positional embedding.\nThe individual token and position embeddings are both of size [1\u0002d], so their sum is\nalso[1\u0002d], This new embedding serves as the input for further processing. Fig. 9.13\nshows the idea.\nX = CompositeEmbeddings(word + position)Transformer BlockJanet1will2back3Janetwillbackthebillthe4bill5\n+++++PositionEmbeddingsWordEmbeddings\nFigure 9.13 A simple way to model position: add an embedding of the absolute position to\nthe token embedding to produce a new embedding of the same dimensionality.",
    "metadata": {
      "source": "9",
      "chunk_id": 14,
      "token_count": 581,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\n16 CHAPTER 9 \u2022 T HETRANSFORMER\nThe \ufb01nal representation of the input, the matrix X, is an [N\u0002d]matrix in which\neach row iis the representation of the ith token in the input, computed by adding\nE[id(i)]\u2014the embedding of the id of the token that occurred at position i\u2014, to P[i],\nthe positional embedding of position i.\nA potential problem with the simple position embedding approach is that there\nwill be plenty of training examples for the initial positions in our inputs and corre-\nspondingly fewer at the outer length limits. These latter embeddings may be poorly\ntrained and may not generalize well during testing. An alternative is to choose a\nstatic function that maps integer inputs to real-valued vectors in a way that better\nhandle sequences of arbitrary length. A combination of sine and cosine functions\nwith differing frequencies was used in the original transformer work. Sinusoidal po-\nsition embeddings may also help in capturing the inherent relationships among the\npositions, like the fact that position 4 in an input is more closely related to position\n5 than it is to position 17.\nA more complex style of positional embedding methods extend this idea of cap-\nturing relationships even further to directly represent relative position instead ofrelative\nposition\nabsolute position, often implemented in the attention mechanism at each layer rather\nthan being added once at the initial input.\n9.5 The Language Modeling Head\nThe last component of the transformer we must introduce is the language modeling\nhead . Here we are using the word head to mean the additional neural circuitry welanguage\nmodeling head\nhead add on top of the basic transformer architecture when we apply pretrained trans-\nformer models to various tasks. The language modeling head is the circuitry we\nneed to do language modeling.\nRecall that language models, from the simple n-gram models of Chapter 3 through\nthe feedforward and RNN language models of Chapter 7 and Chapter 8, are word\npredictors. Given a context of words, they assign a probability to each possible next\nword. For example, if the preceding context is \u201cThanks for all the\u201d and we want to\nknow how likely the next word is \u201c\ufb01sh\u201d we would compute:\nP(\ufb01shjThanks for all the )\nLanguage models give us the ability to assign such a conditional probability to every\npossible next word, giving us a distribution over the entire vocabulary. The n-gram\nlanguage models of Chapter 3 compute the probability of a word given counts of\nits occurrence with the n\u00001 prior words. The context is thus of size n\u00001. For\ntransformer language models, the context is the size of the transformer\u2019s context\nwindow, which can be quite large, like 32K tokens for large models (and much larger\ncontexts of millions of words are possible with special long-context architectures).\nThe job of the language modeling head is to take the output of the \ufb01nal trans-\nformer layer from the last token Nand use it to predict the upcoming word at posi-\ntionN+1. Fig. 9.14 shows how to accomplish this task, taking the output of the last\ntoken at the last layer (the d-dimensional output embedding of shape [1\u0002d]) and\nproducing a probability distribution over words (from which we will choose one to\ngenerate).\nThe \ufb01rst module in Fig. 9.14 is a linear layer, whose job is to project from the\noutput hL\nN, which represents the output token embedding at position Nfrom the \ufb01nal",
    "metadata": {
      "source": "9",
      "chunk_id": 15,
      "token_count": 748,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\n9.5 \u2022 T HELANGUAGE MODELING HEAD 17\nLayer LTransformerBlockSoftmax over vocabulary VUnembedding layer\u20261 x |V|Logits Word probabilities1 x |V|hL1w1w2wNhL2hLNd x |V|1 x d   Unembedding layerU = ETy1y2y|V|\u2026u1u2u|V|\u2026Language Model Headtakes hLN and outputs adistribution over vocabulary V\nFigure 9.14 The language modeling head: the circuit at the top of a transformer that maps from the output\nembedding for token Nfrom the last transformer layer ( hL\nN) to a probability distribution over words in the\nvocabulary V.\nblock L, (hence of shape [1\u0002d]) to the logit vector, or score vector, that will have a logit\nsingle score for each of the jVjpossible words in the vocabulary V. The logit vector\nuis thus of dimensionality 1 \u0002jVj.\nThis linear layer can be learned, but more commonly we tie this matrix to (the\ntranspose of) the embedding matrix E. Recall that in weight tying , we use the weight tying\nsame weights for two different matrices in the model. Thus at the input stage of the\ntransformer the embedding matrix (of shape [jVj\u0002d]) is used to map from a one-hot\nvector over the vocabulary (of shape [1\u0002jVj]) to an embedding (of shape [1\u0002d]).\nAnd then in the language model head, ET, the transpose of the embedding matrix (of\nshape [d\u0002jVj]) is used to map back from an embedding (shape [1\u0002d]) to a vector\nover the vocabulary (shape [1 \u0002jVj]). In the learning process, Ewill be optimized to\nbe good at doing both of these mappings. We therefore sometimes call the transpose\nETtheunembedding layer because it is performing this reverse mapping. unembedding\nA softmax layer turns the logits uinto the probabilities yover the vocabulary.\nu=hL\nNET(9.45)\ny=softmax (u) (9.46)\nWe can use these probabilities to do things like help assign a probability to a\ngiven text. But the most important usage to generate text, which we do by sampling\na word from these probabilities y. We might sample the highest probability word\n(\u2018greedy\u2019 decoding), or use another of the sampling methods we\u2019ll introduce in Sec-\ntion??. In either case, whatever entry ykwe choose from the probability vector y,\nwe generate the word that has that index k.\nFig. 9.15 shows the total stacked architecture for one token i. Note that the input\nto each transformer layer x`\niis the same as the output from the preceding layer h`\u00001\ni.\nNow that we see all these transformer layers spread out on the page, we can point\nout another useful feature of the unembedding layer: as a tool for interpretability of\nthe internals of the transformer that we call the logit lens (Nostalgebraist, 2020). logit lens\nWe can take a vector from any layer of the transformer and, pretending that it is\nthe pre\ufb01nal embedding, simply multiply it by the unembedding layer to get logits,\nand compute a softmax to see the distribution over words that that vector might\nbe representing. This can be a useful window into the internal representations of",
    "metadata": {
      "source": "9",
      "chunk_id": 16,
      "token_count": 735,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\n18 CHAPTER 9 \u2022 T HETRANSFORMER\nwiSample token togenerate at position i+1\nfeedforwardlayer normattentionlayer norm\nU\nInput tokenLanguageModelingHead\nInputEncoding\nEi+\u2026logits\nfeedforwardlayer normattentionlayer normLayer 1Layer 2h1i  =  x2ix1ih2i  =  x3ifeedforwardlayer normattentionlayer normhLi  hL-1i  =  xLiy1y2y|V|\u2026Token probabilitiesu1u2u|V|\u2026softmaxwi+1\nLayer L\nFigure 9.15 A transformer language model (decoder-only), stacking transformer blocks\nand mapping from an input token wito to a predicted next token wi+1.\nthe model. Since the network wasn\u2019t trained to make the internal representations\nfunction in this way, the logit lens doesn\u2019t always work perfectly, but this can still\nbe a useful trick.\nA terminological note before we conclude: You will sometimes see a trans-\nformer used for this kind of unidirectional causal language model called a decoder-\nonly model . This is because this model constitutes roughly half of the encoder-decoder-only\nmodel\ndecoder model for transformers that we\u2019ll see how to apply to machine translation\nin Chapter 13. (Confusingly, the original introduction of the transformer had an\nencoder-decoder architecture, and it was only later that the standard paradigm for\ncausal language model was de\ufb01ned by using only the decoder part of this original\narchitecture).\n9.6 Summary\nThis chapter has introduced the transformer and its components for the task of lan-\nguage modeling. We\u2019ll continue the task of language modeling including issues like\ntraining and sampling in the next chapter.",
    "metadata": {
      "source": "9",
      "chunk_id": 17,
      "token_count": 376,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\nBIBLIOGRAPHICAL AND HISTORICAL NOTES 19\nHere\u2019s a summary of the main points that we covered:\n\u2022 Transformers are non-recurrent networks based on multi-head attention , a\nkind of self-attention . A multi-head attention computation takes an input\nvector xiand maps it to an output aiby adding in vectors from prior tokens,\nweighted by how relevant they are for the processing of the current word.\n\u2022 Atransformer block consists of a residual stream in which the input from\nthe prior layer is passed up to the next layer, with the output of different com-\nponents added to it. These components include a multi-head attention layer\nfollowed by a feedforward layer , each preceded by layer normalizations .\nTransformer blocks are stacked to make deeper and more powerful networks.\n\u2022 The input to a transformer is computed by adding an embedding (computed\nwith an embedding matrix ) to a positional encoding that represents the se-\nquential position of the token in the window.\n\u2022 Language models can be built out of stacks of transformer blocks, with a\nlanguage model head at the top, which applies an unembedding matrix to\nthe output Hof the top layer to generate the logits , which are then passed\nthrough a softmax to generate word probabilities.\n\u2022 Transformer-based language models have a wide context window (200K to-\nkens or even more for very large models with special mechanisms) allowing\nthem to draw on enormous amounts of context to predict upcoming words.\nBibliographical and Historical Notes\nThe transformer (Vaswani et al., 2017) was developed drawing on two lines of prior\nresearch: self-attention andmemory networks .\nEncoder-decoder attention, the idea of using a soft weighting over the encodings\nof input words to inform a generative decoder (see Chapter 13) was developed by\nGraves (2013) in the context of handwriting generation, and Bahdanau et al. (2015)\nfor MT. This idea was extended to self-attention by dropping the need for separate\nencoding and decoding sequences and instead seeing attention as a way of weighting\nthe tokens in collecting information passed from lower layers to higher layers (Ling\net al., 2015; Cheng et al., 2016; Liu et al., 2016).\nOther aspects of the transformer, including the terminology of key, query, and\nvalue, came from memory networks , a mechanism for adding an external read-\nwrite memory to networks, by using an embedding of a query to match keys rep-\nresenting content in an associative memory (Sukhbaatar et al., 2015; Weston et al.,\n2015; Graves et al., 2014).\nMORE HISTORY TBD IN NEXT DRAFT.",
    "metadata": {
      "source": "9",
      "chunk_id": 18,
      "token_count": 566,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\n20 Chapter 9 \u2022 The Transformer\nBa, J. L., J. R. Kiros, and G. E. Hinton. 2016. Layer normal-\nization. NeurIPS workshop .\nBahdanau, D., K. H. Cho, and Y . Bengio. 2015. Neural ma-\nchine translation by jointly learning to align and translate.\nICLR 2015 .\nCheng, J., L. Dong, and M. Lapata. 2016. Long short-term\nmemory-networks for machine reading. EMNLP .\nElhage, N., N. Nanda, C. Olsson, T. Henighan, N. Joseph,\nB. Mann, A. Askell, Y . Bai, A. Chen, T. Conerly, N. Das-\nSarma, D. Drain, D. Ganguli, Z. Hat\ufb01eld-Dodds, D. Her-\nnandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse,\nD. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCan-\ndlish, and C. Olah. 2021. A mathematical framework for\ntransformer circuits. White paper.\nGraves, A. 2013. Generating sequences with recurrent neural\nnetworks. ArXiv.\nGraves, A., G. Wayne, and I. Danihelka. 2014. Neural Tur-\ning machines. ArXiv.\nLing, W., C. Dyer, A. W. Black, I. Trancoso, R. Fermandez,\nS. Amir, L. Marujo, and T. Lu \u00b4\u0131s. 2015. Finding function\nin form: Compositional character models for open vocab-\nulary word representation. EMNLP .\nLiu, Y ., C. Sun, L. Lin, and X. Wang. 2016. Learning natural\nlanguage inference using bidirectional LSTM model and\ninner-attention. ArXiv.\nNostalgebraist. 2020. Interpreting gpt: the logit lens. White\npaper.\nSukhbaatar, S., A. Szlam, J. Weston, and R. Fergus. 2015.\nEnd-to-end memory networks. NeurIPS .\nUszkoreit, J. 2017. Transformer: A novel neural network ar-\nchitecture for language understanding. Google Research\nblog post, Thursday August 31, 2017.\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, \u0141. Kaiser, and I. Polosukhin. 2017. Atten-\ntion is all you need. NeurIPS .\nWeston, J., S. Chopra, and A. Bordes. 2015. Memory net-\nworks. ICLR 2015 .",
    "metadata": {
      "source": "9",
      "chunk_id": 19,
      "token_count": 638,
      "chapter_title": ""
    }
  }
]