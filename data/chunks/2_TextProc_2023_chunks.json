[
  {
    "content": "# 2_TextProc_2023\n\n## Page 1\n\nBasic Text ProcessingRegular Expressions\n\n## Page 2\n\nRegular expressions are used everywhere\u25e6Part of every text processing task\u25e6Not a general NLP solution (for that we use large NLP systems we will see in later lectures)\u25e6But very useful as part of those systems (e.g., for pre-processing or text formatting)\u25e6Necessary for data analysis of text data\u25e6A widely used tool in industry and academics\n2\n\n## Page 3\n\nRegular expressionsA formal language for specifying text stringsHow can we search for mentions of these cute animals in text?\u25e6woodchuck\u25e6woodchucks\u25e6Woodchuck\u25e6Woodchucks\u25e6Groundhog\u25e6groundhogs\n\n## Page 4\n\nRegular Expressions: DisjunctionsLetters inside square brackets []Ranges using the dash [A-Z]PatternMatches[wW]oodchuckWoodchuck,woodchuck[1234567890]Any one digitPatternMatches[A-Z]An upper case letterDrenched Blossoms[a-z]A lower case lettermy beans were impatient[0-9]A singledigitChapter 1: Down the Rabbit Hole\n\n## Page 5\n\nRegular Expressions: Negation in DisjunctionCarat as first character in [] negates the list\u25e6Note: Carat means negation only when it's first in []\u25e6Special characters (., *, +, ?) lose their special meaning inside []PatternMatchesExamples[^A-Z]Notan upper case letterOyfnpripetchik[^Ss]Neither \u2018S\u2019 nor \u2018s\u2019Ihave no exquisite reason\u201d[^.]Not a periodOur resident Djinn[e^]Either e or ^Look up ^now\n\n## Page 6\n\nRegular Expressions: Convenient aliasesPatternExpansionMatchesExamples\\d[0-9]Any digitFahreneit451\\D[^0-9]Any non-digitBlue Moon\\w[a-ZA-Z0-9_]Any alphanumeric or _Daiyu\\W[^\\w]Not alphanumeric or _Look!\\s[ \\r\\t\\n\\f]Whitespace (space, tab)Look\u2423up\\S[^\\s]Not whitespaceLook up\n\n## Page 7\n\nRegular Expressions: More DisjunctionGroundhog is another name for woodchuck!The pipe symbol | for disjunctionPatternMatchesgroundhog|woodchuckwoodchuckyours|mineyoursa|b|c= [abc][gG]roundhog|[Ww]oodchuckWoodchuck\n\n## Page 8\n\nWildcards, optionality, repetition: . ?* +\nStephen C KleenePatternMatchesExamplesbeg.nAny charbeginbegun beg3nbeg nwoodchucks?Optional swoodchuck woodchucksto*0 or more of previous charttotootoooto+1 or more of previous chartotootoootooooKleene *,   Kleene +   \n\n## Page 9\n\nRegular Expressions: Anchors  ^   $PatternMatches^[A-Z] PaloAlto^[^A-Za-z] 1\u201cHello\u201d\\.$The end..$The end?The end!\n\n## Page 10\n\nA note about Python regular expressions\u25e6Regex and Python both use backslash \"\\\" for special characters. You must type extra backslashes!\u25e6\"\\\\d+\"to search for 1 or more digits\u25e6\"\\n\" in Python means the \"newline\" character, not a \"slash\" followed by an \"n\". Need \"\\\\n\"for two characters.\u25e6Instead: use Python's raw string notation for regex:\u25e6r\"[tT]he\"\u25e6r\"\\d+\"matches one or more digits\u25e6instead of \"\\\\d+\"10",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 0,
      "token_count": 786,
      "chapter_title": "2_TextProc_2023"
    }
  },
  {
    "content": "## Page 8\n\nWildcards, optionality, repetition: . ?* +\nStephen C KleenePatternMatchesExamplesbeg.nAny charbeginbegun beg3nbeg nwoodchucks?Optional swoodchuck woodchucksto*0 or more of previous charttotootoooto+1 or more of previous chartotootoootooooKleene *,   Kleene +   \n\n## Page 9\n\nRegular Expressions: Anchors  ^   $PatternMatches^[A-Z] PaloAlto^[^A-Za-z] 1\u201cHello\u201d\\.$The end..$The end?The end!\n\n## Page 10\n\nA note about Python regular expressions\u25e6Regex and Python both use backslash \"\\\" for special characters. You must type extra backslashes!\u25e6\"\\\\d+\"to search for 1 or more digits\u25e6\"\\n\" in Python means the \"newline\" character, not a \"slash\" followed by an \"n\". Need \"\\\\n\"for two characters.\u25e6Instead: use Python's raw string notation for regex:\u25e6r\"[tT]he\"\u25e6r\"\\d+\"matches one or more digits\u25e6instead of \"\\\\d+\"10\n\n## Page 11\n\nThe iterative process of writing regex'sFind me all instances of the word \u201cthe\u201d in a text.theMisses capitalized examples[tT]heIncorrectly returns otheror Theology\\W[tT]he\\W\n\n## Page 12\n\nFalse positives and false negativesThe process we just went through was based on fixing two kinds of errors:1.Not matching things that we should have matched (The)False negatives2.Matching strings that we should not have matched (there, then, other)False positives\n\n## Page 13\n\nCharacterizing work on NLPIn NLP we are always dealing with these kinds of errors.Reducing the error rate for an application often involves two antagonistic efforts: \u25e6Increasing coverage (or recall) (minimizing false negatives).\u25e6Increasing accuracy (or precision) (minimizing false positives)\n\n## Page 14\n\nRegular expressions play a surprisingly large roleWidely used in both academics and industry1.Part of most text processing tasks, even for big neural language model pipelines\u25e6including text formatting and pre-processing2.Very useful for data analysis of any text data\n14\n\n## Page 15\n\nBasic Text ProcessingRegular Expressions\n\n## Page 16\n\nBasic Text ProcessingMore Regular Expressions: Substitutions and ELIZA\n\n## Page 17\n\nSubstitutionsSubstitution in Python and UNIX commands:s/regexp1/pattern/ e.g.:s/colour/color/ \n\n## Page 18\n\nCapture Groups\u2022Say we want to put angles around all numbers:the 35 boxes\u00e0the <35> boxes\u2022Use parens() to \"capture\" a pattern into a numbered register (1, 2, 3\u2026)\u2022Use \\1  to refer to the contents of the registers/([0-9]+)/<\\1>/ \n\n## Page 19\n\nCapture groups: multiple registers/the (.*)er they (.*), the \\1er we \\2/ Matchesthe faster they ran, the faster we ranBut notthe faster they ran, the faster we ate \n\n## Page 20\n\nBut suppose we don't want to capture?Parentheses have a double function: grouping terms, and capturingNon-capturing groups: add a ?: after paren:/(?:some|afew) (people|cats) like some \\1/ matches \u25e6some cats like some cats but not \u25e6some cats like some some",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 1,
      "token_count": 744,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\nBasic Text ProcessingRegular Expressions\n\n## Page 16\n\nBasic Text ProcessingMore Regular Expressions: Substitutions and ELIZA\n\n## Page 17\n\nSubstitutionsSubstitution in Python and UNIX commands:s/regexp1/pattern/ e.g.:s/colour/color/ \n\n## Page 18\n\nCapture Groups\u2022Say we want to put angles around all numbers:the 35 boxes\u00e0the <35> boxes\u2022Use parens() to \"capture\" a pattern into a numbered register (1, 2, 3\u2026)\u2022Use \\1  to refer to the contents of the registers/([0-9]+)/<\\1>/ \n\n## Page 19\n\nCapture groups: multiple registers/the (.*)er they (.*), the \\1er we \\2/ Matchesthe faster they ran, the faster we ranBut notthe faster they ran, the faster we ate \n\n## Page 20\n\nBut suppose we don't want to capture?Parentheses have a double function: grouping terms, and capturingNon-capturing groups: add a ?: after paren:/(?:some|afew) (people|cats) like some \\1/ matches \u25e6some cats like some cats but not \u25e6some cats like some some\n\n## Page 21\n\nLookahead assertions(?= pattern) is true if pattern matches, but is zero-width; doesn't advance character pointer(?! pattern) true if a pattern does not match How to match, at the beginning of a line, any single word that doesn\u2019t start with \u201cVolcano\u201d: /\u02c6(?!Volcano)[A-Za-z]+/ \n\n## Page 22\n\nSimple Application: ELIZAEarly NLP system that imitated a Rogerian psychotherapist \u25e6Joseph Weizenbaum, 1966. Uses pattern matching to match, e.g.,:\u25e6\u201cI need X\u201d and translates them into, e.g.\u25e6\u201cWhat would it mean to you if you got X? \n\n## Page 23\n\nSimple Application: ELIZAMen are all alike.IN WHAT WAYThey're always bugging us about something or other. CAN YOU THINK OF A SPECIFIC EXAMPLE Well, my boyfriend made me come here.YOUR BOYFRIEND MADE YOU COME HERE He says I'm depressed much of the time.I AM SORRY TO HEAR YOU ARE DEPRESSED \n\n## Page 24\n\nHow ELIZA workss/.* I\u2019M (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE \\1/ s/.* I AM (depressed|sad) .*/WHY DO YOU THINK YOU ARE \\1/s/.* all .*/IN WHAT WAY?/ s/.* always .*/CAN YOU THINK OF A SPECIFIC EXAMPLE?/ \n\n## Page 25\n\nBasic Text ProcessingMore Regular Expressions: Substitutions and ELIZA\n\n## Page 26\n\nBasic Text ProcessingWords and Corpora\n\n## Page 27\n\nHow many words in a sentence?\"I do uh main-mainly business data processing\"\u25e6Fragments, filled pauses\"Seuss\u2019s cat in the hat is different from othercats!\" \u25e6Lemma: same stem, part of speech, rough word sense\u25e6cat and cats = same lemma\u25e6Wordform: the full inflected surface form\u25e6cat and cats = different wordforms\n\n## Page 28\n\nHow many words in a sentence?they lay back on the San Francisco grass and looked at the stars and theirType: an element of the vocabulary.Token: an instance of that type in running text.How many?\u25e615 tokens (or 14)\u25e613 types (or 12) (or 11?)",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 2,
      "token_count": 767,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 29",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 3,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "How many words in a corpus?N= number of tokensV= vocabulary = set of types, |V|is size of vocabularyHeaps Law = Herdan'sLaw =                                 where often .67 < \u03b2 < .75i.e., vocabulary size grows with > square root of the number of word tokensTokens = NTypes = |V|Switchboard phoneconversations2.4 million20thousandShakespeare884,00031thousandCOCA440 million2 millionGoogle N-grams1 trillion13+ million2.2\u2022WORDS11duce other complications with regard to de\ufb01ning words. Let\u2019s look at one utterancefrom Switchboard; anutteranceis the spoken correlate of a sentence:utteranceI do uh main- mainly business data processingThis utterance has two kinds ofdis\ufb02uencies. The broken-off wordmain-isdis\ufb02uencycalled afragment. Words likeuhandumare called\ufb01llersor\ufb01lled pauses. Shouldfragment\ufb01lled pausewe consider these to be words? Again, it depends on the application. If we arebuilding a speech transcription system, we might want to eventually strip out thedis\ufb02uencies.But we also sometimes keep dis\ufb02uencies around. Dis\ufb02uencies likeuhorumare actually helpful in speech recognition in predicting the upcoming word, becausethey may signal that the speaker is restarting the clause or idea, and so for speechrecognition they are treated as regular words. Because people use different dis\ufb02u-encies they can also be a cue to speaker identi\ufb01cation. In factClark and Fox Tree(2002)showed thatuhandumhave different meanings. What do you think they are?Are capitalized tokens likeTheyand uncapitalized tokens liketheythe sameword? These are lumped together in some tasks (speech recognition), while for part-of-speech or named-entity tagging, capitalization is a useful feature and is retained.How about in\ufb02ected forms likecatsversuscat? These two words have the samelemmacatbut are different wordforms. Alemmais a set of lexical forms havinglemmathe same stem, the same major part-of-speech, and the same word sense. Theword-formis the full in\ufb02ected or derived form of the word. For morphologically complexwordformlanguages like Arabic, we often need to deal with lemmatization. For many tasks inEnglish, however, wordforms are suf\ufb01cient.How many words are there in English? To answer this question we need todistinguish two ways of talking about words.Typesare the number of distinct wordsword typein a corpus; if the set of words in the vocabulary isV, the number of types is thevocabulary size|V|.Tokensare the total numberNof running words. If we ignoreword tokenpunctuation, the following Brown sentence has 16 tokens and 14 types:They picnicked by the pool, then lay back on the grass and looked at the stars.When we speak about the number of words in the language, we are generallyreferring to word types.CorpusTokens =NTypes =|V|Shakespeare884 thousand31 thousandBrown corpus1 million38 thousandSwitchboard telephone conversations2.4 million20 thousandCOCA440 million2 millionGoogle N-grams1 trillion13 millionFigure 2.11Rough numbers of types and tokens for some English language corpora. Thelargest, the Google N-grams corpus, contains 13 million types, but this count only includestypes appearing 40 or more times, so the true number would be much larger.Fig.2.11shows the rough numbers",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 4,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "word sense. Theword-formis the full in\ufb02ected or derived form of the word. For morphologically complexwordformlanguages like Arabic, we often need to deal with lemmatization. For many tasks inEnglish, however, wordforms are suf\ufb01cient.How many words are there in English? To answer this question we need todistinguish two ways of talking about words.Typesare the number of distinct wordsword typein a corpus; if the set of words in the vocabulary isV, the number of types is thevocabulary size|V|.Tokensare the total numberNof running words. If we ignoreword tokenpunctuation, the following Brown sentence has 16 tokens and 14 types:They picnicked by the pool, then lay back on the grass and looked at the stars.When we speak about the number of words in the language, we are generallyreferring to word types.CorpusTokens =NTypes =|V|Shakespeare884 thousand31 thousandBrown corpus1 million38 thousandSwitchboard telephone conversations2.4 million20 thousandCOCA440 million2 millionGoogle N-grams1 trillion13 millionFigure 2.11Rough numbers of types and tokens for some English language corpora. Thelargest, the Google N-grams corpus, contains 13 million types, but this count only includestypes appearing 40 or more times, so the true number would be much larger.Fig.2.11shows the rough numbers of types and tokens computed from somepopular English corpora. The larger the corpora we look at, the more word typeswe \ufb01nd, and in fact this relationship between the number of types|V|and numberof tokensNis calledHerdan\u2019s Law(Herdan, 1960)orHeaps\u2019 Law(Heaps, 1978)Herdan\u2019s LawHeaps\u2019 Lawafter its discoverers (in linguistics and information retrieval respectively). It is shownin Eq.2.1, wherekandbare positive constants, and 0<b<1.|V|=kNb(2.1)",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 5,
      "token_count": 431,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 30\n\nCorporaWords don't appear out of nowhere! A text is produced by \u2022a specific writer(s), \u2022at a specific time, \u2022in a specific variety,\u2022of a specific language, \u2022for a specific function.\n\n## Page 31\n\nCorpora vary along dimension like\u25e6Language: 7097 languages in the world\u25e6Variety, like African American Language varieties.\u25e6AAE Twitter posts might include forms like \"iont\" (I don't)\u25e6Code switching, e.g., Spanish/English, Hindi/English:S/E: Por primeravezveoa @username actually being hateful! It was beautiful:) [For the first time I get to see @username actually being hateful! it was beautiful:) ] H/E: dost thaor ra-hega... dontwory... but dheryarakhe[\u201che was and will remain a friend ... don\u2019t worry ... but have faith\u201d] \u25e6Genre: newswire, fiction, scientific articles, Wikipedia\u25e6Author Demographics: writer's age, gender, ethnicity, SES \n\n## Page 32\n\nCorpus datasheetsMotivation: \u2022Why was the corpus collected?\u2022By whom? \u2022Who funded it? Situation: In what situation was the text written?Collection process: If it is a subsample how was it sampled? Was there consent? Pre-processing?+Annotation process, language variety, demographics, etc.Gebru et al (2020), Bender and Friedman (2018)\n\n## Page 33\n\nBasic Text ProcessingWords and Corpora\n\n## Page 34\n\nBasic Text ProcessingWord tokenization\n\n## Page 35\n\nText NormalizationEvery NLP task requires text normalization: 1.Tokenizing (segmenting) words2.Normalizing word formats3.Segmenting sentences\n\n## Page 36\n\nSpace-based tokenizationA very simple way to tokenize\u25e6For languages that use space characters between words\u25e6Arabic, Cyrillic, Greek, Latin, etc., based writing systems\u25e6Segment off a token between instances of spacesUnix tools for space-based tokenization\u25e6The \"tr\" command\u25e6Inspired by Ken Church's UNIX for Poets\u25e6Given a text file, output the word tokens and their frequencies\n\n## Page 37\n\nSimple Tokenization in UNIX(Inspired by Ken Church\u2019s UNIX for Poets.)Given a text file, output the word tokens and their frequenciestr -sc\u2019A-Za-z\u2019 \u2019\\n\u2019 < shakes.txt| sort | uniq\u2013c 1945 A72 AARON19 ABBESS5 ABBOT... ...25 Aaron6 Abate1 Abates5 Abbess6 Abbey3 Abbot....   \u2026Change all non-alpha to newlinesSort in alphabetical orderMerge and count each type\n\n## Page 38\n\nThe first step: tokenizingtr -sc\u2019A-Za-z\u2019 \u2019\\n\u2019 < shakes.txt| headTHESONNETSbyWilliamShakespeareFromfairestcreaturesWe...\n\n## Page 39\n\nThe second step: sortingtr -sc\u2019A-Za-z\u2019 \u2019\\n\u2019 < shakes.txt| sort | headAAAAAAAAA...\n\n## Page 40\n\nMore countingMerging upper and lower casetr\u2018A-Z\u2019 \u2018a-z\u2019 < shakes.txt| tr \u2013sc\u2018A-Za-z\u2019 \u2018\\n\u2019 | sort | uniq\u2013c Sorting the countstr\u2018A-Z\u2019 \u2018a-z\u2019 < shakes.txt| tr \u2013sc\u2018A-Za-z\u2019 \u2018\\n\u2019 | sort | uniq\u2013c | sort \u2013n \u2013r23243 the22225 i18618 and16339 to15687 of12780 a12163 you10839 my10005 in8954  dWhat happened here?",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 6,
      "token_count": 767,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 38\n\nThe first step: tokenizingtr -sc\u2019A-Za-z\u2019 \u2019\\n\u2019 < shakes.txt| headTHESONNETSbyWilliamShakespeareFromfairestcreaturesWe...\n\n## Page 39\n\nThe second step: sortingtr -sc\u2019A-Za-z\u2019 \u2019\\n\u2019 < shakes.txt| sort | headAAAAAAAAA...\n\n## Page 40\n\nMore countingMerging upper and lower casetr\u2018A-Z\u2019 \u2018a-z\u2019 < shakes.txt| tr \u2013sc\u2018A-Za-z\u2019 \u2018\\n\u2019 | sort | uniq\u2013c Sorting the countstr\u2018A-Z\u2019 \u2018a-z\u2019 < shakes.txt| tr \u2013sc\u2018A-Za-z\u2019 \u2018\\n\u2019 | sort | uniq\u2013c | sort \u2013n \u2013r23243 the22225 i18618 and16339 to15687 of12780 a12163 you10839 my10005 in8954  dWhat happened here?\n\n## Page 41\n\nIssues in TokenizationCan't just blindly remove punctuation:\u25e6m.p.h., Ph.D., AT&T, cap\u2019n\u25e6prices ($45.55)\u25e6dates (01/02/06)\u25e6URLs (http://www.stanford.edu)\u25e6hashtags (#nlproc)\u25e6email addresses (someone@cs.colorado.edu)Clitic: a word that doesn't stand on its own\u25e6\"are\" inwe're, French \"je\" in j'ai,\"le\" in l'honneurWhen should multiword expressions (MWE) be words?\u25e6New York, rock \u2019n\u2019 roll",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 7,
      "token_count": 333,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 42\n\nTokenization in NLTK16CHAPTER2\u2022REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCEInput:\"The San Francisco-based restaurant,\" they said,\"doesn\u2019t charge$10\".Output:\"TheSanFrancisco-basedrestaurant,\"theysaid,\"doesn\u2019tcharge$10\".In practice, since tokenization needs to be run before any other language pro-cessing, it needs to be very fast. The standard method for tokenization is thereforeto use deterministic algorithms based on regular expressions compiled into very ef-\ufb01cient \ufb01nite state automata. For example, Fig.2.12shows an example of a basicregular expression that can be used to tokenize with thenltk.regexptokenizefunction of the Python-based Natural Language Toolkit (NLTK) (Bird et al. 2009;http://www.nltk.org).>>> text = \u2019That U.S.A. poster-print costs $12.40...\u2019>>> pattern = r\u2019\u2019\u2019(?x) # set flag to allow verbose regexps... ([A-Z]\\.)+ # abbreviations, e.g. U.S.A.... | \\w+(-\\w+)* # words with optional internal hyphens... | \\$?\\d+(\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%... | \\.\\.\\. # ellipsis... | [][.,;\"\u2019?():-_\u2018] # these are separate tokens; includes ], [... \u2019\u2019\u2019>>> nltk.regexp_tokenize(text, pattern)[\u2019That\u2019, \u2019U.S.A.\u2019, \u2019poster-print\u2019, \u2019costs\u2019, \u2019$12.40\u2019, \u2019...\u2019]Figure 2.12A Python trace of regular expression tokenization in the NLTK Python-basednatural language processing toolkit(Bird et al., 2009), commented for readability; the(?x)verbose \ufb02ag tells Python to strip comments and whitespace. Figure from Chapter 3 ofBirdet al. (2009).Carefully designed deterministic algorithms can deal with the ambiguities thatarise, such as the fact that the apostrophe needs to be tokenized differently when usedas a genitive marker (as inthe book\u2019s cover), a quotative as in\u2018The other class\u2019, shesaid, or in clitics likethey\u2019re.Word tokenization is more complex in languages like written Chinese, Japanese,and Thai, which do not use spaces to mark potential word-boundaries. In Chinese,for example, words are composed of characters (calledhanziin Chinese). Eachhanzicharacter generally represents a single unit of meaning (called amorpheme) and ispronounceable as a single syllable. Words are about 2.4 characters long on average.But deciding what counts as a word in Chinese is complex. For example, considerthe following sentence:(2.4)\u2044\u0000\u20ace;\u2265[\u201cYao Ming reaches the \ufb01nals\u201dAsChen et al. (2017)point out, this could be treated as 3 words (\u2018Chinese Treebank\u2019segmentation):(2.5)\u2044\u0000YaoMing\u20acereaches;\u2265[\ufb01nalsor as 5 words (\u2018Peking University\u2019 segmentation):(2.6)\u2044Yao\u0000Ming\u20acereaches;overall\u2265[\ufb01nalsFinally, it is possible in Chinese simply to ignore words altogether and use charactersas the basic elements, treating the sentence as a series of 7 characters:Bird, Loper and Klein (2009),Natural Language Processing with Python. O\u2019Reilly\n\n## Page 43\n\nTokenization in languages without spaces Many languages (like Chinese, Japanese, Thai) don't use spaces to separate words!How do we decide where the token boundaries should be?",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 8,
      "token_count": 790,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 43\n\nTokenization in languages without spaces Many languages (like Chinese, Japanese, Thai) don't use spaces to separate words!How do we decide where the token boundaries should be?\n\n## Page 44\n\nWord tokenization in ChineseChinese words are composed of characters called \"hanzi\" (or sometimes just \"zi\")Each one represents a meaning unit called a morpheme.Each word has on average 2.4 of them.But deciding what counts as a word is complex and not agreed upon.\n\n## Page 45\n\nHow to do word tokenization in Chinese?\u59da\u660e\u8fdb\u5165\u603b\u51b3\u8d5b\u201cYao Ming reaches the finals\u201d3 words?\u59da\u660e\u8fdb\u5165\u603b\u51b3\u8d5bYaoMingreaches  finals 5 words?\u59da\u660e\u8fdb\u5165\u603b\u51b3\u8d5bYao    Ming    reaches    overall    finals 7 characters? (don't use words at all):\u59da\u660e\u8fdb\u5165\u603b\u51b3\u8d5bYao Ming enter enter overall decision game\n\n## Page 46\n\nHow to do word tokenization in Chinese?\u59da\u660e\u8fdb\u5165\u603b\u51b3\u8d5b\u201cYao Ming reaches the finals\u201d3 words?\u59da\u660e\u8fdb\u5165\u603b\u51b3\u8d5bYaoMingreaches  finals 5 words?\u59da\u660e\u8fdb\u5165\u603b\u51b3\u8d5bYao    Ming    reaches    overall    finals 7 characters? (don't use words at all):\u59da\u660e\u8fdb\u5165\u603b\u51b3\u8d5bYao Ming enter enter overall decision game\n\n## Page 47\n\nHow to do word tokenization in Chinese?\u59da\u660e\u8fdb\u5165\u603b\u51b3\u8d5b\u201cYao Ming reaches the finals\u201d3 words?\u59da\u660e\u8fdb\u5165\u603b\u51b3\u8d5bYaoMingreaches  finals 5 words?\u59da\u660e\u8fdb\u5165\u603b\u51b3\u8d5bYao    Ming    reaches    overall    finals 7 characters? (don't use words at all):\u59da\u660e\u8fdb\u5165\u603b\u51b3\u8d5bYao Ming enter enter overall decision game\n\n## Page 48\n\nHow to do word tokenization in Chinese?\u59da\u660e\u8fdb\u5165\u603b\u51b3\u8d5b\u201cYao Ming reaches the finals\u201d3 words?\u59da\u660e\u8fdb\u5165\u603b\u51b3\u8d5bYaoMingreaches  finals 5 words?\u59da\u660e\u8fdb\u5165\u603b\u51b3\u8d5bYao    Ming    reaches    overall    finals 7 characters? (don't use words at all):\u59da\u660e\u8fdb\u5165\u603b\u51b3\u8d5bYao Ming enter enter overall decision game\n\n## Page 49\n\nWord tokenization / segmentationSo in Chinese it's common to just treat each character (zi) as a token.\u2022So the segmentationstep is very simpleIn other languages (like Thai and Japanese), more complex word segmentation is required.\u2022The standard algorithms are neural sequence models trained by supervised machine learning.\n\n## Page 50\n\nBasic Text ProcessingWord tokenization\n\n## Page 51\n\nBasic Text ProcessingByte Pair Encoding\n\n## Page 52\n\nAnother option for text tokenizationInstead of \u2022white-space segmentation\u2022single-character segmentation Use the data to tell us how to tokenize.Subwordtokenization (because tokens can be parts of words as well as whole words)\n\n## Page 53\n\nSubwordtokenizationThree common algorithms:\u25e6Byte-Pair Encoding (BPE) (Sennrichet al., 2016)\u25e6Unigram language modeling tokenization (Kudo, 2018)\u25e6WordPiece(Schuster and Nakajima, 2012)All have 2 parts:\u25e6A token learnerthat takes a raw training corpus and induces a vocabulary (a set of tokens). \u25e6A token segmenterthat takes a raw test sentence and tokenizes it according to that vocabulary",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 9,
      "token_count": 785,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 49\n\nWord tokenization / segmentationSo in Chinese it's common to just treat each character (zi) as a token.\u2022So the segmentationstep is very simpleIn other languages (like Thai and Japanese), more complex word segmentation is required.\u2022The standard algorithms are neural sequence models trained by supervised machine learning.\n\n## Page 50\n\nBasic Text ProcessingWord tokenization\n\n## Page 51\n\nBasic Text ProcessingByte Pair Encoding\n\n## Page 52\n\nAnother option for text tokenizationInstead of \u2022white-space segmentation\u2022single-character segmentation Use the data to tell us how to tokenize.Subwordtokenization (because tokens can be parts of words as well as whole words)\n\n## Page 53\n\nSubwordtokenizationThree common algorithms:\u25e6Byte-Pair Encoding (BPE) (Sennrichet al., 2016)\u25e6Unigram language modeling tokenization (Kudo, 2018)\u25e6WordPiece(Schuster and Nakajima, 2012)All have 2 parts:\u25e6A token learnerthat takes a raw training corpus and induces a vocabulary (a set of tokens). \u25e6A token segmenterthat takes a raw test sentence and tokenizes it according to that vocabulary\n\n## Page 54\n\nByte Pair Encoding (BPE) token learnerLet vocabulary be the set of all individual characters = {A, B, C, D,\u2026, a, b, c, d\u2026.}Repeat:\u25e6Choose the two symbols that are most frequently adjacent in the training corpus (say 'A', 'B') \u25e6Add a new merged symbol 'AB' to the vocabulary\u25e6Replace every adjacent 'A' 'B' in the corpus with 'AB'. Until k merges have been done.",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 10,
      "token_count": 356,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 55\n\nBPE token learner algorithm2.4\u2022TEXTNORMALIZATION19functionBYTE-PAIR ENCODING(stringsC, number of mergesk)returnsvocabVV all unique characters inC# initial set of tokens is charactersfori=1tokdo# merge tokens tilktimestL,tR Most frequent pair of adjacent tokens inCtNEW tL+tR# make new token by concatenatingV V+tNEW# update the vocabularyReplace each occurrence oftL,tRinCwithtNEW# and update the corpusreturnVFigure 2.13The token learner part of the BPE algorithm for taking a corpus broken upinto individual characters or bytes, and learning a vocabulary by iteratively merging tokens.Figure adapted fromBostrom and Durrett (2020).from the training data, greedily, in the order we learned them. (Thus the frequenciesin the test data don\u2019t play a role, just the frequencies in the training data). So \ufb01rstwe segment each test sentence word into characters. Then we apply the \ufb01rst rule:replace every instance oferin the test corpus withr, and then the second rule:replace every instance oferin the test corpus wither, and so on. By the end,if the test corpus contained the wordnewer, it would be tokenized as a fullword. But a new (unknown) word likelowerwould be merged into the twotokenslow er.Of course in real algorithms BPE is run with many thousands of merges on a verylarge input corpus. The result is that most words will be represented as full symbols,and only the very rare words (and unknown words) will have to be represented bytheir parts.2.4.4 Word Normalization, Lemmatization and StemmingWordnormalizationis the task of putting words/tokens in a standard format, choos-normalizationing a single normal form for words with multiple forms likeUSAandUSoruh-huhanduhhuh. This standardization may be valuable, despite the spelling informationthat is lost in the normalization process. For information retrieval or informationextraction about the US, we might want to see information from documents whetherthey mention theUSor theUSA.Case foldingis another kind of normalization. Mapping everything to lowercase foldingcase means thatWoodchuckandwoodchuckare represented identically, which isvery helpful for generalization in many tasks, such as information retrieval or speechrecognition. For sentiment analysis and other text classi\ufb01cation tasks, informationextraction, and machine translation, by contrast, case can be quite helpful and casefolding is generally not done. This is because maintaining the difference between,for example,USthe country andusthe pronoun can outweigh the advantage ingeneralization that case folding would have provided for other words.For many natural language processing situations we also want two morpholog-ically different forms of a word to behave similarly. For example in web search,someone may type the stringwoodchucksbut a useful system might want to alsoreturn pages that mentionwoodchuckwith nos. This is especially common in mor-phologically complex languages like Russian, where for example the wordMoscowhas different endings in the phrasesMoscow,of Moscow,to Moscow, and so on.Lemmatizationis the task of determining that two words have the same root,despite their surface differences. The wordsam,are, andishave the shared lemma\n\n## Page 56\n\nByte Pair Encoding (BPE) AddendumMost subwordalgorithms are run inside space-separated tokens. So we commonly first add a special end-of-word symbol '__' before space in training corpusNext, separate into letters.",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 11,
      "token_count": 755,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 56\n\nByte Pair Encoding (BPE) AddendumMost subwordalgorithms are run inside space-separated tokens. So we commonly first add a special end-of-word symbol '__' before space in training corpusNext, separate into letters.\n\n## Page 57\n\nBPE token learner18CHAPTER2\u2022REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCEThe algorithm is usually run inside words (not merging across word boundaries),so the input corpus is \ufb01rst white-space-separated to give a set of strings, each corre-sponding to the characters of a word, plus a special end-of-word symbol, and itscounts. Let\u2019s see its operation on the following tiny input corpus of 18 word tokenswith counts for each word (the wordlowappears 5 times, the wordnewer6 times,and so on), which would have a starting vocabulary of 11 letters:corpus vocabulary5low, d, e, i, l, n, o, r, s, t, w2lowest6newer3wider2newThe BPE algorithm \ufb01rst count all pairs of adjacent symbols: the most frequentis the pairerbecause it occurs innewer(frequency of 6) andwider(frequency of3) for a total of 9 occurrences1. We then merge these symbols, treatingeras onesymbol, and count again:corpus vocabulary5low, d, e, i, l, n, o, r, s, t, w, er2lowest6n e w er3w i d er2newNow the most frequent pair iser, which we merge; our system has learnedthat there should be a token for word-\ufb01naler, represented aser:corpus vocabulary5low,d,e,i,l,n,o,r,s,t,w,er,er2lowest6n e w er3w i d er2newNextne(total count of 8) get merged tone:corpus vocabulary5low,d,e,i,l,n,o,r,s,t,w,er,er,ne2lowest6ne w er3w i d er2ne wIf we continue, the next merges are:Merge Current Vocabulary(ne, w),d,e,i,l,n,o,r,s,t,w,er,er,ne,new(l, o),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo(lo, w),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low(new, er),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer(low,),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer,lowOnce we\u2019ve learned our vocabulary, thetoken parseris used to tokenize a testsentence. The token parser just runs on the test data the merges we have learned1Note that there can be ties; we could have instead chosen to merger\ufb01rst, since that also has afrequency of 9.Original (very fascinating\n\ud83d\ude44) corpus:low low low low low lowest lowest newer newer newer        newer newer newer wider wider wider new newAdd end-of-word tokens, resulting in this vocabulary:representation",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 12,
      "token_count": 678,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 58",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 13,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BPE token learner18CHAPTER2\u2022REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCEThe algorithm is usually run inside words (not merging across word boundaries),so the input corpus is \ufb01rst white-space-separated to give a set of strings, each corre-sponding to the characters of a word, plus a special end-of-word symbol, and itscounts. Let\u2019s see its operation on the following tiny input corpus of 18 word tokenswith counts for each word (the wordlowappears 5 times, the wordnewer6 times,and so on), which would have a starting vocabulary of 11 letters:corpus vocabulary5low, d, e, i, l, n, o, r, s, t, w2lowest6newer3wider2newThe BPE algorithm \ufb01rst count all pairs of adjacent symbols: the most frequentis the pairerbecause it occurs innewer(frequency of 6) andwider(frequency of3) for a total of 9 occurrences1. We then merge these symbols, treatingeras onesymbol, and count again:corpus vocabulary5low, d, e, i, l, n, o, r, s, t, w, er2lowest6n e w er3w i d er2newNow the most frequent pair iser, which we merge; our system has learnedthat there should be a token for word-\ufb01naler, represented aser:corpus vocabulary5low,d,e,i,l,n,o,r,s,t,w,er,er2lowest6n e w er3w i d er2newNextne(total count of 8) get merged tone:corpus vocabulary5low,d,e,i,l,n,o,r,s,t,w,er,er,ne2lowest6ne w er3w i d er2ne wIf we continue, the next merges are:Merge Current Vocabulary(ne, w),d,e,i,l,n,o,r,s,t,w,er,er,ne,new(l, o),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo(lo, w),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low(new, er),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer(low,),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer,lowOnce we\u2019ve learned our vocabulary, thetoken parseris used to tokenize a testsentence. The token parser just runs on the test data the merges we have learned1Note that there can be ties; we could have instead chosen to merger\ufb01rst, since that also has afrequency of 9.Merge e r to er18CHAPTER2\u2022REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCEThe algorithm is usually run inside words (not merging across word boundaries),so the input corpus is \ufb01rst white-space-separated to give a set of strings, each corre-sponding to the characters of a word, plus a special end-of-word symbol, and itscounts. Let\u2019s see its operation on the following tiny input corpus of 18 word tokenswith counts for each word (the wordlowappears 5 times, the wordnewer6 times,and so on), which would have a starting vocabulary of 11 letters:corpus vocabulary5low, d, e, i, l, n, o, r, s, t, w2lowest6newer3wider2newThe BPE algorithm \ufb01rst count all pairs of adjacent symbols: the most frequentis the pairerbecause it occurs innewer(frequency of 6) andwider(frequency of3) for a total of 9 occurrences1. We then merge",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 14,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "we\u2019ve learned our vocabulary, thetoken parseris used to tokenize a testsentence. The token parser just runs on the test data the merges we have learned1Note that there can be ties; we could have instead chosen to merger\ufb01rst, since that also has afrequency of 9.Merge e r to er18CHAPTER2\u2022REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCEThe algorithm is usually run inside words (not merging across word boundaries),so the input corpus is \ufb01rst white-space-separated to give a set of strings, each corre-sponding to the characters of a word, plus a special end-of-word symbol, and itscounts. Let\u2019s see its operation on the following tiny input corpus of 18 word tokenswith counts for each word (the wordlowappears 5 times, the wordnewer6 times,and so on), which would have a starting vocabulary of 11 letters:corpus vocabulary5low, d, e, i, l, n, o, r, s, t, w2lowest6newer3wider2newThe BPE algorithm \ufb01rst count all pairs of adjacent symbols: the most frequentis the pairerbecause it occurs innewer(frequency of 6) andwider(frequency of3) for a total of 9 occurrences1. We then merge these symbols, treatingeras onesymbol, and count again:corpus vocabulary5low, d, e, i, l, n, o, r, s, t, w, er2lowest6n e w er3w i d er2newNow the most frequent pair iser, which we merge; our system has learnedthat there should be a token for word-\ufb01naler, represented aser:corpus vocabulary5low,d,e,i,l,n,o,r,s,t,w,er,er2lowest6n e w er3w i d er2newNextne(total count of 8) get merged tone:corpus vocabulary5low,d,e,i,l,n,o,r,s,t,w,er,er,ne2lowest6ne w er3w i d er2ne wIf we continue, the next merges are:Merge Current Vocabulary(ne, w),d,e,i,l,n,o,r,s,t,w,er,er,ne,new(l, o),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo(lo, w),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low(new, er),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer(low,),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer,lowOnce we\u2019ve learned our vocabulary, thetoken parseris used to tokenize a testsentence. The token parser just runs on the test data the merges we have learned1Note that there can be ties; we could have instead chosen to merger\ufb01rst, since that also has afrequency of 9.",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 15,
      "token_count": 641,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 59",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 16,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BPEMerge er  _ to er_18CHAPTER2\u2022REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCEThe algorithm is usually run inside words (not merging across word boundaries),so the input corpus is \ufb01rst white-space-separated to give a set of strings, each corre-sponding to the characters of a word, plus a special end-of-word symbol, and itscounts. Let\u2019s see its operation on the following tiny input corpus of 18 word tokenswith counts for each word (the wordlowappears 5 times, the wordnewer6 times,and so on), which would have a starting vocabulary of 11 letters:corpus vocabulary5low, d, e, i, l, n, o, r, s, t, w2lowest6newer3wider2newThe BPE algorithm \ufb01rst count all pairs of adjacent symbols: the most frequentis the pairerbecause it occurs innewer(frequency of 6) andwider(frequency of3) for a total of 9 occurrences1. We then merge these symbols, treatingeras onesymbol, and count again:corpus vocabulary5low, d, e, i, l, n, o, r, s, t, w, er2lowest6n e w er3w i d er2newNow the most frequent pair iser, which we merge; our system has learnedthat there should be a token for word-\ufb01naler, represented aser:corpus vocabulary5low,d,e,i,l,n,o,r,s,t,w,er,er2lowest6n e w er3w i d er2newNextne(total count of 8) get merged tone:corpus vocabulary5low,d,e,i,l,n,o,r,s,t,w,er,er,ne2lowest6ne w er3w i d er2ne wIf we continue, the next merges are:Merge Current Vocabulary(ne, w),d,e,i,l,n,o,r,s,t,w,er,er,ne,new(l, o),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo(lo, w),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low(new, er),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer(low,),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer,lowOnce we\u2019ve learned our vocabulary, thetoken parseris used to tokenize a testsentence. The token parser just runs on the test data the merges we have learned1Note that there can be ties; we could have instead chosen to merger\ufb01rst, since that also has afrequency of 9.18CHAPTER2\u2022REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCEThe algorithm is usually run inside words (not merging across word boundaries),so the input corpus is \ufb01rst white-space-separated to give a set of strings, each corre-sponding to the characters of a word, plus a special end-of-word symbol, and itscounts. Let\u2019s see its operation on the following tiny input corpus of 18 word tokenswith counts for each word (the wordlowappears 5 times, the wordnewer6 times,and so on), which would have a starting vocabulary of 11 letters:corpus vocabulary5low, d, e, i, l, n, o, r, s, t, w2lowest6newer3wider2newThe BPE algorithm \ufb01rst count all pairs of adjacent symbols: the most frequentis the pairerbecause it occurs innewer(frequency of 6) andwider(frequency of3) for a total of 9 occurrences1. We then",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 17,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "we\u2019ve learned our vocabulary, thetoken parseris used to tokenize a testsentence. The token parser just runs on the test data the merges we have learned1Note that there can be ties; we could have instead chosen to merger\ufb01rst, since that also has afrequency of 9.18CHAPTER2\u2022REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCEThe algorithm is usually run inside words (not merging across word boundaries),so the input corpus is \ufb01rst white-space-separated to give a set of strings, each corre-sponding to the characters of a word, plus a special end-of-word symbol, and itscounts. Let\u2019s see its operation on the following tiny input corpus of 18 word tokenswith counts for each word (the wordlowappears 5 times, the wordnewer6 times,and so on), which would have a starting vocabulary of 11 letters:corpus vocabulary5low, d, e, i, l, n, o, r, s, t, w2lowest6newer3wider2newThe BPE algorithm \ufb01rst count all pairs of adjacent symbols: the most frequentis the pairerbecause it occurs innewer(frequency of 6) andwider(frequency of3) for a total of 9 occurrences1. We then merge these symbols, treatingeras onesymbol, and count again:corpus vocabulary5low, d, e, i, l, n, o, r, s, t, w, er2lowest6n e w er3w i d er2newNow the most frequent pair iser, which we merge; our system has learnedthat there should be a token for word-\ufb01naler, represented aser:corpus vocabulary5low,d,e,i,l,n,o,r,s,t,w,er,er2lowest6n e w er3w i d er2newNextne(total count of 8) get merged tone:corpus vocabulary5low,d,e,i,l,n,o,r,s,t,w,er,er,ne2lowest6ne w er3w i d er2ne wIf we continue, the next merges are:Merge Current Vocabulary(ne, w),d,e,i,l,n,o,r,s,t,w,er,er,ne,new(l, o),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo(lo, w),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low(new, er),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer(low,),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer,lowOnce we\u2019ve learned our vocabulary, thetoken parseris used to tokenize a testsentence. The token parser just runs on the test data the merges we have learned1Note that there can be ties; we could have instead chosen to merger\ufb01rst, since that also has afrequency of 9.",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 18,
      "token_count": 637,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 60",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 19,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BPEMerge n  e  to ne18CHAPTER2\u2022REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCEThe algorithm is usually run inside words (not merging across word boundaries),so the input corpus is \ufb01rst white-space-separated to give a set of strings, each corre-sponding to the characters of a word, plus a special end-of-word symbol, and itscounts. Let\u2019s see its operation on the following tiny input corpus of 18 word tokenswith counts for each word (the wordlowappears 5 times, the wordnewer6 times,and so on), which would have a starting vocabulary of 11 letters:corpus vocabulary5low, d, e, i, l, n, o, r, s, t, w2lowest6newer3wider2newThe BPE algorithm \ufb01rst count all pairs of adjacent symbols: the most frequentis the pairerbecause it occurs innewer(frequency of 6) andwider(frequency of3) for a total of 9 occurrences1. We then merge these symbols, treatingeras onesymbol, and count again:corpus vocabulary5low, d, e, i, l, n, o, r, s, t, w, er2lowest6n e w er3w i d er2newNow the most frequent pair iser, which we merge; our system has learnedthat there should be a token for word-\ufb01naler, represented aser:corpus vocabulary5low,d,e,i,l,n,o,r,s,t,w,er,er2lowest6n e w er3w i d er2newNextne(total count of 8) get merged tone:corpus vocabulary5low,d,e,i,l,n,o,r,s,t,w,er,er,ne2lowest6ne w er3w i d er2ne wIf we continue, the next merges are:Merge Current Vocabulary(ne, w),d,e,i,l,n,o,r,s,t,w,er,er,ne,new(l, o),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo(lo, w),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low(new, er),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer(low,),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer,lowOnce we\u2019ve learned our vocabulary, thetoken parseris used to tokenize a testsentence. The token parser just runs on the test data the merges we have learned1Note that there can be ties; we could have instead chosen to merger\ufb01rst, since that also has afrequency of 9.18CHAPTER2\u2022REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCEThe algorithm is usually run inside words (not merging across word boundaries),so the input corpus is \ufb01rst white-space-separated to give a set of strings, each corre-sponding to the characters of a word, plus a special end-of-word symbol, and itscounts. Let\u2019s see its operation on the following tiny input corpus of 18 word tokenswith counts for each word (the wordlowappears 5 times, the wordnewer6 times,and so on), which would have a starting vocabulary of 11 letters:corpus vocabulary5low, d, e, i, l, n, o, r, s, t, w2lowest6newer3wider2newThe BPE algorithm \ufb01rst count all pairs of adjacent symbols: the most frequentis the pairerbecause it occurs innewer(frequency of 6) andwider(frequency of3) for a total of 9 occurrences1. We then",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 20,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "we\u2019ve learned our vocabulary, thetoken parseris used to tokenize a testsentence. The token parser just runs on the test data the merges we have learned1Note that there can be ties; we could have instead chosen to merger\ufb01rst, since that also has afrequency of 9.18CHAPTER2\u2022REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCEThe algorithm is usually run inside words (not merging across word boundaries),so the input corpus is \ufb01rst white-space-separated to give a set of strings, each corre-sponding to the characters of a word, plus a special end-of-word symbol, and itscounts. Let\u2019s see its operation on the following tiny input corpus of 18 word tokenswith counts for each word (the wordlowappears 5 times, the wordnewer6 times,and so on), which would have a starting vocabulary of 11 letters:corpus vocabulary5low, d, e, i, l, n, o, r, s, t, w2lowest6newer3wider2newThe BPE algorithm \ufb01rst count all pairs of adjacent symbols: the most frequentis the pairerbecause it occurs innewer(frequency of 6) andwider(frequency of3) for a total of 9 occurrences1. We then merge these symbols, treatingeras onesymbol, and count again:corpus vocabulary5low, d, e, i, l, n, o, r, s, t, w, er2lowest6n e w er3w i d er2newNow the most frequent pair iser, which we merge; our system has learnedthat there should be a token for word-\ufb01naler, represented aser:corpus vocabulary5low,d,e,i,l,n,o,r,s,t,w,er,er2lowest6n e w er3w i d er2newNextne(total count of 8) get merged tone:corpus vocabulary5low,d,e,i,l,n,o,r,s,t,w,er,er,ne2lowest6ne w er3w i d er2ne wIf we continue, the next merges are:Merge Current Vocabulary(ne, w),d,e,i,l,n,o,r,s,t,w,er,er,ne,new(l, o),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo(lo, w),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low(new, er),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer(low,),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer,lowOnce we\u2019ve learned our vocabulary, thetoken parseris used to tokenize a testsentence. The token parser just runs on the test data the merges we have learned1Note that there can be ties; we could have instead chosen to merger\ufb01rst, since that also has afrequency of 9.",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 21,
      "token_count": 637,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 61\n\nBPEThe next merges are:18CHAPTER2\u2022REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCEThe algorithm is usually run inside words (not merging across word boundaries),so the input corpus is \ufb01rst white-space-separated to give a set of strings, each corre-sponding to the characters of a word, plus a special end-of-word symbol, and itscounts. Let\u2019s see its operation on the following tiny input corpus of 18 word tokenswith counts for each word (the wordlowappears 5 times, the wordnewer6 times,and so on), which would have a starting vocabulary of 11 letters:corpus vocabulary5low, d, e, i, l, n, o, r, s, t, w2lowest6newer3wider2newThe BPE algorithm \ufb01rst count all pairs of adjacent symbols: the most frequentis the pairerbecause it occurs innewer(frequency of 6) andwider(frequency of3) for a total of 9 occurrences1. We then merge these symbols, treatingeras onesymbol, and count again:corpus vocabulary5low, d, e, i, l, n, o, r, s, t, w, er2lowest6n e w er3w i d er2newNow the most frequent pair iser, which we merge; our system has learnedthat there should be a token for word-\ufb01naler, represented aser:corpus vocabulary5low,d,e,i,l,n,o,r,s,t,w,er,er2lowest6n e w er3w i d er2newNextne(total count of 8) get merged tone:corpus vocabulary5low,d,e,i,l,n,o,r,s,t,w,er,er,ne2lowest6ne w er3w i d er2ne wIf we continue, the next merges are:Merge Current Vocabulary(ne, w),d,e,i,l,n,o,r,s,t,w,er,er,ne,new(l, o),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo(lo, w),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low(new, er),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer(low,),d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer,lowOnce we\u2019ve learned our vocabulary, thetoken parseris used to tokenize a testsentence. The token parser just runs on the test data the merges we have learned1Note that there can be ties; we could have instead chosen to merger\ufb01rst, since that also has afrequency of 9.\n\n## Page 62\n\nBPE token segmenteralgorithmOn the test data, run each merge learned from the training data:\u25e6Greedily\u25e6In the order we learned them\u25e6(test frequencies don't play a role)So: merge every e rto er, then merge er _to er_, etc.Result: \u25e6Test set \"n e w e r _\" would be tokenized as a full word \u25e6Test set \"l o w e r _\" would be two tokens: \"low er_\"\n\n## Page 63\n\nProperties of BPE tokensUsually include frequent wordsAnd frequent subwords\u2022Which are often morphemes like -estor \u2013erA morpheme is the smallest meaning-bearing unit of a language\u2022unlikeliest has 3 morphemes un-, likely, and -est\n\n## Page 64\n\nBasic Text ProcessingByte Pair Encoding\n\n## Page 65\n\nBasic Text ProcessingWord Normalization and other issues",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 22,
      "token_count": 785,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 62\n\nBPE token segmenteralgorithmOn the test data, run each merge learned from the training data:\u25e6Greedily\u25e6In the order we learned them\u25e6(test frequencies don't play a role)So: merge every e rto er, then merge er _to er_, etc.Result: \u25e6Test set \"n e w e r _\" would be tokenized as a full word \u25e6Test set \"l o w e r _\" would be two tokens: \"low er_\"\n\n## Page 63\n\nProperties of BPE tokensUsually include frequent wordsAnd frequent subwords\u2022Which are often morphemes like -estor \u2013erA morpheme is the smallest meaning-bearing unit of a language\u2022unlikeliest has 3 morphemes un-, likely, and -est\n\n## Page 64\n\nBasic Text ProcessingByte Pair Encoding\n\n## Page 65\n\nBasic Text ProcessingWord Normalization and other issues\n\n## Page 66\n\nWord NormalizationPutting words/tokens in a standard format\u25e6U.S.A. or USA\u25e6uhhuhor uh-huh\u25e6Fed or fed\u25e6am, is, be, are \n\n## Page 67\n\nCase foldingApplications like IR: reduce all letters to lower case\u25e6Since users tend to use lower case\u25e6Possible exception: upper case in mid-sentence?\u25e6e.g., General Motors\u25e6Fedvs. fed\u25e6SAILvs. sailFor sentiment analysis, MT, Information extraction\u25e6Case is helpful (USversus us is important)\n\n## Page 68\n\nLemmatizationRepresent all words as their lemma, their shared root = dictionary headword form:\u25e6am, are,is \u00aebe\u25e6car, cars, car's, cars'\u00aecar\u25e6Spanish quiero(\u2018I want\u2019), quieres(\u2018you want\u2019) \u00aequerer\u2018want'\u25e6He is reading detective stories \u00aeHe be read detective story \n\n## Page 69\n\nLemmatization is done by Morphological ParsingMorphemes:\u25e6The small meaningful units that make up words\u25e6Stems: The core meaning-bearing units\u25e6Affixes: Parts that adhere to stems, often with grammatical functionsMorphological Parsers:\u25e6Parsecats into two morphemes cat and s\u25e6Parse Spanish amaren(\u2018if in the future they would love\u2019) into morpheme amar\u2018to love\u2019, and the morphological features 3PL and future subjunctive. \n\n## Page 70\n\nStemmingReduce terms to stems, chopping off affixes crudelyThis was not the map we found in Billy Bones\u2019s chest, but an accurate copy, complete in all things-names and heights and soundings-with the single exception of the red crosses and the written notes. Thi wa not the map we found in Billi Bone s chest but an accur copi complet in all thing name and height and sound with the singl except of the red cross and the written note .",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 23,
      "token_count": 621,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 71\n\nPorter StemmerBased on a series of rewrite rules run in series\u25e6A cascade, in which output of each pass fed to next passSome sample rules:20CHAPTER2\u2022REGULAREXPRESSIONS,TEXTNORMALIZATION,EDITDISTANCEbe; the wordsdinneranddinnersboth have the lemmadinner. Lemmatizing each ofthese forms to the same lemma will let us \ufb01nd all mentions of words in Russian likeMoscow. The lemmatized form of a sentence likeHe is reading detective storieswould thus beHe be read detective story.How is lemmatization done? The most sophisticated methods for lemmatizationinvolve completemorphological parsingof the word.Morphologyis the study ofthe way words are built up from smaller meaning-bearing units calledmorphemes.morphemeTwo broad classes of morphemes can be distinguished:stems\u2014the central mor-stempheme of the word, supplying the main meaning\u2014 andaf\ufb01xes\u2014adding \u201cadditional\u201daf\ufb01xmeanings of various kinds. So, for example, the wordfoxconsists of one morpheme(the morphemefox) and the wordcatsconsists of two: the morphemecatand themorpheme-s. A morphological parser takes a word likecatsand parses it into thetwo morphemescatands, or parses a Spanish word likeamaren(\u2018if in the futurethey would love\u2019) into the morphemeamar\u2018to love\u2019, and the morphological features3PLandfuture subjunctive.The Porter StemmerLemmatization algorithms can be complex. For this reason we sometimes make useof a simpler but cruder method, which mainly consists of chopping off word-\ufb01nalaf\ufb01xes. This naive version of morphological analysis is calledstemming. One ofstemmingthe most widely used stemming algorithms is thePorter (1980). The Porter stemmerPorter stemmerapplied to the following paragraph:This was not the map we found in Billy Bones\u2019s chest, butan accurate copy, complete in all things-names and heightsand soundings-with the single exception of the red crossesand the written notes.produces the following stemmed output:Thi wa not the map we found in Billi Bone s chest but anaccur copi complet in all thing name and height and soundwith the singl except of the red cross and the written noteThe algorithm is based on series of rewrite rules run in series, as acascade, incascadewhich the output of each pass is fed as input to the next pass; here is a sampling ofthe rules:ATIONAL!ATE (e.g., relational!relate)ING!\u270fif stem contains vowel (e.g., motoring!motor)SSES!SS (e.g., grasses!grass)Detailed rule lists for the Porter stemmer, as well as code (in Java, Python, etc.)can be found on Martin Porter\u2019s homepage; see also the original paper(Porter, 1980).Simple stemmers can be useful in cases where we need to collapse across differ-ent variants of the same lemma. Nonetheless, they do tend to commit errors of bothover- and under-generalizing, as shown in the table below(Krovetz, 1993):Errors of CommissionErrors of OmissionorganizationorganEuropeanEuropedoingdoeanalysisanalyzesnumericalnumerousnoisenoisypolicypolicesparsesparsity",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 24,
      "token_count": 720,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 72\n\nDealing with complex morphology is necessary for many languages\u25e6e.g., the Turkish word:\u25e6Uygarlastiramadiklarimizdanmissinizcasina\u25e6`(behaving) as if you are among those whom we could not civilize\u2019\u25e6Uygar`civilized\u2019 + las`become\u2019 + tir`cause\u2019 + ama`not able\u2019 + dik`past\u2019 + lar\u2018plural\u2019+ imiz\u2018p1pl\u2019 + dan\u2018abl\u2019 + mis\u2018past\u2019 + siniz\u20182pl\u2019 + casina\u2018as if\u2019 \n\n## Page 73\n\nSentence Segmentation!, ? mostly unambiguous but period\u201c.\u201d is very ambiguous\u25e6Sentence boundary\u25e6Abbreviations like Inc. or Dr.\u25e6Numbers like .02% or 4.3Common algorithm: Tokenize first: use rules or ML to classify a period as either (a) part of the word or (b) a sentence-boundary. \u25e6An abbreviation dictionary can helpSentence segmentation can then often be done by rules based on this tokenization.\n\n## Page 74\n\nBasic Text ProcessingWord Normalization and other issues",
    "metadata": {
      "source": "2_TextProc_2023",
      "chunk_id": 25,
      "token_count": 245,
      "chapter_title": ""
    }
  }
]