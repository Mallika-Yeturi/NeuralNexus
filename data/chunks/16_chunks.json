[
  {
    "content": "# 16\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n16Automatic Speech Recognition\nand Text-to-Speech\nI KNOW not whether\nI see your meaning: if I do, it lies\nUpon the wordy wavelets of your voice,\nDim as an evening shadow in a brook,\nThomas Lovell Beddoes, 1851\nUnderstanding spoken language, or at least transcribing the words into writing, is\none of the earliest goals of computer language processing. In fact, speech processing\npredates the computer by many decades!\nThe \ufb01rst machine that recognized speech\nwas a toy from the 1920s. \u201cRadio Rex\u201d,\nshown to the right, was a celluloid dog\nthat moved (by means of a spring) when\nthe spring was released by 500 Hz acous-\ntic energy. Since 500 Hz is roughly the\n\ufb01rst formant of the vowel [eh] in \u201cRex\u201d,\nRex seemed to come when he was called\n(David, Jr. and Selfridge, 1962).\nIn modern times, we expect more of our automatic systems. The task of auto-\nmatic speech recognition (ASR ) is to map any waveform like this: ASR\nto the appropriate string of words:\nIt's time for lunch!\nAutomatic transcription of speech by any speaker in any environment is still far from\nsolved, but ASR technology has matured to the point where it is now viable for many\npractical tasks. Speech is a natural interface for communicating with smart home ap-\npliances, personal assistants, or cellphones, where keyboards are less convenient, in\ntelephony applications like call-routing (\u201cAccounting, please\u201d) or in sophisticated\ndialogue applications (\u201cI\u2019d like to change the return date of my \ufb02ight\u201d). ASR is also\nuseful for general transcription, for example for automatically generating captions\nfor audio or video text (transcribing movies or videos or live discussions). Transcrip-\ntion is important in \ufb01elds like law where dictation plays an important role. Finally,\nASR is important as part of augmentative communication (interaction between com-\nputers and humans with some disability resulting in dif\ufb01culties or inabilities in typ-\ning or audition). The blind Milton famously dictated Paradise Lost to his daughters,\nand Henry James dictated his later novels after a repetitive stress injury.\nWhat about the opposite problem, going from text to speech? This is a problem\nwith an even longer history. In Vienna in 1769, Wolfgang von Kempelen built for",
    "metadata": {
      "source": "16",
      "chunk_id": 0,
      "token_count": 561,
      "chapter_title": "16"
    }
  },
  {
    "content": "## Page 2\n\n2CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\nthe Empress Maria Theresa the famous Mechanical Turk, a chess-playing automaton\nconsisting of a wooden box \ufb01lled with gears, behind which sat a robot mannequin\nwho played chess by moving pieces with his mechanical arm. The Turk toured Eu-\nrope and the Americas for decades, defeating Napoleon Bonaparte and even playing\nCharles Babbage. The Mechanical Turk might have been one of the early successes\nof arti\ufb01cial intelligence were it not for the fact that it was, alas, a hoax, powered by\na human chess player hidden inside the box.\nWhat is less well known is that von Kempelen, an extraordinarily\nproli\ufb01c inventor, also built between\n1769 and 1790 what was de\ufb01nitely\nnot a hoax: the \ufb01rst full-sentence\nspeech synthesizer, shown partially to\nthe right. His device consisted of a\nbellows to simulate the lungs, a rub-\nber mouthpiece and a nose aperture, a\nreed to simulate the vocal folds, var-\nious whistles for the fricatives, and a\nsmall auxiliary bellows to provide the puff of air for plosives. By moving levers\nwith both hands to open and close apertures, and adjusting the \ufb02exible leather \u201cvo-\ncal tract\u201d, an operator could produce different consonants and vowels.\nMore than two centuries later, we no longer build our synthesizers out of wood\nand leather, nor do we need human operators. The modern task of speech synthesis ,speech\nsynthesis\nalso called text-to-speech orTTS , is exactly the reverse of ASR; to map text: text-to-speech\nTTSIt's time for lunch!\nto an acoustic waveform:\nModern speech synthesis has a wide variety of applications. TTS is used in\nconversational agents that conduct dialogues with people, plays a role in devices\nthat read out loud for the blind or in games, and can be used to speak for sufferers\nof neurological disorders, such as the late astrophysicist Steven Hawking who, after\nhe lost the use of his voice because of ALS, spoke by manipulating a TTS system.\nIn the next sections we\u2019ll show how to do ASR with encoder-decoders, intro-\nduce the CTC loss functions, the standard word error rate evaluation metric, and\ndescribe how acoustic features are extracted. We\u2019ll then see how TTS can be mod-\neled with almost the same algorithm in reverse, and conclude with a brief mention\nof other speech tasks.\n16.1 The Automatic Speech Recognition Task\nBefore describing algorithms for ASR, let\u2019s talk about how the task itself varies.\nOne dimension of variation is vocabulary size. Some ASR tasks can be solved with\nextremely high accuracy, like those with a 2-word vocabulary ( yesversus no) or\nan 11 word vocabulary like digit recognition (recognizing sequences of digits in-digit\nrecognition\ncluding zero tonine plus oh). Open-ended tasks like transcribing videos or human\nconversations, with large vocabularies of up to 60,000 words, are much harder.",
    "metadata": {
      "source": "16",
      "chunk_id": 1,
      "token_count": 682,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3",
    "metadata": {
      "source": "16",
      "chunk_id": 2,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "16.1 \u2022 T HEAUTOMATIC SPEECH RECOGNITION TASK 3\nA second dimension of variation is who the speaker is talking to. Humans speak-\ning to machines (either dictating or talking to a dialogue system) are easier to recog-\nnize than humans speaking to humans. Read speech , in which humans are reading read speech\nout loud, for example in audio books, is also relatively easy to recognize. Recog-\nnizing the speech of two humans talking to each other in conversational speech ,conversational\nspeech\nfor example, for transcribing a business meeting, is the hardest. It seems that when\nhumans talk to machines, or read without an audience present, they simplify their\nspeech quite a bit, talking more slowly and more clearly.\nA third dimension of variation is channel and noise. Speech is easier to recognize\nif it\u2019s recorded in a quiet room with head-mounted microphones than if it\u2019s recorded\nby a distant microphone on a noisy city street, or in a car with the window open.\nA \ufb01nal dimension of variation is accent or speaker-class characteristics. Speech\nis easier to recognize if the speaker is speaking the same dialect or variety that the\nsystem was trained on. Speech by speakers of regional or ethnic dialects, or speech\nby children can be quite dif\ufb01cult to recognize if the system is only trained on speak-\ners of standard dialects, or only adult speakers.\nA number of publicly available corpora with human-created transcripts are used\nto create ASR test and training sets to explore this variation; we mention a few of\nthem here since you will encounter them in the literature. LibriSpeech is a large LibriSpeech\nopen-source read-speech 16 kHz dataset with over 1000 hours of audio books from\nthe LibriV ox project, with transcripts aligned at the sentence level (Panayotov et al.,\n2015). It is divided into an easier (\u201cclean\u201d) and a more dif\ufb01cult portion (\u201cother\u201d)\nwith the clean portion of higher recording quality and with accents closer to US\nEnglish. This was done by running a speech recognizer (trained on read speech from\nthe Wall Street Journal) on all the audio, computing the WER for each speaker based\non the gold transcripts, and dividing the speakers roughly in half, with recordings\nfrom lower-WER speakers called \u201cclean\u201d and recordings from higher-WER speakers\n\u201cother\u201d.\nTheSwitchboard corpus of prompted telephone conversations between strangers Switchboard\nwas collected in the early 1990s; it contains 2430 conversations averaging 6 min-\nutes each, totaling 240 hours of 8 kHz speech and about 3 million words (Godfrey\net al., 1992). Switchboard has the singular advantage of an enormous amount of\nauxiliary hand-done linguistic labeling, including parses, dialogue act tags, phonetic\nand prosodic labeling, and discourse and information structure. The CALLHOME CALLHOME\ncorpus was collected in the late 1990s and consists of 120 unscripted 30-minute\ntelephone conversations between native speakers of English who were usually close\nfriends or family (Canavan et al., 1997).\nThe Santa Barbara Corpus of Spoken American English (Du Bois et al., 2005) is\na large corpus of naturally occurring everyday spoken interactions from all over the\nUnited States, mostly face-to-face conversation, but also town-hall meetings, food\npreparation, on-the-job talk, and classroom lectures. The corpus was anonymized by\nremoving personal names and other identifying information (replaced by pseudonyms\nin the transcripts, and masked in the audio).\nCORAAL is a collection of over 150 sociolinguistic interviews with African CORAAL",
    "metadata": {
      "source": "16",
      "chunk_id": 3,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "from lower-WER speakers called \u201cclean\u201d and recordings from higher-WER speakers\n\u201cother\u201d.\nTheSwitchboard corpus of prompted telephone conversations between strangers Switchboard\nwas collected in the early 1990s; it contains 2430 conversations averaging 6 min-\nutes each, totaling 240 hours of 8 kHz speech and about 3 million words (Godfrey\net al., 1992). Switchboard has the singular advantage of an enormous amount of\nauxiliary hand-done linguistic labeling, including parses, dialogue act tags, phonetic\nand prosodic labeling, and discourse and information structure. The CALLHOME CALLHOME\ncorpus was collected in the late 1990s and consists of 120 unscripted 30-minute\ntelephone conversations between native speakers of English who were usually close\nfriends or family (Canavan et al., 1997).\nThe Santa Barbara Corpus of Spoken American English (Du Bois et al., 2005) is\na large corpus of naturally occurring everyday spoken interactions from all over the\nUnited States, mostly face-to-face conversation, but also town-hall meetings, food\npreparation, on-the-job talk, and classroom lectures. The corpus was anonymized by\nremoving personal names and other identifying information (replaced by pseudonyms\nin the transcripts, and masked in the audio).\nCORAAL is a collection of over 150 sociolinguistic interviews with African CORAAL\nAmerican speakers, with the goal of studying African American Language ( AAL ),\nthe many variations of language used in African American communities (Kendall\nand Farrington, 2020). The interviews are anonymized with transcripts aligned at\nthe utterance level. The CHiME Challenge is a series of dif\ufb01cult shared tasks with CHiME\ncorpora that deal with robustness in ASR. The CHiME 5 task, for example, is ASR of\nconversational speech in real home environments (speci\ufb01cally dinner parties). The",
    "metadata": {
      "source": "16",
      "chunk_id": 4,
      "token_count": 416,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\ncorpus contains recordings of twenty different dinner parties in real homes, each\nwith four participants, and in three locations (kitchen, dining area, living room),\nrecorded both with distant room microphones and with body-worn mikes. The\nHKUST Mandarin Telephone Speech corpus has 1206 ten-minute telephone con- HKUST\nversations between speakers of Mandarin across China, including transcripts of the\nconversations, which are between either friends or strangers (Liu et al., 2006). The\nAISHELL-1 corpus contains 170 hours of Mandarin read speech of sentences taken AISHELL-1\nfrom various domains, read by different speakers mainly from northern China (Bu\net al., 2017).\nFigure 16.1 shows the rough percentage of incorrect words (the word error rate ,\nor WER, de\ufb01ned on page 16) from state-of-the-art systems on some of these tasks.\nNote that the error rate on read speech (like the LibriSpeech audiobook corpus) is\naround 2%; this is a solved task, although these numbers come from systems that re-\nquire enormous computational resources. By contrast, the error rate for transcribing\nconversations between humans is much higher; 5.8 to 11% for the Switchboard and\nCALLHOME corpora. The error rate is higher yet again for speakers of varieties\nlike African American Vernacular English, and yet again for dif\ufb01cult conversational\ntasks like transcription of 4-speaker dinner party speech, which can have error rates\nas high as 81.3%. Character error rates (CER) are also much lower for read Man-\ndarin speech than for natural conversation.\nEnglish Tasks WER %\nLibriSpeech audiobooks 960hour clean 1.4\nLibriSpeech audiobooks 960hour other 2.6\nSwitchboard telephone conversations between strangers 5.8\nCALLHOME telephone conversations between family 11.0\nSociolinguistic interviews, CORAAL (AAL) 27.0\nCHiMe5 dinner parties with body-worn microphones 47.9\nCHiMe5 dinner parties with distant microphones 81.3\nChinese (Mandarin) Tasks CER %\nAISHELL-1 Mandarin read speech corpus 6.7\nHKUST Mandarin Chinese telephone conversations 23.5\nFigure 16.1 Rough Word Error Rates (WER = % of words misrecognized) reported around\n2020 for ASR on various American English recognition tasks, and character error rates (CER)\nfor two Chinese recognition tasks.\n16.2 Feature Extraction for ASR: Log Mel Spectrum\nThe \ufb01rst step in ASR is to transform the input waveform into a sequence of acoustic\nfeature vectors , each vector representing the information in a small time window feature vector\nof the signal. Let\u2019s see how to convert a raw wave\ufb01le to the most commonly used\nfeatures, sequences of log mel spectrum vectors. A speech signal processing course\nis recommended for more details.\n16.2.1 Sampling and Quantization\nThe input to a speech recognizer is a complex series of changes in air pressure.\nThese changes in air pressure obviously originate with the speaker and are caused",
    "metadata": {
      "source": "16",
      "chunk_id": 5,
      "token_count": 699,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5",
    "metadata": {
      "source": "16",
      "chunk_id": 6,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "16.2 \u2022 F EATURE EXTRACTION FOR ASR: L OGMELSPECTRUM 5\nby the speci\ufb01c way that air passes through the glottis and out the oral or nasal cav-\nities. We represent sound waves by plotting the change in air pressure over time.\nOne metaphor which sometimes helps in understanding these graphs is that of a ver-\ntical plate blocking the air pressure waves (perhaps in a microphone in front of a\nspeaker\u2019s mouth, or the eardrum in a hearer\u2019s ear). The graph measures the amount\nofcompression orrarefaction (uncompression) of the air molecules at this plate.\nFigure 16.2 shows a short segment of a waveform taken from the Switchboard corpus\nof telephone speech of the vowel [iy] from someone saying \u201cshe just had a baby\u201d.\nTime (s)0 0.03875\u20130.016970.02283\n0\nFigure 16.2 A waveform of an instance of the vowel [iy] (the last vowel in the word \u201cbaby\u201d). The y-axis\nshows the level of air pressure above and below normal atmospheric pressure. The x-axis shows time. Notice\nthat the wave repeats regularly.\nThe \ufb01rst step in digitizing a sound wave like Fig. 16.2 is to convert the analog\nrepresentations (\ufb01rst air pressure and then analog electric signals in a microphone)\ninto a digital signal. This analog-to-digital conversion has two steps: sampling and sampling\nquantization . To sample a signal, we measure its amplitude at a particular time; the\nsampling rate is the number of samples taken per second. To accurately measure a\nwave, we must have at least two samples in each cycle: one measuring the positive\npart of the wave and one measuring the negative part. More than two samples per\ncycle increases the amplitude accuracy, but fewer than two samples causes the fre-\nquency of the wave to be completely missed. Thus, the maximum frequency wave\nthat can be measured is one whose frequency is half the sample rate (since every\ncycle needs two samples). This maximum frequency for a given sampling rate is\ncalled the Nyquist frequency . Most information in human speech is in frequenciesNyquist\nfrequency\nbelow 10,000 Hz; thus, a 20,000 Hz sampling rate would be necessary for com-\nplete accuracy. But telephone speech is \ufb01ltered by the switching network, and only\nfrequencies less than 4,000 Hz are transmitted by telephones. Thus, an 8,000 Hz\nsampling rate is suf\ufb01cient for telephone-bandwidth speech like the Switchboard\ncorpus, while 16,000 Hz sampling is often used for microphone speech.\nAlthough using higher sampling rates produces higher ASR accuracy, we can\u2019t\ncombine different sampling rates for training and testing ASR systems. Thus if\nwe are testing on a telephone corpus like Switchboard (8 KHz sampling), we must\ndownsample our training corpus to 8 KHz. Similarly, if we are training on mul-\ntiple corpora and one of them includes telephone speech, we downsample all the\nwideband corpora to 8Khz.\nAmplitude measurements are stored as integers, either 8 bit (values from -128\u2013\n127) or 16 bit (values from -32768\u201332767). This process of representing real-valued\nnumbers as integers is called quantization ; all values that are closer together than quantization\nthe minimum granularity (the quantum size) are represented identically. We refer to\neach sample at time index nin the digitized, quantized waveform as x[n].\nOnce data is quantized, it is stored in various formats. One parameter of these",
    "metadata": {
      "source": "16",
      "chunk_id": 7,
      "token_count": 774,
      "chapter_title": ""
    }
  },
  {
    "content": "plete accuracy. But telephone speech is \ufb01ltered by the switching network, and only\nfrequencies less than 4,000 Hz are transmitted by telephones. Thus, an 8,000 Hz\nsampling rate is suf\ufb01cient for telephone-bandwidth speech like the Switchboard\ncorpus, while 16,000 Hz sampling is often used for microphone speech.\nAlthough using higher sampling rates produces higher ASR accuracy, we can\u2019t\ncombine different sampling rates for training and testing ASR systems. Thus if\nwe are testing on a telephone corpus like Switchboard (8 KHz sampling), we must\ndownsample our training corpus to 8 KHz. Similarly, if we are training on mul-\ntiple corpora and one of them includes telephone speech, we downsample all the\nwideband corpora to 8Khz.\nAmplitude measurements are stored as integers, either 8 bit (values from -128\u2013\n127) or 16 bit (values from -32768\u201332767). This process of representing real-valued\nnumbers as integers is called quantization ; all values that are closer together than quantization\nthe minimum granularity (the quantum size) are represented identically. We refer to\neach sample at time index nin the digitized, quantized waveform as x[n].\nOnce data is quantized, it is stored in various formats. One parameter of these\nformats is the sample rate and sample size discussed above; telephone speech is\noften sampled at 8 kHz and stored as 8-bit samples, and microphone data is often\nsampled at 16 kHz and stored as 16-bit samples. Another parameter is the number of",
    "metadata": {
      "source": "16",
      "chunk_id": 8,
      "token_count": 342,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\nchannels . For stereo data or for two-party conversations, we can store both channels channel\nin the same \ufb01le or we can store them in separate \ufb01les. A \ufb01nal parameter is individual\nsample storage\u2014linearly or compressed. One common compression format used for\ntelephone speech is m-law (often written u-law but still pronounced mu-law). The\nintuition of log compression algorithms like m-law is that human hearing is more\nsensitive at small intensities than large ones; the log represents small values with\nmore faithfulness at the expense of more error on large values. The linear (unlogged)\nvalues are generally referred to as linear PCM values (PCM stands for pulse code PCM\nmodulation, but never mind that). Here\u2019s the equation for compressing a linear PCM\nsample value xto 8-bit m-law, (where m=255 for 8 bits):\nF(x) =sgn(x)log(1+mjxj)\nlog(1+m)\u00001\u0014x\u00141 (16.1)\nThere are a number of standard \ufb01le formats for storing the resulting digitized wave-\n\ufb01le, such as Microsoft\u2019s .wav and Apple\u2019s AIFF all of which have special headers;\nsimple headerless \u201craw\u201d \ufb01les are also used. For example, the .wav format is a sub-\nset of Microsoft\u2019s RIFF format for multimedia \ufb01les; RIFF is a general format that\ncan represent a series of nested chunks of data and control information. Figure 16.3\nshows a simple .wav \ufb01le with a single data chunk together with its format chunk.\nFigure 16.3 Microsoft wave\ufb01le header format, assuming simple \ufb01le with one chunk. Fol-\nlowing this 44-byte header would be the data chunk.\n16.2.2 Windowing\nFrom the digitized, quantized representation of the waveform, we need to extract\nspectral features from a small window of speech that characterizes part of a par-\nticular phoneme. Inside this small window, we can roughly think of the signal as\nstationary (that is, its statistical properties are constant within this region). (By stationary\ncontrast, in general, speech is a non-stationary signal, meaning that its statistical non-stationary\nproperties are not constant over time). We extract this roughly stationary portion of\nspeech by using a window which is non-zero inside a region and zero elsewhere, run-\nning this window across the speech signal and multiplying it by the input waveform\nto produce a windowed waveform.\nThe speech extracted from each window is called a frame . The windowing is frame\ncharacterized by three parameters: the window size orframe size of the window\n(its width in milliseconds), the frame stride , (also called shift oroffset ) between stride\nsuccessive windows, and the shape of the window.\nTo extract the signal we multiply the value of the signal at time n,s[n]by the\nvalue of the window at time n,w[n]:\ny[n] =w[n]s[n] (16.2)\nThe window shape sketched in Fig. 16.4 is rectangular ; you can see the ex- rectangular\ntracted windowed signal looks just like the original signal. The rectangular window,",
    "metadata": {
      "source": "16",
      "chunk_id": 9,
      "token_count": 708,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\n16.2 \u2022 F EATURE EXTRACTION FOR ASR: L OGMELSPECTRUM 7\nShift10 msWindow25 msShift10 msWindow25 msWindow25 ms\nFigure 16.4 Windowing, showing a 25 ms rectangular window with a 10ms stride.\nhowever, abruptly cuts off the signal at its boundaries, which creates problems when\nwe do Fourier analysis. For this reason, for acoustic feature creation we more com-\nmonly use the Hamming window, which shrinks the values of the signal toward Hamming\nzero at the window boundaries, avoiding discontinuities. Figure 16.5 shows both;\nthe equations are as follows (assuming a window that is Lframes long):\nrectangular w [n] =\u001a1 0\u0014n\u0014L\u00001\n0 otherwise(16.3)\nHamming w [n] =\u001a\n0:54\u00000:46cos (2pn\nL)0\u0014n\u0014L\u00001\n0 otherwise(16.4)\nTime (s)00.0475896\u20130.50.49990Rectangular windowHamming window\nTime (s)0.004559380.0256563\u20130.48260.49990\nTime (s)0.004559380.0256563\u20130.50.49990\nFigure 16.5 Windowing a sine wave with the rectangular or Hamming windows.\n16.2.3 Discrete Fourier Transform\nThe next step is to extract spectral information for our windowed signal; we need to\nknow how much energy the signal contains at different frequency bands. The tool",
    "metadata": {
      "source": "16",
      "chunk_id": 10,
      "token_count": 342,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\nfor extracting spectral information for discrete frequency bands for a discrete-time\n(sampled) signal is the discrete Fourier transform orDFT .Discrete\nFourier\ntransformDFT The input to the DFT is a windowed signal x[n]:::x[m], and the output, for each\nofNdiscrete frequency bands, is a complex number X[k]representing the magni-\ntude and phase of that frequency component in the original signal. If we plot the\nmagnitude against the frequency, we can visualize the spectrum (see Appendix H\nfor more on spectra). For example, Fig. 16.6 shows a 25 ms Hamming-windowed\nportion of a signal and its spectrum as computed by a DFT (with some additional\nsmoothing).\nTime (s)0.0141752 0.039295\u20130.041210.04414\n0\nFrequency (Hz)0 8000Sound pressure level (dB /Hz)\n\u201320020\n(a) (b)\nFigure 16.6 (a) A 25 ms Hamming-windowed portion of a signal from the vowel [iy]\nand (b) its spectrum computed by a DFT.\nWe do not introduce the mathematical details of the DFT here, except to note\nthat Fourier analysis relies on Euler\u2019s formula , with jas the imaginary unit: Euler\u2019s formula\nejq=cosq+jsinq (16.5)\nAs a brief reminder for those students who have already studied signal processing,\nthe DFT is de\ufb01ned as follows:\nX[k] =N\u00001X\nn=0x[n]e\u0000j2p\nNkn(16.6)\nA commonly used algorithm for computing the DFT is the fast Fourier transformfast Fourier\ntransform\norFFT . This implementation of the DFT is very ef\ufb01cient but only works for values FFT\nofNthat are powers of 2.\n16.2.4 Mel Filter Bank and Log\nThe results of the FFT tell us the energy at each frequency band. Human hearing,\nhowever, is not equally sensitive at all frequency bands; it is less sensitive at higher\nfrequencies. This bias toward low frequencies helps human recognition, since in-\nformation in low frequencies (like formants) is crucial for distinguishing vowels or\nnasals, while information in high frequencies (like stop bursts or fricative noise) is\nless crucial for successful recognition. Modeling this human perceptual property\nimproves speech recognition performance in the same way.\nWe implement this intuition by collecting energies, not equally at each frequency\nband, but according to the melscale, an auditory frequency scale. A mel(Stevens mel\net al. 1937, Stevens and V olkmann 1940) is a unit of pitch. Pairs of sounds that are\nperceptually equidistant in pitch are separated by an equal number of mels. The mel",
    "metadata": {
      "source": "16",
      "chunk_id": 11,
      "token_count": 635,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\n16.3 \u2022 S PEECH RECOGNITION ARCHITECTURE 9\nfrequency mcan be computed from the raw acoustic frequency by a log transforma-\ntion:\nmel(f) =1127ln (1+f\n700) (16.7)\nWe implement this intuition by creating a bank of \ufb01lters that collect energy from\neach frequency band, spread logarithmically so that we have very \ufb01ne resolution\nat low frequencies, and less resolution at high frequencies. Figure 16.7 shows a\nsample bank of triangular \ufb01lters that implement this idea, that can be multiplied by\nthe spectrum to get a mel spectrum.\nm1m2mM...mel spectrum0770000.51AmplitudeFrequency (Hz)8K\nFigure 16.7 The mel \ufb01lter bank (Davis and Mermelstein, 1980). Each triangular \ufb01lter,\nspaced logarithmically along the mel scale, collects energy from a given frequency range.\nFinally, we take the log of each of the mel spectrum values. The human response\nto signal level is logarithmic (like the human response to frequency). Humans are\nless sensitive to slight differences in amplitude at high amplitudes than at low ampli-\ntudes. In addition, using a log makes the feature estimates less sensitive to variations\nin input such as power variations due to the speaker\u2019s mouth moving closer or further\nfrom the microphone.\n16.3 Speech Recognition Architecture\nThe basic architecture for ASR is the encoder-decoder (implemented with either\nRNNs or Transformers), exactly the same architecture introduced for MT in Chap-\nter 13. Generally we start from the log mel spectral features described in the previous\nsection, and map to letters, although it\u2019s also possible to map to induced morpheme-\nlike chunks like wordpieces or BPE.\nFig. 16.8 sketches the standard encoder-decoder architecture, which is com-\nmonly referred to as the attention-based encoder decoder orAED , orlisten attend AED\nand spell (LAS ) after the two papers which \ufb01rst applied it to speech (Chorowskilisten attend\nand spell\net al. 2014, Chan et al. 2016). The input is a sequence of tacoustic feature vectors\nF=f1;f2;:::;ft, one vector per 10 ms frame. The output can be letters or word-\npieces; we\u2019ll assume letters here. Thus the output sequence Y= (hSOSi;y1;:::;ymhEOSi),\nassuming special start of sequence and end of sequence tokens hsosiandheosiand\neach yiis a character; for English we might choose the set:\nyi2fa;b;c;:::;z;0;:::;9;hspacei;hcommai;hperiodi;hapostrophei;hunkig\nOf course the encoder-decoder architecture is particularly appropriate when in-\nput and output sequences have stark length differences, as they do for speech, with\nvery long acoustic feature sequences mapping to much shorter sequences of letters",
    "metadata": {
      "source": "16",
      "chunk_id": 12,
      "token_count": 653,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\nENCODER\u2026DECODER\u2026\u2026ym\nFeature ComputationSubsampling\u2026Hftf180-dimensional log Mel spectrumper frameShorter sequence Xy1<s>iy2ity3t\u2018y4\u2018sy5s y6 ty7tiy8imy9mex1xn\nFigure 16.8 Schematic architecture for an encoder-decoder speech recognizer.\nor words. A single word might be 5 letters long but, supposing it lasts about 2\nseconds, would take 200 acoustic frames (of 10ms each).\nBecause this length difference is so extreme for speech, encoder-decoder ar-\nchitectures for speech need to have a special compression stage that shortens the\nacoustic feature sequence before the encoder stage. (Alternatively, we can use a loss\nfunction that is designed to deal well with compression, like the CTC loss function\nwe\u2019ll introduce in the next section.)\nThe goal of the subsampling is to produce a shorter sequence X=x1;:::;xnthat\nwill be the input to the encoder. The simplest algorithm is a method sometimes\ncalled low frame rate (Pundak and Sainath, 2016): for time iwe stack (concatenate) low frame rate\nthe acoustic feature vector fiwith the prior two vectors fi\u00001andfi\u00002to make a new\nvector three times longer. Then we simply delete fi\u00001and fi\u00002. Thus instead of\n(say) a 40-dimensional acoustic feature vector every 10 ms, we have a longer vector\n(say 120-dimensional) every 30 ms, with a shorter sequence length n=t\n3.1\nAfter this compression stage, encoder-decoders for speech use the same archi-\ntecture as for MT or other text, composed of either RNNs (LSTMs) or Transformers.\nFor inference, the probability of the output string Yis decomposed as:\np(y1;:::; yn) =nY\ni=1p(yijy1;:::; yi\u00001;X) (16.8)\nWe can produce each letter of the output via greedy decoding:\n\u02c6yi=argmax char2Alphabet P(charjy1:::yi\u00001;X) (16.9)\nAlternatively we can use beam search as described in the next section. This is par-\nticularly relevant when we are adding a language model.\nAdding a language model Since an encoder-decoder model is essentially a con-\nditional language model, encoder-decoders implicitly learn a language model for the\noutput domain of letters from their training data. However, the training data (speech\npaired with text transcriptions) may not include suf\ufb01cient text to train a good lan-\nguage model. After all, it\u2019s easier to \ufb01nd enormous amounts of pure text training\n1There are also more complex alternatives for subsampling, like using a convolutional net that down-\nsamples with max pooling, or layers of pyramidal RNNs , RNNs where each successive layer has half\nthe number of RNNs as the previous layer.",
    "metadata": {
      "source": "16",
      "chunk_id": 13,
      "token_count": 676,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\n16.4 \u2022 CTC 11\ndata than it is to \ufb01nd text paired with speech. Thus we can can usually improve a\nmodel at least slightly by incorporating a very large language model.\nThe simplest way to do this is to use beam search to get a \ufb01nal beam of hy-\npothesized sentences; this beam is sometimes called an n-best list . We then use a n-best list\nlanguage model to rescore each hypothesis on the beam. The scoring is done by in- rescore\nterpolating the score assigned by the language model with the encoder-decoder score\nused to create the beam, with a weight ltuned on a held-out set. Also, since most\nmodels prefer shorter sentences, ASR systems normally have some way of adding a\nlength factor. One way to do this is to normalize the probability by the number of\ncharacters in the hypothesis jYjc. The following is thus a typical scoring function\n(Chan et al., 2016):\nscore(YjX) =1\njYjclogP(YjX)+llogPLM(Y) (16.10)\n16.3.1 Learning\nEncoder-decoders for speech are trained with the normal cross-entropy loss gener-\nally used for conditional language models. At timestep iof decoding, the loss is the\nlog probability of the correct token (letter) yi:\nLCE=\u0000logp(yijy1;:::; yi\u00001;X) (16.11)\nThe loss for the entire sentence is the sum of these losses:\nLCE=\u0000mX\ni=1logp(yijy1;:::; yi\u00001;X) (16.12)\nThis loss is then backpropagated through the entire end-to-end model to train the\nentire encoder-decoder.\nAs we described in Chapter 13, we normally use teacher forcing, in which the\ndecoder history is forced to be the correct gold yirather than the predicted \u02c6 yi. It\u2019s\nalso possible to use a mixture of the gold and decoder output, for example using\nthe gold output 90% of the time, but with probability .1 taking the decoder output\ninstead:\nLCE=\u0000logp(yijy1;:::; \u02c6yi\u00001;X) (16.13)\n16.4 CTC\nWe pointed out in the previous section that speech recognition has two particular\nproperties that make it very appropriate for the encoder-decoder architecture, where\nthe encoder produces an encoding of the input that the decoder uses attention to\nexplore. First, in speech we have a very long acoustic input sequence Xmapping to\na much shorter sequence of letters Y, and second, it\u2019s hard to know exactly which\npart of Xmaps to which part of Y.\nIn this section we brie\ufb02y introduce an alternative to encoder-decoder: an algo-\nrithm and loss function called CTC , short for Connectionist Temporal Classi\ufb01ca- CTC\ntion(Graves et al., 2006), that deals with these problems in a very different way. The\nintuition of CTC is to output a single character for every frame of the input, so that",
    "metadata": {
      "source": "16",
      "chunk_id": 14,
      "token_count": 681,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\nthe output is the same length as the input, and then to apply a collapsing function\nthat combines sequences of identical letters, resulting in a shorter sequence.\nLet\u2019s imagine inference on someone saying the word dinner , and let\u2019s suppose\nwe had a function that chooses the most probable letter for each input spectral frame\nrepresentation xi. We\u2019ll call the sequence of letters corresponding to each input\nframe an alignment , because it tells us where in the acoustic signal each letter aligns alignment\nto. Fig. 16.9 shows one such alignment, and what happens if we use a collapsing\nfunction that just removes consecutive duplicate letters.\nX (input)A (alignment)Y (output)dx1ix2ix3nx4nx5nx6nx7ex8rx9rx10rx11rx12rx13rx14dinerwavefile\nFigure 16.9 A naive algorithm for collapsing an alignment between input and letters.\nWell, that doesn\u2019t work; our naive algorithm has transcribed the speech as diner ,\nnotdinner ! Collapsing doesn\u2019t handle double letters. There\u2019s also another problem\nwith our naive function; it doesn\u2019t tell us what symbol to align with silence in the\ninput. We don\u2019t want to be transcribing silence as random letters!\nThe CTC algorithm solves both problems by adding to the transcription alphabet\na special symbol for a blank , which we\u2019ll represent as . The blank can be used in blank\nthe alignment whenever we don\u2019t want to transcribe a letter. Blank can also be used\nbetween letters; since our collapsing function collapses only consecutive duplicate\nletters, it won\u2019t collapse across . More formally, let\u2019s de\ufb01ne the mapping B:a!y\nbetween an alignment aand an output y, which collapses all repeated letters and\nthen removes all blanks. Fig. 16.10 sketches this collapsing function B.\nX (input)A (alignment)remove blanksdx1ix2x3nx4nx5x6nx7ex8rx9rx10rx11rx12x13x14dinernmerge duplicatesdinernY (output)dinern\u2423\u2423\u2423\u2423\u2423\u2423\u2423\nFigure 16.10 The CTC collapsing function B, showing the space blank character ; re-\npeated (consecutive) characters in an alignment Aare removed to form the output Y.\nThe CTC collapsing function is many-to-one; lots of different alignments map\nto the same output string. For example, the alignment shown in Fig. 16.10 is not\nthe only alignment that results in the string dinner . Fig. 16.11 shows some other\nalignments that would produce the same output.\nIt\u2019s useful to think of the set of all alignments that might produce the same output\nY. We\u2019ll use the inverse of our Bfunction, called B\u00001, and represent that set as\nB\u00001(Y).",
    "metadata": {
      "source": "16",
      "chunk_id": 15,
      "token_count": 633,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\n16.4 \u2022 CTC 13\ndinnneeerrr\u2423\u2423ddinnnerr\u2423\u2423\u2423dddinnnerr\u2423i\u2423\u2423\u2423\u2423\u2423\nFigure 16.11 Three other legitimate alignments producing the transcript dinner .\n16.4.1 CTC Inference\nBefore we see how to compute PCTC(YjX)let\u2019s \ufb01rst see how CTC assigns a proba-\nbility to one particular alignment \u02c6A=f\u02c6a1;:::; \u02c6ang. CTC makes a strong conditional\nindependence assumption: it assumes that, given the input X, the CTC model output\natat time tis independent of the output labels at any other time ai. Thus:\nPCTC(AjX) =TY\nt=1p(atjX) (16.14)\nThus to \ufb01nd the best alignment \u02c6A=f\u02c6a1;:::; \u02c6aTgwe can greedily choose the charac-\nter with the max probability at each time step t:\n\u02c6at=argmax\nc2Cpt(cjX) (16.15)\nWe then pass the resulting sequence Ato the CTC collapsing function Bto get the\noutput sequence Y.\nLet\u2019s talk about how this simple inference algorithm for \ufb01nding the best align-\nment A would be implemented. Because we are making a decision at each time\npoint, we can treat CTC as a sequence-modeling task, where we output one letter\n\u02c6ytat time tcorresponding to each input token xt, eliminating the need for a full de-\ncoder. Fig. 16.12 sketches this architecture, where we take an encoder, produce a\nhidden state htat each timestep, and decode by taking a softmax over the character\nvocabulary at each time step.\nENCODER\u2026yn\nFeature ComputationSubsampling\u2026ftf1 log Mel spectrumShorter inputsequence Xy1iy2iy3iy4tx1xnClassi\ufb01er+softmax\u2026ty5\u2026\u2026output lettersequence Y\nFigure 16.12 Inference with CTC: using an encoder-only model, with decoding done by\nsimple softmaxes over the hidden state htat each output step.\nAlas, there is a potential \ufb02aw with the inference algorithm sketched in (Eq. 16.15)\nand Fig. 16.11. The problem is that we chose the most likely alignment A, but the",
    "metadata": {
      "source": "16",
      "chunk_id": 16,
      "token_count": 547,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n14 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\nmost likely alignment may not correspond to the most likely \ufb01nal collapsed output\nstring Y. That\u2019s because there are many possible alignments that lead to the same\noutput string, and hence the most likely output string might not correspond to the\nmost probable alignment. For example, imagine the most probable alignment Afor\nan input X= [x1x2x3]is the string [a b \u000f] but the next two most probable alignments\nare [b\u000fb] and [\u000fb b]. The output Y=[b b], summing over those two alignments,\nmight be more probable than Y=[a b].\nFor this reason, the most probable output sequence Yis the one that has, not\nthe single best CTC alignment, but the highest sum over the probability of all its\npossible alignments:\nPCTC(YjX) =X\nA2B\u00001(Y)P(AjX)\n=X\nA2B\u00001(Y)TY\nt=1p(atjht)\n\u02c6Y=argmax\nYPCTC(YjX) (16.16)\nAlas, summing over all alignments is very expensive (there are a lot of alignments),\nso we approximate this sum by using a version of Viterbi beam search that cleverly\nkeeps in the beam the high-probability alignments that map to the same output string,\nand sums those as an approximation of (Eq. 16.16). See Hannun (2017) for a clear\nexplanation of this extension of beam search for CTC.\nBecause of the strong conditional independence assumption mentioned earlier\n(that the output at time tis independent of the output at time t\u00001, given the input),\nCTC does not implicitly learn a language model over the data (unlike the attention-\nbased encoder-decoder architectures). It is therefore essential when using CTC to\ninterpolate a language model (and some sort of length factor L(Y)) using interpola-\ntion weights that are trained on a devset:\nscore CTC(YjX) =logPCTC(YjX)+l1logPLM(Y)l2L(Y) (16.17)\n16.4.2 CTC Training\nTo train a CTC-based ASR system, we use negative log-likelihood loss with a special\nCTC loss function. Thus the loss for an entire dataset Dis the sum of the negative\nlog-likelihoods of the correct output Yfor each input X:\nLCTC=X\n(X;Y)2D\u0000logPCTC(YjX) (16.18)\nTo compute CTC loss function for a single input pair (X;Y), we need the probability\nof the output Ygiven the input X. As we saw in Eq. 16.16, to compute the probability\nof a given output Ywe need to sum over all the possible alignments that would\ncollapse to Y. In other words:\nPCTC(YjX) =X\nA2B\u00001(Y)TY\nt=1p(atjht) (16.19)\nNaively summing over all possible alignments is not feasible (there are too many\nalignments). However, we can ef\ufb01ciently compute the sum by using dynamic pro-",
    "metadata": {
      "source": "16",
      "chunk_id": 17,
      "token_count": 705,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\n16.4 \u2022 CTC 15\ngramming to merge alignments, with a version of the forward-backward algo-\nrithm also used to train HMMs (Appendix A) and CRFs. The original dynamic pro-\ngramming algorithms for both training and inference are laid out in (Graves et al.,\n2006); see (Hannun, 2017) for a detailed explanation of both.\n16.4.3 Combining CTC and Encoder-Decoder\nIt\u2019s also possible to combine the two architectures/loss functions we\u2019ve described,\nthe cross-entropy loss from the encoder-decoder architecture, and the CTC loss.\nFig. 16.13 shows a sketch. For training, we can simply weight the two losses with a\nltuned on a devset:\nL=\u0000llogPencdec(YjX)\u0000(1\u0000l)logPctc(YjX) (16.20)\nFor inference, we can combine the two with the language model (or the length\npenalty), again with learned weights:\n\u02c6Y=argmax\nY[llogPencdec(YjX)\u0000(1\u0000l)logPCTC(YjX)+glogPLM(Y)](16.21)\nENCODER\u2026DECODER\u2026H<s>it\u2018s timx1xn\u2026\u2026i   t   \u2019   s      t   i   m   e  \u2026CTC LossEncoder-Decoder Loss\nFigure 16.13 Combining the CTC and encoder-decoder loss functions.\n16.4.4 Streaming Models: RNN-T for improving CTC\nBecause of the strong independence assumption in CTC (assuming that the output\nat time tis independent of the output at time t\u00001), recognizers based on CTC\ndon\u2019t achieve as high an accuracy as the attention-based encoder-decoder recog-\nnizers. CTC recognizers have the advantage, however, that they can be used for\nstreaming . Streaming means recognizing words on-line rather than waiting until streaming\nthe end of the sentence to recognize them. Streaming is crucial for many applica-\ntions, from commands to dictation, where we want to start recognition while the\nuser is still talking. Algorithms that use attention need to compute the hidden state\nsequence over the entire input \ufb01rst in order to provide the attention distribution con-\ntext, before the decoder can start decoding. By contrast, a CTC algorithm can input\nletters from left to right immediately.\nIf we want to do streaming, we need a way to improve CTC recognition to re-\nmove the conditional independent assumption, enabling it to know about output his-\ntory. The RNN-Transducer ( RNN-T ), shown in Fig. 16.14, is just such a model RNN-T\n(Graves 2012, Graves et al. 2013). The RNN-T has two main components: a CTC\nacoustic model, and a separate language model component called the predictor that\nconditions on the output token history. At each time step t, the CTC encoder outputs",
    "metadata": {
      "source": "16",
      "chunk_id": 18,
      "token_count": 652,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\n16 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\na hidden state henc\ntgiven the input x1:::xt. The language model predictor takes as in-\nput the previous output token (not counting blanks), outputting a hidden state hpred\nu.\nThe two are passed through another network whose output is then passed through a\nsoftmax to predict the next character.\nPRNN\u0000T(YjX) =X\nA2B\u00001(Y)P(AjX)\n=X\nA2B\u00001(Y)TY\nt=1p(atjht;y<ut)\nENCODERP ( yt,u | x[1..t] , y[1..u-1] )\nxtPREDICTIONNETWORKyu-1JOINT NETWORKhencthpreduSOFTMAXzt,uDECODER\nFigure 16.14 The RNN-T model computing the output token distribution at time tby inte-\ngrating the output of a CTC acoustic encoder and a separate \u2018predictor\u2019 language model.\n16.5 ASR Evaluation: Word Error Rate\nThe standard evaluation metric for speech recognition systems is the word error word error\nrate. The word error rate is based on how much the word string returned by the\nrecognizer (the hypothesized word string) differs from a reference transcription.\nThe \ufb01rst step in computing word error is to compute the minimum edit distance in\nwords between the hypothesized and correct strings, giving us the minimum num-\nber of word substitutions , word insertions , and word deletions necessary to map\nbetween the correct and hypothesized strings. The word error rate (WER) is then\nde\ufb01ned as follows (note that because the equation includes insertions, the error rate\ncan be greater than 100%):\nWord Error Rate =100\u0002Insertions +Substitutions +Deletions\nTotal Words in Correct Transcript\nHere is a sample alignment between a reference and a hypothesis utterance from alignment\nthe CallHome corpus, showing the counts used to compute the error rate:\nREF: i *** ** UM the PHONE IS i LEFT THE portable **** PHONE UPSTAIRS last night\nHYP: i GOT IT TO the ***** FULLEST i LOVE TO portable FORM OF STORES last night\nEval: I I S D S S S I S S\nThis utterance has six substitutions, three insertions, and one deletion:\nWord Error Rate =1006+3+1\n13=76:9%",
    "metadata": {
      "source": "16",
      "chunk_id": 19,
      "token_count": 526,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17",
    "metadata": {
      "source": "16",
      "chunk_id": 20,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "16.5 \u2022 ASR E VALUATION : W ORD ERROR RATE 17\nThe standard method for computing word error rates is a free script called sclite ,\navailable from the National Institute of Standards and Technologies (NIST) (NIST,\n2005). Sclite is given a series of reference (hand-transcribed, gold-standard) sen-\ntences and a matching set of hypothesis sentences. Besides performing alignments,\nand computing word error rate, sclite performs a number of other useful tasks. For\nexample, for error analysis it gives useful information such as confusion matrices\nshowing which words are often misrecognized for others, and summarizes statistics\nof words that are often inserted or deleted. sclite also gives error rates by speaker\n(if sentences are labeled for speaker ID), as well as useful statistics like the sentence\nerror rate , the percentage of sentences with at least one word error.Sentence error\nrate\nStatistical signi\ufb01cance for ASR: MAPSSWE or MacNemar\nAs with other language processing algorithms, we need to know whether a particular\nimprovement in word error rate is signi\ufb01cant or not.\nThe standard statistical tests for determining if two word error rates are different\nis the Matched-Pair Sentence Segment Word Error (MAPSSWE) test, introduced in\nGillick and Cox (1989).\nThe MAPSSWE test is a parametric test that looks at the difference between\nthe number of word errors the two systems produce, averaged across a number of\nsegments. The segments may be quite short or as long as an entire utterance; in\ngeneral, we want to have the largest number of (short) segments in order to justify\nthe normality assumption and to maximize power. The test requires that the errors\nin one segment be statistically independent of the errors in another segment. Since\nASR systems tend to use trigram LMs, we can approximate this requirement by\nde\ufb01ning a segment as a region bounded on both sides by words that both recognizers\nget correct (or by turn/utterance boundaries). Here\u2019s an example from NIST (2007)\nwith four regions:\nI II III IV\nREF: |it was|the best|of|times it|was the worst|of times| |it was\n| | | | | | | |\nSYS A:|ITS |the best|of|times it|IS the worst |of times|OR|it was\n| | | | | | | |\nSYS B:|it was|the best| |times it|WON the TEST |of times| |it was\nIn region I, system A has two errors (a deletion and an insertion) and system B\nhas zero; in region III, system A has one error (a substitution) and system B has two.\nLet\u2019s de\ufb01ne a sequence of variables Zrepresenting the difference between the errors\nin the two systems as follows:\nNi\nAthe number of errors made on segment iby system A\nNi\nB the number of errors made on segment iby system B\nZ Ni\nA\u0000Ni\nB;i=1;2;\u0001\u0001\u0001;nwhere nis the number of segments\nIn the example above, the sequence of Zvalues isf2;\u00001;\u00001;1g. Intuitively, if\nthe two systems are identical, we would expect the average difference, that is, the\naverage of the Zvalues, to be zero. If we call the true average of the differences\nmuz, we would thus like to know whether muz=0. Following closely the original\nproposal and notation of Gillick and Cox (1989), we can estimate the true average",
    "metadata": {
      "source": "16",
      "chunk_id": 21,
      "token_count": 776,
      "chapter_title": ""
    }
  },
  {
    "content": "SYS A:|ITS |the best|of|times it|IS the worst |of times|OR|it was\n| | | | | | | |\nSYS B:|it was|the best| |times it|WON the TEST |of times| |it was\nIn region I, system A has two errors (a deletion and an insertion) and system B\nhas zero; in region III, system A has one error (a substitution) and system B has two.\nLet\u2019s de\ufb01ne a sequence of variables Zrepresenting the difference between the errors\nin the two systems as follows:\nNi\nAthe number of errors made on segment iby system A\nNi\nB the number of errors made on segment iby system B\nZ Ni\nA\u0000Ni\nB;i=1;2;\u0001\u0001\u0001;nwhere nis the number of segments\nIn the example above, the sequence of Zvalues isf2;\u00001;\u00001;1g. Intuitively, if\nthe two systems are identical, we would expect the average difference, that is, the\naverage of the Zvalues, to be zero. If we call the true average of the differences\nmuz, we would thus like to know whether muz=0. Following closely the original\nproposal and notation of Gillick and Cox (1989), we can estimate the true average\nfrom our limited sample as \u02c6mz=Pn\ni=1Zi=n. The estimate of the variance of the Zi\u2019s\nis\ns2\nz=1\nn\u00001nX\ni=1(Zi\u0000mz)2(16.22)",
    "metadata": {
      "source": "16",
      "chunk_id": 22,
      "token_count": 351,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\n18 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\nLet\nW=\u02c6mz\nsz=pn(16.23)\nFor a large enough n(>50),Wwill approximately have a normal distribution with\nunit variance. The null hypothesis is H0:mz=0, and it can thus be rejected if\n2\u0003P(Z\u0015jwj)\u00140:05 (two-tailed) or P(Z\u0015jwj)\u00140:05 (one-tailed), where Zis\nstandard normal and wis the realized value W; these probabilities can be looked up\nin the standard tables of the normal distribution.\nEarlier work sometimes used McNemar\u2019s test for signi\ufb01cance, but McNemar\u2019s McNemar\u2019s test\nis only applicable when the errors made by the system are independent, which is not\ntrue in continuous speech recognition, where errors made on a word are extremely\ndependent on errors made on neighboring words.\nCould we improve on word error rate as a metric? It would be nice, for exam-\nple, to have something that didn\u2019t give equal weight to every word, perhaps valuing\ncontent words like Tuesday more than function words like aorof. While researchers\ngenerally agree that this would be a good idea, it has proved dif\ufb01cult to agree on\na metric that works in every application of ASR. For dialogue systems, however,\nwhere the desired semantic output is more clear, a metric called slot error rate or\nconcept error rate has proved extremely useful; it is discussed in Chapter 15 on page\n??.\n16.6 TTS\nThe goal of text-to-speech (TTS) systems is to map from strings of letters to wave-\nforms, a technology that\u2019s important for a variety of applications from dialogue sys-\ntems to games to education.\nLike ASR systems, TTS systems are generally based on the encoder-decoder\narchitecture, either using LSTMs or Transformers. There is a general difference in\ntraining. The default condition for ASR systems is to be speaker-independent: they\nare trained on large corpora with thousands of hours of speech from many speakers\nbecause they must generalize well to an unseen test speaker. By contrast, in TTS, it\u2019s\nless crucial to use multiple voices, and so basic TTS systems are speaker-dependent:\ntrained to have a consistent voice, on much less data, but all from one speaker. For\nexample, one commonly used public domain dataset, the LJ speech corpus, consists\nof 24 hours of one speaker, Linda Johnson, reading audio books in the LibriV ox\nproject (Ito and Johnson, 2017), much smaller than standard ASR corpora which are\nhundreds or thousands of hours.2\nWe generally break up the TTS task into two components. The \ufb01rst component\nis an encoder-decoder model for spectrogram prediction : it maps from strings of\nletters to mel spectrographs: sequences of mel spectral values over time. Thus we\n2There is also recent TTS research on the task of multi-speaker TTS, in which a system is trained on\nspeech from many speakers, and can switch between different voices.",
    "metadata": {
      "source": "16",
      "chunk_id": 23,
      "token_count": 677,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\n16.6 \u2022 TTS 19\nmight map from this string:\nIt's time for lunch!\nto the following mel spectrogram:\nThe second component maps from mel spectrograms to waveforms. Generating\nwaveforms from intermediate representations like spectrograms is called vocoding vocoding\nand this second component is called a vocoder : vocoder\nThese standard encoder-decoder algorithms for TTS are still quite computation-\nally intensive, so a signi\ufb01cant focus of modern research is on ways to speed them\nup.\n16.6.1 TTS Preprocessing: Text normalization\nBefore either of these two steps, however, TTS systems require text normaliza-\ntion preprocessing for handling non-standard words : numbers, monetary amounts,non-standard\nwords\ndates, and other concepts that are verbalized differently than they are spelled. A TTS\nsystem seeing a number like 151needs to know to verbalize it as one hundred \ufb01fty\noneif it occurs as $151 but as one \ufb01fty one if it occurs in the context 151 Chapulte-\npec Ave. . The number 1750 can be spoken in at least four different ways, depending\non the context:\nseventeen fifty: (in\u201cThe European economy in 1750\u201d )\none seven five zero: (in\u201cThe password is 1750\u201d )\nseventeen hundred and fifty: (in\u201c1750 dollars\u201d )\none thousand, seven hundred, and fifty: (in\u201c1750 dollars\u201d )\nOften the verbalization of a non-standard word depends on its meaning (what\nTaylor (2009) calls its semiotic class ). Fig. 16.15 lays out some English non-\nstandard word types.\nMany classes have preferred realizations. A year is generally read as paired\ndigits (e.g., seventeen fifty for 1750). $3.2 billion must be read out with the\nword dollars at the end, as three point two billion dollars . Some ab-\nbreviations like N.Y. are expanded (to New York ), while other acronyms like GPU\nare pronounced as letter sequences. In languages with grammatical gender, normal-\nization may depend on morphological properties. In French, the phrase 1 mangue\n(\u2018one mangue\u2019) is normalized to une mangue , but 1 ananas (\u2018one pineapple\u2019) is\nnormalized to un ananas . In German, Heinrich IV (\u2018Henry IV\u2019) can be normalized\ntoHeinrich der Vierte ,Heinrich des Vierten ,Heinrich dem Vierten , or\nHeinrich den Vierten depending on the grammatical case of the noun (Demberg,\n2006).",
    "metadata": {
      "source": "16",
      "chunk_id": 24,
      "token_count": 560,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20",
    "metadata": {
      "source": "16",
      "chunk_id": 25,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "20 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\nsemiotic class examples verbalization\nabbreviations gov\u2019t , N.Y., mph government\nacronyms read as letters GPU , D.C., PC, UN, IBM G P U\ncardinal numbers 12, 45, 1/2, 0.6 twelve\nordinal numbers May 7, 3rd, Bill Gates III seventh\nnumbers read as digits Room 101 one oh one\ntimes 3.20, 11:45 eleven forty \ufb01ve\ndates 28/02 (or in US, 2/28) February twenty eighth\nyears 1999 , 80s, 1900s, 2045 nineteen ninety nine\nmoney $3.45 ,e250, $200K three dollars forty \ufb01ve\nmoney in tr/m/billions $3.45 billion three point four \ufb01ve billion dollars\npercentage 75% 3.4% seventy \ufb01ve percent\nFigure 16.15 Some types of non-standard words in text normalization; see Sproat et al.\n(2001) and (van Esch and Sproat, 2018) for many more.\nModern end-to-end TTS systems can learn to do some normalization themselves,\nbut TTS systems are only trained on a limited amount of data (like the 220,000 words\nwe mentioned above for the LJ corpus (Ito and Johnson, 2017)), and so a separate\nnormalization step is important.\nNormalization can be done by rule or by an encoder-decoder model. Rule-based\nnormalization is done in two stages: tokenization and verbalization. In the tokeniza-\ntion stage we hand-write rules to detect non-standard words. These can be regular\nexpressions, like the following for detecting years:\n/(1[89][0-9][0-9])j(20[0-9][0-9]/\nA second pass of rules express how to verbalize each semiotic class. Larger TTS\nsystems instead use more complex rule-systems, like the Kestral system of (Ebden\nand Sproat, 2015), which \ufb01rst classi\ufb01es and parses each input into a normal form\nand then produces text using a verbalization grammar. Rules have the advantage\nthat they don\u2019t require training data, and they can be designed for high precision, but\ncan be brittle, and require expert rule-writers so are hard to maintain.\nThe alternative model is to use encoder-decoder models, which have been shown\nto work better than rules for such transduction tasks, but do require expert-labeled\ntraining sets in which non-standard words have been replaced with the appropriate\nverbalization; such training sets for some languages are available (Sproat and Gor-\nman 2018, Zhang et al. 2019).\nIn the simplest encoder-decoder setting, we simply treat the problem like ma-\nchine translation, training a system to map from:\nThey live at 224 Mission St.\nto\nThey live at two twenty four Mission Street\nWhile encoder-decoder algorithms are highly accurate, they occasionally pro-\nduce errors that are egregious; for example normalizing 45 minutes asforty \ufb01ve mil-\nlimeters . To address this, more complex systems use mechanisms like lightweight\ncovering grammars , which enumerate a large set of possible verbalizations but\ndon\u2019t try to disambiguate, to constrain the decoding to avoid such outputs (Zhang\net al., 2019).\n16.6.2 TTS: Spectrogram prediction\nThe exact same architecture we described for ASR\u2014the encoder-decoder with attention\u2013",
    "metadata": {
      "source": "16",
      "chunk_id": 26,
      "token_count": 775,
      "chapter_title": ""
    }
  },
  {
    "content": "that they don\u2019t require training data, and they can be designed for high precision, but\ncan be brittle, and require expert rule-writers so are hard to maintain.\nThe alternative model is to use encoder-decoder models, which have been shown\nto work better than rules for such transduction tasks, but do require expert-labeled\ntraining sets in which non-standard words have been replaced with the appropriate\nverbalization; such training sets for some languages are available (Sproat and Gor-\nman 2018, Zhang et al. 2019).\nIn the simplest encoder-decoder setting, we simply treat the problem like ma-\nchine translation, training a system to map from:\nThey live at 224 Mission St.\nto\nThey live at two twenty four Mission Street\nWhile encoder-decoder algorithms are highly accurate, they occasionally pro-\nduce errors that are egregious; for example normalizing 45 minutes asforty \ufb01ve mil-\nlimeters . To address this, more complex systems use mechanisms like lightweight\ncovering grammars , which enumerate a large set of possible verbalizations but\ndon\u2019t try to disambiguate, to constrain the decoding to avoid such outputs (Zhang\net al., 2019).\n16.6.2 TTS: Spectrogram prediction\nThe exact same architecture we described for ASR\u2014the encoder-decoder with attention\u2013\ncan be used for the \ufb01rst component of TTS. Here we\u2019ll give a simpli\ufb01ed overview",
    "metadata": {
      "source": "16",
      "chunk_id": 27,
      "token_count": 305,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21",
    "metadata": {
      "source": "16",
      "chunk_id": 28,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "16.6 \u2022 TTS 21\nof the Tacotron2 architecture (Shen et al., 2018), which extends the earlier Tacotron Tacotron2\n(Wang et al., 2017) architecture and the Wavenet vocoder (van den Oord et al., Wavenet\n2016). Fig. 16.16 sketches out the entire architecture.\nThe encoder\u2019s job is to take a sequence of letters and produce a hidden repre-\nsentation representing the letter sequence, which is then used by the attention mech-\nanism in the decoder. The Tacotron2 encoder \ufb01rst maps every input grapheme to\na 512-dimensional character embedding. These are then passed through a stack\nof 3 convolutional layers, each containing 512 \ufb01lters with shape 5 \u00021, i.e. each\n\ufb01lter spanning 5 characters, to model the larger letter context. The output of the\n\ufb01nal convolutional layer is passed through a biLSTM to produce the \ufb01nal encod-\ning. It\u2019s common to use a slightly higher quality (but slower) version of attention\ncalled location-based attention , in which the computation of the avalues (Eq. ??location-based\nattention\nin Chapter 8) makes use of the avalues from the prior time-state.\nIn the decoder, the predicted mel spectrum from the prior time slot is passed\nthrough a small pre-net as a bottleneck. This prior output is then concatenated with\nthe encoder\u2019s attention vector context and passed through 2 LSTM layers. The out-\nput of this LSTM is used in two ways. First, it is passed through a linear layer, and\nsome output processing, to autoregressively predict one 80-dimensional log-mel \ufb01l-\nterbank vector frame (50 ms, with a 12.5 ms stride) at each step. Second, it is passed\nthrough another linear layer to a sigmoid to make a \u201cstop token prediction\u201d decision\nabout whether to stop producing output.",
    "metadata": {
      "source": "16",
      "chunk_id": 29,
      "token_count": 421,
      "chapter_title": ""
    }
  },
  {
    "content": "While linear spectrograms discard phase information (and aretherefore lossy), algorithms such as Grif\ufb01n-Lim [14] are capable ofestimating this discarded information, which enables time-domainconversion via the inverse short-time Fourier transform. Mel spectro-grams discard even more information, presenting a challenging in-verse problem. However, in comparison to the linguistic and acousticfeatures used in WaveNet, the mel spectrogram is a simpler, lower-level acoustic representation of audio signals. It should thereforebe straightforward for a similar WaveNet model conditioned on melspectrograms to generate audio, essentially as a neural vocoder. In-deed, we will show that it is possible to generate high quality audiofrom mel spectrograms using a modi\ufb01ed WaveNet architecture.2.2. Spectrogram Prediction NetworkAs in Tacotron, mel spectrograms are computed through a short-time Fourier transform (STFT) using a 50 ms frame size, 12.5 msframe hop, and a Hann window function. We experimented with a5 ms frame hop to match the frequency of the conditioning inputsin the original WaveNet, but the corresponding increase in temporalresolution resulted in signi\ufb01cantly more pronunciation issues.We transform the STFT magnitude to the mel scale using an 80channel mel \ufb01lterbank spanning 125 Hz to 7.6 kHz, followed by logdynamic range compression. Prior to log compression, the \ufb01lterbankoutput magnitudes are clipped to a minimum value of 0.01 in orderto limit dynamic range in the logarithmic domain.The network is composed of an encoder and a decoder with atten-tion. The encoder converts a character sequence into a hidden featurerepresentation which the decoder consumes to predict a spectrogram.Input characters are represented using a learned 512-dimensionalcharacter embedding, which are passed through a stack of 3 convolu-tional layers each containing 512 \ufb01lters with shape5\u21e51, i.e., whereeach \ufb01lter spans 5 characters, followed by batch normalization [18]and ReLU activations. As in Tacotron, these convolutional layersmodel longer-term context (e.g.,N-grams) in the input charactersequence. The output of the \ufb01nal convolutional layer is passed into asingle bi-directional [19] LSTM [20] layer containing 512 units (256in each direction) to generate the encoded features.The encoder output is consumed by an attention network whichsummarizes the full encoded sequence as a \ufb01xed-length context vectorfor each decoder output step. We use the location-sensitive attentionfrom [21], which extends the additive attention mechanism [22] touse cumulative attention weights from previous decoder time stepsas an additional feature. This encourages the model to move forwardconsistently through the input, mitigating potential failure modeswhere some subsequences are repeated or ignored by the decoder.Attention probabilities are computed after projecting inputs and lo-cation features to 128-dimensional hidden representations. Locationfeatures are computed using 32 1-D convolution \ufb01lters of length 31.The decoder is an autoregressive recurrent neural network whichpredicts a mel spectrogram from the encoded input sequence oneframe at a time. The prediction from the previous time step is \ufb01rstpassed through a smallpre-netcontaining 2 fully connected layersof 256 hidden ReLU units. We found that the pre-net acting as aninformation bottleneck was essential for learning attention. The pre-net output and attention context vector are concatenated and passedthrough a stack of 2 uni-directional LSTM layers with 1024 units.The concatenation of the LSTM output and the attention contextvector is projected through a linear transform to predict the targetspectrogram frame. Finally, the predicted mel spectrogram is passedthrough a 5-layer convolutionalpost-netwhich predicts a residualto add to the prediction to improve the overall reconstruction. Each",
    "metadata": {
      "source": "16",
      "chunk_id": 30,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "'LEVEGXIV\u0004)QFIHHMRK0SGEXMSR\u00047IRWMXMZI\u0004%XXIRXMSR\u0017\u0004'SRZ\u00040E]IVW&MHMVIGXMSREP\u00040781-RTYX\u00048I\\X\u0016\u00040E]IV\u00044VI\u00112IX\u0016\u00040781\u00040E]IVW0MRIEV\u00044VSNIGXMSR0MRIEV\u00044VSNIGXMSR7XST\u00048SOIR\u0019\u0004'SRZ\u00040E]IV\u00044SWX\u00112IX\n0HO\u00036SHFWURJUDP;EZI2IX\u00041S0;EZIJSVQ\u00047EQTPIWFig. 1. Block diagram of the Tacotron 2 system architecture.post-net layer is comprised of 512 \ufb01lters with shape5\u21e51with batchnormalization, followed bytanhactivations on all but the \ufb01nal layer.We minimize the summed mean squared error (MSE) from beforeand after the post-net to aid convergence. We also experimentedwith a log-likelihood loss by modeling the output distribution witha Mixture Density Network [23,24] to avoid assuming a constantvariance over time, but found that these were more dif\ufb01cult to trainand they did not lead to better sounding samples.In parallel to spectrogram frame prediction, the concatenation ofdecoder LSTM output and the attention context is projected downto a scalar and passed through a sigmoid activation to predict theprobability that the output sequence has completed. This \u201cstop token\u201dprediction is used during inference to allow the model to dynamicallydetermine when to terminate generation instead of always generatingfor a \ufb01xed duration. Speci\ufb01cally, generation completes at the \ufb01rstframe for which this probability exceeds a threshold of 0.5.The convolutional layers in the network are regularized usingdropout [25] with probability 0.5, and LSTM layers are regularizedusing zoneout [26] with probability 0.1. In order to introduce outputvariation at inference time, dropout with probability 0.5 is appliedonly to layers in the pre-net of the autoregressive decoder.In contrast to the original Tacotron, our model uses simpler build-ing blocks, using vanilla LSTM and convolutional layers in the en-coder and decoder instead of \u201cCBHG\u201d stacks and GRU recurrentlayers. We do not use a \u201creduction factor\u201d, i.e., each decoder stepcorresponds to a single spectrogram frame.2.3. WaveNet VocoderWe use a modi\ufb01ed version of the WaveNet architecture from [8] toinvert the mel spectrogram feature representation into time-domainwaveform samples. As in the original architecture, there are 30dilated convolution layers, grouped into 3 dilation cycles, i.e., thedilation rate of layer k (k=0...29) is2k(mod 10). To work withthe 12.5 ms frame hop of the spectrogram frames, only 2 upsamplinglayers are used in the conditioning stack instead of 3 layers.Instead of predicting discretized buckets with a softmax layer,we follow PixelCNN++ [27] and Parallel WaveNet [28] and use a 10-component mixture of logistic distributions (MoL) to generate 16-bitsamples at 24 kHz. To compute the logistic mixture distribution, theWaveNet stack output is passed through a ReLU activation followedEncoderDecoderVocoder\nFigure 16.16 The Tacotron2 architecture: An encoder-decoder maps from graphemes to\nmel spectrograms, followed by a vocoder that maps to wave\ufb01les. Figure modi\ufb01ed from Shen\net al. (2018).",
    "metadata": {
      "source": "16",
      "chunk_id": 31,
      "token_count": 788,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 16.16 The Tacotron2 architecture: An encoder-decoder maps from graphemes to\nmel spectrograms, followed by a vocoder that maps to wave\ufb01les. Figure modi\ufb01ed from Shen\net al. (2018).\nThe system is trained on gold log-mel \ufb01lterbank features, using teacher forcing,\nthat is the decoder is fed the correct log-model spectral feature at each decoder step\ninstead of the predicted decoder output from the prior step.\n16.6.3 TTS: Vocoding\nThe vocoder for Tacotron 2 is an adaptation of the WaveNet vocoder (van den Oord WaveNet\net al., 2016). Here we\u2019ll give a somewhat simpli\ufb01ed description of vocoding using\nWaveNet.\nRecall that the goal of the vocoding process here will be to invert a log mel spec-\ntrum representations back into a time-domain waveform representation. WaveNet is\nan autoregressive network, like the language models we introduced in Chapter 8. It",
    "metadata": {
      "source": "16",
      "chunk_id": 32,
      "token_count": 218,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22",
    "metadata": {
      "source": "16",
      "chunk_id": 33,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "22 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\ntakes spectrograms as input and produces audio output represented as sequences of\n8-bit mu-law (page 6). The probability of a waveform , a sequence of 8-bit mu-law\nvalues Y=y1;:::;yt, given an intermediate input mel spectrogram his computed as:\np(Y) =tY\nt=1P(ytjy1;:::;yt\u00001;h1;:::;ht) (16.24)\nThis probability distribution is modeled by a stack of special convolution layers,\nwhich include a speci\ufb01c convolutional structure called dilated convolutions , and a\nspeci\ufb01c non-linearity function.\nA dilated convolution is a subtype of causal convolutional layer. Causal or\nmasked convolutions look only at the past input, rather than the future; the pre-\ndiction of yt+1can only depend on y1;:::;yt, useful for autoregressive left-to-right\nprocessing. In dilated convolutions , at each successive layer we apply the convolu-dilated\nconvolutions\ntional \ufb01lter over a span longer than its length by skipping input values. Thus at time\ntwith a dilation value of 1, a convolutional \ufb01lter of length 2 would see input values\nxtandxt\u00001. But a \ufb01lter with a distillation value of 2 would skip an input, so would\nsee input values xtandxt\u00001. Fig. 16.17 shows the computation of the output at time\ntwith 4 dilated convolution layers with dilation values, 1, 2, 4, and 8.\nBecause models with causal convolutions do not have recurrent connections, they are typically faster\nto train than RNNs, especially when applied to very long sequences. One of the problems of causal\nconvolutions is that they require many layers, or large \ufb01lters to increase the receptive \ufb01eld. For\nexample, in Fig. 2 the receptive \ufb01eld is only 5 (= #layers + \ufb01lter length - 1). In this paper we use\ndilated convolutions to increase the receptive \ufb01eld by orders of magnitude, without greatly increasing\ncomputational cost.\nA dilated convolution (also called `a trous , or convolution with holes) is a convolution where the\n\ufb01lter is applied over an area larger than its length by skipping input values with a certain step. It is\nequivalent to a convolution with a larger \ufb01lter derived from the original \ufb01lter by dilating it with zeros,\nbut is signi\ufb01cantly more ef\ufb01cient. A dilated convolution effectively allows the network to operate on\na coarser scale than with a normal convolution. This is similar to pooling or strided convolutions, but\nhere the output has the same size as the input. As a special case, dilated convolution with dilation\n1yields the standard convolution. Fig. 3 depicts dilated causal convolutions for dilations 1,2,4,\nand8. Dilated convolutions have previously been used in various contexts, e.g. signal processing\n(Holschneider et al., 1989; Dutilleux, 1989), and image segmentation (Chen et al., 2015; Yu &\nKoltun, 2016).\nInputHidden LayerDilation = 1Hidden LayerDilation = 2Hidden LayerDilation = 4OutputDilation = 8\nFigure 3: Visualization of a stack of dilated causal convolutional layers.",
    "metadata": {
      "source": "16",
      "chunk_id": 34,
      "token_count": 774,
      "chapter_title": ""
    }
  },
  {
    "content": "computational cost.\nA dilated convolution (also called `a trous , or convolution with holes) is a convolution where the\n\ufb01lter is applied over an area larger than its length by skipping input values with a certain step. It is\nequivalent to a convolution with a larger \ufb01lter derived from the original \ufb01lter by dilating it with zeros,\nbut is signi\ufb01cantly more ef\ufb01cient. A dilated convolution effectively allows the network to operate on\na coarser scale than with a normal convolution. This is similar to pooling or strided convolutions, but\nhere the output has the same size as the input. As a special case, dilated convolution with dilation\n1yields the standard convolution. Fig. 3 depicts dilated causal convolutions for dilations 1,2,4,\nand8. Dilated convolutions have previously been used in various contexts, e.g. signal processing\n(Holschneider et al., 1989; Dutilleux, 1989), and image segmentation (Chen et al., 2015; Yu &\nKoltun, 2016).\nInputHidden LayerDilation = 1Hidden LayerDilation = 2Hidden LayerDilation = 4OutputDilation = 8\nFigure 3: Visualization of a stack of dilated causal convolutional layers.\nStacked dilated convolutions enable networks to have very large receptive \ufb01elds with just a few lay-\ners, while preserving the input resolution throughout the network as well as computational ef\ufb01ciency.\nIn this paper, the dilation is doubled for every layer up to a limit and then repeated: e.g.\n1,2,4,..., 512,1,2,4,..., 512,1,2,4,..., 512.\nThe intuition behind this con\ufb01guration is two-fold. First, exponentially increasing the dilation factor\nresults in exponential receptive \ufb01eld growth with depth (Yu & Koltun, 2016). For example each\n1,2,4,..., 512block has receptive \ufb01eld of size 1024 , and can be seen as a more ef\ufb01cient and dis-\ncriminative (non-linear) counterpart of a 1\u21e51024 convolution. Second, stacking these blocks further\nincreases the model capacity and the receptive \ufb01eld size.\n2.2 S OFTMAX DISTRIBUTIONS\nOne approach to modeling the conditional distributions p(xt|x1,...,x t\u00001)over the individual\naudio samples would be to use a mixture model such as a mixture density network (Bishop, 1994)\nor mixture of conditional Gaussian scale mixtures (MCGSM) (Theis & Bethge, 2015). However,\nvan den Oord et al. (2016a) showed that a softmax distribution tends to work better, even when the\ndata is implicitly continuous (as is the case for image pixel intensities or audio sample values). One\nof the reasons is that a categorical distribution is more \ufb02exible and can more easily model arbitrary\ndistributions because it makes no assumptions about their shape.\nBecause raw audio is typically stored as a sequence of 16-bit integer values (one per timestep), a\nsoftmax layer would need to output 65,536 probabilities per timestep to model all possible values.\nTo make this more tractable, we \ufb01rst apply a \u00b5-law companding transformation (ITU-T, 1988) to\nthe data, and then quantize it to 256 possible values:\nf(xt) = sign( xt)ln (1 + \u00b5|xt|)\nln (1 + \u00b5),\n3",
    "metadata": {
      "source": "16",
      "chunk_id": 35,
      "token_count": 769,
      "chapter_title": ""
    }
  },
  {
    "content": "increases the model capacity and the receptive \ufb01eld size.\n2.2 S OFTMAX DISTRIBUTIONS\nOne approach to modeling the conditional distributions p(xt|x1,...,x t\u00001)over the individual\naudio samples would be to use a mixture model such as a mixture density network (Bishop, 1994)\nor mixture of conditional Gaussian scale mixtures (MCGSM) (Theis & Bethge, 2015). However,\nvan den Oord et al. (2016a) showed that a softmax distribution tends to work better, even when the\ndata is implicitly continuous (as is the case for image pixel intensities or audio sample values). One\nof the reasons is that a categorical distribution is more \ufb02exible and can more easily model arbitrary\ndistributions because it makes no assumptions about their shape.\nBecause raw audio is typically stored as a sequence of 16-bit integer values (one per timestep), a\nsoftmax layer would need to output 65,536 probabilities per timestep to model all possible values.\nTo make this more tractable, we \ufb01rst apply a \u00b5-law companding transformation (ITU-T, 1988) to\nthe data, and then quantize it to 256 possible values:\nf(xt) = sign( xt)ln (1 + \u00b5|xt|)\nln (1 + \u00b5),\n3\nFigure 16.17 Dilated convolutions, showing one dilation cycle size of 4, i.e., dilation values\nof 1, 2, 4, 8. Figure from van den Oord et al. (2016).\nThe Tacotron 2 synthesizer uses 12 convolutional layers in two cycles with a\ndilation cycle size of 6, meaning that the \ufb01rst 6 layers have dilations of 1, 2, 4, 8, 16,\nand 32. and the next 6 layers again have dilations of 1, 2, 4, 8, 16, and 32. Dilated\nconvolutions allow the vocoder to grow the receptive \ufb01eld exponentially with depth.\nWaveNet predicts mu-law audio samples. Recall from page 6 that this is a stan-\ndard compression for audio in which the values at each sampling timestep are com-\npressed into 8-bits. This means that we can predict the value of each sample with a\nsimple 256-way categorical classi\ufb01er. The output of the dilated convolutions is thus\npassed through a softmax which makes this 256-way decision.\nThe spectrogram prediction encoder-decoder and the WaveNet vocoder are trained\nseparately. After the spectrogram predictor is trained, the spectrogram prediction\nnetwork is run in teacher-forcing mode, with each predicted spectral frame condi-\ntioned on the encoded text input and the previous frame from the ground truth spec-\ntrogram. This sequence of ground truth-aligned spectral features and gold audio\noutput is then used to train the vocoder.\nThis has been only a high-level sketch of the TTS process. There are numer-\nous important details that the reader interested in going further with TTS may want",
    "metadata": {
      "source": "16",
      "chunk_id": 36,
      "token_count": 657,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23\n\n16.7 \u2022 O THER SPEECH TASKS 23\nto look into. For example WaveNet uses a special kind of a gated activation func-\ntion as its non-linearity, and contains residual and skip connections. In practice,\npredicting 8-bit audio values doesn\u2019t as work as well as 16-bit, for which a simple\nsoftmax is insuf\ufb01cient, so decoders use fancier ways as the last step of predicting\naudio sample values, like mixtures of distributions. Finally, the WaveNet vocoder\nas we have described it would be so slow as to be useless; many different kinds of\nef\ufb01ciency improvements are necessary in practice, for example by \ufb01nding ways to\ndo non-autoregressive generation, avoiding the latency of having to wait to generate\neach frame until the prior frame has been generated, and instead making predictions\nin parallel. We encourage the interested reader to consult the original papers and\nvarious version of the code.\n16.6.4 TTS Evaluation\nSpeech synthesis systems are evaluated by human listeners. (The development of a\ngood automatic metric for synthesis evaluation, one that would eliminate the need\nfor expensive and time-consuming human listening experiments, remains an open\nand exciting research topic.)\nWe evaluate the quality of synthesized utterances by playing a sentence to lis-\nteners and ask them to give a mean opinion score (MOS ), a rating of how good MOS\nthe synthesized utterances are, usually on a scale from 1\u20135. We can then compare\nsystems by comparing their MOS scores on the same sentences (using, e.g., paired\nt-tests to test for signi\ufb01cant differences).\nIf we are comparing exactly two systems (perhaps to see if a particular change\nactually improved the system), we can use AB tests . In AB tests, we play the same AB tests\nsentence synthesized by two different systems (an A and a B system). The human\nlisteners choose which of the two utterances they like better. We do this for say\n50 sentences (presented in random order) and compare the number of sentences\npreferred for each system.\n16.7 Other Speech Tasks\nWhile we have focused on speech recognition and TTS in this chapter, there are a\nwide variety of speech-related tasks.\nThe task of wake word detection is to detect a word or short phrase, usually in wake word\norder to wake up a voice-enable assistant like Alexa, Siri, or the Google Assistant.\nThe goal with wake words is build the detection into small devices at the computing\nedge, to maintain privacy by transmitting the least amount of user speech to a cloud-\nbased server. Thus wake word detectors need to be fast, small footprint software that\ncan \ufb01t into embedded devices. Wake word detectors usually use the same frontend\nfeature extraction we saw for ASR, often followed by a whole-word classi\ufb01er.\nSpeaker diarization is the task of determining \u2018who spoke when\u2019 in a longspeaker\ndiarization\nmulti-speaker audio recording, marking the start and end of each speaker\u2019s turns in\nthe interaction. This can be useful for transcribing meetings, classroom speech, or\nmedical interactions. Often diarization systems use voice activity detection (V AD) to\n\ufb01nd segments of continuous speech, extract speaker embedding vectors, and cluster\nthe vectors to group together segments likely from the same speaker. More recent\nwork is investigating end-to-end algorithms to map directly from input speech to a\nsequence of speaker labels for each frame.",
    "metadata": {
      "source": "16",
      "chunk_id": 37,
      "token_count": 737,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24\n\n24 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\nSpeaker recognition , is the task of identifying a speaker. We generally distin-speaker\nrecognition\nguish the subtasks of speaker veri\ufb01cation , where we make a binary decision (is\nthis speaker Xor not?), such as for security when accessing personal information\nover the telephone, and speaker identi\ufb01cation , where we make a one of Ndecision\ntrying to match a speaker\u2019s voice against a database of many speakers . These tasks\nare related to language identi\ufb01cation , in which we are given a wave\ufb01le and mustlanguage\nidenti\ufb01cation\nidentify which language is being spoken; this is useful for example for automatically\ndirecting callers to human operators that speak appropriate languages.\n16.8 Summary\nThis chapter introduced the fundamental algorithms of automatic speech recognition\n(ASR) and text-to-speech (TTS).\n\u2022 The task of speech recognition (or speech-to-text) is to map acoustic wave-\nforms to sequences of graphemes.\n\u2022 The input to a speech recognizer is a series of acoustic waves. that are sam-\npled,quantized , and converted to a spectral representation like the log mel\nspectrum .\n\u2022 Two common paradigms for speech recognition are the encoder-decoder with\nattention model, and models based on the CTC loss function . Attention-\nbased models have higher accuracies, but models based on CTC more easily\nadapt to streaming : outputting graphemes online instead of waiting until the\nacoustic input is complete.\n\u2022 ASR is evaluated using the Word Error Rate; the edit distance between the\nhypothesis and the gold transcription.\n\u2022TTS systems are also based on the encoder-decoder architecture. The en-\ncoder maps letters to an encoding, which is consumed by the decoder which\ngenerates mel spectrogram output. A neural vocoder then reads the spectro-\ngram and generates waveforms.\n\u2022 TTS systems require a \ufb01rst pass of text normalization to deal with numbers\nand abbreviations and other non-standard words.\n\u2022 TTS is evaluated by playing a sentence to human listeners and having them\ngive a mean opinion score (MOS) or by doing AB tests.\nBibliographical and Historical Notes\nASR A number of speech recognition systems were developed by the late 1940s\nand early 1950s. An early Bell Labs system could recognize any of the 10 digits\nfrom a single speaker (Davis et al., 1952). This system had 10 speaker-dependent\nstored patterns, one for each digit, each of which roughly represented the \ufb01rst two\nvowel formants in the digit. They achieved 97%\u201399% accuracy by choosing the pat-\ntern that had the highest relative correlation coef\ufb01cient with the input. Fry (1959)\nand Denes (1959) built a phoneme recognizer at University College, London, that\nrecognized four vowels and nine consonants based on a similar pattern-recognition\nprinciple. Fry and Denes\u2019s system was the \ufb01rst to use phoneme transition probabili-\nties to constrain the recognizer.",
    "metadata": {
      "source": "16",
      "chunk_id": 38,
      "token_count": 667,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 25",
    "metadata": {
      "source": "16",
      "chunk_id": 39,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 25\nThe late 1960s and early 1970s produced a number of important paradigm shifts.\nFirst were a number of feature-extraction algorithms, including the ef\ufb01cient fast\nFourier transform (FFT) (Cooley and Tukey, 1965), the application of cepstral pro-\ncessing to speech (Oppenheim et al., 1968), and the development of LPC for speech\ncoding (Atal and Hanauer, 1971). Second were a number of ways of handling warp-\ning; stretching or shrinking the input signal to handle differences in speaking rate warping\nand segment length when matching against stored patterns. The natural algorithm for\nsolving this problem was dynamic programming, and, as we saw in Appendix A, the\nalgorithm was reinvented multiple times to address this problem. The \ufb01rst applica-\ntion to speech processing was by Vintsyuk (1968), although his result was not picked\nup by other researchers, and was reinvented by Velichko and Zagoruyko (1970) and\nSakoe and Chiba (1971) (and 1984). Soon afterward, Itakura (1975) combined this\ndynamic programming idea with the LPC coef\ufb01cients that had previously been used\nonly for speech coding. The resulting system extracted LPC features from incoming\nwords and used dynamic programming to match them against stored LPC templates.\nThe non-probabilistic use of dynamic programming to match a template against in-\ncoming speech is called dynamic time warping .dynamic time\nwarping\nThe third innovation of this period was the rise of the HMM. Hidden Markov\nmodels seem to have been applied to speech independently at two laboratories around\n1972. One application arose from the work of statisticians, in particular Baum and\ncolleagues at the Institute for Defense Analyses in Princeton who applied HMMs\nto various prediction problems (Baum and Petrie 1966, Baum and Eagon 1967).\nJames Baker learned of this work and applied the algorithm to speech processing\n(Baker, 1975) during his graduate work at CMU. Independently, Frederick Jelinek\nand collaborators (drawing from their research in information-theoretical models\nin\ufb02uenced by the work of Shannon (1948)) applied HMMs to speech at the IBM\nThomas J. Watson Research Center (Jelinek et al., 1975). One early difference was\nthe decoding algorithm; Baker\u2019s DRAGON system used Viterbi (dynamic program-\nming) decoding, while the IBM system applied Jelinek\u2019s stack decoding algorithm\n(Jelinek, 1969). Baker then joined the IBM group for a brief time before founding\nthe speech-recognition company Dragon Systems.\nThe use of the HMM, with Gaussian Mixture Models (GMMs) as the phonetic\ncomponent, slowly spread through the speech community, becoming the dominant\nparadigm by the 1990s. One cause was encouragement by ARPA, the Advanced\nResearch Projects Agency of the U.S. Department of Defense. ARPA started a\n\ufb01ve-year program in 1971 to build 1000-word, constrained grammar, few speaker\nspeech understanding (Klatt, 1977), and funded four competing systems of which\nCarnegie-Mellon University\u2019s Harpy system (Lowerre, 1976), which used a simpli-\n\ufb01ed version of Baker\u2019s HMM-based DRAGON system was the best of the tested sys-\ntems. ARPA (and then DARPA) funded a number of new speech research programs,\nbeginning with 1000-word speaker-independent read-speech tasks like \u201cResource",
    "metadata": {
      "source": "16",
      "chunk_id": 40,
      "token_count": 785,
      "chapter_title": ""
    }
  },
  {
    "content": "Thomas J. Watson Research Center (Jelinek et al., 1975). One early difference was\nthe decoding algorithm; Baker\u2019s DRAGON system used Viterbi (dynamic program-\nming) decoding, while the IBM system applied Jelinek\u2019s stack decoding algorithm\n(Jelinek, 1969). Baker then joined the IBM group for a brief time before founding\nthe speech-recognition company Dragon Systems.\nThe use of the HMM, with Gaussian Mixture Models (GMMs) as the phonetic\ncomponent, slowly spread through the speech community, becoming the dominant\nparadigm by the 1990s. One cause was encouragement by ARPA, the Advanced\nResearch Projects Agency of the U.S. Department of Defense. ARPA started a\n\ufb01ve-year program in 1971 to build 1000-word, constrained grammar, few speaker\nspeech understanding (Klatt, 1977), and funded four competing systems of which\nCarnegie-Mellon University\u2019s Harpy system (Lowerre, 1976), which used a simpli-\n\ufb01ed version of Baker\u2019s HMM-based DRAGON system was the best of the tested sys-\ntems. ARPA (and then DARPA) funded a number of new speech research programs,\nbeginning with 1000-word speaker-independent read-speech tasks like \u201cResource\nManagement\u201d (Price et al., 1988), recognition of sentences read from the Wall Street\nJournal (WSJ), Broadcast News domain (LDC 1998, Graff 1997) (transcription of\nactual news broadcasts, including quite dif\ufb01cult passages such as on-the-street inter-\nviews) and the Switchboard, CallHome, CallFriend, and Fisher domains (Godfrey\net al. 1992, Cieri et al. 2004) (natural telephone conversations between friends or\nstrangers). Each of the ARPA tasks involved an approximately annual bakeoff at bakeoff\nwhich systems were evaluated against each other. The ARPA competitions resulted\nin wide-scale borrowing of techniques among labs since it was easy to see which\nideas reduced errors the previous year, and the competitions were probably an im-",
    "metadata": {
      "source": "16",
      "chunk_id": 41,
      "token_count": 454,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 26",
    "metadata": {
      "source": "16",
      "chunk_id": 42,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "26 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\nportant factor in the eventual spread of the HMM paradigm.\nBy around 1990 neural alternatives to the HMM/GMM architecture for ASR\narose, based on a number of earlier experiments with neural networks for phoneme\nrecognition and other speech tasks. Architectures included the time-delay neural\nnetwork ( TDNN )\u2014the \ufb01rst use of convolutional networks for speech\u2014 (Waibel\net al. 1989, Lang et al. 1990), RNNs (Robinson and Fallside, 1991), and the hybrid hybrid\nHMM/MLP architecture in which a feedforward neural network is trained as a pho-\nnetic classi\ufb01er whose outputs are used as probability estimates for an HMM-based\narchitecture (Morgan and Bourlard 1990, Bourlard and Morgan 1994, Morgan and\nBourlard 1995).\nWhile the hybrid systems showed performance close to the standard HMM/GMM\nmodels, the problem was speed: large hybrid models were too slow to train on the\nCPUs of that era. For example, the largest hybrid system, a feedforward network,\nwas limited to a hidden layer of 4000 units, producing probabilities over only a few\ndozen monophones. Yet training this model still required the research group to de-\nsign special hardware boards to do vector processing (Morgan and Bourlard, 1995).\nA later analytic study showed the performance of such simple feedforward MLPs\nfor ASR increases sharply with more than 1 hidden layer, even controlling for the\ntotal number of parameters (Maas et al., 2017). But the computational resources of\nthe time were insuf\ufb01cient for more layers.\nOver the next two decades a combination of Moore\u2019s law and the rise of GPUs\nallowed deep neural networks with many layers. Performance was getting close to\ntraditional systems on smaller tasks like TIMIT phone recognition by 2009 (Mo-\nhamed et al., 2009), and by 2012, the performance of hybrid systems had surpassed\ntraditional HMM/GMM systems (Jaitly et al. 2012, Dahl et al. 2012, inter alia).\nOriginally it seemed that unsupervised pretraining of the networks using a tech-\nnique like deep belief networks was important, but by 2013, it was clear that for\nhybrid HMM/GMM feedforward networks, all that mattered was to use a lot of data\nand enough layers, although a few other components did improve performance: us-\ning log mel features instead of MFCCs, using dropout, and using recti\ufb01ed linear\nunits (Deng et al. 2013, Maas et al. 2013, Dahl et al. 2013).\nMeanwhile early work had proposed the CTC loss function by 2006 (Graves\net al., 2006), and by 2012 the RNN-Transducer was de\ufb01ned and applied to phone\nrecognition (Graves 2012, Graves et al. 2013), and then to end-to-end speech recog-\nnition rescoring (Graves and Jaitly, 2014), and then recognition (Maas et al., 2015),\nwith advances such as specialized beam search (Hannun et al., 2014). (Our de-\nscription of CTC in the chapter draws on Hannun (2017), which we encourage the\ninterested reader to follow).\nThe encoder-decoder architecture was applied to speech at about the same time",
    "metadata": {
      "source": "16",
      "chunk_id": 43,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "Originally it seemed that unsupervised pretraining of the networks using a tech-\nnique like deep belief networks was important, but by 2013, it was clear that for\nhybrid HMM/GMM feedforward networks, all that mattered was to use a lot of data\nand enough layers, although a few other components did improve performance: us-\ning log mel features instead of MFCCs, using dropout, and using recti\ufb01ed linear\nunits (Deng et al. 2013, Maas et al. 2013, Dahl et al. 2013).\nMeanwhile early work had proposed the CTC loss function by 2006 (Graves\net al., 2006), and by 2012 the RNN-Transducer was de\ufb01ned and applied to phone\nrecognition (Graves 2012, Graves et al. 2013), and then to end-to-end speech recog-\nnition rescoring (Graves and Jaitly, 2014), and then recognition (Maas et al., 2015),\nwith advances such as specialized beam search (Hannun et al., 2014). (Our de-\nscription of CTC in the chapter draws on Hannun (2017), which we encourage the\ninterested reader to follow).\nThe encoder-decoder architecture was applied to speech at about the same time\nby two different groups, in the Listen Attend and Spell system of Chan et al. (2016)\nand the attention-based encoder decoder architecture of Chorowski et al. (2014)\nand Bahdanau et al. (2016). By 2018 Transformers were included in this encoder-\ndecoder architecture. Karita et al. (2019) is a nice comparison of RNNs vs Trans-\nformers in encoder-architectures for ASR, TTS, and speech-to-speech translation.\nPopular toolkits for speech processing include Kaldi (Povey et al., 2011) and Kaldi\nESPnet (Watanabe et al. 2018, Hayashi et al. 2020). ESPnet\nTTS As we noted at the beginning of the chapter, speech synthesis is one of the\nearliest \ufb01elds of speech and language processing. The 18th century saw a number\nof physical models of the articulation process, including the von Kempelen model\nmentioned above, as well as the 1773 vowel model of Kratzenstein in Copenhagen",
    "metadata": {
      "source": "16",
      "chunk_id": 44,
      "token_count": 517,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 27\n\nEXERCISES 27\nusing organ pipes.\nThe early 1950s saw the development of three early paradigms of waveform\nsynthesis: formant synthesis, articulatory synthesis, and concatenative synthesis.\nModern encoder-decoder systems are distant descendants of formant synthesiz-\ners. Formant synthesizers originally were inspired by attempts to mimic human\nspeech by generating arti\ufb01cial spectrograms. The Haskins Laboratories Pattern\nPlayback Machine generated a sound wave by painting spectrogram patterns on a\nmoving transparent belt and using re\ufb02ectance to \ufb01lter the harmonics of a wave-\nform (Cooper et al., 1951); other very early formant synthesizers include those of\nLawrence (1953) and Fant (1951). Perhaps the most well-known of the formant\nsynthesizers were the Klatt formant synthesizer and its successor systems, includ-\ning the MITalk system (Allen et al., 1987) and the Klattalk software used in Digital\nEquipment Corporation\u2019s DECtalk (Klatt, 1982). See Klatt (1975) for details.\nA second early paradigm, concatenative synthesis, seems to have been \ufb01rst pro-\nposed by Harris (1953) at Bell Laboratories; he literally spliced together pieces of\nmagnetic tape corresponding to phones. Soon afterwards, Peterson et al. (1958) pro-\nposed a theoretical model based on diphones, including a database with multiple\ncopies of each diphone with differing prosody, each labeled with prosodic features\nincluding F0, stress, and duration, and the use of join costs based on F0 and formant\ndistance between neighboring units. But such diphone synthesis models were not\nactually implemented until decades later (Dixon and Maxey 1968, Olive 1977). The\n1980s and 1990s saw the invention of unit selection synthesis , based on larger units\nof non-uniform length and the use of a target cost, (Sagisaka 1988, Sagisaka et al.\n1992, Hunt and Black 1996, Black and Taylor 1994, Syrdal et al. 2000).\nA third paradigm, articulatory synthesizers attempt to synthesize speech by\nmodeling the physics of the vocal tract as an open tube. Representative models\ninclude Stevens et al. (1953), Flanagan et al. (1975), and Fant (1986). See Klatt\n(1975) and Flanagan (1972) for more details.\nMost early TTS systems used phonemes as input; development of the text anal-\nysis components of TTS came somewhat later, drawing on NLP. Indeed the \ufb01rst\ntrue text-to-speech system seems to have been the system of Umeda and Teranishi\n(Umeda et al. 1968, Teranishi and Umeda 1968, Umeda 1976), which included a\nparser that assigned prosodic boundaries, as well as accent and stress.\nExercises\n16.1 Analyze each of the errors in the incorrectly recognized transcription of \u201cum\nthe phone is I left the. . . \u201d on page 16. For each one, give your best guess as\nto whether you think it is caused by a problem in signal processing, pronun-\nciation modeling, lexicon size, language model, or pruning in the decoding\nsearch.",
    "metadata": {
      "source": "16",
      "chunk_id": 45,
      "token_count": 729,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 28",
    "metadata": {
      "source": "16",
      "chunk_id": 46,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "28 Chapter 16 \u2022 Automatic Speech Recognition and Text-to-Speech\nAllen, J., M. S. Hunnicut, and D. H. Klatt. 1987. From Text\nto Speech: The MITalk system . Cambridge University\nPress.\nAtal, B. S. and S. Hanauer. 1971. Speech analysis and syn-\nthesis by prediction of the speech wave. JASA , 50:637\u2013\n655.\nBahdanau, D., J. Chorowski, D. Serdyuk, P. Brakel, and\nY . Bengio. 2016. End-to-end attention-based large vo-\ncabulary speech recognition. ICASSP .\nBaker, J. K. 1975. The DRAGON system \u2013 An overview.\nIEEE Transactions on ASSP , ASSP-23(1):24\u201329.\nBaum, L. E. and J. A. Eagon. 1967. An inequality with appli-\ncations to statistical estimation for probabilistic functions\nof Markov processes and to a model for ecology. Bulletin\nof the American Mathematical Society , 73(3):360\u2013363.\nBaum, L. E. and T. Petrie. 1966. Statistical inference for\nprobabilistic functions of \ufb01nite-state Markov chains. An-\nnals of Mathematical Statistics , 37(6):1554\u20131563.\nBlack, A. W. and P. Taylor. 1994. CHATR: A generic speech\nsynthesis system. COLING .\nBourlard, H. and N. Morgan. 1994. Connectionist Speech\nRecognition: A Hybrid Approach . Kluwer.\nBu, H., J. Du, X. Na, B. Wu, and H. Zheng. 2017. AISHELL-\n1: An open-source Mandarin speech corpus and a speech\nrecognition baseline. O-COCOSDA Proceedings .\nCanavan, A., D. Graff, and G. Zipperlen. 1997. CALL-\nHOME American English speech LDC97S42. Linguistic\nData Consortium.\nChan, W., N. Jaitly, Q. Le, and O. Vinyals. 2016. Listen,\nattend and spell: A neural network for large vocabulary\nconversational speech recognition. ICASSP .\nChorowski, J., D. Bahdanau, K. Cho, and Y . Bengio.\n2014. End-to-end continuous speech recognition us-\ning attention-based recurrent NN: First results. NeurIPS\nDeep Learning and Representation Learning Workshop .\nCieri, C., D. Miller, and K. Walker. 2004. The Fisher cor-\npus: A resource for the next generations of speech-to-text.\nLREC .\nCooley, J. W. and J. W. Tukey. 1965. An algorithm for the\nmachine calculation of complex Fourier series. Mathe-\nmatics of Computation , 19(90):297\u2013301.\nCooper, F. S., A. M. Liberman, and J. M. Borst. 1951. The\ninterconversion of audible and visible patterns as a basis\nfor research in the perception of speech. Proceedings of\nthe National Academy of Sciences , 37(5):318\u2013325.\nDahl, G. E., T. N. Sainath, and G. E. Hinton. 2013. Im-\nproving deep neural networks for LVCSR using recti\ufb01ed\nlinear units and dropout. ICASSP .",
    "metadata": {
      "source": "16",
      "chunk_id": 47,
      "token_count": 755,
      "chapter_title": ""
    }
  },
  {
    "content": "attend and spell: A neural network for large vocabulary\nconversational speech recognition. ICASSP .\nChorowski, J., D. Bahdanau, K. Cho, and Y . Bengio.\n2014. End-to-end continuous speech recognition us-\ning attention-based recurrent NN: First results. NeurIPS\nDeep Learning and Representation Learning Workshop .\nCieri, C., D. Miller, and K. Walker. 2004. The Fisher cor-\npus: A resource for the next generations of speech-to-text.\nLREC .\nCooley, J. W. and J. W. Tukey. 1965. An algorithm for the\nmachine calculation of complex Fourier series. Mathe-\nmatics of Computation , 19(90):297\u2013301.\nCooper, F. S., A. M. Liberman, and J. M. Borst. 1951. The\ninterconversion of audible and visible patterns as a basis\nfor research in the perception of speech. Proceedings of\nthe National Academy of Sciences , 37(5):318\u2013325.\nDahl, G. E., T. N. Sainath, and G. E. Hinton. 2013. Im-\nproving deep neural networks for LVCSR using recti\ufb01ed\nlinear units and dropout. ICASSP .\nDahl, G. E., D. Yu, L. Deng, and A. Acero. 2012. Context-\ndependent pre-trained deep neural networks for large-\nvocabulary speech recognition. IEEE Transactions on au-\ndio, speech, and language processing , 20(1):30\u201342.\nDavid, Jr., E. E. and O. G. Selfridge. 1962. Eyes and ears\nfor computers. Proceedings of the IRE (Institute of Radio\nEngineers) , 50:1093\u20131101.\nDavis, K. H., R. Biddulph, and S. Balashek. 1952. Automatic\nrecognition of spoken digits. JASA , 24(6):637\u2013642.Davis, S. and P. Mermelstein. 1980. Comparison of para-\nmetric representations for monosyllabic word recognition\nin continuously spoken sentences. IEEE Transactions on\nASSP , 28(4):357\u2013366.\nDemberg, V . 2006. Letter-to-phoneme conversion for a Ger-\nman text-to-speech system. Diplomarbeit Nr. 47, Univer-\nsit\u00a8at Stuttgart.\nDenes, P. 1959. The design and operation of the mechanical\nspeech recognizer at University College London. Journal\nof the British Institution of Radio Engineers , 19(4):219\u2013\n234. Appears together with companion paper (Fry 1959).\nDeng, L., G. Hinton, and B. Kingsbury. 2013. New types of\ndeep neural network learning for speech recognition and\nrelated applications: An overview. ICASSP .\nDixon, N. and H. Maxey. 1968. Terminal analog synthesis of\ncontinuous speech using the diphone method of segment\nassembly. IEEE Transactions on Audio and Electroacous-\ntics, 16(1):40\u201350.\nDu Bois, J. W., W. L. Chafe, C. Meyer, S. A. Thompson,\nR. Englebretson, and N. Martey. 2005. Santa Barbara cor-\npus of spoken American English, Parts 1-4. Philadelphia:\nLinguistic Data Consortium.",
    "metadata": {
      "source": "16",
      "chunk_id": 48,
      "token_count": 749,
      "chapter_title": ""
    }
  },
  {
    "content": "in continuously spoken sentences. IEEE Transactions on\nASSP , 28(4):357\u2013366.\nDemberg, V . 2006. Letter-to-phoneme conversion for a Ger-\nman text-to-speech system. Diplomarbeit Nr. 47, Univer-\nsit\u00a8at Stuttgart.\nDenes, P. 1959. The design and operation of the mechanical\nspeech recognizer at University College London. Journal\nof the British Institution of Radio Engineers , 19(4):219\u2013\n234. Appears together with companion paper (Fry 1959).\nDeng, L., G. Hinton, and B. Kingsbury. 2013. New types of\ndeep neural network learning for speech recognition and\nrelated applications: An overview. ICASSP .\nDixon, N. and H. Maxey. 1968. Terminal analog synthesis of\ncontinuous speech using the diphone method of segment\nassembly. IEEE Transactions on Audio and Electroacous-\ntics, 16(1):40\u201350.\nDu Bois, J. W., W. L. Chafe, C. Meyer, S. A. Thompson,\nR. Englebretson, and N. Martey. 2005. Santa Barbara cor-\npus of spoken American English, Parts 1-4. Philadelphia:\nLinguistic Data Consortium.\nEbden, P. and R. Sproat. 2015. The Kestrel TTS text\nnormalization system. Natural Language Engineering ,\n21(3):333.\nvan Esch, D. and R. Sproat. 2018. An expanded taxonomy of\nsemiotic classes for text normalization. INTERSPEECH .\nFant, G. M. 1951. Speech communication research. Ing.\nVetenskaps Akad. Stockholm, Sweden , 24:331\u2013337.\nFant, G. M. 1986. Glottal \ufb02ow: Models and interaction.\nJournal of Phonetics , 14:393\u2013399.\nFlanagan, J. L. 1972. Speech Analysis, Synthesis, and Per-\nception . Springer.\nFlanagan, J. L., K. Ishizaka, and K. L. Shipley. 1975. Syn-\nthesis of speech from a dynamic model of the vocal\ncords and vocal tract. The Bell System Technical Jour-\nnal, 54(3):485\u2013506.\nFry, D. B. 1959. Theoretical aspects of mechanical speech\nrecognition. Journal of the British Institution of Radio\nEngineers , 19(4):211\u2013218. Appears together with com-\npanion paper (Denes 1959).\nGillick, L. and S. J. Cox. 1989. Some statistical issues in the\ncomparison of speech recognition algorithms. ICASSP .\nGodfrey, J., E. Holliman, and J. McDaniel. 1992. SWITCH-\nBOARD: Telephone speech corpus for research and de-\nvelopment. ICASSP .\nGraff, D. 1997. The 1996 Broadcast News speech and\nlanguage-model corpus. Proceedings DARPA Speech\nRecognition Workshop .\nGraves, A. 2012. Sequence transduction with recurrent neu-\nral networks. ICASSP .\nGraves, A., S. Fern \u00b4andez, F. Gomez, and J. Schmidhuber.\n2006. Connectionist temporal classi\ufb01cation: Labelling\nunsegmented sequence data with recurrent neural net-\nworks. ICML .\nGraves, A. and N. Jaitly. 2014. Towards end-to-end speech",
    "metadata": {
      "source": "16",
      "chunk_id": 49,
      "token_count": 763,
      "chapter_title": ""
    }
  },
  {
    "content": "cords and vocal tract. The Bell System Technical Jour-\nnal, 54(3):485\u2013506.\nFry, D. B. 1959. Theoretical aspects of mechanical speech\nrecognition. Journal of the British Institution of Radio\nEngineers , 19(4):211\u2013218. Appears together with com-\npanion paper (Denes 1959).\nGillick, L. and S. J. Cox. 1989. Some statistical issues in the\ncomparison of speech recognition algorithms. ICASSP .\nGodfrey, J., E. Holliman, and J. McDaniel. 1992. SWITCH-\nBOARD: Telephone speech corpus for research and de-\nvelopment. ICASSP .\nGraff, D. 1997. The 1996 Broadcast News speech and\nlanguage-model corpus. Proceedings DARPA Speech\nRecognition Workshop .\nGraves, A. 2012. Sequence transduction with recurrent neu-\nral networks. ICASSP .\nGraves, A., S. Fern \u00b4andez, F. Gomez, and J. Schmidhuber.\n2006. Connectionist temporal classi\ufb01cation: Labelling\nunsegmented sequence data with recurrent neural net-\nworks. ICML .\nGraves, A. and N. Jaitly. 2014. Towards end-to-end speech\nrecognition with recurrent neural networks. ICML .",
    "metadata": {
      "source": "16",
      "chunk_id": 50,
      "token_count": 289,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 29",
    "metadata": {
      "source": "16",
      "chunk_id": 51,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Exercises 29\nGraves, A., A.-r. Mohamed, and G. Hinton. 2013.\nSpeech recognition with deep recurrent neural networks.\nICASSP .\nHannun, A. 2017. Sequence modeling with CTC. Distill ,\n2(11).\nHannun, A. Y ., A. L. Maas, D. Jurafsky, and A. Y . Ng. 2014.\nFirst-pass large vocabulary continuous speech recogni-\ntion using bi-directional recurrent DNNs. ArXiv preprint\narXiv:1408.2873.\nHarris, C. M. 1953. A study of the building blocks in speech.\nJASA , 25(5):962\u2013969.\nHayashi, T., R. Yamamoto, K. Inoue, T. Yoshimura,\nS. Watanabe, T. Toda, K. Takeda, Y . Zhang, and X. Tan.\n2020. ESPnet-TTS: Uni\ufb01ed, reproducible, and inte-\ngratable open source end-to-end text-to-speech toolkit.\nICASSP .\nHunt, A. J. and A. W. Black. 1996. Unit selection in a con-\ncatenative speech synthesis system using a large speech\ndatabase. ICASSP .\nItakura, F. 1975. Minimum prediction residual principle ap-\nplied to speech recognition. IEEE Transactions on ASSP ,\nASSP-32:67\u201372.\nIto, K. and L. Johnson. 2017. The LJ speech dataset. https:\n//keithito.com/LJ-Speech-Dataset/ .\nJaitly, N., P. Nguyen, A. Senior, and V . Vanhoucke. 2012.\nApplication of pretrained deep neural networks to large\nvocabulary speech recognition. INTERSPEECH .\nJelinek, F. 1969. A fast sequential decoding algorithm us-\ning a stack. IBM Journal of Research and Development ,\n13:675\u2013685.\nJelinek, F., R. L. Mercer, and L. R. Bahl. 1975. Design of a\nlinguistic statistical decoder for the recognition of contin-\nuous speech. IEEE Transactions on Information Theory ,\nIT-21(3):250\u2013256.\nKarita, S., N. Chen, T. Hayashi, T. Hori, H. Inaguma,\nZ. Jiang, M. Someki, N. E. Y . Soplin, R. Yamamoto,\nX. Wang, S. Watanabe, T. Yoshimura, and W. Zhang.\n2019. A comparative study on transformer vs RNN in\nspeech applications. IEEE ASRU-19 .\nKendall, T. and C. Farrington. 2020. The Corpus of Regional\nAfrican American Language. Version 2020.05. Eugene,\nOR: The Online Resources for African American Lan-\nguage Project. http://oraal.uoregon.edu/coraal.\nKlatt, D. H. 1975. V oice onset time, friction, and aspiration\nin word-initial consonant clusters. Journal of Speech and\nHearing Research , 18:686\u2013706.\nKlatt, D. H. 1977. Review of the ARPA speech understand-\ning project. JASA , 62(6):1345\u20131366.\nKlatt, D. H. 1982. The Klattalk text-to-speech conversion\nsystem. ICASSP .",
    "metadata": {
      "source": "16",
      "chunk_id": 52,
      "token_count": 756,
      "chapter_title": ""
    }
  },
  {
    "content": "uous speech. IEEE Transactions on Information Theory ,\nIT-21(3):250\u2013256.\nKarita, S., N. Chen, T. Hayashi, T. Hori, H. Inaguma,\nZ. Jiang, M. Someki, N. E. Y . Soplin, R. Yamamoto,\nX. Wang, S. Watanabe, T. Yoshimura, and W. Zhang.\n2019. A comparative study on transformer vs RNN in\nspeech applications. IEEE ASRU-19 .\nKendall, T. and C. Farrington. 2020. The Corpus of Regional\nAfrican American Language. Version 2020.05. Eugene,\nOR: The Online Resources for African American Lan-\nguage Project. http://oraal.uoregon.edu/coraal.\nKlatt, D. H. 1975. V oice onset time, friction, and aspiration\nin word-initial consonant clusters. Journal of Speech and\nHearing Research , 18:686\u2013706.\nKlatt, D. H. 1977. Review of the ARPA speech understand-\ning project. JASA , 62(6):1345\u20131366.\nKlatt, D. H. 1982. The Klattalk text-to-speech conversion\nsystem. ICASSP .\nLang, K. J., A. H. Waibel, and G. E. Hinton. 1990. A\ntime-delay neural network architecture for isolated word\nrecognition. Neural networks , 3(1):23\u201343.\nLawrence, W. 1953. The synthesis of speech from signals\nwhich have a low information rate. In W. Jackson, ed.,\nCommunication Theory , 460\u2013469. Butterworth.\nLDC. 1998. LDC Catalog: Hub4 project . Univer-\nsity of Pennsylvania. www.ldc.upenn.edu/Catalog/\nLDC98S71.html .Liu, Y ., P. Fung, Y . Yang, C. Cieri, S. Huang, and D. Graff.\n2006. HKUST/MTS: A very large scale Mandarin tele-\nphone speech corpus. International Conference on Chi-\nnese Spoken Language Processing .\nLowerre, B. T. 1976. The Harpy Speech Recognition System .\nPh.D. thesis, Carnegie Mellon University, Pittsburgh, PA.\nMaas, A., Z. Xie, D. Jurafsky, and A. Y . Ng. 2015. Lexicon-\nfree conversational speech recognition with neural net-\nworks. NAACL HLT .\nMaas, A. L., A. Y . Hannun, and A. Y . Ng. 2013. Recti\ufb01er\nnonlinearities improve neural network acoustic models.\nICML .\nMaas, A. L., P. Qi, Z. Xie, A. Y . Hannun, C. T. Lengerich,\nD. Jurafsky, and A. Y . Ng. 2017. Building dnn acoustic\nmodels for large vocabulary speech recognition. Com-\nputer Speech & Language , 41:195\u2013213.\nMohamed, A., G. E. Dahl, and G. E. Hinton. 2009. Deep\nBelief Networks for phone recognition. NIPS Workshop\non Deep Learning for Speech Recognition and Related\nApplications .\nMorgan, N. and H. Bourlard. 1990. Continuous speech\nrecognition using multilayer perceptrons with hidden\nmarkov models. ICASSP .",
    "metadata": {
      "source": "16",
      "chunk_id": 53,
      "token_count": 756,
      "chapter_title": ""
    }
  },
  {
    "content": "nese Spoken Language Processing .\nLowerre, B. T. 1976. The Harpy Speech Recognition System .\nPh.D. thesis, Carnegie Mellon University, Pittsburgh, PA.\nMaas, A., Z. Xie, D. Jurafsky, and A. Y . Ng. 2015. Lexicon-\nfree conversational speech recognition with neural net-\nworks. NAACL HLT .\nMaas, A. L., A. Y . Hannun, and A. Y . Ng. 2013. Recti\ufb01er\nnonlinearities improve neural network acoustic models.\nICML .\nMaas, A. L., P. Qi, Z. Xie, A. Y . Hannun, C. T. Lengerich,\nD. Jurafsky, and A. Y . Ng. 2017. Building dnn acoustic\nmodels for large vocabulary speech recognition. Com-\nputer Speech & Language , 41:195\u2013213.\nMohamed, A., G. E. Dahl, and G. E. Hinton. 2009. Deep\nBelief Networks for phone recognition. NIPS Workshop\non Deep Learning for Speech Recognition and Related\nApplications .\nMorgan, N. and H. Bourlard. 1990. Continuous speech\nrecognition using multilayer perceptrons with hidden\nmarkov models. ICASSP .\nMorgan, N. and H. A. Bourlard. 1995. Neural networks for\nstatistical recognition of continuous speech. Proceedings\nof the IEEE , 83(5):742\u2013772.\nNIST. 2005. Speech recognition scoring toolkit (sctk) ver-\nsion 2.1. http://www.nist.gov/speech/tools/ .\nNIST. 2007. Matched Pairs Sentence-Segment Word Error\n(MAPSSWE) Test.\nOlive, J. P. 1977. Rule synthesis of speech from dyadic units.\nICASSP77 .\nvan den Oord, A., S. Dieleman, H. Zen, K. Simonyan,\nO. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and\nK. Kavukcuoglu. 2016. WaveNet: A Generative Model\nfor Raw Audio. ISCA Workshop on Speech Synthesis\nWorkshop .\nOppenheim, A. V ., R. W. Schafer, and T. G. J. Stockham.\n1968. Nonlinear \ufb01ltering of multiplied and convolved sig-\nnals. Proceedings of the IEEE , 56(8):1264\u20131291.\nPanayotov, V ., G. Chen, D. Povey, and S. Khudanpur. 2015.\nLibrispeech: an ASR corpus based on public domain au-\ndio books. ICASSP .\nPeterson, G. E., W. S.-Y . Wang, and E. Sivertsen. 1958.\nSegmentation techniques in speech synthesis. JASA ,\n30(8):739\u2013742.\nPovey, D., A. Ghoshal, G. Boulianne, L. Burget, O. Glem-\nbek, N. Goel, M. Hannemann, P. Motlicek, Y . Qian,\nP. Schwarz, J. Silovsk \u00b4y, G. Stemmer, and K. Vesel \u00b4y.\n2011. The Kaldi speech recognition toolkit. ASRU .",
    "metadata": {
      "source": "16",
      "chunk_id": 54,
      "token_count": 746,
      "chapter_title": ""
    }
  },
  {
    "content": "K. Kavukcuoglu. 2016. WaveNet: A Generative Model\nfor Raw Audio. ISCA Workshop on Speech Synthesis\nWorkshop .\nOppenheim, A. V ., R. W. Schafer, and T. G. J. Stockham.\n1968. Nonlinear \ufb01ltering of multiplied and convolved sig-\nnals. Proceedings of the IEEE , 56(8):1264\u20131291.\nPanayotov, V ., G. Chen, D. Povey, and S. Khudanpur. 2015.\nLibrispeech: an ASR corpus based on public domain au-\ndio books. ICASSP .\nPeterson, G. E., W. S.-Y . Wang, and E. Sivertsen. 1958.\nSegmentation techniques in speech synthesis. JASA ,\n30(8):739\u2013742.\nPovey, D., A. Ghoshal, G. Boulianne, L. Burget, O. Glem-\nbek, N. Goel, M. Hannemann, P. Motlicek, Y . Qian,\nP. Schwarz, J. Silovsk \u00b4y, G. Stemmer, and K. Vesel \u00b4y.\n2011. The Kaldi speech recognition toolkit. ASRU .\nPrice, P. J., W. Fisher, J. Bernstein, and D. Pallet. 1988. The\nDARPA 1000-word resource management database for\ncontinuous speech recognition. ICASSP .\nPundak, G. and T. N. Sainath. 2016. Lower frame rate neural\nnetwork acoustic models. INTERSPEECH .\nRobinson, T. and F. Fallside. 1991. A recurrent error prop-\nagation network speech recognition system. Computer\nSpeech & Language , 5(3):259\u2013274.\nSagisaka, Y . 1988. Speech synthesis by rule using an optimal\nselection of non-uniform synthesis units. ICASSP .",
    "metadata": {
      "source": "16",
      "chunk_id": 55,
      "token_count": 434,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 30",
    "metadata": {
      "source": "16",
      "chunk_id": 56,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "30 Chapter 16 \u2022 Automatic Speech Recognition and Text-to-Speech\nSagisaka, Y ., N. Kaiki, N. Iwahashi, and K. Mimura. 1992.\nAtr \u2013 n-talk speech synthesis system. ICSLP .\nSakoe, H. and S. Chiba. 1971. A dynamic programming\napproach to continuous speech recognition. Proceedings\nof the Seventh International Congress on Acoustics , vol-\nume 3. Akad \u00b4emiai Kiad \u00b4o.\nSakoe, H. and S. Chiba. 1984. Dynamic programming al-\ngorithm optimization for spoken word recognition. IEEE\nTransactions on ASSP , ASSP-26(1):43\u201349.\nShannon, C. E. 1948. A mathematical theory of commu-\nnication. Bell System Technical Journal , 27(3):379\u2013423.\nContinued in the following volume.\nShen, J., R. Pang, R. J. Weiss, M. Schuster, N. Jaitly,\nZ. Yang, Z. Chen, Y . Zhang, Y . Wang, R. Skerry-Ryan,\nR. A. Saurous, Y . Agiomyrgiannakis, and Y . Wu. 2018.\nNatural TTS synthesis by conditioning WaveNet on mel\nspectrogram predictions. ICASSP .\nSproat, R., A. W. Black, S. F. Chen, S. Kumar, M. Ostendorf,\nand C. Richards. 2001. Normalization of non-standard\nwords. Computer Speech & Language , 15(3):287\u2013333.\nSproat, R. and K. Gorman. 2018. A brief summary of the\nKaggle text normalization challenge.\nStevens, K. N., S. Kasowski, and G. M. Fant. 1953. An elec-\ntrical analog of the vocal tract. JASA , 25(4):734\u2013742.\nStevens, S. S. and J. V olkmann. 1940. The relation of pitch\nto frequency: A revised scale. The American Journal of\nPsychology , 53(3):329\u2013353.\nStevens, S. S., J. V olkmann, and E. B. Newman. 1937. A\nscale for the measurement of the psychological magni-\ntude pitch. JASA , 8:185\u2013190.\nSyrdal, A. K., C. W. Wightman, A. Conkie, Y . Stylianou,\nM. Beutnagel, J. Schroeter, V . Strom, and K.-S. Lee.\n2000. Corpus-based techniques in the AT&T NEXTGEN\nsynthesis system. ICSLP .\nTaylor, P. 2009. Text-to-Speech Synthesis . Cambridge Uni-\nversity Press.\nTeranishi, R. and N. Umeda. 1968. Use of pronouncing dic-\ntionary in speech synthesis experiments. 6th International\nCongress on Acoustics .\nUmeda, N. 1976. Linguistic rules for text-to-speech synthe-\nsis.Proceedings of the IEEE , 64(4):443\u2013451.\nUmeda, N., E. Matui, T. Suzuki, and H. Omura. 1968. Syn-\nthesis of fairy tale using an analog vocal tract. 6th Inter-\nnational Congress on Acoustics .",
    "metadata": {
      "source": "16",
      "chunk_id": 57,
      "token_count": 755,
      "chapter_title": ""
    }
  },
  {
    "content": "Stevens, S. S., J. V olkmann, and E. B. Newman. 1937. A\nscale for the measurement of the psychological magni-\ntude pitch. JASA , 8:185\u2013190.\nSyrdal, A. K., C. W. Wightman, A. Conkie, Y . Stylianou,\nM. Beutnagel, J. Schroeter, V . Strom, and K.-S. Lee.\n2000. Corpus-based techniques in the AT&T NEXTGEN\nsynthesis system. ICSLP .\nTaylor, P. 2009. Text-to-Speech Synthesis . Cambridge Uni-\nversity Press.\nTeranishi, R. and N. Umeda. 1968. Use of pronouncing dic-\ntionary in speech synthesis experiments. 6th International\nCongress on Acoustics .\nUmeda, N. 1976. Linguistic rules for text-to-speech synthe-\nsis.Proceedings of the IEEE , 64(4):443\u2013451.\nUmeda, N., E. Matui, T. Suzuki, and H. Omura. 1968. Syn-\nthesis of fairy tale using an analog vocal tract. 6th Inter-\nnational Congress on Acoustics .\nVelichko, V . M. and N. G. Zagoruyko. 1970. Automatic\nrecognition of 200 words. International Journal of Man-\nMachine Studies , 2:223\u2013234.\nVintsyuk, T. K. 1968. Speech discrimination by dynamic\nprogramming. Cybernetics , 4(1):52\u201357. Original Rus-\nsian: Kibernetika 4(1):81-88. 1968.\nWaibel, A., T. Hanazawa, G. Hinton, K. Shikano, and K. J.\nLang. 1989. Phoneme recognition using time-delay neu-\nral networks. IEEE Transactions on ASSP , 37(3):328\u2013\n339.\nWang, Y ., R. Skerry-Ryan, D. Stanton, Y . Wu, R. J. Weiss,\nN. Jaitly, Z. Yang, Y . Xiao, Z. Chen, S. Bengio, Q. Le,\nY . Agiomyrgiannakis, R. Clark, and R. A. Saurous. 2017.\nTacotron: Towards end-to-end speech synthesis. INTER-\nSPEECH .Watanabe, S., T. Hori, S. Karita, T. Hayashi, J. Nishitoba,\nY . Unno, N. E. Y . Soplin, J. Heymann, M. Wiesner,\nN. Chen, A. Renduchintala, and T. Ochiai. 2018. ESPnet:\nEnd-to-end speech processing toolkit. INTERSPEECH .\nZhang, H., R. Sproat, A. H. Ng, F. Stahlberg, X. Peng,\nK. Gorman, and B. Roark. 2019. Neural models of text\nnormalization for speech applications. Computational\nLinguistics , 45(2):293\u2013337.",
    "metadata": {
      "source": "16",
      "chunk_id": 58,
      "token_count": 693,
      "chapter_title": ""
    }
  }
]