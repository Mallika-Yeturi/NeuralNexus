[
  {
    "content": "# 14\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n14Question Answering, Informa-\ntion Retrieval, and Retrieval-\nAugmented Generation\nPeople need to know things. So pretty much as soon as there were computers we\nwere asking them questions. Systems in the 1960s were answering questions about\nbaseball statistics and scienti\ufb01c facts. Even \ufb01ctional computers in the 1970s like\nDeep Thought, invented by Douglas Adams in The Hitchhiker\u2019s Guide to the Galaxy ,\nanswered \u201cthe Ultimate Question Of Life, The Universe, and Everything\u201d.1And\nbecause so much knowledge is encoded in text, question answering ( QA) systems QA\nwere performing at human levels even before LLMs: IBM\u2019s Watson system won the\nTV game-show Jeopardy! in 2011, surpassing humans at answering questions like:\nWILLIAM WILKINSON\u2019S \u201cAN ACCOUNT OF THE PRINCIPALITIES OF WALLACHIA AND MOLDOVIA\u201dINSPIRED THIS AUTHOR\u2019S MOST FAMOUS NOVEL\n2\nQuestion answering systems are designed to \ufb01ll human information needs .\nSince a lot of information is present in text form (on the web or in other data like\nour email, or books), question answering is closely related to the task behind search\nengines. Indeed, the distinction is becoming ever more fuzzy, as modern search\nengines are integrated with large language models trained to do question answering.\nQuestion answering systems often focus on a useful subset of information needs:\nfactoid questions , questions of fact or reasoning that can be answered with simple\nfacts expressed in short or medium-length texts, like the following:\n(14.1) Where is the Louvre Museum located?\n(14.2) Where does the energy in a nuclear explosion come from?\n(14.3) How to get a script l in latex?\nModern NLP systems answer these questions using large language models, in\none of two ways. The \ufb01rst is to make use of the method from Chapter 12: prompt\na pretrained and instruction-tuned LLM, an LLM that has been \ufb01netuned on ques-\ntion/answer datasets with the question in the prompt. For example, we could prompt\na causal language model with a string like\nQ: Where is the Louvre Museum located? A:\nhave it do conditional generation given this pre\ufb01x, and take the response as the an-\nswer. The idea is that language models have read a lot of facts in their pretraining\ndata, presumably including the location of the Louvre, and have encoded this infor-\nmation in their parameters.\nSimply prompting an LLM can be a useful approach to answer many factoid\nquestions. But it is not yet a complete solution for question answering.\n1The answer was 42, but unfortunately the question was never revealed.\n2The answer, of course, is \u2018Who is Bram Stoker\u2019, and the novel was Dracula .",
    "metadata": {
      "source": "14",
      "chunk_id": 0,
      "token_count": 654,
      "chapter_title": "14"
    }
  },
  {
    "content": "## Page 2",
    "metadata": {
      "source": "14",
      "chunk_id": 1,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "2CHAPTER 14 \u2022 Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG\nThe \ufb01rst and main problem is that large language models often give the wrong\nanswer! Large language models hallucinate . A hallucination is a response that is hallucinate\nnot faithful to the facts of the world. That is, when asked questions, large language\nmodels simply make up answers that sound reasonable. For example, Dahl et al.\n(2024) found that when asked questions about the legal domain (like about particular\nlegal cases), large language models hallucinated from 69% to 88% of the time!\nAnd it\u2019s not always possible to tell when language models are hallucinating,\npartly because LLMs aren\u2019t well- calibrated . In a calibrated system, the con\ufb01dence calibrated\nof a system in the correctness of its answer is highly correlated with the probability\nof an answer being correct. So if a calibrated system is wrong, at least it might hedge\nits answer or tell us to go check another source. But since language models are not\nwell-calibrated, they often give a very wrong answer with complete certainty (Zhou\net al., 2024).\nA second problem is that simply prompting a large language model doesn\u2019t allow\nus to ask questions about proprietary data. A common use of question answering is\nabout data like our personal email or medical records. Or a company may have\ninternal documents that contain answers for customer service or internal use. Or\nlegal \ufb01rms need to ask questions about legal discovery from proprietary documents.\nFinally, static large language models also have problems with questions about\nrapidly changing information (like questions about something that happened last\nweek) since LLMs won\u2019t have up-to-date information from after their release data.\nFor this reason the most common way to do question-answering with LLMs is\nretrieval-augmented generation orRAG , and that is the method we will focus on RAG\nin this chapter. In RAG we use information retrieval (IR) techniques to retrieveinformation\nretrieval\ndocuments that are likely to have information that might help answer the question.\nThen we use a large language model to generate an answer given these documents.\nBasing our answers on retrieved documents can solve some of the problems with\nusing simple prompting to answer questions. First, it helps ensure that the answer is\ngrounded in facts from some curated dataset. And the system can give the user the\nanswer accompanied by the context of the passage or document the answer came\nfrom. This information can help users have con\ufb01dence in the accuracy of the answer\n(or help them spot when it is wrong!). And these retrieval techniques can be used on\nany proprietary data we want, such as legal or medical data for those applications.\nWe\u2019ll begin by introducing information retrieval, the task of choosing the most\nrelevant document from a document set given a user\u2019s query expressing their infor-\nmation need. We\u2019ll see the classic method based on cosines of sparse tf-idf vectors,\na modern neural \u2018dense\u2019 retrievers based on instead representing queries and docu-\nments neurally with BERT or other language models. We then introduce retriever-\nbased question answering and the retrieval-augmented generation paradigm.\nFinally, we\u2019ll discuss various QA datasets. These are used for \ufb01netuning LLMs\nin instruction tuning, as we saw in Chapter 12. And they are also used as bench-\nmarks, since question answering has an important function as a benchmark for mea-\nsuring the abilities of language models.\n14.1 Information Retrieval\nInformation retrieval orIRis the name of the \ufb01eld encompassing the retrieval of allinformation\nretrieval",
    "metadata": {
      "source": "14",
      "chunk_id": 2,
      "token_count": 777,
      "chapter_title": ""
    }
  },
  {
    "content": "grounded in facts from some curated dataset. And the system can give the user the\nanswer accompanied by the context of the passage or document the answer came\nfrom. This information can help users have con\ufb01dence in the accuracy of the answer\n(or help them spot when it is wrong!). And these retrieval techniques can be used on\nany proprietary data we want, such as legal or medical data for those applications.\nWe\u2019ll begin by introducing information retrieval, the task of choosing the most\nrelevant document from a document set given a user\u2019s query expressing their infor-\nmation need. We\u2019ll see the classic method based on cosines of sparse tf-idf vectors,\na modern neural \u2018dense\u2019 retrievers based on instead representing queries and docu-\nments neurally with BERT or other language models. We then introduce retriever-\nbased question answering and the retrieval-augmented generation paradigm.\nFinally, we\u2019ll discuss various QA datasets. These are used for \ufb01netuning LLMs\nin instruction tuning, as we saw in Chapter 12. And they are also used as bench-\nmarks, since question answering has an important function as a benchmark for mea-\nsuring the abilities of language models.\n14.1 Information Retrieval\nInformation retrieval orIRis the name of the \ufb01eld encompassing the retrieval of allinformation\nretrieval\nIR manner of media based on user information needs. The resulting IR system is often\ncalled a search engine . Our goal in this section is to give a suf\ufb01cient overview of IR",
    "metadata": {
      "source": "14",
      "chunk_id": 3,
      "token_count": 318,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\n14.1 \u2022 I NFORMATION RETRIEVAL 3\nto see its application to question answering. Readers with more interest speci\ufb01cally\nin information retrieval should see the Historical Notes section at the end of the\nchapter and textbooks like Manning et al. (2008).\nThe IR task we consider is called ad hoc retrieval , in which a user poses a ad hoc retrieval\nquery to a retrieval system, which then returns an ordered set of documents from\nsome collection . Adocument refers to whatever unit of text the system indexes and document\nretrieves (web pages, scienti\ufb01c papers, news articles, or even shorter passages like\nparagraphs). A collection refers to a set of documents being used to satisfy user collection\nrequests. A term refers to a word in a collection, but it may also include phrases. term\nFinally, a query represents a user\u2019s information need expressed as a set of terms. query\nThe high-level architecture of an ad hoc retrieval engine is shown in Fig. 14.1.\nDocumentDocumentDocumentDocumentDocumentDocumentQuery ProcessingIndexingSearchDocumentDocumentDocumentDocumentDocumentRanked DocumentsDocumentqueryInvertedIndexqueryvectordocument collection\nFigure 14.1 The architecture of an ad hoc IR system.\nThe basic IR architecture uses the vector space model we introduced in Chap-\nter 6, in which we map queries and document to vectors based on unigram word\ncounts, and use the cosine similarity between the vectors to rank potential documents\n(Salton, 1971). This is thus an example of the bag-of-words model introduced in\nChapter 4, since words are considered independently of their positions.\n14.1.1 Term weighting and document scoring\nLet\u2019s look at the details of how the match between a document and query is scored.\nWe don\u2019t use raw word counts in IR, instead computing a term weight for each term weight\ndocument word. Two term weighting schemes are common: the tf-idf weighting\nintroduced in Chapter 6, and a slightly more powerful variant called BM25 . BM25\nWe\u2019ll reintroduce tf-idf here so readers don\u2019t need to look back at Chapter 6.\nTf-idf (the \u2018-\u2019 here is a hyphen, not a minus sign) is the product of two terms, the\nterm frequency tfand the inverse document frequency idf.\nThe term frequency tells us how frequent the word is; words that occur more\noften in a document are likely to be informative about the document\u2019s contents. We\nusually use the log 10of the word frequency, rather than the raw count. The intuition\nis that a word appearing 100 times in a document doesn\u2019t make that word 100 times\nmore likely to be relevant to the meaning of the document. We also need to do\nsomething special with counts of 0, since we can\u2019t take the log of 0.3\ntft;d=(\n1+log10count (t;d) if count (t;d)>0\n0 otherwise(14.4)\n3We can also use this alternative formulation, which we have used in earlier editions: tf t;d=\nlog10(count (t;d)+1)",
    "metadata": {
      "source": "14",
      "chunk_id": 4,
      "token_count": 663,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4CHAPTER 14 \u2022 Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG\nIf we use log weighting, terms which occur 0 times in a document would have tf =0,\n1 times in a document tf =1+log10(1) =1+0=1, 10 times in a document tf =\n1+log10(10) =2, 100 times tf =1+log10(100) =3, 1000 times tf =4, and so on.\nThedocument frequency dftof a term tis the number of documents it oc-\ncurs in. Terms that occur in only a few documents are useful for discriminating\nthose documents from the rest of the collection; terms that occur across the entire\ncollection aren\u2019t as helpful. The inverse document frequency oridfterm weight\n(Sparck Jones, 1972) is de\ufb01ned as:\nidft=log10N\ndft(14.5)\nwhere Nis the total number of documents in the collection, and df tis the number\nof documents in which term toccurs. The fewer documents in which a term occurs,\nthe higher this weight; the lowest weight of 0 is assigned to terms that occur in every\ndocument.\nHere are some idf values for some words in the corpus of Shakespeare plays,\nranging from extremely informative words that occur in only one play like Romeo ,\nto those that occur in a few like salad orFalstaff , to those that are very common like\nfoolor so common as to be completely non-discriminative since they occur in all 37\nplays like good orsweet .4\nWord df idf\nRomeo 1 1.57\nsalad 2 1.27\nFalstaff 4 0.967\nforest 12 0.489\nbattle 21 0.246\nwit 34 0.037\nfool 36 0.012\ngood 37 0\nsweet 37 0\nThetf-idf value for word tin document dis then the product of term frequency\ntft;dand IDF:\ntf-idf(t;d) =tft;d\u0001idft (14.6)\n14.1.2 Document Scoring\nWe score document dby the cosine of its vector dwith the query vector q:\nscore(q;d) =cos(q;d) =q\u0001d\njqjjdj(14.7)\nAnother way to think of the cosine computation is as the dot product of unit vectors;\nwe \ufb01rst normalize both the query and document vector to unit vectors, by dividing\nby their lengths, and then take the dot product:\nscore(q;d) =cos(q;d) =q\njqj\u0001d\njdj(14.8)\n4Sweet was one of Shakespeare\u2019s favorite adjectives, a fact probably related to the increased use of\nsugar in European recipes around the turn of the 16th century (Jurafsky, 2014, p. 175).",
    "metadata": {
      "source": "14",
      "chunk_id": 5,
      "token_count": 638,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5",
    "metadata": {
      "source": "14",
      "chunk_id": 6,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "14.1 \u2022 I NFORMATION RETRIEVAL 5\nWe can spell out Eq. 14.8, using the tf-idf values and spelling out the dot product as\na sum of products:\nscore(q;d) =X\nt2qtf-idf(t;q)qP\nqi2qtf-idf2(qi;q)\u0001tf-idf(t;d)qP\ndi2dtf-idf2(di;d)(14.9)\nNow let\u2019s use Eq. 14.9 to walk through an example of a tiny query against a\ncollection of 4 nano documents, computing tf-idf values and seeing the rank of the\ndocuments. We\u2019ll assume all words in the following query and documents are down-\ncased and punctuation is removed:\nQuery :sweet love\nDoc 1 :Sweet sweet nurse! Love?\nDoc 2 :Sweet sorrow\nDoc 3 :How sweet is love?\nDoc 4 :Nurse!\nFig. 14.2 shows the computation of the tf-idf cosine between the query and Doc-\nument 1, and the query and Document 2. The cosine is the normalized dot product\nof tf-idf values, so for the normalization we must need to compute the document\nvector lengthsjqj,jd1j, andjd2jfor the query and the \ufb01rst two documents using\nEq. 14.4, Eq. 14.5, Eq. 14.6, and Eq. 14.9 (computations for Documents 3 and 4 are\nalso needed but are left as an exercise for the reader). The dot product between the\nvectors is the sum over dimensions of the product, for each dimension, of the values\nof the two tf-idf vectors for that dimension. This product is only non-zero where\nboth the query and document have non-zero values, so for this example, in which\nonly sweet andlove have non-zero values in the query, the dot product will be the\nsum of the products of those elements of each vector.\nDocument 1 has a higher cosine with the query (0.747) than Document 2 has\nwith the query (0.0779), and so the tf-idf cosine model would rank Document 1\nabove Document 2. This ranking is intuitive given the vector space model, since\nDocument 1 has both terms including two instances of sweet , while Document 2 is\nmissing one of the terms. We leave the computation for Documents 3 and 4 as an\nexercise for the reader.\nIn practice, there are many variants and approximations to Eq. 14.9. For exam-\nple, we might choose to simplify processing by removing some terms. To see this,\nlet\u2019s start by expanding the formula for tf-idf in Eq. 14.9 to explicitly mention the tf\nand idf terms from Eq. 14.6:\nscore(q;d) =X\nt2qtft;q\u0001idftqP\nqi2qtf-idf2(qi;q)\u0001tft;d\u0001idftqP\ndi2dtf-idf2(di;d)(14.10)\nIn one common variant of tf-idf cosine, for example, we drop the idf term for the\ndocument. Eliminating the second copy of the idf term (since the identical term is\nalready computed for the query) turns out to sometimes result in better performance:\nscore(q;d) =X\nt2qtft;q\u0001idftqP\nqi2qtf-idf2(qi;q)\u0001tft;d\u0001idftqP\ndi2dtf-idf2(di;d)(14.11)\nOther variants of tf-idf eliminate various other terms.\nA slightly more complex variant in the tf-idf family is the BM25 weighting BM25",
    "metadata": {
      "source": "14",
      "chunk_id": 7,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6",
    "metadata": {
      "source": "14",
      "chunk_id": 8,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "6CHAPTER 14 \u2022 Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG\nQuery\nword cnt tf df idf tf-idf n\u2019lized = tf-idf/jqj\nsweet 1 1 3 0.125 0.125 0.383\nnurse 0 0 2 0.301 0 0\nlove 1 1 2 0.301 0.301 0.924\nhow 0 0 1 0.602 0 0\nsorrow 0 0 1 0.602 0 0\nis 0 0 1 0.602 0 0\njqj=p\n:1252+:3012=:326\nDocument 1 Document 2\nword cnt tf tf-idf n\u2019lized \u0002q cnt tf tf-idf n\u2019lized \u0002q\nsweet 2 1.301 0.163 0.357 0.137 1 1.000 0.125 0.203 0.0779\nnurse 1 1.000 0.301 0.661 0 0 0 0 0 0\nlove 1 1.000 0.301 0.661 0.610 0 0 0 0 0\nhow 0 0 0 0 0 0 0 0 0 0\nsorrow 0 0 0 0 0 1 1.000 0.602 0.979 0\nis 0 0 0 0 0 0 0 0 0 0\njd1j=p\n:1632+:3012+:3012=:456 jd2j=p\n:1252+:6022=:615\nCosine:Pof column: 0.747 Cosine:Pof column: 0.0779\nFigure 14.2 Computation of tf-idf cosine score between the query and nano-documents 1 (0.747) and 2\n(0.0779), using Eq. 14.4, Eq. 14.5, Eq. 14.6 and Eq. 14.9.\nscheme (sometimes called Okapi BM25 after the Okapi IR system in which it was\nintroduced (Robertson et al., 1995)). BM25 adds two parameters: k, a knob that\nadjust the balance between term frequency and IDF, and b, which controls the im-\nportance of document length normalization. The BM25 score of a document dgiven\na query qis:\nX\nt2qIDFz}|{\nlog\u0012N\ndft\u0013weighted tfz}|{\ntft;d\nk\u0010\n1\u0000b+b\u0010\njdj\njdavgj\u0011\u0011\n+tft;d(14.12)\nwherejdavgjis the length of the average document. When kis 0, BM25 reverts to\nno use of term frequency, just a binary selection of terms in the query (plus idf).\nA large kresults in raw term frequency (plus idf). branges from 1 (scaling by\ndocument length) to 0 (no length scaling). Manning et al. (2008) suggest reasonable\nvalues are k = [1.2,2] and b = 0.75. Kamphuis et al. (2020) is a useful summary of\nthe many minor variants of BM25.\nStop words In the past it was common to remove high-frequency words from both\nthe query and document before representing them. The list of such high-frequency",
    "metadata": {
      "source": "14",
      "chunk_id": 9,
      "token_count": 780,
      "chapter_title": ""
    }
  },
  {
    "content": "introduced (Robertson et al., 1995)). BM25 adds two parameters: k, a knob that\nadjust the balance between term frequency and IDF, and b, which controls the im-\nportance of document length normalization. The BM25 score of a document dgiven\na query qis:\nX\nt2qIDFz}|{\nlog\u0012N\ndft\u0013weighted tfz}|{\ntft;d\nk\u0010\n1\u0000b+b\u0010\njdj\njdavgj\u0011\u0011\n+tft;d(14.12)\nwherejdavgjis the length of the average document. When kis 0, BM25 reverts to\nno use of term frequency, just a binary selection of terms in the query (plus idf).\nA large kresults in raw term frequency (plus idf). branges from 1 (scaling by\ndocument length) to 0 (no length scaling). Manning et al. (2008) suggest reasonable\nvalues are k = [1.2,2] and b = 0.75. Kamphuis et al. (2020) is a useful summary of\nthe many minor variants of BM25.\nStop words In the past it was common to remove high-frequency words from both\nthe query and document before representing them. The list of such high-frequency\nwords to be removed is called a stop list . The intuition is that high-frequency terms stop list\n(often function words like the,a,to) carry little semantic weight and may not help\nwith retrieval, and can also help shrink the inverted index \ufb01les we describe below.\nThe downside of using a stop list is that it makes it dif\ufb01cult to search for phrases\nthat contain words in the stop list. For example, common stop lists would reduce the\nphrase to be or not to be to the phrase not. In modern IR systems, the use of stop lists\nis much less common, partly due to improved ef\ufb01ciency and partly because much\nof their function is already handled by IDF weighting, which downweights function",
    "metadata": {
      "source": "14",
      "chunk_id": 10,
      "token_count": 427,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\n14.1 \u2022 I NFORMATION RETRIEVAL 7\nwords that occur in every document. Nonetheless, stop word removal is occasionally\nuseful in various NLP tasks so is worth keeping in mind.\n14.1.3 Inverted Index\nIn order to compute scores, we need to ef\ufb01ciently \ufb01nd documents that contain words\nin the query. (Any document that contains none of the query terms will have a score\nof 0 and can be ignored.) The basic search problem in IR is thus to \ufb01nd all documents\nd2Cthat contain a term q2Q.\nThe data structure for this task is the inverted index , which we use for mak- inverted index\ning this search ef\ufb01cient, and also conveniently storing useful information like the\ndocument frequency and the count of each term in each document.\nAn inverted index, given a query term, gives a list of documents that contain the\nterm. It consists of two parts, a dictionary and the postings . The dictionary is a list postings\nof terms (designed to be ef\ufb01ciently accessed), each pointing to a postings list for the\nterm. A postings list is the list of document IDs associated with each term, which\ncan also contain information like the term frequency or even the exact positions of\nterms in the document. The dictionary can also store the document frequency for\neach term. For example, a simple inverted index for our 4 sample documents above,\nwith each word containing its document frequency in fg, and a pointer to a postings\nlist that contains document IDs and term counts in [], might look like the following:\nhowf1g!3 [1]\nisf1g!3 [1]\nlovef2g!1 [1]!3 [1]\nnursef2g!1 [1]!4 [1]\nsorryf1g!2 [1]\nsweetf3g!1 [2]!2 [1]!3 [1]\nGiven a list of terms in query, we can very ef\ufb01ciently get lists of all candidate\ndocuments, together with the information necessary to compute the tf-idf scores we\nneed.\nThere are alternatives to the inverted index. For the question-answering domain\nof \ufb01nding Wikipedia pages to match a user query, Chen et al. (2017) show that\nindexing based on bigrams works better than unigrams, and use ef\ufb01cient hashing\nalgorithms rather than the inverted index to make the search ef\ufb01cient.\n14.1.4 Evaluation of Information-Retrieval Systems\nWe measure the performance of ranked retrieval systems using the same precision\nandrecall metrics we have been using. We make the assumption that each docu-\nment returned by the IR system is either relevant to our purposes or not relevant .\nPrecision is the fraction of the returned documents that are relevant, and recall is the\nfraction of all relevant documents that are returned. More formally, let\u2019s assume a\nsystem returns Tranked documents in response to an information request, a subset\nRof these are relevant, a disjoint subset, N, are the remaining irrelevant documents,\nandUdocuments in the collection as a whole are relevant to this request. Precision\nand recall are then de\ufb01ned as:\nPrecision =jRj\njTjRecall =jRj\njUj(14.13)\nUnfortunately, these metrics don\u2019t adequately measure the performance of a system\nthatranks the documents it returns. If we are comparing the performance of two",
    "metadata": {
      "source": "14",
      "chunk_id": 11,
      "token_count": 742,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8CHAPTER 14 \u2022 Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG\nranked retrieval systems, we need a metric that prefers the one that ranks the relevant\ndocuments higher. We need to adapt precision and recall to capture how well a\nsystem does at putting relevant documents higher in the ranking.\nRank Judgment Precision Rank Recall Rank\n1 R 1.0 .11\n2 N .50 .11\n3 R .66 .22\n4 N .50 .22\n5 R .60 .33\n6 R .66 .44\n7 N .57 .44\n8 R .63 .55\n9 N .55 .55\n10 N .50 .55\n11 R .55 .66\n12 N .50 .66\n13 N .46 .66\n14 N .43 .66\n15 R .47 .77\n16 N .44 .77\n17 N .44 .77\n18 R .44 .88\n19 N .42 .88\n20 N .40 .88\n21 N .38 .88\n22 N .36 .88\n23 N .35 .88\n24 N .33 .88\n25 R .36 1.0\nFigure 14.3 Rank-speci\ufb01c precision and recall values calculated as we proceed down\nthrough a set of ranked documents (assuming the collection has 9 relevant documents).\nLet\u2019s turn to an example. Assume the table in Fig. 14.3 gives rank-speci\ufb01c pre-\ncision and recall values calculated as we proceed down through a set of ranked doc-\numents for a particular query; the precisions are the fraction of relevant documents\nseen at a given rank, and recalls the fraction of relevant documents found at the same\nrank. The recall measures in this example are based on this query having 9 relevant\ndocuments in the collection as a whole.\nNote that recall is non-decreasing; when a relevant document is encountered,\nrecall increases, and when a non-relevant document is found it remains unchanged.\nPrecision, on the other hand, jumps up and down, increasing when relevant doc-\numents are found, and decreasing otherwise. The most common way to visualize\nprecision and recall is to plot precision against recall in a precision-recall curve ,precision-recall\ncurve\nlike the one shown in Fig. 14.4 for the data in table 14.3.\nFig. 14.4 shows the values for a single query. But we\u2019ll need to combine values\nfor all the queries, and in a way that lets us compare one system to another. One way\nof doing this is to plot averaged precision values at 11 \ufb01xed levels of recall (0 to 100,\nin steps of 10). Since we\u2019re not likely to have datapoints at these exact levels, we\nuseinterpolated precision values for the 11 recall values from the data points we dointerpolated\nprecision\nhave. We can accomplish this by choosing the maximum precision value achieved\nat any level of recall at or above the one we\u2019re calculating. In other words,\nIntPrecision (r) =max\ni>=rPrecision (i) (14.14)",
    "metadata": {
      "source": "14",
      "chunk_id": 12,
      "token_count": 668,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\n14.1 \u2022 I NFORMATION RETRIEVAL 9\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall0.00.20.40.60.81.0Precision\nFigure 14.4 The precision recall curve for the data in table 14.3.\nThis interpolation scheme not only lets us average performance over a set of queries,\nbut also helps smooth over the irregular precision values in the original data. It is\ndesigned to give systems the bene\ufb01t of the doubt by assigning the maximum preci-\nsion value achieved at higher levels of recall from the one being measured. Fig. 14.5\nand Fig. 14.6 show the resulting interpolated data points from our example.\nInterpolated Precision Recall\n1.0 0.0\n1.0 .10\n.66 .20\n.66 .30\n.66 .40\n.63 .50\n.55 .60\n.47 .70\n.44 .80\n.36 .90\n.36 1.0\nFigure 14.5 Interpolated data points from Fig. 14.3.\nGiven curves such as that in Fig. 14.6 we can compare two systems or approaches\nby comparing their curves. Clearly, curves that are higher in precision across all\nrecall values are preferred. However, these curves can also provide insight into the\noverall behavior of a system. Systems that are higher in precision toward the left\nmay favor precision over recall, while systems that are more geared towards recall\nwill be higher at higher levels of recall (to the right).\nA second way to evaluate ranked retrieval is mean average precision (MAP),mean average\nprecision\nwhich provides a single metric that can be used to compare competing systems or\napproaches. In this approach, we again descend through the ranked list of items,\nbut now we note the precision only at those points where a relevant item has been\nencountered (for example at ranks 1, 3, 5, 6 but not 2 or 4 in Fig. 14.3). For a single\nquery, we average these individual precision measurements over the return set (up\nto some \ufb01xed cutoff). More formally, if we assume that Rris the set of relevant",
    "metadata": {
      "source": "14",
      "chunk_id": 13,
      "token_count": 492,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 CHAPTER 14 \u2022 Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG\nInterpolated Precision Recall Curve\n00.10.20.30.40.50.60.70.80.91\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nRecallPrecision\nFigure 14.6 An 11 point interpolated precision-recall curve. Precision at each of the 11\nstandard recall levels is interpolated for each query from the maximum at any higher level of\nrecall. The original measured precision recall points are also shown.\ndocuments at or above r, then the average precision (AP) for a single query is\nAP=1\njRrjX\nd2RrPrecision r(d) (14.15)\nwhere Precision r(d)is the precision measured at the rank at which document dwas\nfound. For an ensemble of queries Q, we then average over these averages, to get\nour \ufb01nal MAP measure:\nMAP =1\njQjX\nq2QAP(q) (14.16)\nThe MAP for the single query (hence = AP) in Fig. 14.3 is 0.6.\n14.2 Information Retrieval with Dense Vectors\nThe classic tf-idf or BM25 algorithms for IR have long been known to have a con-\nceptual \ufb02aw: they work only if there is exact overlap of words between the query\nand document. In other words, the user posing a query (or asking a question) needs\nto guess exactly what words the writer of the answer might have used, an issue called\nthevocabulary mismatch problem (Furnas et al., 1987).\nThe solution to this problem is to use an approach that can handle synonymy:\ninstead of (sparse) word-count vectors, using (dense) embeddings. This idea was\n\ufb01rst proposed for retrieval in the last century under the name of Latent Semantic\nIndexing approach (Deerwester et al., 1990), but is implemented in modern times\nvia encoders like BERT.\nThe most powerful approach is to present both the query and the document to a\nsingle encoder, allowing the transformer self-attention to see all the tokens of both",
    "metadata": {
      "source": "14",
      "chunk_id": 14,
      "token_count": 501,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\n14.2 \u2022 I NFORMATION RETRIEVAL WITH DENSE VECTORS 11\nthe query and the document, and thus building a representation that is sensitive to\nthe meanings of both query and document. Then a linear layer can be put on top of\nthe [CLS] token to predict a similarity score for the query/document tuple:\nz=BERT (q;[SEP] ;d)[CLS]\nscore(q;d) =softmax (U(z)) (14.17)\nThis architecture is shown in Fig. 14.7a. Usually the retrieval step is not done on\nan entire document. Instead documents are broken up into smaller passages, such\nas non-overlapping \ufb01xed-length chunks of say 100 tokens, and the retriever encodes\nand retrieves these passages rather than entire documents. The query and document\nhave to be made to \ufb01t in the BERT 512-token window, for example by truncating\nthe query to 64 tokens and truncating the document if necessary so that it, the query,\n[CLS], and [SEP] \ufb01t in 512 tokens. The BERT system together with the linear layer\nUcan then be \ufb01ne-tuned for the relevance task by gathering a tuning dataset of\nrelevant and non-relevant passages.\nQueryDocument\u2026\u2026\u2026\u2026\u2026\u2026[sep]s(q,d)zCLSU\nQueryzCLS_QzCLS_D\nDocument\u2026\u2026\u2026\u2026\u2026\u2026\u2022s(q,d)\n(a) (b)\nFigure 14.7 Two ways to do dense retrieval, illustrated by using lines between layers to schematically rep-\nresent self-attention: (a) Use a single encoder to jointly encode query and document and \ufb01netune to produce a\nrelevance score with a linear layer over the CLS token. This is too compute-expensive to use except in rescoring\n(b) Use separate encoders for query and document, and use the dot product between CLS token outputs for the\nquery and document as the score. This is less compute-expensive, but not as accurate.\nThe problem with the full BERT architecture in Fig. 14.7a is the expense in\ncomputation and time. With this architecture, every time we get a query, we have to\npass every single single document in our entire collection through a BERT encoder\njointly with the new query! This enormous use of resources is impractical for real\ncases.\nAt the other end of the computational spectrum is a much more ef\ufb01cient archi-\ntecture, the bi-encoder . In this architecture we can encode the documents in the\ncollection only one time by using two separate encoder models, one to encode the\nquery and one to encode the document. We encode each document, and store all\nthe encoded document vectors in advance. When a query comes in, we encode just\nthis query and then use the dot product between the query vector and the precom-\nputed document vectors as the score for each candidate document (Fig. 14.7b). For\nexample, if we used BERT, we would have two encoders BERT Qand BERT Dand",
    "metadata": {
      "source": "14",
      "chunk_id": 15,
      "token_count": 652,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12",
    "metadata": {
      "source": "14",
      "chunk_id": 16,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "12 CHAPTER 14 \u2022 Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG\nwe could represent the query and document as the [CLS] token of the respective\nencoders (Karpukhin et al., 2020):\nzq=BERT Q(q)[CLS]\nzd=BERT D(d)[CLS]\nscore(q;d) =zq\u0001zd (14.18)\nThe bi-encoder is much cheaper than a full query/document encoder, but is also\nless accurate, since its relevance decision can\u2019t take full advantage of all the possi-\nble meaning interactions between all the tokens in the query and the tokens in the\ndocument.\nThere are numerous approaches that lie in between the full encoder and the bi-\nencoder. One intermediate alternative is to use cheaper methods (like BM25) as the\n\ufb01rst pass relevance ranking for each document, take the top N ranked documents,\nand use expensive methods like the full BERT scoring to rerank only the top N\ndocuments rather than the whole set.\nAnother intermediate approach is the ColBERT approach of Khattab and Za- ColBERT\nharia (2020) and Khattab et al. (2021), shown in Fig. 14.8. This method separately\nencodes the query and document, but rather than encoding the entire query or doc-\nument into one vector, it separately encodes each of them into contextual represen-\ntations for each token. These BERT representations of each document word can be\npre-stored for ef\ufb01ciency. The relevance score between a query qand a document dis\na sum of maximum similarity (MaxSim) operators between tokens in qand tokens\nind. Essentially, for each token in q, ColBERT \ufb01nds the most contextually simi-\nlar token in d, and then sums up these similarities. A relevant document will have\ntokens that are contextually very similar to the query.\nMore formally, a question qis tokenized as [q1;:::; qn], prepended with a [CLS]\nand a special [Q]token, truncated to N=32 tokens (or padded with [MASK] tokens if\nit is shorter), and passed through BERT to get output vectors q= [q1;:::;qN]. The\npassage dwith tokens [d1;:::; dm], is processed similarly, including a [CLS] and\nspecial [D]token. A linear layer is applied on top of dandqto control the output\ndimension, so as to keep the vectors small for storage ef\ufb01ciency, and vectors are\nrescaled to unit length, producing the \ufb01nal vector sequences Eq(length N) and Ed\n(length m). The ColBERT scoring mechanism is:\nscore(q;d) =NX\ni=1mmax\nj=1Eqi\u0001Edj(14.19)\nWhile the interaction mechanism has no tunable parameters, the ColBERT ar-\nchitecture still needs to be trained end-to-end to \ufb01ne-tune the BERT encoders and\ntrain the linear layers (and the special [Q]and[D]embeddings) from scratch. It is\ntrained on tripleshq;d+;d\u0000iof query q, positive document d+and negative docu-\nment d\u0000to produce a score for each document using Eq. 14.19, optimizing model\nparameters using a cross-entropy loss.\nAll the supervised algorithms (like ColBERT or the full-interaction version of\nthe BERT algorithm applied for reranking) need training data in the form of queries\ntogether with relevant and irrelevant passages or documents (positive and negative",
    "metadata": {
      "source": "14",
      "chunk_id": 17,
      "token_count": 766,
      "chapter_title": ""
    }
  },
  {
    "content": "passage dwith tokens [d1;:::; dm], is processed similarly, including a [CLS] and\nspecial [D]token. A linear layer is applied on top of dandqto control the output\ndimension, so as to keep the vectors small for storage ef\ufb01ciency, and vectors are\nrescaled to unit length, producing the \ufb01nal vector sequences Eq(length N) and Ed\n(length m). The ColBERT scoring mechanism is:\nscore(q;d) =NX\ni=1mmax\nj=1Eqi\u0001Edj(14.19)\nWhile the interaction mechanism has no tunable parameters, the ColBERT ar-\nchitecture still needs to be trained end-to-end to \ufb01ne-tune the BERT encoders and\ntrain the linear layers (and the special [Q]and[D]embeddings) from scratch. It is\ntrained on tripleshq;d+;d\u0000iof query q, positive document d+and negative docu-\nment d\u0000to produce a score for each document using Eq. 14.19, optimizing model\nparameters using a cross-entropy loss.\nAll the supervised algorithms (like ColBERT or the full-interaction version of\nthe BERT algorithm applied for reranking) need training data in the form of queries\ntogether with relevant and irrelevant passages or documents (positive and negative\nexamples). There are various semi-supervised ways to get labels; some datasets (like\nMS MARCO Ranking, Section 14.3.2) contain gold positive examples. Negative\nexamples can be sampled randomly from the top-1000 results from some existing\nIR system. If datasets don\u2019t have labeled positive examples, iterative methods like",
    "metadata": {
      "source": "14",
      "chunk_id": 18,
      "token_count": 355,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\n14.3 \u2022 A NSWERING QUESTIONS WITH RAG 13\nQueryDocument\u2026\u2026\u2026\u2026\u2026\u2026s(q,d)MaxSimMaxSimMaxSim\u2211\nnormnormnormnormnormnorm\nFigure 14.8 A sketch of the ColBERT algorithm at inference time. The query and docu-\nment are \ufb01rst passed through separate BERT encoders. Similarity between query and doc-\nument is computed by summing a soft alignment between the contextual representations of\ntokens in the query and the document. Training is end-to-end. (Various details aren\u2019t de-\npicted; for example the query is prepended by a [CLS] and[Q:] tokens, and the document\nby[CLS] and[D:] tokens). Figure adapted from Khattab and Zaharia (2020).\nrelevance-guided supervision can be used (Khattab et al., 2021) which rely on the\nfact that many datasets contain short answer strings. In this method, an existing IR\nsystem is used to harvest examples that do contain short answer strings (the top few\nare taken as positives) or don\u2019t contain short answer strings (the top few are taken as\nnegatives), these are used to train a new retriever, and then the process is iterated.\nEf\ufb01ciency is an important issue, since every possible document must be ranked\nfor its similarity to the query. For sparse word-count vectors, the inverted index\nallows this very ef\ufb01ciently. For dense vector algorithms \ufb01nding the set of dense\ndocument vectors that have the highest dot product with a dense query vector is\nan instance of the problem of nearest neighbor search . Modern systems there-\nfore make use of approximate nearest neighbor vector search algorithms like Faiss Faiss\n(Johnson et al., 2017).\n14.3 Answering Questions with RAG\nThe dominant paradigm for question answering is to answer a user\u2019s question by \ufb01rst\n\ufb01nding supportive text segments from the web or another other large collection of\ndocuments, and then generating an answer based on the documents. The method of\ngenerating based on retrieved documents is called retrieval-augmented generation\norRAG , and the two components are sometimes called the retriever and the reader\n(Chen et al., 2017). Fig. 14.9 sketches out this standard QA model.\nIn the \ufb01rst stage of the 2-stage retrieve and read model in Fig. 14.9 we retrieve\nrelevant passages from a text collection, for example using the dense retrievers of the\nprevious section. In the second reader stage, we generate the answer via retrieval-",
    "metadata": {
      "source": "14",
      "chunk_id": 19,
      "token_count": 553,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n14 CHAPTER 14 \u2022 Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG\nQ: When wasthe premiere ofThe Magic Flute?RelevantDocsA:  1791RetrieverIndexed Docs\nquerydocsLLMpromptReader/Generator\nFigure 14.9 Retrieval-based question answering has two stages: retrieval , which returns relevant documents\nfrom the collection, and reading , in which an LLM generates answers given the documents as a prompt.\naugmented generation . In this method, we take a large pretrained language model,\ngive it the set of retrieved passages and other text as its prompt, and autoregressively\ngenerate a new answer token by token.\n14.3.1 Retrieval-Augmented Generation\nThe standard reader algorithm is to generate from a large language model, condi-\ntioned on the retrieved passages. This method is known as retrieval-augmented\ngeneration , orRAG .retrieval-\naugmented\ngeneration\nRAG Recall that in simple conditional generation, we can cast the task of question\nanswering as word prediction by giving a language model a question and a token\nlikeA:suggesting that an answer should come next:\nQ: Who wrote the book ``The Origin of Species\"? A:\nThen we generate autoregressively conditioned on this text.\nMore formally, recall that simple autoregressive language modeling computes\nthe probability of a string from the previous tokens:\np(x1;:::; xn) =nY\ni=1p(xijx<i)\nAnd simple conditional generation for question answering adds a prompt like Q:,\nfollowed by a query q, and A:, all concatenated:\np(x1;:::; xn) =nY\ni=1p([Q:] ;q;[A:] ;x<i)\nThe advantage of using a large language model is the enormous amount of\nknowledge encoded in its parameters from the text it was pretrained on. But as\nwe mentioned at the start of the chapter, while this kind of simple prompted gener-\nation can work \ufb01ne for many simple factoid questions, it is not a general solution\nfor QA, because it leads to hallucination, is unable to show users textual evidence to\nsupport the answer, and is unable to answer questions from proprietary data.\nThe idea of retrieval-augmented generation is to address these problems by con-\nditioning on the retrieved passages as part of the pre\ufb01x, perhaps with some prompt\ntext like \u201cBased on these texts, answer this question:\u201d. Let\u2019s suppose we have a\nquery q, and call the set of retrieved passages based on it R( q). For example, we\ncould have a prompt like:",
    "metadata": {
      "source": "14",
      "chunk_id": 20,
      "token_count": 565,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\n14.3 \u2022 A NSWERING QUESTIONS WITH RAG 15\nSchematic of a RAG Prompt\nretrieved passage 1\nretrieved passage 2\n...\nretrieved passage n\nBased on these texts, answer this question: Q: Who wrote\nthe book ``The Origin of Species\"? A:\nOr more formally,\np(x1;:::; xn) =nY\ni=1p(xijR(q) ; prompt ; [Q:] ;q;[A:] ;x<i)\nAs with the span-based extraction reader, successfully applying the retrieval-\naugmented generation algorithm for QA requires a successful retriever, and often\na two-stage retrieval algorithm is used in which the retrieval is reranked. Some\ncomplex questions may require multi-hop architectures, in which a query is used to multi-hop\nretrieve documents, which are then appended to the original query for a second stage\nof retrieval. Details of prompt engineering also have to be worked out, like deciding\nwhether to demarcate passages, for example with [SEP] tokens, and so on. Combi-\nnations of private data and public data involving an externally hosted large language\nmodel may lead to privacy concerns that need to be worked out (Arora et al., 2023).\nMuch research in this area also focuses on ways to more tightly integrate the retrieval\nand reader stages.\n14.3.2 Question Answering Datasets\nThere are scores of question answering datasets, used both for instruction tuning and\nfor evaluation of the question answering abilities of language models.\nWe can distinguish the datasets along many dimensions, summarized nicely in\nRogers et al. (2023). One is the original purpose of the questions in the data, whether\nthey were natural information-seeking questions, or whether they were questions\ndesigned for probing : evaluating or testing systems or humans.\nOn the natural side there are datasets like Natural Questions (KwiatkowskiNatural\nQuestions\net al., 2019), a set of anonymized English queries to the Google search engine and\ntheir answers. The answers are created by annotators based on Wikipedia infor-\nmation, and include a paragraph-length long answer and a short span answer. For\nexample the question \u201cWhen are hops added to the brewing process?\u201d has the short\nanswer the boiling process and a long answer which is an entire paragraph from the\nWikipedia page on Brewing .\nA similar natural question set is the MS MARCO (Microsoft Machine Reading MS MARCO\nComprehension) collection of datasets, including 1 million real anonymized English\nquestions from Microsoft Bing query logs together with a human generated answer\nand 9 million passages (Bajaj et al., 2016), that can be used both to test retrieval\nranking and question answering.",
    "metadata": {
      "source": "14",
      "chunk_id": 21,
      "token_count": 580,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\n16 CHAPTER 14 \u2022 Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG\nAlthough many datasets focus on English, natural information-seeking ques-\ntion datasets exist in other languages. The DuReader dataset is a Chinese QA re-\nsource based on search engine queries and community QA (He et al., 2018). TyDi\nQAdataset contains 204K question-answer pairs from 11 typologically diverse lan- TyDi QA\nguages, including Arabic, Bengali, Kiswahili, Russian, and Thai (Clark et al., 2020).\nIn the T YDIQA task, a system is given a question and the passages from a Wiki-\npedia article and must (a) select the passage containing the answer (or N ULL if no\npassage contains the answer), and (b) mark the minimal answer span (or N ULL).\nOn the probing side are datasets like MMLU (Massive Multitask Language Un- MMLU\nderstanding), a commonly-used dataset of 15908 knowledge and reasoning ques-\ntions in 57 areas including medicine, mathematics, computer science, law, and oth-\ners. MMLU questions are sourced from various exams for humans, such as the US\nGraduate Record Exam, Medical Licensing Examination, and Advanced Placement\nexams. So the questions don\u2019t represent people\u2019s information needs, but rather are\ndesigned to test human knowledge for academic or licensing purposes. Fig. 14.10\nshows some examples, with the correct answers in bold.\nSome of the question datasets described above augment each question with pas-\nsage(s) from which the answer can be extracted. These datasets were mainly created\nfor an earlier QA task called reading comprehension in which a model is givenreading\ncomprehension\na question and a document and is required to extract the answer from the given\ndocument. We sometimes call the task of question answering given one or more\ndocuments (for example via RAG), the open book QA task, while the task of an- open book\nswering directly from the LM with no retrieval component at all is the closed book closed book\nQA task.5Thus datasets like Natural Questions can be treated as open book if the\nsolver uses each question\u2019s attached document, or closed book if the documents are\nnot used, while datasets like MMLU are solely closed book.\nAnother dimension of variation is the format of the answer: multiple-choice\nversus freeform. And of course there are variations in prompting, like whether the\nmodel is just the question (zero-shot) or also given demonstrations of answers to\nsimilar questions (few-shot). MMLU offers both zero-shot and few-shot prompt\noptions.\n14.4 Evaluating Question Answering\nThree techniques are commonly employed to evaluate question-answering systems,\nwith the choice depending on the type of question and QA situation. For multiple\nchoice questions like in MMLU, we report exact match:\nExact match : The % of predicted answers that match the gold answer\nexactly.\nFor questions with free text answers, like Natural Questions, we commonly evalu-\nated with token F1score to roughly measure the partial string overlap between the\nanswer and the reference answer:\nF1score : The average token overlap between predicted and gold an-\nswers. Treat the prediction and gold as a bag of tokens, and compute F 1\nfor each question, then return the average F 1over all questions.\n5This repurposes the word for types of exams in which students are allowed to \u2018open their books\u2019 or\nnot.",
    "metadata": {
      "source": "14",
      "chunk_id": 22,
      "token_count": 749,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\n14.4 \u2022 E VALUATING QUESTION ANSWERING 17\nMMLU examples\nCollege Computer Science\nAny set of Boolean operators that is suf\ufb01cient to represent all Boolean ex-\npressions is said to be complete. Which of the following is NOT complete?\n(A) AND, NOT\n(B) NOT, OR\n(C) AND, OR\n(D) NAND\nCollege Physics\nThe primary source of the Sun\u2019s energy is a series of thermonuclear\nreactions in which the energy produced is c2times the mass difference\nbetween\n(A) two hydrogen atoms and one helium atom\n(B)four hydrogen atoms and one helium atom\n(C) six hydrogen atoms and two helium atoms\n(D) three helium atoms and one carbon atom\nInternational Law\nWhich of the following is a treaty-based human rights mechanism?\n(A)The UN Human Rights Committee\n(B) The UN Human Rights Council\n(C) The UN Universal Periodic Review\n(D) The UN special mandates\nPrehistory\nUnlike most other early civilizations, Minoan culture shows little evidence\nof\n(A) trade.\n(B) warfare.\n(C) the development of a common religion.\n(D)conspicuous consumption by elites.\nFigure 14.10 Example problems from MMLU\nFinally, in some situations QA systems give multiple ranked answers. In such cases\nwe evaluated using mean reciprocal rank , orMRR (V oorhees, 1999). MRR ismean\nreciprocal rank\nMRR designed for systems that return a short ranked list of answers or passages for each\ntest set question, which we can compare against the (human-labeled) correct answer.\nFirst, each test set question is scored with the reciprocal of the rank of the \ufb01rst\ncorrect answer. For example if the system returned \ufb01ve answers to a question but\nthe \ufb01rst three are wrong (so the highest-ranked correct answer is ranked fourth), the\nreciprocal rank for that question is1\n4. The score for questions that return no correct\nanswer is 0. The MRR of a system is the average of the scores for each question in\nthe test set. In some versions of MRR, questions with a score of zero are ignored\nin this calculation. More formally, for a system returning ranked answers to each\nquestion in a test set Q, (or in the alternate version, let Qbe the subset of test set",
    "metadata": {
      "source": "14",
      "chunk_id": 23,
      "token_count": 505,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\n18 CHAPTER 14 \u2022 Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG\nquestions that have non-zero scores). MRR is then de\ufb01ned as\nMRR =1\njQjjQjX\ni=11\nrank i(14.20)\n14.5 Summary\nThis chapter introduced the tasks of question answering andinformation retrieval .\n\u2022Question answering (QA) is the task of answering a user\u2019s questions.\n\u2022 We focus in this chapter on the task of retrieval-based question answering,\nin which the user\u2019s questions are intended to be answered by the material in\nsome set of documents (which might be the web).\n\u2022Information Retrieval (IR) is the task of returning documents to a user based\non their information need as expressed in a query . In ranked retrieval, the\ndocuments are returned in ranked order.\n\u2022 The match between a query and a document can be done by \ufb01rst representing\neach of them with a sparse vector that represents the frequencies of words,\nweighted by tf-idf orBM25 . Then the similarity can be measured by cosine.\n\u2022 Documents or queries can instead be represented by dense vectors, by encod-\ning the question and document with an encoder-only model like BERT, and in\nthat case computing similarity in embedding space.\n\u2022 The inverted index is a storage mechanism that makes it very ef\ufb01cient to \ufb01nd\ndocuments that have a particular word.\n\u2022 Ranked retrieval is generally evaluated by mean average precision orinter-\npolated precision .\n\u2022 Question answering systems generally use the retriever /reader architecture.\nIn the retriever stage, an IR system is given a query and returns a set of\ndocuments.\n\u2022 The reader stage is implemented by retrieval-augmented generation , in\nwhich a large language model is prompted with the query and a set of doc-\numents and then conditionally generates a novel answer.\n\u2022 QA can be evaluated by exact match with a known answer if only a single\nanswer is given, with token F 1score for free text answers, or with mean re-\nciprocal rank if a ranked set of answers is given.\nBibliographical and Historical Notes\nQuestion answering was one of the earliest NLP tasks, and early versions of the text-\nbased and knowledge-based paradigms were developed by the very early 1960s. The\ntext-based algorithms generally relied on simple parsing of the question and of the\nsentences in the document, and then looking for matches. This approach was used\nvery early on (Phillips, 1960) but perhaps the most complete early system, and one\nthat strikingly pre\ufb01gures modern relation-based systems, was the Protosynthex sys-\ntem of Simmons et al. (1964). Given a question, Protosynthex \ufb01rst formed a query",
    "metadata": {
      "source": "14",
      "chunk_id": 24,
      "token_count": 588,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\nBIBLIOGRAPHICAL AND HISTORICAL NOTES 19\nfrom the content words in the question, and then retrieved candidate answer sen-\ntences in the document, ranked by their frequency-weighted term overlap with the\nquestion. The query and each retrieved sentence were then parsed with dependency\nparsers, and the sentence whose structure best matches the question structure se-\nlected. Thus the question What do worms eat? would match worms eat grass : both\nhave the subject worms as a dependent of eat, in the version of dependency grammar\nused at the time, while birds eat worms hasbirds as the subject:\nWhat doworms eat Worms eat grass Birds eat worms\nThe alternative knowledge-based paradigm was implemented in the BASEBALL\nsystem (Green et al., 1961). This system answered questions about baseball games\nlike \u201cWhere did the Red Sox play on July 7\u201d by querying a structured database of\ngame information. The database was stored as a kind of attribute-value matrix with\nvalues for attributes of each game:\nMonth = July\nPlace = Boston\nDay = 7\nGame Serial No. = 96\n(Team = Red Sox, Score = 5)\n(Team = Yankees, Score = 3)\nEach question was constituency-parsed using the algorithm of Zellig Harris\u2019s\nTDAP project at the University of Pennsylvania, essentially a cascade of \ufb01nite-state\ntransducers (see the historical discussion in Joshi and Hopely 1999 and Karttunen\n1999). Then in a content analysis phase each word or phrase was associated with a\nprogram that computed parts of its meaning. Thus the phrase \u2018Where\u2019 had code to\nassign the semantics Place = ? , with the result that the question \u201cWhere did the\nRed Sox play on July 7\u201d was assigned the meaning\nPlace = ?\nTeam = Red Sox\nMonth = July\nDay = 7\nThe question is then matched against the database to return the answer. Simmons\n(1965) summarizes other early QA systems.\nAnother important progenitor of the knowledge-based paradigm for question-\nanswering is work that used predicate calculus as the meaning representation lan-\nguage. The LUNAR system (Woods et al. 1972, Woods 1978) was designed to be LUNAR\na natural language interface to a database of chemical facts about lunar geology. It\ncould answer questions like Do any samples have greater than 13 percent aluminum\nby parsing them into a logical form\n(TEST (FOR SOME X16 / (SEQ SAMPLES) : T ; (CONTAIN\u2019 X16\n(NPR* X17 / (QUOTE AL203)) (GREATERTHAN 13 PCT))))\nBy a couple decades later, drawing on new machine learning approaches in NLP,\nZelle and Mooney (1996) proposed to treat knowledge-based QA as a semantic pars-\ning task, by creating the Prolog-based GEOQUERY dataset of questions about US\ngeography. This model was extended by Zettlemoyer and Collins (2005) and 2007.",
    "metadata": {
      "source": "14",
      "chunk_id": 25,
      "token_count": 640,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\n20 CHAPTER 14 \u2022 Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG\nBy a decade later, neural models were applied to semantic parsing (Dong and Lap-\nata 2016, Jia and Liang 2016), and then to knowledge-based question answering by\nmapping text to SQL (Iyer et al., 2017).\nMeanwhile, the information-retrieval paradigm for question answering was in-\n\ufb02uenced by the rise of the web in the 1990s. The U.S. government-sponsored TREC\n(Text REtrieval Conference) evaluations, run annually since 1992, provide a testbed\nfor evaluating information-retrieval tasks and techniques (V oorhees and Harman,\n2005). TREC added an in\ufb02uential QA track in 1999, which led to a wide variety of\nfactoid and non-factoid systems competing in annual evaluations.\nAt that same time, Hirschman et al. (1999) introduced the idea of using chil-\ndren\u2019s reading comprehension tests to evaluate machine text comprehension algo-\nrithms. They acquired a corpus of 120 passages with 5 questions each designed for\n3rd-6th grade children, built an answer extraction system, and measured how well\nthe answers given by their system corresponded to the answer key from the test\u2019s\npublisher. Their algorithm focused on word overlap as a feature; later algorithms\nadded named entity features and more complex similarity between the question and\nthe answer span (Riloff and Thelen 2000, Ng et al. 2000).\nThe DeepQA component of the Watson Jeopardy! system was a large and so-\nphisticated feature-based system developed just before neural systems became com-\nmon. It is described in a series of papers in volume 56 of the IBM Journal of Re-\nsearch and Development, e.g., Ferrucci (2012).\nEarly neural reading comprehension systems drew on the insight common to\nearly systems that answer \ufb01nding should focus on question-passage similarity. Many\nof the architectural outlines of these neural systems were laid out in Hermann et al.\n(2015), Chen et al. (2017), and Seo et al. (2017). These systems focused on datasets\nlike Rajpurkar et al. (2016) and Rajpurkar et al. (2018) and their successors, usually\nusing separate IR algorithms as input to neural reading comprehension systems. The\nparadigm of using dense retrieval with a span-based reader, often with a single end-\nto-end architecture, is exempli\ufb01ed by systems like Lee et al. (2019) or Karpukhin\net al. (2020). An important research area with dense retrieval for open-domain QA\nis training data: using self-supervised methods to avoid having to label positive and\nnegative passages (Sachan et al., 2023).\nEarly work on large language models showed that they stored suf\ufb01cient knowl-\nedge in the pretraining process to answer questions (Petroni et al., 2019; Raffel et al.,\n2020; Radford et al., 2019; Roberts et al., 2020), at \ufb01rst not competitively with\nspecial-purpose question answerers, but then surpassing them. Retrieval-augmented\ngeneration algorithms were \ufb01rst introduced as a way to improve language modeling\n(Khandelwal et al., 2019), but were quickly applied to question answering (Izacard\net al., 2022; Ram et al., 2023; Shi et al., 2023).\nExercises",
    "metadata": {
      "source": "14",
      "chunk_id": 26,
      "token_count": 776,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21",
    "metadata": {
      "source": "14",
      "chunk_id": 27,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Exercises 21\nArora, S., P. Lewis, A. Fan, J. Kahn, and C. R \u00b4e. 2023. Rea-\nsoning over public and private data in retrieval-based sys-\ntems. TACL , 11:902\u2013921.\nBajaj, P., D. Campos, N. Craswell, L. Deng, J. G. ando\nXiaodong Liu, R. Majumder, A. McNamara, B. Mitra,\nT. Nguye, M. Rosenberg, X. Song, A. Stoica, S. Tiwary,\nand T. Wang. 2016. MS MARCO: A human generated\nMAchine Reading COmprehension dataset. NeurIPS .\nChen, D., A. Fisch, J. Weston, and A. Bordes. 2017. Reading\nWikipedia to answer open-domain questions. ACL.\nClark, J. H., E. Choi, M. Collins, D. Garrette,\nT. Kwiatkowski, V . Nikolaev, and J. Palomaki. 2020.\nTyDi QA: A benchmark for information-seeking ques-\ntion answering in typologically diverse languages. TACL ,\n8:454\u2013470.\nDahl, M., V . Magesh, M. Suzgun, and D. E. Ho. 2024. Large\nlegal \ufb01ctions: Pro\ufb01ling legal hallucinations in large lan-\nguage models. Journal of Legal Analysis , 16:64\u201393.\nDeerwester, S. C., S. T. Dumais, T. K. Landauer, G. W. Fur-\nnas, and R. A. Harshman. 1990. Indexing by latent se-\nmantics analysis. JASIS , 41(6):391\u2013407.\nDong, L. and M. Lapata. 2016. Language to logical form\nwith neural attention. ACL.\nFerrucci, D. A. 2012. Introduction to \u201cThis is Watson\u201d. IBM\nJournal of Research and Development , 56(3/4):1:1\u20131:15.\nFurnas, G. W., T. K. Landauer, L. M. Gomez, and S. T.\nDumais. 1987. The vocabulary problem in human-\nsystem communication. Communications of the ACM ,\n30(11):964\u2013971.\nGreen, B. F., A. K. Wolf, C. Chomsky, and K. Laughery.\n1961. Baseball: An automatic question answerer. Pro-\nceedings of the Western Joint Computer Conference 19 .\nHe, W., K. Liu, J. Liu, Y . Lyu, S. Zhao, X. Xiao, Y . Liu,\nY . Wang, H. Wu, Q. She, X. Liu, T. Wu, and H. Wang.\n2018. DuReader: a Chinese machine reading compre-\nhension dataset from real-world applications. Workshop\non Machine Reading for Question Answering .\nHermann, K. M., T. Kocisky, E. Grefenstette, L. Espeholt,\nW. Kay, M. Suleyman, and P. Blunsom. 2015. Teaching\nmachines to read and comprehend. NeurIPS .\nHirschman, L., M. Light, E. Breck, and J. D. Burger. 1999.\nDeep Read: A reading comprehension system. ACL.",
    "metadata": {
      "source": "14",
      "chunk_id": 28,
      "token_count": 754,
      "chapter_title": ""
    }
  },
  {
    "content": "Dumais. 1987. The vocabulary problem in human-\nsystem communication. Communications of the ACM ,\n30(11):964\u2013971.\nGreen, B. F., A. K. Wolf, C. Chomsky, and K. Laughery.\n1961. Baseball: An automatic question answerer. Pro-\nceedings of the Western Joint Computer Conference 19 .\nHe, W., K. Liu, J. Liu, Y . Lyu, S. Zhao, X. Xiao, Y . Liu,\nY . Wang, H. Wu, Q. She, X. Liu, T. Wu, and H. Wang.\n2018. DuReader: a Chinese machine reading compre-\nhension dataset from real-world applications. Workshop\non Machine Reading for Question Answering .\nHermann, K. M., T. Kocisky, E. Grefenstette, L. Espeholt,\nW. Kay, M. Suleyman, and P. Blunsom. 2015. Teaching\nmachines to read and comprehend. NeurIPS .\nHirschman, L., M. Light, E. Breck, and J. D. Burger. 1999.\nDeep Read: A reading comprehension system. ACL.\nIyer, S., I. Konstas, A. Cheung, J. Krishnamurthy, and\nL. Zettlemoyer. 2017. Learning a neural semantic parser\nfrom user feedback. ACL.\nIzacard, G., P. Lewis, M. Lomeli, L. Hosseini, F. Petroni,\nT. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, and\nE. Grave. 2022. Few-shot learning with retrieval aug-\nmented language models. ArXiv preprint.\nJia, R. and P. Liang. 2016. Data recombination for neural\nsemantic parsing. ACL.\nJohnson, J., M. Douze, and H. J \u00b4egou. 2017. Billion-\nscale similarity search with GPUs. ArXiv preprint\narXiv:1702.08734.\nJoshi, A. K. and P. Hopely. 1999. A parser from antiquity.\nIn A. Kornai, ed., Extended Finite State Models of Lan-\nguage , 6\u201315. Cambridge University Press.\nJurafsky, D. 2014. The Language of Food . W. W. Norton,\nNew York.Kamphuis, C., A. P. de Vries, L. Boytsov, and J. Lin. 2020.\nWhich BM25 do you mean? a large-scale reproducibil-\nity study of scoring variants. European Conference on\nInformation Retrieval .\nKarpukhin, V ., B. O \u02d8guz, S. Min, P. Lewis, L. Wu, S. Edunov,\nD. Chen, and W.-t. Yih. 2020. Dense passage retrieval for\nopen-domain question answering. EMNLP .\nKarttunen, L. 1999. Comments on Joshi. In A. Kornai, ed.,\nExtended Finite State Models of Language , 16\u201318. Cam-\nbridge University Press.\nKhandelwal, U., O. Levy, D. Jurafsky, L. Zettlemoyer, and\nM. Lewis. 2019. Generalization through memorization:\nNearest neighbor language models. ICLR .",
    "metadata": {
      "source": "14",
      "chunk_id": 29,
      "token_count": 751,
      "chapter_title": ""
    }
  },
  {
    "content": "In A. Kornai, ed., Extended Finite State Models of Lan-\nguage , 6\u201315. Cambridge University Press.\nJurafsky, D. 2014. The Language of Food . W. W. Norton,\nNew York.Kamphuis, C., A. P. de Vries, L. Boytsov, and J. Lin. 2020.\nWhich BM25 do you mean? a large-scale reproducibil-\nity study of scoring variants. European Conference on\nInformation Retrieval .\nKarpukhin, V ., B. O \u02d8guz, S. Min, P. Lewis, L. Wu, S. Edunov,\nD. Chen, and W.-t. Yih. 2020. Dense passage retrieval for\nopen-domain question answering. EMNLP .\nKarttunen, L. 1999. Comments on Joshi. In A. Kornai, ed.,\nExtended Finite State Models of Language , 16\u201318. Cam-\nbridge University Press.\nKhandelwal, U., O. Levy, D. Jurafsky, L. Zettlemoyer, and\nM. Lewis. 2019. Generalization through memorization:\nNearest neighbor language models. ICLR .\nKhattab, O., C. Potts, and M. Zaharia. 2021. Relevance-\nguided supervision for OpenQA with ColBERT. TACL ,\n9:929\u2013944.\nKhattab, O. and M. Zaharia. 2020. ColBERT: Ef\ufb01cient and\neffective passage search via contextualized late interac-\ntion over BERT. SIGIR .\nKwiatkowski, T., J. Palomaki, O. Red\ufb01eld, M. Collins,\nA. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. De-\nvlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M.-W.\nChang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov.\n2019. Natural questions: A benchmark for question an-\nswering research. TACL , 7:452\u2013466.\nLee, K., M.-W. Chang, and K. Toutanova. 2019. Latent re-\ntrieval for weakly supervised open domain question an-\nswering. ACL.\nManning, C. D., P. Raghavan, and H. Sch \u00a8utze. 2008. Intro-\nduction to Information Retrieval . Cambridge.\nNg, H. T., L. H. Teo, and J. L. P. Kwan. 2000. A ma-\nchine learning approach to answering questions for read-\ning comprehension tests. EMNLP .\nPetroni, F., T. Rockt \u00a8aschel, S. Riedel, P. Lewis, A. Bakhtin,\nY . Wu, and A. Miller. 2019. Language models as knowl-\nedge bases? EMNLP .\nPhillips, A. V . 1960. A question-answering routine. Techni-\ncal Report 16, MIT AI Lab.\nRadford, A., J. Wu, R. Child, D. Luan, D. Amodei, and\nI. Sutskever. 2019. Language models are unsupervised\nmultitask learners. OpenAI tech report.",
    "metadata": {
      "source": "14",
      "chunk_id": 30,
      "token_count": 745,
      "chapter_title": ""
    }
  },
  {
    "content": "swering research. TACL , 7:452\u2013466.\nLee, K., M.-W. Chang, and K. Toutanova. 2019. Latent re-\ntrieval for weakly supervised open domain question an-\nswering. ACL.\nManning, C. D., P. Raghavan, and H. Sch \u00a8utze. 2008. Intro-\nduction to Information Retrieval . Cambridge.\nNg, H. T., L. H. Teo, and J. L. P. Kwan. 2000. A ma-\nchine learning approach to answering questions for read-\ning comprehension tests. EMNLP .\nPetroni, F., T. Rockt \u00a8aschel, S. Riedel, P. Lewis, A. Bakhtin,\nY . Wu, and A. Miller. 2019. Language models as knowl-\nedge bases? EMNLP .\nPhillips, A. V . 1960. A question-answering routine. Techni-\ncal Report 16, MIT AI Lab.\nRadford, A., J. Wu, R. Child, D. Luan, D. Amodei, and\nI. Sutskever. 2019. Language models are unsupervised\nmultitask learners. OpenAI tech report.\nRaffel, C., N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y . Zhou, W. Li, and P. J. Liu. 2020. Exploring\nthe limits of transfer learning with a uni\ufb01ed text-to-text\ntransformer. JMLR , 21(140):1\u201367.\nRajpurkar, P., R. Jia, and P. Liang. 2018. Know what you\ndon\u2019t know: Unanswerable questions for SQuAD. ACL.\nRajpurkar, P., J. Zhang, K. Lopyrev, and P. Liang. 2016.\nSQuAD: 100,000+ questions for machine comprehension\nof text. EMNLP .\nRam, O., Y . Levine, I. Dalmedigos, D. Muhlgay, A. Shashua,\nK. Leyton-Brown, and Y . Shoham. 2023. In-context\nretrieval-augmented language models. ArXiv preprint.\nRiloff, E. and M. Thelen. 2000. A rule-based ques-\ntion answering system for reading comprehension tests.\nANLP/NAACL workshop on reading comprehension tests .\nRoberts, A., C. Raffel, and N. Shazeer. 2020. How much\nknowledge can you pack into the parameters of a lan-\nguage model? EMNLP .",
    "metadata": {
      "source": "14",
      "chunk_id": 31,
      "token_count": 599,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22",
    "metadata": {
      "source": "14",
      "chunk_id": 32,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "22 Chapter 14 \u2022 Question Answering, Information Retrieval, and RAG\nRobertson, S., S. Walker, S. Jones, M. M. Hancock-\nBeaulieu, and M. Gatford. 1995. Okapi at TREC-3.\nOverview of the Third Text REtrieval Conference (TREC-\n3).\nRogers, A., M. Gardner, and I. Augenstein. 2023. QA dataset\nexplosion: A taxonomy of NLP resources for question\nanswering and reading comprehension. ACM Computing\nSurveys , 55(10):1\u201345.\nSachan, D. S., M. Lewis, D. Yogatama, L. Zettlemoyer,\nJ. Pineau, and M. Zaheer. 2023. Questions are all you\nneed to train a dense passage retriever. TACL , 11:600\u2013\n616.\nSalton, G. 1971. The SMART Retrieval System: Experiments\nin Automatic Document Processing . Prentice Hall.\nSeo, M., A. Kembhavi, A. Farhadi, and H. Hajishirzi. 2017.\nBidirectional attention \ufb02ow for machine comprehension.\nICLR .\nShi, W., S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis,\nL. Zettlemoyer, and W.-t. Yih. 2023. REPLUG: Retrieval-\naugmented black-box language models. ArXiv preprint.\nSimmons, R. F. 1965. Answering English questions by com-\nputer: A survey. CACM , 8(1):53\u201370.\nSimmons, R. F., S. Klein, and K. McConlogue. 1964. In-\ndexing and dependency logic for answering English ques-\ntions. American Documentation , 15(3):196\u2013204.\nSparck Jones, K. 1972. A statistical interpretation of term\nspeci\ufb01city and its application in retrieval. Journal of Doc-\numentation , 28(1):11\u201321.\nV oorhees, E. M. 1999. TREC-8 question answering track\nreport. Proceedings of the 8th Text Retrieval Conference .\nV oorhees, E. M. and D. K. Harman. 2005. TREC: Experi-\nment and Evaluation in Information Retrieval . MIT Press.\nWoods, W. A. 1978. Semantics and quanti\ufb01cation in natural\nlanguage question answering. In M. Yovits, ed., Advances\nin Computers , 2\u201364. Academic.\nWoods, W. A., R. M. Kaplan, and B. L. Nash-Webber. 1972.\nThe lunar sciences natural language information system:\nFinal report. Technical Report 2378, BBN.\nZelle, J. M. and R. J. Mooney. 1996. Learning to parse\ndatabase queries using inductive logic programming.\nAAAI .\nZettlemoyer, L. and M. Collins. 2005. Learning to map\nsentences to logical form: Structured classi\ufb01cation with\nprobabilistic categorial grammars. Uncertainty in Arti\ufb01-\ncial Intelligence, UAI\u201905 .\nZettlemoyer, L. and M. Collins. 2007. Online learning\nof relaxed CCG grammars for parsing to logical form.\nEMNLP/CoNLL .",
    "metadata": {
      "source": "14",
      "chunk_id": 33,
      "token_count": 763,
      "chapter_title": ""
    }
  },
  {
    "content": "report. Proceedings of the 8th Text Retrieval Conference .\nV oorhees, E. M. and D. K. Harman. 2005. TREC: Experi-\nment and Evaluation in Information Retrieval . MIT Press.\nWoods, W. A. 1978. Semantics and quanti\ufb01cation in natural\nlanguage question answering. In M. Yovits, ed., Advances\nin Computers , 2\u201364. Academic.\nWoods, W. A., R. M. Kaplan, and B. L. Nash-Webber. 1972.\nThe lunar sciences natural language information system:\nFinal report. Technical Report 2378, BBN.\nZelle, J. M. and R. J. Mooney. 1996. Learning to parse\ndatabase queries using inductive logic programming.\nAAAI .\nZettlemoyer, L. and M. Collins. 2005. Learning to map\nsentences to logical form: Structured classi\ufb01cation with\nprobabilistic categorial grammars. Uncertainty in Arti\ufb01-\ncial Intelligence, UAI\u201905 .\nZettlemoyer, L. and M. Collins. 2007. Online learning\nof relaxed CCG grammars for parsing to logical form.\nEMNLP/CoNLL .\nZhou, K., J. Hwang, X. Ren, and M. Sap. 2024. Relying\non the unreliable: The impact of language models\u2019 reluc-\ntance to express uncertainty. ACL.",
    "metadata": {
      "source": "14",
      "chunk_id": 34,
      "token_count": 326,
      "chapter_title": ""
    }
  }
]