[
  {
    "content": "# E\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\nECombinatory Categorial\nGrammar\nIn this chapter, we provide an overview of categorial grammar (Ajdukiewicz 1935,categorial\ngrammar\nBar-Hillel 1953), an early lexicalized grammar model, as well as an important mod-\nern extension, combinatory categorial grammar , or CCG (Steedman 1996, Steed-combinatory\ncategorial\ngrammarman 1989, Steedman 2000). CCG is a heavily lexicalized approach motivated by\nboth syntactic and semantic considerations. It is an exemplar of a set of computa-\ntionally relevant approaches to grammar that emphasize putting grammatical infor-\nmation in a rich lexicon, including Lexical-Functional Grammar (LFG) (Bresnan,\n1982), Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994),\nand Tree-Adjoining Grammar (TAG) (Joshi, 1985).\nThe categorial approach consists of three major elements: a set of categories,\na lexicon that associates words with categories, and a set of rules that govern how\ncategories combine in context.\nE.1 CCG Categories\nCategories are either atomic elements or single-argument functions that return a cat-\negory as a value when provided with a desired category as argument. More formally,\nwe can de\ufb01ne C, a set of categories for a grammar as follows:\n\u2022A\u0012C, where Ais a given set of atomic elements\n\u2022 (X=Y), (XnY)2C, ifX,Y2C\nThe slash notation shown here is used to de\ufb01ne the functions in the grammar.\nIt speci\ufb01es the type of the expected argument, the direction it is expected be found,\nand the type of the result. Thus, ( X=Y) is a function that seeks a constituent of type\nYto its right and returns a value of X; (XnY) is the same except it seeks its argument\nto the left.\nThe set of atomic categories is typically very small and includes familiar el-\nements such as sentences and noun phrases. Functional categories include verb\nphrases and complex noun phrases among others.\nE.2 The Lexicon\nThe lexicon in a categorial approach consists of assignments of categories to words.\nThese assignments can either be to atomic or functional categories, and due to lexical\nambiguity words can be assigned to multiple categories. Consider the following",
    "metadata": {
      "source": "E",
      "chunk_id": 0,
      "token_count": 563,
      "chapter_title": "E"
    }
  },
  {
    "content": "## Page 2\n\n2APPENDIX E \u2022 C OMBINATORY CATEGORIAL GRAMMAR\nsample lexical entries.\n\ufb02ight : N\nMiami : NP\ncancel :(SnNP)=NP\nNouns and proper nouns like \ufb02ight andMiami are assigned to atomic categories,\nre\ufb02ecting their typical role as arguments to functions. On the other hand, a transitive\nverb like cancel is assigned the category ( SnNP)=NP: a function that seeks an NPon\nits right and returns as its value a function with the type ( SnNP). This function can,\nin turn, combine with an NPon the left, yielding an Sas the result. This captures\nsubcategorization information with a computationally useful, internal structure.\nDitransitive verbs like give, which expect two arguments after the verb, would\nhave the category (( SnNP)=NP)=NP: a function that combines with an NPon its\nright to yield yet another function corresponding to the transitive verb ( SnNP)=NP\ncategory such as the one given above for cancel .\nE.3 Rules\nThe rules of a categorial grammar specify how functions and their arguments com-\nbine. The following two rule templates constitute the basis for all categorial gram-\nmars.\nX=Y Y)X (E.1)\nY XnY)X (E.2)\nThe \ufb01rst rule applies a function to its argument on the right, while the second\nlooks to the left for its argument. We\u2019ll refer to the \ufb01rst as forward function appli-\ncation , and the second as backward function application . The result of applying\neither of these rules is the category speci\ufb01ed as the value of the function being ap-\nplied.\nGiven these rules and a simple lexicon, let\u2019s consider an analysis of the sentence\nUnited serves Miami . Assume that serves is a transitive verb with the category\n(SnNP)=NPand that United andMiami are both simple NPs. Using both forward\nand backward function application, the derivation would proceed as follows:\nUnited serves Miami\nNP (SnNP)=NP NP\n>SnNP\n<S\nCategorial grammar derivations are illustrated growing down from the words,\nrule applications are illustrated with a horizontal line that spans the elements in-\nvolved, with the type of the operation indicated at the right end of the line. In this\nexample, there are two function applications: one forward function application indi-\ncated by the >that applies the verb serves to the NPon its right, and one backward\nfunction application indicated by the <that applies the result of the \ufb01rst to the NP\nUnited on its left.\nEnglish permits the coordination of two constituents of the same type, resulting\nin a new constituent of the same type. The following rule provides the mechanism",
    "metadata": {
      "source": "E",
      "chunk_id": 1,
      "token_count": 589,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\nE.3 \u2022 R ULES 3\nto handle such examples.\nX CONJ X)X (E.3)\nThis rule states that when two constituents of the same category are separated by a\nconstituent of type CONJ they can be combined into a single larger constituent of\nthe same type. The following derivation illustrates the use of this rule.\nWe flew to Geneva and drove to Chamonix\nNP(SnNP)=PP PP =NP NP CONJ (SnNP)=PP PP =NP NP\n> >PP PP\n> >SnNP S nNP\n<F>SnNP\n<S\nHere the two SnNPconstituents are combined via the conjunction operator <F>\nto form a larger constituent of the same type, which can then be combined with the\nsubject NPvia backward function application.\nThese examples illustrate the lexical nature of the categorial grammar approach.\nThe grammatical facts about a language are largely encoded in the lexicon, while the\nrules of the grammar are boiled down to a set of three rules. Unfortunately, the basic\ncategorial approach does not give us any more expressive power than we had with\ntraditional CFG rules; it just moves information from the grammar to the lexicon. To\nmove beyond these limitations CCG includes operations that operate over functions.\nThe \ufb01rst pair of operators permit us to compose adjacent functions.\nX=Y Y=Z)X=Z (E.4)\nYnZ XnY)XnZ (E.5)\nThe \ufb01rst rule, called forward composition , can be applied to adjacent con-forward\ncomposition\nstituents where the \ufb01rst is a function seeking an argument of type Yto its right, and\nthe second is a function that provides Yas a result. This rule allows us to compose\nthese two functions into a single one with the type of the \ufb01rst constituent and the\nargument of the second. Although the notation is a little awkward, the second rule,\nbackward composition is the same, except that we\u2019re looking to the left instead ofbackward\ncomposition\nto the right for the relevant arguments. Both kinds of composition are signalled by a\nBin CCG diagrams, accompanied by a <or>to indicate the direction.\nThe next operator is type raising . Type raising elevates simple categories to the type raising\nstatus of functions. More speci\ufb01cally, type raising takes a category and converts it\nto a function that seeks as an argument a function that takes the original category\nas its argument. The following schema show two versions of type raising: one for\narguments to the right, and one for the left.\nX)T=(TnX) (E.6)\nX)Tn(T=X) (E.7)\nThe category Tin these rules can correspond to any of the atomic or functional\ncategories already present in the grammar.\nA particularly useful example of type raising transforms a simple NPargument\nin subject position to a function that can compose with a following VP. To see how",
    "metadata": {
      "source": "E",
      "chunk_id": 2,
      "token_count": 626,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4APPENDIX E \u2022 C OMBINATORY CATEGORIAL GRAMMAR\nthis works, let\u2019s revisit our earlier example of United serves Miami . Instead of clas-\nsifying United as an NPwhich can serve as an argument to the function attached to\nserve , we can use type raising to reinvent it as a function in its own right as follows.\nNP)S=(SnNP)\nCombining this type-raised constituent with the forward composition rule (E.4) per-\nmits the following alternative to our previous derivation.\nUnited serves Miami\nNP (SnNP)=NP NP\n>TS=(SnNP)\n>BS=NP\n>S\nBy type raising United toS=(SnNP), we can compose it with the transitive verb\nserves to yield the (S=NP)function needed to complete the derivation.\nThere are several interesting things to note about this derivation. First, it pro-\nvides a left-to-right, word-by-word derivation that more closely mirrors the way\nhumans process language. This makes CCG a particularly apt framework for psy-\ncholinguistic studies. Second, this derivation involves the use of an intermediate\nunit of analysis, United serves , that does not correspond to a traditional constituent\nin English. This ability to make use of such non-constituent elements provides CCG\nwith the ability to handle the coordination of phrases that are not proper constituents,\nas in the following example.\n(E.8) We \ufb02ew IcelandAir to Geneva and SwissAir to London.\nHere, the segments that are being coordinated are IcelandAir to Geneva and\nSwissAir to London , phrases that would not normally be considered constituents, as\ncan be seen in the following standard derivation for the verb phrase \ufb02ew IcelandAir\nto Geneva .\n\ufb02ew IcelandAir to Geneva\n(VP=PP)=NP NP PP =NP NP\n> >VP=PP PP\n>VP\nIn this derivation, there is no single constituent that corresponds to IcelandAir\nto Geneva , and hence no opportunity to make use of the <F>operator. Note that\ncomplex CCG categories can get a little cumbersome, so we\u2019ll use VPas a shorthand\nfor (SnNP) in this and the following derivations.\nThe following alternative derivation provides the required element through the\nuse of both backward type raising (E.7) and backward function composition (E.5).\n\ufb02ew IcelandAir to Geneva\n(V P=PP)=NP NP PP =NP NP\n<T >(V P=PP)n((V P=PP)=NP) PP\n<TV Pn(V P=PP)\n<BV Pn((V P=PP)=NP)\n<V P\nApplying the same analysis to SwissAir to London satis\ufb01es the requirements for\nthe<F>operator, yielding the following derivation for our original example (E.8).",
    "metadata": {
      "source": "E",
      "chunk_id": 3,
      "token_count": 602,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\nE.4 \u2022 CCG BANK 5\n\ufb02ew IcelandAir to Geneva and SwissAir to London\n(V P=PP)=NP NP PP =NP NP CONJ NP PP =NP NP\n<T > <T >(V P=PP)n((V P=PP)=NP) PP (V P=PP)n((V P=PP)=NP) PP\n<T <TV Pn(V P=PP) V Pn(V P=PP)\n< <V Pn((V P=PP)=NP) V Pn((V P=PP)=NP)\n<F>V Pn((V P=PP)=NP)\n<V P\nFinally, let\u2019s examine how these advanced operators can be used to handle long-\ndistance dependencies (also referred to as syntactic movement or extraction). As\nmentioned in Appendix D, long-distance dependencies arise from many English\nconstructions including wh-questions, relative clauses, and topicalization. What\nthese constructions have in common is a constituent that appears somewhere dis-\ntant from its usual, or expected, location. Consider the following relative clause as\nan example.\nthe \ufb02ight that United diverted\nHere, divert is a transitive verb that expects two NParguments, a subject NPto its\nleft and a direct object NPto its right; its category is therefore ( SnNP)=NP. However,\nin this example the direct object the \ufb02ight has been \u201cmoved\u201d to the beginning of the\nclause, while the subject United remains in its normal position. What is needed is a\nway to incorporate the subject argument, while dealing with the fact that the \ufb02ight is\nnot in its expected location.\nThe following derivation accomplishes this, again through the combined use of\ntype raising and function composition.\nthe \ufb02ight that United diverted\nNP=N N (NPnNP)=(S=NP) NP (SnNP)=NP\n> >TNP S =(SnNP)\n>BS=NP\n>NPnNP\n<NP\nAs we saw with our earlier examples, the \ufb01rst step of this derivation is type raising\nUnited to the category S=(SnNP) allowing it to combine with diverted via forward\ncomposition. The result of this composition is S=NPwhich preserves the fact that we\nare still looking for an NPto \ufb01ll the missing direct object. The second critical piece\nis the lexical category assigned to the word that: (NPnNP)=(S=NP). This function\nseeks a verb phrase missing an argument to its right, and transforms it into an NP\nseeking a missing element to its left, precisely where we \ufb01nd the \ufb02ight .\nE.4 CCGbank\nAs with phrase-structure approaches, treebanks play an important role in CCG-\nbased approaches to parsing. CCGbank (Hockenmaier and Steedman, 2007) is the\nlargest and most widely used CCG treebank. It was created by automatically trans-\nlating phrase-structure trees from the Penn Treebank via a rule-based approach. The\nmethod produced successful translations of over 99% of the trees in the Penn Tree-\nbank resulting in 48,934 sentences paired with CCG derivations. It also provides a",
    "metadata": {
      "source": "E",
      "chunk_id": 4,
      "token_count": 681,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6APPENDIX E \u2022 C OMBINATORY CATEGORIAL GRAMMAR\nlexicon of 44,000 words with over 1200 categories. Appendix C will discuss how\nthese resources can be used to train CCG parsers.\nE.5 Ambiguity in CCG\nAs is always the case in parsing, managing ambiguity is the key to successful CCG\nparsing. The dif\ufb01culties with CCG parsing arise from the ambiguity caused by the\nlarge number of complex lexical categories combined with the very general nature of\nthe grammatical rules. To see some of the ways that ambiguity arises in a categorial\nframework, consider the following example.\n(E.9) United diverted the \ufb02ight to Reno.\nOur grasp of the role of the \ufb02ight in this example depends on whether the prepo-\nsitional phrase to Reno is taken as a modi\ufb01er of the \ufb02ight , as a modi\ufb01er of the entire\nverb phrase, or as a potential second argument to the verb divert . In a context-free\ngrammar approach, this ambiguity would manifest itself as a choice among the fol-\nlowing rules in the grammar.\nNominal!Nominal PP\nVP!VP PP\nVP!Verb NP PP\nIn a phrase-structure approach we would simply assign the word toto the cate-\ngory Pallowing it to combine with Reno to form a prepositional phrase. The sub-\nsequent choice of grammar rules would then dictate the ultimate derivation. In the\ncategorial approach, we can associate towith distinct categories to re\ufb02ect the ways\nin which it might interact with other elements in a sentence. The fairly abstract\ncombinatoric rules would then sort out which derivations are possible. Therefore,\nthe source of ambiguity arises not from the grammar but rather from the lexicon.\nLet\u2019s see how this works by considering several possible derivations for this\nexample. To capture the case where the prepositional phrase to Reno modi\ufb01es the\n\ufb02ight , we assign the preposition tothe category (NPnNP)=NP, which gives rise to\nthe following derivation.\nUnited diverted the \ufb02ight to Reno\nNP (SnNP)=NP NP =N N (NPnNP)=NP NP\n> >NP NP nNP\n<NP\n>SnNP\n<S\nHere, the category assigned to toexpects to \ufb01nd two arguments: one to the right as\nwith a traditional preposition, and one to the left that corresponds to the NPto be\nmodi\ufb01ed.\nAlternatively, we could assign toto the category (SnS)=NP, which permits the\nfollowing derivation where to Reno modi\ufb01es the preceding verb phrase.",
    "metadata": {
      "source": "E",
      "chunk_id": 5,
      "token_count": 583,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\nE.6 \u2022 CCG P ARSING 7\nUnited diverted the \ufb02ight to Reno\nNP (SnNP)=NP NP =N N (SnS)=NP NP\n> >NP SnS\n>SnNP\n<BSnNP\n<S\nA third possibility is to view divert as a ditransitive verb by assigning it to the\ncategory ((SnNP)=PP)=NP, while treating to Reno as a simple prepositional phrase.\nUnited diverted the \ufb02ight to Reno\nNP ((SnNP)=PP)=NP NP =N N PP =NP NP\n> >NP PP\n>(SnNP)=PP\n>SnNP\n<S\nWhile CCG parsers are still subject to ambiguity arising from the choice of gram-\nmar rules, including the kind of spurious ambiguity discussed above, it should be\nclear that the choice of lexical categories is the primary problem to be addressed in\nCCG parsing.\nE.6 CCG Parsing\nSince the rules in combinatory grammars are either binary or unary, a bottom-up,\ntabular approach based on the CKY algorithm should be directly applicable to CCG\nparsing. Unfortunately, the large number of lexical categories available for each\nword, combined with the promiscuity of CCG\u2019s combinatoric rules, leads to an ex-\nplosion in the number of (mostly useless) constituents added to the parsing table.\nThe key to managing this explosion of zombie constituents is to accurately assess\nand exploit the most likely lexical categories possible for each word\u2014a process\ncalled supertagging.\nThese following sections describe an approach to CCG parsing that make use of\nsupertags, structuring the parsing process as a heuristic search through the use of the\nA* algorithm.\nE.6.1 Supertagging\nChapter 17 introduced the task of part-of-speech tagging, the process of assigning\nthe correct lexical category to each word in a sentence. Supertagging is the corre- supertagging\nsponding task for highly lexicalized grammar frameworks, where the assigned tags\noften dictate much of the derivation for a sentence (Bangalore and Joshi, 1999).\nCCG supertaggers rely on treebanks such as CCGbank to provide both the over-\nall set of lexical categories as well as the allowable category assignments for each\nword in the lexicon. CCGbank includes over 1000 lexical categories, however, in\npractice, most supertaggers limit their tagsets to those tags that occur at least 10",
    "metadata": {
      "source": "E",
      "chunk_id": 6,
      "token_count": 532,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8APPENDIX E \u2022 C OMBINATORY CATEGORIAL GRAMMAR\ntimes in the training corpus. This results in a total of around 425 lexical categories\navailable for use in the lexicon. Note that even this smaller number is large in con-\ntrast to the 45 POS types used by the Penn Treebank tagset.\nAs with traditional part-of-speech tagging, the standard approach to building a\nCCG supertagger is to use supervised machine learning to build a sequence labeler\nfrom hand-annotated training data. To \ufb01nd the most likely sequence of tags given a\nsentence, it is most common to use a neural sequence model, either RNN or Trans-\nformer.\nIt\u2019s also possible, however, to use the CRF tagging model described in Chap-\nter 17, using similar features; the current word wi, its surrounding words within\nlwords, local POS tags and character suf\ufb01xes, and the supertag from the prior\ntimestep, training by maximizing log-likelihood of the training corpus and decoding\nvia the Viterbi algorithm as described in Chapter 17.\nUnfortunately the large number of possible supertags combined with high per-\nword ambiguity leads the naive CRF algorithm to error rates that are too high for\npractical use in a parser. The single best tag sequence \u02c6Twill typically contain too\nmany incorrect tags for effective parsing to take place. To overcome this, we instead\nreturn a probability distribution over the possible supertags for each word in the\ninput. The following table illustrates an example distribution for a simple sentence,\nin which each column represents the probability of each supertag for a given word\nin the context of the input sentence . The \u201c...\u201d represent all the remaining supertags\npossible for each word.\nUnited serves Denver\nN=N: 0.4 (SnNP)=NP: 0.8 NP: 0.9\nNP: 0.3 N: 0.1 N=N: 0.05\nS=S: 0.1 ... ...\nSnS: .05\n...\nTo get the probability of each possible word/tag pair, we\u2019ll need to sum the\nprobabilities of all the supertag sequences that contain that tag at that location. This\ncan be done with the forward-backward algorithm that is also used to train the CRF,\ndescribed in Appendix A.\nE.6.2 CCG Parsing using the A* Algorithm\nThe A* algorithm is a heuristic search method that employs an agenda to \ufb01nd an\noptimal solution. Search states representing partial solutions are added to an agenda\nbased on a cost function, with the least-cost option being selected for further ex-\nploration at each iteration. When a state representing a complete solution is \ufb01rst\nselected from the agenda, it is guaranteed to be optimal and the search terminates.\nThe A* cost function, f(n), is used to ef\ufb01ciently guide the search to a solution.\nThe f-cost has two components: g(n), the exact cost of the partial solution repre-\nsented by the state n, and h(n)a heuristic approximation of the cost of a solution\nthat makes use of n. When h(n)satis\ufb01es the criteria of not overestimating the actual\ncost, A* will \ufb01nd an optimal solution. Not surprisingly, the closer the heuristic can\nget to the actual cost, the more effective A* is at \ufb01nding a solution without having\nto explore a signi\ufb01cant portion of the solution space.\nWhen applied to parsing, search states correspond to edges representing com-\npleted constituents. Each edge speci\ufb01es a constituent\u2019s start and end positions, its",
    "metadata": {
      "source": "E",
      "chunk_id": 7,
      "token_count": 784,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\nE.6 \u2022 CCG P ARSING 9\ngrammatical category, and its f-cost. Here, the gcomponent represents the current\ncost of an edge and the hcomponent represents an estimate of the cost to complete\na derivation that makes use of that edge. The use of A* for phrase structure parsing\noriginated with Klein and Manning (2003), while the CCG approach presented here\nis based on the work of Lewis and Steedman (2014).\nUsing information from a supertagger, an agenda and a parse table are initial-\nized with states representing all the possible lexical categories for each word in the\ninput, along with their f-costs. The main loop removes the lowest cost edge from\nthe agenda and tests to see if it is a complete derivation. If it re\ufb02ects a complete\nderivation it is selected as the best solution and the loop terminates. Otherwise, new\nstates based on the applicable CCG rules are generated, assigned costs, and entered\ninto the agenda to await further processing. The loop continues until a complete\nderivation is discovered, or the agenda is exhausted, indicating a failed parse. The\nalgorithm is given in Fig. E.1.\nfunction CCG-AS TAR-PARSE (words )returns table orfailure\nsupertags SUPERTAGGER (words )\nfori from 1toLENGTH (words )do\nfor all fAj(words [i];A;score)2supertags g\nedge MAKEEDGE(i\u00001,i,A,score )\ntable INSERT EDGE(table ,edge )\nagenda INSERT EDGE(agenda ,edge )\nloop do\nifEMPTY ?(agenda )return failure\ncurrent POP(agenda )\nifCOMPLETED PARSE ?(current )return table\ntable INSERT EDGE(table ,current )\nfor each ruleinAPPLICABLE RULES (current )do\nsuccessor APPLY (rule,current )\nifsuccessor not2inagenda orchart\nagenda INSERT EDGE(agenda ,successor )\nelse if successor2agenda with higher cost\nagenda REPLACE EDGE(agenda ,successor )\nFigure E.1 A*-based CCG parsing.\nE.6.3 Heuristic Functions\nBefore we can de\ufb01ne a heuristic function for our A* search, we need to decide how\nto assess the quality of CCG derivations. We\u2019ll make the simplifying assumption\nthat the probability of a CCG derivation is just the product of the probability of\nthe supertags assigned to the words in the derivation, ignoring the rules used in the\nderivation. More formally, given a sentence Sand derivation Dthat contains supertag\nsequence T, we have:\nP(D;S) = P(T;S) (E.10)\n=nY\ni=1P(tijsi) (E.11)",
    "metadata": {
      "source": "E",
      "chunk_id": 8,
      "token_count": 587,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 APPENDIX E \u2022 C OMBINATORY CATEGORIAL GRAMMAR\nTo better \ufb01t with the traditional A* approach, we\u2019d prefer to have states scored by\na cost function where lower is better (i.e., we\u2019re trying to minimize the cost of a\nderivation). To achieve this, we\u2019ll use negative log probabilities to score deriva-\ntions; this results in the following equation, which we\u2019ll use to score completed\nCCG derivations.\nP(D;S) = P(T;S) (E.12)\n=nX\ni=1\u0000logP(tijsi) (E.13)\nGiven this model, we can de\ufb01ne our f-cost as follows. The f-cost of an edge is\nthe sum of two components: g(n), the cost of the span represented by the edge, and\nh(n), the estimate of the cost to complete a derivation containing that edge (these\nare often referred to as the inside andoutside costs ). We\u2019ll de\ufb01ne g(n)for an edge\nusing Equation E.13. That is, it is just the sum of the costs of the supertags that\ncomprise the span.\nForh(n), we need a score that approximates but never overestimates the actual\ncost of the \ufb01nal derivation. A simple heuristic that meets this requirement assumes\nthat each of the words in the outside span will be assigned its most probable su-\npertag . If these are the tags used in the \ufb01nal derivation, then its score will equal\nthe heuristic. If any other tags are used in the \ufb01nal derivation the f-cost will be\nhigher since the new tags must have higher costs, thus guaranteeing that we will not\noverestimate.\nPutting this all together, we arrive at the following de\ufb01nition of a suitable f-cost\nfor an edge.\nf(wi;j;ti;j) = g(wi;j)+h(wi;j) (E.14)\n=jX\nk=i\u0000logP(tkjwk)+\ni\u00001X\nk=1min\nt2tags(\u0000logP(tjwk))+NX\nk=j+1min\nt2tags(\u0000logP(tjwk))\nAs an example, consider an edge representing the word serves with the supertag N\nin the following example.\n(E.15) United serves Denver.\nTheg-cost for this edge is just the negative log probability of this tag, \u0000log10(0:1),\nor 1. The outside h-cost consists of the most optimistic supertag assignments for\nUnited andDenver , which are N=NandNPrespectively. The resulting f-cost for\nthis edge is therefore 1.443.\nE.6.4 An Example\nFig. E.2 shows the initial agenda and the progress of a complete parse for this ex-\nample. After initializing the agenda and the parse table with information from the\nsupertagger, it selects the best edge from the agenda\u2014the entry for United with the\ntagN=Nand f-cost 0.591. This edge does not constitute a complete parse and is\ntherefore used to generate new states by applying all the relevant grammar rules. In\nthis case, applying forward application to United: N/N andserves: N results in the\ncreation of the edge United serves: N[0,2], 1.795 to the agenda.",
    "metadata": {
      "source": "E",
      "chunk_id": 9,
      "token_count": 721,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\nE.6 \u2022 CCG P ARSING 11\nSkipping ahead, at the third iteration an edge representing the complete deriva-\ntionUnited serves Denver, S[0,3], .716 is added to the agenda. However, the algo-\nrithm does not terminate at this point since the cost of this edge (.716) does not place\nit at the top of the agenda. Instead, the edge representing Denver with the category\nNPis popped. This leads to the addition of another edge to the agenda (type-raising\nDenver ). Only after this edge is popped and dealt with does the earlier state repre-\nsenting a complete derivation rise to the top of the agenda where it is popped, goal\ntested, and returned as a solution.\nUnited serves: N[0,2]1.795\nUnited: N/N.591\nDenver: N/N2.494\nDenver: N1.795\nserves: N1.494\nUnited: S\\S1.494\nUnited: S/S1.1938\nUnited: NP.716\nDenver: NP.591\nserves: (S\\NP)/NP.591\nserves Denver: S\\NP[1,3].591\nUnited serves Denver: S[0,3].716\nDenver: S/(S\\NP)[0,1].591\n1\n23\n456Initial AgendaGoal State\n\u2026\nS: 0.716S/NP: 0.591Unitedserves[0,1][0,2][0,3][1,2][1,3][2,3]N/N: 0.591NP: 0.716S/S: 1.1938S\\S: 1.494\u2026Denver\n(S\\NP)/NP: 0.591N: 1.494\u2026NP: 0.591N: 1.795N/N: 2.494\u2026N: 1.795\nFigure E.2 Example of an A* search for the example \u201cUnited serves Denver\u201d. The circled numbers on the\nblue boxes indicate the order in which the states are popped from the agenda. The costs in each state are based\non f-costs using negative log10probabilities.\nThe effectiveness of the A* approach is re\ufb02ected in the coloring of the states in\nFig. E.2 as well as the \ufb01nal parsing table. The edges shown in blue (including all the\n\n## Page 12\n\n12 APPENDIX E \u2022 C OMBINATORY CATEGORIAL GRAMMAR\ninitial lexical category assignments not explicitly shown) re\ufb02ect states in the search\nspace that never made it to the top of the agenda and, therefore, never contributed any\nedges to the \ufb01nal table. This is in contrast to the PCKY approach where the parser\nsystematically \ufb01lls the parse table with all possible constituents for all possible spans\nin the input, \ufb01lling the table with myriad constituents that do not contribute to the\n\ufb01nal analysis.\nE.7 Summary\nThis chapter has introduced combinatory categorial grammar (CCG):\n\u2022 Combinatorial categorial grammar (CCG) is a computationally relevant lexi-\ncalized approach to grammar and parsing.\n\u2022 Much of the dif\ufb01culty in CCG parsing is disambiguating the highly rich lexical\nentries, and so CCG parsers are generally based on supertagging .\n\u2022 Supertagging is the equivalent of part-of-speech tagging in highly lexicalized\ngrammar frameworks. The tags are very grammatically rich and dictate much\nof the derivation for a sentence.\nBibliographical and Historical Notes",
    "metadata": {
      "source": "E",
      "chunk_id": 10,
      "token_count": 775,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 APPENDIX E \u2022 C OMBINATORY CATEGORIAL GRAMMAR\ninitial lexical category assignments not explicitly shown) re\ufb02ect states in the search\nspace that never made it to the top of the agenda and, therefore, never contributed any\nedges to the \ufb01nal table. This is in contrast to the PCKY approach where the parser\nsystematically \ufb01lls the parse table with all possible constituents for all possible spans\nin the input, \ufb01lling the table with myriad constituents that do not contribute to the\n\ufb01nal analysis.\nE.7 Summary\nThis chapter has introduced combinatory categorial grammar (CCG):\n\u2022 Combinatorial categorial grammar (CCG) is a computationally relevant lexi-\ncalized approach to grammar and parsing.\n\u2022 Much of the dif\ufb01culty in CCG parsing is disambiguating the highly rich lexical\nentries, and so CCG parsers are generally based on supertagging .\n\u2022 Supertagging is the equivalent of part-of-speech tagging in highly lexicalized\ngrammar frameworks. The tags are very grammatically rich and dictate much\nof the derivation for a sentence.\nBibliographical and Historical Notes\n\n## Page 13\n\nBibliographical and Historical Notes 13\nAjdukiewicz, K. 1935. Die syntaktische Konnexit \u00a8at.Stu-\ndia Philosophica , 1:1\u201327. English translation \u201cSyntactic\nConnexion\u201d by H. Weber in McCall, S. (Ed.) 1967. Polish\nLogic , pp. 207\u2013231, Oxford University Press.\nBangalore, S. and A. K. Joshi. 1999. Supertagging: An\napproach to almost parsing. Computational Linguistics ,\n25(2):237\u2013265.\nBar-Hillel, Y . 1953. A quasi-arithmetical notation for syn-\ntactic description. Language , 29:47\u201358.\nBresnan, J., ed. 1982. The Mental Representation of Gram-\nmatical Relations . MIT Press.\nHockenmaier, J. and M. Steedman. 2007. CCGbank: a cor-\npus of CCG derivations and dependency structures ex-\ntracted from the penn treebank. Computational Linguis-\ntics, 33(3):355\u2013396.\nJoshi, A. K. 1985. Tree adjoining grammars: How\nmuch context-sensitivity is required to provide reasonable\nstructural descriptions? In D. R. Dowty, L. Karttunen,\nand A. Zwicky, eds, Natural Language Parsing , 206\u2013250.\nCambridge University Press.\nKlein, D. and C. D. Manning. 2003. A* parsing: Fast exact\nViterbi parse selection. HLT-NAACL .\nLewis, M. and M. Steedman. 2014. A* ccg parsing with a\nsupertag-factored model. EMNLP .\nPollard, C. and I. A. Sag. 1994. Head-Driven Phrase Struc-\nture Grammar . University of Chicago Press.\nSteedman, M. 1989. Constituency and coordination in a\ncombinatory grammar. In M. R. Baltin and A. S. Kroch,\neds, Alternative Conceptions of Phrase Structure , 201\u2013\n231. University of Chicago.\nSteedman, M. 1996. Surface Structure and Interpretation .\nMIT Press. Linguistic Inquiry Monograph, 30.\nSteedman, M. 2000. The Syntactic Process . The MIT Press.",
    "metadata": {
      "source": "E",
      "chunk_id": 11,
      "token_count": 782,
      "chapter_title": ""
    }
  }
]