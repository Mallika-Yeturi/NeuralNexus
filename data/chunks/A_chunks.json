[
  {
    "content": "# A\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\nAHidden Markov Models\nChapter 17 introduced the Hidden Markov Model and applied it to part of speech\ntagging. Part of speech tagging is a fully-supervised learning task, because we have\na corpus of words labeled with the correct part-of-speech tag. But many applications\ndon\u2019t have labeled data. So in this chapter, we introduce the full set of algorithms for\nHMMs, including the key unsupervised learning algorithm for HMM, the Forward-\nBackward algorithm. We\u2019ll repeat some of the text from Chapter 17 for readers who\nwant the whole story laid out in a single chapter.\nA.1 Markov Chains\nThe HMM is based on augmenting the Markov chain. A Markov chain is a model Markov chain\nthat tells us something about the probabilities of sequences of random variables,\nstates , each of which can take on values from some set. These sets can be words, or\ntags, or symbols representing anything, like the weather. A Markov chain makes a\nvery strong assumption that if we want to predict the future in the sequence, all that\nmatters is the current state. The states before the current state have no impact on the\nfuture except via the current state. It\u2019s as if to predict tomorrow\u2019s weather you could\nexamine today\u2019s weather but you weren\u2019t allowed to look at yesterday\u2019s weather.\nWARM3HOT1COLD2.8.6.1.1.3.6.1.1.3\ncharminguniformlyare.1.4.5.5.5.2.6.2\n(a) (b)\nFigure A.1 A Markov chain for weather (a) and one for words (b), showing states and\ntransitions. A start distribution pis required; setting p= [0:1;0:7;0:2]for (a) would mean a\nprobability 0.7 of starting in state 2 (cold), probability 0.1 of starting in state 1 (hot), etc.\nMore formally, consider a sequence of state variables q1;q2;:::;qi. A Markov\nmodel embodies the Markov assumption on the probabilities of this sequence: thatMarkov\nassumption\nwhen predicting the future, the past doesn\u2019t matter, only the present.\nMarkov Assumption: P(qi=ajq1:::qi\u00001) =P(qi=ajqi\u00001) (A.1)\nFigure A.1a shows a Markov chain for assigning a probability to a sequence of\nweather events, for which the vocabulary consists of HOT,COLD , and WARM . The\nstates are represented as nodes in the graph, and the transitions, with their probabil-\nities, as edges. The transitions are probabilities: the values of arcs leaving a given",
    "metadata": {
      "source": "A",
      "chunk_id": 0,
      "token_count": 635,
      "chapter_title": "A"
    }
  },
  {
    "content": "## Page 2\n\n2APPENDIX A \u2022 H IDDEN MARKOV MODELS\nstate must sum to 1. Figure A.1b shows a Markov chain for assigning a probabil-\nity to a sequence of words w1:::wn. This Markov chain should be familiar; in fact,\nit represents a bigram language model, with each edge expressing the probability\np(wijwj)! Given the two models in Fig. A.1, we can assign a probability to any\nsequence from our vocabulary.\nFormally, a Markov chain is speci\ufb01ed by the following components:\nQ=q1q2:::qN a set of Nstates\nA=a11a12:::aN1:::aNN atransition probability matrix A, each ai jrepresent-\ning the probability of moving from state ito state j, s.t.Pn\nj=1ai j=18i\np=p1;p2;:::;pN aninitial probability distribution over states. piis the\nprobability that the Markov chain will start in state i.\nSome states jmay have pj=0, meaning that they cannot\nbe initial states. Also,PN\ni=1pi=1\nBefore you go on, use the sample probabilities in Fig. A.1a (with p= [:1;:7:;2])\nto compute the probability of each of the following sequences:\n(A.2) hot hot hot hot\n(A.3) cold hot cold hot\nWhat does the difference in these probabilities tell you about a real-world weather\nfact encoded in Fig. A.1a?\nA.2 The Hidden Markov Model\nA Markov chain is useful when we need to compute a probability for a sequence\nof observable events. In many cases, however, the events we are interested in are\nhidden : we don\u2019t observe them directly. For example we don\u2019t normally observe hidden\npart-of-speech tags in a text. Rather, we see words, and must infer the tags from the\nword sequence. We call the tags hidden because they are not observed.\nAhidden Markov model (HMM ) allows us to talk about both observed eventsHidden\nMarkov model\n(like words that we see in the input) and hidden events (like part-of-speech tags) that\nwe think of as causal factors in our probabilistic model. An HMM is speci\ufb01ed by\nthe following components:\nQ=q1q2:::qN a set of Nstates\nA=a11:::ai j:::aNN atransition probability matrix A, each ai jrepresenting the probability\nof moving from state ito state j, s.t.PN\nj=1ai j=18i\nB=bi(ot) a sequence of observation likelihoods , also called emission probabili-\nties, each expressing the probability of an observation ot(drawn from a\nvocabulary V=v1;v2;:::;vV) being generated from a state qi\np=p1;p2;:::;pN aninitial probability distribution over states. piis the probability that\nthe Markov chain will start in state i. Some states jmay have pj=0,\nmeaning that they cannot be initial states. Also,Pn\ni=1pi=1\nThe HMM is given as input O=o1o2:::oT: a sequence of Tobservations , each\none drawn from the vocabulary V.\nA \ufb01rst-order hidden Markov model instantiates two simplifying assumptions.\nFirst, as with a \ufb01rst-order Markov chain, the probability of a particular state depends",
    "metadata": {
      "source": "A",
      "chunk_id": 1,
      "token_count": 759,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\nA.2 \u2022 T HEHIDDEN MARKOV MODEL 3\nonly on the previous state:\nMarkov Assumption: P(qijq1:::qi\u00001) =P(qijqi\u00001) (A.4)\nSecond, the probability of an output observation oidepends only on the state that\nproduced the observation qiand not on any other states or any other observations:\nOutput Independence: P(oijq1:::qi;:::; qT;o1;:::; oi;:::; oT) =P(oijqi)(A.5)\nTo exemplify these models, we\u2019ll use a task invented by Jason Eisner (2002).\nImagine that you are a climatologist in the year 2799 studying the history of global\nwarming. You cannot \ufb01nd any records of the weather in Baltimore, Maryland, for\nthe summer of 2020, but you do \ufb01nd Jason Eisner\u2019s diary, which lists how many ice\ncreams Jason ate every day that summer. Our goal is to use these observations to\nestimate the temperature every day. We\u2019ll simplify this weather task by assuming\nthere are only two kinds of days: cold (C) and hot (H). So the Eisner task is as\nfollows:\nGiven a sequence of observations O(each an integer representing the\nnumber of ice creams eaten on a given day) \ufb01nd the \u2018hidden\u2019 sequence\nQof weather states (H or C) which caused Jason to eat the ice cream.\nFigure A.2 shows a sample HMM for the ice cream task. The two hidden states\n(H and C) correspond to hot and cold weather, and the observations (drawn from the\nalphabet O=f1;2;3g) correspond to the number of ice creams eaten by Jason on a\ngiven day.\n\u03c0 = [.2,.8]HOT2COLD1B2P(1 | HOT)           .2P(2 | HOT)     =    .4P(3 | HOT)           .4.6.5.4.5P(1 | COLD)         .5P(2 | COLD)   =    .4P(3 | COLD)         .1B1\nFigure A.2 A hidden Markov model for relating numbers of ice creams eaten by Jason (the\nobservations) to the weather (H or C, the hidden variables).\nAn in\ufb02uential tutorial by Rabiner (1989), based on tutorials by Jack Ferguson in\nthe 1960s, introduced the idea that hidden Markov models should be characterized\nbythree fundamental problems :\nProblem 1 (Likelihood): Given an HMM l= (A;B)and an observation se-\nquence O, determine the likelihood P(Ojl).\nProblem 2 (Decoding): Given an observation sequence Oand an HMM l=\n(A;B), discover the best hidden state sequence Q.\nProblem 3 (Learning): Given an observation sequence Oand the set of states\nin the HMM, learn the HMM parameters AandB.\nWe already saw an example of Problem 2 in Chapter 17. In the next two sections\nwe introduce the Forward and Forward-Backward algorithms to solve Problems 1\nand 3 and give more information on Problem 2",
    "metadata": {
      "source": "A",
      "chunk_id": 2,
      "token_count": 700,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4APPENDIX A \u2022 H IDDEN MARKOV MODELS\nA.3 Likelihood Computation: The Forward Algorithm\nOur \ufb01rst problem is to compute the likelihood of a particular observation sequence.\nFor example, given the ice-cream eating HMM in Fig. A.2, what is the probability\nof the sequence 3 1 3 ? More formally:\nComputing Likelihood: Given an HMM l= (A;B)and an observa-\ntion sequence O, determine the likelihood P(Ojl).\nFor a Markov chain, where the surface observations are the same as the hidden\nevents, we could compute the probability of 3 1 3 just by following the states labeled\n3 1 3 and multiplying the probabilities along the arcs. For a hidden Markov model,\nthings are not so simple. We want to determine the probability of an ice-cream\nobservation sequence like 3 1 3 , but we don\u2019t know what the hidden state sequence\nis!\nLet\u2019s start with a slightly simpler situation. Suppose we already knew the weather\nand wanted to predict how much ice cream Jason would eat. This is a useful part\nof many HMM tasks. For a given hidden state sequence (e.g., hot hot cold ), we can\neasily compute the output likelihood of 3 1 3 .\nLet\u2019s see how. First, recall that for hidden Markov models, each hidden state\nproduces only a single observation. Thus, the sequence of hidden states and the\nsequence of observations have the same length.1\nGiven this one-to-one mapping and the Markov assumptions expressed in Eq. A.4,\nfor a particular hidden state sequence Q=q1;q2;:::;qTand an observation sequence\nO=o1;o2;:::;oT, the likelihood of the observation sequence is\nP(OjQ) =TY\ni=1P(oijqi) (A.6)\nThe computation of the forward probability for our ice-cream observation 3 1 3 from\none possible hidden state sequence hot hot cold is shown in Eq. A.7. Figure A.3\nshows a graphic representation of this computation.\nP(3 1 3jhot hot cold ) = P(3jhot)\u0002P(1jhot)\u0002P(3jcold) (A.7)\ncoldhot3.4hot13.2.1\nFigure A.3 The computation of the observation likelihood for the ice-cream events 3 1 3\ngiven the hidden state sequence hot hot cold .\nBut of course, we don\u2019t actually know what the hidden state (weather) sequence\nwas. We\u2019ll need to compute the probability of ice-cream events 3 1 3 instead by\n1In a variant of HMMs called segmental HMMs (in speech recognition) or semi-HMMs (in text pro-\ncessing) this one-to-one mapping between the length of the hidden state sequence and the length of the\nobservation sequence does not hold.",
    "metadata": {
      "source": "A",
      "chunk_id": 3,
      "token_count": 643,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\nA.3 \u2022 L IKELIHOOD COMPUTATION : THEFORWARD ALGORITHM 5\nsumming over all possible weather sequences, weighted by their probability. First,\nlet\u2019s compute the joint probability of being in a particular weather sequence Qand\ngenerating a particular sequence Oof ice-cream events. In general, this is\nP(O;Q) =P(OjQ)\u0002P(Q) =TY\ni=1P(oijqi)\u0002TY\ni=1P(qijqi\u00001) (A.8)\nThe computation of the joint probability of our ice-cream observation 3 1 3 and one\npossible hidden state sequence hot hot cold is shown in Eq. A.9. Figure A.4 shows\na graphic representation of this computation.\nP(3 1 3 ;hot hot cold ) = P(hotjstart)\u0002P(hotjhot)\u0002P(coldjhot)\n\u0002P(3jhot)\u0002P(1jhot)\u0002P(3jcold) (A.9)\ncoldhot3.4hot.613.4.2.1\nFigure A.4 The computation of the joint probability of the ice-cream events 3 1 3 and the\nhidden state sequence hot hot cold .\nNow that we know how to compute the joint probability of the observations\nwith a particular hidden state sequence, we can compute the total probability of the\nobservations just by summing over all possible hidden state sequences:\nP(O) =X\nQP(O;Q) =X\nQP(OjQ)P(Q) (A.10)\nFor our particular case, we would sum over the eight 3-event sequences cold cold\ncold,cold cold hot , that is,\nP(3 1 3) =P(3 1 3 ;cold cold cold )+P(3 1 3 ;cold cold hot )+P(3 1 3 ;hot hot cold )+:::\nFor an HMM with Nhidden states and an observation sequence of Tobserva-\ntions, there are NTpossible hidden sequences. For real tasks, where NandTare\nboth large, NTis a very large number, so we cannot compute the total observation\nlikelihood by computing a separate observation likelihood for each hidden state se-\nquence and then summing them.\nInstead of using such an extremely exponential algorithm, we use an ef\ufb01cient\nO(N2T)algorithm called the forward algorithm . The forward algorithm is a kindforward\nalgorithm\nofdynamic programming algorithm, that is, an algorithm that uses a table to store\nintermediate values as it builds up the probability of the observation sequence. The\nforward algorithm computes the observation probability by summing over the prob-\nabilities of all possible hidden state paths that could generate the observation se-\nquence, but it does so ef\ufb01ciently by implicitly folding each of these paths into a\nsingle forward trellis .\nFigure A.5 shows an example of the forward trellis for computing the likelihood\nof3 1 3 given the hidden state sequence hot hot cold .",
    "metadata": {
      "source": "A",
      "chunk_id": 4,
      "token_count": 654,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6APPENDIX A \u2022 H IDDEN MARKOV MODELS\n\u03c0HCHCHCP(C|start) * P(3|C).2 * .1P(H|H) * P(1|H).6 * .2P(C|C) * P(1|C).5 * .5P(C|H) * P(1|C).4 * .5P(H|C) * P(1|H).5 * .2P(H|start)*P(3|H).8 * .4\u03b11(2)=.32\u03b11(1) = .02\u03b12(2)= .32*.12 + .02*.1 = .0404\u03b12(1) = .32*.2 + .02*.25 = .069\ntCHq2q1o13o2o313\nFigure A.5 The forward trellis for computing the total observation likelihood for the ice-cream events 3 1 3 .\nHidden states are in circles, observations in squares. The \ufb01gure shows the computation of at(j)for two states at\ntwo time steps. The computation in each cell follows Eq. A.12: at(j) =PN\ni=1at\u00001(i)ai jbj(ot). The resulting\nprobability expressed in each cell is Eq. A.11: at(j) =P(o1;o2:::ot;qt=jjl).\nEach cell of the forward algorithm trellis at(j)represents the probability of be-\ning in state jafter seeing the \ufb01rst tobservations, given the automaton l. The value\nof each cell at(j)is computed by summing over the probabilities of every path that\ncould lead us to this cell. Formally, each cell expresses the following probability:\nat(j) =P(o1;o2:::ot;qt=jjl) (A.11)\nHere, qt=jmeans \u201cthe tthstate in the sequence of states is state j\u201d. We compute\nthis probability at(j)by summing over the extensions of all the paths that lead to\nthe current cell. For a given state qjat time t, the value at(j)is computed as\nat(j) =NX\ni=1at\u00001(i)ai jbj(ot) (A.12)\nThe three factors that are multiplied in Eq. A.12 in extending the previous paths\nto compute the forward probability at time tare\nat\u00001(i) theprevious forward path probability from the previous time step\nai j thetransition probability from previous state qito current state qj\nbj(ot) thestate observation likelihood of the observation symbol otgiven\nthe current state j\nConsider the computation in Fig. A.5 of a2(2), the forward probability of being\nat time step 2 in state 2 having generated the partial observation 3 1. We compute by\nextending the aprobabilities from time step 1, via two paths, each extension con-\nsisting of the three factors above: a1(1)\u0002P(HjC)\u0002P(1jH)anda1(2)\u0002P(HjH)\u0002\nP(1jH).\nFigure A.6 shows another visualization of this induction step for computing the\nvalue in one new cell of the trellis.\nWe give two formal de\ufb01nitions of the forward algorithm: the pseudocode in\nFig. A.7 and a statement of the de\ufb01nitional recursion here.",
    "metadata": {
      "source": "A",
      "chunk_id": 5,
      "token_count": 748,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\nA.3 \u2022 L IKELIHOOD COMPUTATION : THEFORWARD ALGORITHM 7\not-1ota1ja2jaNja3jbj(ot)\u03b1t(j)= \u03a3i \u03b1t-1(i) aij bj(ot) \nq1q2q3qN\nq1qjq2q1q2ot+1ot-2q1q2q3q3qNqN\u03b1t-1(N)\n\u03b1t-1(3)\u03b1t-1(2)\u03b1t-1(1)\u03b1t-2(N)\n\u03b1t-2(3)\u03b1t-2(2)\u03b1t-2(1)\nFigure A.6 Visualizing the computation of a single element at(i)in the trellis by summing\nall the previous values at\u00001, weighted by their transition probabilities a, and multiplying by\nthe observation probability bi(ot). For many applications of HMMs, many of the transition\nprobabilities are 0, so not all previous states will contribute to the forward probability of the\ncurrent state. Hidden states are in circles, observations in squares. Shaded nodes are included\nin the probability computation for at(i).\nfunction FORWARD (observations of len T,state-graph of len N)returns forward-prob\ncreate a probability matrix forward[N,T]\nforeach state sfrom 1toNdo ; initialization step\nforward [s,1] ps\u0003bs(o1)\nforeach time step tfrom 2toTdo ; recursion step\nforeach state sfrom 1toNdo\nforward [s;t] NX\ns0=1forward [s0;t\u00001]\u0003as0;s\u0003bs(ot)\nforwardprob NX\ns=1forward [s;T] ; termination step\nreturn forwardprob\nFigure A.7 The forward algorithm, where forward [s;t]represents at(s).\n1. Initialization:\na1(j) = pjbj(o1)1\u0014j\u0014N\n2. Recursion:\nat(j) =NX\ni=1at\u00001(i)ai jbj(ot); 1\u0014j\u0014N;1<t\u0014T\n3. Termination:\nP(Ojl) =NX\ni=1aT(i)",
    "metadata": {
      "source": "A",
      "chunk_id": 6,
      "token_count": 481,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8",
    "metadata": {
      "source": "A",
      "chunk_id": 7,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "8APPENDIX A \u2022 H IDDEN MARKOV MODELS\nA.4 Decoding: The Viterbi Algorithm\nFor any model, such as an HMM, that contains hidden variables, the task of deter-\nmining which sequence of variables is the underlying source of some sequence of\nobservations is called the decoding task. In the ice-cream domain, given a sequence decoding\nof ice-cream observations 3 1 3 and an HMM, the task of the decoder is to \ufb01nd the\nbest hidden weather sequence ( H H H ). More formally,\nDecoding : Given as input an HMM l= (A;B)and a sequence of ob-\nservations O=o1;o2;:::;oT, \ufb01nd the most probable sequence of states\nQ=q1q2q3:::qT.\nWe might propose to \ufb01nd the best sequence as follows: For each possible hid-\nden state sequence ( HHH ,HHC ,HCH , etc.), we could run the forward algorithm\nand compute the likelihood of the observation sequence given that hidden state se-\nquence. Then we could choose the hidden state sequence with the maximum obser-\nvation likelihood. It should be clear from the previous section that we cannot do this\nbecause there are an exponentially large number of state sequences.\nInstead, the most common decoding algorithms for HMMs is the Viterbi algo-\nrithm . Like the forward algorithm, Viterbi is a kind of dynamic programmingViterbi\nalgorithm\nthat makes uses of a dynamic programming trellis. Viterbi also strongly resembles\nanother dynamic programming variant, the minimum edit distance algorithm of\nChapter 2.\n\u03c0HCHCHCP(C|start) * P(3|C).2 * .1P(H|H) * P(1|H).6 * .2P(C|C) * P(1|C).5 * .5P(C|H) * P(1|C).4 * .5P(H|C) * P(1|H).5 * .2P(H|start)*P(3|H).8 * .4v1(2)=.32v1(1) = .02v2(2)= max(.32*.12, .02*.10) = .038v2(1) = max(.32*.20, .02*.25) = .064\ntCHq2q1o1o2o3313\nFigure A.8 The Viterbi trellis for computing the best path through the hidden state space for the ice-cream\neating events 3 1 3 . Hidden states are in circles, observations in squares. White (un\ufb01lled) circles indicate illegal\ntransitions. The \ufb01gure shows the computation of vt(j)for two states at two time steps. The computation in each\ncell follows Eq. A.14: vt(j) =max 1\u0014i\u0014N\u00001vt\u00001(i)ai jbj(ot). The resulting probability expressed in each cell\nis Eq. A.13: vt(j) =P(q0;q1;:::; qt\u00001;o1;o2;:::; ot;qt=jjl).\nFigure A.8 shows an example of the Viterbi trellis for computing the best hidden\nstate sequence for the observation sequence 3 1 3 . The idea is to process the ob-\nservation sequence left to right, \ufb01lling out the trellis. Each cell of the trellis, vt(j),\nrepresents the probability that the HMM is in state jafter seeing the \ufb01rst tobser-\nvations and passing through the most probable state sequence q1;:::;qt\u00001, given the",
    "metadata": {
      "source": "A",
      "chunk_id": 8,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\nA.4 \u2022 D ECODING : THEVITERBI ALGORITHM 9\nautomaton l. The value of each cell vt(j)is computed by recursively taking the\nmost probable path that could lead us to this cell. Formally, each cell expresses the\nprobability\nvt(j) = max\nq1;:::;qt\u00001P(q1:::qt\u00001;o1;o2:::ot;qt=jjl) (A.13)\nNote that we represent the most probable path by taking the maximum over all\npossible previous state sequences max\nq1;:::;qt\u00001. Like other dynamic programming algo-\nrithms, Viterbi \ufb01lls each cell recursively. Given that we had already computed the\nprobability of being in every state at time t\u00001, we compute the Viterbi probability\nby taking the most probable of the extensions of the paths that lead to the current\ncell. For a given state qjat time t, the value vt(j)is computed as\nvt(j) =Nmax\ni=1vt\u00001(i)ai jbj(ot) (A.14)\nThe three factors that are multiplied in Eq. A.14 for extending the previous paths to\ncompute the Viterbi probability at time tare\nvt\u00001(i) theprevious Viterbi path probability from the previous time step\nai j thetransition probability from previous state qito current state qj\nbj(ot) thestate observation likelihood of the observation symbol otgiven\nthe current state j\nfunction VITERBI (observations of len T,state-graph of len N)returns best-path ,path-prob\ncreate a path probability matrix viterbi[N,T]\nforeach state sfrom 1toNdo ; initialization step\nviterbi [s,1] ps\u0003bs(o1)\nbackpointer [s,1] 0\nforeach time step tfrom 2toTdo ; recursion step\nforeach state sfrom 1toNdo\nviterbi [s,t] Nmax\ns0=1viterbi [s0;t\u00001]\u0003as0;s\u0003bs(ot)\nbackpointer [s,t] Nargmax\ns0=1viterbi [s0;t\u00001]\u0003as0;s\u0003bs(ot)\nbestpathprob Nmax\ns=1viterbi [s;T] ; termination step\nbestpathpointer Nargmax\ns=1viterbi [s;T] ; termination step\nbestpath the path starting at state bestpathpointer , that follows backpointer[] to states back in time\nreturn bestpath ,bestpathprob\nFigure A.9 Viterbi algorithm for \ufb01nding optimal sequence of hidden states. Given an observation sequence\nand an HMM l= (A;B), the algorithm returns the state path through the HMM that assigns maximum likelihood\nto the observation sequence.\nFigure A.9 shows pseudocode for the Viterbi algorithm. Note that the Viterbi\nalgorithm is identical to the forward algorithm except that it takes the max over the\nprevious path probabilities whereas the forward algorithm takes the sum. Note also\nthat the Viterbi algorithm has one component that the forward algorithm doesn\u2019t",
    "metadata": {
      "source": "A",
      "chunk_id": 9,
      "token_count": 686,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 APPENDIX A \u2022 H IDDEN MARKOV MODELS\nhave: backpointers . The reason is that while the forward algorithm needs to pro-\nduce an observation likelihood, the Viterbi algorithm must produce a probability and\nalso the most likely state sequence. We compute this best state sequence by keeping\ntrack of the path of hidden states that led to each state, as suggested in Fig. A.10, and\nthen at the end backtracing the best path to the beginning (the Viterbi backtrace ).Viterbi\nbacktrace\n\u03c0HCHCHCP(C|start) * P(3|C).2 * .1P(H|H) * P(1|H).6 * .2P(C|C) * P(1|C).5 * .5P(C|H) * P(1|C).4 * .5P(H|C) * P(1|H).5 * .2P(H|start)*P(3|H).8 * .4v1(2)=.32v1(1) = .02v2(2)= max(.32*.12, .02*.10) = .038v2(1) = max(.32*.20, .02*.25) = .064\ntCHq2q1o1o2o3313\nFigure A.10 The Viterbi backtrace. As we extend each path to a new state account for the next observation,\nwe keep a backpointer (shown with broken lines) to the best path that led us to this state.\nFinally, we can give a formal de\ufb01nition of the Viterbi recursion as follows:\n1.Initialization:\nv1(j) = pjbj(o1) 1\u0014j\u0014N\nbt1(j) = 0 1\u0014j\u0014N\n2.Recursion\nvt(j) =Nmax\ni=1vt\u00001(i)ai jbj(ot); 1\u0014j\u0014N;1<t\u0014T\nbtt(j) =Nargmax\ni=1vt\u00001(i)ai jbj(ot); 1\u0014j\u0014N;1<t\u0014T\n3.Termination:\nThe best score: P\u0003=Nmax\ni=1vT(i)\nThe start of backtrace: qT\u0003=Nargmax\ni=1vT(i)\nA.5 HMM Training: The Forward-Backward Algorithm\nWe turn to the third problem for HMMs: learning the parameters of an HMM, that\nis, the AandBmatrices. Formally,\nLearning: Given an observation sequence Oand the set of possible\nstates in the HMM, learn the HMM parameters AandB.",
    "metadata": {
      "source": "A",
      "chunk_id": 10,
      "token_count": 583,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\nA.5 \u2022 HMM T RAINING : THEFORWARD -BACKWARD ALGORITHM 11\nThe input to such a learning algorithm would be an unlabeled sequence of ob-\nservations Oand a vocabulary of potential hidden states Q. Thus, for the ice cream\ntask, we would start with a sequence of observations O=f1;3;2;:::;gand the set of\nhidden states HandC.\nThe standard algorithm for HMM training is the forward-backward , orBaum-Forward-\nbackward\nWelch algorithm (Baum, 1972), a special case of the Expectation-Maximization Baum-Welch\norEM algorithm (Dempster et al., 1977). The algorithm will let us train both the EM\ntransition probabilities Aand the emission probabilities Bof the HMM. EM is an\niterative algorithm, computing an initial estimate for the probabilities, then using\nthose estimates to compute a better estimate, and so on, iteratively improving the\nprobabilities that it learns.\nLet us begin by considering the much simpler case of training a fully visible\nMarkov model, where we know both the temperature and the ice cream count for\nevery day. That is, imagine we see the following set of input observations and mag-\nically knew the aligned hidden state sequences:\n3 3 2 1 1 2 1 2 3\nhot hot cold cold cold cold cold hot hot\nThis would easily allow us to compute the HMM parameters just by maximum\nlikelihood estimation from the training data. First, we can compute pfrom the count\nof the 3 initial hidden states:\nph=1=3pc=2=3\nNext we can directly compute the Amatrix from the transitions, ignoring the \ufb01nal\nhidden states:\np(hotjhot) =2=3p(coldjhot) =1=3\np(coldjcold) =2=3p(hotjcold) =1=3\nand the Bmatrix:\nP(1jhot) =0=4=0p(1jcold) =3=5=:6\nP(2jhot) =1=4=:25 p(2jcold=2=5=:4\nP(3jhot) =3=4=:75 p(3jcold) =0\nFor a real HMM, we cannot compute these counts directly from an observation\nsequence since we don\u2019t know which path of states was taken through the machine\nfor a given input. For example, suppose I didn\u2019t tell you the temperature on day 2,\nand you had to guess it, but you (magically) had the above probabilities, and the\ntemperatures on the other days. You could do some Bayesian arithmetic with all the\nother probabilities to get estimates of the likely temperature on that missing day, and\nuse those to get expected counts for the temperatures for day 2.\nBut the real problem is even harder: we don\u2019t know the counts of being in any\nof the hidden states!! The Baum-Welch algorithm solves this by iteratively esti-\nmating the counts. We will start with an estimate for the transition and observation\nprobabilities and then use these estimated probabilities to derive better and better\nprobabilities. And we\u2019re going to do this by computing the forward probability for\nan observation and then dividing that probability mass among all the different paths\nthat contributed to this forward probability.\nTo understand the algorithm, we need to de\ufb01ne a useful probability related to the\nforward probability and called the backward probability . The backward probabil-backward\nprobability",
    "metadata": {
      "source": "A",
      "chunk_id": 11,
      "token_count": 758,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 APPENDIX A \u2022 H IDDEN MARKOV MODELS\nitybis the probability of seeing the observations from time t+1 to the end, given\nthat we are in state iat time t(and given the automaton l):\nbt(i) =P(ot+1;ot+2:::oTjqt=i;l) (A.15)\nIt is computed inductively in a similar manner to the forward algorithm.\n1.Initialization:\nbT(i) = 1;1\u0014i\u0014N\n2.Recursion\nbt(i) =NX\nj=1ai jbj(ot+1)bt+1(j);1\u0014i\u0014N;1\u0014t<T\n3.Termination:\nP(Ojl) =NX\nj=1pjbj(o1)b1(j)\nFigure A.11 illustrates the backward induction step.\not+1otai1ai2aiNai3b1(ot+1)\u03b2t(i)= \u03a3j \u03b2t+1(j) aij  bj(ot+1) \nq1q2q3qN\nq1qiq2q1q2ot-1q3qN\u03b2t+1(N)\n\u03b2t+1(3)\u03b2t+1(2)\u03b2t+1(1)b2(ot+1)b3(ot+1)bN(ot+1)\nFigure A.11 The computation of bt(i)by summing all the successive values bt+1(j)\nweighted by their transition probabilities ai jand their observation probabilities bj(ot+1).\nWe are now ready to see how the forward and backward probabilities can help\ncompute the transition probability ai jand observation probability bi(ot)from an ob-\nservation sequence, even though the actual path taken through the model is hidden.\nLet\u2019s begin by seeing how to estimate \u02c6 ai jby a variant of simple maximum like-\nlihood estimation:\n\u02c6ai j=expected number of transitions from state ito state j\nexpected number of transitions from state i(A.16)\nHow do we compute the numerator? Here\u2019s the intuition. Assume we had some\nestimate of the probability that a given transition i!jwas taken at a particular\npoint in time tin the observation sequence. If we knew this probability for each\nparticular time t, we could sum over all times tto estimate the total count for the\ntransition i!j.",
    "metadata": {
      "source": "A",
      "chunk_id": 12,
      "token_count": 510,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\nA.5 \u2022 HMM T RAINING : THEFORWARD -BACKWARD ALGORITHM 13\nMore formally, let\u2019s de\ufb01ne the probability xtas the probability of being in state\niat time tand state jat time t+1, given the observation sequence and of course the\nmodel:\nxt(i;j) =P(qt=i;qt+1=jjO;l) (A.17)\nTo compute xt, we \ufb01rst compute a probability which is similar to xt, but differs in\nincluding the probability of the observation; note the different conditioning of O\nfrom Eq. A.17:\nnot-quite- xt(i;j) =P(qt=i;qt+1=j;Ojl) (A.18)\not+2ot+1\u03b1t(i)ot-1otaijbj(ot+1) sisj\u03b2t+1(j)\nFigure A.12 Computation of the joint probability of being in state iat time tand state jat\ntime t+1. The \ufb01gure shows the various probabilities that need to be combined to produce\nP(qt=i;qt+1=j;Ojl): the aandbprobabilities, the transition probability ai jand the\nobservation probability bj(ot+1). After Rabiner (1989) which is \u00a91989 IEEE.\nFigure A.12 shows the various probabilities that go into computing not-quite- xt:\nthe transition probability for the arc in question, the aprobability before the arc, the\nbprobability after the arc, and the observation probability for the symbol just after\nthe arc. These four are multiplied together to produce not-quite- xtas follows:\nnot-quite- xt(i;j) =at(i)ai jbj(ot+1)bt+1(j) (A.19)\nTo compute xtfrom not-quite- xt, we follow the laws of probability and divide by\nP(Ojl), since\nP(XjY;Z) =P(X;YjZ)\nP(YjZ)(A.20)\nThe probability of the observation given the model is simply the forward proba-\nbility of the whole utterance (or alternatively, the backward probability of the whole\nutterance):\nP(Ojl) =NX\nj=1at(j)bt(j) (A.21)",
    "metadata": {
      "source": "A",
      "chunk_id": 13,
      "token_count": 493,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n14 APPENDIX A \u2022 H IDDEN MARKOV MODELS\nSo, the \ufb01nal equation for xtis\nxt(i;j) =at(i)ai jbj(ot+1)bt+1(j)PN\nj=1at(j)bt(j)(A.22)\nThe expected number of transitions from state ito state jis then the sum over all\ntofx. For our estimate of ai jin Eq. A.16, we just need one more thing: the total\nexpected number of transitions from state i. We can get this by summing over all\ntransitions out of state i. Here\u2019s the \ufb01nal formula for \u02c6 ai j:\n\u02c6ai j=PT\u00001\nt=1xt(i;j)PT\u00001\nt=1PN\nk=1xt(i;k)(A.23)\nWe also need a formula for recomputing the observation probability. This is the\nprobability of a given symbol vkfrom the observation vocabulary V, given a state j:\n\u02c6bj(vk). We will do this by trying to compute\n\u02c6bj(vk) =expected number of times in state jand observing symbol vk\nexpected number of times in state j(A.24)\nFor this, we will need to know the probability of being in state jat time t, which\nwe will call gt(j):\ngt(j) =P(qt=jjO;l) (A.25)\nOnce again, we will compute this by including the observation sequence in the\nprobability:\ngt(j) =P(qt=j;Ojl)\nP(Ojl)(A.26)\not+1\u03b1t(j)ot-1otsj\u03b2t(j)\nFigure A.13 The computation of gt(j), the probability of being in state jat time t. Note\nthatgis really a degenerate case of xand hence this \ufb01gure is like a version of Fig. A.12 with\nstate icollapsed with state j. After Rabiner (1989) which is \u00a91989 IEEE.\nAs Fig. A.13 shows, the numerator of Eq. A.26 is just the product of the forward\nprobability and the backward probability:\ngt(j) =at(j)bt(j)\nP(Ojl)(A.27)",
    "metadata": {
      "source": "A",
      "chunk_id": 14,
      "token_count": 482,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\nA.5 \u2022 HMM T RAINING : THEFORWARD -BACKWARD ALGORITHM 15\nWe are ready to compute b. For the numerator, we sum gt(j)for all time steps\ntin which the observation otis the symbol vkthat we are interested in. For the\ndenominator, we sum gt(j)over all time steps t. The result is the percentage of the\ntimes that we were in state jand saw symbol vk(the notationPT\nt=1s:t:Ot=vkmeans\n\u201csum over all tfor which the observation at time twasvk\u201d):\n\u02c6bj(vk) =PT\nt=1s:t:Ot=vkgt(j)\nPT\nt=1gt(j)(A.28)\nWe now have ways in Eq. A.23 and Eq. A.28 to re-estimate the transition Aand ob-\nservation Bprobabilities from an observation sequence O, assuming that we already\nhave a previous estimate of AandB.\nThese re-estimations form the core of the iterative forward-backward algorithm.\nThe forward-backward algorithm (Fig. A.14) starts with some initial estimate of the\nHMM parameters l= (A;B). We then iteratively run two steps. Like other cases of\nthe EM (expectation-maximization) algorithm, the forward-backward algorithm has\ntwo steps: the expectation step, or E-step , and the maximization step, or M-step . E-step\nM-step In the E-step, we compute the expected state occupancy count gand the expected\nstate transition count xfrom the earlier AandBprobabilities. In the M-step, we use\ngandxto recompute new AandBprobabilities.\nfunction FORWARD -BACKWARD (observations of len T,output vocabulary V ,hidden\nstate set Q )returns HMM=(A,B)\ninitialize AandB\niterate until convergence\nE-step\ngt(j) =at(j)bt(j)\naT(qF)8tandj\nxt(i;j) =at(i)ai jbj(ot+1)bt+1(j)\naT(qF)8t;i;andj\nM-step\n\u02c6ai j=T\u00001X\nt=1xt(i;j)\nT\u00001X\nt=1NX\nk=1xt(i;k)\n\u02c6bj(vk) =TX\nt=1s:t:Ot=vkgt(j)\nTX\nt=1gt(j)\nreturn A,B\nFigure A.14 The forward-backward algorithm.\nAlthough in principle the forward-backward algorithm can do completely unsu-\npervised learning of the AandBparameters, in practice the initial conditions are\nvery important. For this reason the algorithm is often given extra information. For\nexample, for HMM-based speech recognition, the HMM structure is often set by\nhand, and only the emission ( B) and (non-zero) Atransition probabilities are trained\nfrom a set of observation sequences O.",
    "metadata": {
      "source": "A",
      "chunk_id": 15,
      "token_count": 631,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\n16 APPENDIX A \u2022 H IDDEN MARKOV MODELS\nA.6 Summary\nThis chapter introduced the hidden Markov model for probabilistic sequence clas-\nsi\ufb01cation .\n\u2022 Hidden Markov models ( HMMs ) are a way of relating a sequence of obser-\nvations to a sequence of hidden classes orhidden states that explain the\nobservations.\n\u2022 The process of discovering the sequence of hidden states, given the sequence\nof observations, is known as decoding orinference . The Viterbi algorithm is\ncommonly used for decoding.\n\u2022 The parameters of an HMM are the Atransition probability matrix and the B\nobservation likelihood matrix. Both can be trained with the Baum-Welch or\nforward-backward algorithm.\nBibliographical and Historical Notes\nAs we discussed in Chapter 17, Markov chains were \ufb01rst used by Markov (1913)\n(translation Markov 2006), to predict whether an upcoming letter in Pushkin\u2019s Eu-\ngene Onegin would be a vowel or a consonant. The hidden Markov model was de-\nveloped by Baum and colleagues at the Institute for Defense Analyses in Princeton\n(Baum and Petrie 1966, Baum and Eagon 1967).\nTheViterbi algorithm was \ufb01rst applied to speech and language processing in the\ncontext of speech recognition by Vintsyuk (1968) but has what Kruskal (1983) calls\na \u201cremarkable history of multiple independent discovery and publication\u201d. Kruskal\nand others give at least the following independently-discovered variants of the algo-\nrithm published in four separate \ufb01elds:\nCitation Field\nViterbi (1967) information theory\nVintsyuk (1968) speech processing\nNeedleman and Wunsch (1970) molecular biology\nSakoe and Chiba (1971) speech processing\nSankoff (1972) molecular biology\nReichert et al. (1973) molecular biology\nWagner and Fischer (1974) computer science\nThe use of the term Viterbi is now standard for the application of dynamic pro-\ngramming to any kind of probabilistic maximization problem in speech and language\nprocessing. For non-probabilistic problems (such as for minimum edit distance), the\nplain term dynamic programming is often used. Forney, Jr. (1973) wrote an early\nsurvey paper that explores the origin of the Viterbi algorithm in the context of infor-\nmation and communications theory.\nOur presentation of the idea that hidden Markov models should be characterized\nby three fundamental problems was modeled after an in\ufb02uential tutorial by Rabiner\n(1989), which was itself based on tutorials by Jack Ferguson of IDA in the 1960s.\nJelinek (1997) and Rabiner and Juang (1993) give very complete descriptions of the\nforward-backward algorithm as applied to the speech recognition problem. Jelinek\n(1997) also shows the relationship between forward-backward and EM.",
    "metadata": {
      "source": "A",
      "chunk_id": 16,
      "token_count": 644,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17",
    "metadata": {
      "source": "A",
      "chunk_id": 17,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Bibliographical and Historical Notes 17\nBaum, L. E. 1972. An inequality and associated maximiza-\ntion technique in statistical estimation for probabilistic\nfunctions of Markov processes. Inequalities III: Pro-\nceedings of the 3rd Symposium on Inequalities . Academic\nPress.\nBaum, L. E. and J. A. Eagon. 1967. An inequality with appli-\ncations to statistical estimation for probabilistic functions\nof Markov processes and to a model for ecology. Bulletin\nof the American Mathematical Society , 73(3):360\u2013363.\nBaum, L. E. and T. Petrie. 1966. Statistical inference for\nprobabilistic functions of \ufb01nite-state Markov chains. An-\nnals of Mathematical Statistics , 37(6):1554\u20131563.\nDempster, A. P., N. M. Laird, and D. B. Rubin. 1977. Max-\nimum likelihood from incomplete data via the EMalgo-\nrithm. Journal of the Royal Statistical Society , 39(1):1\u2013\n21.\nEisner, J. 2002. An interactive spreadsheet for teaching\nthe forward-backward algorithm. Proceedings of the\nACL Workshop on Effective Tools and Methodologies for\nTeaching NLP and CL .\nForney, Jr., G. D. 1973. The Viterbi algorithm. Proceedings\nof the IEEE , 61(3):268\u2013278.\nJelinek, F. 1997. Statistical Methods for Speech Recognition .\nMIT Press.\nKruskal, J. B. 1983. An overview of sequence comparison.\nIn D. Sankoff and J. B. Kruskal, eds, Time Warps, String\nEdits, and Macromolecules: The Theory and Practice of\nSequence Comparison , 1\u201344. Addison-Wesley.\nMarkov, A. A. 1913. Essai d\u2019une recherche statistique sur\nle texte du roman \u201cEugene Onegin\u201d illustrant la liaison\ndes epreuve en chain (\u2018Example of a statistical investiga-\ntion of the text of \u201cEugene Onegin\u201d illustrating the de-\npendence between samples in chain\u2019). Izvistia Impera-\ntorskoi Akademii Nauk (Bulletin de l\u2019Acad \u00b4emie Imp \u00b4eriale\ndes Sciences de St.-P \u00b4etersbourg) , 7:153\u2013162.\nMarkov, A. A. 2006. Classical text in translation: A. A.\nMarkov, an example of statistical investigation of the text\nEugene Onegin concerning the connection of samples in\nchains. Science in Context , 19(4):591\u2013600. Translated by\nDavid Link.\nNeedleman, S. B. and C. D. Wunsch. 1970. A general\nmethod applicable to the search for similarities in the\namino-acid sequence of two proteins. Journal of Molec-\nular Biology , 48:443\u2013453.\nRabiner, L. R. 1989. A tutorial on hidden Markov models\nand selected applications in speech recognition. Proceed-\nings of the IEEE , 77(2):257\u2013286.\nRabiner, L. R. and B. H. Juang. 1993. Fundamentals of\nSpeech Recognition . Prentice Hall.\nReichert, T. A., D. N. Cohen, and A. K. C. Wong. 1973.\nAn application of information theory to genetic mutations",
    "metadata": {
      "source": "A",
      "chunk_id": 18,
      "token_count": 763,
      "chapter_title": ""
    }
  },
  {
    "content": "torskoi Akademii Nauk (Bulletin de l\u2019Acad \u00b4emie Imp \u00b4eriale\ndes Sciences de St.-P \u00b4etersbourg) , 7:153\u2013162.\nMarkov, A. A. 2006. Classical text in translation: A. A.\nMarkov, an example of statistical investigation of the text\nEugene Onegin concerning the connection of samples in\nchains. Science in Context , 19(4):591\u2013600. Translated by\nDavid Link.\nNeedleman, S. B. and C. D. Wunsch. 1970. A general\nmethod applicable to the search for similarities in the\namino-acid sequence of two proteins. Journal of Molec-\nular Biology , 48:443\u2013453.\nRabiner, L. R. 1989. A tutorial on hidden Markov models\nand selected applications in speech recognition. Proceed-\nings of the IEEE , 77(2):257\u2013286.\nRabiner, L. R. and B. H. Juang. 1993. Fundamentals of\nSpeech Recognition . Prentice Hall.\nReichert, T. A., D. N. Cohen, and A. K. C. Wong. 1973.\nAn application of information theory to genetic mutations\nand the matching of polypeptide sequences. Journal of\nTheoretical Biology , 42:245\u2013261.\nSakoe, H. and S. Chiba. 1971. A dynamic programming\napproach to continuous speech recognition. Proceedings\nof the Seventh International Congress on Acoustics , vol-\nume 3. Akad \u00b4emiai Kiad \u00b4o.\nSankoff, D. 1972. Matching sequences under deletion-\ninsertion constraints. Proceedings of the National\nAcademy of Sciences , 69:4\u20136.Vintsyuk, T. K. 1968. Speech discrimination by dynamic\nprogramming. Cybernetics , 4(1):52\u201357. Original Rus-\nsian: Kibernetika 4(1):81-88. 1968.\nViterbi, A. J. 1967. Error bounds for convolutional codes and\nan asymptotically optimum decoding algorithm. IEEE\nTransactions on Information Theory , IT-13(2):260\u2013269.\nWagner, R. A. and M. J. Fischer. 1974. The string-to-string\ncorrection problem. Journal of the ACM , 21:168\u2013173.",
    "metadata": {
      "source": "A",
      "chunk_id": 19,
      "token_count": 533,
      "chapter_title": ""
    }
  }
]