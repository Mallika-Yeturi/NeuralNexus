[
  {
    "content": "# 10\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n10Large Language Models\n\u201cHow much do we know at any time? Much more, or so I believe, than we\nknow we know. \u201d\nAgatha Christie, The Moving Finger\nFluent speakers of a language bring an enormous amount of knowledge to bear dur-\ning comprehension and production. This knowledge is embodied in many forms,\nperhaps most obviously in the vocabulary, the rich representations we have of words\nand their meanings and usage. This makes the vocabulary a useful lens to explore\nthe acquisition of knowledge from text, by both people and machines.\nEstimates of the size of adult vocabularies vary widely both within and across\nlanguages. For example, estimates of the vocabulary size of young adult speakers of\nAmerican English range from 30,000 to 100,000 depending on the resources used\nto make the estimate and the de\ufb01nition of what it means to know a word. What\nis agreed upon is that the vast majority of words that mature speakers use in their\nday-to-day interactions are acquired early in life through spoken interactions with\ncaregivers and peers, usually well before the start of formal schooling. This active\nvocabulary (usually on the order of 2000 words for young speakers) is extremely\nlimited compared to the size of the adult vocabulary, and is quite stable, with very\nfew additional words learned via casual conversation beyond this early stage. Obvi-\nously, this leaves a very large number of words to be acquired by other means.\nA simple consequence of these facts is that children have to learn about 7 to 10\nwords a day, every single day , to arrive at observed vocabulary levels by the time they\nare 20 years of age. And indeed empirical estimates of vocabulary growth in late el-\nementary through high school are consistent with this rate. How do children achieve\nthis rate of vocabulary growth? The bulk of this knowledge acquisition seems to\nhappen as a by-product of reading, as part of the rich processing and reasoning that\nwe perform when we read. Research into the average amount of time children spend\nreading, and the lexical diversity of the texts they read, indicate that it is possible\nto achieve the desired rate. But the mechanism behind this rate of learning must\nbe remarkable indeed, since at some points during learning the rate of vocabulary\ngrowth exceeds the rate at which new words are appearing to the learner!\nSuch facts have motivated the distributional hypothesis of Chapter 6, which sug-\ngests that aspects of meaning can be learned solely from the texts we encounter over\nour lives, based on the complex association of words with the words they co-occur\nwith (and with the words that those words occur with). The distributional hypothe-\nsis suggests both that we can acquire remarkable amounts of knowledge from text,\nand that this knowledge can be brought to bear long after its initial acquisition. Of\ncourse, grounding from real-world interaction or other modalities can help build\neven more powerful models, but even text alone is remarkably useful.\nIn this chapter we formalize this idea of pretraining \u2014learning knowledge about pretraining\nlanguage and the world from vast amounts of text\u2014and call the resulting pretrained\nlanguage models large language models . Large language models exhibit remark-",
    "metadata": {
      "source": "10",
      "chunk_id": 0,
      "token_count": 709,
      "chapter_title": "10"
    }
  },
  {
    "content": "## Page 2\n\n2CHAPTER 10 \u2022 L ARGE LANGUAGE MODELS\nable performance on all sorts of natural language tasks because of the knowledge\nthey learn in pretraining, and they will play a role throughout the rest of this book.\nThey have been especially transformative for tasks where we need to produce text,\nlike summarization, machine translation, question answering, or chatbots.\nWe\u2019ll start by seeing how to apply the transformer of Chapter 9 to language\nmodeling, in a setting often called causal or autoregressive language models, in\nwhich we iteratively predict words left-to-right from earlier words. We\u2019ll \ufb01rst in-\ntroduce training, seeing how language models are self-trained by iteratively being\ntaught to guess the next word in the text from the prior words.\nWe\u2019ll then talk about the process of text generation. The application of LLMs\nto generate text has vastly broadened the scope of NLP,. Text generation, code-\ngeneration, and image-generation together constitute the important new area of gen-\nerative AI . We\u2019ll introduce speci\ufb01c algorithms for generating text from a language generative AI\nmodel, like greedy decoding andsampling . And we\u2019ll see that almost any NLP\ntask can be modeled as word prediction in a large language model, if we think about\nit in the right way. We\u2019ll work through an example of using large language mod-\nels to solve one classic NLP task of summarization (generating a short text that\nsummarizes some larger document).\n10.1 Large Language Models with Transformers\nThe prior chapter introduced most of the components of a transformer in the domain\nof language modeling: the transformer block including multi-head attention , the\nlanguage modeling head , and the positional encoding of the input. In the following\nsections we\u2019ll introduce the remaining aspects of the transformer LLM: sampling\nandtraining . Before we do that, we use this section to talk about why and how we\napply transformer-based large language models to NLP tasks.\nThe tasks we will describe are all cases of conditional generation . Conditionalconditional\ngeneration\ngeneration is the task of generating text conditioned on an input piece of text. That\nis, we give the LLM an input piece of text, generally called a prompt , and then have\nthe LLM continue generating text token by token, conditioned on the prompt and\nthe previously generated tokens. The fact that transformers have such long contexts\n(many thousands of tokens) makes them very powerful for conditional generation,\nbecause they can look back so far into the prompting text.\nConsider the simple task of text completion, illustrated in Fig. 10.1. Here a\nlanguage model is given a text pre\ufb01x and is asked to generate a possible completion.\nNote that as the generation process proceeds, the model has direct access to the\npriming context as well as to all of its own subsequently generated outputs (at least\nas much as \ufb01ts in the large context window). This ability to incorporate the entirety\nof the earlier context and generated outputs at each time step is the key to the power\nof large language models built from transformers.\nSo why should we care about predicting upcoming words or tokens? The in-\nsight of large language modeling is that many practical NLP tasks can be cast as\nword prediction , and that a powerful-enough language model can solve them with\na high degree of accuracy. For example, we can cast sentiment analysis as language\nmodeling by giving a language model a context like:\nThe sentiment of the sentence ``I like Jackie Chan\" is:\nand comparing the following conditional probability of the words \u201cpositive\u201d and the",
    "metadata": {
      "source": "10",
      "chunk_id": 1,
      "token_count": 751,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\n10.1 \u2022 L ARGE LANGUAGE MODELS WITH TRANSFORMERS 3\nPre\ufb01x TextCompletion Text\nEncoderTransformerBlocksSoftmax\nlongall\nandthanksforallthe\nthe\u2026UUUnencoder layerLanguage ModelingHeadlogits\nSo\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\u2026\nFigure 10.1 Left-to-right (also called autoregressive) text completion with transformer-based large language\nmodels. As each token is generated, it gets added onto the context as a pre\ufb01x for generating the next token.\nword \u201cnegative\u201d to see which is higher:\nP(positivejThe sentiment of the sentence ``I like Jackie Chan\" is: )\nP(negativejThe sentiment of the sentence ``I like Jackie Chan\" is: )\nIf the word \u201cpositive\u201d is more probable, we say the sentiment of the sentence is\npositive, otherwise we say the sentiment is negative.\nWe can also cast more complex tasks as word prediction. Consider question\nanswering, in which the system is given a question (for example a question with\na simple factual answer) and must give a textual answer; we introduce this task in\ndetail in Chapter 14. We can cast the task of question answering as word prediction\nby giving a language model a question and a token like A:suggesting that an answer\nshould come next:\nQ: Who wrote the book ``The Origin of Species\"? A:\nIf we ask a language model to compute the probability distribution over possible\nnext words given this pre\ufb01x:\nP(wjQ: Who wrote the book ``The Origin of Species\"? A: )\nand look at which words whave high probabilities, we might expect to see that\nCharles is very likely, and then if we choose Charles and continue and ask\nP(wjQ: Who wrote the book ``The Origin of Species\"? A: Charles )\nwe might now see that Darwin is the most probable token, and select it.\nConditional generation can even be used to accomplish tasks that must generate\nlonger responses. Consider the task of text summarization , which is to take a longtext\nsummarization\ntext, such as a full-length article, and produce an effective shorter summary of it. We\ncan cast summarization as language modeling by giving a large language model a\ntext, and follow the text by a token like tl;dr ; this token is short for something like",
    "metadata": {
      "source": "10",
      "chunk_id": 2,
      "token_count": 512,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4",
    "metadata": {
      "source": "10",
      "chunk_id": 3,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "4CHAPTER 10 \u2022 L ARGE LANGUAGE MODELS\n\u2018too long; didn\u2019t read\u2019 and in recent years people often use this token, especially in\ninformal work emails, when they are going to give a short summary. Since this token\nis suf\ufb01ciently frequent in language model training data, language models have seen\nmany texts in which the token occurs before a summary, and hence will interpret the\ntoken as instructions to generate a summary. We can then do conditional generation:\ngive the language model this pre\ufb01x, and then have it generate the following words,\none by one, and take the entire response as a summary. Fig. 10.2 shows an example\nof a text and a human-produced summary from a widely-used summarization corpus\nconsisting of CNN and Daily Mail news articles.\nOriginal Article\nThe only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuff\nand offering it for sale online? People are actually buying it. For $89, self-styled entrepreneur\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box \u2013 enough\nfor 10 to 15 snowballs, he says.\nBut not if you live in New England or surrounding states. \u201cWe will not ship snow to any states\nin the northeast!\u201d says Waring\u2019s website, ShipSnowYo.com. \u201cWe\u2019re in the business of expunging\nsnow!\u201d\nHis website and social media accounts claim to have \ufb01lled more than 133 orders for snow \u2013 more\nthan 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set a\nrecord this winter for the snowiest month in its history. Most residents see the huge piles of snow\nchoking their yards and sidewalks as a nuisance, but Waring saw an opportunity.\nAccording to Boston.com, it all started a few weeks ago, when Waring and his wife were shov-\neling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston. He\njoked about shipping the stuff to friends and family in warmer states, and an idea was born. [...]\nSummary\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box \u2013 enough\nfor 10 to 15 snowballs, he says. But not if you live in New England or surrounding states.\nFigure 10.2 Excerpt from a sample article and its summary from the CNN/Daily Mail summarization corpus\n(Hermann et al., 2015), (Nallapati et al., 2016).\nIf we take this full article and append the token tl;dr , we can use this as the con-\ntext to prime the generation process to produce a summary as illustrated in Fig. 10.3.\nAgain, what makes transformers able to succeed at this task (as compared, say, to\nthe primitive n-gram language model) is that attention can incorporate information\nfrom the large context window, giving the model access to the original article as well\nas to the newly generated text throughout the process.\nWhich words do we generate at each step? One simple way to generate words\nis to always generate the most likely word given the context. Generating the most\nlikely word given the context is called greedy decoding . A greedy algorithm is onegreedy\ndecoding\nthat make a choice that is locally optimal, whether or not it will turn out to have\nbeen the best choice with hindsight. Thus in greedy decoding, at each time step in\ngeneration, the output ytis chosen by computing the probability for each possible\noutput (every word in the vocabulary) and then choosing the highest probability\nword (the argmax):",
    "metadata": {
      "source": "10",
      "chunk_id": 4,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 10.2 Excerpt from a sample article and its summary from the CNN/Daily Mail summarization corpus\n(Hermann et al., 2015), (Nallapati et al., 2016).\nIf we take this full article and append the token tl;dr , we can use this as the con-\ntext to prime the generation process to produce a summary as illustrated in Fig. 10.3.\nAgain, what makes transformers able to succeed at this task (as compared, say, to\nthe primitive n-gram language model) is that attention can incorporate information\nfrom the large context window, giving the model access to the original article as well\nas to the newly generated text throughout the process.\nWhich words do we generate at each step? One simple way to generate words\nis to always generate the most likely word given the context. Generating the most\nlikely word given the context is called greedy decoding . A greedy algorithm is onegreedy\ndecoding\nthat make a choice that is locally optimal, whether or not it will turn out to have\nbeen the best choice with hindsight. Thus in greedy decoding, at each time step in\ngeneration, the output ytis chosen by computing the probability for each possible\noutput (every word in the vocabulary) and then choosing the highest probability\nword (the argmax):\n\u02c6wt=argmaxw2VP(wjw<t) (10.1)\nIn practice, however, we don\u2019t use greedy decoding with large language models.\nA major problem with greedy decoding is that because the words it chooses are (by\nde\ufb01nition) extremely predictable, the resulting text is generic and often quite repeti-\ntive. Indeed, greedy decoding is so predictable that it is deterministic; if the context",
    "metadata": {
      "source": "10",
      "chunk_id": 5,
      "token_count": 365,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\n10.2 \u2022 S AMPLING FOR LLM G ENERATION 5\nOriginal StoryGenerated Summary\n\u2026ideaKyle\nwasborn.KyleWaring\nWaringonlyThe\u2026will\nDelimiterwillUUU\ntl;drLM Head\nE\nE\nE\nE\nE\nE\nE\nE\u2026\nFigure 10.3 Summarization with large language models using the tl;dr token and context-based autore-\ngressive generation.\nis identical, and the probabilistic model is the same, greedy decoding will always re-\nsult in generating exactly the same string. We\u2019ll see in Chapter 13 that an extension\nto greedy decoding called beam search works well in tasks like machine translation,\nwhich are very constrained in that we are always generating a text in one language\nconditioned on a very speci\ufb01c text in another language. In most other tasks, how-\never, people prefer text which has been generated by more sophisticated methods,\ncalled sampling methods , that introduce a bit more diversity into the generations.\nWe\u2019ll see how to do that in the next few sections.\n10.2 Sampling for LLM Generation\nThe core of the generation process for large language models is the task of choosing\nthe single word to generate next based on the context and based on the probabilities\nthat the model assigns to possible words. This task of choosing a word to generate\nbased on the model\u2019s probabilities is called decoding . Decoding from a language decoding\nmodel in a left-to-right manner (or right-to-left for languages like Arabic in which\nwe read from right to left), and thus repeatedly choosing the next word conditioned\non our previous choices is called autoregressive generation orcausal LM genera-autoregressive\ngeneration\ntion.1(As we\u2019ll see, alternatives like the masked language models of Chapter 11 are\nnon-causal because they can predict words based on both past and future words).\nThe most common method for decoding in large language models is sampling .\nRecall from Chapter 3 that sampling from a model\u2019s distribution over words means sampling\nto choose random words according to their probability assigned by the model. That\nis, we iteratively choose a word to generate according to its probability in context\n1Technically an autoregressive model predicts a value at time tbased on a linear function of the values\nat times t\u00001,t\u00002, and so on. Although language models are not linear (since they have many layers of\nnon-linearities), we loosely refer to this generation technique as autoregressive since the word generated\nat each time step is conditioned on the word selected by the network from the previous step.",
    "metadata": {
      "source": "10",
      "chunk_id": 6,
      "token_count": 550,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6CHAPTER 10 \u2022 L ARGE LANGUAGE MODELS\nas de\ufb01ned by the model. Thus we are more likely to generate words that the model\nthinks have a high probability in the context and less likely to generate words that\nthe model thinks have a low probability.\nWe saw back in Chapter 3 on page ??how to generate text from a unigram lan-\nguage model , by repeatedly randomly sampling words according to their probability\nuntil we either reach a pre-determined length or select the end-of-sentence token. To\ngenerate text from a trained transformer language model we\u2019ll just generalize this\nmodel a bit: at each step we\u2019ll sample words according to their probability condi-\ntioned on our previous choices , and we\u2019ll use a transformer language model as the\nprobability model that tells us this probability.\nWe can formalize this algorithm for generating a sequence of words W=w1;w2;:::; wN\nuntil we hit the end-of-sequence token, using x\u0018p(x)to mean \u2018choose xby sam-\npling from the distribution p(x):\ni 1\nwi\u0018p(w)\nwhile wi!= EOS\ni i + 1\nwi\u0018p(wijw<i)\nThe algorithm above is called random sampling , and it turns out random sam-random\nsampling\npling doesn\u2019t work well enough. The problem is that even though random sampling\nis mostly going to generate sensible, high-probable words, there are many odd, low-\nprobability words in the tail of the distribution, and even though each one is low-\nprobability, if you add up all the rare words, they constitute a large enough portion\nof the distribution that they get chosen often enough to result in generating weird\nsentences. For this reason, instead of random sampling, we usually use sampling\nmethods that avoid generating the very unlikely words.\nThe sampling methods we introduce below each have parameters that enable\ntrading off two important factors in generation: quality anddiversity . Methods\nthat emphasize the most probable words tend to produce generations that are rated\nby people as more accurate, more coherent, and more factual, but also more boring\nand more repetitive. Methods that give a bit more weight to the middle-probability\nwords tend to be more creative and more diverse, but less factual and more likely to\nbe incoherent or otherwise low-quality.\n10.2.1 Top- ksampling\nTop-k sampling is a simple generalization of greedy decoding. Instead of choosing top-k sampling\nthe single most probable word to generate, we \ufb01rst truncate the distribution to the\ntopkmost likely words, renormalize to produce a legitimate probability distribution,\nand then randomly sample from within these kwords according to their renormalized\nprobabilities. More formally:\n1. Choose in advance a number of words k\n2. For each word in the vocabulary V, use the language model to compute the\nlikelihood of this word given the context p(wtjw<t)\n3. Sort the words by their likelihood, and throw away any word that is not one of\nthe top kmost probable words.\n4. Renormalize the scores of the kwords to be a legitimate probability distribu-\ntion.",
    "metadata": {
      "source": "10",
      "chunk_id": 7,
      "token_count": 669,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\n10.2 \u2022 S AMPLING FOR LLM G ENERATION 7\n5. Randomly sample a word from within these remaining kmost-probable words\naccording to its probability.\nWhen k=1, top- ksampling is identical to greedy decoding. Setting kto a larger\nnumber than 1 leads us to sometimes select a word which is not necessarily the most\nprobable, but is still probable enough, and whose choice results in generating more\ndiverse but still high-enough-quality text.\n10.2.2 Nucleus or top- psampling\nOne problem with top- ksampling is that kis \ufb01xed, but the shape of the probability\ndistribution over words differs in different contexts. If we set k=10, sometimes\nthe top 10 words will be very likely and include most of the probability mass, but\nother times the probability distribution will be \ufb02atter and the top 10 words will only\ninclude a small part of the probability mass.\nAn alternative, called top-p sampling ornucleus sampling (Holtzman et al., top-p sampling\n2020), is to keep not the top kwords, but the top ppercent of the probability mass.\nThe goal is the same; to truncate the distribution to remove the very unlikely words.\nBut by measuring probability rather than the number of words, the hope is that the\nmeasure will be more robust in very different contexts, dynamically increasing and\ndecreasing the pool of word candidates.\nGiven a distribution P(wtjw<t), we sort the distribution from most probable, and\nthen the top- pvocabulary V(p)is the smallest set of words such that\nX\nw2V(p)P(wjw<t)\u0015p: (10.2)\n10.2.3 Temperature sampling\nIntemperature sampling , we don\u2019t truncate the distribution, but instead reshapetemperature\nsampling\nit. The intuition for temperature sampling comes from thermodynamics, where a\nsystem at a high temperature is very \ufb02exible and can explore many possible states,\nwhile a system at a lower temperature is likely to explore a subset of lower energy\n(better) states. In low-temperature sampling, we smoothly increase the probability\nof the most probable words and decrease the probability of the rare words.\nWe implement this intuition by simply dividing the logit by a temperature param-\netertbefore we normalize it by passing it through the softmax. In low-temperature\nsampling, t2(0;1]. Thus instead of computing the probability distribution over the\nvocabulary directly from the logit as in the following (repeated from Eq. ??):\ny=softmax (u) (10.3)\nwe instead \ufb01rst divide the logits by t, computing the probability vector yas\ny=softmax (u=t) (10.4)\nWhy does this work? When tis close to 1 the distribution doesn\u2019t change much.\nBut the lower tis, the larger the scores being passed to the softmax (dividing by a\nsmaller fraction t\u00141 results in making each score larger). Recall that one of the\nuseful properties of a softmax is that it tends to push high values toward 1 and low\nvalues toward 0. Thus when larger numbers are passed to a softmax the result is\na distribution with increased probabilities of the most high-probability words and\ndecreased probabilities of the low probability words, making the distribution more\ngreedy. As tapproaches 0 the probability of the most likely word approaches 1.",
    "metadata": {
      "source": "10",
      "chunk_id": 8,
      "token_count": 741,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8CHAPTER 10 \u2022 L ARGE LANGUAGE MODELS\nNote, by the way, that there can be other situations where we may want to do\nsomething quite different and \ufb02atten the word probability distribution instead of\nmaking it greedy. Temperature sampling can help with this situation too, in this case\nhigh-temperature sampling, in which case we use t>1.\n10.3 Pretraining Large Language Models\nHow do we teach a transformer to be a language model? What is the algorithm and\nwhat data do we train on?\n10.3.1 Self-supervised training algorithm\nTo train a transformer as a language model, we use the same self-supervision (or self-supervision\nself-training ) algorithm we saw in Section ??: we take a corpus of text as training\nmaterial and at each time step task the model to predict the next word. We call\nsuch a model self-supervised because we don\u2019t have to add any special gold labels\nto the data; the natural sequence of words is its own supervision! We simply train the\nmodel to minimize the error in predicting the true next word in the training sequence,\nusing cross-entropy as the loss function.\nRecall that the cross-entropy loss measures the difference between a predicted\nprobability distribution and the correct distribution.\nLCE=\u0000X\nw2Vyt[w]log\u02c6yt[w] (10.5)\nIn the case of language modeling, the correct distribution ytcomes from knowing the\nnext word. This is represented as a one-hot vector corresponding to the vocabulary\nwhere the entry for the actual next word is 1, and all the other entries are 0. Thus,\nthe cross-entropy loss for language modeling is determined by the probability the\nmodel assigns to the correct next word (all other words get multiplied by zero). So\nat time tthe CE loss in Eq. 10.5 can be simpli\ufb01ed as the negative log probability the\nmodel assigns to the next word in the training sequence.\nLCE(\u02c6yt;yt) =\u0000log\u02c6yt[wt+1] (10.6)\nThus at each word position tof the input, the model takes as input the correct se-\nquence of tokens w1:t, and uses them to compute a probability distribution over\npossible next words so as to compute the model\u2019s loss for the next token wt+1. Then\nwe move to the next word, we ignore what the model predicted for the next word\nand instead use the correct sequence of tokens w1:t+1to estimate the probability of\ntoken wt+2. This idea that we always give the model the correct history sequence to\npredict the next word (rather than feeding the model its best case from the previous\ntime step) is called teacher forcing . teacher forcing\nFig. 10.4 illustrates the general training approach. At each step, given all the\npreceding words, the \ufb01nal transformer layer produces an output distribution over\nthe entire vocabulary. During training, the probability assigned to the correct word\nis used to calculate the cross-entropy loss for each item in the sequence. The loss\nfor a training sequence is the average cross-entropy loss over the entire sequence.\nThe weights in the network are adjusted to minimize the average CE loss over the\ntraining sequence via gradient descent.",
    "metadata": {
      "source": "10",
      "chunk_id": 9,
      "token_count": 695,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9",
    "metadata": {
      "source": "10",
      "chunk_id": 10,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "10.3 \u2022 P RETRAINING LARGE LANGUAGE MODELS 9\nlongandthanksforNext tokenallLoss\u2026=\n<latexit sha1_base64=\"AovqpaL476UmJ1EU1xZPgDZ70tQ=\">AAAB9nicbVDLSsNAFL2pr1pfURcu3AwWwY0lEakui25cVrAPaEqYTCbt0EkmzEzEEvIrbkTcKPgZ/oJ/Y9Jm09YDA4dzznDvPV7MmdKW9WtU1tY3Nreq27Wd3b39A/PwqKtEIgntEMGF7HtYUc4i2tFMc9qPJcWhx2nPm9wXfu+ZSsVE9KSnMR2GeBSxgBGsc8k1Ty4dLkZo6qZOiPVYhimO/CyruWbdalgzoFVil6QOJdqu+eP4giQhjTThWKmBbcV6mGKpGeE0qzmJojEmEzyi6WztDJ3nko8CIfMXaTRTF3I4VGoaenmy2E0te4X4nzdIdHA7TFkUJ5pGZD4oSDjSAhUdIJ9JSjSf5gQTyfINERljiYnOmypOt5cPXSXdq4bdbDQfr+utu7KEKpzCGVyADTfQggdoQwcIZPAGn/BlvBivxrvxMY9WjPLPMSzA+P4DPEiSHA==</latexit>\u0000logyand\nStackedTransformerBlocksSolongandthanksfor\u2026\u2026\n\u2026\nU\nInput tokensx1x2LanguageModelingHeadx3x4x5InputEncoding\nE1+\nE2+\nE3+\nE4+\nE5+\u2026\u2026\u2026\u2026\u2026\nU\nU\nU\nU\u2026logitslogitslogitslogitslogits\u2026",
    "metadata": {
      "source": "10",
      "chunk_id": 11,
      "token_count": 464,
      "chapter_title": ""
    }
  },
  {
    "content": "StackedTransformerBlocksSolongandthanksfor\u2026\u2026\n\u2026\nU\nInput tokensx1x2LanguageModelingHeadx3x4x5InputEncoding\nE1+\nE2+\nE3+\nE4+\nE5+\u2026\u2026\u2026\u2026\u2026\nU\nU\nU\nU\u2026logitslogitslogitslogitslogits\u2026\n<latexit sha1_base64=\"q3ZgXDyG7qtkT7t8hT47RdlwYG4=\">AAAB+XicbVDLSsNAFJ3UV62vWHe6GVsEN5bERXUlBUVcVrAPaEqYTCft0MlMmJkIIQT8AT/CTRE3Cv6Ev+DfmLTdtPXAwOGcM9x7jxcyqrRl/RqFtfWNza3idmlnd2//wDwst5WIJCYtLJiQXQ8pwignLU01I91QEhR4jHS88W3ud56JVFTwJx2HpB+gIac+xUhnkmseXzhMDGHsJk6A9EgGiR4hPlZpWnLNqlWzpoCrxJ6TauP0tXw3qdw0XfPHGQgcBYRrzJBSPdsKdT9BUlPMSFpyIkVChMdoSJLp5ik8y6QB9IXMHtdwqi7kUKBUHHhZMl9PLXu5+J/Xi7R/3U8oDyNNOJ4N8iMGtYB5DXBAJcGaxRlBWNJsQ4hHSCKss7Ly0+3lQ1dJ+7Jm12v1x6yDezBDEZyACjgHNrgCDfAAmqAFMHgBE/AJvozEeDPejY9ZtGDM/xyBBRjff79pldo=</latexit>\u0000logythanks\nFigure 10.4 Training a transformer as a language model.\nNote the key difference between this \ufb01gure and the earlier RNN-based version\nshown in Fig. ??. There the calculation of the outputs and the losses at each step was\ninherently serial given the recurrence in the calculation of the hidden states. With\ntransformers, each training item can be processed in parallel since the output for\neach element in the sequence is computed separately.\nLarge models are generally trained by \ufb01lling the full context window (for exam-\nple 4096 tokens for GPT4 or 8192 for Llama 3) with text. If documents are shorter\nthan this, multiple documents are packed into the window with a special end-of-text\ntoken between them. The batch size for gradient descent is usually quite large (the\nlargest GPT-3 model uses a batch size of 3.2 million tokens).\n10.3.2 Training corpora for large language models\nLarge language models are mainly trained on text scraped from the web, augmented\nby more carefully curated data. Because these training corpora are so large, they are\nlikely to contain many natural examples that can be helpful for NLP tasks, such as\nquestion and answer pairs (for example from FAQ lists), translations of sentences\nbetween various languages, documents together with their summaries, and so on.\nWeb text is usually taken from corpora of automatically-crawled web pages like\nthecommon crawl , a series of snapshots of the entire web produced by the non- common crawl",
    "metadata": {
      "source": "10",
      "chunk_id": 12,
      "token_count": 769,
      "chapter_title": ""
    }
  },
  {
    "content": "shown in Fig. ??. There the calculation of the outputs and the losses at each step was\ninherently serial given the recurrence in the calculation of the hidden states. With\ntransformers, each training item can be processed in parallel since the output for\neach element in the sequence is computed separately.\nLarge models are generally trained by \ufb01lling the full context window (for exam-\nple 4096 tokens for GPT4 or 8192 for Llama 3) with text. If documents are shorter\nthan this, multiple documents are packed into the window with a special end-of-text\ntoken between them. The batch size for gradient descent is usually quite large (the\nlargest GPT-3 model uses a batch size of 3.2 million tokens).\n10.3.2 Training corpora for large language models\nLarge language models are mainly trained on text scraped from the web, augmented\nby more carefully curated data. Because these training corpora are so large, they are\nlikely to contain many natural examples that can be helpful for NLP tasks, such as\nquestion and answer pairs (for example from FAQ lists), translations of sentences\nbetween various languages, documents together with their summaries, and so on.\nWeb text is usually taken from corpora of automatically-crawled web pages like\nthecommon crawl , a series of snapshots of the entire web produced by the non- common crawl\npro\ufb01t Common Crawl ( https://commoncrawl.org/ ) that each have billions of\nwebpages. Various versions of common crawl data exist, such as the Colossal Clean\nCrawled Corpus (C4; Raffel et al. 2020), a corpus of 156 billion tokens of English\nthat is \ufb01ltered in various ways (deduplicated, removing non-natural language like\ncode, sentences with offensive words from a blocklist). This C4 corpus seems to\nconsist in large part of patent text documents, Wikipedia, and news sites (Dodge\net al., 2021).\nWikipedia plays a role in lots of language model training, as do corpora of books.\nThe Pile (Gao et al., 2020) is an 825 GB English text corpus that is constructed by The Pile\npublicly released code, containing again a large amount of text scraped from the web",
    "metadata": {
      "source": "10",
      "chunk_id": 13,
      "token_count": 486,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10",
    "metadata": {
      "source": "10",
      "chunk_id": 14,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "10 CHAPTER 10 \u2022 L ARGE LANGUAGE MODELS\nas well as books and Wikipedia; Fig. 10.5 shows its composition. Dolma is a larger\nopen corpus of English, created with public tools, containing three trillion tokens,\nwhich similarly consists of web text, academic papers, code, books, encyclopedic\nmaterials, and social media (Soldaini et al., 2024).\nFigure 1: Treemap of Pile components by effective size.\ntroduce a new \ufb01ltered subset of Common Crawl,\nPile-CC, with improved extraction quality.\nThrough our analyses, we con\ufb01rm that the Pile is\nsigni\ufb01cantly distinct from pure Common Crawl\ndata. Additionally, our evaluations show that the\nexisting GPT-2 and GPT-3 models perform poorly\non many components of the Pile, and that models\ntrained on the Pile signi\ufb01cantly outperform both\nraw and \ufb01ltered Common Crawl models. To com-\nplement the performance evaluations, we also per-\nform an exploratory analysis of the text within the\nPile to provide a detailed picture of the data. We\nhope that our extensive documentation of the con-\nstruction and characteristics of the Pile will help\nresearchers make informed decisions about poten-\ntial downstream applications.\nFinally, we make publicly available the preprocess-\ning code for the constituent datasets of the Pile and\nthe code for constructing alternative versions2. In\nthe interest of reproducibility, we also document\nall processing performed on each dataset (and the\nPile as a whole) in as much detail as possible. For\nfurther details about the processing of each dataset,\nsee Section 2and Appendix C.\n2https://github.com/EleutherAI/\nthe-pile1.1 Contributions\nThe core contributions of this paper are:\n1.The introduction of a 825.18GiB english-\nlanguage dataset for language modeling com-\nbining 22 diverse sources.\n2.The introduction of 14new language model-\ning datasets, which we expect to be of inde-\npendent interest to researchers.\n3.Evaluations demonstrating signi\ufb01cant im-\nprovements across many domains by GPT-2-\nsized models trained on this new dataset, com-\npared to training on CC-100 and raw Common\nCrawl.\n4.The investigation and documentation of this\ndataset, which we hope will better inform re-\nsearchers about how to use it as well as moti-\nvate them to undertake similar investigations\nof their own data.\n2 The Pile Datasets\nThe Pile is composed of 22 constituent sub-datasets,\nas shown in Table 1. Following Brown et al. (2020 ),\nwe increase the weights of higher quality compo-\nnents, with certain high-quality datasets such as\nWikipedia being seen up to 3 times (\u201cepochs\u201d) for\n2\nFigure 10.5 The Pile corpus, showing the size of different components, color coded as\nacademic (articles from PubMed and ArXiv, patents from the USPTA; internet (webtext in-\ncluding a subset of the common crawl as well as Wikipedia), prose (a large corpus of books),\ndialogue (including movie subtitles and chat data), and misc.. Figure from Gao et al. (2020).\nFiltering for quality and safety Pretraining data drawn from the web is \ufb01ltered\nfor both quality andsafety . Quality \ufb01lters are classi\ufb01ers that assign a score to each\ndocument. Quality is of course subjective, so different quality \ufb01lters are trained",
    "metadata": {
      "source": "10",
      "chunk_id": 15,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "pared to training on CC-100 and raw Common\nCrawl.\n4.The investigation and documentation of this\ndataset, which we hope will better inform re-\nsearchers about how to use it as well as moti-\nvate them to undertake similar investigations\nof their own data.\n2 The Pile Datasets\nThe Pile is composed of 22 constituent sub-datasets,\nas shown in Table 1. Following Brown et al. (2020 ),\nwe increase the weights of higher quality compo-\nnents, with certain high-quality datasets such as\nWikipedia being seen up to 3 times (\u201cepochs\u201d) for\n2\nFigure 10.5 The Pile corpus, showing the size of different components, color coded as\nacademic (articles from PubMed and ArXiv, patents from the USPTA; internet (webtext in-\ncluding a subset of the common crawl as well as Wikipedia), prose (a large corpus of books),\ndialogue (including movie subtitles and chat data), and misc.. Figure from Gao et al. (2020).\nFiltering for quality and safety Pretraining data drawn from the web is \ufb01ltered\nfor both quality andsafety . Quality \ufb01lters are classi\ufb01ers that assign a score to each\ndocument. Quality is of course subjective, so different quality \ufb01lters are trained\nin different ways, but often to value high-quality reference corpora like Wikipedia,\nbooks, and particular websites and to avoid websites with lots of PII(Personal Iden- PII\nti\ufb01able Information) or adult content. Filters also remove boilerplate text which is\nvery frequent on the web. Another kind of quality \ufb01ltering is deduplication, which\ncan be done at various levels, so as to remove duplicate documents, duplicate web\npages, or duplicate text. Quality \ufb01ltering generally improves language model per-\nformance (Longpre et al., 2024b; Llama Team, 2024).\nSafety \ufb01ltering is again a subjective decision, and often includes toxicity detec-\ntion based on running off-the-shelf toxicity classi\ufb01ers. This can have mixed results.\nOne problem is that current toxicity classi\ufb01ers mistakenly \ufb02ag non-toxic data if it\nis generated by speakers of minority dialects like African American English (Xu\net al., 2021). Another problem is that models trained on toxicity-\ufb01ltered data, while\nsomewhat less toxic, are also worse at detecting toxicity themselves (Longpre et al.,\n2024b). These issues make the question of how to do better safety \ufb01ltering an im-\nportant open problem.\nUsing large datasets scraped from the web to train language models poses ethical\nand legal questions:\nCopyright: Much of the text in these large datasets (like the collections of \ufb01c-\ntion and non-\ufb01ction books) is copyrighted. In some countries, like the United\nStates, the fair use doctrine may allow copyrighted content to be used for\ntransformative uses, but it\u2019s not clear if that remains true if the language mod-\nels are used to generate text that competes with the market for the text they",
    "metadata": {
      "source": "10",
      "chunk_id": 16,
      "token_count": 666,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\n10.3 \u2022 P RETRAINING LARGE LANGUAGE MODELS 11\nare trained on (Henderson et al., 2023).\nData consent: Owners of websites can indicate that they don\u2019t want their sites\nto be crawled by web crawlers (either via a robots.txt \ufb01le, or via Terms of\nService). Recently there has been a sharp increase in the number of web-\nsites that have indicated that they don\u2019t want large language model builders\ncrawling their sites for training data (Longpre et al., 2024a). Because it\u2019s not\nclear what legal status these indications have in different countries, or whether\nthese restrictions are retroactive, what effect this will have on large pretraining\ndatasets is unclear.\nPrivacy: Large web datasets also have privacy issues since they contain private\ninformation like phone numbers and IP addresses. While \ufb01lters are used to try\nto remove websites likely to contain large amounts of personal information,\nsuch \ufb01ltering isn\u2019t suf\ufb01cient.\n10.3.3 Finetuning\nAlthough the enormous pretraining data for a large language model includes text\nfrom many domains, it\u2019s often the case that we want to apply it in a new domain or\ntask that might not have appeared suf\ufb01ciently in the pre-training data. For example,\nwe might want a language model that\u2019s specialized to legal or medical text. Or we\nmight have a multilingual language model that knows many languages but might\nbene\ufb01t from some more data in our particular language of interest. Or we want a\nlanguage model that is specialized to a particular task.\nIn such cases, we can simply continue training the model on relevant data from\nthe new domain or language (Gururangan et al., 2020). This process of taking a fully\npretrained model and running additional training passes on some new data is called\n\ufb01netuning . Fig. 10.6 sketches the paradigm. \ufb01netuning\nFine-tuning Data\nPretraining DataPretraining\n\u2026\n\u2026\n\u2026Fine-tuning\n\u2026\n\u2026\n\u2026Pretrained LMFine-tuned LM\nFigure 10.6 Pretraining and \ufb01netuning. A pre-trained model can be \ufb01netuned to a par-\nticular domain, dataset, or task. There are many different ways to \ufb01netune, depending on\nexactly which parameters are updated from the \ufb01netuning data: all the parameters, some of\nthe parameters, or only the parameters of speci\ufb01c extra circuitry.\nWe\u2019ll introduce four related kinds of \ufb01netuning in this chapter and the two fol-\nlowing chapters. In all four cases, \ufb01netuning means the process of taking a pre-\ntrained model and further adapting some or all of its parameters to some new data.\nBut they differ on exactly which parameters get updated.\nIn the \ufb01rst kind of \ufb01netuning we retrain all the parameters of the model on this\nnew data, using the same method (word prediction) and loss function (cross-entropy\nloss) as for pretraining. In a sense it\u2019s as if the new data were at the tail end of",
    "metadata": {
      "source": "10",
      "chunk_id": 17,
      "token_count": 666,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 CHAPTER 10 \u2022 L ARGE LANGUAGE MODELS\nthe pretraining data, and so you\u2019ll sometimes see this method called continued pre-\ntraining .continued\npretraining\nRetraining all the parameters of the model is very slow and expensive when the\nlanguage model is huge. So instead we can freeze some of the parameters (i.e., leave freeze\nthem unchanged from their pretrained value) and train only a subset of parameters\non the new data. In Section 10.5.3 we\u2019ll describe this second variety of \ufb01netun-\ning, called parameter-ef\ufb01cient \ufb01netuning , orPEFT . because we ef\ufb01ciently select\nspeci\ufb01c parameters to update when \ufb01netuning, and leave the rest in their pretrained\nvalues.\nIn Chapter 11 we\u2019ll introduce a third kind of \ufb01netuning, also parameter-ef\ufb01cient.\nIn this version, the goal is to use a language model as a kind of classi\ufb01er or labeler\nfor a speci\ufb01c task. For example we might train the model to be a sentiment classi\ufb01er.\nWe do this by adding extra neural circuitry (an extra head ) after the top layer of the\nmodel. This classi\ufb01cation head takes as input some of the top layer embeddings of\nthe transformer and produces as output a classi\ufb01cation. In this method, most com-\nmonly used with masked language models like BERT, we freeze the entire pretrained\nmodel and only train the classi\ufb01cation head on some new data, usually labeled with\nsome class that we want to predict.\nFinally, in Chapter 12 we\u2019ll introduce a fourth kind of \ufb01netuning, that is a cru-\ncial component of the largest language models: supervised \ufb01netuning orSFT. SFT\nis often used for instruction \ufb01netuning , in which we want a pretrained language\nmodel to learn to follow text instructions, for example to answer questions or follow\na command to write something. Here we create a dataset of prompts and desired\nresponses (for example questions and their answers, or commands and their ful-\n\ufb01llments), and we train the language model using the normal cross-entropy loss to\npredict each token in the instruction prompt iteratively, essentially training it to pro-\nduce the desired response from the command in the prompt. It\u2019s called supervised\nbecause unlike in pretraining, where we just take any data and predict the words in\nit, we build the special \ufb01netuning dataset by hand, creating supervised responses to\neach command.\nOften everything that happens after pretraining is lumped together as post-training ;\nwe\u2019ll discuss the various parts of post-training in Chapter 12.\n10.4 Evaluating Large Language Models\nPerplexity As we \ufb01rst saw in Chapter 3, one way to evaluate language models is\nto measure how well they predict unseen text. Intuitively, good models are those that\nassign higher probabilities to unseen data (are less surprised when encountering the\nnew words).\nWe instantiate this intuition by using perplexity to measure the quality of a perplexity\nlanguage model. Recall from page ??that the perplexity of a model qon an unseen\ntest set is the inverse probability that qassigns to the test set, normalized by the test\nset length. For a test set of ntokens w1:n, the perplexity is\nPerplexityq(w1:n) = Pq(w1:n)\u00001\nn\n=ns\n1\nPq(w1:n)(10.7)\nTo visualize how perplexity can be computed as a function of the probabilities the",
    "metadata": {
      "source": "10",
      "chunk_id": 18,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\n10.4 \u2022 E VALUATING LARGE LANGUAGE MODELS 13\nLM computes for each new word, we can use the chain rule to expand the computa-\ntion of probability of the test set:\nPerplexityq(w1:n) =nvuutnY\ni=11\nPq(wijw<i)(10.8)\nNote that because of the inverse in Eq. 10.7, the higher the probability of the word\nsequence, the lower the perplexity. Thus the the lower the perplexity of a model on\nthe data, the better the model . Minimizing perplexity is equivalent to maximizing\nthe test set probability according to the language model.\nOne caveat: because perplexity depends on the length of a text, it is very sensitive\nto differences in the tokenization algorithm. That means that it\u2019s hard to exactly\ncompare perplexities produced by two language models if they have very different\ntokenizers. For this reason perplexity is best used when comparing language models\nthat use the same tokenizer.\nOther factors While the predictive accuracy of a language model, as measured by\nperplexity, is a very useful metric, we also care about different kinds of accuracy, for\nthe downstream tasks we apply our language model to. For each task like machine\ntranslation, summarization, question answering, speech recognition, and dialogue,\nwe can measure the accuracy at those tasks. Future chapters will introduce task-\nspeci\ufb01c metrics that allow us to evaluate how accuracy or correct language models\nare at these downstream tasks.\nBut when evaluating models we also care about factors besides any of these\nkinds of accuracy (Dodge et al., 2019; Ethayarajh and Jurafsky, 2020). For example,\nwe often care about how a big a model is, and how long it takes to train or do\ninference. This can matter because we have constraints on time either for training\nor at inference. Or we may have constraints on memory, since the GPUs we run\nour models on have \ufb01xed memory sizes. Big models also use more energy, and we\nprefer models that use less energy, both to reduce the environmental impact of the\nmodel and to reduce the \ufb01nancial cost of building or deploying it. We can target\nour evaluation to these factors by measuring performance normalized to a giving\ncompute or memory budget. We can also directly measure the energy usage of our\nmodel in kWh or in kilograms of CO 2emitted (Strubell et al., 2019; Henderson\net al., 2020; Liang et al., 2023).\nAnother feature that a language model evaluation can measure is fairness. We\nknow that language models are biased, exhibiting gendered and racial stereotypes,\nor decreased performance for language from or about certain demographics groups.\nThere are language model evaluation benchmarks that measure the strength of these\nbiases, such as StereoSet (Nadeem et al., 2021), RealToxicityPrompts (Gehman\net al., 2020), and BBQ (Parrish et al., 2022) among many others. We also want\nlanguage models whose performance is equally fair to different groups. For exam-\nple, we could chose an evaluation that is fair in a Rawlsian sense by maximizing the\nwelfare of the worst-off group (Rawls, 2001; Hashimoto et al., 2018; Sagawa et al.,\n2020).\nFinally, there are many kinds of leaderboards like Dynabench (Kiela et al., 2021)\nand general evaluation protocols like HELM (Liang et al., 2023); we will return to\nthese in later chapters when we introduce evaluation metrics for speci\ufb01c tasks like\nquestion answering and information retrieval.",
    "metadata": {
      "source": "10",
      "chunk_id": 19,
      "token_count": 794,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n14 CHAPTER 10 \u2022 L ARGE LANGUAGE MODELS\n10.5 Dealing with Scale\nLarge language models are large. For example the Llama 3.1 405B Instruct model\nfrom Meta has 405 billion parameters ( L=126 layers, a model dimensionality of\nd=16,384, A=128 attention heads) and was trained on 15.6 terabytes of text tokens\n(Llama Team, 2024), using a vocabulary of 128K tokens. So there is a lot of research\non understanding how LLMs scale, and especially how to implement them given\nlimited resources. In the next few sections we discuss how to think about scale (the\nconcept of scaling laws ), and important techniques for getting language models to\nwork ef\ufb01ciently, such as the KV cache and parameter-ef\ufb01cient \ufb01ne tuning.\n10.5.1 Scaling laws\nThe performance of large language models has shown to be mainly determined by\n3 factors: model size (the number of parameters not counting embeddings), dataset\nsize (the amount of training data), and the amount of compute used for training. That\nis, we can improve a model by adding parameters (adding more layers or having\nwider contexts or both), by training on more data, or by training for more iterations.\nThe relationships between these factors and performance are known as scaling\nlaws . Roughly speaking, the performance of a large language model (the loss) scales scaling laws\nas a power-law with each of these three properties of model training.\nFor example, Kaplan et al. (2020) found the following three relationships for\nlossLas a function of the number of non-embedding parameters N, the dataset size\nD, and the compute budget C, for models training with limited parameters, dataset,\nor compute budget, if in each case the other two properties are held constant:\nL(N) =\u0012Nc\nN\u0013aN\n(10.9)\nL(D) =\u0012Dc\nD\u0013aD\n(10.10)\nL(C) =\u0012Cc\nC\u0013aC\n(10.11)\nThe number of (non-embedding) parameters Ncan be roughly computed as fol-\nlows (ignoring biases, and with das the input and output dimensionality of the\nmodel, dattnas the self-attention layer size, and dffthe size of the feedforward layer):\nN\u00192d nlayer(2dattn+dff)\n\u001912nlayerd2(10.12)\n(assuming dattn=dff=4=d)\nThus GPT-3, with n=96 layers and dimensionality d=12288, has 12\u000296\u0002\n122882\u0019175 billion parameters.\nThe values of Nc,Dc,Cc,aN,aD, and aCdepend on the exact transformer\narchitecture, tokenization, and vocabulary size, so rather than all the precise values,\nscaling laws focus on the relationship with loss.2\nScaling laws can be useful in deciding how to train a model to a particular per-\nformance, for example by looking at early in the training curve, or performance with\n2For the initial experiment in Kaplan et al. (2020) the precise values were aN= 0.076, Nc= 8.8\u00021013\n(parameters), aD= 0.095, Dc= 5.4\u00021013(tokens), aC= 0.050, Cc= 3.1\u0002108(peta\ufb02op-days).",
    "metadata": {
      "source": "10",
      "chunk_id": 20,
      "token_count": 745,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\n10.5 \u2022 D EALING WITH SCALE 15\nsmaller amounts of data, to predict what the loss would be if we were to add more\ndata or increase model size. Other aspects of scaling laws can also tell us how much\ndata we need to add when scaling up a model.\n10.5.2 KV Cache\nWe saw in Fig. ??and in Eq. ??(repeated below) how the attention vector can be\nvery ef\ufb01ciently computed in parallel for training, via two matrix multiplications:\nA=softmax\u0012QK|\npdk\u0013\nV (10.13)\nUnfortunately we can\u2019t do quite the same ef\ufb01cient computation in inference as\nin training. That\u2019s because at inference time, we iteratively generate the next tokens\none at a time. For a new token that we have just generated, call it xi, we need to\ncompute its query, key, and values by multiplying by WQ,WK, and WVrespec-\ntively. But it would be a waste of computation time to recompute the key and value\nvectors for all the prior tokens x<i; at prior steps we already computed these key\nand value vectors! So instead of recomputing these, whenever we compute the key\nand value vectors we store them in memory in the KV cache , and then we can just KV cache\ngrab them from the cache when we need them. Fig. 10.7 modi\ufb01es Fig. ??to show\nthe computation that takes place for a single new token, showing which values we\ncan take from the cache rather than recompute.\nq4k1k2k4QKTQKTv1v2v3v4V\nq4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4x==xa4A\n1 x dkdk x N1 x NN x dv1 x dv\nk3\nFigure 10.7 Parts of the attention computation (extracted from Fig. ??) showing, in black,\nthe vectors that can be stored in the cache rather than recomputed when computing the atten-\ntion score for the 4th token.\n10.5.3 Parameter Ef\ufb01cient Fine Tuning\nAs we mentioned above, it\u2019s very common to take a language model and give it more\ninformation about a new domain by \ufb01netuning it (continuing to train it to predict\nupcoming words) on some additional data.\nFine-tuning can be very dif\ufb01cult with very large language models, because there\nare enormous numbers of parameters to train; each pass of batch gradient descent\nhas to backpropagate through many many huge layers. This makes \ufb01netuning huge\nlanguage models extremely expensive in processing power, in memory, and in time.\nFor this reason, there are alternative methods that allow a model to be \ufb01netuned\nwithout changing all the parameters. Such methods are called parameter-ef\ufb01cient\n\ufb01ne tuning or sometimes PEFT , because we ef\ufb01ciently select a subset of parametersparameter-\nef\ufb01cient \ufb01ne\ntuning\nPEFT to update when \ufb01netuning. For example we freeze some of the parameters (don\u2019t\nchange them), and only update some particular subset of parameters.",
    "metadata": {
      "source": "10",
      "chunk_id": 21,
      "token_count": 694,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\n16 CHAPTER 10 \u2022 L ARGE LANGUAGE MODELS\nHere we describe one such model, called LoRA , forLow-RankAdaptation. The LoRA\nintuition of LoRA is that transformers have many dense layers which perform matrix\nmultiplication (for example the WQ,WK,WV,WOlayers in the attention computa-\ntion). Instead of updating these layers during \ufb01netuning, with LoRA we freeze these\nlayers and instead update a low-rank approximation that has fewer parameters.\nConsider a matrix Wof dimensionality [N\u0002d]that needs to be updated during\n\ufb01netuning via gradient descent. Normally this matrix would get updates DWof\ndimensionality [N\u0002d], for updating the N\u0002dparameters after gradient descent. In\nLoRA, we freeze Wand update instead a low-rank decomposition of W. We create\ntwo matrices AandB, where Ahas size [N\u0002r]andBhas size [r\u0002d], and we choose\nrto be quite small, r<<min(d;N). During \ufb01netuning we update AandBinstead\nofW. That is, we replace W+DWwithW+BA. Fig. 10.8 shows the intuition.\nFor replacing the forward pass h=xW, the new forward pass is instead:\nh=xW+xAB (10.14)\nhPretrained WeightsWdkrkABrxd11k\nd\u00d7\nFigure 10.8 The intuition of LoRA. We freeze Wto its pretrained values, and instead \ufb01ne-\ntune by training a pair of matrices AandB, updating those instead of W, and just sum Wand\nthe updated AB.\nLoRA has a number of advantages. It dramatically reduces hardware require-\nments, since gradients don\u2019t have to be calculated for most parameters. The weight\nupdates can be simply added in to the pretrained weights, since BAis the same size\nasW). That means it doesn\u2019t add any time during inference. And it also means it\u2019s\npossible to build LoRA modules for different domains and just swap them in and\nout by adding them in or subtracting them from W.\nIn its original version LoRA was applied just to the matrices in the attention\ncomputation (the WQ,WK,WV, andWOlayers). Many variants of LoRA exist.",
    "metadata": {
      "source": "10",
      "chunk_id": 22,
      "token_count": 496,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\n10.6 \u2022 P OTENTIAL HARMS FROM LANGUAGE MODELS 17\n10.6 Potential Harms from Language Models\nLarge pretrained neural language models exhibit many of the potential harms dis-\ncussed in Chapter 4 and Chapter 6. Many of these harms become realized when\npretrained language models are used for any downstream task, particularly those\ninvolving text generation, whether question answering, machine translation, or in\nassistive technologies like writing aids or web search query completion, or predic-\ntive typing for email (Olteanu et al., 2020).\nFor example, language models are prone to saying things that are false, a prob-\nlem called hallucination . Language models are trained to generate text that is pre- hallucination\ndictable and coherent, but the training algorithms we have seen so far don\u2019t have\nany way to enforce that the text that is generated is correct or true. This causes\nenormous problems for any application where the facts matter! We\u2019ll return to this\nissue in Chapter 14 where we introduce proposed mitigation methods like retrieval\naugmented generation .\nA second source of harm is that language models can generate toxic language . toxic language\nGehman et al. (2020) show that even completely non-toxic prompts can lead large\nlanguage models to output hate speech and abuse their users. Language models also\ngenerate stereotypes (Cheng et al., 2023) and negative attitudes (Brown et al., 2020;\nSheng et al., 2019) about many demographic groups.\nOne source of biases is the training data. Gehman et al. (2020) shows that large\nlanguage model training datasets include toxic text scraped from banned sites. There\nare other biases than toxicity: the training data is disproportionately generated by\nauthors from the US and from developed countries. Such biased population samples\nlikely skew the resulting generation toward the perspectives or topics of this group\nalone. Furthermore, language models can amplify demographic and other biases in\ntraining data, just as we saw for embedding models in Chapter 6.\nDatasets can be another source of harms. We already saw in Section 10.3.2\nthat using pretraining corpora scraped from the web can lead to harms related to\ncopyright and data consent. We also mentioned that pretraining data can tend to\nhave private information like phone numbers and addresses. This is problematic\nbecause large language models can leak information from their training data. That\nis, an adversary can extract training-data text from a language model such as a per-\nson\u2019s name, phone number, and address (Henderson et al. 2017, Carlini et al. 2021).\nThis becomes even more problematic when large language models are trained on\nextremely sensitive private datasets such as electronic health records.\nLanguage models can also be used by malicious actors for generating text for\nmisinformation , phishing, or other socially harmful activities (Brown et al., 2020).\nMcGuf\ufb01e and Newhouse (2020) show how large language models generate text that\nemulates online extremists, with the risk of amplifying extremist movements and\ntheir attempt to radicalize and recruit.\nFinding ways to mitigate all these harms is an important current research area in\nNLP. At the very least, carefully analyzing the data used to pretrain large language\nmodels is important as a way of understanding issues of toxicity, bias, privacy, and\nfair use, making it extremely important that language models include datasheets\n(page ??) ormodel cards (page ??) giving full replicable information on the cor-\npora used to train them. Open-source models can specify their exact training data.\nRequirements that models are transparent in such ways is also in the process of being\nincorporated into the regulations of various national governments.",
    "metadata": {
      "source": "10",
      "chunk_id": 23,
      "token_count": 791,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\n18 CHAPTER 10 \u2022 L ARGE LANGUAGE MODELS\n10.7 Summary\nThis chapter has introduced the large language model, and how it can be built out of\nthe transformer. Here\u2019s a summary of the main points that we covered:\n\u2022 Many NLP tasks\u2014such as question answering, summarization, sentiment,\nand machine translation\u2014can be cast as tasks of word prediction and hence\naddressed with Large language models.\n\u2022 Large language models are generally pretrained on large datasets of 100s of\nbillions of words generally scraped from the web.\n\u2022 These datasets need to be \ufb01ltered for quality and balanced for domains by\nupsampling and downsampling. Addressing some problems with pretraining\ndata, like toxicity, are open research problems.\n\u2022 The choice of which word to generate in large language models is generally\ndone by using a sampling algorithm.\n\u2022 Language models are evaluated by perplexity but there are also evaluations\nof accuracy downstream tasks, and ways to measure other factors like fairness\nand energy use.\n\u2022 There are various computational tricks for making large language models\nmore ef\ufb01cient, such as the KV cache andparameter-ef\ufb01cient \ufb01netuning .\n\u2022 Because of their ability to be used in so many ways, language models also\nhave the potential to cause harms. Some harms include hallucinations, bias,\nstereotypes, misinformation and propaganda, and violations of privacy and\ncopyright.\nBibliographical and Historical Notes\nAs we discussed in Chapter 3, the earliest language models were the n-gram lan-\nguage models developed (roughly simultaneously and independently) by Fred Je-\nlinek and colleagues at the IBM Thomas J. Watson Research Center, and James\nBaker at CMU. It was the Jelinek and the IBM team who \ufb01rst coined the term lan-\nguage model to mean a model of the way any kind of linguistic property (grammar,\nsemantics, discourse, speaker characteristics), in\ufb02uenced word sequence probabil-\nities (Jelinek et al., 1975). They contrasted the language model with the acoustic\nmodel which captured acoustic/phonetic characteristics of phone sequences.\nN-gram language models were very widely used over the next 30 years and more,\nacross a wide variety of NLP tasks like speech recognition and machine translations,\noften as one of multiple components of the model. The contexts for these n-gram\nmodels grew longer, with 5-gram models used quite commonly by very ef\ufb01cient LM\ntoolkits (Stolcke, 2002; Hea\ufb01eld, 2011).\nThe roots of the neural language model lie in multiple places. One was the\napplication in the 1990s, again in Jelinek\u2019s group at IBM Research, of discrimi-\nnative classi\ufb01ers to language models. Roni Rosenfeld in his dissertation (Rosen-\nfeld, 1992) \ufb01rst applied logistic regression (under the name maximum entropy or\nmaxent models) to language modeling in that IBM lab, and published a more fully\nformed version in Rosenfeld (1996). His model integrated various sorts of infor-\nmation in a logistic regression predictor, including n-gram information along with",
    "metadata": {
      "source": "10",
      "chunk_id": 24,
      "token_count": 676,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19",
    "metadata": {
      "source": "10",
      "chunk_id": 25,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 19\nother features from the context, including distant n-grams and pairs of associated\nwords called trigger pairs . Rosenfeld\u2019s model pre\ufb01gured modern language models\nby being a statistical word predictor trained in a self-supervised manner simply by\nlearning to predict upcoming words in a corpus.\nAnother was the \ufb01rst use of pretrained embeddings to model word meaning in the\nLSA/LSI models (Deerwester et al., 1988). Recall from the history section of Chap-\nter 6 that in LSA (latent semantic analysis) a term-document matrix was trained on a\ncorpus and then singular value decomposition was applied and the \ufb01rst 300 dimen-\nsions were used as a vector embedding to represent words. Landauer et al. (1997)\n\ufb01rst used the word \u201cembedding\u201d. In addition to their development of the idea of pre-\ntraining and of embeddings, the LSA community also developed ways to combine\nLSA embeddings with n-grams in an integrated language model (Bellegarda, 1997;\nCoccaro and Jurafsky, 1998).\nIn a very in\ufb02uential series of papers developing the idea of neural language\nmodels , (Bengio et al. 2000; Bengio et al. 2003; Bengio et al. 2006), Yoshua Ben-\ngio and colleagues drew on the central ideas of both these lines of self-supervised\nlanguage modeling work, (the discriminatively trained word predictor, and the pre-\ntrained embeddings). Like the maxent models of Rosenfeld, Bengio\u2019s model used\nthe next word in running text as its supervision signal. Like the LSA models, Ben-\ngio\u2019s model learned an embedding, but unlike the LSA models did it as part of the\nprocess of language modeling. The Bengio et al. (2003) model was a neural lan-\nguage model: a neural network that learned to predict the next word from prior\nwords, and did so via learning embeddings as part of the prediction process.\nThe neural language model was extended in various ways over the years, perhaps\nmost importantly in the form of the RNN language model of Mikolov et al. (2010)\nand Mikolov et al. (2011). The RNN language model was perhaps the \ufb01rst neural\nmodel that was accurate enough to surpass the performance of a traditional 5-gram\nlanguage model.\nSoon afterwards, Mikolov et al. (2013a) and Mikolov et al. (2013b) proposed to\nsimplify the hidden layer of these neural net language models to create pretrained\nword2vec word embeddings.\nThe static embedding models like LSA and word2vec instantiated a particular\nmodel of pretraining: a representation was trained on a pretraining dataset, and then\nthe representations could be used in further tasks. \u2018Dai and Le (2015) and (Peters\net al., 2018) reframed this idea by proposing models that were pretrained using a\nlanguage model objective, and then the identical model could be either frozen and\ndirectly applied for language modeling or further \ufb01netuned still using a language\nmodel objective. For example ELMo used a biLSTM self-supervised on a large\npretrained dataset using a language model objective, then \ufb01netuned on a domain-\nspeci\ufb01c dataset, and then froze the weights and added task-speci\ufb01c heads. The\nELMo work was particularly in\ufb02uential and its appearance was perhaps the mo-\nment when it became clear to the community that language models could be used as",
    "metadata": {
      "source": "10",
      "chunk_id": 26,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "model that was accurate enough to surpass the performance of a traditional 5-gram\nlanguage model.\nSoon afterwards, Mikolov et al. (2013a) and Mikolov et al. (2013b) proposed to\nsimplify the hidden layer of these neural net language models to create pretrained\nword2vec word embeddings.\nThe static embedding models like LSA and word2vec instantiated a particular\nmodel of pretraining: a representation was trained on a pretraining dataset, and then\nthe representations could be used in further tasks. \u2018Dai and Le (2015) and (Peters\net al., 2018) reframed this idea by proposing models that were pretrained using a\nlanguage model objective, and then the identical model could be either frozen and\ndirectly applied for language modeling or further \ufb01netuned still using a language\nmodel objective. For example ELMo used a biLSTM self-supervised on a large\npretrained dataset using a language model objective, then \ufb01netuned on a domain-\nspeci\ufb01c dataset, and then froze the weights and added task-speci\ufb01c heads. The\nELMo work was particularly in\ufb02uential and its appearance was perhaps the mo-\nment when it became clear to the community that language models could be used as\na general solution for NLP problems.\nTransformers were \ufb01rst applied as encoder-decoders (Vaswani et al., 2017) and\nthen to masked language modeling (Devlin et al., 2019) (as we\u2019ll see in Chapter 13\nand Chapter 11). Radford et al. (2019) then showed that the transformer-based au-\ntoregressive language model GPT2 could perform zero-shot on many NLP tasks like\nsummarization and question answering.\nThe technology used for transformer-based language models can also be applied\nto other domains and tasks, like vision, speech, and genetics. the term foundation",
    "metadata": {
      "source": "10",
      "chunk_id": 27,
      "token_count": 412,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\n20 CHAPTER 10 \u2022 L ARGE LANGUAGE MODELS\nmodel is sometimes used as a more general term for this use of large languagefoundation\nmodel\nmodel technology across domains and areas, when the elements we are computing\nover are not necessarily words. Bommasani et al. (2021) is a broad survey that\nsketches the opportunities and risks of foundation models, with special attention to\nlarge language models.",
    "metadata": {
      "source": "10",
      "chunk_id": 28,
      "token_count": 92,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21",
    "metadata": {
      "source": "10",
      "chunk_id": 29,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Bibliographical and Historical Notes 21\nBellegarda, J. R. 1997. A latent semantic analysis framework\nfor large-span language modeling. EUROSPEECH .\nBengio, Y ., R. Ducharme, and P. Vincent. 2000. A neural\nprobabilistic language model. NeurIPS .\nBengio, Y ., R. Ducharme, P. Vincent, and C. Jauvin. 2003.\nA neural probabilistic language model. JMLR , 3:1137\u2013\n1155.\nBengio, Y ., H. Schwenk, J.-S. Sen \u00b4ecal, F. Morin, and J.-L.\nGauvain. 2006. Neural probabilistic language models. In\nInnovations in Machine Learning , 137\u2013186. Springer.\nBommasani, R., D. A. Hudson, E. Adeli, R. Altman,\nS. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosse-\nlut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card,\nR. Castellon, N. S. Chatterji, A. S. Chen, K. A. Creel,\nJ. Davis, D. Demszky, C. Donahue, M. Doumbouya,\nE. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh,\nL. Fei-Fei, C. Finn, T. Gale, L. E. Gillespie, K. Goel,\nN. D. Goodman, S. Grossman, N. Guha, T. Hashimoto,\nP. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu,\nJ. Huang, T. F. Icard, S. Jain, D. Jurafsky, P. Kalluri,\nS. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W.\nKoh, M. S. Krass, R. Krishna, R. Kuditipudi, A. Ku-\nmar, F. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Lev-\nent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning,\nS. P. Mirchandani, E. Mitchell, Z. Munyikwa, S. Nair,\nA. Narayan, D. Narayanan, B. Newman, A. Nie, J. C.\nNiebles, H. Nilforoshan, J. F. Nyarko, G. Ogut, L. Orr,\nI. Papadimitriou, J. S. Park, C. Piech, E. Portelance,\nC. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong,\nY . H. Roohani, C. Ruiz, J. Ryan, C. R\u2019e, D. Sadigh,\nS. Sagawa, K. Santhanam, A. Shih, K. P. Srinivasan,\nA. Tamkin, R. Taori, A. W. Thomas, F. Tram `er, R. E.\nWang, W. Wang, B. Wu, J. Wu, Y . Wu, S. M. Xie, M. Ya-",
    "metadata": {
      "source": "10",
      "chunk_id": 30,
      "token_count": 767,
      "chapter_title": ""
    }
  },
  {
    "content": "mar, F. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Lev-\nent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning,\nS. P. Mirchandani, E. Mitchell, Z. Munyikwa, S. Nair,\nA. Narayan, D. Narayanan, B. Newman, A. Nie, J. C.\nNiebles, H. Nilforoshan, J. F. Nyarko, G. Ogut, L. Orr,\nI. Papadimitriou, J. S. Park, C. Piech, E. Portelance,\nC. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong,\nY . H. Roohani, C. Ruiz, J. Ryan, C. R\u2019e, D. Sadigh,\nS. Sagawa, K. Santhanam, A. Shih, K. P. Srinivasan,\nA. Tamkin, R. Taori, A. W. Thomas, F. Tram `er, R. E.\nWang, W. Wang, B. Wu, J. Wu, Y . Wu, S. M. Xie, M. Ya-\nsunaga, J. You, M. A. Zaharia, M. Zhang, T. Zhang,\nX. Zhang, Y . Zhang, L. Zheng, K. Zhou, and P. Liang.\n2021. On the opportunities and risks of foundation mod-\nels.ArXiv .\nBrown, T., B. Mann, N. Ryder, M. Subbiah, J. Kaplan,\nP. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger,\nT. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu,\nC. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,\nS. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,\nA. Radford, I. Sutskever, and D. Amodei. 2020. Language\nmodels are few-shot learners. NeurIPS , volume 33.\nCarlini, N., F. Tramer, E. Wallace, M. Jagielski, A. Herbert-\nV oss, K. Lee, A. Roberts, T. Brown, D. Song, U. Er-\nlingsson, et al. 2021. Extracting training data from large\nlanguage models. 30th USENIX Security Symposium\n(USENIX Security 21) .\nCheng, M., E. Durmus, and D. Jurafsky. 2023. Marked per-\nsonas: Using natural language prompts to measure stereo-\ntypes in language models. ACL.\nCoccaro, N. and D. Jurafsky. 1998. Towards better integra-\ntion of semantic predictors in statistical language model-\ning.ICSLP .\nDai, A. M. and Q. V . Le. 2015. Semi-supervised sequence\nlearning. NeurIPS .\nDeerwester, S. C., S. T. Dumais, G. W. Furnas, R. A. Harsh-",
    "metadata": {
      "source": "10",
      "chunk_id": 31,
      "token_count": 755,
      "chapter_title": ""
    }
  },
  {
    "content": "A. Radford, I. Sutskever, and D. Amodei. 2020. Language\nmodels are few-shot learners. NeurIPS , volume 33.\nCarlini, N., F. Tramer, E. Wallace, M. Jagielski, A. Herbert-\nV oss, K. Lee, A. Roberts, T. Brown, D. Song, U. Er-\nlingsson, et al. 2021. Extracting training data from large\nlanguage models. 30th USENIX Security Symposium\n(USENIX Security 21) .\nCheng, M., E. Durmus, and D. Jurafsky. 2023. Marked per-\nsonas: Using natural language prompts to measure stereo-\ntypes in language models. ACL.\nCoccaro, N. and D. Jurafsky. 1998. Towards better integra-\ntion of semantic predictors in statistical language model-\ning.ICSLP .\nDai, A. M. and Q. V . Le. 2015. Semi-supervised sequence\nlearning. NeurIPS .\nDeerwester, S. C., S. T. Dumais, G. W. Furnas, R. A. Harsh-\nman, T. K. Landauer, K. E. Lochbaum, and L. Streeter.\n1988. Computer information retrieval using latent seman-\ntic structure: US Patent 4,839,853.Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova. 2019.\nBERT: Pre-training of deep bidirectional transformers for\nlanguage understanding. NAACL HLT .\nDodge, J., S. Gururangan, D. Card, R. Schwartz, and N. A.\nSmith. 2019. Show your work: Improved reporting of\nexperimental results. EMNLP .\nDodge, J., M. Sap, A. Marasovi \u00b4c, W. Agnew, G. Ilharco,\nD. Groeneveld, M. Mitchell, and M. Gardner. 2021. Doc-\numenting large webtext corpora: A case study on the\ncolossal clean crawled corpus. EMNLP .\nEthayarajh, K. and D. Jurafsky. 2020. Utility is in the eye of\nthe user: A critique of NLP leaderboards. EMNLP .\nGao, L., T. Hoppe, A. Thite, S. Biderman, C. Foster,\nN. Nabeshima, S. Black, J. Phang, S. Presser, L. Golding,\nH. He, and C. Leahy. 2020. The Pile: An 800GB dataset\nof diverse text for language modeling. ArXiv preprint.\nGehman, S., S. Gururangan, M. Sap, Y . Choi, and N. A.\nSmith. 2020. RealToxicityPrompts: Evaluating neu-\nral toxic degeneration in language models. Findings of\nEMNLP .\nGururangan, S., A. Marasovi \u00b4c, S. Swayamdipta, K. Lo,\nI. Beltagy, D. Downey, and N. A. Smith. 2020. Don\u2019t\nstop pretraining: Adapt language models to domains and\ntasks. ACL.\nHashimoto, T., M. Srivastava, H. Namkoong, and P. Liang.\n2018. Fairness without demographics in repeated loss\nminimization. ICML .",
    "metadata": {
      "source": "10",
      "chunk_id": 32,
      "token_count": 766,
      "chapter_title": ""
    }
  },
  {
    "content": "the user: A critique of NLP leaderboards. EMNLP .\nGao, L., T. Hoppe, A. Thite, S. Biderman, C. Foster,\nN. Nabeshima, S. Black, J. Phang, S. Presser, L. Golding,\nH. He, and C. Leahy. 2020. The Pile: An 800GB dataset\nof diverse text for language modeling. ArXiv preprint.\nGehman, S., S. Gururangan, M. Sap, Y . Choi, and N. A.\nSmith. 2020. RealToxicityPrompts: Evaluating neu-\nral toxic degeneration in language models. Findings of\nEMNLP .\nGururangan, S., A. Marasovi \u00b4c, S. Swayamdipta, K. Lo,\nI. Beltagy, D. Downey, and N. A. Smith. 2020. Don\u2019t\nstop pretraining: Adapt language models to domains and\ntasks. ACL.\nHashimoto, T., M. Srivastava, H. Namkoong, and P. Liang.\n2018. Fairness without demographics in repeated loss\nminimization. ICML .\nHea\ufb01eld, K. 2011. KenLM: Faster and smaller language\nmodel queries. Workshop on Statistical Machine Trans-\nlation .\nHenderson, P., J. Hu, J. Romoff, E. Brunskill, D. Jurafsky,\nand J. Pineau. 2020. Towards the systematic reporting\nof the energy and carbon footprints of machine learning.\nJournal of Machine Learning Research , 21(248):1\u201343.\nHenderson, P., X. Li, D. Jurafsky, T. Hashimoto, M. A. Lem-\nley, and P. Liang. 2023. Foundation models and fair use.\nJMLR , 24(400):1\u201379.\nHenderson, P., K. Sinha, N. Angelard-Gontier, N. R. Ke,\nG. Fried, R. Lowe, and J. Pineau. 2017. Ethical chal-\nlenges in data-driven dialogue systems. AAAI/ACM AI\nEthics and Society Conference .\nHermann, K. M., T. Ko \u02c7cisk\u00b4y, E. Grefenstette, L. Espeholt,\nW. Kay, M. Suleyman, and P. Blunsom. 2015. Teaching\nmachines to read and comprehend. NeurIPS .\nHoltzman, A., J. Buys, L. Du, M. Forbes, and Y . Choi. 2020.\nThe curious case of neural text degeneration. ICLR .\nJelinek, F., R. L. Mercer, and L. R. Bahl. 1975. Design of a\nlinguistic statistical decoder for the recognition of contin-\nuous speech. IEEE Transactions on Information Theory ,\nIT-21(3):250\u2013256.\nKaplan, J., S. McCandlish, T. Henighan, T. B. Brown,\nB. Chess, R. Child, S. Gray, A. Radford, J. Wu, and\nD. Amodei. 2020. Scaling laws for neural language mod-\nels. ArXiv preprint.\nKiela, D., M. Bartolo, Y . Nie, D. Kaushik, A. Geiger, Z. Wu,",
    "metadata": {
      "source": "10",
      "chunk_id": 33,
      "token_count": 761,
      "chapter_title": ""
    }
  },
  {
    "content": "Ethics and Society Conference .\nHermann, K. M., T. Ko \u02c7cisk\u00b4y, E. Grefenstette, L. Espeholt,\nW. Kay, M. Suleyman, and P. Blunsom. 2015. Teaching\nmachines to read and comprehend. NeurIPS .\nHoltzman, A., J. Buys, L. Du, M. Forbes, and Y . Choi. 2020.\nThe curious case of neural text degeneration. ICLR .\nJelinek, F., R. L. Mercer, and L. R. Bahl. 1975. Design of a\nlinguistic statistical decoder for the recognition of contin-\nuous speech. IEEE Transactions on Information Theory ,\nIT-21(3):250\u2013256.\nKaplan, J., S. McCandlish, T. Henighan, T. B. Brown,\nB. Chess, R. Child, S. Gray, A. Radford, J. Wu, and\nD. Amodei. 2020. Scaling laws for neural language mod-\nels. ArXiv preprint.\nKiela, D., M. Bartolo, Y . Nie, D. Kaushik, A. Geiger, Z. Wu,\nB. Vidgen, G. Prasad, A. Singh, P. Ringshia, et al. 2021.\nDynabench: Rethinking benchmarking in nlp. NAACL\nHLT.",
    "metadata": {
      "source": "10",
      "chunk_id": 34,
      "token_count": 318,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22",
    "metadata": {
      "source": "10",
      "chunk_id": 35,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "22 Chapter 10 \u2022 Large Language Models\nLandauer, T. K., D. Laham, B. Rehder, and M. E. Schreiner.\n1997. How well can passage meaning be derived with-\nout using word order? A comparison of Latent Semantic\nAnalysis and humans. COGSCI .\nLiang, P., R. Bommasani, T. Lee, D. Tsipras, D. Soylu,\nM. Yasunaga, Y . Zhang, D. Narayanan, Y . Wu, A. Ku-\nmar, B. Newman, B. Yuan, B. Yan, C. Zhang, C. Cos-\ngrove, C. D. Manning, C. R \u00b4e, D. Acosta-Navas, D. A.\nHudson, E. Zelikman, E. Durmus, F. Ladhak, F. Rong,\nH. Ren, H. Yao, J. Wang, K. Santhanam, L. Orr, L. Zheng,\nM. Yuksekgonul, M. Suzgun, N. Kim, N. Guha, N. Chat-\nterji, O. Khattab, P. Henderson, Q. Huang, R. Chi, S. M.\nXie, S. Santurkar, S. Ganguli, T. Hashimoto, T. Icard,\nT. Zhang, V . Chaudhary, W. Wang, X. Li, Y . Mai,\nY . Zhang, and Y . Koreeda. 2023. Holistic evaluation of\nlanguage models. Transactions on Machine Learning Re-\nsearch .\nLlama Team. 2024. The llama 3 herd of models.\nLongpre, S., R. Mahari, A. Lee, C. Lund, H. Oderinwale,\nW. Brannon, N. Saxena, N. Obeng-Marnu, T. South,\nC. Hunter, et al. 2024a. Consent in crisis: The rapid de-\ncline of the ai data commons. ArXiv preprint.\nLongpre, S., G. Yauney, E. Reif, K. Lee, A. Roberts,\nB. Zoph, D. Zhou, J. Wei, K. Robinson, D. Mimno, and\nD. Ippolito. 2024b. A pretrainer\u2019s guide to training data:\nMeasuring the effects of data age, domain coverage, qual-\nity, & toxicity. NAACL HLT .\nMcGuf\ufb01e, K. and A. Newhouse. 2020. The radicalization\nrisks of GPT-3 and advanced neural language models.\nArXiv preprint arXiv:2009.06807.\nMikolov, T., K. Chen, G. S. Corrado, and J. Dean. 2013a. Ef-\n\ufb01cient estimation of word representations in vector space.\nICLR 2013 .\nMikolov, T., M. Kara\ufb01 \u00b4at, L. Burget, J. \u02c7Cernock `y, and\nS. Khudanpur. 2010. Recurrent neural network based lan-\nguage model. INTERSPEECH .\nMikolov, T., S. Kombrink, L. Burget, J. H. \u02c7Cernock `y, and\nS. Khudanpur. 2011. Extensions of recurrent neural net-\nwork language model. ICASSP .",
    "metadata": {
      "source": "10",
      "chunk_id": 36,
      "token_count": 755,
      "chapter_title": ""
    }
  },
  {
    "content": "B. Zoph, D. Zhou, J. Wei, K. Robinson, D. Mimno, and\nD. Ippolito. 2024b. A pretrainer\u2019s guide to training data:\nMeasuring the effects of data age, domain coverage, qual-\nity, & toxicity. NAACL HLT .\nMcGuf\ufb01e, K. and A. Newhouse. 2020. The radicalization\nrisks of GPT-3 and advanced neural language models.\nArXiv preprint arXiv:2009.06807.\nMikolov, T., K. Chen, G. S. Corrado, and J. Dean. 2013a. Ef-\n\ufb01cient estimation of word representations in vector space.\nICLR 2013 .\nMikolov, T., M. Kara\ufb01 \u00b4at, L. Burget, J. \u02c7Cernock `y, and\nS. Khudanpur. 2010. Recurrent neural network based lan-\nguage model. INTERSPEECH .\nMikolov, T., S. Kombrink, L. Burget, J. H. \u02c7Cernock `y, and\nS. Khudanpur. 2011. Extensions of recurrent neural net-\nwork language model. ICASSP .\nMikolov, T., I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean. 2013b. Distributed representations of words and\nphrases and their compositionality. NeurIPS .\nNadeem, M., A. Bethke, and S. Reddy. 2021. StereoSet:\nMeasuring stereotypical bias in pretrained language mod-\nels.ACL.\nNallapati, R., B. Zhou, C. dos Santos, C \u00b8 . Gulc \u00b8ehre, and\nB. Xiang. 2016. Abstractive text summarization using\nsequence-to-sequence RNNs and beyond. CoNLL .\nOlteanu, A., F. Diaz, and G. Kazai. 2020. When are search\ncompletion suggestions problematic? CSCW .\nParrish, A., A. Chen, N. Nangia, V . Padmakumar, J. Phang,\nJ. Thompson, P. M. Htut, and S. Bowman. 2022. BBQ: A\nhand-built bias benchmark for question answering. Find-\nings of ACL 2022 .\nPeters, M., M. Neumann, M. Iyyer, M. Gardner, C. Clark,\nK. Lee, and L. Zettlemoyer. 2018. Deep contextualized\nword representations. NAACL HLT .\nRadford, A., J. Wu, R. Child, D. Luan, D. Amodei, and\nI. Sutskever. 2019. Language models are unsupervised\nmultitask learners. OpenAI tech report.Raffel, C., N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y . Zhou, W. Li, and P. J. Liu. 2020. Exploring\nthe limits of transfer learning with a uni\ufb01ed text-to-text\ntransformer. JMLR , 21(140):1\u201367.\nRawls, J. 2001. Justice as fairness: A restatement . Harvard\nUniversity Press.\nRosenfeld, R. 1992. Adaptive Statistical Language Mod-",
    "metadata": {
      "source": "10",
      "chunk_id": 37,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "J. Thompson, P. M. Htut, and S. Bowman. 2022. BBQ: A\nhand-built bias benchmark for question answering. Find-\nings of ACL 2022 .\nPeters, M., M. Neumann, M. Iyyer, M. Gardner, C. Clark,\nK. Lee, and L. Zettlemoyer. 2018. Deep contextualized\nword representations. NAACL HLT .\nRadford, A., J. Wu, R. Child, D. Luan, D. Amodei, and\nI. Sutskever. 2019. Language models are unsupervised\nmultitask learners. OpenAI tech report.Raffel, C., N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y . Zhou, W. Li, and P. J. Liu. 2020. Exploring\nthe limits of transfer learning with a uni\ufb01ed text-to-text\ntransformer. JMLR , 21(140):1\u201367.\nRawls, J. 2001. Justice as fairness: A restatement . Harvard\nUniversity Press.\nRosenfeld, R. 1992. Adaptive Statistical Language Mod-\neling: A Maximum Entropy Approach . Ph.D. thesis,\nCarnegie Mellon University.\nRosenfeld, R. 1996. A maximum entropy approach to adap-\ntive statistical language modeling. Computer Speech and\nLanguage , 10:187\u2013228.\nSagawa, S., P. W. Koh, T. B. Hashimoto, and P. Liang. 2020.\nDistributionally robust neural networks for group shifts:\nOn the importance of regularization for worst-case gener-\nalization. ICLR .\nSheng, E., K.-W. Chang, P. Natarajan, and N. Peng. 2019.\nThe woman worked as a babysitter: On biases in language\ngeneration. EMNLP .\nSoldaini, L., R. Kinney, A. Bhagia, D. Schwenk, D. Atkin-\nson, R. Authur, B. Bogin, K. Chandu, J. Dumas,\nY . Elazar, V . Hofmann, A. H. Jha, S. Kumar, L. Lucy,\nX. Lyu, N. Lambert, I. Magnusson, J. Morrison,\nN. Muennighoff, A. Naik, C. Nam, M. E. Peters,\nA. Ravichander, K. Richardson, Z. Shen, E. Strubell,\nN. Subramani, O. Tafjord, P. Walsh, L. Zettlemoyer, N. A.\nSmith, H. Hajishirzi, I. Beltagy, D. Groeneveld, J. Dodge,\nand K. Lo. 2024. Dolma: An open corpus of three trillion\ntokens for language model pretraining research. ArXiv\npreprint.\nStolcke, A. 2002. SRILM \u2013 an extensible language modeling\ntoolkit. ICSLP .\nStrubell, E., A. Ganesh, and A. McCallum. 2019. Energy\nand policy considerations for deep learning in NLP. ACL.\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, \u0141. Kaiser, and I. Polosukhin. 2017. Atten-",
    "metadata": {
      "source": "10",
      "chunk_id": 38,
      "token_count": 761,
      "chapter_title": ""
    }
  },
  {
    "content": "X. Lyu, N. Lambert, I. Magnusson, J. Morrison,\nN. Muennighoff, A. Naik, C. Nam, M. E. Peters,\nA. Ravichander, K. Richardson, Z. Shen, E. Strubell,\nN. Subramani, O. Tafjord, P. Walsh, L. Zettlemoyer, N. A.\nSmith, H. Hajishirzi, I. Beltagy, D. Groeneveld, J. Dodge,\nand K. Lo. 2024. Dolma: An open corpus of three trillion\ntokens for language model pretraining research. ArXiv\npreprint.\nStolcke, A. 2002. SRILM \u2013 an extensible language modeling\ntoolkit. ICSLP .\nStrubell, E., A. Ganesh, and A. McCallum. 2019. Energy\nand policy considerations for deep learning in NLP. ACL.\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, \u0141. Kaiser, and I. Polosukhin. 2017. Atten-\ntion is all you need. NeurIPS .\nXu, A., E. Pathak, E. Wallace, S. Gururangan, M. Sap,\nand D. Klein. 2021. Detoxifying language models risks\nmarginalizing minority voices. NAACL HLT .",
    "metadata": {
      "source": "10",
      "chunk_id": 39,
      "token_count": 327,
      "chapter_title": ""
    }
  }
]