[
  {
    "content": "# transformer24aug\n\n## Page 1\n\nTransformersIntroduction to Transformers\n\n## Page 2\n\nLLMs are built out of transformersTransformer: a specific kind of network architecture, like a fancier feedforward network, but based on attentionProvided proper attribution is provided, Google hereby grants permission toreproduce the tables and \ufb01gures in this paper solely for use in journalistic orscholarly works.Attention Is All You NeedAshish Vaswani\u21e4Google Brainavaswani@google.comNoam Shazeer\u21e4Google Brainnoam@google.comNiki Parmar\u21e4Google Researchnikip@google.comJakob Uszkoreit\u21e4Google Researchusz@google.comLlion Jones\u21e4Google Researchllion@google.comAidan N. Gomez\u21e4\u2020University of Torontoaidan@cs.toronto.edu\u0141ukasz Kaiser\u21e4Google Brainlukaszkaiser@google.comIllia Polosukhin\u21e4\u2021illia.polosukhin@gmail.comAbstractThe dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder. The bestperforming models also connect the encoder and decoder through an attentionmechanism. We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely. Experiments on two machine translation tasks show these models tobe superior in quality while being more parallelizable and requiring signi\ufb01cantlyless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, includingensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.8 aftertraining for 3.5 days on eight GPUs, a small fraction of the training costs of thebest models from the literature. We show that the Transformer generalizes well toother tasks by applying it successfully to English constituency parsing both withlarge and limited training data.\u21e4Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and startedthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \ufb01rst Transformer models andhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-headattention and the parameter-free position representation and became the other person involved in nearly everydetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase andtensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, andef\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of andimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.\u2020Work performed while at Google Brain.\u2021Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n\n## Page 3\n\nA very approximate timeline1990 Static Word Embeddings2003 Neural Language Model2008 Multi-Task Learning2015 Attention2017 Transformer2018 Contextual Word Embeddings and Pretraining2019 Prompting\n\n## Page 4\n\nTransformersAttention",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 0,
      "token_count": 743,
      "chapter_title": "transformer24aug"
    }
  },
  {
    "content": "## Page 3\n\nA very approximate timeline1990 Static Word Embeddings2003 Neural Language Model2008 Multi-Task Learning2015 Attention2017 Transformer2018 Contextual Word Embeddings and Pretraining2019 Prompting\n\n## Page 4\n\nTransformersAttention\n\n## Page 5\n\nInstead of starting with the big picture\nStackedTransformerBlocksSolongandthanksforlongandthanksforNext tokenall\u2026\u2026\n\u2026\nU\nInput tokensx1x2LanguageModelingHead\nx3x4x5InputEncoding\nE1+\nE2+\nE3+\nE4+\nE5+\u2026\u2026\u2026\u2026\u2026\nU\nU\nU\nU\u2026logitslogitslogitslogitslogits\nStackedTransformerBlocksSolongandthanksforlongandthanksforNext tokenall\u2026\u2026\n\u2026\nU\nInput tokensx1x2LanguageModelingHead\nx3x4x5InputEncoding\nE1+\nE2+\nE3+\nE4+\nE5+\u2026\u2026\u2026\u2026\u2026\nU\nU\nU\nU\u2026logitslogitslogitslogitslogitsLet's consider the embeddings for an individual word from a particular layer\n\n## Page 6\n\nProblem with static embeddings (word2vec)They are static!  The embedding for a word doesn't reflect how its meaning changes in context. The chicken didn't cross the road because it was too tiredWhat is the meaning represented in the static embedding for \"it\"? \n\n## Page 7\n\nContextual Embeddings\u2022Intuition: a representation of meaning of a word should be different in different contexts!\u2022Contextual Embedding: each word has a different vector that expresses different meanings depending on the surrounding words\u2022How to compute contextual embeddings?\u2022Attention\n\n## Page 8\n\nContextual EmbeddingsThe chicken didn't cross the road because itWhat should be the properties of \"it\"?The chicken didn't cross the road because it was too tiredThe chicken didn't cross the road because it was too wideAt this point in the sentence, it's probably referring to either the chicken or the street \n\n## Page 9\n\nIntuition of attentionBuild up the contextual embedding from a word by selectively integrating information from all the neighboring wordsWe say that a word \"attends to\" some neighboring words more than others\n\n## Page 10\n\nIntuition of attention: testThechickendidn\u2019tcrosstheroadbecauseitwastootiredThechickendidn\u2019tcrosstheroadbecauseitwastootiredLayer k+1Layer kself-attention distributioncolumns corresponding to input tokens\n\n## Page 11\n\nAttention definitionA mechanism for helping compute the embedding for a token by selectively attending to and integrating information from surrounding tokens (at the previous layer).More formally: a method for doing a weighted sum of vectors.\n\n## Page 12\n\nAttention is left-to-right\nattentionattentionSelf-AttentionLayerattentionattentionattentiona1a2a3a4a5x3x4x5x1x2",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 1,
      "token_count": 614,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 2,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Simplified version of attention: a sum of prior words weighted by their similarity with the current wordGiven a sequence of token embeddings: x1 x2   x3   x4   x5   x6   x7   xiProduce: ai = a weighted sum of x1 through x7 (and xi)Weighted by their similarity to xi10.1\u2022THETRANSFORMER:ASELF-ATTENTIONNETWORK5\nSelf-AttentionLayerx1a1\nx2a2a3a4a5\nx3x4x5Figure 10.2Information \ufb02ow in a causal (or masked) self-attention model. In processingeach element of the sequence, the model attends to all the inputs up to, and including, thecurrent one. Unlike RNNs, the computations at each time step are independent of all theother steps and therefore can be performed in parallel.10.1.3 Self-attention more formallyWe\u2019ve given the intuition of self-attention (as a way to compute representations of aword at a given layer by integrating information from words at the previous layer)and we\u2019ve de\ufb01ned context as all the prior words in the input. Let\u2019s now introducethe self-attention computation itself.The core intuition of attention is the idea ofcomparingan item of interest to acollection of other items in a way that reveals their relevance in the current context.In the case of self-attention for language, the set of comparisons are to other words(or tokens) within a given sequence. The result of these comparisons is then used tocompute an output sequence for the current input sequence. For example, returningto Fig.10.2, the computation ofa3is based on a set of comparisons between theinputx3and its preceding elementsx1andx2, and tox3itself.How shall we compare words to other words? Since our representations forwords are vectors, we\u2019ll make use of our old friend thedot productthat we usedfor computing word similarity in Chapter 6, and also played a role in attention inChapter 9. Let\u2019s refer to the result of this comparison between wordsiandjas ascore (we\u2019ll be updating this equation to add attention to the computation of thisscore):Verson 1:score(xi,xj)=xi\u00b7xj(10.4)The result of a dot product is a scalar value ranging from\u0000\u2022to\u2022, the largerthe value the more similar the vectors that are being compared. Continuing with ourexample, the \ufb01rst step in computingy3would be to compute three scores:x3\u00b7x1,x3\u00b7x2andx3\u00b7x3. Then to make effective use of these scores, we\u2019ll normalize themwith a softmax to create a vector of weights,aij, that indicates the proportionalrelevance of each input to the input elementithat is the current focus of attention.aij=softmax(score(xi,xj))8j\uf8ffi(10.5)=exp(score(xi,xj))Pik=1exp(score(xi,xk))8j\uf8ffi(10.6)Of course, the softmax weight will likely be highest for the current focus elementi, sincevecxiis very similar to itself, resulting in a high dot product. But othercontext words may also be similar toi, and the softmax will also assign some weightto those words.Given the proportional scores ina, we generate an output valueaiby summing10.1\u2022THETRANSFORMER:ASELF-ATTENTIONNETWORK5\nSelf-AttentionLayerx1a1\nx2a2a3a4a5",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 3,
      "token_count": 758,
      "chapter_title": ""
    }
  },
  {
    "content": "x3x4x5Figure 10.2Information \ufb02ow in a causal (or masked) self-attention model. In processingeach element of the sequence, the model attends to all the inputs up to, and including, thecurrent one. Unlike RNNs, the computations at each time step are independent of all theother steps and therefore can be performed in parallel.10.1.3 Self-attention more formallyWe\u2019ve given the intuition of self-attention (as a way to compute representations of aword at a given layer by integrating information from words at the previous layer)and we\u2019ve de\ufb01ned context as all the prior words in the input. Let\u2019s now introducethe self-attention computation itself.The core intuition of attention is the idea ofcomparingan item of interest to acollection of other items in a way that reveals their relevance in the current context.In the case of self-attention for language, the set of comparisons are to other words(or tokens) within a given sequence. The result of these comparisons is then used tocompute an output sequence for the current input sequence. For example, returningto Fig.10.2, the computation ofa3is based on a set of comparisons between theinputx3and its preceding elementsx1andx2, and tox3itself.How shall we compare words to other words? Since our representations forwords are vectors, we\u2019ll make use of our old friend thedot productthat we usedfor computing word similarity in Chapter 6, and also played a role in attention inChapter 9. Let\u2019s refer to the result of this comparison between wordsiandjas ascore (we\u2019ll be updating this equation to add attention to the computation of thisscore):Verson 1:score(xi,xj)=xi\u00b7xj(10.4)The result of a dot product is a scalar value ranging from\u0000\u2022to\u2022, the largerthe value the more similar the vectors that are being compared. Continuing with ourexample, the \ufb01rst step in computingy3would be to compute three scores:x3\u00b7x1,x3\u00b7x2andx3\u00b7x3. Then to make effective use of these scores, we\u2019ll normalize themwith a softmax to create a vector of weights,aij, that indicates the proportionalrelevance of each input to the input elementithat is the current focus of attention.aij=softmax(score(xi,xj))8j\uf8ffi(10.5)=exp(score(xi,xj))Pik=1exp(score(xi,xk))8j\uf8ffi(10.6)Of course, the softmax weight will likely be highest for the current focus elementi, sincevecxiis very similar to itself, resulting in a high dot product. But othercontext words may also be similar toi, and the softmax will also assign some weightto those words.Given the proportional scores ina, we generate an output valueaiby summing6CHAPTER10\u2022TRANSFORMERS ANDLARGELANGUAGEMODELSthe inputs seen so far, each weighted by itsavalue.ai=Xj\uf8ffiaijxj(10.7)The steps embodied in Equations10.4through10.7represent the core of anattention-based approach: a set of comparisons to relevant items in some context,a normalization of those scores to provide a probability distribution, followed by aweighted sum using this distribution. The outputais the result of this straightfor-ward computation over the inputs.This kind of simple attention can be useful, and indeed we saw in Chapter 9how to use this simple idea of attention for LSTM-based encoder-decoder modelsfor machine translation. But transformers allow us to create a more sophisticatedway of representing how words can contribute to the representation of longer inputs.Consider the three different roles that each input embedding plays during the courseof the attention process.\u2022Asthe",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 4,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "course, the softmax weight will likely be highest for the current focus elementi, sincevecxiis very similar to itself, resulting in a high dot product. But othercontext words may also be similar toi, and the softmax will also assign some weightto those words.Given the proportional scores ina, we generate an output valueaiby summing6CHAPTER10\u2022TRANSFORMERS ANDLARGELANGUAGEMODELSthe inputs seen so far, each weighted by itsavalue.ai=Xj\uf8ffiaijxj(10.7)The steps embodied in Equations10.4through10.7represent the core of anattention-based approach: a set of comparisons to relevant items in some context,a normalization of those scores to provide a probability distribution, followed by aweighted sum using this distribution. The outputais the result of this straightfor-ward computation over the inputs.This kind of simple attention can be useful, and indeed we saw in Chapter 9how to use this simple idea of attention for LSTM-based encoder-decoder modelsfor machine translation. But transformers allow us to create a more sophisticatedway of representing how words can contribute to the representation of longer inputs.Consider the three different roles that each input embedding plays during the courseof the attention process.\u2022Asthe current focus of attentionwhen being compared to all of the otherpreceding inputs. We\u2019ll refer to this role as aquery.query\u2022In its role asa preceding inputbeing compared to the current focus of atten-tion. We\u2019ll refer to this role as akey.key\u2022And \ufb01nally, as avalueused to compute the output for the current focus ofvalueattention.To capture these three different roles, transformers introduce weight matricesWQ,WK, andWV. These weights will be used to project each input vectorxiintoa representation of its role as a key, query, or value.qi=xiWQ;ki=xiWK;vi=xiWV(10.8)The inputsxand outputsyof transformers, as well as the intermediate vectors afterthe various layers like the attention output vectora, all have the same dimensionality1\u21e5d. We\u2019ll have a dimensiondkfor the key and query vectors, and a separatedimensiondvfor the value vectors. In the original transformer work (Vaswani et al.,2017),dwas 512,dkanddvwere both 64. The shapes of the transform matrices arethenWQ2Rd\u21e5dk,WK2Rd\u21e5dk, andWV2Rd\u21e5dv.Given these projections, the score between a current focus of attention,xi, andan element in the preceding context,xj, consists of a dot product between its queryvectorqiand the preceding element\u2019s key vectorskj. This dot product has the rightshape since both the query and the key are of dimensionality 1\u21e5dk. Let\u2019s updateour previous comparison calculation to re\ufb02ect this, replacing Eq.10.4with Eq.10.9:Verson 2:score(xi,xj)=qi\u00b7kj(10.9)The ensuing softmax calculation resulting inai,jremains the same, but the outputcalculation foraiis now based on a weighted sum over the value vectorsv.ai=Xj\uf8ffiaijvj(10.10)Again, the softmax weightaijwill likely be highest for the current focus elementi, and so the value foryiwill be most in\ufb02uenced byvi. But the model will also payattention to other contextual words if they are similar toi, allowing their values to",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 5,
      "token_count": 753,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\nIntuition of attention: test\nx1  x2  x3  x4  x5  x6  x7   xiThechickendidn\u2019tcrosstheroadbecauseitwastootiredThechickendidn\u2019tcrosstheroadbecauseitwastootiredLayer k+1Layer kself-attention distributioncolumns corresponding to input tokens\n\n## Page 15\n\nAn Actual Attention Head: slightly more complicatedHigh-level idea: instead of using vectors (like xi and x4) directly, we'll represent 3 separate roles each vector xi plays:\u2022query: As the current element being compared to the preceding inputs. \u2022key: as a preceding input that is being compared to the current element to determine a similarity\u2022value: a value of a preceding element that gets weighted and summed \n\n## Page 16\n\nThechickendidn\u2019tcrosstheroadbecauseitwastootiredThechickendidn\u2019tcrosstheroadbecauseitwastootiredLayer k+1Layer kself-attention distributioncolumns corresponding to input tokensAttention intuition\nx1  x2  x3  x4  x5  x6  x7   xiquery\nvalues\n\n## Page 17\n\nIntuition of attention: \nx1  x2  x3  x4  x5  x6  x7  xiquery\nvalueskvkvkvkvkvkvkvkeyskvThechickendidn\u2019tcrosstheroadbecauseitwastootiredThechickendidn\u2019tcrosstheroadbecauseitwastootiredLayer k+1Layer kself-attention distributioncolumns corresponding to input tokens",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 6,
      "token_count": 352,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\nAn Actual Attention Head: slightly more complicatedWe'll use matrices to project each vector xi into a representation of its role as query, key, value:\u2022query: WQ\u2022key: WK\u2022value: WV9.1\u2022ATTENTION5the softmax weight will likely be highest forxi, sincexiis very similar to itself,resulting in a high dot product. But other context words may also be similar toi, andthe softmax will also assign some weight to those words. Then we use these weightsas theavalues in Eq.9.6to compute the weighted sum that is oura3.The simpli\ufb01ed attention in equations9.6\u20139.8demonstrates the attention-basedapproach to computingai: compare thexito prior vectors, normalize those scoresinto a probability distribution used to weight the sum of the prior vector. But nowwe\u2019re ready to remove the simpli\ufb01cations.A single attention head using query, key, and value matricesNow that we\u2019veseen a simple intuition of attention, let\u2019s introduce the actualattention head, theattention headversion of attention that\u2019s used in transformers. (The wordheadis often used inheadtransformers to refer to speci\ufb01c structured layers). The attention head allows us todistinctly represent three different roles that each input embedding plays during thecourse of the attention process:\u2022Asthe current elementbeing compared to the preceding inputs. We\u2019ll refer tothis role as aquery.query\u2022In its role asa preceding inputthat is being compared to the current elementto determine a similarity weight. We\u2019ll refer to this role as akey.key\u2022And \ufb01nally, as avalueof a preceding element that gets weighted and summedvalueup to compute the output for the current element.To capture these three different roles, transformers introduce weight matricesWQ,WK, andWV. These weights will project each input vectorxiinto a represen-tation of its role as a key, query, or value:qi=xiWQ;ki=xiWK;vi=xiWV(9.9)Given these projections, when we are computing the similarity of the current ele-mentxiwith some prior elementxj, we\u2019ll use the dot product between the currentelement\u2019squeryvectorqiand the preceding element\u2019skeyvectorkj. Furthermore,the result of a dot product can be an arbitrarily large (positive or negative) value, andexponentiating large values can lead to numerical issues and loss of gradients duringtraining. To avoid this, we scale the dot product by a factor related to the size of theembeddings, via diving by the square root of the dimensionality of the query andkey vectors (dk). We thus replace the simpli\ufb01ed Eq.9.7with Eq.9.11. The ensuingsoftmax calculation resulting inaijremains the same, but the output calculation foraiis now based on a weighted sum over the value vectorsv(Eq.9.13).Here\u2019s a \ufb01nal set of equations for computing self-attention for a single self-attention output vectoraifrom a single input vectorxi. This version of attentioncomputesaiby summing thevaluesof the prior elements, each weighted by thesimilarity of itskeyto thequeryfrom the current element:qi=xiWQ;kj=xjWK;vj=xjWV(9.10)score(xi,xj)=qi\u00b7kjpdk(9.11)aij=softmax(score(xi,xj))8j\uf8ffi(9.12)ai=Xj\uf8ffiaijvj(9.13)We illustrate this in Fig.9.4for the case of calculating the value of the third outputa3in a sequence.",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 7,
      "token_count": 782,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\nAn Actual Attention Head: slightly more complicatedGiven these 3 representation of xiTo compute  similarity of current element xi with some prior element xjWe\u2019ll use dot product between  qi and kj. And instead of summing up xj ,  we'll sum up vj9.1\u2022ATTENTION5the softmax weight will likely be highest forxi, sincexiis very similar to itself,resulting in a high dot product. But other context words may also be similar toi, andthe softmax will also assign some weight to those words. Then we use these weightsas theavalues in Eq.9.6to compute the weighted sum that is oura3.The simpli\ufb01ed attention in equations9.6\u20139.8demonstrates the attention-basedapproach to computingai: compare thexito prior vectors, normalize those scoresinto a probability distribution used to weight the sum of the prior vector. But nowwe\u2019re ready to remove the simpli\ufb01cations.A single attention head using query, key, and value matricesNow that we\u2019veseen a simple intuition of attention, let\u2019s introduce the actualattention head, theattention headversion of attention that\u2019s used in transformers. (The wordheadis often used inheadtransformers to refer to speci\ufb01c structured layers). The attention head allows us todistinctly represent three different roles that each input embedding plays during thecourse of the attention process:\u2022Asthe current elementbeing compared to the preceding inputs. We\u2019ll refer tothis role as aquery.query\u2022In its role asa preceding inputthat is being compared to the current elementto determine a similarity weight. We\u2019ll refer to this role as akey.key\u2022And \ufb01nally, as avalueof a preceding element that gets weighted and summedvalueup to compute the output for the current element.To capture these three different roles, transformers introduce weight matricesWQ,WK, andWV. These weights will project each input vectorxiinto a represen-tation of its role as a key, query, or value:qi=xiWQ;ki=xiWK;vi=xiWV(9.9)Given these projections, when we are computing the similarity of the current ele-mentxiwith some prior elementxj, we\u2019ll use the dot product between the currentelement\u2019squeryvectorqiand the preceding element\u2019skeyvectorkj. Furthermore,the result of a dot product can be an arbitrarily large (positive or negative) value, andexponentiating large values can lead to numerical issues and loss of gradients duringtraining. To avoid this, we scale the dot product by a factor related to the size of theembeddings, via diving by the square root of the dimensionality of the query andkey vectors (dk). We thus replace the simpli\ufb01ed Eq.9.7with Eq.9.11. The ensuingsoftmax calculation resulting inaijremains the same, but the output calculation foraiis now based on a weighted sum over the value vectorsv(Eq.9.13).Here\u2019s a \ufb01nal set of equations for computing self-attention for a single self-attention output vectoraifrom a single input vectorxi. This version of attentioncomputesaiby summing thevaluesof the prior elements, each weighted by thesimilarity of itskeyto thequeryfrom the current element:qi=xiWQ;kj=xjWK;vj=xjWV(9.10)score(xi,xj)=qi\u00b7kjpdk(9.11)aij=softmax(score(xi,xj))8j\uf8ffi(9.12)ai=Xj\uf8ffiaijvj(9.13)We illustrate this in Fig.9.4for the case of calculating the value of the third outputa3in a sequence.",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 8,
      "token_count": 795,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\nFinal equations for one attention head9.1\u2022ATTENTION5the softmax weight will likely be highest forxi, sincexiis very similar to itself,resulting in a high dot product. But other context words may also be similar toi, andthe softmax will also assign some weight to those words. Then we use these weightsas theavalues in Eq.9.6to compute the weighted sum that is oura3.The simpli\ufb01ed attention in equations9.6\u20139.8demonstrates the attention-basedapproach to computingai: compare thexito prior vectors, normalize those scoresinto a probability distribution used to weight the sum of the prior vector. But nowwe\u2019re ready to remove the simpli\ufb01cations.A single attention head using query, key, and value matricesNow that we\u2019veseen a simple intuition of attention, let\u2019s introduce the actualattention head, theattention headversion of attention that\u2019s used in transformers. (The wordheadis often used inheadtransformers to refer to speci\ufb01c structured layers). The attention head allows us todistinctly represent three different roles that each input embedding plays during thecourse of the attention process:\u2022Asthe current elementbeing compared to the preceding inputs. We\u2019ll refer tothis role as aquery.query\u2022In its role asa preceding inputthat is being compared to the current elementto determine a similarity weight. We\u2019ll refer to this role as akey.key\u2022And \ufb01nally, as avalueof a preceding element that gets weighted and summedvalueup to compute the output for the current element.To capture these three different roles, transformers introduce weight matricesWQ,WK, andWV. These weights will project each input vectorxiinto a represen-tation of its role as a key, query, or value:qi=xiWQ;ki=xiWK;vi=xiWV(9.9)Given these projections, when we are computing the similarity of the current ele-mentxiwith some prior elementxj, we\u2019ll use the dot product between the currentelement\u2019squeryvectorqiand the preceding element\u2019skeyvectorkj. Furthermore,the result of a dot product can be an arbitrarily large (positive or negative) value, andexponentiating large values can lead to numerical issues and loss of gradients duringtraining. To avoid this, we scale the dot product by a factor related to the size of theembeddings, via diving by the square root of the dimensionality of the query andkey vectors (dk). We thus replace the simpli\ufb01ed Eq.9.7with Eq.9.11. The ensuingsoftmax calculation resulting inaijremains the same, but the output calculation foraiis now based on a weighted sum over the value vectorsv(Eq.9.13).Here\u2019s a \ufb01nal set of equations for computing self-attention for a single self-attention output vectoraifrom a single input vectorxi. This version of attentioncomputesaiby summing thevaluesof the prior elements, each weighted by thesimilarity of itskeyto thequeryfrom the current element:qi=xiWQ;kj=xjWK;vj=xjWV(9.10)score(xi,xj)=qi\u00b7kjpdk(9.11)aij=softmax(score(xi,xj))8j\uf8ffi(9.12)ai=Xj\uf8ffiaijvj(9.13)We illustrate this in Fig.9.4for the case of calculating the value of the third outputa3in a sequence.",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 9,
      "token_count": 745,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21\n\nCalculating the value of a36. Sum the weighted value vectors4. Turn into \ud835\udefci,j weights via softmaxa3\n1. Generate key, query, value vectors2. Compare x3\u2019s query withthe keys for x1, x2, and x3Output of self-attention\nWk\nWvWqx1kqvx3kqvx2kqv\n\u00d7\u00d7\nWkWkWqWqWvWv5. Weigh each value vector\u00f7\u221adk3. Divide score by \u221adk\u00f7\u221adk\u00f7\u221adk\ud835\udefc3,1\ud835\udefc3,2\ud835\udefc3,3\n\n## Page 22\n\nActual Attention: slightly more complicated\u2022Instead of one attention head, we'll have lots of them!\u2022Intuition: each head might be attending to the context for different purposes\u2022Different linguistic relationships or patterns in the context9.2\u2022TRANSFORMERBLOCKS7shows an intuition.qci=xiWQc;kcj=xjWKc;vcj=xjWVc;8c1\uf8ffc\uf8ffh(9.14)scorec(xi,xj)=qci\u00b7kcjpdk(9.15)acij=softmax(scorec(xi,xj))8j\uf8ffi(9.16)headci=Xj\uf8ffiacijvcj(9.17)ai=(head1\u0000head2...\u0000headh)WO(9.18)MultiHeadAttention(xi,[x1,\u00b7\u00b7\u00b7,xN]) =ai(9.19)The output of each of thehheads is of shape 1\u21e5dv, and so the output of themulti-head layer withhheads consists ofhvectors of shape 1\u21e5dv. These are con-catenated to produce a single output with dimensionality 1\u21e5hdv. Then we use yetanother linear projectionWO2Rhdv\u21e5dto reshape it, resulting in the multi-headattention vectoraiwith the correct output shape[1xd]at each inputi.\naixi-1xixi-2xi-3WK1Head 1WV1WQ1\u2026\u2026WK2Head 2WV2WQ2WK8Head 8WV8WQ8aiWO  [hdv x d][1 x dv ][1 x d]\n[1 x d][1 x hdv ]Project down to dConcatenate OutputsEach headattends di\ufb00erentlyto context\u2026[1 x dv ]\nFigure 9.5The multi-head attention computation for inputxi, producing outputai. A multi-head attentionlayer hashheads, each with its own key, query and value weight matrices. The outputs from each of the headsare concatenated and then projected down tod, thus producing an output of the same size as the input.9.2 Transformer BlocksThe self-attention calculation lies at the core of what\u2019s called a transformer block,which, in addition to the self-attention layer, includes three other kinds of layers: (1)a feedforward layer, (2) residual connections, and (3) normalizing layers (colloqui-ally called \u201clayer norm\u201d).Fig.9.6illustrates a transformer block, sketching a common way of thinkingabout the block that is called theresidual stream(Elhage et al.,2021). In the resid-residual streamual stream viewpoint, we consider the processing of an individual tokenithroughthe transformer block as a single stream ofd-dimensional representations for tokenpositioni. This residual stream starts with the original input vector, and the various",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 10,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23\n\nMulti-head attention\naixi-1xixi-2xi-3WK1Head 1WV1WQ1\u2026\u2026WK2Head 2WV2WQ2WK8Head 8WV8WQ8aiWO  [hdv x d][1 x dv ][1 x d]\n[1 x d][1 x hdv ]Project down to dConcatenate OutputsEach headattends di\ufb00erentlyto context\u2026[1 x dv ]\n\n## Page 24\n\nSummaryAttention is a method for enriching the representation of a token by incorporating contextual informationThe result: the embedding for each word will be different in different contexts!Contextual embeddings: a representation of word meaning in its context.We'll see in the next lecture that attention can also be viewed as a way to move information from one token to another.\n\n## Page 25\n\nTransformersAttention\n\n## Page 26\n\nTransformersThe Transformer Block\n\n## Page 27\n\nStackedTransformerBlocksSolongandthanksforlongandthanksforNext tokenall\u2026\u2026\n\u2026\nU\nInput tokensx1x2LanguageModelingHead\nx3x4x5InputEncoding\nE1+\nE2+\nE3+\nE4+\nE5+\u2026\u2026\u2026\u2026\u2026\nU\nU\nU\nU\u2026logitslogitslogitslogitslogitsReminder: transformer language model\n\n## Page 28\n\nThe residual stream: each token gets passed up and modified\nLayer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1+\u2026\u2026",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 11,
      "token_count": 335,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24\n\nSummaryAttention is a method for enriching the representation of a token by incorporating contextual informationThe result: the embedding for each word will be different in different contexts!Contextual embeddings: a representation of word meaning in its context.We'll see in the next lecture that attention can also be viewed as a way to move information from one token to another.\n\n## Page 25\n\nTransformersAttention\n\n## Page 26\n\nTransformersThe Transformer Block\n\n## Page 27\n\nStackedTransformerBlocksSolongandthanksforlongandthanksforNext tokenall\u2026\u2026\n\u2026\nU\nInput tokensx1x2LanguageModelingHead\nx3x4x5InputEncoding\nE1+\nE2+\nE3+\nE4+\nE5+\u2026\u2026\u2026\u2026\u2026\nU\nU\nU\nU\u2026logitslogitslogitslogitslogitsReminder: transformer language model\n\n## Page 28\n\nThe residual stream: each token gets passed up and modified\nLayer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1+\u2026\u2026\n\n## Page 29\n\nWe'll need nonlinearities, so a feedforward layer\nLayer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1+\u2026\u20268CHAPTER9\u2022THETRANSFORMER\nLayer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1\n+\u2026\u2026ResidualStream\nFigure 9.6The architecture of a transformer block showing theresidual stream. This\ufb01gure shows theprenormversion of the architecture, in which the layer norms happen beforethe attention and feedforward layers rather than after.components read their input from the residual stream and add their output back intothe stream.The input at the bottom of the stream is an embedding for a token, which hasdimensionalityd. This initial embedding gets passed up (byresidual connections),and is progressively added to by the other components of the transformer: theat-tention layerthat we have seen, and thefeedforward layerthat we will introduce.Before the attention and feedforward layer is a computation called thelayer norm.Thus the initial vector is passed through a layer norm and attention layer, andthe result is added back into the stream, in this case to the original input vectorxi. And then this summed vector is again passed through another layer norm and afeedforward layer, and the output of those is added back into the residual, and we\u2019llusehito refer to the resulting output of the transformer block for tokeni. (In earlierdescriptions the residual stream was often described using a different metaphor asresidual connectionsthat add the input of a component to its output, but the residualstream is a more perspicuous way of visualizing the transformer.)We\u2019ve already seen the attention layer, so let\u2019s now introduce the feedforwardand layer norm computations in the context of processing a single inputxiat tokenpositioni.Feedforward layerThe feedforward layer is a fully-connected 2-layer network,i.e., one hidden layer, two weight matrices, as introduced in Chapter 7. The weightsare the same for each token positioni, but are different from layer to layer. Itis common to make the dimensionalitydffof the hidden layer of the feedforwardnetwork be larger than the model dimensionalityd. (For example in the originaltransformer model,d=512 anddff=2048.)FFN(xi)=ReLU(xiW1+b1)W2+b2(9.20)Layer NormAt two stages in the transformer block wenormalizethe vector (Baet al.,2016). This process, calledlayer norm(short for layer normalization), is onelayer norm",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 12,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 30\n\nLayer norm: the vector xi is normalized twice\nLayer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1+\u2026\u2026",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 13,
      "token_count": 44,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 31",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 14,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Layer NormLayer norm is a variation of the z-score from statistics, applied to a single vec- tor in a hidden layer 9.2\u2022TRANSFORMERBLOCKS9of many forms of normalization that can be used to improve training performancein deep neural networks by keeping the values of a hidden layer in a range thatfacilitates gradient-based training.Layer norm is a variation of thez-scorefrom statistics, applied to a single vec-tor in a hidden layer. That is, the term layer norm is a bit confusing; layer normisnotapplied to an entire transformer layer, but just to the embedding vector of asingle token. Thus the input to layer norm is a single vector of dimensionalitydand the output is that vector normalized, again of dimensionalityd. The \ufb01rst step inlayer normalization is to calculate the mean,\u00b5, and standard deviation,s, over theelements of the vector to be normalized. Given an embedding vectorxof dimen-sionalityd, these values are calculated as follows.\u00b5=1ddXi=1xi(9.21)s=vuut1ddXi=1(xi\u0000\u00b5)2(9.22)Given these values, the vector components are normalized by subtracting the meanfrom each and dividing by the standard deviation. The result of this computation isa new vector with zero mean and a standard deviation of one.\u02c6x=(x\u0000\u00b5)s(9.23)Finally, in the standard implementation of layer normalization, two learnable param-eters,gandb, representing gain and offset values, are introduced.LayerNorm(x)=g(x\u0000\u00b5)s+b(9.24)Putting it all togetherThe function computed by a transformer block can be ex-pressed by breaking it down with one equation for each component computation,usingt(of shape[1\u21e5d]) to stand for transformer and superscripts to demarcateeach computation inside the block:t1i=LayerNorm(xi)(9.25)t2i=MultiHeadAttention(t1i,\u21e5x11,\u00b7\u00b7\u00b7,x1N\u21e4)(9.26)t3i=t2i+xi(9.27)t4i=LayerNorm(t3i)(9.28)t5i=FFN(t4i)(9.29)hi=t5i+t3i(9.30)Notice that the only component that takes as input information from other tokens(other residual streams) is multi-head attention, which (as we see from (9.27)) looksat all the neighboring tokens in the context. The output from attention, however, isthen added into this token\u2019s embedding stream. In fact,Elhage et al.(2021) show thatwe can view attention heads as literally moving information from the residual streamof a neighboring token into the current stream. The high-dimensional embeddingspace at each position thus contains information about the current token and aboutneighboring tokens, albeit in different subspaces of the vector space. Fig.9.7showsa visualization of this movement.9.2\u2022TRANSFORMERBLOCKS9of many forms of normalization that can be used to improve training performancein deep neural networks by keeping the values of a hidden layer in a range thatfacilitates gradient-based training.Layer norm is a variation of thez-scorefrom statistics, applied to a single vec-tor in a hidden layer. That is, the term layer norm is a bit confusing; layer normisnotapplied to an entire transformer layer, but just to the embedding vector of asingle token. Thus the input to layer norm is a single vector of dimensionalitydand the output is that vector normalized, again of dimensionalityd. The \ufb01rst step inlayer normalization is to calculate the mean,\u00b5, and standard deviation,s, over theelements of the vector to be normalized. Given an embedding vectorxof",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 15,
      "token_count": 793,
      "chapter_title": ""
    }
  },
  {
    "content": "information from other tokens(other residual streams) is multi-head attention, which (as we see from (9.27)) looksat all the neighboring tokens in the context. The output from attention, however, isthen added into this token\u2019s embedding stream. In fact,Elhage et al.(2021) show thatwe can view attention heads as literally moving information from the residual streamof a neighboring token into the current stream. The high-dimensional embeddingspace at each position thus contains information about the current token and aboutneighboring tokens, albeit in different subspaces of the vector space. Fig.9.7showsa visualization of this movement.9.2\u2022TRANSFORMERBLOCKS9of many forms of normalization that can be used to improve training performancein deep neural networks by keeping the values of a hidden layer in a range thatfacilitates gradient-based training.Layer norm is a variation of thez-scorefrom statistics, applied to a single vec-tor in a hidden layer. That is, the term layer norm is a bit confusing; layer normisnotapplied to an entire transformer layer, but just to the embedding vector of asingle token. Thus the input to layer norm is a single vector of dimensionalitydand the output is that vector normalized, again of dimensionalityd. The \ufb01rst step inlayer normalization is to calculate the mean,\u00b5, and standard deviation,s, over theelements of the vector to be normalized. Given an embedding vectorxof dimen-sionalityd, these values are calculated as follows.\u00b5=1ddXi=1xi(9.21)s=vuut1ddXi=1(xi\u0000\u00b5)2(9.22)Given these values, the vector components are normalized by subtracting the meanfrom each and dividing by the standard deviation. The result of this computation isa new vector with zero mean and a standard deviation of one.\u02c6x=(x\u0000\u00b5)s(9.23)Finally, in the standard implementation of layer normalization, two learnable param-eters,gandb, representing gain and offset values, are introduced.LayerNorm(x)=g(x\u0000\u00b5)s+b(9.24)Putting it all togetherThe function computed by a transformer block can be ex-pressed by breaking it down with one equation for each component computation,usingt(of shape[1\u21e5d]) to stand for transformer and superscripts to demarcateeach computation inside the block:t1i=LayerNorm(xi)(9.25)t2i=MultiHeadAttention(t1i,\u21e5x11,\u00b7\u00b7\u00b7,x1N\u21e4)(9.26)t3i=t2i+xi(9.27)t4i=LayerNorm(t3i)(9.28)t5i=FFN(t4i)(9.29)hi=t5i+t3i(9.30)Notice that the only component that takes as input information from other tokens(other residual streams) is multi-head attention, which (as we see from (9.27)) looksat all the neighboring tokens in the context. The output from attention, however, isthen added into this token\u2019s embedding stream. In fact,Elhage et al.(2021) show thatwe can view attention heads as literally moving information from the residual streamof a neighboring token into the current stream. The high-dimensional embeddingspace at each position thus contains information about the current token and aboutneighboring tokens, albeit in different subspaces of the vector space. Fig.9.7showsa visualization of this movement.9.2\u2022TRANSFORMERBLOCKS9of many forms of normalization that can be used to improve training performancein deep neural networks by keeping the values of a hidden layer in a range thatfacilitates gradient-based training.Layer norm is a variation of thez-scorefrom statistics, applied to a single vec-tor in a hidden layer. That",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 16,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "that the only component that takes as input information from other tokens(other residual streams) is multi-head attention, which (as we see from (9.27)) looksat all the neighboring tokens in the context. The output from attention, however, isthen added into this token\u2019s embedding stream. In fact,Elhage et al.(2021) show thatwe can view attention heads as literally moving information from the residual streamof a neighboring token into the current stream. The high-dimensional embeddingspace at each position thus contains information about the current token and aboutneighboring tokens, albeit in different subspaces of the vector space. Fig.9.7showsa visualization of this movement.9.2\u2022TRANSFORMERBLOCKS9of many forms of normalization that can be used to improve training performancein deep neural networks by keeping the values of a hidden layer in a range thatfacilitates gradient-based training.Layer norm is a variation of thez-scorefrom statistics, applied to a single vec-tor in a hidden layer. That is, the term layer norm is a bit confusing; layer normisnotapplied to an entire transformer layer, but just to the embedding vector of asingle token. Thus the input to layer norm is a single vector of dimensionalitydand the output is that vector normalized, again of dimensionalityd. The \ufb01rst step inlayer normalization is to calculate the mean,\u00b5, and standard deviation,s, over theelements of the vector to be normalized. Given an embedding vectorxof dimen-sionalityd, these values are calculated as follows.\u00b5=1ddXi=1xi(9.21)s=vuut1ddXi=1(xi\u0000\u00b5)2(9.22)Given these values, the vector components are normalized by subtracting the meanfrom each and dividing by the standard deviation. The result of this computation isa new vector with zero mean and a standard deviation of one.\u02c6x=(x\u0000\u00b5)s(9.23)Finally, in the standard implementation of layer normalization, two learnable param-eters,gandb, representing gain and offset values, are introduced.LayerNorm(x)=g(x\u0000\u00b5)s+b(9.24)Putting it all togetherThe function computed by a transformer block can be ex-pressed by breaking it down with one equation for each component computation,usingt(of shape[1\u21e5d]) to stand for transformer and superscripts to demarcateeach computation inside the block:t1i=LayerNorm(xi)(9.25)t2i=MultiHeadAttention(t1i,\u21e5x11,\u00b7\u00b7\u00b7,x1N\u21e4)(9.26)t3i=t2i+xi(9.27)t4i=LayerNorm(t3i)(9.28)t5i=FFN(t4i)(9.29)hi=t5i+t3i(9.30)Notice that the only component that takes as input information from other tokens(other residual streams) is multi-head attention, which (as we see from (9.27)) looksat all the neighboring tokens in the context. The output from attention, however, isthen added into this token\u2019s embedding stream. In fact,Elhage et al.(2021) show thatwe can view attention heads as literally moving information from the residual streamof a neighboring token into the current stream. The high-dimensional embeddingspace at each position thus contains information about the current token and aboutneighboring tokens, albeit in different subspaces of the vector space. Fig.9.7showsa visualization of this movement.",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 17,
      "token_count": 737,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 32\n\nPutting together a single transformer block9.2\u2022TRANSFORMERBLOCKS9of many forms of normalization that can be used to improve training performancein deep neural networks by keeping the values of a hidden layer in a range thatfacilitates gradient-based training.Layer norm is a variation of thez-scorefrom statistics, applied to a single vec-tor in a hidden layer. That is, the term layer norm is a bit confusing; layer normisnotapplied to an entire transformer layer, but just to the embedding vector of asingle token. Thus the input to layer norm is a single vector of dimensionalitydand the output is that vector normalized, again of dimensionalityd. The \ufb01rst step inlayer normalization is to calculate the mean,\u00b5, and standard deviation,s, over theelements of the vector to be normalized. Given an embedding vectorxof dimen-sionalityd, these values are calculated as follows.\u00b5=1ddXi=1xi(9.21)s=vuut1ddXi=1(xi\u0000\u00b5)2(9.22)Given these values, the vector components are normalized by subtracting the meanfrom each and dividing by the standard deviation. The result of this computation isa new vector with zero mean and a standard deviation of one.\u02c6x=(x\u0000\u00b5)s(9.23)Finally, in the standard implementation of layer normalization, two learnable param-eters,gandb, representing gain and offset values, are introduced.LayerNorm(x)=g(x\u0000\u00b5)s+b(9.24)Putting it all togetherThe function computed by a transformer block can be ex-pressed by breaking it down with one equation for each component computation,usingt(of shape[1\u21e5d]) to stand for transformer and superscripts to demarcateeach computation inside the block:t1i=LayerNorm(xi)(9.25)t2i=MultiHeadAttention(t1i,\u21e5x11,\u00b7\u00b7\u00b7,x1N\u21e4)(9.26)t3i=t2i+xi(9.27)t4i=LayerNorm(t3i)(9.28)t5i=FFN(t4i)(9.29)hi=t5i+t3i(9.30)Notice that the only component that takes as input information from other tokens(other residual streams) is multi-head attention, which (as we see from (9.27)) looksat all the neighboring tokens in the context. The output from attention, however, isthen added into this token\u2019s embedding stream. In fact,Elhage et al.(2021) show thatwe can view attention heads as literally moving information from the residual streamof a neighboring token into the current stream. The high-dimensional embeddingspace at each position thus contains information about the current token and aboutneighboring tokens, albeit in different subspaces of the vector space. Fig.9.7showsa visualization of this movement.Layer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1+\u2026\u2026\n\n## Page 33\n\nA transformer is a stack of these blocksso all the vectors are of the same dimensionality d\nLayer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1+\u2026\u2026Layer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1+\u2026\u2026\nBlock 1Block 2",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 18,
      "token_count": 727,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 33\n\nA transformer is a stack of these blocksso all the vectors are of the same dimensionality d\nLayer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1+\u2026\u2026Layer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1+\u2026\u2026\nBlock 1Block 2\n\n## Page 34\n\nResidual streams and attentionNotice that all  parts of the transformer block apply to 1 residual stream (1 token).Except attention, which takes information from other tokens Elhage et al. (2021) show that we can view attention heads as literally moving information from the residual stream of a neighboring token into the current stream .\nToken Aresidual streamToken Bresidual stream\n\n## Page 35\n\nTransformersThe Transformer Block\n\n## Page 36\n\nTransformersParallelizing Attention Computation",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 19,
      "token_count": 197,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 37",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 20,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Parallelizing computation using XFor attention/transformer block we've been computing a single output at a single time step i in a single residual stream. But we can pack the  N tokens of the input sequence into a single matrix X of size [N \u00d7 d]. Each row of X is the embedding of one token of the input. X can have 1K-32K rows, each of the dimensionality of the embedding d (the model dimension)9.3\u2022PARALLELIZING COMPUTATION USING A SINGLE MATRIXX11dimension).Parallelizing attentionLet\u2019s \ufb01rst see this for a single attention head and then turnto multiple heads, and then add in the rest of the components in the transformerblock. For one head we multiplyXby the key, query, and value matricesWQofshape[d\u21e5dk],WKof shape[d\u21e5dk], andWVof shape[d\u21e5dv], to produce matricesQof shape[N\u21e5dk],K2RN\u21e5dk, andV2RN\u21e5dv, containing all the key, query, andvalue vectors:Q=XWQ;K=XWK;V=XWV(9.31)Given these matrices we can compute all the requisite query-key comparisons simul-taneously by multiplyingQandK|in a single matrix multiplication. The product isof shapeN\u21e5N, visualized in Fig.9.9.q1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NNq1\u2022k2q1\u2022k3q1\u2022k4q2\u2022k3q2\u2022k4q3\u2022k4Figure 9.8TheN\u21e5NQK|matrix showing how it computes allqi\u00b7kjcomparisons in asingle matrix multiple.Once we have thisQK|matrix, we can very ef\ufb01ciently scale these scores, takethe softmax, and then multiply the result byVresulting in a matrix of shapeN\u21e5d:a vector embedding representation for each token in the input. We\u2019ve reduced theentire self-attention step for an entire sequence ofNtokens for one head to thefollowing computation:A=softmax\u2713mask\u2713QK|pdk\u25c6\u25c6V(9.32)Masking out the futureYou may have noticed that we introduced a mask functionin Eq.9.32above. This is because the self-attention computation as we\u2019ve describedit has a problem: the calculation inQK|results in a score for each query valueto every key value,including those that follow the query. This is inappropriate inthe setting of language modeling: guessing the next word is pretty simple if youalready know it! To \ufb01x this, the elements in the upper-triangular portion of thematrix are zeroed out (set to\u0000\u2022), thus eliminating any knowledge of words thatfollow in the sequence. This is done in practice by adding a mask matrixMinwhichMij=\u0000\u20228j>i(i.e. for the upper-triangular portion) andMij=0 otherwise.Fig.9.9shows the resulting maskedQK|matrix. (we\u2019ll see in Chapter 11 how tomake use of words in the future for tasks that need it).Fig.9.10shows a schematic of all the computations for a single attention headparallelized in matrix form.Fig.9.8and Fig.9.9also make it clear that attention is quadratic in the lengthof the input, since at each layer we need to compute dot products between each pairof tokens in the input. This makes it expensive to compute attention over very longdocuments (like entire",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 21,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "out the futureYou may have noticed that we introduced a mask functionin Eq.9.32above. This is because the self-attention computation as we\u2019ve describedit has a problem: the calculation inQK|results in a score for each query valueto every key value,including those that follow the query. This is inappropriate inthe setting of language modeling: guessing the next word is pretty simple if youalready know it! To \ufb01x this, the elements in the upper-triangular portion of thematrix are zeroed out (set to\u0000\u2022), thus eliminating any knowledge of words thatfollow in the sequence. This is done in practice by adding a mask matrixMinwhichMij=\u0000\u20228j>i(i.e. for the upper-triangular portion) andMij=0 otherwise.Fig.9.9shows the resulting maskedQK|matrix. (we\u2019ll see in Chapter 11 how tomake use of words in the future for tasks that need it).Fig.9.10shows a schematic of all the computations for a single attention headparallelized in matrix form.Fig.9.8and Fig.9.9also make it clear that attention is quadratic in the lengthof the input, since at each layer we need to compute dot products between each pairof tokens in the input. This makes it expensive to compute attention over very longdocuments (like entire novels). Nonetheless modern large language models manageto use quite long contexts of thousands or tens of thousands of tokens.",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 22,
      "token_count": 308,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 38\n\nQKTNow can do a single matrix multiply to combine Q and KTq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NNq1\u2022k2q1\u2022k3q1\u2022k4q2\u2022k3q2\u2022k4q3\u2022k4",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 23,
      "token_count": 100,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 39\n\nParallelizing attention\u2022Scale the  scores, take the softmax, and then multiply the result by V resulting in a matrix of shape N \u00d7 d\u2022An attention vector for each input token9.3\u2022PARALLELIZING COMPUTATION USING A SINGLE MATRIXX11dimension).Parallelizing attentionLet\u2019s \ufb01rst see this for a single attention head and then turnto multiple heads, and then add in the rest of the components in the transformerblock. For one head we multiplyXby the key, query, and value matricesWQofshape[d\u21e5dk],WKof shape[d\u21e5dk], andWVof shape[d\u21e5dv], to produce matricesQof shape[N\u21e5dk],K2RN\u21e5dk, andV2RN\u21e5dv, containing all the key, query, andvalue vectors:Q=XWQ;K=XWK;V=XWV(9.31)Given these matrices we can compute all the requisite query-key comparisons simul-taneously by multiplyingQandK|in a single matrix multiplication. The product isof shapeN\u21e5N, visualized in Fig.9.9.q1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NNq1\u2022k2q1\u2022k3q1\u2022k4q2\u2022k3q2\u2022k4q3\u2022k4Figure 9.8TheN\u21e5NQK|matrix showing how it computes allqi\u00b7kjcomparisons in asingle matrix multiple.Once we have thisQK|matrix, we can very ef\ufb01ciently scale these scores, takethe softmax, and then multiply the result byVresulting in a matrix of shapeN\u21e5d:a vector embedding representation for each token in the input. We\u2019ve reduced theentire self-attention step for an entire sequence ofNtokens for one head to thefollowing computation:A=softmax\u2713mask\u2713QK|pdk\u25c6\u25c6V(9.32)Masking out the futureYou may have noticed that we introduced a mask functionin Eq.9.32above. This is because the self-attention computation as we\u2019ve describedit has a problem: the calculation inQK|results in a score for each query valueto every key value,including those that follow the query. This is inappropriate inthe setting of language modeling: guessing the next word is pretty simple if youalready know it! To \ufb01x this, the elements in the upper-triangular portion of thematrix are zeroed out (set to\u0000\u2022), thus eliminating any knowledge of words thatfollow in the sequence. This is done in practice by adding a mask matrixMinwhichMij=\u0000\u20228j>i(i.e. for the upper-triangular portion) andMij=0 otherwise.Fig.9.9shows the resulting maskedQK|matrix. (we\u2019ll see in Chapter 11 how tomake use of words in the future for tasks that need it).Fig.9.10shows a schematic of all the computations for a single attention headparallelized in matrix form.Fig.9.8and Fig.9.9also make it clear that attention is quadratic in the lengthof the input, since at each layer we need to compute dot products between each pairof tokens in the input. This makes it expensive to compute attention over very longdocuments (like entire novels). Nonetheless modern large language models manageto use quite long contexts of thousands or tens of thousands of tokens.",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 24,
      "token_count": 770,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 40\n\nMasking out the future\u2022What is this mask function?QKT has a score for each query dot every key, including those that follow the query.\u2022Guessing the next word is pretty simple if you already know it! 9.3\u2022PARALLELIZING COMPUTATION USING A SINGLE MATRIXX11dimension).Parallelizing attentionLet\u2019s \ufb01rst see this for a single attention head and then turnto multiple heads, and then add in the rest of the components in the transformerblock. For one head we multiplyXby the key, query, and value matricesWQofshape[d\u21e5dk],WKof shape[d\u21e5dk], andWVof shape[d\u21e5dv], to produce matricesQof shape[N\u21e5dk],K2RN\u21e5dk, andV2RN\u21e5dv, containing all the key, query, andvalue vectors:Q=XWQ;K=XWK;V=XWV(9.31)Given these matrices we can compute all the requisite query-key comparisons simul-taneously by multiplyingQandK|in a single matrix multiplication. The product isof shapeN\u21e5N, visualized in Fig.9.9.q1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NNq1\u2022k2q1\u2022k3q1\u2022k4q2\u2022k3q2\u2022k4q3\u2022k4Figure 9.8TheN\u21e5NQK|matrix showing how it computes allqi\u00b7kjcomparisons in asingle matrix multiple.Once we have thisQK|matrix, we can very ef\ufb01ciently scale these scores, takethe softmax, and then multiply the result byVresulting in a matrix of shapeN\u21e5d:a vector embedding representation for each token in the input. We\u2019ve reduced theentire self-attention step for an entire sequence ofNtokens for one head to thefollowing computation:A=softmax\u2713mask\u2713QK|pdk\u25c6\u25c6V(9.32)Masking out the futureYou may have noticed that we introduced a mask functionin Eq.9.32above. This is because the self-attention computation as we\u2019ve describedit has a problem: the calculation inQK|results in a score for each query valueto every key value,including those that follow the query. This is inappropriate inthe setting of language modeling: guessing the next word is pretty simple if youalready know it! To \ufb01x this, the elements in the upper-triangular portion of thematrix are zeroed out (set to\u0000\u2022), thus eliminating any knowledge of words thatfollow in the sequence. This is done in practice by adding a mask matrixMinwhichMij=\u0000\u20228j>i(i.e. for the upper-triangular portion) andMij=0 otherwise.Fig.9.9shows the resulting maskedQK|matrix. (we\u2019ll see in Chapter 11 how tomake use of words in the future for tasks that need it).Fig.9.10shows a schematic of all the computations for a single attention headparallelized in matrix form.Fig.9.8and Fig.9.9also make it clear that attention is quadratic in the lengthof the input, since at each layer we need to compute dot products between each pairof tokens in the input. This makes it expensive to compute attention over very longdocuments (like entire novels). Nonetheless modern large language models manageto use quite long contexts of thousands or tens of thousands of tokens.",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 25,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 41",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 26,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Masking out the futureAdd \u2013\u221e to cells in upper triangleThe softmax will turn it to 09.3\u2022PARALLELIZING COMPUTATION USING A SINGLE MATRIXX11dimension).Parallelizing attentionLet\u2019s \ufb01rst see this for a single attention head and then turnto multiple heads, and then add in the rest of the components in the transformerblock. For one head we multiplyXby the key, query, and value matricesWQofshape[d\u21e5dk],WKof shape[d\u21e5dk], andWVof shape[d\u21e5dv], to produce matricesQof shape[N\u21e5dk],K2RN\u21e5dk, andV2RN\u21e5dv, containing all the key, query, andvalue vectors:Q=XWQ;K=XWK;V=XWV(9.31)Given these matrices we can compute all the requisite query-key comparisons simul-taneously by multiplyingQandK|in a single matrix multiplication. The product isof shapeN\u21e5N, visualized in Fig.9.9.q1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NNq1\u2022k2q1\u2022k3q1\u2022k4q2\u2022k3q2\u2022k4q3\u2022k4Figure 9.8TheN\u21e5NQK|matrix showing how it computes allqi\u00b7kjcomparisons in asingle matrix multiple.Once we have thisQK|matrix, we can very ef\ufb01ciently scale these scores, takethe softmax, and then multiply the result byVresulting in a matrix of shapeN\u21e5d:a vector embedding representation for each token in the input. We\u2019ve reduced theentire self-attention step for an entire sequence ofNtokens for one head to thefollowing computation:A=softmax\u2713mask\u2713QK|pdk\u25c6\u25c6V(9.32)Masking out the futureYou may have noticed that we introduced a mask functionin Eq.9.32above. This is because the self-attention computation as we\u2019ve describedit has a problem: the calculation inQK|results in a score for each query valueto every key value,including those that follow the query. This is inappropriate inthe setting of language modeling: guessing the next word is pretty simple if youalready know it! To \ufb01x this, the elements in the upper-triangular portion of thematrix are zeroed out (set to\u0000\u2022), thus eliminating any knowledge of words thatfollow in the sequence. This is done in practice by adding a mask matrixMinwhichMij=\u0000\u20228j>i(i.e. for the upper-triangular portion) andMij=0 otherwise.Fig.9.9shows the resulting maskedQK|matrix. (we\u2019ll see in Chapter 11 how tomake use of words in the future for tasks that need it).Fig.9.10shows a schematic of all the computations for a single attention headparallelized in matrix form.Fig.9.8and Fig.9.9also make it clear that attention is quadratic in the lengthof the input, since at each layer we need to compute dot products between each pairof tokens in the input. This makes it expensive to compute attention over very longdocuments (like entire novels). Nonetheless modern large language models manageto use quite long contexts of thousands or tens of thousands of",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 27,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "noticed that we introduced a mask functionin Eq.9.32above. This is because the self-attention computation as we\u2019ve describedit has a problem: the calculation inQK|results in a score for each query valueto every key value,including those that follow the query. This is inappropriate inthe setting of language modeling: guessing the next word is pretty simple if youalready know it! To \ufb01x this, the elements in the upper-triangular portion of thematrix are zeroed out (set to\u0000\u2022), thus eliminating any knowledge of words thatfollow in the sequence. This is done in practice by adding a mask matrixMinwhichMij=\u0000\u20228j>i(i.e. for the upper-triangular portion) andMij=0 otherwise.Fig.9.9shows the resulting maskedQK|matrix. (we\u2019ll see in Chapter 11 how tomake use of words in the future for tasks that need it).Fig.9.10shows a schematic of all the computations for a single attention headparallelized in matrix form.Fig.9.8and Fig.9.9also make it clear that attention is quadratic in the lengthof the input, since at each layer we need to compute dot products between each pairof tokens in the input. This makes it expensive to compute attention over very longdocuments (like entire novels). Nonetheless modern large language models manageto use quite long contexts of thousands or tens of thousands of tokens.q1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NN\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 28,
      "token_count": 370,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 42",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 29,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Another point: Attention is quadratic in length9.3\u2022PARALLELIZING COMPUTATION USING A SINGLE MATRIXX11dimension).Parallelizing attentionLet\u2019s \ufb01rst see this for a single attention head and then turnto multiple heads, and then add in the rest of the components in the transformerblock. For one head we multiplyXby the key, query, and value matricesWQofshape[d\u21e5dk],WKof shape[d\u21e5dk], andWVof shape[d\u21e5dv], to produce matricesQof shape[N\u21e5dk],K2RN\u21e5dk, andV2RN\u21e5dv, containing all the key, query, andvalue vectors:Q=XWQ;K=XWK;V=XWV(9.31)Given these matrices we can compute all the requisite query-key comparisons simul-taneously by multiplyingQandK|in a single matrix multiplication. The product isof shapeN\u21e5N, visualized in Fig.9.9.q1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NNq1\u2022k2q1\u2022k3q1\u2022k4q2\u2022k3q2\u2022k4q3\u2022k4Figure 9.8TheN\u21e5NQK|matrix showing how it computes allqi\u00b7kjcomparisons in asingle matrix multiple.Once we have thisQK|matrix, we can very ef\ufb01ciently scale these scores, takethe softmax, and then multiply the result byVresulting in a matrix of shapeN\u21e5d:a vector embedding representation for each token in the input. We\u2019ve reduced theentire self-attention step for an entire sequence ofNtokens for one head to thefollowing computation:A=softmax\u2713mask\u2713QK|pdk\u25c6\u25c6V(9.32)Masking out the futureYou may have noticed that we introduced a mask functionin Eq.9.32above. This is because the self-attention computation as we\u2019ve describedit has a problem: the calculation inQK|results in a score for each query valueto every key value,including those that follow the query. This is inappropriate inthe setting of language modeling: guessing the next word is pretty simple if youalready know it! To \ufb01x this, the elements in the upper-triangular portion of thematrix are zeroed out (set to\u0000\u2022), thus eliminating any knowledge of words thatfollow in the sequence. This is done in practice by adding a mask matrixMinwhichMij=\u0000\u20228j>i(i.e. for the upper-triangular portion) andMij=0 otherwise.Fig.9.9shows the resulting maskedQK|matrix. (we\u2019ll see in Chapter 11 how tomake use of words in the future for tasks that need it).Fig.9.10shows a schematic of all the computations for a single attention headparallelized in matrix form.Fig.9.8and Fig.9.9also make it clear that attention is quadratic in the lengthof the input, since at each layer we need to compute dot products between each pairof tokens in the input. This makes it expensive to compute attention over very longdocuments (like entire novels). Nonetheless modern large language models manageto use quite long contexts of thousands or tens of thousands of",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 30,
      "token_count": 734,
      "chapter_title": ""
    }
  },
  {
    "content": "noticed that we introduced a mask functionin Eq.9.32above. This is because the self-attention computation as we\u2019ve describedit has a problem: the calculation inQK|results in a score for each query valueto every key value,including those that follow the query. This is inappropriate inthe setting of language modeling: guessing the next word is pretty simple if youalready know it! To \ufb01x this, the elements in the upper-triangular portion of thematrix are zeroed out (set to\u0000\u2022), thus eliminating any knowledge of words thatfollow in the sequence. This is done in practice by adding a mask matrixMinwhichMij=\u0000\u20228j>i(i.e. for the upper-triangular portion) andMij=0 otherwise.Fig.9.9shows the resulting maskedQK|matrix. (we\u2019ll see in Chapter 11 how tomake use of words in the future for tasks that need it).Fig.9.10shows a schematic of all the computations for a single attention headparallelized in matrix form.Fig.9.8and Fig.9.9also make it clear that attention is quadratic in the lengthof the input, since at each layer we need to compute dot products between each pairof tokens in the input. This makes it expensive to compute attention over very longdocuments (like entire novels). Nonetheless modern large language models manageto use quite long contexts of thousands or tens of thousands of tokens.q1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NN\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 31,
      "token_count": 370,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 43",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 32,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Attention again\nq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2\u2022k2q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k2q3\u2022k3\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221eq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3q1\u2022k2q2\u2022k3q1\u2022k3q3\u2022k4q2\u2022k4q1\u2022k4x=QKT maskedmask=q1\u2022k1q2\u2022k1q4\u2022k1q3\u2022k1q1\u2022k1q1\u2022k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dvq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2\u2022k2q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k2q3\u2022k3\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221eq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3q1\u2022k2q2\u2022k3q1\u2022k3q3\u2022k4q2\u2022k4q1\u2022k4x=QKT maskedmask=q1\u2022k1q2\u2022k1q4\u2022k1q3\u2022k1q1\u2022k1q1\u2022k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 33,
      "token_count": 675,
      "chapter_title": ""
    }
  },
  {
    "content": "N x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dvq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2\u2022k2q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k2q3\u2022k3\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221eq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3q1\u2022k2q2\u2022k3q1\u2022k3q3\u2022k4q2\u2022k4q1\u2022k4x=QKT maskedmask=q1\u2022k1q2\u2022k1q4\u2022k1q3\u2022k1q1\u2022k1q1\u2022k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dv\nq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2\u2022k2q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k2q3\u2022k3\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221eq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3q1\u2022k2q2\u2022k3q1\u2022k3q3\u2022k4q2\u2022k4q1\u2022k4x=QKT maskedmask=q1\u2022k1q2\u2022k1q4\u2022k1q3\u2022k1q1\u2022k1q1\u2022k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 34,
      "token_count": 714,
      "chapter_title": ""
    }
  },
  {
    "content": "N x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dvq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2\u2022k2q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k2q3\u2022k3\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221eq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3q1\u2022k2q2\u2022k3q1\u2022k3q3\u2022k4q2\u2022k4q1\u2022k4x=QKT maskedmask=q1\u2022k1q2\u2022k1q4\u2022k1q3\u2022k1q1\u2022k1q1\u2022k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dv\nq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2\u2022k2q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k2q3\u2022k3\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221eq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3q1\u2022k2q2\u2022k3q1\u2022k3q3\u2022k4q2\u2022k4q1\u2022k4x=QKT maskedmask=q1\u2022k1q2\u2022k1q4\u2022k1q3\u2022k1q1\u2022k1q1\u2022k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 35,
      "token_count": 714,
      "chapter_title": ""
    }
  },
  {
    "content": "N x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dvq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2\u2022k2q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k2q3\u2022k3\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221eq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3q1\u2022k2q2\u2022k3q1\u2022k3q3\u2022k4q2\u2022k4q1\u2022k4x=QKT maskedmask=q1\u2022k1q2\u2022k1q4\u2022k1q3\u2022k1q1\u2022k1q1\u2022k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dv",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 36,
      "token_count": 398,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 44\n\nParallelizing Multi-head Attention9.4\u2022THE INPUT:EMBEDDINGS FOR TOKEN AND POSITION13the self-attention outputAof shape [N\u21e5d].Qi=XWQi;Ki=XWKi;Vi=XWVi(9.33)headi=SelfAttention(Qi,Ki,Vi)=softmax\u2713QiKi|pdk\u25c6Vi(9.34)MultiHeadAttention(X)=(head1\u0000head2...\u0000headh)WO(9.35)Putting it all together with the parallel input matrixXThe function computedin parallel by an entire layer ofNtransformer block over the entireNinput tokenscan be expressed as:O=LayerNorm(X+MultiHeadAttention(X))(9.36)H=LayerNorm(O+FFN(O))(9.37)Or we can break it down with one equation for each component computation, usingT(of shape[N\u21e5d]) to stand for transformer and superscripts to demarcate eachcomputation inside the block:T1=MultiHeadAttention(X)(9.38)T2=X+T1(9.39)T3=LayerNorm(T2)(9.40)T4=FFN(T3)(9.41)T5=T4+T3(9.42)H=LayerNorm(T5)(9.43)Here when we use a notation like FFN(T3)we mean that the same FFN is appliedin parallel to each of theNembedding vectors in the window. Similarly, each of theNtokens is normed in parallel in the LayerNorm. Crucially, the input and outputdimensions of transformer blocks are matched so they can be stacked. Since eachtokenxiat the input to the block has dimensionalityd, that means the inputXandoutputHare both of shape[N\u21e5d].9.4 The input: embeddings for token and positionLet\u2019s talk about where the inputXcomes from. Given a sequence ofNtokens (Nisthe context length in tokens), the matrixXof shape[N\u21e5d]has anembeddingforembeddingeach word in the context. The transformer does this by separately computing twoembeddings: an input token embedding, and an input positional embedding.A token embedding, introduced in Chapter 7 and Chapter 8, is a vector of di-mensiondthat will be our initial representation for the input token. (As we passvectors up through the transformer layers in the residual stream, this embeddingrepresentation will change and grow, incorporating context and playing a differentrole depending on the kind of language model we are building.) The set of initialembeddings are stored in the embedding matrixE, which has a row for each of the|V|tokens in the vocabulary. Thus each word is a row vector ofddimensions, andEhas shape[|V|\u21e5d].Given an input token string likeThanks for all thewe \ufb01rst convert the tokensinto vocabulary indices (these were created when we \ufb01rst tokenized the input using",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 37,
      "token_count": 635,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 45",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 38,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Parallelizing Multi-head Attentionor9.4\u2022THE INPUT:EMBEDDINGS FOR TOKEN AND POSITION13the self-attention outputAof shape [N\u21e5d].Qi=XWQi;Ki=XWKi;Vi=XWVi(9.33)headi=SelfAttention(Qi,Ki,Vi)=softmax\u2713QiKi|pdk\u25c6Vi(9.34)MultiHeadAttention(X)=(head1\u0000head2...\u0000headh)WO(9.35)Putting it all together with the parallel input matrixXThe function computedin parallel by an entire layer ofNtransformer block over the entireNinput tokenscan be expressed as:O=LayerNorm(X+MultiHeadAttention(X))(9.36)H=LayerNorm(O+FFN(O))(9.37)Or we can break it down with one equation for each component computation, usingT(of shape[N\u21e5d]) to stand for transformer and superscripts to demarcate eachcomputation inside the block:T1=MultiHeadAttention(X)(9.38)T2=X+T1(9.39)T3=LayerNorm(T2)(9.40)T4=FFN(T3)(9.41)T5=T4+T3(9.42)H=LayerNorm(T5)(9.43)Here when we use a notation like FFN(T3)we mean that the same FFN is appliedin parallel to each of theNembedding vectors in the window. Similarly, each of theNtokens is normed in parallel in the LayerNorm. Crucially, the input and outputdimensions of transformer blocks are matched so they can be stacked. Since eachtokenxiat the input to the block has dimensionalityd, that means the inputXandoutputHare both of shape[N\u21e5d].9.4 The input: embeddings for token and positionLet\u2019s talk about where the inputXcomes from. Given a sequence ofNtokens (Nisthe context length in tokens), the matrixXof shape[N\u21e5d]has anembeddingforembeddingeach word in the context. The transformer does this by separately computing twoembeddings: an input token embedding, and an input positional embedding.A token embedding, introduced in Chapter 7 and Chapter 8, is a vector of di-mensiondthat will be our initial representation for the input token. (As we passvectors up through the transformer layers in the residual stream, this embeddingrepresentation will change and grow, incorporating context and playing a differentrole depending on the kind of language model we are building.) The set of initialembeddings are stored in the embedding matrixE, which has a row for each of the|V|tokens in the vocabulary. Thus each word is a row vector ofddimensions, andEhas shape[|V|\u21e5d].Given an input token string likeThanks for all thewe \ufb01rst convert the tokensinto vocabulary indices (these were created when we \ufb01rst tokenized the input using9.4\u2022THE INPUT:EMBEDDINGS FOR TOKEN AND POSITION13the self-attention outputAof shape [N\u21e5d].Qi=XWQi;Ki=XWKi;Vi=XWVi(9.33)headi=SelfAttention(Qi,Ki,Vi)=softmax\u2713QiKi|pdk\u25c6Vi(9.34)MultiHeadAttention(X)=(head1\u0000head2...\u0000headh)WO(9.35)Putting it all together with the parallel input matrixXThe function computedin parallel by an entire layer ofNtransformer block over the entireNinput tokenscan be expressed as:O=LayerNorm(X+MultiHeadAttention(X))(9.36)H=LayerNorm(O+FFN(O))(9.37)Or we can break it",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 39,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "we passvectors up through the transformer layers in the residual stream, this embeddingrepresentation will change and grow, incorporating context and playing a differentrole depending on the kind of language model we are building.) The set of initialembeddings are stored in the embedding matrixE, which has a row for each of the|V|tokens in the vocabulary. Thus each word is a row vector ofddimensions, andEhas shape[|V|\u21e5d].Given an input token string likeThanks for all thewe \ufb01rst convert the tokensinto vocabulary indices (these were created when we \ufb01rst tokenized the input using9.4\u2022THE INPUT:EMBEDDINGS FOR TOKEN AND POSITION13the self-attention outputAof shape [N\u21e5d].Qi=XWQi;Ki=XWKi;Vi=XWVi(9.33)headi=SelfAttention(Qi,Ki,Vi)=softmax\u2713QiKi|pdk\u25c6Vi(9.34)MultiHeadAttention(X)=(head1\u0000head2...\u0000headh)WO(9.35)Putting it all together with the parallel input matrixXThe function computedin parallel by an entire layer ofNtransformer block over the entireNinput tokenscan be expressed as:O=LayerNorm(X+MultiHeadAttention(X))(9.36)H=LayerNorm(O+FFN(O))(9.37)Or we can break it down with one equation for each component computation, usingT(of shape[N\u21e5d]) to stand for transformer and superscripts to demarcate eachcomputation inside the block:T1=MultiHeadAttention(X)(9.38)T2=X+T1(9.39)T3=LayerNorm(T2)(9.40)T4=FFN(T3)(9.41)T5=T4+T3(9.42)H=LayerNorm(T5)(9.43)Here when we use a notation like FFN(T3)we mean that the same FFN is appliedin parallel to each of theNembedding vectors in the window. Similarly, each of theNtokens is normed in parallel in the LayerNorm. Crucially, the input and outputdimensions of transformer blocks are matched so they can be stacked. Since eachtokenxiat the input to the block has dimensionalityd, that means the inputXandoutputHare both of shape[N\u21e5d].9.4 The input: embeddings for token and positionLet\u2019s talk about where the inputXcomes from. Given a sequence ofNtokens (Nisthe context length in tokens), the matrixXof shape[N\u21e5d]has anembeddingforembeddingeach word in the context. The transformer does this by separately computing twoembeddings: an input token embedding, and an input positional embedding.A token embedding, introduced in Chapter 7 and Chapter 8, is a vector of di-mensiondthat will be our initial representation for the input token. (As we passvectors up through the transformer layers in the residual stream, this embeddingrepresentation will change and grow, incorporating context and playing a differentrole depending on the kind of language model we are building.) The set of initialembeddings are stored in the embedding matrixE, which has a row for each of the|V|tokens in the vocabulary. Thus each word is a row vector ofddimensions, andEhas shape[|V|\u21e5d].Given an input token string likeThanks for all thewe \ufb01rst convert the tokensinto vocabulary indices (these were created when we \ufb01rst tokenized the input using",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 40,
      "token_count": 756,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 46\n\nTransformersParallelizing Attention Computation\n\n## Page 47\n\nTransformersInput and output: Position embeddings and the Language Model Head\n\n## Page 48\n\nToken and Position EmbeddingsThe matrix X (of shape [N \u00d7 d]) has an embedding for each word in the context. This embedding is created by adding two distinct embedding for each input\u2022token embedding\u2022positional embedding\n\n## Page 49\n\nToken EmbeddingsEmbedding matrix E has shape [|V | \u00d7  d ]. \u2022One row for each of the |V | tokens in the vocabulary. \u2022Each word is a row vector of d dimensionsGiven:  string \"Thanks for all the\"1. Tokenize with BPE and convert into vocab indicesw = [5,4000,10532,2224] 2. Select the corresponding rows from E, each row an embedding\u2022  (row 5, row 4000, row 10532, row 2224). \n\n## Page 50\n\nPosition EmbeddingsThere are many methods, but we'll just describe the simplest: absolute position.Goal: learn a position embedding matrix Epos of shape [1 \u00d7 N ]. Start with randomly initialized embeddings\u2022one for each integer up to some maximum length. \u2022i.e., just as we have an embedding for token fish, we\u2019ll have an embedding for position 3 and position 17.\u2022As with word embeddings, these position embeddings are learned along with other parameters during training. \n\n## Page 51\n\nEach x is just the sum of word and position embeddings\nX = CompositeEmbeddings(word + position)Transformer BlockJanet1will2back3Janetwillbackthebillthe4bill5\n+++++PositionEmbeddingsWordEmbeddings\n\n## Page 52\n\nLanguage modeling head\nLayer LTransformerBlockSoftmax over vocabulary VUnembedding layer\u20261 x |V|Logits Word probabilities1 x |V|hL1w1w2wNhL2hLNd x |V|1 x d   Unembedding    layer = ETy1y2y|V|\u2026u1u2u|V|\u2026Language Model Headtakes hLN and outputs adistribution over vocabulary V\n\n## Page 53\n\nLanguage modeling head\nLayer LTransformerBlockSoftmax over vocabulary VUnembedding layer\u20261 x |V|Logits Word probabilities1 x |V|hL1w1w2wNhL2hLNd x |V|1 x d   Unembedding    layer = ETy1y2y|V|\u2026u1u2u|V|\u2026Language Model Headtakes hLN and outputs adistribution over vocabulary VUnembedding layer:  linear layer projects from hLN (shape [1 \u00d7 d]) to logit vector Why \"unembedding\"? Tied to ETWeight tying, we use the same weights for two different matricesUnembedding layer maps from an embedding to a 1x|V| vector of logits",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 41,
      "token_count": 609,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 54",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 42,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Language modeling head\nLayer LTransformerBlockSoftmax over vocabulary VUnembedding layer\u20261 x |V|Logits Word probabilities1 x |V|hL1w1w2wNhL2hLNd x |V|1 x d   Unembedding    layer = ETy1y2y|V|\u2026u1u2u|V|\u2026Language Model Headtakes hLN and outputs adistribution over vocabulary VLogits, the score vector uOne score for each of the |V | possible words in the vocabulary V . Shape 1 \u00d7 |V |. Softmax turns the logits into probabilities over vocabulary. Shape 1 \u00d7 |V |. 16CHAPTER9\u2022THETRANSFORMERlanguage models of Chapter 3 compute the probability of a word given counts ofits occurrence with then\u00001 prior words. The context is thus of sizen\u00001. Fortransformer language models, the context is the size of the transformer\u2019s contextwindow, which can be quite large: 2K, 4K, even 32K tokens for very large models.The job of the language modeling head is to take the output of the \ufb01nal trans-former layer from the last tokenNand use it to predict the upcoming word at posi-tionN+1. Fig.9.14shows how to accomplish this task, taking the output of the lasttoken at the last layer (thed-dimensional output embedding of shape[1\u21e5d]) andproducing a probability distribution over words (from which we will choose one togenerate).\nLayer LTransformerBlockSoftmax over vocabulary VUnembedding layer\u20261 x |V|Logits Word probabilities1 x |V|hL1w1w2wNhL2hLNd x |V|1 x d   Unembedding layerU = ETy1y2y|V|\u2026u1u2u|V|\u2026Language Model Headtakes hLN and outputs adistribution over vocabulary V",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 43,
      "token_count": 406,
      "chapter_title": ""
    }
  },
  {
    "content": "Layer LTransformerBlockSoftmax over vocabulary VUnembedding layer\u20261 x |V|Logits Word probabilities1 x |V|hL1w1w2wNhL2hLNd x |V|1 x d   Unembedding layerU = ETy1y2y|V|\u2026u1u2u|V|\u2026Language Model Headtakes hLN and outputs adistribution over vocabulary V\nFigure 9.14The language modeling head: the circuit at the top of a transformer that maps from the outputembedding for tokenNfrom the last transformer layer (hLN) to a probability distribution over words in thevocabularyV.The \ufb01rst module in Fig.9.14is a linear layer, whose job is to project from theoutputhLN, which represents the output token embedding at positionNfrom the \ufb01nalblockL, (hence of shape[1\u21e5d]) to thelogitvector, or score vector, that will have alogitsingle score for each of the|V|possible words in the vocabularyV. The logit vectoruis thus of dimensionality 1\u21e5|V|.This linear layer can be learned, but more commonly we tie this matrix to (thetranspose of) the embedding matrixE. Recall that inweight tying, we use theweight tyingsame weights for two different matrices in the model. Thus at the input stage of thetransformer the embedding matrix (of shape[|V|\u21e5d]) is used to map from a one-hotvector over the vocabulary (of shape[1\u21e5|V|]) to an embedding (of shape[1\u21e5d]).And then in the language model head,ET, the transpose of the embedding matrix (ofshape[d\u21e5|V|]) is used to map back from an embedding (shape[1\u21e5d]) to a vectorover the vocabulary (shape [1\u21e5|V|]). In the learning process,Ewill be optimized tobe good at doing both of these mappings. We therefore sometimes call the transposeETtheunembeddinglayer because it is performing this reverse mapping.unembeddingA softmax layer turns the logitsuinto the probabilitiesyover the vocabulary.u=hLNET(9.44)y=softmax(u)(9.45)We can use these probabilities to do things like help assign a probability to agiven text. But the most important usage to generate text, which we do bysampling",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 44,
      "token_count": 510,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 55\n\nThe final transformermodel\nwiSample token togenerate at position i+1\nfeedforwardlayer normattentionlayer norm\nU\nInput tokenLanguageModelingHead\nInputEncoding\nEi+\u2026logits\nfeedforwardlayer normattentionlayer normLayer 1Layer 2h1i  =  x2ix1ih2i  =  x3ifeedforwardlayer normattentionlayer normhLi  hL-1i  =  xLiy1y2y|V|\u2026Token probabilitiesu1u2u|V|\u2026softmaxwi+1\nLayer L\nwiSample token togenerate at position i+1\nfeedforwardlayer normattentionlayer norm\nU\nInput tokenLanguageModelingHead\nInputEncoding\nEi+\u2026logits\nfeedforwardlayer normattentionlayer normLayer 1Layer 2h1i  =  x2ix1ih2i  =  x3ifeedforwardlayer normattentionlayer normhLi  hL-1i  =  xLiy1y2y|V|\u2026Token probabilitiesu1u2u|V|\u2026softmaxwi+1\nLayer L\n\n## Page 56\n\nTransformersInput and output: Position embeddings and the Language Model Head",
    "metadata": {
      "source": "transformer24aug",
      "chunk_id": 45,
      "token_count": 264,
      "chapter_title": ""
    }
  }
]