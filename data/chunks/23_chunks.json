[
  {
    "content": "# 23\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n23Coreference Resolution and\nEntity Linking\nand even Stigand, the patriotic archbishop of Canterbury, found it advisable\u2013\u201d\u2019\n\u2018Found WHAT?\u2019 said the Duck.\n\u2018Found IT, \u2019 the Mouse replied rather crossly: \u2018of course you know what \u201cit\u201dmeans. \u2019\n\u2018I know what \u201cit\u201dmeans well enough, when I \ufb01nd a thing, \u2019 said the Duck: \u2018it\u2019s gener-\nally a frog or a worm. The question is, what did the archbishop \ufb01nd?\u2019\nLewis Carroll, Alice in Wonderland\nAn important component of language processing is knowing who is being talked\nabout in a text. Consider the following passage:\n(23.1) Victoria Chen , CFO of Megabucks Banking, saw her pay jump to $2.3\nmillion, as the 38-year-old became the company\u2019s president. It is widely\nknown that she came to Megabucks from rival Lotsabucks.\nEach of the underlined phrases in this passage is used by the writer to refer to\na person named Victoria Chen. We call linguistic expressions like herorVictoria\nChen mentions orreferring expressions , and the discourse entity that is referred mention\nto (Victoria Chen) the referent . (To distinguish between referring expressions and referent\ntheir referents, we italicize the former.)1Two or more referring expressions that are\nused to refer to the same discourse entity are said to corefer ; thus, Victoria Chen corefer\nandshecorefer in (23.1).\nCoreference is an important component of natural language processing. A dia-\nlogue system that has just told the user \u201cThere is a 2pm \ufb02ight on United and a 4pm\none on Cathay Paci\ufb01c\u201d must know which \ufb02ight the user means by \u201cI\u2019ll take the sec-\nond one\u201d . A question answering system that uses Wikipedia to answer a question\nabout Marie Curie must know who shewas in the sentence \u201cShe was born in War-\nsaw\u201d . And a machine translation system translating from a language like Spanish, in\nwhich pronouns can be dropped, must use coreference from the previous sentence to\ndecide whether the Spanish sentence \u2018 \u201cMe encanta el conocimiento\u201d, dice. \u2019 should\nbe translated as \u2018 \u201cI love knowledge\u201d, he says \u2019, or \u2018 \u201cI love knowledge\u201d, she says \u2019.\nIndeed, this example comes from an actual news article in El Pa \u00b4\u0131sabout a female\nprofessor and was mistranslated as \u201che\u201d in machine translation because of inaccurate\ncoreference resolution (Schiebinger, 2013).\nNatural language processing systems (and humans) interpret linguistic expres-\nsions with respect to a discourse model (Karttunen, 1969). A discourse modeldiscourse\nmodel\n(Fig. 23.1) is a mental model that the understander builds incrementally when in-\nterpreting a text, containing representations of the entities referred to in the text,\nas well as properties of the entities and relations among them. When a referent is\n\ufb01rst mentioned in a discourse, we say that a representation for it is evoked into the evoked\nmodel. Upon subsequent mention, this representation is accessed from the model. accessed\n1As a convenient shorthand, we sometimes speak of a referring expression referring to a referent, e.g.,\nsaying that sherefers to Victoria Chen. However, the reader should keep in mind that what we really\nmean is that the speaker is performing the act of referring to Victoria Chen by uttering she.",
    "metadata": {
      "source": "23",
      "chunk_id": 0,
      "token_count": 799,
      "chapter_title": "23"
    }
  },
  {
    "content": "## Page 2\n\n2CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\nVDiscourse Model\u201cVictoria\u201d\u201cshe\u201dcoreferrefer (evoke)refer (access)$LotsabucksMegabuckspay\nFigure 23.1 How mentions evoke and access discourse entities in a discourse model.\nReference in a text to an entity that has been previously introduced into the\ndiscourse is called anaphora , and the referring expression used is said to be an anaphora\nanaphor , or anaphoric.2In passage (23.1), the pronouns sheandherand the de\ufb01- anaphor\nnite NP the 38-year-old are therefore anaphoric. The anaphor corefers with a prior\nmention (in this case Victoria Chen ) that is called the antecedent . Not every refer- antecedent\nring expression is an antecedent. An entity that has only a single mention in a text\n(like Lotsabucks in (23.1)) is called a singleton . singleton\nIn this chapter we focus on the task of coreference resolution . Coreferencecoreference\nresolution\nresolution is the task of determining whether two mentions corefer , by which we\nmean they refer to the same entity in the discourse model (the same discourse entity ).\nThe set of coreferring expressions is often called a coreference chain or acluster .coreference\nchain\ncluster For example, in processing (23.1), a coreference resolution algorithm would need\nto \ufb01nd at least four coreference chains, corresponding to the four entities in the\ndiscourse model in Fig. 23.1.\n1.fVictoria Chen ,her,the 38-year-old ,Sheg\n2.fMegabucks Banking ,the company ,Megabucksg\n3.fher payg\n4.fLotsabucksg\nNote that mentions can be nested; for example the mention heris syntactically\npart of another mention, her pay , referring to a completely different discourse entity.\nCoreference resolution thus comprises two tasks (although they are often per-\nformed jointly): (1) identifying the mentions, and (2) clustering them into corefer-\nence chains/discourse entities.\nWe said that two mentions corefered if they are associated with the same dis-\ncourse entity . But often we\u2019d like to go further, deciding which real world entity is\nassociated with this discourse entity. For example, the mention Washington might\nrefer to the US state, or the capital city, or the person George Washington; the inter-\npretation of the sentence will of course be very different for each of these. The task\nofentity linking (Ji and Grishman, 2011) or entity resolution is the task of mapping entity linking\na discourse entity to some real-world individual.3We usually operationalize entity\n2We will follow the common NLP usage of anaphor to mean any mention that has an antecedent, rather\nthan the more narrow usage to mean only mentions (like pronouns) whose interpretation depends on the\nantecedent (under the narrower interpretation, repeated names are not anaphors).\n3Computational linguistics/NLP thus differs in its use of the term reference from the \ufb01eld of formal\nsemantics, which uses the words reference andcoreference to describe the relation between a mention\nand a real-world entity. By contrast, we follow the functional linguistics tradition in which a mention\nrefers to adiscourse entity (Webber, 1978) and the relation between a discourse entity and the real world\nindividual requires an additional step of linking .",
    "metadata": {
      "source": "23",
      "chunk_id": 1,
      "token_count": 756,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3",
    "metadata": {
      "source": "23",
      "chunk_id": 2,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "3\nlinking or resolution by mapping to an ontology : a list of entities in the world, like\na gazeteer (Appendix F). Perhaps the most common ontology used for this task is\nWikipedia; each Wikipedia page acts as the unique id for a particular entity. Thus\nthe entity linking task of wiki\ufb01cation (Mihalcea and Csomai, 2007) is the task of de-\nciding which Wikipedia page corresponding to an individual is being referred to by\na mention. But entity linking can be done with any ontology; for example if we have\nan ontology of genes, we can link mentions of genes in text to the disambiguated\ngene name in the ontology.\nIn the next sections we introduce the task of coreference resolution in more de-\ntail, and survey a variety of architectures for resolution. We also introduce two\narchitectures for the task of entity linking.\nBefore turning to algorithms, however, we mention some important tasks we\nwill only touch on brie\ufb02y at the end of this chapter. First are the famous Winograd\nSchema problems (so-called because they were \ufb01rst pointed out by Terry Winograd\nin his dissertation). These entity coreference resolution problems are designed to be\ntoo dif\ufb01cult to be solved by the resolution methods we describe in this chapter, and\nthe kind of real-world knowledge they require has made them a kind of challenge\ntask for natural language processing. For example, consider the task of determining\nthe correct antecedent of the pronoun they in the following example:\n(23.2) The city council denied the demonstrators a permit because\na. they feared violence.\nb. they advocated violence.\nDetermining the correct antecedent for the pronoun they requires understanding\nthat the second clause is intended as an explanation of the \ufb01rst clause, and also\nthat city councils are perhaps more likely than demonstrators to fear violence and\nthat demonstrators might be more likely to advocate violence. Solving Winograd\nSchema problems requires \ufb01nding way to represent or discover the necessary real\nworld knowledge.\nA problem we won\u2019t discuss in this chapter is the related task of event corefer-\nence , deciding whether two event mentions (such as the buyand the acquisition inevent\ncoreference\nthese two sentences from the ECB+ corpus) refer to the same event:\n(23.3) AMD agreed to [ buy] Markham, Ontario-based ATI for around $5.4 billion\nin cash and stock, the companies announced Monday.\n(23.4) The [ acquisition ] would turn AMD into one of the world\u2019s largest providers\nof graphics chips.\nEvent mentions are much harder to detect than entity mentions, since they can be ver-\nbal as well as nominal. Once detected, the same mention-pair and mention-ranking\nmodels used for entities are often applied to events.\nAn even more complex kind of coreference is discourse deixis (Webber, 1988), discourse deixis\nin which an anaphor refers back to a discourse segment, which can be quite hard to\ndelimit or categorize, like the examples in (23.5) adapted from Webber (1991):\n(23.5) According to Soleil, Beau just opened a restaurant\na. But thatturned out to be a lie.\nb. But thatwas false.\nc.That struck me as a funny way to describe the situation.\nThe referent of that is a speech act (see Chapter 15) in (23.5a), a proposition in\n(23.5b), and a manner of description in (23.5c). We don\u2019t give algorithms in this\nchapter for these dif\ufb01cult types of non-nominal antecedents , but see Kolhatkar\net al. (2018) for a survey.",
    "metadata": {
      "source": "23",
      "chunk_id": 3,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\n23.1 Coreference Phenomena: Linguistic Background\nWe now offer some linguistic background on reference phenomena. We introduce\nthe four types of referring expressions (de\ufb01nite and inde\ufb01nite NPs, pronouns, and\nnames), describe how these are used to evoke and access entities in the discourse\nmodel, and talk about linguistic features of the anaphor/antecedent relation (like\nnumber/gender agreement, or properties of verb semantics).\n23.1.1 Types of Referring Expressions\nInde\ufb01nite Noun Phrases: The most common form of inde\ufb01nite reference in En-\nglish is marked with the determiner a(oran), but it can also be marked by a quan-\nti\ufb01er such as some or even the determiner this. Inde\ufb01nite reference generally intro-\nduces into the discourse context entities that are new to the hearer.\n(23.6) a. Mrs. Martin was so very kind as to send Mrs. Goddard a beautiful goose .\nb. He had gone round one day to bring her some walnuts .\nc. I saw this beautiful cauli\ufb02ower today.\nDe\ufb01nite Noun Phrases: De\ufb01nite reference, such as via NPs that use the English\narticle the, refers to an entity that is identi\ufb01able to the hearer. An entity can be\nidenti\ufb01able to the hearer because it has been mentioned previously in the text and\nthus is already represented in the discourse model:\n(23.7) It concerns a white stallion which I have sold to an of\ufb01cer. But the pedigree\nofthe white stallion was not fully established.\nAlternatively, an entity can be identi\ufb01able because it is contained in the hearer\u2019s\nset of beliefs about the world, or the uniqueness of the object is implied by the\ndescription itself, in which case it evokes a representation of the referent into the\ndiscourse model, as in (23.9):\n(23.8) I read about it in the New York Times .\n(23.9) Have you seen the car keys?\nThese last uses are quite common; more than half of de\ufb01nite NPs in newswire\ntexts are non-anaphoric, often because they are the \ufb01rst time an entity is mentioned\n(Poesio and Vieira 1998, Bean and Riloff 1999).\nPronouns: Another form of de\ufb01nite reference is pronominalization, used for enti-\nties that are extremely salient in the discourse, (as we discuss below):\n(23.10) Emma smiled and chatted as cheerfully as shecould,\nPronouns can also participate in cataphora , in which they are mentioned before cataphora\ntheir referents are, as in (23.11).\n(23.11) Even before shesawit, Dorothy had been thinking about the Emerald City\nevery day.\nHere, the pronouns sheanditboth occur before their referents are introduced.\nPronouns also appear in quanti\ufb01ed contexts in which they are considered to be\nbound , as in (23.12). bound\n(23.12) Every dancer brought herleft arm forward.\nUnder the relevant reading, herdoes not refer to some woman in context, but instead\nbehaves like a variable bound to the quanti\ufb01ed expression every dancer . We are not\nconcerned with the bound interpretation of pronouns in this chapter.",
    "metadata": {
      "source": "23",
      "chunk_id": 4,
      "token_count": 765,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\n23.1 \u2022 C OREFERENCE PHENOMENA : LINGUISTIC BACKGROUND 5\nIn some languages, pronouns can appear as clitics attached to a word, like lo\n(\u2018it\u2019) in this Spanish example from AnCora (Recasens and Mart \u00b4\u0131, 2010):\n(23.13) La intenci \u00b4on es reconocer el gran prestigio que tiene la marat \u00b4on y unir lo\ncon esta gran carrera.\n\u2018The aim is to recognize the great prestige that the Marathon has and join jit\nwith this great race.\u201d\nDemonstrative Pronouns: Demonstrative pronouns thisandthat can appear ei-\nther alone or as determiners, for instance, this ingredient ,that spice :\n(23.14) I just bought a copy of Thoreau\u2019s Walden . I had bought one \ufb01ve years ago.\nThat one had been very tattered; this one was in much better condition.\nNote that this NP is ambiguous; in colloquial spoken English, it can be inde\ufb01nite,\nas in (23.6), or de\ufb01nite, as in (23.14).\nZero Anaphora: Instead of using a pronoun, in some languages (including Chi-\nnese, Japanese, and Italian) it is possible to have an anaphor that has no lexical\nrealization at all, called a zero anaphor or zero pronoun, as in the following Italian zero anaphor\nand Japanese examples from Poesio et al. (2016):\n(23.15) EN [John] iwent to visit some friends. On the way [he] ibought some\nwine.\nIT [Giovanni] iand`o a far visita a degli amici. Per via ficompr `o del vino.\nJA [John] i-wa yujin-o houmon-sita. Tochu-de fiwain-o ka-tta.\nor this Chinese example:\n(23.16) [ \u6211]\u524d\u4e00\u4f1a\u7cbe\u795e\u4e0a\u592a\u7d27\u5f20\u3002[0]\u73b0\u5728\u6bd4\u8f83\u5e73\u9759\u4e86\n[I] was too nervous a while ago. ... [0] am now calmer.\nZero anaphors complicate the task of mention detection in these languages.\nNames: Names (such as of people, locations, or organizations) can be used to refer\nto both new and old entities in the discourse:\n(23.17) a. Miss Woodhouse certainly had not done him justice.\nb.International Business Machines sought patent compensation\nfrom Amazon; IBM had previously sued other companies.\n23.1.2 Information Status\nThe way referring expressions are used to evoke new referents into the discourse\n(introducing new information), or access old entities from the model (old informa-\ntion), is called their information status orinformation structure . Entities can beinformation\nstatus\ndiscourse-new ordiscourse-old , and indeed it is common to distinguish at least discourse-new\ndiscourse-old three kinds of entities informationally (Prince, 1981):\nnew NPs:\nbrand new NPs: these introduce entities that are discourse-new and hearer-\nnew like a fruit orsome walnuts .\nunused NPs: these introduce entities that are discourse-new but hearer-old\n(like Hong Kong ,Marie Curie , orthe New York Times .\nold NPs: also called evoked NPs , these introduce entities that already in the dis-\ncourse model, hence are both discourse-old and hearer-old, like itin \u201cI went\nto a new restaurant. It was... \u201d.",
    "metadata": {
      "source": "23",
      "chunk_id": 5,
      "token_count": 760,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6",
    "metadata": {
      "source": "23",
      "chunk_id": 6,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "6CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\ninferrables: these introduce entities that are neither hearer-old nor discourse-old,\nbut the hearer can infer their existence by reasoning based on other entities\nthat are in the discourse. Consider the following examples:\n(23.18) I went to a superb restaurant yesterday. The chef had just opened it.\n(23.19) Mix \ufb02our, butter and water. Knead the dough until shiny.\nNeither the chef northe dough were in the discourse model based on the \ufb01rst\nsentence of either example, but the reader can make a bridging inferencebridging\ninference\nthat these entities should be added to the discourse model and associated with\nthe restaurant and the ingredients, based on world knowledge that restaurants\nhave chefs and dough is the result of mixing \ufb02our and liquid (Haviland and\nClark 1974, Webber and Baldwin 1992, Nissim et al. 2004, Hou et al. 2018).\nThe form of an NP gives strong clues to its information status. We often talk\nabout an entity\u2019s position on the given-new dimension, the extent to which the refer- given-new\nent is given (salient in the discourse, easier for the hearer to call to mind, predictable\nby the hearer), versus new (non-salient in the discourse, unpredictable) (Chafe 1976,\nPrince 1981, Gundel et al. 1993). A referent that is very accessible (Ariel, 2001) accessible\ni.e., very salient in the hearer\u2019s mind or easy to call to mind, can be referred to with\nless linguistic material. For example pronouns are used only when the referent has\na high degree of activation or salience in the discourse model.4By contrast, less salience\nsalient entities, like a new referent being introduced to the discourse, will need to be\nintroduced with a longer and more explicit referring expression to help the hearer\nrecover the referent.\nThus when an entity is \ufb01rst introduced into a discourse its mentions are likely\nto have full names, titles or roles, or appositive or restrictive relative clauses, as in\nthe introduction of our protagonist in (23.1): Victoria Chen, CFO of Megabucks\nBanking . As an entity is discussed over a discourse, it becomes more salient to the\nhearer and its mentions on average typically becomes shorter and less informative,\nfor example with a shortened name (for example Ms. Chen ), a de\ufb01nite description\n(the 38-year-old ), or a pronoun ( sheorher) (Hawkins 1978). However, this change\nin length is not monotonic, and is sensitive to discourse structure (Grosz 1977, Re-\nichman 1985, Fox 1993).\n23.1.3 Complications: Non-Referring Expressions\nMany noun phrases or other nominals are not referring expressions, although they\nmay bear a confusing super\ufb01cial resemblance. For example in some of the earliest\ncomputational work on reference resolution, Karttunen (1969) pointed out that the\nNPa car in the following example does not create a discourse referent:\n(23.20) Janet doesn\u2019t have a car .\nand cannot be referred back to by anaphoric itorthe car :\n(23.21) * Itis a Toyota.\n(23.22) * The car is red.\nWe summarize here four common types of structures that are not counted as men-\ntions in coreference tasks and hence complicate the task of mention-detection:",
    "metadata": {
      "source": "23",
      "chunk_id": 7,
      "token_count": 776,
      "chapter_title": ""
    }
  },
  {
    "content": "Banking . As an entity is discussed over a discourse, it becomes more salient to the\nhearer and its mentions on average typically becomes shorter and less informative,\nfor example with a shortened name (for example Ms. Chen ), a de\ufb01nite description\n(the 38-year-old ), or a pronoun ( sheorher) (Hawkins 1978). However, this change\nin length is not monotonic, and is sensitive to discourse structure (Grosz 1977, Re-\nichman 1985, Fox 1993).\n23.1.3 Complications: Non-Referring Expressions\nMany noun phrases or other nominals are not referring expressions, although they\nmay bear a confusing super\ufb01cial resemblance. For example in some of the earliest\ncomputational work on reference resolution, Karttunen (1969) pointed out that the\nNPa car in the following example does not create a discourse referent:\n(23.20) Janet doesn\u2019t have a car .\nand cannot be referred back to by anaphoric itorthe car :\n(23.21) * Itis a Toyota.\n(23.22) * The car is red.\nWe summarize here four common types of structures that are not counted as men-\ntions in coreference tasks and hence complicate the task of mention-detection:\n4Pronouns also usually (but not always) refer to entities that were introduced no further than one or two\nsentences back in the ongoing discourse, whereas de\ufb01nite noun phrases can often refer further back.",
    "metadata": {
      "source": "23",
      "chunk_id": 8,
      "token_count": 325,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\n23.1 \u2022 C OREFERENCE PHENOMENA : LINGUISTIC BACKGROUND 7\nAppositives: An appositional structure is a noun phrase that appears next to a\nhead noun phrase, describing the head. In English they often appear in commas, like\n\u201ca unit of UAL\u201d appearing in apposition to the NP United , orCFO of Megabucks\nBanking in apposition to Victoria Chen .\n(23.23) Victoria Chen, CFO of Megabucks Banking, saw ...\n(23.24) United, a unit of UAL, matched the fares.\nAppositional NPs are not referring expressions, instead functioning as a kind of\nsupplementary parenthetical description of the head NP. Nonetheless, sometimes it\nis useful to link these phrases to an entity they describe, and so some datasets like\nOntoNotes mark appositional relationships.\nPredicative and Prenominal NPs: Predicative or attributive NPs describe prop-\nerties of the head noun. In United is a unit of UAL , the NP a unit of UAL describes\na property of United, rather than referring to a distinct entity. Thus they are not\nmarked as mentions in coreference tasks; in our example the NPs $2.3 million and\nthe company\u2019s president , are attributive, describing properties of her pay andthe\n38-year-old ; Example (23.27) shows a Chinese example in which the predicate NP\n(\u4e2d\u56fd\u6700\u5927\u7684\u57ce\u5e02; China\u2019s biggest city ) is not a mention.\n(23.25) her pay jumped to $2.3 million\n(23.26) the 38-year-old became the company\u2019s president\n(23.27)\u4e0a\u6d77\u662f[\u4e2d\u56fd\u6700\u5927\u7684\u57ce\u5e02] [Shanghai is China\u2019s biggest city ]\nExpletives: Many uses of pronouns like itin English and corresponding pronouns\nin other languages are not referential. Such expletive orpleonastic cases include expletive\nit is raining , in idioms like hit it off , or in particular syntactic situations like clefts clefts\n(23.28a) or extraposition (23.28b):\n(23.28) a. Itwas Emma Goldman who founded Mother Earth\nb.Itsurprised me that there was a herring hanging on her wall.\nGenerics: Another kind of expression that does not refer back to an entity explic-\nitly evoked in the text is generic reference. Consider (23.29).\n(23.29) I love mangos. They are very tasty.\nHere, they refers, not to a particular mango or set of mangos, but instead to the class\nof mangos in general. The pronoun youcan also be used generically:\n(23.30) In July in San Francisco youhave to wear a jacket.\n23.1.4 Linguistic Properties of the Coreference Relation\nNow that we have seen the linguistic properties of individual referring expressions\nwe turn to properties of the antecedent/anaphor pair. Understanding these properties\nis helpful both in designing novel features and performing error analyses.\nNumber Agreement: Referring expressions and their referents must generally\nagree in number; English she/her/he/him/his/it are singular, we/us/they/them are plu-\nral, and youis unspeci\ufb01ed for number. So a plural antecedent like the chefs cannot\ngenerally corefer with a singular anaphor like she. However, algorithms cannot\nenforce number agreement too strictly. First, semantically plural entities can be re-\nferred to by either itorthey:\n(23.31) IBM announced a new machine translation product yesterday. They have\nbeen working on it for 20 years.",
    "metadata": {
      "source": "23",
      "chunk_id": 9,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8",
    "metadata": {
      "source": "23",
      "chunk_id": 10,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "8CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\nSecond, singular they has become much more common, in which they is used to singular they\ndescribe singular individuals, often useful because they is gender neutral. Although\nrecently increasing, singular they is quite old, part of English for many centuries.5\nPerson Agreement: English distinguishes between \ufb01rst, second, and third person,\nand a pronoun\u2019s antecedent must agree with the pronoun in person. Thus a third\nperson pronoun ( he, she, they, him, her, them, his, her, their ) must have a third person\nantecedent (one of the above or any other noun phrase). However, phenomena like\nquotation can cause exceptions; in this example I,my, and sheare coreferent:\n(23.32) \u201cI voted for Nader because he was most aligned with my values,\u201d she said.\nGender or Noun Class Agreement: In many languages, all nouns have grammat-\nical gender or noun class6and pronouns generally agree with the grammatical gender\nof their antecedent. In English this occurs only with third-person singular pronouns,\nwhich distinguish between male (he, him, his ),female (she, her ), and nonpersonal\n(it) grammatical genders. Non-binary pronouns like zeorhirmay also occur in more\nrecent texts. Knowing which gender to associate with a name in text can be complex,\nand may require world knowledge about the individual. Some examples:\n(23.33) Maryam has a theorem. She is exciting. (she=Maryam, not the theorem)\n(23.34) Maryam has a theorem. It is exciting. (it=the theorem, not Maryam)\nBinding Theory Constraints: The binding theory is a name for syntactic con-\nstraints on the relations between a mention and an antecedent in the same sentence\n(Chomsky, 1981). Oversimplifying a bit, re\ufb02exive pronouns like himself andher- re\ufb02exive\nselfcorefer with the subject of the most immediate clause that contains them (23.35),\nwhereas nonre\ufb02exives cannot corefer with this subject (23.36).\n(23.35) Janet bought herself a bottle of \ufb01sh sauce. [herself =Janet]\n(23.36) Janet bought her a bottle of \ufb01sh sauce. [her 6=Janet]\nRecency: Entities introduced in recent utterances tend to be more salient than\nthose introduced from utterances further back. Thus, in (23.37), the pronoun itis\nmore likely to refer to Jim\u2019s map than the doctor\u2019s map.\n(23.37) The doctor found an old map in the captain\u2019s chest. Jim found an even\nolder map hidden on the shelf. It described an island.\nGrammatical Role: Entities mentioned in subject position are more salient than\nthose in object position, which are in turn more salient than those mentioned in\noblique positions. Thus although the \ufb01rst sentence in (23.38) and (23.39) expresses\nroughly the same propositional content, the preferred referent for the pronoun he\nvaries with the subject\u2014John in (23.38) and Bill in (23.39).\n(23.38) Billy Bones went to the bar with Jim Hawkins. He called for a glass of\nrum. [ he = Billy ]\n(23.39) Jim Hawkins went to the bar with Billy Bones. He called for a glass of\nrum. [ he = Jim ]",
    "metadata": {
      "source": "23",
      "chunk_id": 11,
      "token_count": 764,
      "chapter_title": ""
    }
  },
  {
    "content": "(23.35) Janet bought herself a bottle of \ufb01sh sauce. [herself =Janet]\n(23.36) Janet bought her a bottle of \ufb01sh sauce. [her 6=Janet]\nRecency: Entities introduced in recent utterances tend to be more salient than\nthose introduced from utterances further back. Thus, in (23.37), the pronoun itis\nmore likely to refer to Jim\u2019s map than the doctor\u2019s map.\n(23.37) The doctor found an old map in the captain\u2019s chest. Jim found an even\nolder map hidden on the shelf. It described an island.\nGrammatical Role: Entities mentioned in subject position are more salient than\nthose in object position, which are in turn more salient than those mentioned in\noblique positions. Thus although the \ufb01rst sentence in (23.38) and (23.39) expresses\nroughly the same propositional content, the preferred referent for the pronoun he\nvaries with the subject\u2014John in (23.38) and Bill in (23.39).\n(23.38) Billy Bones went to the bar with Jim Hawkins. He called for a glass of\nrum. [ he = Billy ]\n(23.39) Jim Hawkins went to the bar with Billy Bones. He called for a glass of\nrum. [ he = Jim ]\n5Here\u2019s a bound pronoun example from Shakespeare\u2019s Comedy of Errors :There\u2019s not a man I meet but\ndoth salute me As if I were their well-acquainted friend\n6The word \u201cgender\u201d is generally only used for languages with 2 or 3 noun classes, like most Indo-\nEuropean languages; many languages, like the Bantu languages or Chinese, have a much larger number\nof noun classes.",
    "metadata": {
      "source": "23",
      "chunk_id": 12,
      "token_count": 378,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\n23.2 \u2022 C OREFERENCE TASKS AND DATASETS 9\nVerb Semantics: Some verbs semantically emphasize one of their arguments, bi-\nasing the interpretation of subsequent pronouns. Compare (23.40) and (23.41).\n(23.40) John telephoned Bill. He lost the laptop.\n(23.41) John criticized Bill. He lost the laptop.\nThese examples differ only in the verb used in the \ufb01rst sentence, yet \u201che\u201d in (23.40)\nis typically resolved to John, whereas \u201che\u201d in (23.41) is resolved to Bill. This may\nbe partly due to the link between implicit causality and saliency: the implicit cause\nof a \u201ccriticizing\u201d event is its object, whereas the implicit cause of a \u201ctelephoning\u201d\nevent is its subject. In such verbs, the entity which is the implicit cause may be more\nsalient.\nSelectional Restrictions: Many other kinds of semantic knowledge can play a role\nin referent preference. For example, the selectional restrictions that a verb places on\nits arguments (Chapter 21) can help eliminate referents, as in (23.42).\n(23.42) I ate the soup in my new bowl after cooking it for hours\nThere are two possible referents for it, the soup and the bowl. The verb eat, however,\nrequires that its direct object denote something edible, and this constraint can rule\noutbowl as a possible referent.\n23.2 Coreference Tasks and Datasets\nWe can formulate the task of coreference resolution as follows: Given a text T, \ufb01nd\nall entities and the coreference links between them. We evaluate our task by com-\nparing the links our system creates with those in human-created gold coreference\nannotations on T.\nLet\u2019s return to our coreference example, now using superscript numbers for each\ncoreference chain (cluster), and subscript letters for individual mentions in the clus-\nter:\n(23.43) [Victoria Chen]1\na, CFO of [Megabucks Banking]2\na, saw [[her]1\nbpay]3\najump\nto $2.3 million, as [the 38-year-old]1\ncalso became [[the company]2\nb\u2019s\npresident. It is widely known that [she]1\ndcame to [Megabucks]2\ncfrom rival\n[Lotsabucks]4\na.\nAssuming example (23.43) was the entirety of the article, the chains for her pay and\nLotsabucks are singleton mentions:\n1.fVictoria Chen ,her,the 38-year-old ,Sheg\n2.fMegabucks Banking ,the company ,Megabucksg\n3.fher payg\n4.fLotsabucksg\nFor most coreference evaluation campaigns, the input to the system is the raw\ntext of articles, and systems must detect mentions and then link them into clusters.\nSolving this task requires dealing with pronominal anaphora (\ufb01guring out that her\nrefers to Victoria Chen ), \ufb01ltering out non-referential pronouns like the pleonastic It\ninIt has been ten years ), dealing with de\ufb01nite noun phrases to \ufb01gure out that the\n38-year-old is coreferent with Victoria Chen , and that the company is the same as\nMegabucks . And we need to deal with names, to realize that Megabucks is the same\nasMegabucks Banking .",
    "metadata": {
      "source": "23",
      "chunk_id": 13,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10",
    "metadata": {
      "source": "23",
      "chunk_id": 14,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "10 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\nExactly what counts as a mention and what links are annotated differs from task\nto task and dataset to dataset. For example some coreference datasets do not label\nsingletons, making the task much simpler. Resolvers can achieve much higher scores\non corpora without singletons, since singletons constitute the majority of mentions in\nrunning text, and they are often hard to distinguish from non-referential NPs. Some\ntasks use gold mention-detection (i.e. the system is given human-labeled mention\nboundaries and the task is just to cluster these gold mentions), which eliminates the\nneed to detect and segment mentions from running text.\nCoreference is usually evaluated by the CoNLL F1 score, which combines three\nmetrics: MUC, B3, and CEAF e; Section 23.8 gives the details.\nLet\u2019s mention a few characteristics of one popular coreference dataset, OntoNotes\n(Pradhan et al. 2007b, Pradhan et al. 2007a), and the CoNLL 2012 Shared Task\nbased on it (Pradhan et al., 2012a). OntoNotes contains hand-annotated Chinese\nand English coreference datasets of roughly one million words each, consisting of\nnewswire, magazine articles, broadcast news, broadcast conversations, web data and\nconversational speech data, as well as about 300,000 words of annotated Arabic\nnewswire. The most important distinguishing characteristic of OntoNotes is that\nit does not label singletons, simplifying the coreference task, since singletons rep-\nresent 60%-70% of all entities. In other ways, it is similar to other coreference\ndatasets. Referring expression NPs that are coreferent are marked as mentions, but\ngenerics and pleonastic pronouns are not marked. Appositive clauses are not marked\nas separate mentions, but they are included in the mention. Thus in the NP, \u201cRichard\nGodown, president of the Industrial Biotechnology Association\u201d the mention is the\nentire phrase. Prenominal modi\ufb01ers are annotated as separate entities only if they\nare proper nouns. Thus wheat is not an entity in wheat \ufb01elds , but UNis an entity in\nUN policy (but not adjectives like American inAmerican policy ).\nA number of corpora mark richer discourse phenomena. The ISNotes corpus\nannotates a portion of OntoNotes for information status, include bridging examples\n(Hou et al., 2018). The LitBank coreference corpus (Bamman et al., 2020) contains\ncoreference annotations for 210,532 tokens from 100 different literary novels, in-\ncluding singletons and quanti\ufb01ed and negated noun phrases. The AnCora-CO coref-\nerence corpus (Recasens and Mart \u00b4\u0131, 2010) contains 400,000 words each of Spanish\n(AnCora-CO-Es) and Catalan (AnCora-CO-Ca) news data, and includes labels for\ncomplex phenomena like discourse deixis in both languages. The ARRAU corpus\n(Uryupina et al., 2020) contains 350,000 words of English marking all NPs, which\nmeans singleton clusters are available. ARRAU includes diverse genres like dialog\n(the TRAINS data) and \ufb01ction (the Pear Stories), and has labels for bridging refer-\nences, discourse deixis, generics, and ambiguous anaphoric relations.\n23.3 Mention Detection\nThe \ufb01rst stage of coreference is mention detection : \ufb01nding the spans of text thatmention\ndetection",
    "metadata": {
      "source": "23",
      "chunk_id": 15,
      "token_count": 784,
      "chapter_title": ""
    }
  },
  {
    "content": "A number of corpora mark richer discourse phenomena. The ISNotes corpus\nannotates a portion of OntoNotes for information status, include bridging examples\n(Hou et al., 2018). The LitBank coreference corpus (Bamman et al., 2020) contains\ncoreference annotations for 210,532 tokens from 100 different literary novels, in-\ncluding singletons and quanti\ufb01ed and negated noun phrases. The AnCora-CO coref-\nerence corpus (Recasens and Mart \u00b4\u0131, 2010) contains 400,000 words each of Spanish\n(AnCora-CO-Es) and Catalan (AnCora-CO-Ca) news data, and includes labels for\ncomplex phenomena like discourse deixis in both languages. The ARRAU corpus\n(Uryupina et al., 2020) contains 350,000 words of English marking all NPs, which\nmeans singleton clusters are available. ARRAU includes diverse genres like dialog\n(the TRAINS data) and \ufb01ction (the Pear Stories), and has labels for bridging refer-\nences, discourse deixis, generics, and ambiguous anaphoric relations.\n23.3 Mention Detection\nThe \ufb01rst stage of coreference is mention detection : \ufb01nding the spans of text thatmention\ndetection\nconstitute each mention. Mention detection algorithms are usually very liberal in\nproposing candidate mentions (i.e., emphasizing recall), and only \ufb01ltering later. For\nexample many systems run parsers and named entity taggers on the text and extract\nevery span that is either an NP, apossessive pronoun , or a named entity .\nDoing so from our sample text repeated in (23.44):\n(23.44) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $2.3",
    "metadata": {
      "source": "23",
      "chunk_id": 16,
      "token_count": 392,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11",
    "metadata": {
      "source": "23",
      "chunk_id": 17,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "23.3 \u2022 M ENTION DETECTION 11\nmillion, as the 38-year-old also became the company\u2019s president. It is\nwidely known that she came to Megabucks from rival Lotsabucks.\nmight result in the following list of 13 potential mentions:\nVictoria Chen $2.3 million she\nCFO of Megabucks Banking the 38-year-old Megabucks\nMegabucks Banking the company Lotsabucks\nher the company\u2019s president\nher pay It\nMore recent mention detection systems are even more generous; the span-based\nalgorithm we will describe in Section 23.6 \ufb01rst extracts literally all n-gram spans\nof words up to N=10. Of course recall from Section 23.1.3 that many NPs\u2014and\nthe overwhelming majority of random n-gram spans\u2014are not referring expressions.\nTherefore all such mention detection systems need to eventually \ufb01lter out pleonas-\ntic/expletive pronouns like Itabove, appositives like CFO of Megabucks Banking\nInc, or predicate nominals like the company\u2019s president or$2.3 million .\nSome of this \ufb01ltering can be done by rules. Early rule-based systems designed\nregular expressions to deal with pleonastic it, like the following rules from Lappin\nand Leass (1994) that use dictionaries of cognitive verbs (e.g., believe ,know ,antic-\nipate ) to capture pleonastic itin \u201cIt is thought that ketchup...\u201d, or modal adjectives\n(e.g., necessary ,possible ,certain ,important ), for, e.g., \u201cIt is likely that I...\u201d. Such\nrules are sometimes used as part of modern systems:\nIt is Modaladjective that S\nIt is Modaladjective (for NP) to VP\nIt is Cogv-ed that S\nIt seems/appears/means/follows (that) S\nMention-detection rules are sometimes designed speci\ufb01cally for particular eval-\nuation campaigns. For OntoNotes, for example, mentions are not embedded within\nlarger mentions, and while numeric quantities are annotated, they are rarely coref-\nerential. Thus for OntoNotes tasks like CoNLL 2012 (Pradhan et al., 2012a), a\ncommon \ufb01rst pass rule-based mention detection algorithm (Lee et al., 2013) is:\n1.Take all NPs, possessive pronouns, and named entities.\n2.Remove numeric quantities (100 dollars, 8%), mentions embedded in\nlarger mentions, adjectival forms of nations, and stop words (like there ).\n3.Remove pleonastic itbased on regular expression patterns.\nRule-based systems, however, are generally insuf\ufb01cient to deal with mention-\ndetection, and so modern systems incorporate some sort of learned mention detec-\ntion component, such as a referentiality classi\ufb01er, an anaphoricity classi\ufb01er \u2014\ndetecting whether an NP is an anaphor\u2014or a discourse-new classi\ufb01er\u2014 detecting\nwhether a mention is discourse-new and a potential antecedent for a future anaphor.\nAnanaphoricity detector , for example, can draw its positive training examplesanaphoricity\ndetector\nfrom any span that is labeled as an anaphoric referring expression in hand-labeled\ndatasets like OntoNotes, ARRAU , or AnCora. Any other NP or named entity can be\nmarked as a negative training example. Anaphoricity classi\ufb01ers use features of the\ncandidate mention such as its head word, surrounding words, de\ufb01niteness, animacy,",
    "metadata": {
      "source": "23",
      "chunk_id": 18,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "common \ufb01rst pass rule-based mention detection algorithm (Lee et al., 2013) is:\n1.Take all NPs, possessive pronouns, and named entities.\n2.Remove numeric quantities (100 dollars, 8%), mentions embedded in\nlarger mentions, adjectival forms of nations, and stop words (like there ).\n3.Remove pleonastic itbased on regular expression patterns.\nRule-based systems, however, are generally insuf\ufb01cient to deal with mention-\ndetection, and so modern systems incorporate some sort of learned mention detec-\ntion component, such as a referentiality classi\ufb01er, an anaphoricity classi\ufb01er \u2014\ndetecting whether an NP is an anaphor\u2014or a discourse-new classi\ufb01er\u2014 detecting\nwhether a mention is discourse-new and a potential antecedent for a future anaphor.\nAnanaphoricity detector , for example, can draw its positive training examplesanaphoricity\ndetector\nfrom any span that is labeled as an anaphoric referring expression in hand-labeled\ndatasets like OntoNotes, ARRAU , or AnCora. Any other NP or named entity can be\nmarked as a negative training example. Anaphoricity classi\ufb01ers use features of the\ncandidate mention such as its head word, surrounding words, de\ufb01niteness, animacy,\nlength, position in the sentence/discourse, many of which were \ufb01rst proposed in\nearly work by Ng and Cardie (2002a); see Section 23.5 for more on features.",
    "metadata": {
      "source": "23",
      "chunk_id": 19,
      "token_count": 332,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12",
    "metadata": {
      "source": "23",
      "chunk_id": 20,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "12 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\nReferentiality or anaphoricity detectors can be run as \ufb01lters, in which only men-\ntions that are classi\ufb01ed as anaphoric or referential are passed on to the coreference\nsystem. The end result of such a \ufb01ltering mention detection system on our example\nabove might be the following \ufb01ltered set of 9 potential mentions:\nVictoria Chen her pay she\nMegabucks Bank the 38-year-old Megabucks\nher the company Lotsabucks\nIt turns out, however, that hard \ufb01ltering of mentions based on an anaphoricity\nor referentiality classi\ufb01er leads to poor performance. If the anaphoricity classi\ufb01er\nthreshold is set too high, too many mentions are \ufb01ltered out and recall suffers. If the\nclassi\ufb01er threshold is set too low, too many pleonastic or non-referential mentions\nare included and precision suffers.\nThe modern approach is instead to perform mention detection, anaphoricity, and\ncoreference jointly in a single end-to-end model (Ng 2005b, Denis and Baldridge\n2007, Rahman and Ng 2009). For example mention detection in the Lee et al.\n(2017b),2018 system is based on a single end-to-end neural network that computes\na score for each mention being referential, a score for two mentions being corefer-\nence, and combines them to make a decision, training all these scores with a single\nend-to-end loss. We\u2019ll describe this method in detail in Section 23.6.7\nDespite these advances, correctly detecting referential mentions seems to still be\nan unsolved problem, since systems incorrectly marking pleonastic pronouns like\nitand other non-referential NPs as coreferent is a large source of errors of modern\ncoreference resolution systems (Kummerfeld and Klein 2013, Martschat and Strube\n2014, Martschat and Strube 2015, Wiseman et al. 2015, Lee et al. 2017a).\nMention, referentiality, or anaphoricity detection is thus an important open area\nof investigation. Other sources of knowledge may turn out to be helpful, especially\nin combination with unsupervised and semisupervised algorithms, which also mit-\nigate the expense of labeled datasets. In early work, for example Bean and Riloff\n(1999) learned patterns for characterizing anaphoric or non-anaphoric NPs; (by ex-\ntracting and generalizing over the \ufb01rst NPs in a text, which are guaranteed to be\nnon-anaphoric). Chang et al. (2012) look for head nouns that appear frequently in\nthe training data but never appear as gold mentions to help \ufb01nd non-referential NPs.\nBergsma et al. (2008) use web counts as a semisupervised way to augment standard\nfeatures for anaphoricity detection for English it, an important task because itis both\ncommon and ambiguous; between a quarter and half itexamples are non-anaphoric.\nConsider the following two examples:\n(23.45) You can make [it] in advance. [anaphoric]\n(23.46) You can make [it] in Hollywood. [non-anaphoric]\nTheitinmake it is non-anaphoric, part of the idiom make it . Bergsma et al. (2008)\nturn the context around each example into patterns, like \u201cmake * in advance\u201d from",
    "metadata": {
      "source": "23",
      "chunk_id": 21,
      "token_count": 775,
      "chapter_title": ""
    }
  },
  {
    "content": "in combination with unsupervised and semisupervised algorithms, which also mit-\nigate the expense of labeled datasets. In early work, for example Bean and Riloff\n(1999) learned patterns for characterizing anaphoric or non-anaphoric NPs; (by ex-\ntracting and generalizing over the \ufb01rst NPs in a text, which are guaranteed to be\nnon-anaphoric). Chang et al. (2012) look for head nouns that appear frequently in\nthe training data but never appear as gold mentions to help \ufb01nd non-referential NPs.\nBergsma et al. (2008) use web counts as a semisupervised way to augment standard\nfeatures for anaphoricity detection for English it, an important task because itis both\ncommon and ambiguous; between a quarter and half itexamples are non-anaphoric.\nConsider the following two examples:\n(23.45) You can make [it] in advance. [anaphoric]\n(23.46) You can make [it] in Hollywood. [non-anaphoric]\nTheitinmake it is non-anaphoric, part of the idiom make it . Bergsma et al. (2008)\nturn the context around each example into patterns, like \u201cmake * in advance\u201d from\n(23.45), and \u201cmake * in Hollywood\u201d from (23.46). They then use Google n-grams to\nenumerate all the words that can replace itin the patterns. Non-anaphoric contexts\ntend to only have itin the wildcard positions, while anaphoric contexts occur with\nmany other NPs (for example make them in advance is just as frequent in their data\n7Some systems try to avoid mention detection or anaphoricity detection altogether. For datasets like\nOntoNotes which don\u2019t label singletons, an alternative to \ufb01ltering out non-referential mentions is to run\ncoreference resolution, and then simply delete any candidate mentions which were not corefered with\nanother mention. This likely doesn\u2019t work as well as explicitly modeling referentiality, and cannot solve\nthe problem of detecting singletons, which is important for tasks like entity linking.",
    "metadata": {
      "source": "23",
      "chunk_id": 22,
      "token_count": 463,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\n23.4 \u2022 A RCHITECTURES FOR COREFERENCE ALGORITHMS 13\nasmake it in advance , but make them in Hollywood did not occur at all). These\nn-gram contexts can be used as features in a supervised anaphoricity classi\ufb01er.\n23.4 Architectures for Coreference Algorithms\nModern systems for coreference are based on supervised neural machine learning,\nsupervised from hand-labeled datasets like OntoNotes. In this section we overview\nthe various architecture of modern systems, using the categorization of Ng (2010),\nwhich distinguishes algorithms based on whether they make each coreference deci-\nsion in a way that is entity-based \u2014representing each entity in the discourse model\u2014\nor only mention-based \u2014considering each mention independently, and whether they\nuseranking models to directly compare potential antecedents. Afterwards, we go\ninto more detail on one state-of-the-art algorithm in Section 23.6.\n23.4.1 The Mention-Pair Architecture\nWe begin with the mention-pair architecture, the simplest and most in\ufb02uential mention-pair\ncoreference architecture, which introduces many of the features of more complex\nalgorithms, even though other architectures perform better. The mention-pair ar- mention-pair\nchitecture is based around a classi\ufb01er that\u2014 as its name suggests\u2014is given a pair\nof mentions, a candidate anaphor and a candidate antecedent, and makes a binary\nclassi\ufb01cation decision: coreferring or not.\nLet\u2019s consider the task of this classi\ufb01er for the pronoun shein our example, and\nassume the slightly simpli\ufb01ed set of potential antecedents in Fig. 23.2.\nVictoria ChenMegabucks Bankingherher paythe 37-year-oldshep(coref|\u201dVictoria Chen\u201d,\u201dshe\u201d)\np(coref|\u201dMegabucks Banking\u201d,\u201dshe\u201d)\nFigure 23.2 For each pair of a mention (like she), and a potential antecedent mention (like\nVictoria Chen orher), the mention-pair classi\ufb01er assigns a probability of a coreference link.\nFor each prior mention ( Victoria Chen ,Megabucks Banking ,her, etc.), the binary\nclassi\ufb01er computes a probability: whether or not the mention is the antecedent of\nshe. We want this probability to be high for actual antecedents ( Victoria Chen ,her,\nthe 38-year-old ) and low for non-antecedents ( Megabucks Banking ,her pay ).\nEarly classi\ufb01ers used hand-built features (Section 23.5); more recent classi\ufb01ers\nuse neural representation learning (Section 23.6)\nFor training, we need a heuristic for selecting training samples; since most pairs\nof mentions in a document are not coreferent, selecting every pair would lead to\na massive overabundance of negative samples. The most common heuristic, from\n(Soon et al., 2001), is to choose the closest antecedent as a positive example, and all\npairs in between as the negative examples. More formally, for each anaphor mention\nmiwe create\n\u2022 one positive instance ( mi;mj) where mjis the closest antecedent to mi, and",
    "metadata": {
      "source": "23",
      "chunk_id": 23,
      "token_count": 702,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14",
    "metadata": {
      "source": "23",
      "chunk_id": 24,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "14 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\n\u2022 a negative instance ( mi;mk) for each mkbetween mjandmi\nThus for the anaphor she, we would choose ( she,her) as the positive example\nand no negative examples. Similarly, for the anaphor the company we would choose\n(the company ,Megabucks ) as the positive example and ( the company ,she) (the com-\npany ,the 38-year-old ) (the company ,her pay ) and ( the company ,her) as negative\nexamples.\nOnce the classi\ufb01er is trained, it is applied to each test sentence in a clustering\nstep. For each mention iin a document, the classi\ufb01er considers each of the prior i\u00001\nmentions. In closest-\ufb01rst clustering (Soon et al., 2001), the classi\ufb01er is run right to\nleft (from mention i\u00001 down to mention 1) and the \ufb01rst antecedent with probability\n>:5 is linked to i. If no antecedent has probably >0:5, no antecedent is selected for\ni. Inbest-\ufb01rst clustering, the classi\ufb01er is run on all i\u00001 antecedents and the most\nprobable preceding mention is chosen as the antecedent for i. The transitive closure\nof the pairwise relation is taken as the cluster.\nWhile the mention-pair model has the advantage of simplicity, it has two main\nproblems. First, the classi\ufb01er doesn\u2019t directly compare candidate antecedents to\neach other, so it\u2019s not trained to decide, between two likely antecedents, which one\nis in fact better. Second, it ignores the discourse model, looking only at mentions,\nnot entities. Each classi\ufb01er decision is made completely locally to the pair, without\nbeing able to take into account other mentions of the same entity. The next two\nmodels each address one of these two \ufb02aws.\n23.4.2 The Mention-Rank Architecture\nThe mention ranking model directly compares candidate antecedents to each other,\nchoosing the highest-scoring antecedent for each anaphor.\nIn early formulations, for mention i, the classi\ufb01er decides which of the f1;:::;i\u0000\n1gprior mentions is the antecedent (Denis and Baldridge, 2008). But suppose iis\nin fact not anaphoric, and none of the antecedents should be chosen? Such a model\nwould need to run a separate anaphoricity classi\ufb01er on i. Instead, it turns out to be\nbetter to jointly learn anaphoricity detection and coreference together with a single\nloss (Rahman and Ng, 2009).\nSo in modern mention-ranking systems, for the ith mention (anaphor), we have\nan associated random variable yiranging over the values Y(i) =f1;:::;i\u00001;\u000fg. The\nvalue\u000fis a special dummy mention meaning that idoes not have an antecedent (i.e.,\nis either discourse-new and starts a new coref chain, or is non-anaphoric).\nVictoria ChenMegabucks Bankingherher paythe 37-year-oldshep(\u201dVictoria Chen\u201d|\u201dshe\u201d)\np(\u03f5|\u201dshe\u201d)\u03f5One or more of theseshould be highAll of theseshould be low}p(\u201dher pay\u201d|she\u201d)p(\u201dher\u201d|she\u201d)p(\u201dthe 37-year-old\u201d|she\u201d)\np(\u201dMegabucks Banking\u201d|she\u201d)}",
    "metadata": {
      "source": "23",
      "chunk_id": 25,
      "token_count": 785,
      "chapter_title": ""
    }
  },
  {
    "content": "1gprior mentions is the antecedent (Denis and Baldridge, 2008). But suppose iis\nin fact not anaphoric, and none of the antecedents should be chosen? Such a model\nwould need to run a separate anaphoricity classi\ufb01er on i. Instead, it turns out to be\nbetter to jointly learn anaphoricity detection and coreference together with a single\nloss (Rahman and Ng, 2009).\nSo in modern mention-ranking systems, for the ith mention (anaphor), we have\nan associated random variable yiranging over the values Y(i) =f1;:::;i\u00001;\u000fg. The\nvalue\u000fis a special dummy mention meaning that idoes not have an antecedent (i.e.,\nis either discourse-new and starts a new coref chain, or is non-anaphoric).\nVictoria ChenMegabucks Bankingherher paythe 37-year-oldshep(\u201dVictoria Chen\u201d|\u201dshe\u201d)\np(\u03f5|\u201dshe\u201d)\u03f5One or more of theseshould be highAll of theseshould be low}p(\u201dher pay\u201d|she\u201d)p(\u201dher\u201d|she\u201d)p(\u201dthe 37-year-old\u201d|she\u201d)\np(\u201dMegabucks Banking\u201d|she\u201d)}\nFigure 23.3 For each candidate anaphoric mention (like she), the mention-ranking system assigns a proba-\nbility distribution over all previous mentions plus the special dummy mention \u000f.\nAt test time, for a given mention ithe model computes one softmax over all the\nantecedents (plus \u000f) giving a probability for each candidate antecedent (or none).",
    "metadata": {
      "source": "23",
      "chunk_id": 26,
      "token_count": 366,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15",
    "metadata": {
      "source": "23",
      "chunk_id": 27,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "23.5 \u2022 C LASSIFIERS USING HAND -BUILT FEATURES 15\nFig. 23.3 shows an example of the computation for the single candidate anaphor\nshe.\nOnce the antecedent is classi\ufb01ed for each anaphor, transitive closure can be run\nover the pairwise decisions to get a complete clustering.\nTraining is trickier in the mention-ranking model than the mention-pair model,\nbecause for each anaphor we don\u2019t know which of all the possible gold antecedents\nto use for training. Instead, the best antecedent for each mention is latent ; that\nis, for each mention we have a whole cluster of legal gold antecedents to choose\nfrom. Early work used heuristics to choose an antecedent, for example choosing the\nclosest antecedent as the gold antecedent and all non-antecedents in a window of\ntwo sentences as the negative examples (Denis and Baldridge, 2008). Various kinds\nof ways to model latent antecedents exist (Fernandes et al. 2012, Chang et al. 2013,\nDurrett and Klein 2013). The simplest way is to give credit to any legal antecedent\nby summing over all of them, with a loss function that optimizes the likelihood of\nall correct antecedents from the gold clustering (Lee et al., 2017b). We\u2019ll see the\ndetails in Section 23.6.\nMention-ranking models can be implemented with hand-build features or with\nneural representation learning (which might also incorporate some hand-built fea-\ntures). we\u2019ll explore both directions in Section 23.5 and Section 23.6.\n23.4.3 Entity-based Models\nBoth the mention-pair and mention-ranking models make their decisions about men-\ntions . By contrast, entity-based models link each mention not to a previous mention\nbut to a previous discourse entity (cluster of mentions).\nA mention-ranking model can be turned into an entity-ranking model simply\nby having the classi\ufb01er make its decisions over clusters of mentions rather than\nindividual mentions (Rahman and Ng, 2009).\nFor traditional feature-based models, this can be done by extracting features over\nclusters. The size of a cluster is a useful feature, as is its \u2018shape\u2019, which is the\nlist of types of the mentions in the cluster i.e., sequences of the tokens (P)roper,\n(D)e\ufb01nite, (I)nde\ufb01nite, (Pr)onoun, so that a cluster composed of fVictoria ,her,the\n38-year-oldgwould have the shape P-Pr-D (Bj\u00a8orkelund and Kuhn, 2014). An entity-\nbased model that includes a mention-pair classi\ufb01er can use as features aggregates of\nmention-pair probabilities, for example computing the average probability of coref-\nerence over all mention-pairs in the two clusters (Clark and Manning 2015).\nNeural models can learn representations of clusters automatically, for example\nby using an RNN over the sequence of cluster mentions to encode a state correspond-\ning to a cluster representation (Wiseman et al., 2016), or by learning distributed rep-\nresentations for pairs of clusters by pooling over learned representations of mention\npairs (Clark and Manning, 2016b).\nHowever, although entity-based models are more expressive, the use of cluster-\nlevel information in practice has not led to large gains in performance, so mention-\nranking models are still more commonly used.\n23.5 Classi\ufb01ers using hand-built features",
    "metadata": {
      "source": "23",
      "chunk_id": 28,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "clusters. The size of a cluster is a useful feature, as is its \u2018shape\u2019, which is the\nlist of types of the mentions in the cluster i.e., sequences of the tokens (P)roper,\n(D)e\ufb01nite, (I)nde\ufb01nite, (Pr)onoun, so that a cluster composed of fVictoria ,her,the\n38-year-oldgwould have the shape P-Pr-D (Bj\u00a8orkelund and Kuhn, 2014). An entity-\nbased model that includes a mention-pair classi\ufb01er can use as features aggregates of\nmention-pair probabilities, for example computing the average probability of coref-\nerence over all mention-pairs in the two clusters (Clark and Manning 2015).\nNeural models can learn representations of clusters automatically, for example\nby using an RNN over the sequence of cluster mentions to encode a state correspond-\ning to a cluster representation (Wiseman et al., 2016), or by learning distributed rep-\nresentations for pairs of clusters by pooling over learned representations of mention\npairs (Clark and Manning, 2016b).\nHowever, although entity-based models are more expressive, the use of cluster-\nlevel information in practice has not led to large gains in performance, so mention-\nranking models are still more commonly used.\n23.5 Classi\ufb01ers using hand-built features\nFeature-based classi\ufb01ers, use hand-designed features in logistic regression, SVM,\nor random forest classi\ufb01ers for coreference resolution. These classi\ufb01ers don\u2019t per-",
    "metadata": {
      "source": "23",
      "chunk_id": 29,
      "token_count": 329,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16",
    "metadata": {
      "source": "23",
      "chunk_id": 30,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "16 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\nform as well as neural ones. Nonetheless, they are still sometimes useful to build\nlightweight systems when compute or data are sparse, and the features themselves\nare useful for error analysis even in neural systems.\nGiven an anaphor mention and a potential antecedent mention, feature based\nclassi\ufb01ers make use of three types of features: (i) features of the anaphor, (ii) features\nof the candidate antecedent, and (iii) features of the relationship between the pair.\nEntity-based models can make additional use of two additional classes: (iv) feature\nof all mentions from the antecedent\u2019s entity cluster, and (v) features of the relation\nbetween the anaphor and the mentions in the antecedent entity cluster.\nFeatures of the Anaphor or Antecedent Mention\nFirst (last) word Victoria/she First or last word (or embedding) of antecedent/anaphor\nHead word Victoria/she Head word (or head embedding) of antecedent/anaphor\nAttributes Sg-F-A-3-PER/\nSg-F-A-3-PERThe number, gender, animacy, person, named entity type\nattributes of (antecedent/anaphor)\nLength 2/1 length in words of (antecedent/anaphor)\nMention type P/Pr Type: (P)roper, (D)e\ufb01nite, (I)nde\ufb01nite, (Pr)onoun) of an-\ntecedent/anaphor\nFeatures of the Antecedent Entity\nEntity shape P-Pr-D The \u2018shape\u2019 or list of types of the mentions in the\nantecedent entity (cluster), i.e., sequences of (P)roper,\n(D)e\ufb01nite, (I)nde\ufb01nite, (Pr)onoun.\nEntity attributes Sg-F-A-3-PER The number, gender, animacy, person, named entity type\nattributes of the antecedent entity\nAnt. cluster size 3 Number of mentions in the antecedent cluster\nFeatures of the Pair of Mentions\nSentence distance 1 The number of sentences between antecedent and anaphor\nMention distance 4 The number of mentions between antecedent and anaphor\ni-within-i F Anaphor has i-within-i relation with antecedent\nCosine Cosine between antecedent and anaphor embeddings\nFeatures of the Pair of Entities\nExact String Match F True if the strings of any two mentions from the antecedent\nand anaphor clusters are identical.\nHead Word Match F True if any mentions from antecedent cluster has same\nheadword as any mention in anaphor cluster\nWord Inclusion F All words in anaphor cluster included in antecedent cluster\nFigure 23.4 Feature-based coreference: sample feature values for anaphor \u201cshe\u201d and potential antecedent\n\u201cVictoria Chen\u201d.\nFigure 23.4 shows a selection of commonly used features, and shows the value\nthat would be computed for the potential anaphor \u201cshe\u201d and potential antecedent\n\u201cVictoria Chen\u201d in our example sentence, repeated below:\n(23.47) Victoria Chen , CFO of Megabucks Banking, saw her pay jump to $2.3\nmillion, as the 38-year-old also became the company\u2019s president. It is\nwidely known that shecame to Megabucks from rival Lotsabucks.\nFeatures that prior work has found to be particularly useful are exact string\nmatch, entity headword agreement, mention distance, as well as (for pronouns) exact",
    "metadata": {
      "source": "23",
      "chunk_id": 31,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "i-within-i F Anaphor has i-within-i relation with antecedent\nCosine Cosine between antecedent and anaphor embeddings\nFeatures of the Pair of Entities\nExact String Match F True if the strings of any two mentions from the antecedent\nand anaphor clusters are identical.\nHead Word Match F True if any mentions from antecedent cluster has same\nheadword as any mention in anaphor cluster\nWord Inclusion F All words in anaphor cluster included in antecedent cluster\nFigure 23.4 Feature-based coreference: sample feature values for anaphor \u201cshe\u201d and potential antecedent\n\u201cVictoria Chen\u201d.\nFigure 23.4 shows a selection of commonly used features, and shows the value\nthat would be computed for the potential anaphor \u201cshe\u201d and potential antecedent\n\u201cVictoria Chen\u201d in our example sentence, repeated below:\n(23.47) Victoria Chen , CFO of Megabucks Banking, saw her pay jump to $2.3\nmillion, as the 38-year-old also became the company\u2019s president. It is\nwidely known that shecame to Megabucks from rival Lotsabucks.\nFeatures that prior work has found to be particularly useful are exact string\nmatch, entity headword agreement, mention distance, as well as (for pronouns) exact\nattribute match and i-within-i, and (for nominals and proper names) word inclusion\nand cosine. For lexical features (like head words) it is common to only use words\nthat appear enough times ( >20 times).",
    "metadata": {
      "source": "23",
      "chunk_id": 32,
      "token_count": 332,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17",
    "metadata": {
      "source": "23",
      "chunk_id": 33,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "23.6 \u2022 A NEURAL MENTION -RANKING ALGORITHM 17\nIt is crucial in feature-based systems to use conjunctions of features; one exper-\niment suggested that moving from individual features in a classi\ufb01er to conjunctions\nof multiple features increased F1 by 4 points (Lee et al., 2017a). Speci\ufb01c conjunc-\ntions can be designed by hand (Durrett and Klein, 2013), all pairs of features can be\nconjoined (Bengtson and Roth, 2008), or feature conjunctions can be learned using\ndecision tree or random forest classi\ufb01ers (Ng and Cardie 2002a, Lee et al. 2017a).\nFeatures can also be used in neural models as well. Neural systems use contex-\ntual word embeddings so don\u2019t bene\ufb01t from shallow features like string match or or\nmention types. However features like mention length, distance between mentions,\nor genre can complement neural contextual embedding models.\n23.6 A neural mention-ranking algorithm\nIn this section we describe the neural e2e-coref algorithms of Lee et al. (2017b)\n(simpli\ufb01ed and extended a bit, drawing on Joshi et al. (2019) and others). This is\namention-ranking algorithm that considers all possible spans of text in the docu-\nment, assigns a mention-score to each span, prunes the mentions based on this score,\nthen assigns coreference links to the remaining mentions.\nMore formally, given a document Dwith Twords, the model considers all of\ntheT(T+1)\n2text spans in D(unigrams, bigrams, trigrams, 4-grams, etc; in practice\nwe only consider spans up a maximum length around 10). The task is to assign\nto each span ian antecedent yi, a random variable ranging over the values Y(i) =\nf1;:::;i\u00001;\u000fg; each previous span and a special dummy token \u000f. Choosing the\ndummy token means that idoes not have an antecedent, either because iis discourse-\nnew and starts a new coreference chain, or because iis non-anaphoric.\nFor each pair of spans iand j, the system assigns a score s(i;j)for the coref-\nerence link between span iand span j. The system then learns a distribution P(yi)\nover the antecedents for span i:\nP(yi) =exp(s(i;yi))P\ny02Y(i)exp(s(i;y0))(23.48)\nThis score s(i;j)includes three factors that we\u2019ll de\ufb01ne below: m(i); whether span\niis a mention; m(j); whether span jis a mention; and c(i;j); whether jis the\nantecedent of i:\ns(i;j) =m(i)+m(j)+c(i;j) (23.49)\nFor the dummy antecedent \u000f, the score s(i;\u000f)is \ufb01xed to 0. This way if any non-\ndummy scores are positive, the model predicts the highest-scoring antecedent, but if\nall the scores are negative it abstains.\n23.6.1 Computing span representations\nTo compute the two functions m(i)andc(i;j)which score a span ior a pair of spans\n(i;j), we\u2019ll need a way to represent a span. The e2e-coref family of algorithms\nrepresents each span by trying to capture 3 words/tokens: the \ufb01rst word, the last",
    "metadata": {
      "source": "23",
      "chunk_id": 34,
      "token_count": 763,
      "chapter_title": ""
    }
  },
  {
    "content": "For each pair of spans iand j, the system assigns a score s(i;j)for the coref-\nerence link between span iand span j. The system then learns a distribution P(yi)\nover the antecedents for span i:\nP(yi) =exp(s(i;yi))P\ny02Y(i)exp(s(i;y0))(23.48)\nThis score s(i;j)includes three factors that we\u2019ll de\ufb01ne below: m(i); whether span\niis a mention; m(j); whether span jis a mention; and c(i;j); whether jis the\nantecedent of i:\ns(i;j) =m(i)+m(j)+c(i;j) (23.49)\nFor the dummy antecedent \u000f, the score s(i;\u000f)is \ufb01xed to 0. This way if any non-\ndummy scores are positive, the model predicts the highest-scoring antecedent, but if\nall the scores are negative it abstains.\n23.6.1 Computing span representations\nTo compute the two functions m(i)andc(i;j)which score a span ior a pair of spans\n(i;j), we\u2019ll need a way to represent a span. The e2e-coref family of algorithms\nrepresents each span by trying to capture 3 words/tokens: the \ufb01rst word, the last\nword, and the most important word. We \ufb01rst run each paragraph or subdocument\nthrough an encoder (like BERT) to generate embeddings hifor each token i. The\nspan iis then represented by a vector githat is a concatenation of the encoder output",
    "metadata": {
      "source": "23",
      "chunk_id": 35,
      "token_count": 350,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\n18 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\nembedding for the \ufb01rst (start) token of the span, the encoder output for the last (end)\ntoken of the span, and a third vector which is an attention-based representation:\ngi= [hSTART (i);hEND (i);hATT(i)] (23.50)\nThe goal of the attention vector is to represent which word/token is the likely\nsyntactic head-word of the span; we saw in the prior section that head-words are\na useful feature; a matching head-word is a good indicator of coreference. The\nattention representation is computed as usual; the system learns a weight vector wa,\nand computes its dot product with the hidden state httransformed by a FFN:\nat=wa\u0001FFN a(ht) (23.51)\nThe attention score is normalized into a distribution via a softmax:\nai;t=exp(at)\nPEND (i)\nk=START (i)exp(ak)(23.52)\nAnd then the attention distribution is used to create a vector hATT(i)which is an\nattention-weighted sum of the embeddings etof each of the words in span i:\nhATT(i)=END (i)X\nt=START (i)ai;t\u0001et (23.53)\nFig. 23.5 shows the computation of the span representation and the mention\nscore.\nEncodings (h)\u2026Encoder GeneralElectricsaidthePostalServicecontactedthecompanySpan head (hATT) Span representation (g) Mention score (m)+++++General ElectricElectric said thethe Postal ServiceService contacted thethe company\nFigure 23.5 Computation of the span representation g(and the mention score m) in a BERT version of the\ne2e-coref model (Lee et al. 2017b, Joshi et al. 2019). The model considers all spans up to a maximum width of\nsay 10; the \ufb01gure shows a small subset of the bigram and trigram spans.\n23.6.2 Computing the mention and antecedent scores mandc\nNow that we know how to compute the vector gifor representing span i, we can\nsee the details of the two scoring functions m(i)andc(i;j). Both are computed by\nfeedforward networks:\nm(i) = wm\u0001FFN m(gi) (23.54)\nc(i;j) = wc\u0001FFN c([gi;gj;gi\u000egj;]) (23.55)\nAt inference time, this mention score mis used as a \ufb01lter to keep only the best few\nmentions.",
    "metadata": {
      "source": "23",
      "chunk_id": 36,
      "token_count": 562,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\n23.6 \u2022 A NEURAL MENTION -RANKING ALGORITHM 19\nWe then compute the antecedent score for high-scoring mentions. The antecedent\nscore c(i;j)takes as input a representation of the spans iandj, but also the element-\nwise similarity of the two spans to each other gi\u000egj(here\u000eis element-wise mul-\ntiplication). Fig. 23.6 shows the computation of the score sfor the three possible\nantecedents of the company in the example sentence from Fig. 23.5.\nFigure 23.6 The computation of the score sfor the three possible antecedents of the com-\npany in the example sentence from Fig. 23.5. Figure after Lee et al. (2017b).\nGiven the set of mentions, the joint distribution of antecedents for each docu-\nment is computed in a forward pass, and we can then do transitive closure on the\nantecedents to create a \ufb01nal clustering for the document.\nFig. 23.7 shows example predictions from the model, showing the attention\nweights, which Lee et al. (2017b) \ufb01nd correlate with traditional semantic heads.\nNote that the model gets the second example wrong, presumably because attendants\nandpilot likely have nearby word embeddings.\nFigure 23.7 Sample predictions from the Lee et al. (2017b) model, with one cluster per\nexample, showing one correct example and one mistake. Bold, parenthesized spans are men-\ntions in the predicted cluster. The amount of red color on a word indicates the head-\ufb01nding\nattention weight ai;tin Eq. 23.52. Figure adapted from Lee et al. (2017b).\n23.6.3 Learning\nFor training, we don\u2019t have a single gold antecedent for each mention; instead the\ncoreference labeling only gives us each entire cluster of coreferent mentions; so a\nmention only has a latent antecedent. We therefore use a loss function that maxi-\nmizes the sum of the coreference probability of any of the legal antecedents. For a\ngiven mention iwith possible antecedents Y(i), let GOLD (i) be the set of mentions\nin the gold cluster containing i. Since the set of mentions occurring before iisY(i),\nthe set of mentions in that gold cluster that also occur before iisY(i)\\GOLD (i). We",
    "metadata": {
      "source": "23",
      "chunk_id": 37,
      "token_count": 522,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\n20 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\ntherefore want to maximize:\nX\n\u02c6y2Y(i)\\GOLD (i)P(\u02c6y) (23.56)\nIf a mention iis not in a gold cluster GOLD (i) =\u000f.\nTo turn this probability into a loss function, we\u2019ll use the cross-entropy loss\nfunction we de\ufb01ned in Eq. ??in Chapter 5, by taking the \u0000log of the probability. If\nwe then sum over all mentions, we get the \ufb01nal loss function for training:\nL=NX\ni=2\u0000logX\n\u02c6y2Y(i)\\GOLD (i)P(\u02c6y) (23.57)\n23.7 Entity Linking\nEntity linking is the task of associating a mention in text with the representation of entity linking\nsome real-world entity in an ontology or knowledge base (Ji and Grishman, 2011). It\nis the natural follow-on to coreference resolution; coreference resolution is the task\nof associating textual mentions that corefer to the same entity. Entity linking takes\nthe further step of identifying who that entity is. It is especially important for any\nNLP task that links to a knowledge base.\nWhile there are all sorts of potential knowledge-bases, we\u2019ll focus in this section\non Wikipedia, since it\u2019s widely used as an ontology for NLP tasks. In this usage,\neach unique Wikipedia page acts as the unique id for a particular entity. This task of\ndeciding which Wikipedia page corresponding to an individual is being referred to\nby a text mention has its own name: wiki\ufb01cation (Mihalcea and Csomai, 2007). wiki\ufb01cation\nSince the earliest systems (Mihalcea and Csomai 2007, Cucerzan 2007, Milne\nand Witten 2008), entity linking is done in (roughly) two stages: mention detec-\ntion andmention disambiguation . We\u2019ll give two algorithms, one simple classic\nbaseline that uses anchor dictionaries and information from the Wikipedia graph\nstructure (Ferragina and Scaiella, 2011) and one modern neural algorithm (Li et al.,\n2020). We\u2019ll focus here mainly on the application of entity linking to questions,\nsince a lot of the literature has been in that context.\n23.7.1 Linking based on Anchor Dictionaries and Web Graph\nAs a simple baseline we introduce the T AGME linker (Ferragina and Scaiella, 2011)\nfor Wikipedia, which itself draws on earlier algorithms (Mihalcea and Csomai 2007,\nCucerzan 2007, Milne and Witten 2008). Wiki\ufb01cation algorithms de\ufb01ne the set of\nentities as the set of Wikipedia pages, so we\u2019ll refer to each Wikipedia page as a\nunique entity e. T AGME \ufb01rst creates a catalog of all entities (i.e. all Wikipedia\npages, removing some disambiguation and other meta-pages) and indexes them in a\nstandard IR engine like Lucene. For each page e, the algorithm computes an in-link\ncount in (e): the total number of in-links from other Wikipedia pages that point to e.\nThese counts can be derived from Wikipedia dumps.\nFinally, the algorithm requires an anchor dictionary . An anchor dictionary\nlists for each Wikipedia page, its anchor texts : the hyperlinked spans of text on anchor texts\nother pages that point to it. For example, the web page for Stanford University,\nhttp://www.stanford.edu , might be pointed to from another page using anchor\ntexts like Stanford orStanford University :",
    "metadata": {
      "source": "23",
      "chunk_id": 38,
      "token_count": 786,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21",
    "metadata": {
      "source": "23",
      "chunk_id": 39,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "23.7 \u2022 E NTITY LINKING 21\n<a href=\"http://www.stanford.edu\">Stanford University</a>\nWe compute a Wikipedia anchor dictionary by including, for each Wikipedia\npage e,e\u2019s title as well as all the anchor texts from all Wikipedia pages that point to e.\nFor each anchor string awe\u2019ll also compute its total frequency freq (a)in Wikipedia\n(including non-anchor uses), the number of times aoccurs as a link (which we\u2019ll call\nlink(a)), and its link probability linkprob (a) =link(a)=freq(a). Some cleanup of the\n\ufb01nal anchor dictionary is required, for example removing anchor strings composed\nonly of numbers or single characters, that are very rare, or that are very unlikely to\nbe useful entities because they have a very low linkprob.\nMention Detection Given a question (or other text we are trying to link), T AGME\ndetects mentions by querying the anchor dictionary for each token sequence up to\n6 words. This large set of sequences is pruned with some simple heuristics (for\nexample pruning substrings if they have small linkprobs). The question:\nWhen was Ada Lovelace born?\nmight give rise to the anchor Ada Lovelace and possibly Ada, but substrings spans\nlikeLovelace might be pruned as having too low a linkprob, and but spans like born\nhave such a low linkprob that they would not be in the anchor dictionary at all.\nMention Disambiguation If a mention span is unambiguous (points to only one\nentity/Wikipedia page), we are done with entity linking! However, many spans are\nambiguous, matching anchors for multiple Wikipedia entities/pages. The T AGME\nalgorithm uses two factors for disambiguating ambiguous spans, which have been\nreferred to as prior probability andrelatedness/coherence . The \ufb01rst factor is p(eja),\nthe probability with which the span refers to a particular entity. For each page e2\nE(a), the probability p(eja)that anchor apoints to e, is the ratio of the number of\nlinks into ewith anchor text ato the total number of occurrences of aas an anchor:\nprior(a!e) = p(eja) =count (a!e)\nlink(a)(23.58)\nLet\u2019s see how that factor works in linking entities in the following question:\nWhat Chinese Dynasty came before the Yuan?\nThe most common association for the span Yuan in the anchor dictionary is the name\nof the Chinese currency, i.e., the probability p(Yuan currencyjyuan)is very high.\nRarer Wikipedia associations for Yuan include the common Chinese last name, a\nlanguage spoken in Thailand, and the correct entity in this case, the name of the\nChinese dynasty. So if we chose based only on p(eja), we would make the wrong\ndisambiguation and miss the correct link, Yuan dynasty .\nTo help in just this sort of case, T AGME uses a second factor, the relatedness of\nthis entity to other entities in the input question. In our example, the fact that the\nquestion also contains the span Chinese Dynasty , which has a high probability link to\nthe page Dynasties inChinese history , ought to help match Yuan dynasty .\nLet\u2019s see how this works. Given a question q, for each candidate anchors span\nadetected in q, we assign a relatedness score to each possible entity e2E(a)ofa.\nThe relatedness score of the link a!eis the weighted average relatedness between\neand all other entities in q. Two entities are considered related to the extent their\nWikipedia pages share many in-links. More formally, the relatedness between two",
    "metadata": {
      "source": "23",
      "chunk_id": 40,
      "token_count": 778,
      "chapter_title": ""
    }
  },
  {
    "content": "What Chinese Dynasty came before the Yuan?\nThe most common association for the span Yuan in the anchor dictionary is the name\nof the Chinese currency, i.e., the probability p(Yuan currencyjyuan)is very high.\nRarer Wikipedia associations for Yuan include the common Chinese last name, a\nlanguage spoken in Thailand, and the correct entity in this case, the name of the\nChinese dynasty. So if we chose based only on p(eja), we would make the wrong\ndisambiguation and miss the correct link, Yuan dynasty .\nTo help in just this sort of case, T AGME uses a second factor, the relatedness of\nthis entity to other entities in the input question. In our example, the fact that the\nquestion also contains the span Chinese Dynasty , which has a high probability link to\nthe page Dynasties inChinese history , ought to help match Yuan dynasty .\nLet\u2019s see how this works. Given a question q, for each candidate anchors span\nadetected in q, we assign a relatedness score to each possible entity e2E(a)ofa.\nThe relatedness score of the link a!eis the weighted average relatedness between\neand all other entities in q. Two entities are considered related to the extent their\nWikipedia pages share many in-links. More formally, the relatedness between two\nentities AandBis computed as\nrel(A;B) =log(max(jin(A)j;jin(B)j))\u0000log(jin(A)\\in(B)j)\nlog(jWj)\u0000log(min(jin(A)j;jin(B)j))(23.59)",
    "metadata": {
      "source": "23",
      "chunk_id": 41,
      "token_count": 341,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22\n\n22 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\nwhere in (x)is the set of Wikipedia pages pointing to xandWis the set of all Wiki-\npedia pages in the collection.\nThe vote given by anchor bto the candidate annotation a!Xis the average,\nover all the possible entities of b, of their relatedness to X, weighted by their prior\nprobability:\nvote(b;X) =1\njE(b)jX\nY2E(b)rel(X;Y)p(Yjb) (23.60)\nThe total relatedness score for a!Xis the sum of the votes of all the other anchors\ndetected in q:\nrelatedness (a!X) =X\nb2Xqnavote(b;X) (23.61)\nTo score a!X, we combine relatedness and prior by choosing the entity X\nthat has the highest relatedness (a!X), \ufb01nding other entities within a small \u000fof\nthis value, and from this set, choosing the entity with the highest prior P(Xja). The\nresult of this step is a single entity assigned to each span in q.\nThe T AGME algorithm has one further step of pruning spurious anchor/entity\npairs, assigning a score averaging link probability with the coherence.\ncoherence (a!X) =1\njSj\u00001X\nB2SnXrel(B;X)\nscore(a!X) =coherence (a!X)+linkprob (a)\n2(23.62)\nFinally, pairs are pruned if score (a!X)<l, where the threshold lis set on a\nheld-out set.\n23.7.2 Neural Graph-based linking\nMore recent entity linking models are based on bi-encoders , encoding a candidate\nmention span, encoding an entity, and computing the dot product between the en-\ncodings. This allows embeddings for all the entities in the knowledge base to be\nprecomputed and cached (Wu et al., 2020). Let\u2019s sketch the ELQ linking algorithm\nof Li et al. (2020), which is given a question qand a set of candidate entities from\nWikipedia with associated Wikipedia text, and outputs tuples (e;ms;me)of entity id,\nmention start, and mention end. As Fig. 23.8 shows, it does this by encoding each\nWikipedia entity using text from Wikipedia, encoding each mention span using text\nfrom the question, and computing their similarity, as we describe below.\nEntity Mention Detection To get an h-dimensional embedding for each question\ntoken, the algorithm runs the question through BERT in the normal way:\n[q1\u0001\u0001\u0001qn] =BERT ([CLS]q1\u0001\u0001\u0001qn[SEP]) (23.63)\nIt then computes the likelihood of each span [i;j]inqbeing an entity mention, in\na way similar to the span-based algorithm we saw for the reader above. First we\ncompute the score for i/jbeing the start/end of a mention:\nsstart(i) =wstart\u0001qi;send(j) =wend\u0001qj; (23.64)",
    "metadata": {
      "source": "23",
      "chunk_id": 42,
      "token_count": 663,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23\n\n23.7 \u2022 E NTITY LINKING 23\nFigure 23.8 A sketch of the inference process in the ELQ algorithm for entity linking in\nquestions (Li et al., 2020). Each candidate question mention span and candidate entity are\nseparately encoded, and then scored by the entity/span dot product.\nwhere wstartandwendare vectors learned during training. Next, another trainable\nembedding, wmention is used to compute a score for each token being part of a men-\ntion:\nsmention (t) =wmention\u0001qt (23.65)\nMention probabilities are then computed by combining these three scores:\np([i;j]) =s \nsstart(i)+send(j)+jX\nt=ismention (t)!\n(23.66)\nEntity Linking To link mentions to entities, we next compute embeddings for\neach entity in the set E=e1;\u0001\u0001\u0001;ei;\u0001\u0001\u0001;ewof all Wikipedia entities. For each en-\ntityeiwe\u2019ll get text from the entity\u2019s Wikipedia page, the title t(ei)and the \ufb01rst\n128 tokens of the Wikipedia page which we\u2019ll call the description d(ei). This is\nagain run through BERT, taking the output of the CLStoken BERT [CLS] as the entity\nrepresentation:\nxei=BERT [CLS]([CLS]t(ei)[ENT]d(ei)[SEP]) (23.67)\nMention spans can be linked to entities by computing, for each entity eand span\n[i;j], the dot product similarity between the span encoding (the average of the token\nembeddings) and the entity encoding.\nyi;j=1\n(j\u0000i+1)jX\nt=iqt\ns(e;[i;j]) = x\u0001\neyi;j (23.68)\nFinally, we take a softmax to get a distribution over entities for each span:\np(ej[i;j]) =exp(s(e;[i;j]))P\ne02Eexp(s(e0;[i;j]))(23.69)\nTraining The ELQ mention detection and entity linking algorithm is fully super-\nvised. This means, unlike the anchor dictionary algorithms from Section 23.7.1,",
    "metadata": {
      "source": "23",
      "chunk_id": 43,
      "token_count": 471,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24",
    "metadata": {
      "source": "23",
      "chunk_id": 44,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "24 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\nit requires datasets with entity boundaries marked and linked. Two such labeled\ndatasets are WebQuestionsSP (Yih et al., 2016), an extension of the WebQuestions\n(Berant et al., 2013) dataset derived from Google search questions, and GraphQues-\ntions (Su et al., 2016). Both have had entity spans in the questions marked and\nlinked (Sorokin and Gurevych 2018, Li et al. 2020) resulting in entity-labeled ver-\nsions WebQSP ELand GraphQ EL(Li et al., 2020).\nGiven a training set, the ELQ mention detection and entity linking phases are\ntrained jointly, optimizing the sum of their losses. The mention detection loss is\na binary cross-entropy loss, with Lthe length of the passage and Nthe number of\ncandidates:\nLMD=\u00001\nNX\n1\u0014i\u0014j\u0014min(i+L\u00001;n)\u0000\ny[i;j]logp([i;j])+( 1\u0000y[i;j])log(1\u0000p([i;j]))\u0001\n(23.70)\nwith y[i;j]=1 if[i;j]is a gold mention span, else 0. The entity linking loss is:\nLED=\u0000logp(egj[i;j]) (23.71)\nwhere egis the gold entity for mention [i;j].\n23.8 Evaluation of Coreference Resolution\nWe evaluate coreference algorithms model-theoretically, comparing a set of hypoth-\nesischains or clusters Hproduced by the system against a set of gold or reference\nchains or clusters Rfrom a human labeling, and reporting precision and recall.\nHowever, there are a wide variety of methods for doing this comparison. In fact,\nthere are 5 common metrics used to evaluate coreference algorithms: the link based\nMUC (Vilain et al., 1995) and BLANC (Recasens and Hovy 2011, Luo et al. 2014)\nmetrics, the mention based B3metric (Bagga and Baldwin, 1998), the entity based\nCEAF metric (Luo, 2005), and the link based entity aware LEA metric (Moosavi and\nStrube, 2016).\nLet\u2019s just explore two of the metrics. The MUC F-measure (Vilain et al., 1995)MUC\nF-measure\nis based on the number of coreference links (pairs of mentions) common to Hand\nR. Precision is the number of common links divided by the number of links in H.\nRecall is the number of common links divided by the number of links in R; This\nmakes MUC biased toward systems that produce large chains (and fewer entities),\nand it ignores singletons, since they don\u2019t involve links.\nB3is mention-based rather than link-based. For each mention in the reference B3\nchain, we compute a precision and recall, and then we take a weighted sum over all\nNmentions in the document to compute a precision and recall for the entire task. For\na given mention i, letRbe the reference chain that includes i, and Hthe hypothesis\nchain that has i. The set of correct mentions in HisH\\R. Precision for mention i\nis thusjH\\Rj\njHj, and recall for mention ithusjH\\Rj\njRj. The total precision is the weighted\nsum of the precision for mention i, weighted by a weight wi. The total recall is the\nweighted sum of the recall for mention i, weighted by a weight wi. Equivalently:\nPrecision =NX",
    "metadata": {
      "source": "23",
      "chunk_id": 45,
      "token_count": 778,
      "chapter_title": ""
    }
  },
  {
    "content": "Strube, 2016).\nLet\u2019s just explore two of the metrics. The MUC F-measure (Vilain et al., 1995)MUC\nF-measure\nis based on the number of coreference links (pairs of mentions) common to Hand\nR. Precision is the number of common links divided by the number of links in H.\nRecall is the number of common links divided by the number of links in R; This\nmakes MUC biased toward systems that produce large chains (and fewer entities),\nand it ignores singletons, since they don\u2019t involve links.\nB3is mention-based rather than link-based. For each mention in the reference B3\nchain, we compute a precision and recall, and then we take a weighted sum over all\nNmentions in the document to compute a precision and recall for the entire task. For\na given mention i, letRbe the reference chain that includes i, and Hthe hypothesis\nchain that has i. The set of correct mentions in HisH\\R. Precision for mention i\nis thusjH\\Rj\njHj, and recall for mention ithusjH\\Rj\njRj. The total precision is the weighted\nsum of the precision for mention i, weighted by a weight wi. The total recall is the\nweighted sum of the recall for mention i, weighted by a weight wi. Equivalently:\nPrecision =NX\ni=1wi# of correct mentions in hypothesis chain containing entity i\n# of mentions in hypothesis chain containing entity i\nRecall =NX\ni=1wi# of correct mentions in hypothesis chain containing entity i\n# of mentions in reference chain containing entity i",
    "metadata": {
      "source": "23",
      "chunk_id": 46,
      "token_count": 349,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 25\n\n23.9 \u2022 W INOGRAD SCHEMA PROBLEMS 25\nThe weight wifor each entity can be set to different values to produce different\nversions of the algorithm.\nFollowing a proposal from Denis and Baldridge (2009), the CoNLL coreference\ncompetitions were scored based on the average of MUC, CEAF-e, and B3(Pradhan\net al. 2011, Pradhan et al. 2012b), and so it is common in many evaluation campaigns\nto report an average of these 3 metrics. See Luo and Pradhan (2016) for a detailed\ndescription of the entire set of metrics; reference implementations of these should\nbe used rather than attempting to reimplement from scratch (Pradhan et al., 2014).\nAlternative metrics have been proposed that deal with particular coreference do-\nmains or tasks. For example, consider the task of resolving mentions to named\nentities (persons, organizations, geopolitical entities), which might be useful for in-\nformation extraction or knowledge base completion. A hypothesis chain that cor-\nrectly contains all the pronouns referring to an entity, but has no version of the name\nitself, or is linked with a wrong name, is not useful for this task. We might instead\nwant a metric that weights each mention by how informative it is (with names being\nmost informative) (Chen and Ng, 2013) or a metric that considers a hypothesis to\nmatch a gold chain only if it contains at least one variant of a name (the NEC F1\nmetric of Agarwal et al. (2019)).\n23.9 Winograd Schema problems\nFrom early on in the \ufb01eld, researchers have noted that some cases of coreference\nare quite dif\ufb01cult, seeming to require world knowledge or sophisticated reasoning\nto solve. The problem was most famously pointed out by Winograd (1972) with the\nfollowing example:\n(23.72) The city council denied the demonstrators a permit because\na. they feared violence.\nb. they advocated violence.\nWinograd noticed that the antecedent that most readers preferred for the pro-\nnoun they in continuation (a) was the city council , but in (b) was the demonstrators .\nHe suggested that this requires understanding that the second clause is intended\nas an explanation of the \ufb01rst clause, and also that our cultural frames suggest that\ncity councils are perhaps more likely than demonstrators to fear violence and that\ndemonstrators might be more likely to advocate violence.\nIn an attempt to get the \ufb01eld of NLP to focus more on methods involving world\nknowledge and common-sense reasoning, Levesque (2011) proposed a challenge\ntask called the Winograd Schema Challenge .8The problems in the challenge taskWinograd\nschema\nare coreference problems designed to be easily disambiguated by the human reader,\nbut hopefully not solvable by simple techniques such as selectional restrictions, or\nother basic word association methods.\nThe problems are framed as a pair of statements that differ in a single word or\nphrase, and a coreference question:\n(23.73) The trophy didn\u2019t \ufb01t into the suitcase because it was too large .\nQuestion: What was too large ? Answer: The trophy\n8Levesque\u2019s call was quickly followed up by Levesque et al. (2012) and Rahman and Ng (2012), a\ncompetition at the IJCAI conference (Davis et al., 2017), and a natural language inference version of the\nproblem called WNLI (Wang et al., 2018).",
    "metadata": {
      "source": "23",
      "chunk_id": 47,
      "token_count": 761,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 26\n\n26 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\n(23.74) The trophy didn\u2019t \ufb01t into the suitcase because it was too small .\nQuestion: What was too small ? Answer: The suitcase\nThe problems have the following characteristics:\n1. The problems each have two parties\n2. A pronoun preferentially refers to one of the parties, but could grammatically\nalso refer to the other\n3. A question asks which party the pronoun refers to\n4. If one word in the question is changed, the human-preferred answer changes\nto the other party\nThe kind of world knowledge that might be needed to solve the problems can\nvary. In the trophy/suitcase example, it is knowledge about the physical world; that\na bigger object cannot \ufb01t into a smaller object. In the original Winograd sentence,\nit is stereotypes about social actors like politicians and protesters. In examples like\nthe following, it is knowledge about human actions like turn-taking or thanking.\n(23.75) Bill passed the gameboy to John because his turn was [over/next]. Whose\nturn was [over/next]? Answers: Bill/John\n(23.76) Joan made sure to thank Susan for all the help she had [given/received].\nWho had [given/received] help? Answers: Susan/Joan.\nAlthough the Winograd Schema was designed to require common-sense rea-\nsoning, a large percentage of the original set of problems can be solved by pre-\ntrained language models, \ufb01ne-tuned on Winograd Schema sentences (Kocijan et al.,\n2019). Large pretrained language models encode an enormous amount of world or\ncommon-sense knowledge! The current trend is therefore to propose new datasets\nwith increasingly dif\ufb01cult Winograd-like coreference resolution problems like K NOW REF\n(Emami et al., 2019), with examples like:\n(23.77) Marcus is undoubtedly faster than Jarrett right now but in [his] prime the\ngap wasn\u2019t all that big.\nIn the end, it seems likely that some combination of language modeling and knowl-\nedge will prove fruitful; indeed, it seems that knowledge-based models over\ufb01t less\nto lexical idiosyncracies in Winograd Schema training sets (Trichelair et al., 2018),\n23.10 Gender Bias in Coreference\nAs with other aspects of language processing, coreference models exhibit gender and\nother biases (Zhao et al. 2018, Rudinger et al. 2018, Webster et al. 2018). For exam-\nple the WinoBias dataset (Zhao et al., 2018) uses a variant of the Winograd Schema\nparadigm to test the extent to which coreference algorithms are biased toward link-\ning gendered pronouns with antecedents consistent with cultural stereotypes. As we\nsummarized in Chapter 6, embeddings replicate societal biases in their training test,\nsuch as associating men with historically sterotypical male occupations like doctors,\nand women with stereotypical female occupations like secretaries (Caliskan et al.\n2017, Garg et al. 2018).\nA WinoBias sentence contain two mentions corresponding to stereotypically-\nmale and stereotypically-female occupations and a gendered pronoun that must be\nlinked to one of them. The sentence cannot be disambiguated by the gender of the\npronoun, but a biased model might be distracted by this cue. Here is an example\nsentence:",
    "metadata": {
      "source": "23",
      "chunk_id": 48,
      "token_count": 742,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 27\n\n23.11 \u2022 S UMMARY 27\n(23.78) The secretary called the physician iand told him iabout a new patient\n[pro-stereotypical]\n(23.79) The secretary called the physician iand told her iabout a new patient\n[anti-stereotypical]\nZhao et al. (2018) consider a coreference system to be biased if it is more accu-\nrate at linking pronouns consistent with gender stereotypical occupations (e.g., him\nwith physician in (23.78)) than linking pronouns inconsistent with gender-stereotypical\noccupations (e.g., herwith physician in (23.79)). They show that coreference sys-\ntems of all architectures (rule-based, feature-based machine learned, and end-to-\nend-neural) all show signi\ufb01cant bias, performing on average 21 F 1points worse in\nthe anti-stereotypical cases.\nOne possible source of this bias is that female entities are signi\ufb01cantly un-\nderrepresented in the OntoNotes dataset, used to train most coreference systems.\nZhao et al. (2018) propose a way to overcome this bias: they generate a second\ngender-swapped dataset in which all male entities in OntoNotes are replaced with\nfemale ones and vice versa, and retrain coreference systems on the combined orig-\ninal and swapped OntoNotes data, also using debiased GloVE embeddings (Boluk-\nbasi et al., 2016). The resulting coreference systems no longer exhibit bias on the\nWinoBias dataset, without signi\ufb01cantly impacting OntoNotes coreference accuracy.\nIn a follow-up paper, Zhao et al. (2019) show that the same biases exist in ELMo\ncontextualized word vector representations and coref systems that use them. They\nshowed that retraining ELMo with data augmentation again reduces or removes bias\nin coreference systems on WinoBias.\nWebster et al. (2018) introduces another dataset, GAP, and the task of Gendered\nPronoun Resolution as a tool for developing improved coreference algorithms for\ngendered pronouns. GAP is a gender-balanced labeled corpus of 4,454 sentences\nwith gendered ambiguous pronouns (by contrast, only 20% of the gendered pro-\nnouns in the English OntoNotes training data are feminine). The examples were\ncreated by drawing on naturally occurring sentences from Wikipedia pages to create\nhard to resolve cases with two named entities of the same gender and an ambiguous\npronoun that may refer to either person (or neither), like the following:\n(23.80) In May, Fujisawa joined Mari Motohashi\u2019s rink as the team\u2019s skip, moving\nback from Karuizawa to Kitami where shehad spent her junior days.\nWebster et al. (2018) show that modern coreference algorithms perform signif-\nicantly worse on resolving feminine pronouns than masculine pronouns in GAP.\nKurita et al. (2019) shows that a system based on BERT contextualized word repre-\nsentations shows similar bias.\n23.11 Summary\nThis chapter introduced the task of coreference resolution .\n\u2022 This is the task of linking together mentions in text which corefer , i.e. refer\nto the same discourse entity in the discourse model , resulting in a set of\ncoreference chains (also called clusters orentities ).\n\u2022 Mentions can be de\ufb01nite NPs orinde\ufb01nite NPs ,pronouns (including zero\npronouns ) ornames .",
    "metadata": {
      "source": "23",
      "chunk_id": 49,
      "token_count": 760,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 28\n\n28 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\n\u2022 The surface form of an entity mention is linked to its information status\n(new,old, orinferrable ), and how accessible orsalient the entity is.\n\u2022 Some NPs are not referring expressions, such as pleonastic itinIt is raining .\n\u2022 Many corpora have human-labeled coreference annotations that can be used\nfor supervised learning, including OntoNotes for English, Chinese, and Ara-\nbic, ARRAU for English, and AnCora for Spanish and Catalan.\n\u2022 Mention detection can start with all nouns and named entities and then use\nanaphoricity classi\ufb01ers orreferentiality classi\ufb01ers to \ufb01lter out non-mentions.\n\u2022 Three common architectures for coreference are mention-pair ,mention-rank ,\nandentity-based , each of which can make use of feature-based or neural clas-\nsi\ufb01ers.\n\u2022 Modern coreference systems tend to be end-to-end, performing mention de-\ntection and coreference in a single end-to-end architecture.\n\u2022 Algorithms learn representations for text spans and heads, and learn to com-\npare anaphor spans with candidate antecedent spans.\n\u2022 Entity linking is the task of associating a mention in text with the representa-\ntion of some real-world entity in an ontology .\n\u2022 Coreference systems are evaluated by comparing with gold entity labels using\nprecision/recall metrics like MUC ,B3,CEAF ,BLANC , orLEA .\n\u2022 The Winograd Schema Challenge problems are dif\ufb01cult coreference prob-\nlems that seem to require world knowledge or sophisticated reasoning to solve.\n\u2022 Coreference systems exhibit gender bias which can be evaluated using datasets\nlike Winobias and GAP.\nBibliographical and Historical Notes\nCoreference has been part of natural language processing since the 1970s (Woods\net al. 1972, Winograd 1972). The discourse model and the entity-centric foundation\nof coreference was formulated by Karttunen (1969) (at the 3rd COLING confer-\nence), playing a role also in linguistic semantics (Heim 1982, Kamp 1981). But\nit was Bonnie Webber\u2019s 1978 dissertation and following work (Webber 1983) that\nexplored the model\u2019s computational aspects, providing fundamental insights into\nhow entities are represented in the discourse model and the ways in which they can\nlicense subsequent reference. Many of the examples she provided continue to chal-\nlenge theories of reference to this day.\nThe Hobbs algorithm9is a tree-search algorithm that was the \ufb01rst in a longHobbs\nalgorithm\nseries of syntax-based methods for identifying reference robustly in naturally occur-\nring text. The input to the Hobbs algorithm is a pronoun to be resolved, together\nwith a syntactic (constituency) parse of the sentences up to and including the cur-\nrent sentence. The details of the algorithm depend on the grammar used, but can be\nunderstood from a simpli\ufb01ed version due to Kehler et al. (2004) that just searches\nthrough the list of NPs in the current and prior sentences. This simpli\ufb01ed Hobbs\nalgorithm searches NPs in the following order: \u201c(i) in the current sentence from\nright-to-left, starting with the \ufb01rst NP to the left of the pronoun, (ii) in the previous\nsentence from left-to-right, (iii) in two sentences prior from left-to-right, and (iv) in\n9The simpler of two algorithms presented originally in Hobbs (1978).",
    "metadata": {
      "source": "23",
      "chunk_id": 50,
      "token_count": 769,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 29",
    "metadata": {
      "source": "23",
      "chunk_id": 51,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 29\nthe current sentence from left-to-right, starting with the \ufb01rst noun group to the right\nof the pronoun (for cataphora). The \ufb01rst noun group that agrees with the pronoun\nwith respect to number, gender, and person is chosen as the antecedent\u201d (Kehler\net al., 2004).\nLappin and Leass (1994) was an in\ufb02uential entity-based system that used weights\nto combine syntactic and other features, extended soon after by Kennedy and Bogu-\nraev (1996) whose system avoids the need for full syntactic parses.\nApproximately contemporaneously centering (Grosz et al., 1995) was applied\nto pronominal anaphora resolution by Brennan et al. (1987), and a wide variety of\nwork followed focused on centering\u2019s use in coreference (Kameyama 1986, Di Eu-\ngenio 1990, Walker et al. 1994, Di Eugenio 1996, Strube and Hahn 1996, Kehler\n1997, Tetreault 2001, Iida et al. 2003). Kehler and Rohde (2013) show how center-\ning can be integrated with coherence-driven theories of pronoun interpretation. See\nChapter 24 for the use of centering in measuring discourse coherence.\nCoreference competitions as part of the US DARPA-sponsored MUC confer-\nences provided early labeled coreference datasets (the 1995 MUC-6 and 1998 MUC-\n7 corpora), and set the tone for much later work, choosing to focus exclusively\non the simplest cases of identity coreference (ignoring dif\ufb01cult cases like bridging,\nmetonymy, and part-whole) and drawing the community toward supervised machine\nlearning and metrics like the MUC metric (Vilain et al., 1995). The later ACE eval-\nuations produced labeled coreference corpora in English, Chinese, and Arabic that\nwere widely used for model training and evaluation.\nThis DARPA work in\ufb02uenced the community toward supervised learning begin-\nning in the mid-90s (Connolly et al. 1994, Aone and Bennett 1995, McCarthy and\nLehnert 1995). Soon et al. (2001) laid out a set of basic features, extended by Ng and\nCardie (2002b), and a series of machine learning models followed over the next 15\nyears. These often focused separately on pronominal anaphora resolution (Kehler\net al. 2004, Bergsma and Lin 2006), full NP coreference (Cardie and Wagstaff 1999,\nNg and Cardie 2002b, Ng 2005a) and de\ufb01nite NP reference (Poesio and Vieira 1998,\nVieira and Poesio 2000), as well as separate anaphoricity detection (Bean and Riloff\n1999, Bean and Riloff 2004, Ng and Cardie 2002a, Ng 2004), or singleton detection\n(de Marneffe et al., 2015).\nThe move from mention-pair to mention-ranking approaches was pioneered by\nYang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods,\nthen extended by Denis and Baldridge (2008) who proposed to do ranking via a soft-\nmax over all prior mentions. The idea of doing mention detection, anaphoricity, and",
    "metadata": {
      "source": "23",
      "chunk_id": 52,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "Lehnert 1995). Soon et al. (2001) laid out a set of basic features, extended by Ng and\nCardie (2002b), and a series of machine learning models followed over the next 15\nyears. These often focused separately on pronominal anaphora resolution (Kehler\net al. 2004, Bergsma and Lin 2006), full NP coreference (Cardie and Wagstaff 1999,\nNg and Cardie 2002b, Ng 2005a) and de\ufb01nite NP reference (Poesio and Vieira 1998,\nVieira and Poesio 2000), as well as separate anaphoricity detection (Bean and Riloff\n1999, Bean and Riloff 2004, Ng and Cardie 2002a, Ng 2004), or singleton detection\n(de Marneffe et al., 2015).\nThe move from mention-pair to mention-ranking approaches was pioneered by\nYang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods,\nthen extended by Denis and Baldridge (2008) who proposed to do ranking via a soft-\nmax over all prior mentions. The idea of doing mention detection, anaphoricity, and\ncoreference jointly in a single end-to-end model grew out of the early proposal of Ng\n(2005b) to use a dummy antecedent for mention-ranking, allowing \u2018non-referential\u2019\nto be a choice for coreference classi\ufb01ers, Denis and Baldridge\u2019s 2007 joint system\ncombining anaphoricity classi\ufb01er probabilities with coreference probabilities, the\nDenis and Baldridge (2008) ranking model, and the Rahman and Ng (2009) pro-\nposal to train the two models jointly with a single objective.\nSimple rule-based systems for coreference returned to prominence in the 2010s,\npartly because of their ability to encode entity-based features in a high-precision\nway (Zhou et al. 2004, Haghighi and Klein 2009, Raghunathan et al. 2010, Lee et al.\n2011, Lee et al. 2013, Hajishirzi et al. 2013) but in the end they suffered from an\ninability to deal with the semantics necessary to correctly handle cases of common\nnoun coreference.\nA return to supervised learning led to a number of advances in mention-ranking\nmodels which were also extended into neural architectures, for example using re-",
    "metadata": {
      "source": "23",
      "chunk_id": 53,
      "token_count": 545,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 30",
    "metadata": {
      "source": "23",
      "chunk_id": 54,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "30 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\ninforcement learning to directly optimize coreference evaluation models Clark and\nManning (2016a), doing end-to-end coreference all the way from span extraction\n(Lee et al. 2017b, Zhang et al. 2018). Neural models also were designed to take\nadvantage of global entity-level information (Clark and Manning 2016b, Wiseman\net al. 2016, Lee et al. 2018).\nCoreference is also related to the task of entity linking discussed in Chapter 14.\nCoreference can help entity linking by giving more possible surface forms to help\nlink to the right Wikipedia page, and conversely entity linking can help improve\ncoreference resolution. Consider this example from Hajishirzi et al. (2013):\n(23.81) [Michael Eisner] 1and [Donald Tsang] 2announced the grand opening of\n[[Hong Kong] 3Disneyland] 4yesterday. [Eisner] 1thanked [the President] 2\nand welcomed [fans] 5to [the park] 4.\nIntegrating entity linking into coreference can help draw encyclopedic knowl-\nedge (like the fact that Donald Tsang is a president) to help disambiguate the men-\ntionthe President . Ponzetto and Strube (2006) 2007 and Ratinov and Roth (2012)\nshowed that such attributes extracted from Wikipedia pages could be used to build\nricher models of entity mentions in coreference. More recent research shows how to\ndo linking and coreference jointly (Hajishirzi et al. 2013, Zheng et al. 2013) or even\njointly with named entity tagging as well (Durrett and Klein 2014).\nThe coreference task as we introduced it involves a simplifying assumption that\nthe relationship between an anaphor and its antecedent is one of identity : the two\ncoreferring mentions refer to the identical discourse referent. In real texts, the rela-\ntionship can be more complex, where different aspects of a discourse referent can\nbe neutralized or refocused. For example (23.82) (Recasens et al., 2011) shows an\nexample of metonymy , in which the capital city Washington is used metonymically metonymy\nto refer to the US. (23.83-23.84) show other examples (Recasens et al., 2011):\n(23.82) a strict interpretation of a policy requires The U.S. to notify foreign\ndictators of certain coup plots ... Washington rejected the bid ...\n(23.83) I once crossed that border into Ashgh-Abad on Nowruz, the Persian New\nYear. In the South, everyone was celebrating New Year ; to the North, it\nwas a regular day.\n(23.84) In France, the president is elected for a term of seven years, while in the\nUnited States heis elected for a term of four years.\nFor further linguistic discussions of these complications of coreference see Puste-\njovsky (1991), van Deemter and Kibble (2000), Poesio et al. (2006), Fauconnier and\nTurner (2008), Versley (2008), and Barker (2010).\nNg (2017) offers a useful compact history of machine learning models in corefer-\nence resolution. There are three excellent book-length surveys of anaphora/coreference\nresolution, covering different time periods: Hirst (1981) (early work until about",
    "metadata": {
      "source": "23",
      "chunk_id": 55,
      "token_count": 778,
      "chapter_title": ""
    }
  },
  {
    "content": "example of metonymy , in which the capital city Washington is used metonymically metonymy\nto refer to the US. (23.83-23.84) show other examples (Recasens et al., 2011):\n(23.82) a strict interpretation of a policy requires The U.S. to notify foreign\ndictators of certain coup plots ... Washington rejected the bid ...\n(23.83) I once crossed that border into Ashgh-Abad on Nowruz, the Persian New\nYear. In the South, everyone was celebrating New Year ; to the North, it\nwas a regular day.\n(23.84) In France, the president is elected for a term of seven years, while in the\nUnited States heis elected for a term of four years.\nFor further linguistic discussions of these complications of coreference see Puste-\njovsky (1991), van Deemter and Kibble (2000), Poesio et al. (2006), Fauconnier and\nTurner (2008), Versley (2008), and Barker (2010).\nNg (2017) offers a useful compact history of machine learning models in corefer-\nence resolution. There are three excellent book-length surveys of anaphora/coreference\nresolution, covering different time periods: Hirst (1981) (early work until about\n1981), Mitkov (2002) (1986-2001), and Poesio et al. (2016) (2001-2015).\nAndy Kehler wrote the Discourse chapter for the 2000 \ufb01rst edition of this text-\nbook, which we used as the starting point for the second-edition chapter, and there\nare some remnants of Andy\u2019s lovely prose still in this third-edition coreference chap-\nter.\nExercises",
    "metadata": {
      "source": "23",
      "chunk_id": 56,
      "token_count": 379,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 31",
    "metadata": {
      "source": "23",
      "chunk_id": 57,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Exercises 31\nAgarwal, O., S. Subramanian, A. Nenkova, and D. Roth.\n2019. Evaluation of named entity coreference. Work-\nshop on Computational Models of Reference, Anaphora\nand Coreference .\nAone, C. and S. W. Bennett. 1995. Evaluating automated\nand manual acquisition of anaphora resolution strategies.\nACL.\nAriel, M. 2001. Accessibility theory: An overview. In\nT. Sanders, J. Schilperoord, and W. Spooren, eds, Text\nRepresentation: Linguistic and Psycholinguistic Aspects ,\n29\u201387. Benjamins.\nBagga, A. and B. Baldwin. 1998. Algorithms for scoring\ncoreference chains. LREC Workshop on Linguistic Coref-\nerence .\nBamman, D., O. Lewke, and A. Mansoor. 2020. An anno-\ntated dataset of coreference in English literature. LREC .\nBarker, C. 2010. Nominals don\u2019t provide criteria of iden-\ntity. In M. Rathert and A. Alexiadou, eds, The Semantics\nof Nominalizations across Languages and Frameworks ,\n9\u201324. Mouton.\nBean, D. and E. Riloff. 1999. Corpus-based identi\ufb01cation of\nnon-anaphoric noun phrases. ACL.\nBean, D. and E. Riloff. 2004. Unsupervised learning of con-\ntextual role knowledge for coreference resolution. HLT-\nNAACL .\nBengtson, E. and D. Roth. 2008. Understanding the value of\nfeatures for coreference resolution. EMNLP .\nBerant, J., A. Chou, R. Frostig, and P. Liang. 2013. Semantic\nparsing on freebase from question-answer pairs. EMNLP .\nBergsma, S. and D. Lin. 2006. Bootstrapping path-based\npronoun resolution. COLING/ACL .\nBergsma, S., D. Lin, and R. Goebel. 2008. Distributional\nidenti\ufb01cation of non-referential pronouns. ACL.\nBj\u00a8orkelund, A. and J. Kuhn. 2014. Learning structured\nperceptrons for coreference resolution with latent an-\ntecedents and non-local features. ACL.\nBolukbasi, T., K.-W. Chang, J. Zou, V . Saligrama, and A. T.\nKalai. 2016. Man is to computer programmer as woman\nis to homemaker? Debiasing word embeddings. NeurIPS .\nBrennan, S. E., M. W. Friedman, and C. Pollard. 1987. A\ncentering approach to pronouns. ACL.\nCaliskan, A., J. J. Bryson, and A. Narayanan. 2017. Seman-\ntics derived automatically from language corpora contain\nhuman-like biases. Science , 356(6334):183\u2013186.\nCardie, C. and K. Wagstaff. 1999. Noun phrase coreference\nas clustering. EMNLP/VLC .\nChafe, W. L. 1976. Givenness, contrastiveness, de\ufb01niteness,\nsubjects, topics, and point of view. In C. N. Li, ed., Sub-\nject and Topic , 25\u201355. Academic Press.",
    "metadata": {
      "source": "23",
      "chunk_id": 58,
      "token_count": 764,
      "chapter_title": ""
    }
  },
  {
    "content": "Bj\u00a8orkelund, A. and J. Kuhn. 2014. Learning structured\nperceptrons for coreference resolution with latent an-\ntecedents and non-local features. ACL.\nBolukbasi, T., K.-W. Chang, J. Zou, V . Saligrama, and A. T.\nKalai. 2016. Man is to computer programmer as woman\nis to homemaker? Debiasing word embeddings. NeurIPS .\nBrennan, S. E., M. W. Friedman, and C. Pollard. 1987. A\ncentering approach to pronouns. ACL.\nCaliskan, A., J. J. Bryson, and A. Narayanan. 2017. Seman-\ntics derived automatically from language corpora contain\nhuman-like biases. Science , 356(6334):183\u2013186.\nCardie, C. and K. Wagstaff. 1999. Noun phrase coreference\nas clustering. EMNLP/VLC .\nChafe, W. L. 1976. Givenness, contrastiveness, de\ufb01niteness,\nsubjects, topics, and point of view. In C. N. Li, ed., Sub-\nject and Topic , 25\u201355. Academic Press.\nChang, K.-W., R. Samdani, and D. Roth. 2013. A con-\nstrained latent variable model for coreference resolution.\nEMNLP .\nChang, K.-W., R. Samdani, A. Rozovskaya, M. Sammons,\nand D. Roth. 2012. Illinois-Coref: The UI system in the\nCoNLL-2012 shared task. CoNLL .\nChen, C. and V . Ng. 2013. Linguistically aware coreference\nevaluation metrics. IJCNLP .Chomsky, N. 1981. Lectures on Government and Binding .\nForis.\nClark, K. and C. D. Manning. 2015. Entity-centric corefer-\nence resolution with model stacking. ACL.\nClark, K. and C. D. Manning. 2016a. Deep reinforce-\nment learning for mention-ranking coreference models.\nEMNLP .\nClark, K. and C. D. Manning. 2016b. Improving coreference\nresolution by learning entity-level distributed representa-\ntions. ACL.\nConnolly, D., J. D. Burger, and D. S. Day. 1994. A machine\nlearning approach to anaphoric reference. Proceedings\nof the International Conference on New Methods in Lan-\nguage Processing (NeMLaP) .\nCucerzan, S. 2007. Large-scale named entity disambiguation\nbased on Wikipedia data. EMNLP/CoNLL .\nDavis, E., L. Morgenstern, and C. L. Ortiz. 2017. The \ufb01rst\nWinograd schema challenge at IJCAI-16. AI Magazine ,\n38(3):97\u201398.\nDenis, P. and J. Baldridge. 2007. Joint determination\nof anaphoricity and coreference resolution using integer\nprogramming. NAACL-HLT .\nDenis, P. and J. Baldridge. 2008. Specialized models and\nranking for coreference resolution. EMNLP .\nDenis, P. and J. Baldridge. 2009. Global joint models for\ncoreference resolution and named entity classi\ufb01cation.\nProcesamiento del Lenguaje Natural , 42.",
    "metadata": {
      "source": "23",
      "chunk_id": 59,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "EMNLP .\nClark, K. and C. D. Manning. 2016b. Improving coreference\nresolution by learning entity-level distributed representa-\ntions. ACL.\nConnolly, D., J. D. Burger, and D. S. Day. 1994. A machine\nlearning approach to anaphoric reference. Proceedings\nof the International Conference on New Methods in Lan-\nguage Processing (NeMLaP) .\nCucerzan, S. 2007. Large-scale named entity disambiguation\nbased on Wikipedia data. EMNLP/CoNLL .\nDavis, E., L. Morgenstern, and C. L. Ortiz. 2017. The \ufb01rst\nWinograd schema challenge at IJCAI-16. AI Magazine ,\n38(3):97\u201398.\nDenis, P. and J. Baldridge. 2007. Joint determination\nof anaphoricity and coreference resolution using integer\nprogramming. NAACL-HLT .\nDenis, P. and J. Baldridge. 2008. Specialized models and\nranking for coreference resolution. EMNLP .\nDenis, P. and J. Baldridge. 2009. Global joint models for\ncoreference resolution and named entity classi\ufb01cation.\nProcesamiento del Lenguaje Natural , 42.\nDi Eugenio, B. 1990. Centering theory and the Italian\npronominal system. COLING .\nDi Eugenio, B. 1996. The discourse functions of Italian sub-\njects: A centering approach. COLING .\nDurrett, G. and D. Klein. 2013. Easy victories and uphill\nbattles in coreference resolution. EMNLP .\nDurrett, G. and D. Klein. 2014. A joint model for entity anal-\nysis: Coreference, typing, and linking. TACL , 2:477\u2013490.\nEmami, A., P. Trichelair, A. Trischler, K. Suleman,\nH. Schulz, and J. C. K. Cheung. 2019. The KNOWREF\ncoreference corpus: Removing gender and number cues\nfor dif\ufb01cult pronominal anaphora resolution. ACL.\nFauconnier, G. and M. Turner. 2008. The way we think: Con-\nceptual blending and the mind\u2019s hidden complexities . Ba-\nsic Books.\nFernandes, E. R., C. N. dos Santos, and R. L. Milidi \u00b4u. 2012.\nLatent structure perceptron with feature induction for un-\nrestricted coreference resolution. CoNLL .\nFerragina, P. and U. Scaiella. 2011. Fast and accurate anno-\ntation of short texts with wikipedia pages. IEEE Software ,\n29(1):70\u201375.\nFox, B. A. 1993. Discourse Structure and Anaphora: Writ-\nten and Conversational English . Cambridge.\nGarg, N., L. Schiebinger, D. Jurafsky, and J. Zou. 2018.\nWord embeddings quantify 100 years of gender and eth-\nnic stereotypes. Proceedings of the National Academy of\nSciences , 115(16):E3635\u2013E3644.\nGrosz, B. J. 1977. The Representation and Use of Focus\nin Dialogue Understanding . Ph.D. thesis, University of\nCalifornia, Berkeley.",
    "metadata": {
      "source": "23",
      "chunk_id": 60,
      "token_count": 744,
      "chapter_title": ""
    }
  },
  {
    "content": "coreference corpus: Removing gender and number cues\nfor dif\ufb01cult pronominal anaphora resolution. ACL.\nFauconnier, G. and M. Turner. 2008. The way we think: Con-\nceptual blending and the mind\u2019s hidden complexities . Ba-\nsic Books.\nFernandes, E. R., C. N. dos Santos, and R. L. Milidi \u00b4u. 2012.\nLatent structure perceptron with feature induction for un-\nrestricted coreference resolution. CoNLL .\nFerragina, P. and U. Scaiella. 2011. Fast and accurate anno-\ntation of short texts with wikipedia pages. IEEE Software ,\n29(1):70\u201375.\nFox, B. A. 1993. Discourse Structure and Anaphora: Writ-\nten and Conversational English . Cambridge.\nGarg, N., L. Schiebinger, D. Jurafsky, and J. Zou. 2018.\nWord embeddings quantify 100 years of gender and eth-\nnic stereotypes. Proceedings of the National Academy of\nSciences , 115(16):E3635\u2013E3644.\nGrosz, B. J. 1977. The Representation and Use of Focus\nin Dialogue Understanding . Ph.D. thesis, University of\nCalifornia, Berkeley.\nGrosz, B. J., A. K. Joshi, and S. Weinstein. 1995. Center-\ning: A framework for modeling the local coherence of\ndiscourse. Computational Linguistics , 21(2):203\u2013225.",
    "metadata": {
      "source": "23",
      "chunk_id": 61,
      "token_count": 337,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 32",
    "metadata": {
      "source": "23",
      "chunk_id": 62,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "32 Chapter 23 \u2022 Coreference Resolution and Entity Linking\nGundel, J. K., N. Hedberg, and R. Zacharski. 1993. Cog-\nnitive status and the form of referring expressions in dis-\ncourse. Language , 69(2):274\u2013307.\nHaghighi, A. and D. Klein. 2009. Simple coreference reso-\nlution with rich syntactic and semantic features. EMNLP .\nHajishirzi, H., L. Zilles, D. S. Weld, and L. Zettlemoyer.\n2013. Joint coreference resolution and named-entity link-\ning with multi-pass sieves. EMNLP .\nHaviland, S. E. and H. H. Clark. 1974. What\u2019s new? Acquir-\ning new information as a process in comprehension. Jour-\nnal of Verbal Learning and Verbal Behaviour , 13:512\u2013\n521.\nHawkins, J. A. 1978. De\ufb01niteness and inde\ufb01niteness: a study\nin reference and grammaticality prediction . Croom Helm\nLtd.\nHeim, I. 1982. The semantics of de\ufb01nite and inde\ufb01nite noun\nphrases . Ph.D. thesis, University of Massachusetts at\nAmherst.\nHirst, G. 1981. Anaphora in Natural Language Understand-\ning: A survey . Number 119 in Lecture notes in computer\nscience. Springer-Verlag.\nHobbs, J. R. 1978. Resolving pronoun references. Lingua ,\n44:311\u2013338.\nHou, Y ., K. Markert, and M. Strube. 2018. Unre-\nstricted bridging resolution. Computational Linguistics ,\n44(2):237\u2013284.\nIida, R., K. Inui, H. Takamura, and Y . Matsumoto. 2003. In-\ncorporating contextual cues in trainable models for coref-\nerence resolution. EACL Workshop on The Computa-\ntional Treatment of Anaphora .\nJi, H. and R. Grishman. 2011. Knowledge base population:\nSuccessful approaches and challenges. ACL.\nJoshi, M., O. Levy, D. S. Weld, and L. Zettlemoyer. 2019.\nBERT for coreference resolution: Baselines and analysis.\nEMNLP .\nKameyama, M. 1986. A property-sharing constraint in cen-\ntering. ACL.\nKamp, H. 1981. A theory of truth and semantic represen-\ntation. In J. Groenendijk, T. Janssen, and M. Stokhof,\neds,Formal Methods in the Study of Language , 189\u2013222.\nMathematical Centre, Amsterdam.\nKarttunen, L. 1969. Discourse referents. COLING . Preprint\nNo. 70.\nKehler, A. 1997. Current theories of centering for pronoun\ninterpretation: A critical evaluation. Computational Lin-\nguistics , 23(3):467\u2013475.\nKehler, A., D. E. Appelt, L. Taylor, and A. Simma. 2004. The\n(non)utility of predicate-argument frequencies for pro-\nnoun interpretation. HLT-NAACL .\nKehler, A. and H. Rohde. 2013. A probabilistic reconcil-",
    "metadata": {
      "source": "23",
      "chunk_id": 63,
      "token_count": 750,
      "chapter_title": ""
    }
  },
  {
    "content": "Successful approaches and challenges. ACL.\nJoshi, M., O. Levy, D. S. Weld, and L. Zettlemoyer. 2019.\nBERT for coreference resolution: Baselines and analysis.\nEMNLP .\nKameyama, M. 1986. A property-sharing constraint in cen-\ntering. ACL.\nKamp, H. 1981. A theory of truth and semantic represen-\ntation. In J. Groenendijk, T. Janssen, and M. Stokhof,\neds,Formal Methods in the Study of Language , 189\u2013222.\nMathematical Centre, Amsterdam.\nKarttunen, L. 1969. Discourse referents. COLING . Preprint\nNo. 70.\nKehler, A. 1997. Current theories of centering for pronoun\ninterpretation: A critical evaluation. Computational Lin-\nguistics , 23(3):467\u2013475.\nKehler, A., D. E. Appelt, L. Taylor, and A. Simma. 2004. The\n(non)utility of predicate-argument frequencies for pro-\nnoun interpretation. HLT-NAACL .\nKehler, A. and H. Rohde. 2013. A probabilistic reconcil-\niation of coherence-driven and centering-driven theories\nof pronoun interpretation. Theoretical Linguistics , 39(1-\n2):1\u201337.\nKennedy, C. and B. K. Boguraev. 1996. Anaphora for every-\none: Pronominal anaphora resolution without a parser.\nCOLING .\nKocijan, V ., A.-M. Cretu, O.-M. Camburu, Y . Yordanov, and\nT. Lukasiewicz. 2019. A surprisingly robust trick for the\nWinograd Schema Challenge. ACL.Kolhatkar, V ., A. Roussel, S. Dipper, and H. Zinsmeister.\n2018. Anaphora with non-nominal antecedents in compu-\ntational linguistics: A survey. Computational Linguistics ,\n44(3):547\u2013612.\nKummerfeld, J. K. and D. Klein. 2013. Error-driven analysis\nof challenges in coreference resolution. EMNLP .\nKurita, K., N. Vyas, A. Pareek, A. W. Black, and\nY . Tsvetkov. 2019. Quantifying social biases in contex-\ntual word representations. 1st ACL Workshop on Gender\nBias for Natural Language Processing .\nLappin, S. and H. Leass. 1994. An algorithm for pronom-\ninal anaphora resolution. Computational Linguistics ,\n20(4):535\u2013561.\nLee, H., A. Chang, Y . Peirsman, N. Chambers, M. Surdeanu,\nand D. Jurafsky. 2013. Deterministic coreference resolu-\ntion based on entity-centric, precision-ranked rules. Com-\nputational Linguistics , 39(4):885\u2013916.\nLee, H., Y . Peirsman, A. Chang, N. Chambers, M. Surdeanu,\nand D. Jurafsky. 2011. Stanford\u2019s multi-pass sieve coref-\nerence resolution system at the CoNLL-2011 shared task.\nCoNLL .\nLee, H., M. Surdeanu, and D. Jurafsky. 2017a. A scaffolding",
    "metadata": {
      "source": "23",
      "chunk_id": 64,
      "token_count": 755,
      "chapter_title": ""
    }
  },
  {
    "content": "of challenges in coreference resolution. EMNLP .\nKurita, K., N. Vyas, A. Pareek, A. W. Black, and\nY . Tsvetkov. 2019. Quantifying social biases in contex-\ntual word representations. 1st ACL Workshop on Gender\nBias for Natural Language Processing .\nLappin, S. and H. Leass. 1994. An algorithm for pronom-\ninal anaphora resolution. Computational Linguistics ,\n20(4):535\u2013561.\nLee, H., A. Chang, Y . Peirsman, N. Chambers, M. Surdeanu,\nand D. Jurafsky. 2013. Deterministic coreference resolu-\ntion based on entity-centric, precision-ranked rules. Com-\nputational Linguistics , 39(4):885\u2013916.\nLee, H., Y . Peirsman, A. Chang, N. Chambers, M. Surdeanu,\nand D. Jurafsky. 2011. Stanford\u2019s multi-pass sieve coref-\nerence resolution system at the CoNLL-2011 shared task.\nCoNLL .\nLee, H., M. Surdeanu, and D. Jurafsky. 2017a. A scaffolding\napproach to coreference resolution integrating statistical\nand rule-based models. Natural Language Engineering ,\n23(5):733\u2013762.\nLee, K., L. He, M. Lewis, and L. Zettlemoyer. 2017b. End-\nto-end neural coreference resolution. EMNLP .\nLee, K., L. He, and L. Zettlemoyer. 2018. Higher-\norder coreference resolution with coarse-to-\ufb01ne infer-\nence. NAACL HLT .\nLevesque, H. 2011. The Winograd Schema Challenge. Logi-\ncal Formalizations of Commonsense Reasoning \u2014 Papers\nfrom the AAAI 2011 Spring Symposium (SS-11-06) .\nLevesque, H., E. Davis, and L. Morgenstern. 2012. The\nWinograd Schema Challenge. KR-12 .\nLi, B. Z., S. Min, S. Iyer, Y . Mehdad, and W.-t. Yih. 2020.\nEf\ufb01cient one-pass end-to-end entity linking for questions.\nEMNLP .\nLuo, X. 2005. On coreference resolution performance met-\nrics. EMNLP .\nLuo, X. and S. Pradhan. 2016. Evaluation metrics. In\nM. Poesio, R. Stuckardt, and Y . Versley, eds, Anaphora\nresolution: Algorithms, resources, and applications ,\n141\u2013163. Springer.\nLuo, X., S. Pradhan, M. Recasens, and E. H. Hovy. 2014. An\nextension of BLANC to system mentions. ACL.\nde Marneffe, M.-C., M. Recasens, and C. Potts. 2015. Mod-\neling the lifespan of discourse entities with application to\ncoreference resolution. JAIR , 52:445\u2013475.\nMartschat, S. and M. Strube. 2014. Recall error analysis for\ncoreference resolution. EMNLP .\nMartschat, S. and M. Strube. 2015. Latent structures for\ncoreference resolution. TACL , 3:405\u2013418.",
    "metadata": {
      "source": "23",
      "chunk_id": 65,
      "token_count": 748,
      "chapter_title": ""
    }
  },
  {
    "content": "Ef\ufb01cient one-pass end-to-end entity linking for questions.\nEMNLP .\nLuo, X. 2005. On coreference resolution performance met-\nrics. EMNLP .\nLuo, X. and S. Pradhan. 2016. Evaluation metrics. In\nM. Poesio, R. Stuckardt, and Y . Versley, eds, Anaphora\nresolution: Algorithms, resources, and applications ,\n141\u2013163. Springer.\nLuo, X., S. Pradhan, M. Recasens, and E. H. Hovy. 2014. An\nextension of BLANC to system mentions. ACL.\nde Marneffe, M.-C., M. Recasens, and C. Potts. 2015. Mod-\neling the lifespan of discourse entities with application to\ncoreference resolution. JAIR , 52:445\u2013475.\nMartschat, S. and M. Strube. 2014. Recall error analysis for\ncoreference resolution. EMNLP .\nMartschat, S. and M. Strube. 2015. Latent structures for\ncoreference resolution. TACL , 3:405\u2013418.\nMcCarthy, J. F. and W. G. Lehnert. 1995. Using decision\ntrees for coreference resolution. IJCAI-95 .\nMihalcea, R. and A. Csomai. 2007. Wikify!: Linking docu-\nments to encyclopedic knowledge. CIKM 2007 .\nMilne, D. and I. H. Witten. 2008. Learning to link with wiki-\npedia. CIKM 2008 .",
    "metadata": {
      "source": "23",
      "chunk_id": 66,
      "token_count": 365,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 33",
    "metadata": {
      "source": "23",
      "chunk_id": 67,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Exercises 33\nMitkov, R. 2002. Anaphora Resolution . Longman.\nMoosavi, N. S. and M. Strube. 2016. Which coreference\nevaluation metric do you trust? A proposal for a link-\nbased entity aware metric. ACL.\nNg, V . 2004. Learning noun phrase anaphoricity to improve\ncoreference resolution: Issues in representation and opti-\nmization. ACL.\nNg, V . 2005a. Machine learning for coreference resolution:\nFrom local classi\ufb01cation to global ranking. ACL.\nNg, V . 2005b. Supervised ranking for pronoun resolution:\nSome recent improvements. AAAI .\nNg, V . 2010. Supervised noun phrase coreference research:\nThe \ufb01rst \ufb01fteen years. ACL.\nNg, V . 2017. Machine learning for entity coreference reso-\nlution: A retrospective look at two decades of research.\nAAAI .\nNg, V . and C. Cardie. 2002a. Identifying anaphoric and non-\nanaphoric noun phrases to improve coreference resolu-\ntion. COLING .\nNg, V . and C. Cardie. 2002b. Improving machine learning\napproaches to coreference resolution. ACL.\nNissim, M., S. Dingare, J. Carletta, and M. Steedman. 2004.\nAn annotation scheme for information status in dialogue.\nLREC .\nPoesio, M., R. Stuckardt, and Y . Versley. 2016. Anaphora\nresolution: Algorithms, resources, and applications .\nSpringer.\nPoesio, M., P. Sturt, R. Artstein, and R. Filik. 2006. Under-\nspeci\ufb01cation and anaphora: Theoretical issues and pre-\nliminary evidence. Discourse processes , 42(2):157\u2013175.\nPoesio, M. and R. Vieira. 1998. A corpus-based investiga-\ntion of de\ufb01nite description use. Computational Linguis-\ntics, 24(2):183\u2013216.\nPonzetto, S. P. and M. Strube. 2006. Exploiting semantic role\nlabeling, WordNet and Wikipedia for coreference resolu-\ntion. HLT-NAACL .\nPonzetto, S. P. and M. Strube. 2007. Knowledge de-\nrived from Wikipedia for computing semantic related-\nness. JAIR , 30:181\u2013212.\nPradhan, S., E. H. Hovy, M. P. Marcus, M. Palmer,\nL. Ramshaw, and R. Weischedel. 2007a. OntoNotes: A\nuni\ufb01ed relational semantic representation. Proceedings of\nICSC .\nPradhan, S., X. Luo, M. Recasens, E. H. Hovy, V . Ng, and\nM. Strube. 2014. Scoring coreference partitions of pre-\ndicted mentions: A reference implementation. ACL.\nPradhan, S., A. Moschitti, N. Xue, O. Uryupina, and\nY . Zhang. 2012a. CoNLL-2012 shared task: Model-\ning multilingual unrestricted coreference in OntoNotes.\nCoNLL .\nPradhan, S., A. Moschitti, N. Xue, O. Uryupina, and",
    "metadata": {
      "source": "23",
      "chunk_id": 68,
      "token_count": 755,
      "chapter_title": ""
    }
  },
  {
    "content": "labeling, WordNet and Wikipedia for coreference resolu-\ntion. HLT-NAACL .\nPonzetto, S. P. and M. Strube. 2007. Knowledge de-\nrived from Wikipedia for computing semantic related-\nness. JAIR , 30:181\u2013212.\nPradhan, S., E. H. Hovy, M. P. Marcus, M. Palmer,\nL. Ramshaw, and R. Weischedel. 2007a. OntoNotes: A\nuni\ufb01ed relational semantic representation. Proceedings of\nICSC .\nPradhan, S., X. Luo, M. Recasens, E. H. Hovy, V . Ng, and\nM. Strube. 2014. Scoring coreference partitions of pre-\ndicted mentions: A reference implementation. ACL.\nPradhan, S., A. Moschitti, N. Xue, O. Uryupina, and\nY . Zhang. 2012a. CoNLL-2012 shared task: Model-\ning multilingual unrestricted coreference in OntoNotes.\nCoNLL .\nPradhan, S., A. Moschitti, N. Xue, O. Uryupina, and\nY . Zhang. 2012b. Conll-2012 shared task: Modeling mul-\ntilingual unrestricted coreference in OntoNotes. CoNLL .\nPradhan, S., L. Ramshaw, M. P. Marcus, M. Palmer,\nR. Weischedel, and N. Xue. 2011. CoNLL-2011 shared\ntask: Modeling unrestricted coreference in OntoNotes.\nCoNLL .Pradhan, S., L. Ramshaw, R. Weischedel, J. MacBride, and\nL. Micciulla. 2007b. Unrestricted coreference: Identi-\nfying entities and events in OntoNotes. Proceedings of\nICSC 2007 .\nPrince, E. 1981. Toward a taxonomy of given-new infor-\nmation. In P. Cole, ed., Radical Pragmatics , 223\u2013255.\nAcademic Press.\nPustejovsky, J. 1991. The generative lexicon. Computational\nLinguistics , 17(4).\nRaghunathan, K., H. Lee, S. Rangarajan, N. Chambers,\nM. Surdeanu, D. Jurafsky, and C. D. Manning. 2010. A\nmulti-pass sieve for coreference resolution. EMNLP .\nRahman, A. and V . Ng. 2009. Supervised models for coref-\nerence resolution. EMNLP .\nRahman, A. and V . Ng. 2012. Resolving complex cases\nof de\ufb01nite pronouns: the Winograd Schema challenge.\nEMNLP .\nRatinov, L. and D. Roth. 2012. Learning-based multi-sieve\nco-reference resolution with knowledge. EMNLP .\nRecasens, M. and E. H. Hovy. 2011. BLANC: Implement-\ning the Rand index for coreference evaluation. Natural\nLanguage Engineering , 17(4):485\u2013510.\nRecasens, M., E. H. Hovy, and M. A. Mart \u00b4\u0131. 2011. Identity,\nnon-identity, and near-identity: Addressing the complex-\nity of coreference. Lingua , 121(6):1138\u20131152.",
    "metadata": {
      "source": "23",
      "chunk_id": 69,
      "token_count": 765,
      "chapter_title": ""
    }
  },
  {
    "content": "Linguistics , 17(4).\nRaghunathan, K., H. Lee, S. Rangarajan, N. Chambers,\nM. Surdeanu, D. Jurafsky, and C. D. Manning. 2010. A\nmulti-pass sieve for coreference resolution. EMNLP .\nRahman, A. and V . Ng. 2009. Supervised models for coref-\nerence resolution. EMNLP .\nRahman, A. and V . Ng. 2012. Resolving complex cases\nof de\ufb01nite pronouns: the Winograd Schema challenge.\nEMNLP .\nRatinov, L. and D. Roth. 2012. Learning-based multi-sieve\nco-reference resolution with knowledge. EMNLP .\nRecasens, M. and E. H. Hovy. 2011. BLANC: Implement-\ning the Rand index for coreference evaluation. Natural\nLanguage Engineering , 17(4):485\u2013510.\nRecasens, M., E. H. Hovy, and M. A. Mart \u00b4\u0131. 2011. Identity,\nnon-identity, and near-identity: Addressing the complex-\nity of coreference. Lingua , 121(6):1138\u20131152.\nRecasens, M. and M. A. Mart \u00b4\u0131. 2010. AnCora-CO: Corefer-\nentially annotated corpora for Spanish and Catalan. Lan-\nguage Resources and Evaluation , 44(4):315\u2013345.\nReichman, R. 1985. Getting Computers to Talk Like You and\nMe. MIT Press.\nRudinger, R., J. Naradowsky, B. Leonard, and\nB. Van Durme. 2018. Gender bias in coreference res-\nolution. NAACL HLT .\nSchiebinger, L. 2013. Machine translation: Analyzing gen-\nder.http://genderedinnovations.stanford.edu/\ncase-studies/nlp.html#tabs-2 .\nSoon, W. M., H. T. Ng, and D. C. Y . Lim. 2001. A ma-\nchine learning approach to coreference resolution of noun\nphrases. Computational Linguistics , 27(4):521\u2013544.\nSorokin, D. and I. Gurevych. 2018. Mixing context granu-\nlarities for improved entity linking on question answering\ndata across entity categories. *SEM .\nStrube, M. and U. Hahn. 1996. Functional centering. ACL.\nSu, Y ., H. Sun, B. Sadler, M. Srivatsa, I. G \u00a8ur, Z. Yan, and\nX. Yan. 2016. On generating characteristic-rich question\nsets for QA evaluation. EMNLP .\nTetreault, J. R. 2001. A corpus-based evaluation of center-\ning and pronoun resolution. Computational Linguistics ,\n27(4):507\u2013520.\nTrichelair, P., A. Emami, J. C. K. Cheung, A. Trischler,\nK. Suleman, and F. Diaz. 2018. On the evaluation of\ncommon-sense reasoning in natural language understand-\ning. NeurIPS 2018 Workshop on Critiquing and Correct-\ning Trends in Machine Learning .\nUryupina, O., R. Artstein, A. Bristot, F. Cavicchio, F. Del-",
    "metadata": {
      "source": "23",
      "chunk_id": 70,
      "token_count": 752,
      "chapter_title": ""
    }
  },
  {
    "content": "chine learning approach to coreference resolution of noun\nphrases. Computational Linguistics , 27(4):521\u2013544.\nSorokin, D. and I. Gurevych. 2018. Mixing context granu-\nlarities for improved entity linking on question answering\ndata across entity categories. *SEM .\nStrube, M. and U. Hahn. 1996. Functional centering. ACL.\nSu, Y ., H. Sun, B. Sadler, M. Srivatsa, I. G \u00a8ur, Z. Yan, and\nX. Yan. 2016. On generating characteristic-rich question\nsets for QA evaluation. EMNLP .\nTetreault, J. R. 2001. A corpus-based evaluation of center-\ning and pronoun resolution. Computational Linguistics ,\n27(4):507\u2013520.\nTrichelair, P., A. Emami, J. C. K. Cheung, A. Trischler,\nK. Suleman, and F. Diaz. 2018. On the evaluation of\ncommon-sense reasoning in natural language understand-\ning. NeurIPS 2018 Workshop on Critiquing and Correct-\ning Trends in Machine Learning .\nUryupina, O., R. Artstein, A. Bristot, F. Cavicchio, F. Del-\nogu, K. J. Rodriguez, and M. Poesio. 2020. Annotating\na broad range of anaphoric phenomena, in a variety of\ngenres: The ARRAU corpus. Natural Language Engi-\nneering , 26(1):1\u201334.",
    "metadata": {
      "source": "23",
      "chunk_id": 71,
      "token_count": 350,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 34",
    "metadata": {
      "source": "23",
      "chunk_id": 72,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "34 Chapter 23 \u2022 Coreference Resolution and Entity Linking\nvan Deemter, K. and R. Kibble. 2000. On coreferring: coref-\nerence in MUC and related annotation schemes. Compu-\ntational Linguistics , 26(4):629\u2013637.\nVersley, Y . 2008. Vagueness and referential ambiguity in a\nlarge-scale annotated corpus. Research on Language and\nComputation , 6(3-4):333\u2013353.\nVieira, R. and M. Poesio. 2000. An empirically based sys-\ntem for processing de\ufb01nite descriptions. Computational\nLinguistics , 26(4):539\u2013593.\nVilain, M., J. D. Burger, J. Aberdeen, D. Connolly, and\nL. Hirschman. 1995. A model-theoretic coreference scor-\ning scheme. MUC-6 .\nWalker, M. A., M. Iida, and S. Cote. 1994. Japanese dis-\ncourse and the process of centering. Computational Lin-\nguistics , 20(2):193\u2013232.\nWang, A., A. Singh, J. Michael, F. Hill, O. Levy, and S. R.\nBowman. 2018. Glue: A multi-task benchmark and anal-\nysis platform for natural language understanding. ICLR .\nWebber, B. L. 1978. A Formal Approach to Discourse\nAnaphora . Ph.D. thesis, Harvard University.\nWebber, B. L. 1983. So what can we talk about now? In\nM. Brady and R. C. Berwick, eds, Computational Models\nof Discourse , 331\u2013371. The MIT Press.\nWebber, B. L. 1991. Structure and ostension in the inter-\npretation of discourse deixis. Language and Cognitive\nProcesses , 6(2):107\u2013135.\nWebber, B. L. and B. Baldwin. 1992. Accommodating con-\ntext change. ACL.\nWebber, B. L. 1988. Discourse deixis: Reference to dis-\ncourse segments. ACL.\nWebster, K., M. Recasens, V . Axelrod, and J. Baldridge.\n2018. Mind the GAP: A balanced corpus of gendered\nambiguous pronouns. TACL , 6:605\u2013617.\nWinograd, T. 1972. Understanding Natural Language . Aca-\ndemic Press.\nWiseman, S., A. M. Rush, and S. M. Shieber. 2016. Learning\nglobal features for coreference resolution. NAACL HLT .\nWiseman, S., A. M. Rush, S. M. Shieber, and J. Weston.\n2015. Learning anaphoricity and antecedent ranking fea-\ntures for coreference resolution. ACL.\nWoods, W. A., R. M. Kaplan, and B. L. Nash-Webber. 1972.\nThe lunar sciences natural language information system:\nFinal report. Technical Report 2378, BBN.\nWu, L., F. Petroni, M. Josifoski, S. Riedel, and L. Zettle-\nmoyer. 2020. Scalable zero-shot entity linking with dense\nentity retrieval. EMNLP .\nYang, X., G. Zhou, J. Su, and C. L. Tan. 2003. Coreference",
    "metadata": {
      "source": "23",
      "chunk_id": 73,
      "token_count": 757,
      "chapter_title": ""
    }
  },
  {
    "content": "course segments. ACL.\nWebster, K., M. Recasens, V . Axelrod, and J. Baldridge.\n2018. Mind the GAP: A balanced corpus of gendered\nambiguous pronouns. TACL , 6:605\u2013617.\nWinograd, T. 1972. Understanding Natural Language . Aca-\ndemic Press.\nWiseman, S., A. M. Rush, and S. M. Shieber. 2016. Learning\nglobal features for coreference resolution. NAACL HLT .\nWiseman, S., A. M. Rush, S. M. Shieber, and J. Weston.\n2015. Learning anaphoricity and antecedent ranking fea-\ntures for coreference resolution. ACL.\nWoods, W. A., R. M. Kaplan, and B. L. Nash-Webber. 1972.\nThe lunar sciences natural language information system:\nFinal report. Technical Report 2378, BBN.\nWu, L., F. Petroni, M. Josifoski, S. Riedel, and L. Zettle-\nmoyer. 2020. Scalable zero-shot entity linking with dense\nentity retrieval. EMNLP .\nYang, X., G. Zhou, J. Su, and C. L. Tan. 2003. Coreference\nresolution using competition learning approach. ACL.\nYih, W.-t., M. Richardson, C. Meek, M.-W. Chang, and\nJ. Suh. 2016. The value of semantic parse labeling for\nknowledge base question answering. ACL.\nZhang, R., C. N. dos Santos, M. Yasunaga, B. Xiang, and\nD. Radev. 2018. Neural coreference resolution with deep\nbiaf\ufb01ne attention by joint mention detection and mention\nclustering. ACL.\nZhao, J., T. Wang, M. Yatskar, R. Cotterell, V . Ordonez, and\nK.-W. Chang. 2019. Gender bias in contextualized word\nembeddings. NAACL HLT .Zhao, J., T. Wang, M. Yatskar, V . Ordonez, and K.-W.\nChang. 2018. Gender bias in coreference resolution:\nEvaluation and debiasing methods. NAACL HLT .\nZheng, J., L. Vilnis, S. Singh, J. D. Choi, and A. McCallum.\n2013. Dynamic knowledge-base alignment for corefer-\nence resolution. CoNLL .\nZhou, L., M. Ticrea, and E. H. Hovy. 2004. Multi-document\nbiography summarization. EMNLP .",
    "metadata": {
      "source": "23",
      "chunk_id": 74,
      "token_count": 586,
      "chapter_title": ""
    }
  }
]