[
  {
    "content": "# D\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\nDConstituency Grammars\nBecause the Night by Bruce Springsteen and Patty Smith\nThe Fire Next Time by James Baldwin\nIf on a winter\u2019s night a traveler by Italo Calvino\nLove Actually by Richard Curtis\nSuddenly Last Summer by Tennessee Williams\nA Scanner Darkly by Philip K. Dick\nSix titles that are not constituents, from Geoffrey K. Pullum on\nLanguage Log (who was pointing out their incredible rarity).\nThe study of grammar has an ancient pedigree. The grammar of Sanskrit was\ndescribed by the Indian grammarian P \u00afan.ini sometime between the 7th and 4th cen-\nturies BCE, in his famous treatise the As .t.\u00afadhy \u00afay\u00af\u0131 (\u20188 books\u2019). And our word syntax syntax\ncomes from the Greek s\u00b4yntaxis , meaning \u201csetting out together or arrangement\u201d, and\nrefers to the way words are arranged together. We have seen various syntactic no-\ntions in previous chapters: ordering of sequences of words (Chapter 2), probabilities\nfor these word sequences (Chapter 3), and the use of part-of-speech categories as\na grammatical equivalence class for words (Chapter 17). In this chapter and the\nnext three we introduce a variety of syntactic phenomena that go well beyond these\nsimpler approaches, together with formal models for capturing them in a computa-\ntionally useful manner.\nThe bulk of this chapter is devoted to context-free grammars. Context-free gram-\nmars are the backbone of many formal models of the syntax of natural language (and,\nfor that matter, of computer languages). As such, they play a role in many computa-\ntional applications, including grammar checking, semantic interpretation, dialogue\nunderstanding, and machine translation. They are powerful enough to express so-\nphisticated relations among the words in a sentence, yet computationally tractable\nenough that ef\ufb01cient algorithms exist for parsing sentences with them (as we show in\nChapter 18). Here we also introduce the concept of lexicalized grammars, focusing\non one example, combinatory categorial grammar , orCCG .\nIn Chapter 20 we introduce a formal model of grammar called syntactic depen-\ndencies that is an alternative to these constituency grammars, and we\u2019ll give algo-\nrithms for dependency parsing . Both constituency and dependency formalisms are\nimportant for language processing.\nFinally, we provide a brief overview of the grammar of English, illustrated from\na domain with relatively simple sentences called ATIS (Air Traf\ufb01c Information Sys-\ntem) (Hemphill et al., 1990). ATIS systems were an early spoken language system\nfor users to book \ufb02ights, by expressing sentences like I\u2019d like to \ufb02y to Atlanta .",
    "metadata": {
      "source": "D",
      "chunk_id": 0,
      "token_count": 631,
      "chapter_title": "D"
    }
  },
  {
    "content": "## Page 2\n\n2APPENDIX D \u2022 C ONSTITUENCY GRAMMARS\nD.1 Constituency\nSyntactic constituency is the idea that groups of words can behave as single units,\nor constituents. Part of developing a grammar involves building an inventory of the\nconstituents in the language. How do words group together in English? Consider\nthenoun phrase , a sequence of words surrounding at least one noun. Here are some noun phrase\nexamples of noun phrases (thanks to Damon Runyon):\nHarry the Horse a high-class spot such as Mindy\u2019s\nthe Broadway coppers the reason he comes into the Hot Box\nthey three parties from Brooklyn\nWhat evidence do we have that these words group together (or \u201cform constituents\u201d)?\nOne piece of evidence is that they can all appear in similar syntactic environments,\nfor example, before a verb.\nthree parties from Brooklyn arrive . . .\na high-class spot such as Mindy\u2019s attracts . . .\nthe Broadway coppers love. . .\nthey sit\nBut while the whole noun phrase can occur before a verb, this is not true of each\nof the individual words that make up a noun phrase. The following are not grammat-\nical sentences of English (recall that we use an asterisk (*) to mark fragments that\nare not grammatical English sentences):\n*from arrive . . .*asattracts . . .\n*the is. . . *spot sat. . .\nThus, to correctly describe facts about the ordering of these words in English, we\nmust be able to say things like \u201c Noun Phrases can occur before verbs \u201d.\nOther kinds of evidence for constituency come from what are called preposed or preposed\npostposed constructions. For example, the prepositional phrase on September sev- postposed\nenteenth can be placed in a number of different locations in the following examples,\nincluding at the beginning (preposed) or at the end (postposed):\nOn September seventeenth , I\u2019d like to \ufb02y from Atlanta to Denver\nI\u2019d like to \ufb02y on September seventeenth from Atlanta to Denver\nI\u2019d like to \ufb02y from Atlanta to Denver on September seventeenth\nBut again, while the entire phrase can be placed differently, the individual words\nmaking up the phrase cannot be:\n*On September , I\u2019d like to \ufb02y seventeenth from Atlanta to Denver\n*On I\u2019d like to \ufb02y September seventeenth from Atlanta to Denver\n*I\u2019d like to \ufb02y on September from Atlanta to Denver seventeenth\nD.2 Context-Free Grammars\nThe most widely used formal system for modeling constituent structure in English\nand other natural languages is the Context-Free Grammar , or CFG . Context- CFG",
    "metadata": {
      "source": "D",
      "chunk_id": 1,
      "token_count": 568,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\nD.2 \u2022 C ONTEXT -FREE GRAMMARS 3\nfree grammars are also called Phrase-Structure Grammars , and the formalism\nis equivalent to Backus-Naur Form , orBNF . The idea of basing a grammar on\nconstituent structure dates back to the psychologist Wilhelm Wundt 1900 but was\nnot formalized until Chomsky (1956) and, independently, Backus (1959).\nA context-free grammar consists of a set of rules orproductions , each of which rules\nexpresses the ways that symbols of the language can be grouped and ordered to-\ngether, and a lexicon of words and symbols. For example, the following productions lexicon\nexpress that an NP(ornoun phrase ) can be composed of either a ProperNoun or NP\na determiner ( Det) followed by a Nominal ; aNominal in turn can consist of one or\nmore Noun s.1\nNP!Det Nominal\nNP!ProperNoun\nNominal!NounjNominal Noun\nContext-free rules can be hierarchically embedded, so we can combine the previous\nrules with others, like the following, that express facts about the lexicon:\nDet!a\nDet!the\nNoun!\ufb02ight\nThe symbols that are used in a CFG are divided into two classes. The symbols\nthat correspond to words in the language (\u201cthe\u201d, \u201cnightclub\u201d) are called terminal terminal\nsymbols; the lexicon is the set of rules that introduce these terminal symbols. The\nsymbols that express abstractions over these terminals are called non-terminals . In non-terminal\neach context-free rule, the item to the right of the arrow ( !) is an ordered list of one\nor more terminals and non-terminals; to the left of the arrow is a single non-terminal\nsymbol expressing some cluster or generalization. The non-terminal associated with\neach word in the lexicon is its lexical category, or part of speech.\nA CFG can be thought of in two ways: as a device for generating sentences\nand as a device for assigning a structure to a given sentence. Viewing a CFG as a\ngenerator, we can read the !arrow as \u201crewrite the symbol on the left with the string\nof symbols on the right\u201d.\nSo starting from the symbol: NP\nwe can use our \ufb01rst rule to rewrite NPas: Det Nominal\nand then rewrite Nominal as: Noun\nand \ufb01nally rewrite these parts-of-speech as: a \ufb02ight\nWe say the string a \ufb02ight can be derived from the non-terminal NP. Thus, a CFG\ncan be used to generate a set of strings. This sequence of rule expansions is called a\nderivation of the string of words. It is common to represent a derivation by a parse derivation\ntree (commonly shown inverted with the root at the top). Figure D.1 shows the tree parse tree\nrepresentation of this derivation.\nIn the parse tree shown in Fig. D.1, we can say that the node NPdominates dominates\nall the nodes in the tree ( Det,Nom ,Noun ,a,\ufb02ight ). We can say further that it\nimmediately dominates the nodes DetandNom .\nThe formal language de\ufb01ned by a CFG is the set of strings that are derivable\nfrom the designated start symbol . Each grammar must have one designated start start symbol\n1When talking about these rules we can pronounce the rightarrow !as \u201cgoes to\u201d, and so we might\nread the \ufb01rst rule above as \u201cNP goes to Det Nominal\u201d.",
    "metadata": {
      "source": "D",
      "chunk_id": 2,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4APPENDIX D \u2022 C ONSTITUENCY GRAMMARS\nNP\nNom\nNoun\n\ufb02ightDet\na\nFigure D.1 A parse tree for \u201ca \ufb02ight\u201d.\nsymbol, which is often called S. Since context-free grammars are often used to de\ufb01ne\nsentences, Sis usually interpreted as the \u201csentence\u201d node, and the set of strings that\nare derivable from Sis the set of sentences in some simpli\ufb01ed version of English.\nLet\u2019s add a few additional rules to our inventory. The following rule expresses\nthe fact that a sentence can consist of a noun phrase followed by a verb phrase : verb phrase\nS!NP VP I prefer a morning \ufb02ight\nA verb phrase in English consists of a verb followed by assorted other things;\nfor example, one kind of verb phrase consists of a verb followed by a noun phrase:\nVP!Verb NP prefer a morning \ufb02ight\nOr the verb may be followed by a noun phrase and a prepositional phrase:\nVP!Verb NP PP leave Boston in the morning\nOr the verb phrase may have a verb followed by a prepositional phrase alone:\nVP!Verb PP leaving on Thursday\nA prepositional phrase generally has a preposition followed by a noun phrase.\nFor example, a common type of prepositional phrase in the ATIS corpus is used to\nindicate location or direction:\nPP!Preposition NP from Los Angeles\nTheNPinside a PPneed not be a location; PPs are often used with times and\ndates, and with other nouns as well; they can be arbitrarily complex. Here are ten\nexamples from the ATIS corpus:\nto Seattle on these \ufb02ights\nin Minneapolis about the ground transportation in Chicago\non Wednesday of the round trip \ufb02ight on United Airlines\nin the evening of the AP \ufb01fty seven \ufb02ight\non the ninth of July with a stopover in Nashville\nFigure D.2 gives a sample lexicon, and Fig. D.3 summarizes the grammar rules\nwe\u2019ve seen so far, which we\u2019ll call L0. Note that we can use the or-symbol jto\nindicate that a non-terminal has alternate possible expansions.\nWe can use this grammar to generate sentences of this \u201cATIS-language\u201d. We\nstart with S, expand it to NP VP , then choose a random expansion of NP(let\u2019s say, to\nI), and a random expansion of VP(let\u2019s say, to Verb NP ), and so on until we generate\nthe string I prefer a morning \ufb02ight . Figure D.4 shows a parse tree that represents a\ncomplete derivation of I prefer a morning \ufb02ight .\nWe can also represent a parse tree in a more compact format called bracketed\nnotation ; here is the bracketed representation of the parse tree of Fig. D.4:bracketed\nnotation",
    "metadata": {
      "source": "D",
      "chunk_id": 3,
      "token_count": 613,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\nD.2 \u2022 C ONTEXT -FREE GRAMMARS 5\nNoun!\ufb02ightsj\ufb02ightjbreezejtripjmorning\nVerb!isjpreferjlikejneedjwantj\ufb02yjdo\nAdjective!cheapestjnon-stopj\ufb01rstjlatest\njotherjdirect\nPronoun!mejIjyoujit\nProper-Noun!AlaskajBaltimorejLos Angeles\njChicagojUnitedjAmerican\nDeterminer!thejajanjthisjthesejthat\nPreposition!fromjtojonjnearjin\nConjunction!andjorjbut\nFigure D.2 The lexicon for L0.\nGrammar Rules Examples\nS!NP VP I + want a morning \ufb02ight\nNP!Pronoun I\njProper-Noun Los Angeles\njDet Nominal a + \ufb02ight\nNominal!Nominal Noun morning + \ufb02ight\njNoun \ufb02ights\nVP!Verb do\njVerb NP want + a \ufb02ight\njVerb NP PP leave + Boston + in the morning\njVerb PP leaving + on Thursday\nPP!Preposition NP from + Los Angeles\nFigure D.3 The grammar for L0, with example phrases for each rule.\nS\nVP\nNP\nNom\nNoun\n\ufb02ightNom\nNoun\nmorningDet\naVerb\npreferNP\nPro\nI\nFigure D.4 The parse tree for \u201cI prefer a morning \ufb02ight\u201d according to grammar L0.\n(D.1) [ S[NP[ProI]] [VP[Vprefer] [ NP[Deta] [Nom [Nmorning] [ Nom [N\ufb02ight]]]]]]\nA CFG like that of L0de\ufb01nes a formal language. We saw in Chapter 2 that a for-\nmal language is a set of strings. Sentences (strings of words) that can be derived by a\ngrammar are in the formal language de\ufb01ned by that grammar, and are called gram-\nmatical sentences. Sentences that cannot be derived by a given formal grammar are grammatical\nnot in the language de\ufb01ned by that grammar and are referred to as ungrammatical . ungrammatical",
    "metadata": {
      "source": "D",
      "chunk_id": 4,
      "token_count": 493,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6APPENDIX D \u2022 C ONSTITUENCY GRAMMARS\nThis hard line between \u201cin\u201d and \u201cout\u201d characterizes all formal languages but is only\na very simpli\ufb01ed model of how natural languages really work. This is because de-\ntermining whether a given sentence is part of a given natural language (say, English)\noften depends on the context. In linguistics, the use of formal languages to model\nnatural languages is called generative grammar since the language is de\ufb01ned bygenerative\ngrammar\nthe set of possible sentences \u201cgenerated\u201d by the grammar.\nD.2.1 Formal De\ufb01nition of Context-Free Grammar\nWe conclude this section with a quick, formal description of a context-free gram-\nmar and the language it generates. A context-free grammar Gis de\ufb01ned by four\nparameters: N;S;R;S(technically this is a \u201c4-tuple\u201d).\nNa set of non-terminal symbols (orvariables )\nSa set of terminal symbols (disjoint from N)\nRa set of rules or productions, each of the form A!b,\nwhere Ais a non-terminal,\nbis a string of symbols from the in\ufb01nite set of strings (S[N)\u0003\nSa designated start symbol and a member of N\nFor the remainder of the book we adhere to the following conventions when dis-\ncussing the formal properties of context-free grammars (as opposed to explaining\nparticular facts about English or other languages).\nCapital letters like A,B, and S Non-terminals\nS The start symbol\nLower-case Greek letters like a,b, and g Strings drawn from (S[N)\u0003\nLower-case Roman letters like u,v, and w Strings of terminals\nA language is de\ufb01ned through the concept of derivation. One string derives an-\nother one if it can be rewritten as the second one by some series of rule applications.\nMore formally, following Hopcroft and Ullman (1979),\nifA!bis a production of Randaandgare any strings in the set\n(S[N)\u0003, then we say that aAgdirectly derives abg , oraAg)abg . directly derives\nDerivation is then a generalization of direct derivation:\nLeta1;a2;:::; ambe strings in (S[N)\u0003;m\u00151, such that\na1)a2;a2)a3;:::; am\u00001)am\nWe say that a1derives am, ora1\u0003)am. derives\nWe can then formally de\ufb01ne the language LGgenerated by a grammar Gas the\nset of strings composed of terminal symbols that can be derived from the designated\nstart symbol S.\nLG=fwjwis inS\u0003andS\u0003)wg\nThe problem of mapping from a string of words to its parse tree is called syn-\ntactic parsing ; we de\ufb01ne algorithms for constituency parsing in Chapter 18.syntactic\nparsing",
    "metadata": {
      "source": "D",
      "chunk_id": 5,
      "token_count": 623,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\nD.3 \u2022 S OME GRAMMAR RULES FOR ENGLISH 7\nD.3 Some Grammar Rules for English\nIn this section, we introduce a few more aspects of the phrase structure of English;\nfor consistency we will continue to focus on sentences from the ATIS domain. Be-\ncause of space limitations, our discussion is necessarily limited to highlights. Read-\ners are strongly advised to consult a good reference grammar of English, such as\nHuddleston and Pullum (2002).\nD.3.1 Sentence-Level Constructions\nIn the small grammar L0, we provided only one sentence-level construction for\ndeclarative sentences like I prefer a morning \ufb02ight . Among the large number of\nconstructions for English sentences, four are particularly common and important:\ndeclaratives, imperatives, yes-no questions, and wh-questions.\nSentences with declarative structure have a subject noun phrase followed by declarative\na verb phrase, like \u201cI prefer a morning \ufb02ight\u201d. Sentences with this structure have\na great number of different uses that we follow up on in Chapter 15. Here are a\nnumber of examples from the ATIS domain:\nI want a \ufb02ight from Ontario to Chicago\nThe \ufb02ight should be eleven a.m. tomorrow\nThe return \ufb02ight should leave at around seven p.m.\nSentences with imperative structure often begin with a verb phrase and have imperative\nno subject. They are called imperative because they are almost always used for\ncommands and suggestions; in the ATIS domain they are commands to the system.\nShow the lowest fare\nGive me Sunday\u2019s \ufb02ights arriving in Las Vegas from New York City\nList all \ufb02ights between \ufb01ve and seven p.m.\nWe can model this sentence structure with another rule for the expansion of S:\nS!VP\nSentences with yes-no question structure are often (though not always) used to yes-no question\nask questions; they begin with an auxiliary verb, followed by a subject NP, followed\nby a VP. Here are some examples. Note that the third example is not a question at\nall but a request; Chapter 15 discusses the uses of these question forms to perform\ndifferent pragmatic functions such as asking, requesting, or suggesting.\nDo any of these \ufb02ights have stops?\nDoes American\u2019s \ufb02ight eighteen twenty \ufb01ve serve dinner?\nCan you give me the same information for United?\nHere\u2019s the rule:\nS!Aux NP VP\nThe most complex sentence-level structures we examine here are the various wh-\nstructures. These are so named because one of their constituents is a wh-phrase , that wh-phrase\nis, one that includes a wh-word (who, whose, when, where, what, which, how, why ). wh-word\nThese may be broadly grouped into two classes of sentence-level structures. The\nwh-subject-question structure is identical to the declarative structure, except that\nthe \ufb01rst noun phrase contains some wh-word.",
    "metadata": {
      "source": "D",
      "chunk_id": 6,
      "token_count": 629,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8APPENDIX D \u2022 C ONSTITUENCY GRAMMARS\nWhat airlines \ufb02y from Burbank to Denver?\nWhich \ufb02ights depart Burbank after noon and arrive in Denver by six p.m?\nWhose \ufb02ights serve breakfast?\nHere is a rule. Exercise D.7 discusses rules for the constituents that make up the\nWh-NP .\nS!Wh-NP VP\nIn the wh-non-subject-question structure, the wh-phrase is not the subject of thewh-non-subject-\nquestion\nsentence, and so the sentence includes another subject. In these types of sentences\nthe auxiliary appears before the subject NP, just as in the yes-no question structures.\nHere is an example followed by a sample rule:\nWhat \ufb02ights do you have from Burbank to Tacoma Washington?\nS!Wh-NP Aux NP VP\nConstructions like the wh-non-subject-question contain what are called long-\ndistance dependencies because the Wh-NP what \ufb02ights is far away from the predi-long-distance\ndependencies\ncate that it is semantically related to, the main verb have in the VP. In some models\nof parsing and understanding compatible with the grammar rule above, long-distance\ndependencies like the relation between \ufb02ights andhave are thought of as a semantic\nrelation. In such models, the job of \ufb01guring out that \ufb02ights is the argument of have is\ndone during semantic interpretation. Other models of parsing represent the relation-\nship between \ufb02ights andhave as a syntactic relation, and the grammar is modi\ufb01ed to\ninsert a small marker called a trace orempty category after the verb. We discuss\nempty-category models when we introduce the Penn Treebank on page 15.\nD.3.2 Clauses and Sentences\nBefore we move on, we should clarify the status of the Srules in the grammars we\njust described. Srules are intended to account for entire sentences that stand alone\nas fundamental units of discourse. However, Scan also occur on the right-hand side\nof grammar rules and hence can be embedded within larger sentences. Clearly then,\nthere\u2019s more to being an Sthan just standing alone as a unit of discourse.\nWhat differentiates sentence constructions (i.e., the Srules) from the rest of the\ngrammar is the notion that they are in some sense complete . In this way they corre-\nspond to the notion of a clause , which traditional grammars often describe as form- clause\ning a complete thought. One way of making this notion of \u201ccomplete thought\u201d more\nprecise is to say an Sis a node of the parse tree below which the main verb of the S\nhas all of its arguments . We de\ufb01ne verbal arguments later, but for now let\u2019s just see\nan illustration from the tree for I prefer a morning \ufb02ight in Fig. D.4 on page 5. The\nverb prefer has two arguments: the subject Iand the object a morning \ufb02ight . One of\nthe arguments appears below the VPnode, but the other one, the subject NP, appears\nonly below the Snode.\nD.3.3 The Noun Phrase\nOurL0grammar introduced three of the most frequent types of noun phrases that\noccur in English: pronouns, proper nouns and the NP!Det Nominal construction.\nThe central focus of this section is on the last type since that is where the bulk of\nthe syntactic complexity resides. These noun phrases consist of a head, the central\nnoun in the noun phrase, along with various modi\ufb01ers that can occur before or after\nthe head noun. Let\u2019s take a close look at the various parts.",
    "metadata": {
      "source": "D",
      "chunk_id": 7,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\nD.3 \u2022 S OME GRAMMAR RULES FOR ENGLISH 9\nThe Determiner\nNoun phrases can begin with simple lexical determiners:\na stop the \ufb02ights this \ufb02ight\nthose \ufb02ights any \ufb02ights some \ufb02ights\nThe role of the determiner can also be \ufb01lled by more complex expressions:\nUnited\u2019s \ufb02ight\nUnited\u2019s pilot\u2019s union\nDenver\u2019s mayor\u2019s mother\u2019s canceled \ufb02ight\nIn these examples, the role of the determiner is \ufb01lled by a possessive expression\nconsisting of a noun phrase followed by an \u2019sas a possessive marker, as in the\nfollowing rule.\nDet!NP0s\nThe fact that this rule is recursive (since an NPcan start with a Det) helps us model\nthe last two examples above, in which a sequence of possessive expressions serves\nas a determiner.\nUnder some circumstances determiners are optional in English. For example,\ndeterminers may be omitted if the noun they modify is plural:\n(D.2) Show me \ufb02ights from San Francisco to Denver on weekdays\nAs we saw in Chapter 17, mass nouns also don\u2019t require determination. Recall that\nmass nouns often (not always) involve something that is treated like a substance\n(including e.g., water andsnow ), don\u2019t take the inde\ufb01nite article \u201c a\u201d, and don\u2019t tend\nto pluralize. Many abstract nouns are mass nouns ( music ,homework ). Mass nouns\nin the ATIS domain include breakfast ,lunch , and dinner :\n(D.3) Does this \ufb02ight serve dinner?\nThe Nominal\nThe nominal construction follows the determiner and contains any pre- and post-\nhead noun modi\ufb01ers. As indicated in grammar L0, in its simplest form a nominal\ncan consist of a single noun.\nNominal!Noun\nAs we\u2019ll see, this rule also provides the basis for the bottom of various recursive\nrules used to capture more complex nominal constructions.\nBefore the Head Noun\nA number of different kinds of word classes can appear before the head noun but\nafter the determiner (the \u201cpostdeterminers\u201d) in a nominal. These include cardinalcardinal\nnumbers\nnumbers ,ordinal numbers ,quanti\ufb01ers , and adjectives . Examples of cardinalordinal\nnumbers\nquanti\ufb01ers numbers:\ntwo friends one stop\nOrdinal numbers include \ufb01rst,second ,third , and so on, but also words like next,\nlast,past,other , and another :\nthe \ufb01rst one the next day the second leg\nthe last \ufb02ight the other American \ufb02ight\nSome quanti\ufb01ers ( many ,(a) few ,several ) occur only with plural count nouns:",
    "metadata": {
      "source": "D",
      "chunk_id": 8,
      "token_count": 595,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 APPENDIX D \u2022 C ONSTITUENCY GRAMMARS\nmany fares\nAdjectives occur after quanti\ufb01ers but before nouns.\na\ufb01rst-class fare a non-stop \ufb02ight\nthelongest layover the earliest lunch \ufb02ight\nAdjectives can also be grouped into a phrase called an adjective phrase or AP.adjective\nphrase\nAPs can have an adverb before the adjective (see Chapter 17 for de\ufb01nitions of ad-\njectives and adverbs):\ntheleast expensive fare\nAfter the Head Noun\nA head noun can be followed by postmodi\ufb01ers . Three kinds of nominal postmodi-\n\ufb01ers are common in English:\nprepositional phrases all \ufb02ights from Cleveland\nnon-\ufb01nite clauses any \ufb02ights arriving after eleven a.m.\nrelative clauses a \ufb02ight that serves breakfast\nThey are especially common in the ATIS corpus since they are used to mark the\norigin and destination of \ufb02ights.\nHere are some examples of prepositional phrase postmodi\ufb01ers, with brackets\ninserted to show the boundaries of each PP; note that two or more PPs can be strung\ntogether within a single NP:\nall \ufb02ights [from Cleveland] [to Newark]\narrival [in San Jose] [before seven p.m.]\na reservation [on \ufb02ight six oh six] [from Tampa] [to Montreal]\nHere\u2019s a new nominal rule to account for postnominal PPs:\nNominal!Nominal PP\nThe three most common kinds of non-\ufb01nite postmodi\ufb01ers are the gerundive ( - non-\ufb01nite\ning),-ed, and in\ufb01nitive forms.\nGerundive postmodi\ufb01ers are so called because they consist of a verb phrase that gerundive\nbegins with the gerundive ( -ing) form of the verb. Here are some examples:\nany of those [leaving on Thursday]\nany \ufb02ights [arriving after eleven a.m.]\n\ufb02ights [arriving within thirty minutes of each other]\nWe can de\ufb01ne the Nominals with gerundive modi\ufb01ers as follows, making use of\na new non-terminal GerundVP :\nNominal!Nominal GerundVP\nWe can make rules for GerundVP constituents by duplicating all of our VP pro-\nductions, substituting GerundV forV.\nGerundVP!GerundV NP\njGerundV PPjGerundVjGerundV NP PP\nGerundV can then be de\ufb01ned as\nGerundV!beingjarrivingjleavingj:::\nThe phrases in italics below are examples of the two other common kinds of\nnon-\ufb01nite clauses, in\ufb01nitives and -edforms:",
    "metadata": {
      "source": "D",
      "chunk_id": 9,
      "token_count": 617,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\nD.3 \u2022 S OME GRAMMAR RULES FOR ENGLISH 11\nthe last \ufb02ight to arrive in Boston\nI need to have dinner served\nWhich is the aircraft used by this \ufb02ight ?\nA postnominal relative clause (more correctly a restrictive relative clause ), is\na clause that often begins with a relative pronoun (thatandwho are the most com-relative\npronoun\nmon). The relative pronoun functions as the subject of the embedded verb in the\nfollowing examples:\na \ufb02ight that serves breakfast\n\ufb02ights that leave in the morning\nthe one that leaves at ten thirty \ufb01ve\nWe might add rules like the following to deal with these:\nNominal!Nominal RelClause\nRelClause!(whojthat)VP\nThe relative pronoun may also function as the object of the embedded verb, as\nin the following example; we leave for the reader the exercise of writing grammar\nrules for more complex relative clauses of this kind.\nthe earliest American Airlines \ufb02ight that I can get\nVarious postnominal modi\ufb01ers can be combined:\na \ufb02ight [from Phoenix to Detroit] [leaving Monday evening]\nevening \ufb02ights [from Nashville to Houston] [that serve dinner]\na friend [living in Denver] [that would like to visit me in DC]\nBefore the Noun Phrase\nWord classes that modify and appear before NPs are called predeterminers . Many predeterminers\nof these have to do with number or amount; a common predeterminer is all:\nall the \ufb02ights all \ufb02ights all non-stop \ufb02ights\nThe example noun phrase given in Fig. D.5 illustrates some of the complexity\nthat arises when these rules are combined.\nD.3.4 The Verb Phrase\nThe verb phrase consists of the verb and a number of other constituents. In the\nsimple rules we have built so far, these other constituents include NPs and PPs and\ncombinations of the two:\nVP!Verb disappear\nVP!Verb NP prefer a morning \ufb02ight\nVP!Verb NP PP leave Boston in the morning\nVP!Verb PP leaving on Thursday\nVerb phrases can be signi\ufb01cantly more complicated than this. Many other kinds\nof constituents, such as an entire embedded sentence, can follow the verb. These are\ncalled sentential complements :sentential\ncomplements\nYou [ VP[Vsaid [ Syou had a two hundred sixty-six dollar fare]]\n[VP[VTell] [ NPme] [ Show to get from the airport to downtown]]\nI [VP[Vthink [ SI would like to take the nine thirty \ufb02ight]]",
    "metadata": {
      "source": "D",
      "chunk_id": 10,
      "token_count": 572,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 APPENDIX D \u2022 C ONSTITUENCY GRAMMARS\nNP\nNP\nNom\nGerundiveVP\nleaving before 10Nom\nPP\nto TampaNom\nPP\nfrom DenverNom\nNoun\n\ufb02ightsNom\nNoun\nmorningDet\nthePreDet\nall\nFigure D.5 A parse tree for \u201call the morning \ufb02ights from Denver to Tampa leaving before 10\u201d.\nHere\u2019s a rule for these:\nVP!Verb S\nSimilarly, another potential constituent of the VPis another VP. This is often the\ncase for verbs like want ,would like ,try,intend ,need :\nI want [ VPto \ufb02y from Milwaukee to Orlando]\nHi, I want [ VPto arrange three \ufb02ights]\nWhile a verb phrase can have many possible kinds of constituents, not every\nverb is compatible with every verb phrase. For example, the verb want can be used\neither with an NPcomplement ( I want a \ufb02ight . . . ) or with an in\ufb01nitive VPcomple-\nment ( I want to \ufb02y to . . . ). By contrast, a verb like \ufb01ndcannot take this sort of VP\ncomplement ( * I found to \ufb02y to Dallas ).\nThis idea that verbs are compatible with different kinds of complements is a very\nold one; traditional grammar distinguishes between transitive verbs like \ufb01nd, which transitive\ntake a direct object NP(I found a \ufb02ight ), and intransitive verbs like disappear , intransitive\nwhich do not ( *I disappeared a \ufb02ight ).\nWhere traditional grammars subcategorize verbs into these two categories (tran- subcategorize\nsitive and intransitive), modern grammars distinguish as many as 100 subcategories.\nWe say that a verb like \ufb01ndsubcategorizes for anNP, and a verb like want sub-subcategorizes\nfor\ncategorizes for either an NPor a non-\ufb01nite VP. We also call these constituents the\ncomplements of the verb (hence our use of the term sentential complement above). complements\nSo we say that want can take a VPcomplement. These possible sets of complements\nare called the subcategorization frame for the verb. Another way of talking aboutsubcategorization\nframe\nthe relation between the verb and these other constituents is to think of the verb as\na logical predicate and the constituents as logical arguments of the predicate. So we\ncan think of such predicate-argument relations as FIND (I,A FLIGHT ) or WANT (I,TO\nFLY). We talk more about this view of verbs and arguments in Appendix F when we\ntalk about predicate calculus representations of verb semantics. Subcategorization\nframes for a set of example verbs are given in Fig. D.6.",
    "metadata": {
      "source": "D",
      "chunk_id": 11,
      "token_count": 614,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\nD.3 \u2022 S OME GRAMMAR RULES FOR ENGLISH 13\nFrame Verb Example\n/ 0 eat, sleep I ate\nNP prefer, \ufb01nd, leave Find [ NPthe \ufb02ight from Pittsburgh to Boston]\nNP NP show, give Show [ NPme] [ NPairlines with \ufb02ights from Pittsburgh]\nPPfromPPto \ufb02y, travel I would like to \ufb02y [ PPfrom Boston] [ PPto Philadelphia]\nNP PP with help, load Can you help [ NPme] [ PPwith a \ufb02ight]\nVPto prefer, want, need I would prefer [ VPto to go by United Airlines]\nS mean Does this mean [ SAA has a hub in Boston]\nFigure D.6 Subcategorization frames for a set of example verbs.\nWe can capture the association between verbs and their complements by making\nseparate subtypes of the class Verb (e.g., Verb-with-NP-complement ,Verb-with-Inf-\nVP-complement ,Verb-with-S-complement , and so on):\nVerb-with-NP-complement !\ufb01ndjleavejrepeatj:::\nVerb-with-S-complement !thinkjbelievejsayj:::\nVerb-with-Inf-VP-complement !wantjtryjneedj:::\nEach VPrule could then be modi\ufb01ed to require the appropriate verb subtype:\nVP!Verb-with-no-complement disappear\nVP!Verb-with-NP-comp NP prefer a morning \ufb02ight\nVP!Verb-with-S-comp S said there were two \ufb02ights\nA problem with this approach is the signi\ufb01cant increase in the number of rules and\nthe associated loss of generality.\nD.3.5 Coordination\nThe major phrase types discussed here can be conjoined with conjunctions likeand, conjunctions\nor, and butto form larger constructions of the same type. For example, a coordinate coordinate\nnoun phrase can consist of two other noun phrases separated by a conjunction:\nPlease repeat [ NP[NPthe \ufb02ights] and[NPthe costs]]\nI need to know [ NP[NPthe aircraft] and[NPthe \ufb02ight number]]\nHere\u2019s a rule that allows these structures:\nNP!NP and NP\nNote that the ability to form coordinate phrases through conjunctions is often\nused as a test for constituency. Consider the following examples, which differ from\nthe ones given above in that they lack the second determiner.\nPlease repeat the [ Nom[Nom\ufb02ights] and[Nomcosts]]\nI need to know the [ Nom[Nomaircraft] and[Nom\ufb02ight number]]\nThe fact that these phrases can be conjoined is evidence for the presence of the\nunderlying Nominal constituent we have been making use of. Here\u2019s a rule for this:\nNominal!Nominal and Nominal\nThe following examples illustrate conjunctions involving VPs and Ss.",
    "metadata": {
      "source": "D",
      "chunk_id": 12,
      "token_count": 620,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n14 APPENDIX D \u2022 C ONSTITUENCY GRAMMARS\nWhat \ufb02ights do you have [ VP[VPleaving Denver] and[VParriving in\nSan Francisco]]\n[S[SI\u2019m interested in a \ufb02ight from Dallas to Washington] and[SI\u2019m\nalso interested in going to Baltimore]]\nThe rules for VPandSconjunctions mirror the NPone given above.\nVP!VP and VP\nS!S and S\nSince all the major phrase types can be conjoined in this fashion, it is also possible\nto represent this conjunction fact more generally; a number of grammar formalisms\nsuch as GPSG (Gazdar et al., 1985) do this using metarules like: metarules\nX!X and X\nThis metarule states that any non-terminal can be conjoined with the same non-\nterminal to yield a constituent of the same type; the variable Xmust be designated\nas a variable that stands for any non-terminal rather than a non-terminal itself.\nD.4 Treebanks\nSuf\ufb01ciently robust grammars consisting of context-free grammar rules can be used\nto assign a parse tree to any sentence. This means that it is possible to build a\ncorpus where every sentence in the collection is paired with a corresponding parse\ntree. Such a syntactically annotated corpus is called a treebank . Treebanks play treebank\nan important role in parsing, as we discuss in Chapter 18, as well as in linguistic\ninvestigations of syntactic phenomena.\nA wide variety of treebanks have been created, generally through the use of\nparsers (of the sort described in the next few chapters) to automatically parse each\nsentence, followed by the use of humans (linguists) to hand-correct the parses. The\nPenn Treebank project (whose POS tagset we introduced in Chapter 17) has pro- Penn Treebank\nduced treebanks from the Brown, Switchboard, ATIS, and Wall Street Journal cor-\npora of English, as well as treebanks in Arabic and Chinese. A number of treebanks\nuse the dependency representation we will introduce in Chapter 20, including many\nthat are part of the Universal Dependencies project (Nivre et al., 2016).\nD.4.1 Example: The Penn Treebank Project\nFigure D.7 shows sentences from the Brown and ATIS portions of the Penn Tree-\nbank.2Note the formatting differences for the part-of-speech tags; such small dif-\nferences are common and must be dealt with in processing treebanks. The Penn\nTreebank part-of-speech tagset was de\ufb01ned in Chapter 17. The use of LISP-style\nparenthesized notation for trees is extremely common and resembles the bracketed\nnotation we saw earlier in (D.1). For those who are not familiar with it we show a\nstandard node-and-line tree representation in Fig. D.8.\nFigure D.9 shows a tree from the Wall Street Journal . This tree shows another\nfeature of the Penn Treebanks: the use of traces (-NONE- nodes) to mark long- traces\n2The Penn Treebank project released treebanks in multiple languages and in various stages; for exam-\nple, there were Treebank I (Marcus et al., 1993), Treebank II (Marcus et al., 1994), and Treebank III\nreleases of English treebanks. We use Treebank III for our examples.",
    "metadata": {
      "source": "D",
      "chunk_id": 13,
      "token_count": 728,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\nD.4 \u2022 T REEBANKS 15\n((S\n(NP-SBJ (DT That)\n(JJ cold) (, ,)\n(JJ empty) (NN sky) )\n(VP (VBD was)\n(ADJP-PRD (JJ full)\n(PP (IN of)\n(NP (NN fire)\n(CC and)\n(NN light) ))))\n(. .) ))((S\n(NP-SBJ The/DT flight/NN )\n(VP should/MD\n(VP arrive/VB\n(PP-TMP at/IN\n(NP eleven/CD a.m/RB ))\n(NP-TMP tomorrow/NN )))))\n(a) (b)\nFigure D.7 Parsed sentences from the LDC Treebank3 version of the (a) Brown and (b)\nATIS corpora.\nS\n.\n.VP\nADJP-PRD\nPP\nNP\nNN\nlightCC\nandNN\n\ufb01reIN\nofJJ\nfullVBD\nwasNP-SBJ\nNN\nskyJJ\nempty,\n,JJ\ncoldDT\nThat\nFigure D.8 The tree corresponding to the Brown corpus sentence in the previous \ufb01gure.\ndistance dependencies or syntactic movement . For example, quotations often fol-syntactic\nmovement\nlow a quotative verb like say. But in this example, the quotation \u201cWe would have\nto wait until we have collected on those assets\u201d precedes the words he said . An\nempty Scontaining only the node -NONE- marks the position after said where the\nquotation sentence often occurs. This empty node is marked (in Treebanks II and\nIII) with the index 2, as is the quotation Sat the beginning of the sentence. Such\nco-indexing may make it easier for some parsers to recover the fact that this fronted\nor topicalized quotation is the complement of the verb said. A similar -NONE- node\nmarks the fact that there is no syntactic subject right before the verb to wait ; instead,\nthe subject is the earlier NP We . Again, they are both co-indexed with the index 1.\nThe Penn Treebank II and Treebank III releases added further information to\nmake it easier to recover the relationships between predicates and arguments. Cer-\ntain phrases were marked with tags indicating the grammatical function of the phrase\n(as surface subject, logical topic, cleft, non-VP predicates) its presence in particular\ntext categories (headlines, titles), and its semantic function (temporal phrases, lo-",
    "metadata": {
      "source": "D",
      "chunk_id": 14,
      "token_count": 529,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\n16 APPENDIX D \u2022 C ONSTITUENCY GRAMMARS\n( (S (`` ``)\n(S-TPC-2\n(NP-SBJ-1 (PRP We) )\n(VP (MD would)\n(VP (VB have)\n(S\n(NP-SBJ (-NONE- *-1) )\n(VP (TO to)\n(VP (VB wait)\n(SBAR-TMP (IN until)\n(S\n(NP-SBJ (PRP we) )\n(VP (VBP have)\n(VP (VBN collected)\n(PP-CLR (IN on)\n(NP (DT those)(NNS assets)))))))))))))\n(, ,) ('' '')\n(NP-SBJ (PRP he) )\n(VP (VBD said)\n(S (-NONE- *T*-2) ))\n(. .) ))\nFigure D.9 A sentence from the Wall Street Journal portion of the LDC Penn Treebank.\nNote the use of the empty -NONE- nodes.\ncations) (Marcus et al. 1994, Bies et al. 1995). Figure D.9 shows examples of the\n-SBJ (surface subject) and -TMP (temporal phrase) tags. Figure D.8 shows in addi-\ntion the -PRD tag, which is used for predicates that are not VPs (the one in Fig. D.8\nis an ADJP). We\u2019ll return to the topic of grammatical function when we consider\ndependency grammars and parsing in Chapter 19.\nD.4.2 Treebanks as Grammars\nThe sentences in a treebank implicitly constitute a grammar of the language repre-\nsented by the corpus being annotated. For example, from the three parsed sentences\nin Fig. D.7 and Fig. D.9, we can extract each of the CFG rules in them. For simplic-\nity, let\u2019s strip off the rule suf\ufb01xes ( -SBJ and so on). The resulting grammar is shown\nin Fig. D.10.\nThe grammar used to parse the Penn Treebank is relatively \ufb02at, resulting in very\nmany and very long rules. For example, among the approximately 4,500 different\nrules for expanding VPs are separate rules for PP sequences of any length and every\npossible arrangement of verb arguments:\nVP!VBD PP\nVP!VBD PP PP\nVP!VBD PP PP PP\nVP!VBD PP PP PP PP\nVP!VB ADVP PP\nVP!VB PP ADVP\nVP!ADVP VB PP\nas well as even longer rules, such as\nVP!VBP PP PP PP PP PP ADVP PP",
    "metadata": {
      "source": "D",
      "chunk_id": 15,
      "token_count": 558,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\nD.4 \u2022 T REEBANKS 17\nGrammar Lexicon\nS!NP VP . PRP!wejhe\nS!NP VP DT!thejthatjthose\nS!\u201cS\u201d, NP VP . JJ!coldjemptyjfull\nS!-NONE- NN!skyj\ufb01rejlightj\ufb02ightjtomorrow\nNP!DT NN NNS!assets\nNP!DT NNS CC!and\nNP!NN CC NN IN!ofjatjuntiljon\nNP!CD RB CD!eleven\nNP!DT JJ , JJ NN RB!a.m.\nNP!PRP VB!arrivejhavejwait\nNP!-NONE- VBD!wasjsaid\nVP!MD VP VBP!have\nVP!VBD ADJP VBN!collected\nVP!VBD S MD!shouldjwould\nVP!VBN PP TO!to\nVP!VB S\nVP!VB SBAR\nVP!VBP VP\nVP!VBN PP\nVP!TO VP\nSBAR!IN S\nADJP!JJ PP\nPP!IN NP\nFigure D.10 A sample of the CFG grammar rules and lexical entries that would be ex-\ntracted from the three treebank sentences in Fig. D.7 and Fig. D.9.\nwhich comes from the VPmarked in italics:\nThis mostly happens because we go from football in the fall to lifting in the\nwinter to football again in the spring .\nSome of the many thousands of NPrules include\nNP!DT JJ NN\nNP!DT JJ NNS\nNP!DT JJ NN NN\nNP!DT JJ JJ NN\nNP!DT JJ CD NNS\nNP!RB DT JJ NN NN\nNP!RB DT JJ JJ NNS\nNP!DT JJ JJ NNP NNS\nNP!DT NNP NNP NNP NNP JJ NN\nNP!DT JJ NNP CC JJ JJ NN NNS\nNP!RB DT JJS NN NN SBAR\nNP!DT VBG JJ NNP NNP CC NNP\nNP!DT JJ NNS , NNS CC NN NNS NN\nNP!DT JJ JJ VBG NN NNP NNP FW NNP\nNP!NP JJ , JJ `` SBAR '' NNS\nThe last two of those rules, for example, come from the following two noun phrases:\n[DTThe] [JJstate-owned ] [JJindustrial ] [VBGholding ] [NNcompany ] [NNPInstituto ] [NNPNacional ]\n[FWde] [NNPIndustria ]\n[NPShearson\u2019s ] [JJeasy-to-\ufb01lm ],[JJblack-and-white ]\u201c[SBAR Where We Stand ]\u201d[NNScommercials ]\nViewed as a large grammar in this way, the Penn Treebank III Wall Street Journal\ncorpus, which contains about 1 million words, also has about 1 million non-lexical\nrule tokens, consisting of about 17,500 distinct rule types.",
    "metadata": {
      "source": "D",
      "chunk_id": 16,
      "token_count": 646,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\n18 APPENDIX D \u2022 C ONSTITUENCY GRAMMARS\nS(dumped)\nVP(dumped)\nPP(into)\nNP(bin)\nNN(bin)\nbinDT(a)\naP\nintoNP(sacks)\nNNS(sacks)\nsacksVBD(dumped)\ndumpedNP(workers)\nNNS(workers)\nworkers\nFigure D.11 A lexicalized tree from Collins (1999).\nVarious facts about the treebank grammars, such as their large numbers of \ufb02at\nrules, pose problems for probabilistic parsing algorithms. For this reason, it is com-\nmon to make various modi\ufb01cations to a grammar extracted from a treebank. We\ndiscuss these further in Appendix C.\nD.4.3 Heads and Head-Finding\nWe suggested informally earlier that syntactic constituents could be associated with\na lexical head ;Nis the head of an NP,Vis the head of a VP. This idea of a head\nfor each constituent dates back to Bloom\ufb01eld 1914, and is central to the dependency\ngrammars and dependency parsing we\u2019ll introduce in Chapter 19. Heads are also\nimportant in probabilistic parsing (Appendix C) and in constituent-based grammar\nformalisms like Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994)..\nIn one simple model of lexical heads, each context-free rule is associated with\na head (Charniak 1997, Collins 1999). The head is the word in the phrase that is\ngrammatically the most important. Heads are passed up the parse tree; thus, each\nnon-terminal in a parse tree is annotated with a single word, which is its lexical\nhead. Figure D.11 shows an example of such a tree from Collins (1999), in which\neach non-terminal is annotated with its head.\nFor the generation of such a tree, each CFG rule must be augmented to identify\none right-side constituent to be the head child. The headword for a node is then set to\nthe headword of its head child. Choosing these head children is simple for textbook\nexamples ( NNis the head of NP) but is complicated and indeed controversial for\nmost phrases. (Should the complementizer toor the verb be the head of an in\ufb01nite\nverb phrase?) Modern linguistic theories of syntax generally include a component\nthat de\ufb01nes heads (see, e.g., (Pollard and Sag, 1994)).\nAn alternative approach to \ufb01nding a head is used in most practical computational\nsystems. Instead of specifying head rules in the grammar itself, heads are identi\ufb01ed\ndynamically in the context of trees for speci\ufb01c sentences. In other words, once\na sentence is parsed, the resulting tree is walked to decorate each node with the\nappropriate head. Most current systems rely on a simple set of handwritten rules,\nsuch as a practical one for Penn Treebank grammars given in Collins (1999) but\ndeveloped originally by Magerman (1995). For example, the rule for \ufb01nding the\nhead of an NPis as follows (Collins, 1999, p. 238):\n\u2022 If the last word is tagged POS, return last-word.",
    "metadata": {
      "source": "D",
      "chunk_id": 17,
      "token_count": 689,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19",
    "metadata": {
      "source": "D",
      "chunk_id": 18,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "D.5 \u2022 G RAMMAR EQUIVALENCE AND NORMAL FORM 19\n\u2022 Else search from right to left for the \ufb01rst child which is an NN, NNP, NNPS, NX, POS,\nor JJR.\n\u2022 Else search from left to right for the \ufb01rst child which is an NP.\n\u2022 Else search from right to left for the \ufb01rst child which is a $, ADJP, or PRN.\n\u2022 Else search from right to left for the \ufb01rst child which is a CD.\n\u2022 Else search from right to left for the \ufb01rst child which is a JJ, JJS, RB or QP.\n\u2022 Else return the last word\nSelected other rules from this set are shown in Fig. D.12. For example, for VP\nrules of the form VP!Y1\u0001\u0001\u0001Yn, the algorithm would start from the left of Y1\u0001\u0001\u0001\nYnlooking for the \ufb01rst Yiof type TO; if no TOs are found, it would search for the\n\ufb01rstYiof type VBD; if no VBDs are found, it would search for a VBN, and so on.\nSee Collins (1999) for more details.\nParent Direction Priority List\nADJP Left NNS QP NN $ ADVP JJ VBN VBG ADJP JJR NP JJS DT FW RBR RBS\nSBAR RB\nADVP Right RB RBR RBS FW ADVP TO CD JJR JJ IN NP JJS NN\nPRN Left\nPRT Right RP\nQP Left $ IN NNS NN JJ RB DT CD NCD QP JJR JJS\nS Left TO IN VP S SBAR ADJP UCP NP\nSBAR Left WHNP WHPP WHADVP WHADJP IN DT S SQ SINV SBAR FRAG\nVP Left TO VBD VBN MD VBZ VB VBG VBP VP ADJP NN NNS NP\nFigure D.12 Some head rules from Collins (1999). The head rules are also called a head percolation table .\nD.5 Grammar Equivalence and Normal Form\nA formal language is de\ufb01ned as a (possibly in\ufb01nite) set of strings of words. This\nsuggests that we could ask if two grammars are equivalent by asking if they gener-\nate the same set of strings. In fact, it is possible to have two distinct context-free\ngrammars generate the same language.\nWe usually distinguish two kinds of grammar equivalence: weak equivalence\nandstrong equivalence . Two grammars are strongly equivalent if they generate the\nsame set of strings andif they assign the same phrase structure to each sentence\n(allowing merely for renaming of the non-terminal symbols). Two grammars are\nweakly equivalent if they generate the same set of strings but do not assign the same\nphrase structure to each sentence.\nIt is sometimes useful to have a normal form for grammars, in which each of normal form\nthe productions takes a particular form. For example, a context-free grammar is in\nChomsky normal form (CNF) (Chomsky, 1963) if it is \u000f-free and if in additionChomsky\nnormal form\neach production is either of the form A!B C orA!a. That is, the right-hand side\nof each rule either has two non-terminal symbols or one terminal symbol. Chomsky\nnormal form grammars are binary branching , that is they have binary trees (downbinary\nbranching\nto the prelexical nodes). We make use of this binary branching property in the CKY\nparsing algorithm in Chapter 18.\nAny context-free grammar can be converted into a weakly equivalent Chomsky\nnormal form grammar. For example, a rule of the form\nA!B C D",
    "metadata": {
      "source": "D",
      "chunk_id": 19,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\n20 APPENDIX D \u2022 C ONSTITUENCY GRAMMARS\ncan be converted into the following two CNF rules (Exercise D. ??asks the reader to\nformulate the complete algorithm):\nA!B X\nX!C D\nSometimes using binary branching can actually produce smaller grammars. For\nexample, the sentences that might be characterized as\nVP -> VBD NP PP*\nare represented in the Penn Treebank by this series of rules:\nVP!VBD NP PP\nVP!VBD NP PP PP\nVP!VBD NP PP PP PP\nVP!VBD NP PP PP PP PP\n...\nbut could also be generated by the following two-rule grammar:\nVP!VBD NP PP\nVP!VP PP\nThe generation of a symbol A with a potentially in\ufb01nite sequence of symbols B with\na rule of the form A!A Bis known as Chomsky-adjunction .Chomsky-\nadjunction\nD.6 Summary\nThis chapter has introduced a number of fundamental concepts in syntax through\nthe use of context-free grammars .\n\u2022 In many languages, groups of consecutive words act as a group or a con-\nstituent , which can be modeled by context-free grammars (which are also\nknown as phrase-structure grammars ).\n\u2022 A context-free grammar consists of a set of rules orproductions , expressed\nover a set of non-terminal symbols and a set of terminal symbols. Formally,\na particular context-free language is the set of strings that can be derived\nfrom a particular context-free grammar .\n\u2022 Agenerative grammar is a traditional name in linguistics for a formal lan-\nguage that is used to model the grammar of a natural language.\n\u2022 There are many sentence-level grammatical constructions in English; declar-\native ,imperative ,yes-no question , and wh-question are four common types;\nthese can be modeled with context-free rules.\n\u2022 An English noun phrase can have determiners ,numbers ,quanti\ufb01ers , and\nadjective phrases preceding the head noun , which can be followed by a num-\nber of postmodi\ufb01ers ;gerundive andin\ufb01nitive VPs are common possibilities.\n\u2022Subjects in English agree with the main verb in person and number.\n\u2022 Verbs can be subcategorized by the types of complements they expect. Sim-\nple subcategories are transitive and intransitive ; most grammars include\nmany more categories than these.\n\u2022Treebanks of parsed sentences exist for many genres of English and for many\nlanguages. Treebanks can be searched with tree-search tools.",
    "metadata": {
      "source": "D",
      "chunk_id": 20,
      "token_count": 541,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21",
    "metadata": {
      "source": "D",
      "chunk_id": 21,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 21\n\u2022 Any context-free grammar can be converted to Chomsky normal form , in\nwhich the right-hand side of each rule has either two non-terminals or a single\nterminal.\nBibliographical and Historical Notes\nAccording to Percival (1976), the idea of breaking up a sentence into a hierarchy of\nconstituents appeared in the V\u00a8olkerpsychologie of the groundbreaking psychologist\nWilhelm Wundt (Wundt, 1900):\n...den sprachlichen Ausdruck f \u00a8ur die willk \u00a8urliche Gliederung einer Ge-\nsammtvorstellung in ihre in logische Beziehung zueinander gesetzten\nBestandteile\n[the linguistic expression for the arbitrary division of a total idea\ninto its constituent parts placed in logical relations to one another]\nWundt\u2019s idea of constituency was taken up into linguistics by Leonard Bloom-\n\ufb01eld in his early book An Introduction to the Study of Language (Bloom\ufb01eld, 1914).\nBy the time of his later book, Language (Bloom\ufb01eld, 1933), what was then called\n\u201cimmediate-constituent analysis\u201d was a well-established method of syntactic study\nin the United States. By contrast, traditional European grammar, dating from the\nClassical period, de\ufb01ned relations between words rather than constituents, and Eu-\nropean syntacticians retained this emphasis on such dependency grammars, the sub-\nject of Chapter 19.\nAmerican Structuralism saw a number of speci\ufb01c de\ufb01nitions of the immediate\nconstituent, couched in terms of their search for a \u201cdiscovery procedure\u201d: a method-\nological algorithm for describing the syntax of a language. In general, these attempt\nto capture the intuition that \u201cThe primary criterion of the immediate constituent\nis the degree in which combinations behave as simple units\u201d (Bazell, 1952/1966, p.\n284). The most well known of the speci\ufb01c de\ufb01nitions is Harris\u2019 idea of distributional\nsimilarity to individual units, with the substitutability test. Essentially, the method\nproceeded by breaking up a construction into constituents by attempting to substitute\nsimple structures for possible constituents\u2014if a substitution of a simple form, say,\nman, was substitutable in a construction for a more complex set (like intense young\nman), then the form intense young man was probably a constituent. Harris\u2019s test was\nthe beginning of the intuition that a constituent is a kind of equivalence class.\nThe \ufb01rst formalization of this idea of hierarchical constituency was the phrase-\nstructure grammar de\ufb01ned in Chomsky (1956) and further expanded upon (and\nargued against) in Chomsky (1957) and Chomsky (1956/1975). From this time on,\nmost generative linguistic theories were based at least in part on context-free gram-\nmars or generalizations of them (such as Head-Driven Phrase Structure Grammar\n(Pollard and Sag, 1994), Lexical-Functional Grammar (Bresnan, 1982), the Mini-\nmalist Program (Chomsky, 1995), and Construction Grammar (Kay and Fillmore,\n1999), inter alia); many of these theories used schematic context-free templates\nknown as X-bar schemata , which also relied on the notion of syntactic head.X-bar\nschemata\nShortly after Chomsky\u2019s initial work, the context-free grammar was reinvented\nby Backus (1959) and independently by Naur et al. (1960) in their descriptions of",
    "metadata": {
      "source": "D",
      "chunk_id": 22,
      "token_count": 776,
      "chapter_title": ""
    }
  },
  {
    "content": "simple structures for possible constituents\u2014if a substitution of a simple form, say,\nman, was substitutable in a construction for a more complex set (like intense young\nman), then the form intense young man was probably a constituent. Harris\u2019s test was\nthe beginning of the intuition that a constituent is a kind of equivalence class.\nThe \ufb01rst formalization of this idea of hierarchical constituency was the phrase-\nstructure grammar de\ufb01ned in Chomsky (1956) and further expanded upon (and\nargued against) in Chomsky (1957) and Chomsky (1956/1975). From this time on,\nmost generative linguistic theories were based at least in part on context-free gram-\nmars or generalizations of them (such as Head-Driven Phrase Structure Grammar\n(Pollard and Sag, 1994), Lexical-Functional Grammar (Bresnan, 1982), the Mini-\nmalist Program (Chomsky, 1995), and Construction Grammar (Kay and Fillmore,\n1999), inter alia); many of these theories used schematic context-free templates\nknown as X-bar schemata , which also relied on the notion of syntactic head.X-bar\nschemata\nShortly after Chomsky\u2019s initial work, the context-free grammar was reinvented\nby Backus (1959) and independently by Naur et al. (1960) in their descriptions of\nthe ALGOL programming language; Backus (1996) noted that he was in\ufb02uenced by\nthe productions of Emil Post and that Naur\u2019s work was independent of his (Backus\u2019)",
    "metadata": {
      "source": "D",
      "chunk_id": 23,
      "token_count": 337,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22\n\n22 APPENDIX D \u2022 C ONSTITUENCY GRAMMARS\nown. After this early work, a great number of computational models of natural\nlanguage processing were based on context-free grammars because of the early de-\nvelopment of ef\ufb01cient algorithms to parse these grammars (see Chapter 18).\nThre are various classes of extensions to CFGs, many designed to handle long-\ndistance dependencies in the syntax. (Other grammars instead treat long-distance-\ndependent items as being related semantically rather than syntactically (Kay and\nFillmore 1999, Culicover and Jackendoff 2005).\nOne extended formalism is Tree Adjoining Grammar (TAG) (Joshi, 1985).\nThe primary TAG data structure is the tree, rather than the rule. Trees come in two\nkinds: initial trees andauxiliary trees . Initial trees might, for example, represent\nsimple sentential structures, and auxiliary trees add recursion into a tree. Trees are\ncombined by two operations called substitution andadjunction . The adjunction\noperation handles long-distance dependencies. See Joshi (1985) for more details.\nTree Adjoining Grammar is a member of the family of mildly context-sensitive\nlanguages .\nWe mentioned on page 15 another way of handling long-distance dependencies,\nbased on the use of empty categories and co-indexing. The Penn Treebank uses\nthis model, which draws (in various Treebank corpora) from the Extended Standard\nTheory and Minimalism (Radford, 1997).\nReaders interested in the grammar of English should get one of the three large\nreference grammars of English: Huddleston and Pullum (2002), Biber et al. (1999),\nand Quirk et al. (1985).\nThere are many good introductory textbooks on syntax from different perspec-\ntives. Sag et al. (2003) is an introduction to syntax from a generative perspective, generative\nfocusing on the use of phrase-structure rules, uni\ufb01cation, and the type hierarchy in\nHead-Driven Phrase Structure Grammar. Van Valin, Jr. and La Polla (1997) is an\nintroduction from a functional perspective, focusing on cross-linguistic data and on functional\nthe functional motivation for syntactic structures.\nExercises\nD.1 Draw tree structures for the following ATIS phrases:\n1. Dallas\n2. from Denver\n3. after \ufb01ve p.m.\n4. arriving in Washington\n5. early \ufb02ights\n6. all redeye \ufb02ights\n7. on Thursday\n8. a one-way fare\n9. any delays in Denver\nD.2 Draw tree structures for the following ATIS sentences:\n1. Does American Airlines have a \ufb02ight between \ufb01ve a.m. and six a.m.?\n2. I would like to \ufb02y on American Airlines.\n3. Please repeat that.\n4. Does American 487 have a \ufb01rst-class section?\n5. I need to \ufb02y between Philadelphia and Atlanta.\n6. What is the fare from Atlanta to Denver?",
    "metadata": {
      "source": "D",
      "chunk_id": 24,
      "token_count": 655,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23\n\nEXERCISES 23\n7. Is there an American Airlines \ufb02ight from Philadelphia to Dallas?\nD.3 Assume a grammar that has many VPrules for different subcategorizations,\nas expressed in Section D.3.4, and differently subcategorized verb rules like\nVerb-with-NP-complement . How would the rule for postnominal relative clauses\n(D.4) need to be modi\ufb01ed if we wanted to deal properly with examples like\nthe earliest \ufb02ight that you have ? Recall that in such examples the pronoun\nthatis the object of the verb get. Your rules should allow this noun phrase but\nshould correctly rule out the ungrammatical S *I get .\nD.4 Does your solution to the previous problem correctly model the NP the earliest\n\ufb02ight that I can get ? How about the earliest \ufb02ight that I think my mother\nwants me to book for her ? Hint: this phenomenon is called long-distance\ndependency .\nD.5 Write rules expressing the verbal subcategory of English auxiliaries; for ex-\nample, you might have a rule verb-with-bare-stem-VP-complement !can.\nD.6 NPs like Fortune\u2019s of\ufb01ce ormy uncle\u2019s marks are called possessive orgenitive possessive\ngenitive noun phrases. We can model possessive noun phrases by treating the sub-NP\nlikeFortune\u2019s ormy uncle\u2019s as a determiner of the following head noun. Write\ngrammar rules for English possessives. You may treat \u2019sas if it were a separate\nword (i.e., as if there were always a space before \u2019s).\nD.7 Page 8 discussed the need for a Wh-NP constituent. The simplest Wh-NP is\none of the Wh-pronouns (who, whom, whose, which ). The Wh-words what\nandwhich can be determiners: which four will you have? ,what credit do you\nhave with the Duke? Write rules for the different types of Wh-NP s.",
    "metadata": {
      "source": "D",
      "chunk_id": 25,
      "token_count": 437,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24",
    "metadata": {
      "source": "D",
      "chunk_id": 26,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "24 Appendix D \u2022 Constituency Grammars\nBackus, J. W. 1959. The syntax and semantics of the\nproposed international algebraic language of the Zurich\nACM-GAMM Conference. Information Processing: Pro-\nceedings of the International Conference on Information\nProcessing, Paris . UNESCO.\nBackus, J. W. 1996. Transcript of question and answer ses-\nsion. In R. L. Wexelblat, ed., History of Programming\nLanguages , page 162. Academic Press.\nBazell, C. E. 1952/1966. The correspondence fallacy in\nstructural linguistics. In E. P. Hamp, F. W. Householder,\nand R. Austerlitz, eds, Studies by Members of the En-\nglish Department, Istanbul University (3), reprinted in\nReadings in Linguistics II (1966) , 271\u2013298. University of\nChicago Press.\nBiber, D., S. Johansson, G. Leech, S. Conrad, and E. Fine-\ngan. 1999. Longman Grammar of Spoken and Written\nEnglish . Pearson.\nBies, A., M. Ferguson, K. Katz, and R. MacIntyre. 1995.\nBracketing guidelines for Treebank II style Penn Tree-\nbank Project.\nBloom\ufb01eld, L. 1914. An Introduction to the Study of Lan-\nguage . Henry Holt and Company.\nBloom\ufb01eld, L. 1933. Language . University of Chicago\nPress.\nBresnan, J., ed. 1982. The Mental Representation of Gram-\nmatical Relations . MIT Press.\nCharniak, E. 1997. Statistical parsing with a context-free\ngrammar and word statistics. AAAI .\nChomsky, N. 1956. Three models for the description of\nlanguage. IRE Transactions on Information Theory ,\n2(3):113\u2013124.\nChomsky, N. 1956/1975. The Logical Structure of Linguistic\nTheory . Plenum.\nChomsky, N. 1957. Syntactic Structures . Mouton.\nChomsky, N. 1963. Formal properties of grammars. In R. D.\nLuce, R. Bush, and E. Galanter, eds, Handbook of Math-\nematical Psychology , volume 2, 323\u2013418. Wiley.\nChomsky, N. 1995. The Minimalist Program . MIT Press.\nCollins, M. 1999. Head-Driven Statistical Models for Natu-\nral Language Parsing . Ph.D. thesis, University of Penn-\nsylvania, Philadelphia.\nCulicover, P. W. and R. Jackendoff. 2005. Simpler Syntax .\nOxford University Press.\nGazdar, G., E. Klein, G. K. Pullum, and I. A. Sag. 1985.\nGeneralized Phrase Structure Grammar . Blackwell.\nHarris, Z. S. 1946. From morpheme to utterance. Language ,\n22(3):161\u2013183.\nHemphill, C. T., J. Godfrey, and G. Doddington. 1990. The\nATIS spoken language systems pilot corpus. Speech and\nNatural Language Workshop .\nHopcroft, J. E. and J. D. Ullman. 1979. Introduction to Au-\ntomata Theory, Languages, and Computation . Addison-\nWesley.",
    "metadata": {
      "source": "D",
      "chunk_id": 27,
      "token_count": 744,
      "chapter_title": ""
    }
  },
  {
    "content": "Chomsky, N. 1963. Formal properties of grammars. In R. D.\nLuce, R. Bush, and E. Galanter, eds, Handbook of Math-\nematical Psychology , volume 2, 323\u2013418. Wiley.\nChomsky, N. 1995. The Minimalist Program . MIT Press.\nCollins, M. 1999. Head-Driven Statistical Models for Natu-\nral Language Parsing . Ph.D. thesis, University of Penn-\nsylvania, Philadelphia.\nCulicover, P. W. and R. Jackendoff. 2005. Simpler Syntax .\nOxford University Press.\nGazdar, G., E. Klein, G. K. Pullum, and I. A. Sag. 1985.\nGeneralized Phrase Structure Grammar . Blackwell.\nHarris, Z. S. 1946. From morpheme to utterance. Language ,\n22(3):161\u2013183.\nHemphill, C. T., J. Godfrey, and G. Doddington. 1990. The\nATIS spoken language systems pilot corpus. Speech and\nNatural Language Workshop .\nHopcroft, J. E. and J. D. Ullman. 1979. Introduction to Au-\ntomata Theory, Languages, and Computation . Addison-\nWesley.\nHuddleston, R. and G. K. Pullum. 2002. The Cambridge\nGrammar of the English Language . Cambridge Univer-\nsity Press.Joshi, A. K. 1985. Tree adjoining grammars: How\nmuch context-sensitivity is required to provide reasonable\nstructural descriptions? In D. R. Dowty, L. Karttunen,\nand A. Zwicky, eds, Natural Language Parsing , 206\u2013250.\nCambridge University Press.\nKay, P. and C. J. Fillmore. 1999. Grammatical constructions\nand linguistic generalizations: The What\u2019s X Doing Y?\nconstruction. Language , 75(1):1\u201333.\nMagerman, D. M. 1995. Statistical decision-tree models for\nparsing. ACL.\nMarcus, M. P., G. Kim, M. A. Marcinkiewicz, R. MacIn-\ntyre, A. Bies, M. Ferguson, K. Katz, and B. Schasberger.\n1994. The Penn Treebank: Annotating predicate argu-\nment structure. HLT.\nMarcus, M. P., B. Santorini, and M. A. Marcinkiewicz. 1993.\nBuilding a large annotated corpus of English: The Penn\ntreebank. Computational Linguistics , 19(2):313\u2013330.\nNaur, P., J. W. Backus, F. L. Bauer, J. Green, C. Katz,\nJ. McCarthy, A. J. Perlis, H. Rutishauser, K. Samelson,\nB. Vauquois, J. H. Wegstein, A. van Wijnagaarden, and\nM. Woodger. 1960. Report on the algorithmic language\nALGOL 60. CACM , 3(5):299\u2013314. Revised in CACM\n6:1, 1-17, 1963.\nNivre, J., M.-C. de Marneffe, F. Ginter, Y . Goldberg, J. Haji \u02c7c,\nC. D. Manning, R. McDonald, S. Petrov, S. Pyysalo,",
    "metadata": {
      "source": "D",
      "chunk_id": 28,
      "token_count": 754,
      "chapter_title": ""
    }
  },
  {
    "content": "tyre, A. Bies, M. Ferguson, K. Katz, and B. Schasberger.\n1994. The Penn Treebank: Annotating predicate argu-\nment structure. HLT.\nMarcus, M. P., B. Santorini, and M. A. Marcinkiewicz. 1993.\nBuilding a large annotated corpus of English: The Penn\ntreebank. Computational Linguistics , 19(2):313\u2013330.\nNaur, P., J. W. Backus, F. L. Bauer, J. Green, C. Katz,\nJ. McCarthy, A. J. Perlis, H. Rutishauser, K. Samelson,\nB. Vauquois, J. H. Wegstein, A. van Wijnagaarden, and\nM. Woodger. 1960. Report on the algorithmic language\nALGOL 60. CACM , 3(5):299\u2013314. Revised in CACM\n6:1, 1-17, 1963.\nNivre, J., M.-C. de Marneffe, F. Ginter, Y . Goldberg, J. Haji \u02c7c,\nC. D. Manning, R. McDonald, S. Petrov, S. Pyysalo,\nN. Silveira, R. Tsarfaty, and D. Zeman. 2016. Univer-\nsal Dependencies v1: A multilingual treebank collection.\nLREC .\nPercival, W. K. 1976. On the historical source of immedi-\nate constituent analysis. In J. D. McCawley, ed., Syntax\nand Semantics Volume 7, Notes from the Linguistic Un-\nderground , 229\u2013242. Academic Press.\nPollard, C. and I. A. Sag. 1994. Head-Driven Phrase Struc-\nture Grammar . University of Chicago Press.\nQuirk, R., S. Greenbaum, G. Leech, and J. Svartvik. 1985.\nA Comprehensive Grammar of the English Language .\nLongman.\nRadford, A. 1997. Syntactic Theory and the Structure of\nEnglish: A Minimalist Approach . Cambridge University\nPress.\nSag, I. A., T. Wasow, and E. M. Bender, eds. 2003. Syntac-\ntic Theory: A Formal Introduction . CSLI Publications,\nStanford, CA.\nVan Valin, Jr., R. D. and R. La Polla. 1997. Syntax: Structure,\nMeaning, and Function . Cambridge University Press.\nWundt, W. 1900. V\u00a8olkerpsychologie: eine Untersuchung der\nEntwicklungsgesetze von Sprache, Mythus, und Sitte . W.\nEngelmann, Leipzig. Band II: Die Sprache, Zweiter Teil.",
    "metadata": {
      "source": "D",
      "chunk_id": 29,
      "token_count": 615,
      "chapter_title": ""
    }
  }
]