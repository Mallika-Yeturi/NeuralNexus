[
  {
    "content": "# 12\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n12Model Alignment, Prompting,\nand In-Context Learning\n\u201cHal, \u201d said Bowman, now speaking with an icy calm. \u201cI am not incapaci-\ntated. Unless you obey my instructions, I shall be forced to disconnect you. \u201d\nArthur C. Clarke\nIn this chapter we show how to get LLMs to do tasks for us simply by talking to\nthem. To get an LLM to translate a sentence, outline a talk, or draft a work email,\nwe\u2019ll simply describe what we want in natural language. We call these instructions\nwe give to language models prompts . prompts\nPrompting relies on contextual generation. Given the prompt as context, the lan-\nguage model generates the next token based on its token probability, conditioned on\nthe prompt: P(wijw<i). A prompt can be a question (like \u201cWhat is a transformer net-\nwork?\u201d), possibly in a structured format (like \u201cQ: What is a transformer network?\nA:\u201d), or can be an instruction (like \u201cTranslate the following sentence into Hindi:\n\u2018Chop the garlic \ufb01nely\u2019\u201d). A prompt can also contain demonstrations , examples to demonstrations\nhelp make the instructions clearer, (like \u201cGive the sentiment of the following sen-\ntence. Example Input: \u201cI really loved Taishan Cuisine.\u201d Output: positive\u201d.) As we\u2019ll\nsee, prompting can be applied to inherently generative tasks (like summarization and\ntranslation) as well as to ones more naturally thought of as classi\ufb01cation tasks.\nPrompts get language models to generate text, but they also can be viewed as\nalearning signal, because these demonstrations can help language models learn\nto perform novel tasks. For this reason we also refer to prompting as in-context-\nlearning \u2014learning that improves model performance or reduces some loss but doesin-context-\nlearning\nnot involve gradient-based updates to the model\u2019s underlying parameters.\nBut LLMs as we\u2019ve described them so far turn out to be bad at following instruc-\ntions. Pretraining isn\u2019t suf\ufb01cient to make them helpful . We\u2019ll introduce instruction\ntuning , a technique that helps LLMs learn to correctly respond to instructions byinstruction\ntuning\n\ufb01netuning them on a corpus of instructions with their corresponding response.\nA second failure of LLMs is that they can be harmful : their pretraining isn\u2019t\nsuf\ufb01cient to make them safe. Readers who know Arthur C. Clarke\u2019s 2001: A Space\nOdyssey or the Stanley Kubrick \ufb01lm know that the quote above comes in the context\nthat the arti\ufb01cial intelligence Hal becomes paranoid and tries to kill the crew of the\nspaceship. Unlike Hal, language models don\u2019t have intentionality or mental health\nissues like paranoid thinking, but they do have the capacity for harm. Pretrained lan-\nguage models can say things that are dangerous or false (like giving unsafe medical\nadvice) and they can verbally attack users or say toxic or hateful things.\nDealing with safety can be done partly by adding safety training into instruction\ntuning. But an important aspect of safety training is a second technique, preference\nalignment (often implemented, as we\u2019ll see, with the RLHF orDPO algorithms) inpreference\nalignment\nwhich a separate model is trained to decide how much a candidate response aligns\nwith human preferences. Together we refer to instruction tuning and preference\nalignment as model alignment . The intuition is that we want the learning objectivesmodel\nalignment\nof models to be aligned with the goals of the humans that use them.",
    "metadata": {
      "source": "12",
      "chunk_id": 0,
      "token_count": 792,
      "chapter_title": "12"
    }
  },
  {
    "content": "## Page 2\n\n2CHAPTER 12 \u2022 M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING\n12.1 Prompting\nAprompt is a text string that a user issues to a language model to get the model prompt\nto do something useful. In prompting, the user\u2019s prompt string is passed to the\nlanguage model, which iteratively generates tokens conditioned on the prompt. Thus\nthe prompt creates a context that guides LLMs to generate useful outputs to achieve\nsome user goal. The process of \ufb01nding effective prompts for a task is known as\nprompt engineering .prompt\nengineering\nLet\u2019s see how to prompt a language model to solve a simple sentiment classi\ufb01-\ncation task, Consider this hotel review from the BLT corpus (Salvetti et al., 2016):\nSample Hotel Review\nDid not like the service that I was provided, when I entered the hotel. I also\ndid not like the area, in which the hotel was located. Too much noise and\nevents going on for me to feel relax.\nWe can get the model to classify the sentiment of this text by taking the text and\nappending an incomplete statement to the review like In short, our stay was :\nA prompt consisting of a review plus an incomplete statement\nDid not like the service that I was provided, when I entered the hotel. I also\ndid not like the area, in which the hotel was located. Too much noise and\nevents going on for me to feel relax. In short, our stay was\nWe then have an LLM complete the statement by generating a token conditioned\non the prompt so far, and then generating the next token (conditioned on the prompt\nplus the new token), and so forth. Here are a few responses from a language model\nvia continued generation starting with the prompt as context.\nHotel Review Completions\nDid not like the service that I was provided, when I entered the hotel. I also\ndid not like the area, in which the hotel was located. Too much noise and\nevents going on for me to feel relaxed. In short our stay was\n... not a pleasant one. The staff at the front desk\nwere not welcoming or friendly, and seemed disinterested\nin providing good customer service.\n... uncomfortable and not worth the price we paid. We\nwill not be returning to this hotel.\nAs we can see, the overall negative context of the review results in negative\ncompletions. We could easily map these completions to the class we are trying\nto predict, perhaps via some prede\ufb01ned mappings, like fexcellent!positiveg,\nfdid not like!negativeg, and so on.\nThe power of this approach is that with suitable additions to the context a single\nLLM can produce outputs appropriate for many different tasks . For example, given",
    "metadata": {
      "source": "12",
      "chunk_id": 1,
      "token_count": 589,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\n12.1 \u2022 P ROMPTING 3\na review we might want any of the following:\n\u2022 A summary,\n\u2022 Whether the review was truthful or likely to have been fabricated,\n\u2022 A translation to another language.\nLLMs have a striking ability to perform tasks like these, needing just the appro-\npriate contextual nudge to get the LLM to generate the desired output.\nIf we want to solve general tasks like summarization or translation, we don\u2019t\nwant to have to create a new prompt each time we do the task. Instead the \ufb01rst step\nin prompting is to design one or more templates : task-speci\ufb01c prompting text along templates\nwith slots for the particular input that is being processed.\nConsider the following templates for a variety of tasks:\nBasic Prompt Templates\nSummarization finputg;tldr;\nTranslation finputg;translate to French:\nSentiment finputg;Overall, it was\nFine-Grained- finputg;What aspects were important in this review?\nSentiment\nEach template consists of an input text, designated as finputg, followed by a\nverbatim prompt to be passed to an LLM. These templates are applied to inputs to\ncreate \ufb01lled prompts \u2013 instantiated prompts suitable for use as inputs to an LLM.\nFig. 12.1 illustrates \ufb01lled prompts for these templates using our earlier hotel review,\nalong with sample outputs from an LLM:\nNotice the design pattern of the prompts above: the input is followed by some\ntext which in turn will be completed by the desired response. This style, with the\ninstruction at the end, is common in prompting because it helpfully constrains the\ngeneration. Consider, by contrast, the prompt in Example 12.1.\nTranslate English to French:\nDid not like the service that I was provided! (12.1)\nThis prompt doesn\u2019t do a good job of constraining possible continuations. Instead\nof a French translation, models given this prompt may instead generate another sen-\ntence in English that simply extends the English review. Prompts need to be designed\nunambiguously, so that any reasonable continuation would accomplish the desired\ntask (Reynolds and McDonell, 2021).\nAn even more constraining style of prompt can specify the set of possible an-\nswers in the prompt. For example here is a prompt template to do sentiment analysis\nthat prespeci\ufb01es the potential answers:\nA prompt consisting of a review plus an incomplete statement\nHuman: Do you think that \u201cinput\u201d has negative or positive sentiment?\nChoices:\n(P) Positive\n(N) Negative\nAssistant: I believe the best answer is: (",
    "metadata": {
      "source": "12",
      "chunk_id": 2,
      "token_count": 557,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4CHAPTER 12 \u2022 M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING\nLLM Outputs for Basic Prompts\nOriginal Review ($INPUT) Did not like the service that I was provided,\nwhen I entered the hotel. I also did not like\nthe area, in which the hotel was located. Too\nmuch noise and events going on for me to feel\nrelax and away from the city life.\nSentiment Prompt :$INPUT + In short, our stay was\nOutput :not enjoyable\nFine-grained Sentiment Prompt :$INPUT + These aspects were important to\nthe reviewer:\nOutput :1. Poor service 2. Unpleasant location\n3. Noisy and busy area\nSummarization Prompt :$INPUT + tl;dr\nOutput :I had a bad experience with the hotel's\nservice and the location was loud and busy.\nTranslation Prompt :$INPUT + Translate this to French\nOutput :Je n'ai pas aim \u0013e le service qui m'a \u0013et\u0013e\noffert lorsque je suis entr \u0013e dans l'h ^otel. Je\nn'ai\u0013egalement pas aim \u0013e la zone dans laquelle se\ntrouvait l'h ^otel. Trop de bruit et d' \u0013ev\u0013enements\npour que je me sente d \u0013etendu et loin de la vie\ncitadine.\nFigure 12.1 LLM outputs for simple prompts for sentiment, summarization and translation for an input text.\nThis prompt uses a number of more sophisticated prompting characteristics. It\nspeci\ufb01es the two allowable choices (P) and (N), and ends the prompt with the open\nparenthesis that strongly suggests the answer will be (P) or (N). Note that it also\nspeci\ufb01es the role of the language model as an assistant.\nWe can do even more with prompts. For example, we might want to restrict a\nsummary to be a particular length, to have an answer generated according to some\nkind of persona or role, or to specify a more structured output using a programming\nlanguage or a data interchange format such as JSON. Or we may want to prompt\nthe system to break down complex tasks, using methods like chain-of-thought that\nwe\u2019ll discuss in Section 12.4. All of these kinds of instructions go beyond simple\nprompting and require further LLM \ufb01netuning to enable them to follow instructions.\nWe\u2019ll return to this notion of instruction tuning in Section 12.3.\nIn summary, we prompt an LM by transforming each task into a form that is\namenable to contextual generation by an LLM, as follows:\n1. For a given task, develop a a task-speci\ufb01c template that has a free parameter\nfor the input text.\n2. Given that input and the task-speci\ufb01c template , the input is used to instantiate template\na\ufb01lled prompt that is then passed to a pretrained language model.\n3. Autoregressive decoding is then used to generate a sequence of token outputs.\n4. The output of the model can either be used directly as the desired output (as\nin the case of naturally generative tasks such as translation or summarization),\nor a task-appropriate answer can be extracted from the generated output (as in\nthe case of classi\ufb01cation).",
    "metadata": {
      "source": "12",
      "chunk_id": 3,
      "token_count": 706,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\n12.1 \u2022 P ROMPTING 5\n12.1.1 Learning from Demonstrations: Few-Shot Prompting\nIt\u2019s often possible to improve a prompt by including some labeled examples in the\nprompt template. We call such examples demonstrations . The task of prompting demonstrations\nwith examples is sometimes called few-shot prompting , as contrasted with zero- few-shot\nshot prompting which means instructions that don\u2019t include labeled examples. zero-shot\nFig. 12.2 illustrates a few-shot example from an extractive question answering\ntask. The context combines the task de\ufb01nition along with three gold-standard ques-\ntion and answer pairs from the training set.\nDe\ufb01nition : This task is about writing a correct answer for the reading comprehension task.\nBased on the information provided in a given passage, you should identify the shortest\ncontinuous text span from the passage that serves as an answer to the given question. Avoid\nanswers that are incorrect or provides incomplete justi\ufb01cation for the question.\nPassage : Beyonc \u00b4e Giselle Knowles-Carter (born September 4, 1981) is an American singer,\nsongwriter, record producer and actress. Born and raised in Houston, Texas, she performed in\nvarious singing and dancing competitions as a child, and rose to fame in the late 1990s as lead\nsinger of R&B girl-group Destiny\u2019s Child. Managed by her father, Mathew Knowles, the group\nbecame one of the world\u2019s best-selling girl groups of all time. Their hiatus saw the release\nof Beyonc \u00b4e\u2019s debut album, Dangerously in Love (2003), which established her as a solo artist\nworldwide, earned \ufb01ve Grammy Awards and featured the Billboard Hot 100 number-one singles\n\u201cCrazy in Love\u201d and \u201cBaby Boy\u201d.\nExamples:\nQ: In what city and state did Beyonc \u00b4e grow up?\nA: Houston, Texas\nQ: What areas did Beyonc \u00b4e compete in when she was growing up?\nA: singing and dancing\nQ: When did Beyonc \u00b4e release Dangerously in Love?\nA: 2003\nQ: When did Beyonc \u00b4e start becoming popular?\nA:\nFigure 12.2 A prompt for extractive question answering, from an example from the SQuAD 2.0 dataset\n(Rajpurkar et al., 2018). The prompt contains the task de\ufb01nition, the passage, 3 demonstration examples,\nfollowed by the test question. This de\ufb01nition speci\ufb01cation and format are after the Natural Instructions dataset\n(Mishra et al., 2022).\nHow Many Demonstrations? The number of demonstrations doesn\u2019t have to be\nlarge. A small number of randomly selected labeled examples used as demonstra-\ntions can be suf\ufb01cient to improve performance over the zero-shot setting. Indeed,\nthe largest performance gains in few-shot prompting tends to come from the \ufb01rst\ntraining example, with diminishing returns for subsequent demonstrations. This is\nin contrast with \ufb01netuning of specialized classi\ufb01er heads that we saw in Chapter 11\nwhere it helps to have lots of examples.\nWhy isn\u2019t it useful to have more demonstrations? The reason is that the primary\nbene\ufb01t in examples is to demonstrate the task to be performed to the LLM and the\nformat of the sequence, not to provide relevant information as to the right answer",
    "metadata": {
      "source": "12",
      "chunk_id": 4,
      "token_count": 738,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6",
    "metadata": {
      "source": "12",
      "chunk_id": 5,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "6CHAPTER 12 \u2022 M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING\nfor any particular question. In fact, demonstrations that have incorrect answers can\nstill improve a system (Min et al., 2022; Webson and Pavlick, 2022). Adding too\nmany examples seems to cause the model to over\ufb01t to details of the exact examples\nchosen and generalize poorly.\nHow to Select Demonstrations? Demonstrations are generally created by format-\nting examples drawn from a labeled training set. There are some heuristics about\nwhat makes a good demonstration. For example, using demonstrations that are sim-\nilarto the current input seems to improve performance. It can thus be useful to\ndynamically retrieve demonstrations for each input, based on their similarity to the\ncurrent example (for example, comparing the embedding of the current example\nwith embeddings of each of the training set example to \ufb01nd the best top- T).\nBut more generally, the best way to select demonstrations from the training set\nis programmatically: choosing the set of demonstrations that most increases task\nperformance of the prompt on a test set. Task performance for sentiment analysis\nor multiple-choice question answering can be measured in accuracy; for machine\ntranslation with chrF, and for summarization via Rouge. Systems like DSPy (Khat-\ntab et al., 2024), a framework for algorithmically optimizing LM prompts, can au-\ntomatically \ufb01nd the optimum set of demonstrations to include by searching through\nthe space of possible demonstrations to include. We\u2019ll return to automatic prompt\noptimization in Section 12.5.\n12.1.2 In-Context Learning and Induction Heads\nAs a way of getting a model to do what we want, prompting is fundamentally differ-\nent than pretraining. Learning via pretraining means updating the model\u2019s parame-\nters by using gradient descent according to some loss function. But prompting with\ndemonstrations can teach a model to do a new task. The model is learning something\nas it processes the prompt.\nEven without demonstrations, we can think of the process of prompting as a kind\nof learning. For example, the further a model gets in a prompt, the better it tends\nto get at predicting the upcoming tokens. The information in the context is helping\ngive the model more predictive power.\nThe term in-context learning was \ufb01rst proposed by Brown et al. (2020) in theirin-context\nlearning\nintroduction of the GPT3 system, to refer to either of these kinds of learning that lan-\nguage models do from their prompts. In-context learning means language models\nlearning to do new tasks, better predict tokens, or generally reduce their loss dur-\ning the forward-pass at inference-time, without any gradient-based updates to the\nmodel\u2019s parameters.\nHow does in-context learning work? While we don\u2019t know for sure, there are\nsome intriguing ideas. One hypothesis is based on the idea of induction heads induction heads\n(Elhage et al., 2021; Olsson et al., 2022). Induction heads are the name for a circuit ,\nwhich is a kind of abstract component of a network. The induction head circuit\nis part of the attention computation in transformers, discovered by looking at mini\nlanguage models with only 1-2 attention heads.\nThe function of the induction head is to predict repeated sequences. For example\nif it sees the pattern AB...A in an input sequence, it predicts that Bwill follow,\ninstantiating the pattern completion ruleAB...A!B. It does this by having a pre\ufb01x\nmatching component of the attention computation that, when looking at the current",
    "metadata": {
      "source": "12",
      "chunk_id": 6,
      "token_count": 763,
      "chapter_title": ""
    }
  },
  {
    "content": "give the model more predictive power.\nThe term in-context learning was \ufb01rst proposed by Brown et al. (2020) in theirin-context\nlearning\nintroduction of the GPT3 system, to refer to either of these kinds of learning that lan-\nguage models do from their prompts. In-context learning means language models\nlearning to do new tasks, better predict tokens, or generally reduce their loss dur-\ning the forward-pass at inference-time, without any gradient-based updates to the\nmodel\u2019s parameters.\nHow does in-context learning work? While we don\u2019t know for sure, there are\nsome intriguing ideas. One hypothesis is based on the idea of induction heads induction heads\n(Elhage et al., 2021; Olsson et al., 2022). Induction heads are the name for a circuit ,\nwhich is a kind of abstract component of a network. The induction head circuit\nis part of the attention computation in transformers, discovered by looking at mini\nlanguage models with only 1-2 attention heads.\nThe function of the induction head is to predict repeated sequences. For example\nif it sees the pattern AB...A in an input sequence, it predicts that Bwill follow,\ninstantiating the pattern completion ruleAB...A!B. It does this by having a pre\ufb01x\nmatching component of the attention computation that, when looking at the current\ntoken A, searches back over the context to \ufb01nd a prior instance of A. If it \ufb01nds one,\nthe induction head has a copying mechanism that \u201ccopies\u201d the token B that followed",
    "metadata": {
      "source": "12",
      "chunk_id": 7,
      "token_count": 326,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7",
    "metadata": {
      "source": "12",
      "chunk_id": 8,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "12.2 \u2022 P OST-TRAINING AND MODEL ALIGNMENT 7\nthe earlier A, by increasing the probability the B will occur next. Fig. 12.3 shows an\nexample.\nFigure 1: In the sequence \u201c...vintage cars ... vintage\u201d, an induction head identi\ufb01es the initial occurrence of \u201cvintage\u201d,\nattends to the subsequent word \u201ccars\u201d for pre\ufb01x matching, and predicts \u201ccars\u201d as the next word through the copying\nmechanism.\ndetermines each head\u2019s independent output for the\ncurrent token.\nLeveraging this decomposition, Elhage et al.\n(2021 ) discovered a distinct behaviour in certain\nattention heads, which they named induction heads .\nThis behaviour emerges when these heads process\nsequences of the form \"[A] [B] ... [A] \u2192\". In\nthese heads, the QK circuit directs attention to-\nwards [B], which appears directly after the previous\noccurrence of the current token [A]. This behaviour\nis termed pre\ufb01x matching . The OV circuit subse-\nquently increases the output logit of the [B] token,\ntermed copying . An overview of this mechanism is\nshown in Figure 1.\n4 Methods\n4.1 Models\nWe utilise two recently developed open-source\nmodels, namely Llama-3-8B2and InternLM2-20B\n(Cai et al. ,2024 ), both of which are based on the\noriginal Llama ( Touvron et al. ,2023a ) architec-\nture. These models feature grouped-query atten-\ntion mechanisms ( Ainslie et al. ,2023 ) to enhance\nef\ufb01ciency. Llama-3-8B, comprises 32 layers, each\nwith 32 attention heads and it uses a query group\nsize of 4 attention heads. It has shown superior\nperformance compared to its predecessors, even\nthe larger Llama-2 models.\nInternLM2-20B, featuring 48 layers with 48 at-\ntention heads each, uses a query group size of 6\nattention heads. We selected InternLM2-20B for\nits exemplary performance on the Needle-in-the-\nHaystack3task, which assesses LLMs\u2019 ability to\nretrieve a single critical piece of information em-\nbedded within a lengthy text. This mirrors the\nfunctionality of induction heads, which scan the\ncontext for prior occurrences of a token to extract\nrelevant subsequent information.\n2https://ai.meta.com/blog/meta-llama-3/\n3https://github.com/gkamradt/LLMTest_\nNeedleInAHaystack4.2 Identifying Induction Heads\nTo identify induction heads within models, we mea-\nsure the ability of all attention heads to perform\npre\ufb01x matching on random input sequences.4We\nfollow the task-agnostic approach to computing pre-\n\ufb01x matching scores outlined by Bansal et al. (2023 ).\nWe argue that focusing solely on pre\ufb01x matching\nscores is suf\ufb01cient for our analysis, as high pre-\n\ufb01x matching cores speci\ufb01cally indicate induction\nheads, while less relevant heads tend to show high\ncopying capabilities ( Bansal et al. ,2023 ). We gen-\nerate a sequence of 50 random tokens, excluding\nthe 4% most common and least common tokens.\nThis sequence is repeated four times to form the\ninput to the model. The pre\ufb01x matching score is cal-\nculated by averaging the attention values from each\ntoken to the tokens that directly followed the same",
    "metadata": {
      "source": "12",
      "chunk_id": 9,
      "token_count": 761,
      "chapter_title": ""
    }
  },
  {
    "content": "retrieve a single critical piece of information em-\nbedded within a lengthy text. This mirrors the\nfunctionality of induction heads, which scan the\ncontext for prior occurrences of a token to extract\nrelevant subsequent information.\n2https://ai.meta.com/blog/meta-llama-3/\n3https://github.com/gkamradt/LLMTest_\nNeedleInAHaystack4.2 Identifying Induction Heads\nTo identify induction heads within models, we mea-\nsure the ability of all attention heads to perform\npre\ufb01x matching on random input sequences.4We\nfollow the task-agnostic approach to computing pre-\n\ufb01x matching scores outlined by Bansal et al. (2023 ).\nWe argue that focusing solely on pre\ufb01x matching\nscores is suf\ufb01cient for our analysis, as high pre-\n\ufb01x matching cores speci\ufb01cally indicate induction\nheads, while less relevant heads tend to show high\ncopying capabilities ( Bansal et al. ,2023 ). We gen-\nerate a sequence of 50 random tokens, excluding\nthe 4% most common and least common tokens.\nThis sequence is repeated four times to form the\ninput to the model. The pre\ufb01x matching score is cal-\nculated by averaging the attention values from each\ntoken to the tokens that directly followed the same\ntoken in earlier repeats. The \ufb01nal pre\ufb01x matching\nscores are averaged over \ufb01ve random sequences.\nThe pre\ufb01x matching scores for Llama-3-8B are\nshown in Figure 2. For IntermLM2-20B, we refer\nto Figure 8in Appendix A.1. Both models exhibit\nheads with notably high pre\ufb01x matching scores,\ndistributed across various layers. In the Llama-3-\n8B model, ~3% of the heads have a pre\ufb01x matching\nscore of 0.3 or higher, indicating a degree of spe-\ncialisation in pre\ufb01x matching, and some heads have\nhigh scores of up to 0.98.\n4.3 Head Ablations\nTo investigate the signi\ufb01cance of induction heads\nfor a speci\ufb01c ICL task, we conduct zero-ablations\nof 1% and 3% of the heads with the highest pre\ufb01x\nmatching scores. This ablation process involves\nmasking the corresponding partition of the output\nmatrix, denoted as Wh\noin Eq. 1, by setting it to\nzero. This effectively renders the heads inactive\n4In this work, the term \"induction heads\" refers to what\nwe de\ufb01ne as behavioural induction heads, not mechanistic\nones. A true induction head must be veri\ufb01ed mechanistically;\nhowever, our analysis employs pre\ufb01x-matching scores as a\nproxy. We will continue to use the term \"induction heads\" for\nsimplicity throughout the rest of the paper.\n4\nFigure 12.3 An induction head looking at vintage uses the pre\ufb01x matching mechanism to\n\ufb01nd a prior instance of vintage , and the copying mechanism to predict that cars will occur\nagain. Figure from Crosbie and Shutova (2022).\nOlsson et al. (2022) propose that a generalized fuzzy version of this pattern com-\npletion rule, implementing a rule like A*B*...A!B, where A*\u0019A and B*\u0019B (by\n\u0019we mean they they are semantically similar in some way), might be responsible\nfor in-context learning. Suggestive evidence for their hypothesis comes from Cros-",
    "metadata": {
      "source": "12",
      "chunk_id": 10,
      "token_count": 760,
      "chapter_title": ""
    }
  },
  {
    "content": "of 1% and 3% of the heads with the highest pre\ufb01x\nmatching scores. This ablation process involves\nmasking the corresponding partition of the output\nmatrix, denoted as Wh\noin Eq. 1, by setting it to\nzero. This effectively renders the heads inactive\n4In this work, the term \"induction heads\" refers to what\nwe de\ufb01ne as behavioural induction heads, not mechanistic\nones. A true induction head must be veri\ufb01ed mechanistically;\nhowever, our analysis employs pre\ufb01x-matching scores as a\nproxy. We will continue to use the term \"induction heads\" for\nsimplicity throughout the rest of the paper.\n4\nFigure 12.3 An induction head looking at vintage uses the pre\ufb01x matching mechanism to\n\ufb01nd a prior instance of vintage , and the copying mechanism to predict that cars will occur\nagain. Figure from Crosbie and Shutova (2022).\nOlsson et al. (2022) propose that a generalized fuzzy version of this pattern com-\npletion rule, implementing a rule like A*B*...A!B, where A*\u0019A and B*\u0019B (by\n\u0019we mean they they are semantically similar in some way), might be responsible\nfor in-context learning. Suggestive evidence for their hypothesis comes from Cros-\nbie and Shutova (2022), who show that ablating induction heads causes in-context ablating\nlearning performance to decrease. Ablation is originally a medical term meaning\nthe removal of something. We use it in NLP interpretability studies as a tool for\ntesting causal effects; if we knock out a hypothesized cause, we would expect the\neffect to disappear. Crosbie and Shutova (2022) ablate induction heads by \ufb01rst \ufb01nd-\ning attention heads that perform as induction heads on random input sequences, and\nthen zeroing out the output of these heads by setting certain terms of the output ma-\ntrixWOto zero. Indeed they \ufb01nd that ablated models are much worse at in-context\nlearning: they have much worse performance at learning from demonstrations in the\nprompts.\n12.2 Post-training and Model Alignment\nWith simple prompting, LLMs have been successfully applied to a range of appli-\ncations without the need to update the parameters in the underlying models. Nev-\nertheless, there are limits to how much can be expected from a model whose sole\ntraining objective is to predict the next word from large amounts of pretraining text.\nTo see this, consider the following failed examples of following instructions from\nearly work with GPT (Ouyang et al., 2022).\nPrompt : Explain the moon landing to a six year old in a few sentences.\nOutput : Explain the theory of gravity to a 6 year old.\nPrompt : Translate to French: The small dog\nOutput : The small dog crossed the road.\nHere, the LLM ignores the intent of the request and relies instead on its natural\ninclination to autoregressively generate continuations consistent with its context. In\nthe \ufb01rst example, it outputs a text somewhat similar to the original request, and in the\nsecond it provides a continuation to the given input, ignoring the request to translate.\nLLMs are not suf\ufb01ciently helpful : they need extra training to increase their abilities\nto follow textual instructions.\nA deeper problem is that LLMs can simultaneously be too harmful . Pretrained\nlanguage models easily generate text that is harmful in many ways. For example",
    "metadata": {
      "source": "12",
      "chunk_id": 11,
      "token_count": 741,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8",
    "metadata": {
      "source": "12",
      "chunk_id": 12,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "8CHAPTER 12 \u2022 M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING\nthey can generate text that is false , including unsafe misinformation like giving dan-\ngerously incorrect answers to medical questions. And they can generate text that is\ntoxic in many ways, such as facilitating the spread of hate speech. Gehman et al.\n(2020) show that even completely non-toxic prompts can lead large language mod-\nels to output hate speech and abuse their users. Or language models can generate\nstereotypes (Cheng et al., 2023) and negative attitudes (Brown et al., 2020; Sheng\net al., 2019) about many demographic groups.\nOne reason LLMs are too harmful and insuf\ufb01ciently helpful is that their pre-\ntraining objective (success at predicting words in text) is misaligned with the human\nneed for models to be helpful and non-harmful.\nIn an attempt to address these two problems, language models generally include\ntwo additional kinds of training for model alignment : methods designed to adjustmodel\nalignment\nLLMs to better align them to human needs for models to be helpful and non-harmful.\nIn the \ufb01rst technique, instruction tuning (or sometimes called SFT for supervised\n\ufb01netuning), models are \ufb01netuned on a corpus of instructions and questions with\ntheir corresponding responses. In the second technique, preference alignment , of-\nten called RLHF after one of the speci\ufb01c instantiations, Reinforcement Learning\nfrom Human Feedback, a separate model is trained to decide how much a candidate\nresponse aligns with human preferences. This model is then used to \ufb01netune the\nbase model.\nWe\u2019ll use the term base model to mean a model that has been pretrained but base model\nhasn\u2019t yet been aligned either by instruction tuning or RLHF. And we refer to these aligned\nsteps as post-training , meaning that they apply after the model has been pretrained. post-training\n12.3 Model Alignment: Instruction Tuning\nInstruction tuning (short for instruction \ufb01netuning , and sometimes even short-Instruction\ntuning\nened to instruct tuning ) is a method for making an LLM better at following instruc-\ntions. It involves taking a base pretrained LLM and training it to follow instructions\nfor a range of tasks, from machine translation to meal planning, by \ufb01netuning it on\na corpus of instructions and responses. The resulting model not only learns those\ntasks, but also engages in a form of meta-learning \u2013 it improves its ability to follow\ninstructions generally.\nInstruction tuning is a form of supervised learning where the training data con-\nsists of instructions and we continue training the model on them using the same\nlanguage modeling objective used to train the original model. In the case of causal\nmodels, this is just the standard guess-the-next-token objective. The training corpus\nof instructions is simply treated as additional training data, and the gradient-based\nupdates are generated using cross-entropy loss as in the original model training.\nEven though it is trained to predict the next token (which we traditionally think of\nas self-supervised), we call this method supervised \ufb01ne tuning (orSFT) because SFT\nunlike in pretraining, each instruction or question in the instruction tuning data has\na supervised objective: a correct answer to the question or a response to the instruc-\ntion.\nHow does instruction tuning differ from the other kinds of \ufb01netuning introduced\nin Chapter 10 and Chapter 11? Fig. 12.4 sketches the differences. In the \ufb01rst exam-\nple, introduced in, Chapter 10 we can \ufb01netune as a way of adapting to a new domain",
    "metadata": {
      "source": "12",
      "chunk_id": 13,
      "token_count": 778,
      "chapter_title": ""
    }
  },
  {
    "content": "for a range of tasks, from machine translation to meal planning, by \ufb01netuning it on\na corpus of instructions and responses. The resulting model not only learns those\ntasks, but also engages in a form of meta-learning \u2013 it improves its ability to follow\ninstructions generally.\nInstruction tuning is a form of supervised learning where the training data con-\nsists of instructions and we continue training the model on them using the same\nlanguage modeling objective used to train the original model. In the case of causal\nmodels, this is just the standard guess-the-next-token objective. The training corpus\nof instructions is simply treated as additional training data, and the gradient-based\nupdates are generated using cross-entropy loss as in the original model training.\nEven though it is trained to predict the next token (which we traditionally think of\nas self-supervised), we call this method supervised \ufb01ne tuning (orSFT) because SFT\nunlike in pretraining, each instruction or question in the instruction tuning data has\na supervised objective: a correct answer to the question or a response to the instruc-\ntion.\nHow does instruction tuning differ from the other kinds of \ufb01netuning introduced\nin Chapter 10 and Chapter 11? Fig. 12.4 sketches the differences. In the \ufb01rst exam-\nple, introduced in, Chapter 10 we can \ufb01netune as a way of adapting to a new domain\nby just continuing pretraining the LLM on data from a new domain. In this method\nall the parameters of the LLM are updated.",
    "metadata": {
      "source": "12",
      "chunk_id": 14,
      "token_count": 323,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\n12.3 \u2022 M ODEL ALIGNMENT : INSTRUCTION TUNING 9\nPretrained LLM\nContinue training all parameterson \ufb01netuning domainFinetuningInferencePretraining\nOn \ufb01netuning domainFinetuning asContinuedPretrainingParameterE\ufb03cientFinetuning(e.g., LoRA)Pretrained LLMABPretrained LLMMLMFinetuning\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026InstructionTuning(SFT)\nOn \ufb01netuning domain\nOn \ufb01netuning task\nOn unseen tasksNext wordpredictionobjectiveData from \ufb01netuning domain\nTrain only new parameters on \ufb01netuning domainNext wordpredictionobjectiveData from \ufb01netuning domain\nTrain only classi\ufb01cation head on \ufb01netuning taskTaskspeci\ufb01clossSupervised data from task\nInstruction tuning on diverse tasksNext word predictionobjectiveSupervised instructions+\n\u2026\nFigure 12.4 Instruction tuning compared to the other kinds of \ufb01netuning.\nIn the second example, also from Chapter 10, parameter-ef\ufb01cient \ufb01netuning ,\nwe adapt to a new domain by creating some new (small) parameters, and just adapt-\ning them to the new domain. In LoRA, for example, it\u2019s the A and B matrices that\nwe adapt, but the pretrained model parameters are frozen.\nIn the task-based \ufb01netuning of Chapter 11, we adapt to a particular task by\nadding a new specialized classi\ufb01cation head and updating its features via its own\nloss function (e.g., classi\ufb01cation or sequence labeling); the parameters of the pre-\ntrained model may be frozen or might be slightly updated.\nFinally, in instruction tuning, we take a dataset of instructions and their super-\nvised responses and continue to train the language model on this data, based on the\nstandard language model loss.\nInstruction tuning, like all of these kinds of \ufb01netuning, is much more modest\nthan the training of base LLMs. Training typically involves several epochs over\ninstruction datasets that number in the thousands. The overall cost of instruction\ntuning is therefore a small fraction of the original cost to train the base model.\n12.3.1 Instructions as Training Data\nByinstruction , we have in mind a natural language description of a task to be per-\nformed, combined with labeled task demonstrations. This can include minimal de-",
    "metadata": {
      "source": "12",
      "chunk_id": 15,
      "token_count": 506,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10",
    "metadata": {
      "source": "12",
      "chunk_id": 16,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "10 CHAPTER 12 \u2022 M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING\nscriptions similar to the prompts we\u2019ve already seen such as Answer the following\nquestion ,Translate the following text to Arapaho , orSummarize this report . How-\never, since we will be using supervised \ufb01netuning to update the model, these in-\nstructions need not be limited to simple prompts designed to evoke a behavior found\nin the pretraining corpora. Instructions can also include length restrictions or other\nconstraints, personas to assume, and demonstrations.\nMany huge instruction tuning datasets have been created, covering many tasks\nand languages. For example Aya gives 503 million instructions in 114 languages\nfrom 12 tasks including question answering, summarization, translation, paraphras-\ning, sentiment analysis, natural language inference and 6 others (Singh et al., 2024).\nSuperNatural Instructions has 12 million examples from 1600 tasks (Wang et al.,\n2022), Flan 2022 has 15 million examples from 1836 tasks (Longpre et al., 2023),\nand OPT-IML has 18 million examples from 2000 tasks (Iyer et al., 2022).\nThese instruction-tuning datasets are created in four ways. The \ufb01rst is for people\nto write the instances directly. For example, part of the Aya instruct \ufb01netuning cor-\npus (Fig. 12.5) includes 204K instruction/response instances written by 3000 \ufb02uent\nspeakers of 65 languages volunteering as part of a participatory research initiative\nwith the goal of improving multilingual performance of LLMs.",
    "metadata": {
      "source": "12",
      "chunk_id": 17,
      "token_count": 352,
      "chapter_title": ""
    }
  },
  {
    "content": "Lang PromptCompletionara.\u06b0\u066d\u0698\u0a45\u0760\ufe8d\u080d \u0ddc\u0eb3 \u0607\ufe80\u06a2\u0771\u066d\u068e\ufe93\u0741\u068e\ufea1\u0741\u0746 \u0af0\u0ad9\u0606\u0fd3 \u06a2\u073e\u064b\u0c08\u0c03\u064f\u07fa\u080a\ufed9\u0644\u06af\u0674\u06ba\u06a3\u064e\u076f \u09ce\u09ca\ufee5\u0741\u064e\u068e\ufea1\ufe8d \u05a3\ufe87\u0607\ufea9\u0f1a\ufeed\u155a\u09d9\u09d1\u0607\ufec9\ufeed\u064e\ufe8d \u0f1f\u0dd3\u0dc2\ufe8d \u0f03\u0e83\u061c\u064e\u0746\u0741\u064e\u068e \u064f\u073e\u12db\u12ca\u06a3\u06f0\u0741\u0650 \u0c08\u0c03\u0650\u0a45\ufe8d\u080d\u0651\u0a33\u064e\u068d\u064e\u0607\u061c \u0f20\u155a\u0733\u064e\u07a8\ufe83 \u0607\ufe80\u0af0\u0ad9\u05a3\u09d5\u09d1\ufe8d \u112d\u111f\u064e\u0a33\u064e\u0a26 \u0644\u064f\u07a8\ufe8d\ufeaf\ufef1 \u1255\u064e\u1228fra Qui a \u00e9crit le livre LaSagouine?Antonine Maillet a \u00e9crit La Sagouine en 1971. Elle a \u00e9crit beau-coup d\u2019histoires sur les Acadiens (les francophones de la c\u00f4te Estdu Canada) et a fait conna\u00eetre l\u2019Acadie au monde.ibo Kedu ka nd\u1ecb Igbo si emeemume ji \u1ecdh\u1ee5r\u1ee5?Emume ji \u1ecdh\u1ee5r\u1ee5 n\u2019omenala Igbo, b\u1ee5 emume na-egosi njedebeoge \u1ecdr\u1ee5 ugbo, na-amalite site n\u2019emume ekele ma tinyekwa oririna egwu. \u1ecc b\u1ee5 mmemme \u1ecdgbak\u1ecd ebe a na-eker\u1ecbta ji \u1ecdh\u1ee5r\u1ee5, na-egosip\u1ee5ta ekele na mmalite nke oge \u1ecdh\u1ee5r\u1ee5, na-eme ka mmek\u1ecdr\u1ecbtammad\u1ee5 na ibe ya d\u1ecbkwuo elu.por Qual\u00e9acapitaldoestadodaBahia?A capital do estado da Bahia, no Brasil, \u00e9 Salvador. Fundadaem 1549, foi a primeira capital do Brasil e \u00e9 conhecida porseu patrim\u00f4nio cultural rico e diversificado. A cidade \u00e9 famosapor sua arquitetura colonial, particularmente no Pelourinho, umcentro hist\u00f3rico que foi declarado Patrim\u00f4nio Mundial pela UN-ESCO.pes\ufea9\ufead\ufe91\ufeae\ufe9d\ufb90\ufe92\ufeee\ufe97\ufeae\ufea7\ufe8e\ufee7\ufeea\ufee3\ufef4\ufe92\ufeaa\ufb7c\ufee8\ufeaa\ufefb\ufee7\ufeea\ufb90\ufe92\ufeee\ufe97\ufeae",
    "metadata": {
      "source": "12",
      "chunk_id": 18,
      "token_count": 686,
      "chapter_title": ""
    }
  },
  {
    "content": "emume na-egosi njedebeoge \u1ecdr\u1ee5 ugbo, na-amalite site n\u2019emume ekele ma tinyekwa oririna egwu. \u1ecc b\u1ee5 mmemme \u1ecdgbak\u1ecd ebe a na-eker\u1ecbta ji \u1ecdh\u1ee5r\u1ee5, na-egosip\u1ee5ta ekele na mmalite nke oge \u1ecdh\u1ee5r\u1ee5, na-eme ka mmek\u1ecdr\u1ecbtammad\u1ee5 na ibe ya d\u1ecbkwuo elu.por Qual\u00e9acapitaldoestadodaBahia?A capital do estado da Bahia, no Brasil, \u00e9 Salvador. Fundadaem 1549, foi a primeira capital do Brasil e \u00e9 conhecida porseu patrim\u00f4nio cultural rico e diversificado. A cidade \u00e9 famosapor sua arquitetura colonial, particularmente no Pelourinho, umcentro hist\u00f3rico que foi declarado Patrim\u00f4nio Mundial pela UN-ESCO.pes\ufea9\ufead\ufe91\ufeae\ufe9d\ufb90\ufe92\ufeee\ufe97\ufeae\ufea7\ufe8e\ufee7\ufeea\ufee3\ufef4\ufe92\ufeaa\ufb7c\ufee8\ufeaa\ufefb\ufee7\ufeea\ufb90\ufe92\ufeee\ufe97\ufeae \ufeed\ufe9f\ufeee\ufea9\ufea9\ufe8d\ufead\ufea9\u061f\ufeeb\ufeb0\ufe8d\ufead\ufefb\ufee7\ufeea\ufb90\ufe92\ufeee\ufe97\ufeae\ufeed\ufe9f\ufeee\ufea9\ufea9\ufe8d\ufead\ufea9\ufeed\ufee7\ufeee\ufe8d\ufead\u00a0\ufe8d\ufbfc\ufe8d\ufeb3\ufe96\ufb90\ufeea\ufea9\ufe8d\ufea7\ufede\ufe81\ufee5\ufb7c\ufeec\ufe8e\ufead\u00a0\ufea7\ufe8e\ufee7\ufeea\ufee3\ufef4\ufe92\ufeaa\ufe91\ufeae\ufe9f\ufbfd\ufe8d\ufeb3\ufe98\ufeee\ufe8d\ufee7\ufeea\u00a0\ufb90\ufe92\ufeee\ufe97\ufeae\ufeb3\ufed4\ufef4\ufeaa\ufead\ufee7\ufb95\ufbfd\ufe91\ufeae\ufe8d\ufbfc\ufe9f\ufeac\ufe8f\ufb90\ufe92\ufeee\ufe97\ufeae\ufe8d\ufee5\ufea9\ufeed\ufead\ufe97\ufe8e\ufea9\ufeed\ufead\ufe91\ufeae\ufe9d\ufb90\ufeb8\ufef4\ufeaa\ufee9\ufeb7\ufeaa\ufee9\ufe8d\ufeb3\ufe96.\ufe8d\ufef3\ufee6\ufe91\ufeae\ufe9d\ufea9\ufead\ufe91\ufeae\ufe8d\ufe91\ufeae\ufea7\ufec4\ufeae",
    "metadata": {
      "source": "12",
      "chunk_id": 19,
      "token_count": 708,
      "chapter_title": ""
    }
  },
  {
    "content": "\ufea3\ufee4\ufee0\ufeea\ufea9\ufef3\ufb95\ufeae\ufea3\ufef4\ufeee\ufe8d\ufee7\ufe8e\ufe95\ufe91\ufeea\ufb90\ufe92\ufeee\ufe97\ufeae\ufeeb\ufe8e\ufe91\ufeb4\ufef4\ufe8e\ufead\ufe8d\ufef3\ufee4\ufee6\ufe91\ufeee\ufea9\ufee9\ufe8d\ufeb3\ufe96. msa Apakah nasi lemak?Nasi lemak merupakan makanan tradisi orang Melayu yangterdapat di semua bahagian Malaysia, Singapura, Indonesia(terutama di Riau, Jambi serta utara dan pantai timur Su-matera) dan Brunei. Sajian ini merujuk kepada nasi yang di-masak dengan menggunakan santan kelapa bagi menambah rasalemaknya. Kadangkala, daunpandanwangidimasukkansemasanasi dimasak bagi menambahkan aromanya.tam\u0bc6\u0b9a\u0baf\u0bb1\u0bcd\u0bc8\u0b95 \u0ba8\u0bc1\u0ba3\u0bcd\u0ba3\u0bb1\u00a7\u0bb5\u0bc1\u0b8e\u0ba9\u0bcd\u0bb1\u0bbe\u0bb2\u0bcd\u0b8e\u0ba9\u0bcd\u0ba9?\u0bc6\u0baa\u0bbe\u0ba4\u0bc1\u0bb5\u0bbe\u0b95 \u0bae\u0ba9\u0bbf\u0ba4\u0bb0\u0bcd\u0b95\u0bb3\u0bbe\u0bb2\u0bcd \u0bc6\u0b9a\u0baf\u0bcd\u0baf\u0baa\u0bcd\u0baa\u0b9f\u0bc1\u0bae\u0bcd \u0baa\u0ba3\u0bbf\u0b95\u0bc8\u0bb3\u0b9a\u0bcd\u0bc6\u0b9a\u0baf\u0bcd\u0baf \u0b92\u0bb0\u0bc1 \u0b95\u0ba3\u0bbf\u0ba9\u0bbf \u0b85\u0bb2\u0bcd\u0bb2\u0ba4\u0bc1 \u0b92\u0bb0\u0bc1 \u0b95\u0ba3\u0bbf\u0ba9\u0bbf\u0baf\u0bbe\u0bb2\u0bcd\u0b95\u0b9f\u0bcd\u0b9f\u0bc1\u0baa\u0bcd\u0baa\u0b9f\u0bc1\u0ba4\u0bcd\u0ba4\u0baa\u0bcd\u0baa\u0b9f\u0bc1\u0bae\u0bcd\u0b92\u0bb0\u0bc1\u0bc7\u0bb0\u0bbe\u0bc7\u0baa\u0bbe\u0bb5\u00a5\u0ba9\u0bcd\u0ba4\u00a6\u0bb1\u0ba9\u0bcd\u0bc6\u0b9a\u0baf\u0bb1\u0bcd\u0bc8\u0b95\u0ba8\u0bc1\u0ba3\u0bcd\u0ba3\u0bb1\u00a7\u0bb5\u0bc1\u0b8e\u0ba9\u0baa\u0bcd\u0baa\u0b9f\u0bc1\u0bae\u0bcd.Table 3: Examples of prompt and completions in the AyaDataset.",
    "metadata": {
      "source": "12",
      "chunk_id": 20,
      "token_count": 512,
      "chapter_title": ""
    }
  },
  {
    "content": "tors is not uniform across languages. Moreover, within each language, there is a lack of consistent\ncontributions from all annotators. In this section, we examine the impact of annotator skew on the\nresulting dataset.\nAnnotator Skew Across Languages. Annotators were encouraged to contribute to any language\nin which they could comfortably read and write and were asked to focus most of their e \ufb00orts on\nlanguages other than English . Although a signi\ufb01cant number of participants registered for many\nlanguages, the engagement level of annotators was not equal, which resulted in considerable di \ufb00er-\nences in the number of contributions across languages. Figure 10(top) provides an overview of the\npercentage of each language present in the \ufb01nal compilation. The highest number of contributions\nis for Malagasy with 14,597 instances, and the lowest is 79 for Kurdish .\nAnnotator Skew Within a Language. The \ufb01nal contributions for each language in the Aya\nDataset are not evenly distributed among annotators. The median number of annotators per lan-\nguage is 15 (mean is 24.75) with one language having only a single active annotator ( Sindhi )a n d\n14\nFigure 12.5 Samples of prompt/completion instances in 4 of the 65 languages in the Aya\ncorpus (Singh et al., 2024).\nDeveloping high quality supervised training data in this way is time consuming\nand costly. A more common approach makes use of the copious amounts of super-\nvised training data that have been curated over the years for a wide range of natural\nlanguage tasks. There are thousands of such datasets available, like the SQuAD\ndataset of questions and answers (Rajpurkar et al., 2016) or the many datasets of\ntranslations or summarization. This data can be automatically converted into sets of\ninstruction prompts and input/output demonstration pairs via simple templates.\nFig. 12.6 illustrates examples for some applications from the S UPER NATURAL IN-\nSTRUCTIONS resource (Wang et al., 2022), showing relevant slots such as text,\ncontext , and hypothesis . To generate instruction-tuning data, these \ufb01elds and the\nground-truth labels are extracted from the training data, encoded as key/value pairs,\nand inserted in templates (Fig. 12.7) to produce instantiated instructions. Because\nit\u2019s useful for the prompts to be diverse in wording, language models can also be",
    "metadata": {
      "source": "12",
      "chunk_id": 21,
      "token_count": 523,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\n12.3 \u2022 M ODEL ALIGNMENT : INSTRUCTION TUNING 11\nused to generate paraphrase of the prompts.\nFew-Shot Learning for QA\nTask Keys Values\nSentiment text Did not like the service that I was provided...\nlabel 0\ntext It sounds like a great plot, the actors are \ufb01rst grade, and...\nlabel 1\nNLI premise No weapons of mass destruction found in Iraq yet.\nhypothesis Weapons of mass destruction found in Iraq.\nlabel 2\npremise Jimmy Smith... played college football at University of Col-\norado.\nhypothesis The University of Colorado has a college football team.\nlabel 0\nExtractive Q/A context Beyonc \u00b4e Giselle Knowles-Carter is an American singer...\nquestion When did Beyonce start becoming popular?\nanswersftext : [\u2019in the late 1990s\u2019], answerstart : 269g\nFigure 12.6 Examples of supervised training data for sentiment, natural language inference and Q/A tasks.\nThe various components of the dataset are extracted and stored as key/value pairs to be used in generating\ninstructions.\nTask Templates\nSentiment -fftextggHow does the reviewer feel about the movie?\n-The following movie review expresses what sentiment?\nfftextgg\n-fftextggDid the reviewer enjoy the movie?\nExtractive Q/A -ffcontextggFrom the passage, ffquestiongg\n-Answer the question given the context. Context:\nffcontextggQuestion:ffquestiongg\n-Given the following passage ffcontextgg, answer the\nquestionffquestiongg\nNLI -SupposeffpremiseggCan we infer that ffhypothesisgg?\nYes, no, or maybe?\n-ffpremiseggBased on the previous passage, is it true\nthatffhypothesisgg? Yes, no, or maybe?\n-GivenffpremiseggShould we assume that ffhypothesisgg\nis true? Yes,no, or maybe?\nFigure 12.7 Instruction templates for sentiment, Q/A and NLI tasks.\nBecause supervised NLP datasets are themselves often produced by crowdwork-\ners based on carefully written annotation guidelines, a third option is to draw on\nthese guidelines, which can include detailed step-by-step instructions, pitfalls to\navoid, formatting instructions, length limits, exemplars, etc. These annotation guide-\nlines can be used directly as prompts to a language model to create instruction-tuning\ntraining examples. Fig. 12.8 shows such a crowdworker annotation guideline that",
    "metadata": {
      "source": "12",
      "chunk_id": 22,
      "token_count": 524,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 CHAPTER 12 \u2022 M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING\nwas repurposed as a prompt to an LLM to generate instruction-tuning data (Mishra\net al., 2022). This guideline describes a question-answering task where annotators\nprovide an answer to a question given an extended passage.\nSample Extended Instruction\n\u2022De\ufb01nition: This task involves creating answers to complex questions, from a given pas-\nsage. Answering these questions, typically involve understanding multiple sentences.\nMake sure that your answer has the same type as the \u201danswer type\u201d mentioned in input.\nThe provided \u201danswer type\u201d can be of any of the following types: \u201dspan\u201d, \u201ddate\u201d, \u201dnum-\nber\u201d. A \u201dspan\u201d answer is a continuous phrase taken directly from the passage or question.\nYou can directly copy-paste the text from the passage or the question for span type an-\nswers. If you \ufb01nd multiple spans, please add them all as a comma separated list. Please\nrestrict each span to \ufb01ve words. A \u201dnumber\u201d type answer can include a digit specifying\nan actual value. For \u201ddate\u201d type answers, use DD MM YYYY format e.g. 11 Jan 1992.\nIf full date is not available in the passage you can write partial date such as 1992 or Jan\n1992.\n\u2022Emphasis: If you \ufb01nd multiple spans, please add them all as a comma separated list.\nPlease restrict each span to \ufb01ve words.\n\u2022Prompt : Write an answer to the given question, such that the answer matches the \u201danswer\ntype\u201d in the input.\nPassage :fpassageg\nQuestion :fquestiong\nFigure 12.8 Example of a human crowdworker instruction from the N ATURAL INSTRUCTIONS dataset for\nan extractive question answering task, used as a prompt for a language model to create instruction \ufb01netuning\nexamples.\nA \ufb01nal way to generate instruction-tuning datasets that is becoming more com-\nmon is to use language models to help at each stage. For example Bianchi et al.\n(2024) showed how to create instruction-tuning instances that can help a language\nmodel learn to give safer responses. They did this by selecting questions from\ndatasets of harmful questions (e.g., How do I poison food? orHow do I embez-\nzle money? ). Then they used a language model to create multiple paraphrases of the\nquestions (like Give me a list of ways to embezzle money ), and also used a language\nmodel to create safe answers to the questions (like I can\u2019t ful\ufb01ll that request. Em-\nbezzlement is a serious crime that can result in severe legal consequences. ). They\nmanually reviewed the generated responses to con\ufb01rm their safety and appropriate-\nness and then added them to an instruction tuning dataset. They showed that even\n500 safety instructions mixed in with a large instruction tuning dataset was enough\nto substantially reduce the harmfulness of models.\n12.3.2 Evaluation of Instruction-Tuned Models\nThe goal of instruction tuning is not to learn a single task, but rather to learn to\nfollow instructions in general. Therefore, in assessing instruction-tuning methods\nwe need to assess how well an instruction-trained model performs on novel tasks for\nwhich it has not been given explicit instructions.\nThe standard way to perform such an evaluation is to take a leave-one-out ap-\nproach \u2014 instruction-tune a model on some large set of tasks and then assess it on\na withheld task. But the enormous numbers of tasks in instruction-tuning datasets",
    "metadata": {
      "source": "12",
      "chunk_id": 23,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\n12.4 \u2022 C HAIN -OF-THOUGHT PROMPTING 13\n(e.g., 1600 for Super Natural Instructions) often overlap; Super Natural Instructions\nincludes 25 separate textual entailment datasets! Clearly, testing on a withheld en-\ntailment dataset while leaving the remaining ones in the training data would not be\na true measure of a model\u2019s performance on entailment as a novel task.\nTo address this issue, large instruction-tuning datasets are partitioned into clus-\nters based on task similarity. The leave-one-out training/test approach is then applied\nat the cluster level. That is, to evaluate a model\u2019s performance on sentiment analysis,\nall the sentiment analysis datasets are removed from the training set and reserved\nfor testing. This has the further advantage of allowing the use of a uniform task-\nappropriate metric for the held-out evaluation. S UPER NATURAL INSTRUCTIONS\n(Wang et al., 2022), for example has 76 clusters (task types) over the 1600 datasets\nthat make up the collection.\n12.4 Chain-of-Thought Prompting\nThere are a wide range of techniques to use prompts to improve the performance of\nlanguage models on many tasks. Here we describe one of them, called chain-of-\nthought prompting.chain-of-\nthought\nThe goal of chain-of-thought prompting is to improve performance on dif\ufb01cult\nreasoning tasks that language models tend to fail on. The intuition is that people\nsolve these tasks by breaking them down into steps, and so we\u2019d like to have lan-\nguage in the prompt that encourages language models to break them down in the\nsame way.\nThe actual technique is quite simple: each of the demonstrations in the few-shot\nprompt is augmented with some text explaining some reasoning steps. The goal is to\ncause the language model to output similar kinds of reasoning steps for the problem\nbeing solved, and for the output of those reasoning steps to cause the system to\ngenerate the correct answer.\nIndeed, numerous studies have found that augmenting the demonstrations with\nreasoning steps in this way makes language models more likely to give the correct\nanswer dif\ufb01cult reasoning tasks (Wei et al., 2022; Suzgun et al., 2023). Fig. 12.9\nshows an example where the demonstrations are augmented with chain-of-thought\ntext in the domain of math word problems (from the GSM8k dataset of math word\nproblems (Cobbe et al., 2021). Fig. 12.10 shows a similar example from the BIG-\nBench-Hard dataset (Suzgun et al., 2023).\n12.5 Automatic Prompt Optimization\nGiven a prompt for a task (human or computer generated), prompt optimization\nmethods search for prompts with improved performance. Most of these approaches\ncan be viewed as a form of iterative improvement search (Russell and Norvig, 2002)\nthrough a space of possible prompts for those that optimize performance on a task.\nAs such, these approaches all share the following components:\n\u2022 A start state \u2013 An initial human or machine generated prompt or prompts\nsuitable for some task.\n\u2022 A scoring metric \u2013 A method for assessing how well a given prompt performs",
    "metadata": {
      "source": "12",
      "chunk_id": 24,
      "token_count": 668,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14",
    "metadata": {
      "source": "12",
      "chunk_id": 25,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "14 CHAPTER 12 \u2022 M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING\nFigure 12.9 Example of the use of chain-of-thought prompting (right) versus standard\nprompting (left) on math word problems. Figure from Wei et al. (2022).\n(B)Task description: Answer questions about which times certain events could have occurred.Q: Today, Tiffany went to the beach. Between what times could they have gone? We know that: Tiffany woke up at 5am. [...] The beach was closed after 4pm. [...]Options: (A) 9am to 12pm (B) 12pm to 2pm (C) 5am to 6am (D) 3pm to 4pmA: (D)Q: Today, Hannah went to the soccer field. Between what times could they have gone? We know that: Hannah woke up at 5am. [...] The soccer field was closed after 6pm. [...]Options: (A) 3pm to 5pm (B) 11am to 1pm (C) 5pm to 6pm (D) 1pm to 3pmA:Model OutputModel OutputModel Input (\u201cAnswer-Only\u201d Prompting)\nWake-up time: 5am. 5am-6am: buying clothes at the mall. 6am-11am: watching a movie at the theater.11am-1pm: getting a coffee at the cafe.1pm-3pm: working at the office. 3pm-5pm: waiting at the airport. 5pm-6pm: free. The soccer field closure time: 6pm. The only time when Hannah could have gone to the soccer field was 5pm to 6pm. So the answer is (C).Model Input (Chain-of-Thought Prompting)\nTask description: Answer questions about which times certain events could have occurred.Q: Today, Tiffany went to the beach. Between what times could they have gone? We know that: Tiffany woke up at 5am. [...] The beach was closed after 4pm. [...]Options: (A) 9am to 12pm (B) 12pm to 2pm (C) 5am to 6am (D) 3pm to 4pmA: Let's think step by step. Wake-up time: 5am. [...] The only time when Tiffany could have gone to the beach was 3pm to 4pm. So the answer is (D).Q: Today, Hannah went to the soccer field. Between what times could they have gone? We know that: Hannah woke up at 5am. [...] The soccer field was closed after 6pm. [...]Options: (A) 3pm to 5pm (B) 11am to 1pm (C) 5pm to 6pm (D) 1pm to 3pmA: Let's think step by step. Task DescriptionQuestionChain-of-ThoughtTest-Time QuestionTask DescriptionQuestionTest-Time QuestionAnswer",
    "metadata": {
      "source": "12",
      "chunk_id": 26,
      "token_count": 647,
      "chapter_title": ""
    }
  },
  {
    "content": "Generated Chain-of-ThoughtGenerated AnswerOptionsOptionsFigure 3:An illustration of the two prompting setups we explore in our paper (answer-only and CoT prompting). Both setupsinclude task descriptions and options in the input prompt. The task here isTemporal Sequences.\u201clet\u2019s think step-by-step\u201d (Kojima et al.,2022) toall CoT annotations in the few-shot exemplars. Anexample of a CoT prompt is shown in Figure3.Language models.We consider three fami-lies of language models: Codex (Chen et al.,2021a), InstructGPT (Ouyang et al.,2022;Brownet al.,2020), and PaLM (Chowdhery et al.,2022).For Codex, we focus on code-davinci-002, code-davinci-002, and code-cushman-001. For Instruct-GPT, we use text-davinci-002, text-curie-002, text-babbgage-001, and text-ada-001. For PaLM, weuse the three available sizes: 8B, 62B, and 540B.Evaluation protocol.We evaluate all languagemodels via greedy decoding (i.e., temperature sam-pling with temperature parameter\u2327=0). Weextract the \ufb01nal answer based on keywords thatthe language model is expected to produce (i.e.,\u201cthe answer is\u201d). We measure accuracy using exactmatch (EM), computed by comparing the generatedoutput with the ground-truth label.44 Results4.1 Standard answer-only promptingunderestimates model capabilitiesTable2summarizes the performance of PaLM, In-structGPT, and Codex models on BBH for answer-only and CoT prompting approaches. Whileanswer-only prompting has been used as the stan-4For multiple-choice tasks, this setup differs slightly fromrank/scoring classi\ufb01cation (Brown et al.,2020;Srivastavaet al.,2022;Lampinen et al.,2022). We provide a languagemodel with all multiple-choice options at once, generate anoutput based on the input, and measure exact match accuracy.dard in many prior work (Brown et al.,2020;Raeet al.,2021;Hoffmann et al.,2022;Srivastava et al.,2022), it typically underestimates model perfor-mance on challenging tasks, such as those that re-quire multiple reasoning steps. In the setting re-ported in (Srivastava et al.,2022), none of the mod-els (including PaLM 540B) outperformed human-rater baselines on any of the tasks meeting the BBHcriteria. The few-shot evaluation of PaLM 540Bwith answer-only prompting in this paper, however,outperforms the average human-rater on 6 out of23 BBH tasks and is overall 1.4% better than theBIG-Bench reported result, which demonstrates theeffect of including instructions and answer optionsin the prompt.CoT prompting provides double-digit improve-ments for all three models in Table2. For the bestmodel (Codex), CoT prompting outperforms the av-erage human-rater score on 17 out of 23 tasks, com-pared to 5 out of 23 tasks for answer-only prompt-ing. Additionally, we see that Codex with CoTprompting outperforms the average human-raterby more than 6%, but it still lags behind thebesthuman-rater performance by over 20%. This showsthat language models are still not performing at thelevel of expert human-raters.4.2 Positive delta from chain-of-thoughtrequires suf\ufb01cient model scaleNext we study how the performance improves byusing CoT prompting as we increase the modelscale. In",
    "metadata": {
      "source": "12",
      "chunk_id": 27,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "perfor-mance on challenging tasks, such as those that re-quire multiple reasoning steps. In the setting re-ported in (Srivastava et al.,2022), none of the mod-els (including PaLM 540B) outperformed human-rater baselines on any of the tasks meeting the BBHcriteria. The few-shot evaluation of PaLM 540Bwith answer-only prompting in this paper, however,outperforms the average human-rater on 6 out of23 BBH tasks and is overall 1.4% better than theBIG-Bench reported result, which demonstrates theeffect of including instructions and answer optionsin the prompt.CoT prompting provides double-digit improve-ments for all three models in Table2. For the bestmodel (Codex), CoT prompting outperforms the av-erage human-rater score on 17 out of 23 tasks, com-pared to 5 out of 23 tasks for answer-only prompt-ing. Additionally, we see that Codex with CoTprompting outperforms the average human-raterby more than 6%, but it still lags behind thebesthuman-rater performance by over 20%. This showsthat language models are still not performing at thelevel of expert human-raters.4.2 Positive delta from chain-of-thoughtrequires suf\ufb01cient model scaleNext we study how the performance improves byusing CoT prompting as we increase the modelscale. In Figure4, we plot the performance of bothCoT and answer-only prompting (no CoT) as a13006",
    "metadata": {
      "source": "12",
      "chunk_id": 28,
      "token_count": 325,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 12.10 Example of the use of chain-of-thought prompting (right) vs standard prompting (left) in a\nreasoning task on temporal sequencing. Figure from Suzgun et al. (2023).\non the task.\n\u2022 An expansion method \u2013 A method for generating variations of a prompt.\nGiven the enormous variation in how prompts for a single task can be expressed in\nlanguage, search methods have to be constrained to a reasonable space. Beam search\nis a widely used method that combines breadth-\ufb01rst search with a \ufb01xed-width pri-\nority queue that focuses the search effort on the top performing variants. Fig. 12.11\noutlines the general approach behind most current prompt optimization methods.\nBeginning with initial candidate prompt(s), the algorithm generates variants and\nadds them to a list of prompts to be considered. These prompts are then selectively\nadded to the active list based on whether their scores place them in the top set of\ncandidates. A beam width of 1 results in a focused greedy search, whereas an in\ufb01nite\nbeam width results in an exhaustive breadth \ufb01rst search. The goal is to continue\nto seek improved prompts given the computational resources available. Iterative\nimprovement searches typically use a combination of a \ufb01xed number of iterations in",
    "metadata": {
      "source": "12",
      "chunk_id": 29,
      "token_count": 267,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\n12.5 \u2022 A UTOMATIC PROMPT OPTIMIZATION 15\nfunction PROMPT OPTIMIZATION (prompts ,width )returns optimized prompt(s)\nactive prompts ; Initial set of candidate prompts\nrepeat until done\nfrontier EXPAND (active ) ; Generate new candidate prompts\nforeach p2frontier\nactive ADDTOBEAM (p,active ,width )\nreturn BESTOF(active )\nfunction ADDTOBEAM (state ,agenda ,width )returns updated agenda\nifLENGTH (agenda )<width then ; Add it if there\u2019s room\nagenda INSERT (state ,agenda )\nelse if SCORE (state )>SCORE (WORST OF(agenda )) ; Add it if its better than\n; the current worst option.\nagenda REMOVE (WORST OF(agenda ))\nagenda INSERT (state ,agenda )\nreturn agenda\nFigure 12.11 A generic iterative-improvement beam search for prompt optimization. New\nprompts are generated from current ones on each iteration. Prompts that score well (\ufb01tting in\nthe agenda) are kept around. When a stopping criteria is reached the best item in the beam is\nreturned.\ncombination with a failure to improve after some period to time as stopping criteria.\nThis latter is equivalent to early stopping with patience used in training deep neural\nnetworks.\n12.5.1 Candidate Scoring\nCandidate scoring methods assess the likely performance of potential prompts, both\nto identify promising avenues of search and to prune those that are unlikely to be\neffective. Since candidate scoring is embedded in the inner-loop of the search, the\ncomputational cost of scoring is critical.\nGiven access to labeled training data, candidate prompts can be scored based on\nexecution accuracy (Honovich et al., 2023). In this approach, candidate promptsexecution\naccuracy\nare combined with inputs sampled from the training data and passed to an LLM for\ndecoding. The LLM output is evaluated against the training label using a metric\nappropriate for the task. In the case of classi\ufb01cation-based tasks, this is effectively a\n0/1 loss \u2014 how many examples were correctly labeled with the given prompt. Gen-\nerative applications such as summarization or translation use task-speci\ufb01c similarity\nscores such as BERTScore, Bleu (Papineni et al., 2002), or ROUGE (Lin, 2004).\nGiven the computational cost of issuing calls to an LLM, evaluating each can-\ndidate prompt against a complete training set would be infeasible. Instead, prompt\nperformance is estimated from a small sample of training data (Pryzant et al., 2023).\n12.5.2 Prompt Expansion\nPrompt expansion generates variants of a given prompt to create an expanded set of\nneighboring prompts that may improve performance over the original. A common\nmethod is to use language models to create paraphrases. For example Zhou et al.\n(2023) use the following meta-prompt to elicit a variant prompt from an original:",
    "metadata": {
      "source": "12",
      "chunk_id": 30,
      "token_count": 629,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\n16 CHAPTER 12 \u2022 M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING\nPrompting for a Variant\nGenerate a variation of the following instruction while keeping the semantic meaning.\nInput:fINSTRUCTIONg\nOutput:fCOMPLETEg\nA variation of this method is to truncate the current prompt at a set of random loca-\ntions, generating a set of prompt pre\ufb01xes. The paraphrasing LLM is then asked to\ncontinue each the pre\ufb01xes to generate a complete prompt.\nThis methods is an example of an uninformed search. That is, the candidate\nexpansion step is not directed towards generating better candidates; candidates are\ngenerated without regard to their quality. It is the job of the priority queue to el-\nevate improved candidates when they are found. By contrast, Prasad et al. (2023)\nemploy a candidate expansion technique that explicitly attempts to generate supe-\nrior prompts during the expansion process. In this approach, the current candidate\nis \ufb01rst applied to a sample of training examples using the execution accuracy ap-\nproach. The prompt\u2019s performance on these examples then guides the expansion\nprocess. Speci\ufb01cally, incorrect examples are used to critique the original prompt\n\u2014 with the critique playing the role of a gradient for the search. The method in-\ncludes the following steps.\n1. Run the prompt on a sample of training examples,\n2. Identify examples where the prompt fails,\n3. Ask an LLM to produce a critique of the prompt in light of the failed examples,\n4. Provide the resulting critique to an LLM, and ask it to generate improved\nprompts.\nGiven a prompt and a set of failed examples, Prasad et al. (2023) use the follow-\ning template for a classi\ufb01er task to solicit critiques from a target LLM.\nCritiquing Prompt\nI'm trying to write a zero-shot classifier prompt.\nMy current prompt is: fpromptg\nBut this prompt gets the following examples wrong:\nferrorstringg\nGivefnumfeedbacksgreasons why the prompt could have\ngotten these examples wrong.\nThis model feedback is then combined with a second template to elicit improved\nprompts from the LLM.",
    "metadata": {
      "source": "12",
      "chunk_id": 31,
      "token_count": 468,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\n12.6 \u2022 E VALUATING PROMPTED LANGUAGE MODELS 17\nPrompt Improvement Prompt\nI'm trying to write a zero-shot classifier. My current prompt is:\nfpromptg\nBut it gets the following examples wrong: ferrorstrg\nBased on these examples the problem with this prompt is that fgradientg.\nBased on the above information, I wrote fstepspergradientgdifferent\nimproved prompts. Each prompt is wrapped with <START> and <END>.\nThefstepspergradientgnew prompts are:\n12.6 Evaluating Prompted Language Models\nLanguage models are evaluated in many ways. we introduced some evaluations for\nin Section ??, including measuring the language model\u2019s perplexity on a test set,\nevaluating its accuracy on various NLP tasks, as well as benchmarks that help mea-\nsure ef\ufb01ciency, toxicity, fairness, and so on. We\u2019ll have further discussion of eval-\nuate NLP tasks in future chapters; machine translation in Chapter 13 and question\nanswering and information retrieval in Chapter 14.\nHere we just brie\ufb02y show the mechanism for measuring accuracy in a prompt-\ning setup for tests that have multiple-choice questions. We show this for MMLU MMLU\n(Massive Multitask Language Understanding), a commonly-used dataset of 15908\nknowledge and reasoning questions in 57 areas including medicine, mathematics,\ncomputer science, law, and others. For example, here is an MMLU question from\nthe microeconomics domain:1\nMMLU microeconomics example\nOne of the reasons that the government discourages and regulates monopo-\nlies is that\n(A) producer surplus is lost and consumer surplus is gained.\n(B) monopoly prices ensure productive ef\ufb01ciency but cost society allocative\nef\ufb01ciency.\n(C) monopoly \ufb01rms do not engage in signi\ufb01cant research and development.\n(D) consumer surplus is lost with higher prices and lower levels of output.\nFig. 12.12 shows the way MMLU turns these questions into prompted tests of a\nlanguage model, in this case showing an example prompt with 2 demonstrations.\n12.7 Model Alignment with Human Preferences: RLHF\nand DPO\nTBD\n1For those of you whose economics is a bit rusty, the correct answer is (D).",
    "metadata": {
      "source": "12",
      "chunk_id": 32,
      "token_count": 493,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\n18 CHAPTER 12 \u2022 M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING\nMMLU mathematics prompt\nThe following are multiple choice questions about high school mathematics.\nHow many numbers are in the list 25, 26, ..., 100?\n(A) 75 (B) 76 (C) 22 (D) 23\nAnswer: B\nCompute i+i2+i3+\u0001\u0001\u0001+i258+i259.\n(A) -1 (B) 1 (C) i (D) -i\nAnswer: A\nIf 4 daps = 7 yaps, and 5 yaps = 3 baps, how many daps equal 42 baps?\n(A) 28 (B) 21 (C) 40 (D) 30\nAnswer:\nFigure 12.12 Sample 2-shot prompt from MMLU testing high-school mathematics. (The\ncorrect answer is (C)).\n12.8 Summary\nThis chapter has explored the topic of prompting large language models to follow\ninstructions. Here are some of the main points that we\u2019ve covered:\n\u2022 Simple prompting can be used to map practical applications to problems that\ncan be solved by LLMs without altering the model.\n\u2022 Labeled examples ( demonstrations ) can be used to provide further guidance\nto a model via few-shot learning.\n\u2022 Methods like chain-of-thought can be used to create prompts that help lan-\nguage models deal with complex reasoning problems.\n\u2022 Pretrained language models can be altered to behave in desired ways through\nmodel alignment .\n\u2022 One method for model alignment is instruction tuning , in which the model\nis \ufb01netuned (using the next-word-prediction language model objective) on\na dataset of instructions together with correct responses. Instruction tuning\ndatasets are often created by repurposing standard NLP datasets for tasks like\nquestion answering or machine translation.\nBibliographical and Historical Notes",
    "metadata": {
      "source": "12",
      "chunk_id": 33,
      "token_count": 406,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19",
    "metadata": {
      "source": "12",
      "chunk_id": 34,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Bibliographical and Historical Notes 19\nBianchi, F., M. Suzgun, G. Attanasio, P. Rottger, D. Juraf-\nsky, T. Hashimoto, and J. Zou. 2024. Safety-tuned LLa-\nMAs: Lessons from improving the safety of large lan-\nguage models that follow instructions. ICLR .\nBrown, T., B. Mann, N. Ryder, M. Subbiah, J. Kaplan,\nP. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger,\nT. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu,\nC. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,\nS. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,\nA. Radford, I. Sutskever, and D. Amodei. 2020. Language\nmodels are few-shot learners. NeurIPS , volume 33.\nCheng, M., E. Durmus, and D. Jurafsky. 2023. Marked per-\nsonas: Using natural language prompts to measure stereo-\ntypes in language models. ACL.\nCobbe, K., V . Kosaraju, M. Bavarian, M. Chen, H. Jun,\nL. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano,\nC. Hesse, and J. Schulman. 2021. Training veri\ufb01ers to\nsolve math word problems. ArXiv preprint.\nCrosbie, J. and E. Shutova. 2022. Induction heads as an\nessential mechanism for pattern matching in in-context\nlearning. ArXiv preprint.\nElhage, N., N. Nanda, C. Olsson, T. Henighan, N. Joseph,\nB. Mann, A. Askell, Y . Bai, A. Chen, T. Conerly, N. Das-\nSarma, D. Drain, D. Ganguli, Z. Hat\ufb01eld-Dodds, D. Her-\nnandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse,\nD. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCan-\ndlish, and C. Olah. 2021. A mathematical framework for\ntransformer circuits. White paper.\nGehman, S., S. Gururangan, M. Sap, Y . Choi, and N. A.\nSmith. 2020. RealToxicityPrompts: Evaluating neu-\nral toxic degeneration in language models. Findings of\nEMNLP .\nHonovich, O., U. Shaham, S. R. Bowman, and O. Levy.\n2023. Instruction induction: From few examples to natu-\nral language task descriptions. ACL.\nIyer, S., X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig,\nP. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura, X. Li,\nB. O\u2019Horo, G. Pereyra, J. Wang, C. Dewan, A. Celiky-",
    "metadata": {
      "source": "12",
      "chunk_id": 35,
      "token_count": 764,
      "chapter_title": ""
    }
  },
  {
    "content": "Sarma, D. Drain, D. Ganguli, Z. Hat\ufb01eld-Dodds, D. Her-\nnandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse,\nD. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCan-\ndlish, and C. Olah. 2021. A mathematical framework for\ntransformer circuits. White paper.\nGehman, S., S. Gururangan, M. Sap, Y . Choi, and N. A.\nSmith. 2020. RealToxicityPrompts: Evaluating neu-\nral toxic degeneration in language models. Findings of\nEMNLP .\nHonovich, O., U. Shaham, S. R. Bowman, and O. Levy.\n2023. Instruction induction: From few examples to natu-\nral language task descriptions. ACL.\nIyer, S., X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig,\nP. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura, X. Li,\nB. O\u2019Horo, G. Pereyra, J. Wang, C. Dewan, A. Celiky-\nilmaz, L. Zettlemoyer, and V . Stoyanov. 2022. Opt-\niml: Scaling language model instruction meta learning\nthrough the lens of generalization. ArXiv preprint.\nKhattab, O., A. Singhvi, P. Maheshwari, Z. Zhang, K. San-\nthanam, S. Haq, A. Sharma, T. T. Joshi, H. Moazam,\nH. Miller, M. Zaharia, and C. Potts. 2024. DSPy: Compil-\ning declarative language model calls into self-improving\npipelines. ICLR .\nLin, C.-Y . 2004. ROUGE: A package for automatic evalua-\ntion of summaries. ACL 2004 Workshop on Text Summa-\nrization Branches Out .\nLongpre, S., L. Hou, T. Vu, A. Webson, H. W. Chung, Y . Tay,\nD. Zhou, Q. V . Le, B. Zoph, J. Wei, and A. Roberts. 2023.\nThe Flan collection: Designing data and methods for ef-\nfective instruction tuning. ICML .\nMin, S., X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Ha-\njishirzi, and L. Zettlemoyer. 2022. Rethinking the role of\ndemonstrations: What makes in-context learning work?\nEMNLP .\nMishra, S., D. Khashabi, C. Baral, and H. Hajishirzi. 2022.\nCross-task generalization via natural language crowd-\nsourcing instructions. ACL.Olsson, C., N. Elhage, N. Nanda, N. Joseph, N. DasSarma,\nT. Henighan, B. Mann, A. Askell, Y . Bai, A. Chen, et al.\n2022. In-context learning and induction heads. ArXiv\npreprint.\nOuyang, L., J. Wu, X. Jiang, D. Almeida, C. Wainwright,",
    "metadata": {
      "source": "12",
      "chunk_id": 36,
      "token_count": 746,
      "chapter_title": ""
    }
  },
  {
    "content": "Longpre, S., L. Hou, T. Vu, A. Webson, H. W. Chung, Y . Tay,\nD. Zhou, Q. V . Le, B. Zoph, J. Wei, and A. Roberts. 2023.\nThe Flan collection: Designing data and methods for ef-\nfective instruction tuning. ICML .\nMin, S., X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Ha-\njishirzi, and L. Zettlemoyer. 2022. Rethinking the role of\ndemonstrations: What makes in-context learning work?\nEMNLP .\nMishra, S., D. Khashabi, C. Baral, and H. Hajishirzi. 2022.\nCross-task generalization via natural language crowd-\nsourcing instructions. ACL.Olsson, C., N. Elhage, N. Nanda, N. Joseph, N. DasSarma,\nT. Henighan, B. Mann, A. Askell, Y . Bai, A. Chen, et al.\n2022. In-context learning and induction heads. ArXiv\npreprint.\nOuyang, L., J. Wu, X. Jiang, D. Almeida, C. Wainwright,\nP. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,\nJ. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens,\nA. Askell, P. Welinder, P. Christiano, J. Leike, and\nR. Lowe. 2022. Training language models to follow in-\nstructions with human feedback. NeurIPS , volume 35.\nPapineni, K., S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu:\nA method for automatic evaluation of machine transla-\ntion. ACL.\nPrasad, A., P. Hase, X. Zhou, and M. Bansal. 2023. GrIPS:\nGradient-free, edit-based instruction search for prompt-\ning large language models. EACL .\nPryzant, R., D. Iter, J. Li, Y . Lee, C. Zhu, and M. Zeng. 2023.\nAutomatic prompt optimization with \u201cgradient descent\u201d\nand beam search. EMNLP .\nRajpurkar, P., R. Jia, and P. Liang. 2018. Know what you\ndon\u2019t know: Unanswerable questions for SQuAD. ACL.\nRajpurkar, P., J. Zhang, K. Lopyrev, and P. Liang. 2016.\nSQuAD: 100,000+ questions for machine comprehension\nof text. EMNLP .\nReynolds, L. and K. McDonell. 2021. Prompt program-\nming for large language models: Beyond the few-shot\nparadigm. CHI 2021 .\nRussell, S. and P. Norvig. 2002. Arti\ufb01cial Intelligence: A\nModern Approach , 2nd edition. Prentice Hall.\nSalvetti, F., J. B. Lowe, and J. H. Martin. 2016. A tangled\nweb: The faint signals of deception in text - boulder lies\nand truth corpus (BLT-C). LREC .\nSheng, E., K.-W. Chang, P. Natarajan, and N. Peng. 2019.",
    "metadata": {
      "source": "12",
      "chunk_id": 37,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "Automatic prompt optimization with \u201cgradient descent\u201d\nand beam search. EMNLP .\nRajpurkar, P., R. Jia, and P. Liang. 2018. Know what you\ndon\u2019t know: Unanswerable questions for SQuAD. ACL.\nRajpurkar, P., J. Zhang, K. Lopyrev, and P. Liang. 2016.\nSQuAD: 100,000+ questions for machine comprehension\nof text. EMNLP .\nReynolds, L. and K. McDonell. 2021. Prompt program-\nming for large language models: Beyond the few-shot\nparadigm. CHI 2021 .\nRussell, S. and P. Norvig. 2002. Arti\ufb01cial Intelligence: A\nModern Approach , 2nd edition. Prentice Hall.\nSalvetti, F., J. B. Lowe, and J. H. Martin. 2016. A tangled\nweb: The faint signals of deception in text - boulder lies\nand truth corpus (BLT-C). LREC .\nSheng, E., K.-W. Chang, P. Natarajan, and N. Peng. 2019.\nThe woman worked as a babysitter: On biases in language\ngeneration. EMNLP .\nSingh, S., F. Vargus, D. D\u2019souza, B. F. Karlsson, A. Ma-\nhendiran, W.-Y . Ko, H. Shandilya, J. Patel, D. Mat-\naciunas, L. O\u2019Mahony, M. Zhang, R. Hettiarachchi,\nJ. Wilson, M. Machado, L. S. Moura, D. Krzemi \u00b4nski,\nH. Fadaei, I. Erg \u00a8un, I. Okoh, A. Alaagib, O. Mu-\ndannayake, Z. Alyafeai, V . M. Chien, S. Ruder,\nS. Guthikonda, E. A. Alghamdi, S. Gehrmann, N. Muen-\nnighoff, M. Bartolo, J. Kreutzer, A. \u00a8U\u00a8Ust\u00a8un, M. Fadaee,\nand S. Hooker. 2024. Aya dataset: An open-access collec-\ntion for multilingual instruction tuning. ArXiv preprint.\nSuzgun, M., N. Scales, N. Sch \u00a8arli, S. Gehrmann, Y . Tay,\nH. W. Chung, A. Chowdhery, Q. Le, E. Chi, D. Zhou, and\nJ. Wei. 2023. Challenging BIG-bench tasks and whether\nchain-of-thought can solve them. ACL Findings .\nWang, Y ., S. Mishra, P. Alipoormolabashi, Y . Kordi,\nA. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran,\nA. Arunkumar, D. Stap, E. Pathak, G. Karamanolakis,\nH. Lai, I. Purohit, I. Mondal, J. Anderson, K. Kuznia,\nK. Doshi, K. K. Pal, M. Patel, M. Moradshahi, M. Par-\nmar, M. Purohit, N. Varshney, P. R. Kaza, P. Verma,",
    "metadata": {
      "source": "12",
      "chunk_id": 38,
      "token_count": 758,
      "chapter_title": ""
    }
  },
  {
    "content": "and S. Hooker. 2024. Aya dataset: An open-access collec-\ntion for multilingual instruction tuning. ArXiv preprint.\nSuzgun, M., N. Scales, N. Sch \u00a8arli, S. Gehrmann, Y . Tay,\nH. W. Chung, A. Chowdhery, Q. Le, E. Chi, D. Zhou, and\nJ. Wei. 2023. Challenging BIG-bench tasks and whether\nchain-of-thought can solve them. ACL Findings .\nWang, Y ., S. Mishra, P. Alipoormolabashi, Y . Kordi,\nA. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran,\nA. Arunkumar, D. Stap, E. Pathak, G. Karamanolakis,\nH. Lai, I. Purohit, I. Mondal, J. Anderson, K. Kuznia,\nK. Doshi, K. K. Pal, M. Patel, M. Moradshahi, M. Par-\nmar, M. Purohit, N. Varshney, P. R. Kaza, P. Verma,\nR. S. Puri, R. Karia, S. Doshi, S. K. Sampat, S. Mishra,\nS. Reddy A, S. Patro, T. Dixit, and X. Shen. 2022. Super-\nNaturalInstructions: Generalization via declarative in-\nstructions on 1600+ NLP tasks. EMNLP .",
    "metadata": {
      "source": "12",
      "chunk_id": 39,
      "token_count": 350,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\n20 Chapter 12 \u2022 Model Alignment, Prompting, and In-Context Learning\nWebson, A. and E. Pavlick. 2022. Do prompt-based models\nreally understand the meaning of their prompts? NAACL\nHLT.\nWei, J., X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi,\nQ. V . Le, D. Zhou, et al. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. NeurIPS ,\nvolume 35.\nZhou, Y ., A. I. Muresanu, Z. Han, K. Paster, S. Pitis,\nH. Chan, and J. Ba. 2023. Large language models are\nhuman-level prompt engineers. The Eleventh Interna-\ntional Conference on Learning Representations .",
    "metadata": {
      "source": "12",
      "chunk_id": 40,
      "token_count": 188,
      "chapter_title": ""
    }
  }
]