[
  {
    "content": "# B\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\nBSpelling Correction and the\nNoisy Channel\nALGERNON : But my own sweet Cecily, I have never written you any letters.\nCECILY : You need hardly remind me of that, Ernest. I remember only too well\nthat I was forced to write your letters for you. I wrote always three times a week,\nand sometimes oftener.\nALGERNON : Oh, do let me read them, Cecily?\nCECILY : Oh, I couldn\u2019t possibly. They would make you far too conceited. The\nthree you wrote me after I had broken off the engagement are so beautiful, and\nso badly spelled, that even now I can hardly read them without crying a little.\nOscar Wilde, The Importance of Being Earnest\nLike Oscar Wilde\u2019s fabulous Cecily, a lot of people were thinking about spelling\nduring the last turn of the century. Gilbert and Sullivan provide many examples. The\nGondoliers \u2019 Giuseppe, for example, worries that his private secretary is \u201cshaky in his\nspelling\u201d, while Iolanthe \u2019s Phyllis can \u201cspell every word that she uses\u201d. Thorstein\nVeblen\u2019s explanation (in his 1899 classic The Theory of the Leisure Class ) was that\na main purpose of the \u201carchaic, cumbrous, and ineffective\u201d English spelling system\nwas to be dif\ufb01cult enough to provide a test of membership in the leisure class.\nWhatever the social role of spelling, we can certainly agree that many more of\nus are like Cecily than like Phyllis. Estimates for the frequency of spelling errors\nin human-typed text vary from 1-2% for carefully retyping already printed text to\n10-15% for web queries.\nIn this chapter we introduce the problem of detecting and correcting spelling\nerrors. Fixing spelling errors is an integral part of writing in the modern world,\nwhether this writing is part of texting on a phone, sending email, writing longer\ndocuments, or \ufb01nding information on the web. Modern spell correctors aren\u2019t perfect\n(indeed, autocorrect-gone-wrong is a popular source of amusement on the web) but\nthey are ubiquitous in pretty much any software that relies on keyboard input.\nSpelling correction is often considered from two perspectives. Non-word spelling\ncorrection is the detection and correction of spelling errors that result in non-words\n(like graffe forgiraffe ). By contrast, real word spelling correction is the task of\ndetecting and correcting spelling errors even if they accidentally result in an actual\nword of English ( real-word errors ). This can happen from typographical errorsreal-word\nerrors\n(insertion, deletion, transposition) that accidentally produce a real word (e.g., there\nforthree ), orcognitive errors where the writer substituted the wrong spelling of a\nhomophone or near-homophone (e.g., dessert fordesert , orpiece forpeace ).\nNon-word errors are detected by looking for any word not found in a dictio-\nnary. For example, the misspelling graffe above would not occur in a dictionary.\nThe larger the dictionary the better; modern systems often use enormous dictio-\nnaries derived from the web. To correct non-word spelling errors we \ufb01rst generate",
    "metadata": {
      "source": "B",
      "chunk_id": 0,
      "token_count": 726,
      "chapter_title": "B"
    }
  },
  {
    "content": "## Page 2\n\n2APPENDIX B \u2022 S PELLING CORRECTION AND THE NOISY CHANNEL\ncandidates : real words that have a similar letter sequence to the error. Candidate candidates\ncorrections from the spelling error graffe might include giraffe ,graf,gaffe ,grail , or\ncraft. We then rank the candidates using a distance metric between the source and\nthe surface error. We\u2019d like a metric that shares our intuition that giraffe is a more\nlikely source than grail forgraffe because giraffe is closer in spelling to graffe than\ngrail is to graffe . The minimum edit distance algorithm from Chapter 2 will play a\nrole here. But we\u2019d also like to prefer corrections that are more frequent words, or\nmore likely to occur in the context of the error. The noisy channel model introduced\nin the next section offers a way to formalize this intuition.\nReal word spelling error detection is a much more dif\ufb01cult task, since any word\nin the input text could be an error. Still, it is possible to use the noisy channel to \ufb01nd\ncandidates for each word wtyped by the user, and rank the correction that is most\nlikely to have been the user\u2019s original intention.\nB.1 The Noisy Channel Model\nIn this section we introduce the noisy channel model and show how to apply it to\nthe task of detecting and correcting spelling errors. The noisy channel model was\napplied to the spelling correction task at about the same time by researchers at AT&T\nBell Laboratories (Kernighan et al. 1990, Church and Gale 1991) and IBM Watson\nResearch (Mays et al., 1991).\ndecoder noisy wordoriginal wordnoisy channelguessed wordnoisy 1noisy 2noisy Nword hyp1word hyp2...word hyp3\nFigure B.1 In the noisy channel model, we imagine that the surface form we see is actually\na \u201cdistorted\u201d form of an original word passed through a noisy channel. The decoder passes\neach hypothesis through a model of this channel and picks the word that best matches the\nsurface noisy word.\nThe intuition of the noisy channel model (see Fig. B.1) is to treat the misspelled noisy channel\nword as if a correctly spelled word had been \u201cdistorted\u201d by being passed through a\nnoisy communication channel.\nThis channel introduces \u201cnoise\u201d in the form of substitutions or other changes to\nthe letters, making it hard to recognize the \u201ctrue\u201d word. Our goal, then, is to build a\nmodel of the channel. Given this model, we then \ufb01nd the true word by passing every\nword of the language through our model of the noisy channel and seeing which one\ncomes the closest to the misspelled word.\nThis noisy channel model is a kind of Bayesian inference . We see an obser- Bayesian",
    "metadata": {
      "source": "B",
      "chunk_id": 1,
      "token_count": 604,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\nB.1 \u2022 T HENOISY CHANNEL MODEL 3\nvation x(a misspelled word) and our job is to \ufb01nd the word wthat generated this\nmisspelled word. Out of all possible words in the vocabulary Vwe want to \ufb01nd the\nword wsuch that P(wjx)is highest. We use the hat notation \u02c6 to mean \u201cour estimate\nof the correct word\u201d.\n\u02c6w=argmax\nw2VP(wjx) (B.1)\nThe function argmax xf(x)means \u201cthe xsuch that f(x)is maximized\u201d. Equa- argmax\ntion B.1 thus means, that out of all words in the vocabulary, we want the particular\nword that maximizes the right-hand side P(wjx).\nThe intuition of Bayesian classi\ufb01cation is to use Bayes\u2019 rule to transform Eq. B.1\ninto a set of other probabilities. Bayes\u2019 rule is presented in Eq. B.2; it gives us a way\nto break down any conditional probability P(ajb)into three other probabilities:\nP(ajb) =P(bja)P(a)\nP(b)(B.2)\nWe can then substitute Eq. B.2 into Eq. B.1 to get Eq. B.3:\n\u02c6w=argmax\nw2VP(xjw)P(w)\nP(x)(B.3)\nWe can conveniently simplify Eq. B.3 by dropping the denominator P(x). Why\nis that? Since we are choosing a potential correction word out of all words, we will\nbe computingP(xjw)P(w)\nP(x)for each word. But P(x)doesn\u2019t change for each word; we\nare always asking about the most likely word for the same observed error x, which\nmust have the same probability P(x). Thus, we can choose the word that maximizes\nthis simpler formula:\n\u02c6w=argmax\nw2VP(xjw)P(w) (B.4)\nTo summarize, the noisy channel model says that we have some true underlying\nword w, and we have a noisy channel that modi\ufb01es the word into some possible\nmisspelled observed surface form. The likelihood orchannel model of the noisy likelihood\nchannel model channel producing any particular observation sequence xis modeled by P(xjw). The\nprior probability of a hidden word is modeled by P(w). We can compute the mostprior\nprobability\nprobable word \u02c6 wgiven that we\u2019ve seen some observed misspelling xby multiply-\ning the prior P(w)and the likelihood P(xjw)and choosing the word for which this\nproduct is greatest.\nWe apply the noisy channel approach to correcting non-word spelling errors by\ntaking any word not in our spelling dictionary, generating a list of candidate words ,\nranking them according to Eq. B.4, and picking the highest-ranked one. We can\nmodify Eq. B.4 to refer to this list of candidate words instead of the full vocabulary\nVas follows:\n\u02c6w=argmax\nw2Cchannel modelz}|{\nP(xjw)priorz}|{\nP(w) (B.5)\nThe noisy channel algorithm is shown in Fig. B.2.\nTo see the details of the computation of the likelihood and the prior (language\nmodel), let\u2019s walk through an example, applying the algorithm to the example mis-\nspelling acress . The \ufb01rst stage of the algorithm proposes candidate corrections by",
    "metadata": {
      "source": "B",
      "chunk_id": 2,
      "token_count": 746,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4APPENDIX B \u2022 S PELLING CORRECTION AND THE NOISY CHANNEL\nfunction NOISY CHANNEL SPELLING (word x ,dict D , lm,editprob )returns correction\nifx=2D\ncandidates, edits All strings at edit distance 1 from xthat are2D, and their edit\nforeach c;ein candidates, edits\nchannel editprob(e)\nprior lm(x)\nscore[c] = log channel + log prior\nreturn argmaxcscore[c]\nFigure B.2 Noisy channel model for spelling correction for unknown words.\n\ufb01nding words that have a similar spelling to the input word. Analysis of spelling\nerror data has shown that the majority of spelling errors consist of a single-letter\nchange and so we often make the simplifying assumption that these candidates have\nan edit distance of 1 from the error word. To \ufb01nd this list of candidates we\u2019ll use\nthe minimum edit distance algorithm introduced in Chapter 2, but extended so that\nin addition to insertions, deletions, and substitutions, we\u2019ll add a fourth type of edit,\ntranspositions, in which two letters are swapped. The version of edit distance with\ntransposition is called Damerau-Levenshtein edit distance. Applying all such sin-Damerau-\nLevenshtein\ngle transformations to acress yields the list of candidate words in Fig. B.3.\nTransformation\nCorrect Error Position\nError Correction Letter Letter (Letter #) Type\nacress actress t \u2014 2 deletion\nacress cress \u2014 a 0 insertion\nacress caress ca ac 0 transposition\nacress access c r 2 substitution\nacress across o e 3 substitution\nacress acres \u2014 s 5 insertion\nacress acres \u2014 s 4 insertion\nFigure B.3 Candidate corrections for the misspelling acress and the transformations that\nwould have produced the error (after Kernighan et al. (1990)). \u201c\u2014\u201d represents a null letter.\nOnce we have a set of a candidates, to score each one using Eq. B.5 requires that\nwe compute the prior and the channel model.\nThe prior probability of each correction P(w)is the language model probability\nof the word win context, which can be computed using any language model, from\nunigram to trigram or 4-gram. For this example let\u2019s start in the following table by\nassuming a unigram language model. We computed the language model from the\n404,253,213 words in the Corpus of Contemporary English (COCA).\nw count(w) p(w)\nactress 9,321 .0000231\ncress 220 .000000544\ncaress 686 .00000170\naccess 37,038 .0000916\nacross 120,844 .000299\nacres 12,874 .0000318\nHow can we estimate the likelihood P(xjw), also called the channel model or channel model",
    "metadata": {
      "source": "B",
      "chunk_id": 3,
      "token_count": 623,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\nB.1 \u2022 T HENOISY CHANNEL MODEL 5\nerror model ? A perfect model of the probability that a word will be mistyped would error model\ncondition on all sorts of factors: who the typist was, whether the typist was left-\nhanded or right-handed, and so on. Luckily, we can get a pretty reasonable estimate\nofP(xjw)just by looking at local context: the identity of the correct letter itself, the\nmisspelling, and the surrounding letters. For example, the letters mandnare often\nsubstituted for each other; this is partly a fact about their identity (these two letters\nare pronounced similarly and they are next to each other on the keyboard) and partly\na fact about context (because they are pronounced similarly and they occur in similar\ncontexts).\nA simple model might estimate, for example, p(acressjacross )just using the\nnumber of times that the letter ewas substituted for the letter oin some large corpus\nof errors. To compute the probability for each edit in this way we\u2019ll need a confu-\nsion matrix that contains counts of errors. In general, a confusion matrix lists theconfusion\nmatrix\nnumber of times one thing was confused with another. Thus for example a substi-\ntution matrix will be a square matrix of size 26 \u000226 (or more generally jAj\u0002jAj,\nfor an alphabet A) that represents the number of times one letter was incorrectly\nused instead of another. Following Kernighan et al. (1990) we\u2019ll use four confusion\nmatrices.\ndel[x;y]: count( xytyped as x)\nins[x;y]: count( xtyped as xy)\nsub[x;y]: count( xtyped as y)\ntrans[x;y]: count( xytyped as yx)\nNote that we\u2019ve conditioned the insertion and deletion probabilities on the previ-\nous character; we could instead have chosen to condition on the following character.\nWhere do we get these confusion matrices? One way is to extract them from\nlists of misspellings like the following:\nadditional : addional, additonal\nenvironments : enviornments, enviorments, enviroments\npreceded : preceeded\n...\nThere are lists available on Wikipedia and from Roger Mitton ( http://www.\ndcs.bbk.ac.uk/ ~ROGER/corpora.html ) and Peter Norvig ( http://norvig.\ncom/ngrams/ ). Norvig also gives the counts for each single-character edit that can\nbe used to directly create the error model probabilities.\nAn alternative approach used by Kernighan et al. (1990) is to compute the ma-\ntrices by iteratively using this very spelling error correction algorithm itself. The\niterative algorithm \ufb01rst initializes the matrices with equal values; thus, any character\nis equally likely to be deleted, equally likely to be substituted for any other char-\nacter, etc. Next, the spelling error correction algorithm is run on a set of spelling\nerrors. Given the set of typos paired with their predicted corrections, the confusion\nmatrices can now be recomputed, the spelling algorithm run again, and so on. This\niterative algorithm is an instance of the important EM algorithm (Dempster et al.,\n1977), which we discuss in Appendix A.\nOnce we have the confusion matrices, we can estimate P(xjw)as follows (where",
    "metadata": {
      "source": "B",
      "chunk_id": 4,
      "token_count": 713,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6APPENDIX B \u2022 S PELLING CORRECTION AND THE NOISY CHANNEL\nwiis the ith character of the correct word w) and xiis the ith character of the typo x:\nP(xjw) =8\n>>>>>>>>>>>><\n>>>>>>>>>>>>:del[xi\u00001;wi]\ncount [xi\u00001wi];if deletion\nins[xi\u00001;wi]\ncount [wi\u00001];if insertion\nsub[xi;wi]\ncount [wi];if substitution\ntrans[wi;wi+1]\ncount [wiwi+1];if transposition(B.6)\nUsing the counts from Kernighan et al. (1990) results in the error model proba-\nbilities for acress shown in Fig. B.4.\nCandidate Correct Error\nCorrection Letter Letter x jw P(xjw)\nactress t - c|ct .000117\ncress - a a|# .00000144\ncaress ca ac ac|ca .00000164\naccess c r r|c .000000209\nacross o e e|o .0000093\nacres - s es|e .0000321\nacres - s ss|s .0000342\nFigure B.4 Channel model for acress ; the probabilities are taken from the del[],ins[],\nsub[], and trans [] confusion matrices as shown in Kernighan et al. (1990).\nFigure B.5 shows the \ufb01nal probabilities for each of the potential corrections;\nthe unigram prior is multiplied by the likelihood (computed with Eq. B.6 and the\nconfusion matrices). The \ufb01nal column shows the product, multiplied by 109just for\nreadability.\nCandidate Correct Error\nCorrection Letter Letter x jw P(xjw) P(w) 109*P(xjw)P(w)\nactress t - c|ct .000117 .0000231 2.7\ncress - a a|# .00000144 .000000544 0.00078\ncaress ca ac ac|ca .00000164 .00000170 0.0028\naccess c r r|c .000000209 .0000916 0.019\nacross o e e|o .0000093 .000299 2.8\nacres - s es|e .0000321 .0000318 1.0\nacres - s ss|s .0000342 .0000318 1.0\nFigure B.5 Computation of the ranking for each candidate correction, using the language\nmodel shown earlier and the error model from Fig. B.4. The \ufb01nal score is multiplied by 109\nfor readability.\nThe computations in Fig. B.5 show that our implementation of the noisy channel\nmodel chooses across as the best correction, and actress as the second most\nlikely word.\nUnfortunately, the algorithm was wrong here; the writer\u2019s intention becomes\nclear from the context: . . . was called a \u201cstellar and versatile acress whose com-\nbination of sass and glamour has de\ufb01ned her. . . \u201d. The surrounding words make it\nclear that actress and not across was the intended word.",
    "metadata": {
      "source": "B",
      "chunk_id": 5,
      "token_count": 679,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\nB.2 \u2022 R EAL-WORD SPELLING ERRORS 7\nFor this reason, it is important to use larger language models than unigrams.\nFor example, if we use the Corpus of Contemporary American English to compute\nbigram probabilities for the words actress andacross in their context using add-one\nsmoothing, we get the following probabilities:\nP(actressjversatile) =:000021\nP(acrossjversatile) =:000021\nP(whosejactress) =:0010\nP(whosejacross) =:000006\nMultiplying these out gives us the language model estimate for the two candi-\ndates in context:\nP(\u201cversatile actress whose\u201d) =:000021\u0003:0010 =210\u000210\u000010\nP(\u201cversatile across whose\u201d) =:000021\u0003:000006 =1\u000210\u000010\nCombining the language model with the error model in Fig. B.5, the bigram\nnoisy channel model now chooses the correct word actress .\nEvaluating spell correction algorithms is generally done by holding out a train-\ning, development and test set from lists of errors like those on the Norvig and Mitton\nsites mentioned above.\nB.2 Real-word spelling errors\nThe noisy channel approach can also be applied to detect and correct real-word\nspelling errors , errors that result in an actual word of English. This can happen fromreal-word error\ndetection\ntypographical errors (insertion, deletion, transposition) that accidentally produce a\nreal word (e.g., there forthree ) or because the writer substituted the wrong spelling\nof a homophone or near-homophone (e.g., dessert fordesert , orpiece forpeace ). A\nnumber of studies suggest that between 25% and 40% of spelling errors are valid\nEnglish words as in the following examples (Kukich, 1992):\nThis used to belong to thew queen. They are leaving in about \ufb01fteen minuets to go to her house.\nThe design anconstruction of the system will take more than a year.\nCan they lave him my messages?\nThe study was conducted mainly beJohn Black.\nThe noisy channel can deal with real-word errors as well. Let\u2019s begin with a\nversion of the noisy channel model \ufb01rst proposed by Mays et al. (1991) to deal\nwith these real-word spelling errors. Their algorithm takes the input sentence X=\nfx1;x2;:::; xk;:::; xng, generates a large set of candidate correction sentences C(X),\nthen picks the sentence with the highest language model probability.\nTo generate the candidate correction sentences, we start by generating a set of\ncandidate words for each input word xi. The candidates, C(xi), include every En-\nglish word with a small edit distance from xi. With edit distance 1, a common choice\n(Mays et al., 1991), the candidate set for the real word error thew (a rare word mean-\ning \u2018muscular strength\u2019) might be C(thew) = fthe, thaw, threw, them, thwe g. We then\nmake the simplifying assumption that every sentence has only one error. Thus the\nset of candidate sentences C(X)for a sentence X = Only two of thew apples\nwould be:",
    "metadata": {
      "source": "B",
      "chunk_id": 6,
      "token_count": 704,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8APPENDIX B \u2022 S PELLING CORRECTION AND THE NOISY CHANNEL\nonly two of thew apples\noily two of thew apples\nonly too of thew apples\nonly to of thew apples\nonly tao of the apples\nonly two on thew apples\nonly two off thew apples\nonly two of the apples\nonly two of threw apples\nonly two of thew applies\nonly two of thew dapples\n...\nEach sentence is scored by the noisy channel:\n\u02c6W=argmax\nW2C(X)P(XjW)P(W) (B.7)\nForP(W), we can use the trigram probability of the sentence.\nWhat about the channel model? Since these are real words, we need to consider\nthe possibility that the input word is not an error. Let\u2019s say that the channel proba-\nbility of writing a word correctly, P(wjw), isa; we can make different assumptions\nabout exactly what the value of ais in different tasks; perhaps ais .95, assum-\ning people write 1 word wrong out of 20, for some tasks, or maybe .99 for others.\nMays et al. (1991) proposed a simple model: given a typed word x, let the channel\nmodel P(xjw)beawhen x=w, and then just distribute 1 \u0000aevenly over all other\ncandidate corrections C(x):\np(xjw) =8\n>>><\n>>>:a ifx=w\n1\u0000a\njC(x)jifx2C(x)\n0 otherwise(B.8)\nNow we can replace the equal distribution of 1 \u0000aover all corrections in Eq. B.8;\nwe\u2019ll make the distribution proportional to the edit probability from the more sophis-\nticated channel model from Eq. B.6 that used the confusion matrices.\nLet\u2019s see an example of this integrated noisy channel model applied to a real\nword. Suppose we see the string two of thew . The author might have intended\nto type the real word thew (\u2018muscular strength\u2019). But thew here could also be a\ntypo for the or some other word. For the purposes of this example let\u2019s consider\nedit distance 1, and only the following \ufb01ve candidates the,thaw ,threw , and thwe\n(a rare name) and the string as typed, thew . We took the edit probabilities from\nNorvig\u2019s 2009 analysis of this example. For the language model probabilities, we\nused a Stupid Backoff model (Section ??) trained on the Google n-grams:\nP(thejtwo of) = 0.476012\nP(thewjtwo of) = 9.95051 \u000210\u00008\nP(thawjtwo of) = 2.09267 \u000210\u00007\nP(threwjtwo of) = 8.9064 \u000210\u00007\nP(themjtwo of) = 0.00144488\nP(thwejtwo of) = 5.18681 \u000210\u00009\nHere we\u2019ve just computed probabilities for the single phrase two of thew , but\nthe model applies to entire sentences; so if the example in context was two of thew",
    "metadata": {
      "source": "B",
      "chunk_id": 7,
      "token_count": 700,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9",
    "metadata": {
      "source": "B",
      "chunk_id": 8,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "B.3 \u2022 N OISY CHANNEL MODEL : THESTATE OF THE ART 9\npeople , we\u2019d need to also multiply in probabilities for P(people jof the), P(peoplejof\nthew), P(peoplejof threw), and so on.\nFollowing Norvig (2009), we assume that the probability of a word being a typo\nin this task is .05, meaning that a=P(wjw)is .95. Fig. B.6 shows the computation.\nx w xjw P (xjw)P(wjwi\u00002;wi\u00001)108P(xjw)P(wjwi\u00002;wi\u00001)\nthew the ewje 0.000007 0.48 333\nthew thew a=0.95 9.95\u000210\u000089.45\nthew thaw eja 0.001 2.1 \u000210\u000070.0209\nthew threw hjhr 0.000008 8.9 \u000210\u000070.000713\nthew thwe ewjwe 0.000003 5.2 \u000210\u000090.00000156\nFigure B.6 The noisy channel model on 5 possible candidates for thew , with a Stupid\nBackoff trigram language model computed from the Google n-gram corpus and the error\nmodel from Norvig (2009).\nFor the error phrase two of thew , the model correctly picks theas the correction.\nBut note that a lower error rate might change things; in a task where the probability\nof an error is low enough ( ais very high), the model might instead decide that the\nword thew was what the writer intended.\nB.3 Noisy Channel Model: The State of the Art\nState of the art implementations of noisy channel spelling correction make a number\nof extensions to the simple models we presented above.\nFirst, rather than make the assumption that the input sentence has only a sin-\ngle error, modern systems go through the input one word at a time, using the noisy\nchannel to make a decision for that word. But if we just run the basic noisy chan-\nnel system described above on each word, it is prone to overcorrecting , replacing\ncorrect but rare words (for example names) with more frequent words (Whitelaw\net al. 2009, Wilcox-O\u2019Hearn 2014). Modern algorithms therefore need to augment\nthe noisy channel with methods for detecting whether or not a real word should ac-\ntually be corrected. For example state of the art systems like Google\u2019s (Whitelaw\net al., 2009) use a blacklist, forbidding certain tokens (like numbers, punctuation,\nand single letter words) from being changed. Such systems are also more cautious\nin deciding whether to trust a candidate correction. Instead of just choosing a candi-\ndate correction if it has a higher probability P(wjx)than the word itself, these more\ncareful systems choose to suggest a correction wover keeping the non-correction x\nonly if the difference in probabilities is suf\ufb01ciently great. The best correction wis\nchosen only if:\nlogP(wjx)\u0000logP(xjx)>q\nDepending on the speci\ufb01c application, spell-checkers may decide to autocorrect autocorrect\n(automatically change a spelling to a hypothesized correction) or merely to \ufb02ag the\nerror and offer suggestions. This decision is often made by another classi\ufb01er which\ndecides whether the best candidate is good enough, using features such as the dif-\nference in log probabilities between the candidates (we\u2019ll introduce algorithms for\nclassi\ufb01cation in the next chapter).",
    "metadata": {
      "source": "B",
      "chunk_id": 9,
      "token_count": 777,
      "chapter_title": ""
    }
  },
  {
    "content": "et al. 2009, Wilcox-O\u2019Hearn 2014). Modern algorithms therefore need to augment\nthe noisy channel with methods for detecting whether or not a real word should ac-\ntually be corrected. For example state of the art systems like Google\u2019s (Whitelaw\net al., 2009) use a blacklist, forbidding certain tokens (like numbers, punctuation,\nand single letter words) from being changed. Such systems are also more cautious\nin deciding whether to trust a candidate correction. Instead of just choosing a candi-\ndate correction if it has a higher probability P(wjx)than the word itself, these more\ncareful systems choose to suggest a correction wover keeping the non-correction x\nonly if the difference in probabilities is suf\ufb01ciently great. The best correction wis\nchosen only if:\nlogP(wjx)\u0000logP(xjx)>q\nDepending on the speci\ufb01c application, spell-checkers may decide to autocorrect autocorrect\n(automatically change a spelling to a hypothesized correction) or merely to \ufb02ag the\nerror and offer suggestions. This decision is often made by another classi\ufb01er which\ndecides whether the best candidate is good enough, using features such as the dif-\nference in log probabilities between the candidates (we\u2019ll introduce algorithms for\nclassi\ufb01cation in the next chapter).\nModern systems also use much larger dictionaries than early systems. Ahmad\nand Kondrak (2005) found that a 100,000 word UNIX dictionary only contained",
    "metadata": {
      "source": "B",
      "chunk_id": 10,
      "token_count": 326,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10",
    "metadata": {
      "source": "B",
      "chunk_id": 11,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "10 APPENDIX B \u2022 S PELLING CORRECTION AND THE NOISY CHANNEL\n73% of the word types in their corpus of web queries, missing words like pics, mul-\ntiplayer, google, xbox, clipart , and mallorca . For this reason modern systems often\nuse much larger dictionaries automatically derived from very large lists of unigrams\nlike the Google n-gram corpus. Whitelaw et al. (2009), for example, used the most\nfrequently occurring ten million word types in a large sample of web pages. Because\nthis list will include lots of misspellings, their system requires a more sophisticated\nerror model. The fact that words are generally more frequent than their misspellings\ncan be used in candidate suggestion, by building a set of words and spelling vari-\nations that have similar contexts, sorting by frequency, treating the most frequent\nvariant as the source, and learning an error model from the difference, whether from\nweb text (Whitelaw et al., 2009) or from query logs (Cucerzan and Brill, 2004).\nWords can also be automatically added to the dictionary when a user rejects a cor-\nrection, and systems running on phones can automatically add words from the user\u2019s\naddress book or calendar.\nWe can also improve the performance of the noisy channel model by changing\nhow the prior and the likelihood are combined. In the standard model they are just\nmultiplied together. But often these probabilities are not commensurate; the lan-\nguage model or the channel model might have very different ranges. Alternatively\nfor some task or dataset we might have reason to trust one of the two models more.\nTherefore we use a weighted combination, by raising one of the factors to a power\nl:\n\u02c6w=argmax\nw2VP(xjw)P(w)l(B.9)\nor in log space:\n\u02c6w=argmax\nw2VlogP(xjw)+llogP(w) (B.10)\nWe then tune the parameter lon a development test set.\nFinally, if our goal is to do real-word spelling correction only for speci\ufb01c con-\nfusion sets likepeace/piece ,affect/effect ,weather/whether , or even grammar cor- confusion sets\nrection examples like among/between , we can train supervised classi\ufb01ers to draw on\nmany features of the context and make a choice between the two candidates. Such\nclassi\ufb01ers can achieve very high accuracy for these speci\ufb01c sets, especially when\ndrawing on large-scale features from web statistics (Golding and Roth 1999, Lapata\nand Keller 2004, Bergsma et al. 2009, Bergsma et al. 2010).\nB.3.1 Improved Edit Models: Partitions and Pronunciation\nOther recent research has focused on improving the channel model P(tjc). One\nimportant extension is the ability to compute probabilities for multiple-letter trans-\nformations. For example Brill and Moore (2000) propose a channel model that\n(informally) models an error as being generated by a typist \ufb01rst choosing a word,\nthen choosing a partition of the letters of that word, and then typing each partition,\npossibly erroneously. For example, imagine a person chooses the word physical ,\nthen chooses the partition ph y s i c al . She would then generate each parti-\ntion, possibly with errors. For example the probability that she would generate the\nstring fisikle with partition f i s i k le would be p(fjph)\u0003p(ijy)\u0003p(sjs)\u0003",
    "metadata": {
      "source": "B",
      "chunk_id": 12,
      "token_count": 758,
      "chapter_title": ""
    }
  },
  {
    "content": "rection examples like among/between , we can train supervised classi\ufb01ers to draw on\nmany features of the context and make a choice between the two candidates. Such\nclassi\ufb01ers can achieve very high accuracy for these speci\ufb01c sets, especially when\ndrawing on large-scale features from web statistics (Golding and Roth 1999, Lapata\nand Keller 2004, Bergsma et al. 2009, Bergsma et al. 2010).\nB.3.1 Improved Edit Models: Partitions and Pronunciation\nOther recent research has focused on improving the channel model P(tjc). One\nimportant extension is the ability to compute probabilities for multiple-letter trans-\nformations. For example Brill and Moore (2000) propose a channel model that\n(informally) models an error as being generated by a typist \ufb01rst choosing a word,\nthen choosing a partition of the letters of that word, and then typing each partition,\npossibly erroneously. For example, imagine a person chooses the word physical ,\nthen chooses the partition ph y s i c al . She would then generate each parti-\ntion, possibly with errors. For example the probability that she would generate the\nstring fisikle with partition f i s i k le would be p(fjph)\u0003p(ijy)\u0003p(sjs)\u0003\np(iji)\u0003p(kjk)\u0003p(lejal). Unlike the Damerau-Levenshtein edit distance, the Brill-\nMoore channel model can thus model edit probabilities like P(fjph)orP(lejal), or",
    "metadata": {
      "source": "B",
      "chunk_id": 13,
      "token_count": 335,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\nB.3 \u2022 N OISY CHANNEL MODEL : THESTATE OF THE ART 11\nthe high likelihood of P(entjant). Furthermore, each edit is conditioned on where\nit is in the word ( beginning, middle, end ) so instead of P(fjph)the model actually\nestimates P(fjph;beginning ).\nMore formally, let R be a partition of the typo string xinto adjacent (possibly\nempty) substrings, and T be a partition of the candidate string. Brill and Moore\n(2000) then approximates the total likelihood P(xjw)(e.g., P( fisiklejphysical ))\nby the probability of the single best partition:\nP(xjw)\u0019 max\nR;T s:t:jTj=jRjjRjX\ni=1P(TijRi;position ) (B.11)\nThe probability of each transform P(TijRi)can be learned from a training set of\ntriples of an error, the correct string, and the number of times it occurs. For example\ngiven a training pair akgsual /actual , standard minimum edit distance is used to\nproduce an alignment:\nactualakgsual\nThis alignment corresponds to the sequence of edit operations:\na!a,c!k,\u000f!g t!s,u!u,a!a,l!l\nEach nonmatch substitution is then expanded to incorporate up to N additional\nedits; For N=2, we would expand c!kto:\nac!ak\nc!cg\nac!akg\nct!kgs\nEach of these multiple edits then gets a fractional count, and the probability for\neach edit a!bis then estimated from counts in the training corpus of triples as\ncount (a!b)\ncount (a).\nAnother research direction in channel models is the use of pronunciation in ad-\ndition to spelling. Pronunciation is an important feature in some non-noisy-channel\nalgorithms for spell correction like the GNU aspell algorithm (Atkinson, 2011), aspell\nwhich makes use of the metaphone pronunciation of a word (Philips, 1990). Meta-\nphone is a series of rules that map a word to a normalized representation of its\npronunciation. Some example rules:\n\u2022 \u201cDrop duplicate adjacent letters, except for C.\u201d\n\u2022 \u201cIf the word begins with \u2018KN\u2019, \u2018GN\u2019, \u2018PN\u2019, \u2018AE\u2019, \u2018WR\u2019, drop the \ufb01rst letter.\u201d\n\u2022 \u201cDrop \u2018B\u2019 if after \u2018M\u2019 and if it is at the end of the word\u201d\nAspell works similarly to the channel component of the noisy channel model, \ufb01nding\nall words in the dictionary whose pronunciation string is a short edit distance (1 or\n2 pronunciation letters) from the typo, and then scoring this list of candidates by\na metric that combines two edit distances: the pronunciation edit distance and the\nweighted letter edit distance.\nPronunciation can also be incorporated directly the noisy channel model. For ex-\nample the Toutanova and Moore (2002) model, like aspell, interpolates two channel",
    "metadata": {
      "source": "B",
      "chunk_id": 14,
      "token_count": 648,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 APPENDIX B \u2022 S PELLING CORRECTION AND THE NOISY CHANNEL\nfunction SOUNDEX (name )returns soundex form\n1. Keep the \ufb01rst letter of name\n2. Drop all occurrences of non-initial a, e, h, i, o, u, w, y.\n3. Replace the remaining letters with the following numbers:\nb, f, p, v!1\nc, g, j, k, q, s, x, z!2\nd, t!3\nl!4\nm, n!5\nr!6\n4. Replace any sequences of identical numbers, only if they derive from two or more\nletters that were adjacent in the original name, with a single number (e.g., 666 !6).\n5. Convert to the form Letter Digit Digit Digit by dropping digits past the third\n(if necessary) or padding with trailing zeros (if necessary).\nFigure B.7 The Soundex Algorithm\nmodels, one based on spelling and one based on pronunciation. The pronunciation\nmodel is based on using letter-to-sound models to translate each input word and letter-to-sound\neach dictionary word into a sequences of phones representing the pronunciation of phones\nthe word. For example actress andaktress would both map to the phone string\nae k t r ix s . See Chapter 16 on the task of letter-to-sound or grapheme-to-\nphoneme .\nSome additional string distance functions have been proposed for dealing specif-\nically with names . These are mainly used for the task of deduplication (deciding if deduplication\ntwo names in a census list or other namelist are the same) rather than spell-checking.\nThe Soundex algorithm (Knuth 1973, Odell and Russell 1918/1922) is an older\nmethod used originally for census records for representing people\u2019s names. It has the\nadvantage that versions of the names that are slightly misspelled will still have the\nsame representation as correctly spelled names. (e.g., Jurafsky, Jarofsky, Jarovsky,\nand Jarovski all map to J612). The algorithm is shown in Fig. B.7.\nInstead of Soundex, more recent work uses Jaro-Winkler distance, which is Jaro-Winkler\nan edit distance algorithm designed for names that allows characters to be moved\nlonger distances in longer names, and also gives a higher similarity to strings that\nhave identical initial characters (Winkler, 2006).\nBibliographical and Historical Notes\nAlgorithms for spelling error detection and correction have existed since at least\nBlair (1960). Most early algorithms were based on similarity keys like the Soundex\nalgorithm (Odell and Russell 1918/1922, Knuth 1973). Damerau (1964) gave a\ndictionary-based algorithm for error detection; most error-detection algorithms since\nthen have been based on dictionaries. Early research (Peterson, 1986) had suggested\nthat spelling dictionaries might need to be kept small because large dictionaries con-\ntain very rare words (wont, veery) that resemble misspellings of other words, but\nDamerau and Mays (1989) found that in practice larger dictionaries proved more\nhelpful. Damerau (1964) also gave a correction algorithm that worked for single\nerrors.\nThe idea of modeling language transmission as a Markov source passed through",
    "metadata": {
      "source": "B",
      "chunk_id": 15,
      "token_count": 723,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\nEXERCISES 13\na noisy channel model was developed very early on by Claude Shannon (1948).\nThe idea of combining a prior and a likelihood to deal with the noisy channel was\ndeveloped at IBM Research by Raviv (1967), for the similar task of optical char-\nacter recognition (OCR ). While earlier spell-checkers like Kashyap and Oommen\n(1983) had used likelihood-based models of edit distance, the idea of combining a\nprior and a likelihood seems not to have been applied to the spelling correction task\nuntil researchers at AT&T Bell Laboratories (Kernighan et al. 1990, Church and\nGale 1991) and IBM Watson Research (Mays et al., 1991) roughly simultaneously\nproposed noisy channel spelling correction. Much later, the Mays et al. (1991) algo-\nrithm was reimplemented and tested on standard datasets by Wilcox-O\u2019Hearn et al.\n(2008), who showed its high performance.\nMost algorithms since Wagner and Fischer (1974) have relied on dynamic pro-\ngramming.\nRecent focus has been on using the web both for language models and for train-\ning the error model, and on incorporating additional features in spelling, like the\npronunciation models described earlier, or other information like parses or semantic\nrelatedness (Jones and Martin 1997, Hirst and Budanitsky 2005).\nSee Mitton (1987) for a survey of human spelling errors, and Kukich (1992)\nfor an early survey of spelling error detection and correction. Norvig (2007) gives\na nice explanation and a Python implementation of the noisy channel model, with\nmore details and an ef\ufb01cient algorithm presented in Norvig (2009).\nExercises\nB.1 Suppose we want to apply add-one smoothing to the likelihood term (channel\nmodel) P(xjw)of a noisy channel model of spelling. For simplicity, pretend\nthat the only possible operation is deletion. The MLE estimate for deletion\nis given in Eq. B.6, which is P(xjw) =del[xi\u00001;wi]\ncount (xi\u00001wi). What is the estimate for\nP(xjw)if we use add-one smoothing on the deletion edit model? Assume the\nonly characters we use are lower case a-z, that there are Vword types in our\ncorpus, and Ntotal characters, not counting spaces.",
    "metadata": {
      "source": "B",
      "chunk_id": 16,
      "token_count": 521,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14",
    "metadata": {
      "source": "B",
      "chunk_id": 17,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "14 Appendix B \u2022 Spelling Correction and the Noisy Channel\nAhmad, F. and G. Kondrak. 2005. Learning a spelling error\nmodel from search query logs. EMNLP .\nAtkinson, K. 2011. Gnu aspell.\nBergsma, S., D. Lin, and R. Goebel. 2009. Web-scale n-gram\nmodels for lexical disambiguation. IJCAI .\nBergsma, S., E. Pitler, and D. Lin. 2010. Creating robust\nsupervised classi\ufb01ers via web-scale n-gram data. ACL.\nBlair, C. R. 1960. A program for correcting spelling errors.\nInformation and Control , 3:60\u201367.\nBrill, E. and R. C. Moore. 2000. An improved error model\nfor noisy channel spelling correction. ACL.\nChurch, K. W. and W. A. Gale. 1991. Probability scoring for\nspelling correction. Statistics and Computing , 1(2):93\u2013\n103.\nCucerzan, S. and E. Brill. 2004. Spelling correction as an\niterative process that exploits the collective knowledge of\nweb users. EMNLP , volume 4.\nDamerau, F. J. 1964. A technique for computer detection and\ncorrection of spelling errors. CACM , 7(3):171\u2013176.\nDamerau, F. J. and E. Mays. 1989. An examination of un-\ndetected typing errors. Information Processing and Man-\nagement , 25(6):659\u2013664.\nDempster, A. P., N. M. Laird, and D. B. Rubin. 1977. Max-\nimum likelihood from incomplete data via the EMalgo-\nrithm. Journal of the Royal Statistical Society , 39(1):1\u2013\n21.\nGolding, A. R. and D. Roth. 1999. A Winnow based ap-\nproach to context-sensitive spelling correction. Machine\nLearning , 34(1-3):107\u2013130.\nHirst, G. and A. Budanitsky. 2005. Correcting real-word\nspelling errors by restoring lexical cohesion. Natural\nLanguage Engineering , 11:87\u2013111.\nJones, M. P. and J. H. Martin. 1997. Contextual spelling cor-\nrection using latent semantic analysis. ANLP .\nKashyap, R. L. and B. J. Oommen. 1983. Spelling correction\nusing probabilistic methods. Pattern Recognition Letters ,\n2:147\u2013154.\nKernighan, M. D., K. W. Church, and W. A. Gale. 1990.\nA spelling correction program base on a noisy channel\nmodel. COLING , volume II.\nKnuth, D. E. 1973. Sorting and Searching: The Art of Com-\nputer Programming Volume 3 . Addison-Wesley.\nKukich, K. 1992. Techniques for automatically correcting\nwords in text. ACM Computing Surveys , 24(4):377\u2013439.\nLapata, M. and F. Keller. 2004. The web as a baseline: Eval-\nuating the performance of unsupervised web-based mod-\nels for a range of NLP tasks. HLT-NAACL .\nMays, E., F. J. Damerau, and R. L. Mercer. 1991. Context",
    "metadata": {
      "source": "B",
      "chunk_id": 18,
      "token_count": 761,
      "chapter_title": ""
    }
  },
  {
    "content": "spelling errors by restoring lexical cohesion. Natural\nLanguage Engineering , 11:87\u2013111.\nJones, M. P. and J. H. Martin. 1997. Contextual spelling cor-\nrection using latent semantic analysis. ANLP .\nKashyap, R. L. and B. J. Oommen. 1983. Spelling correction\nusing probabilistic methods. Pattern Recognition Letters ,\n2:147\u2013154.\nKernighan, M. D., K. W. Church, and W. A. Gale. 1990.\nA spelling correction program base on a noisy channel\nmodel. COLING , volume II.\nKnuth, D. E. 1973. Sorting and Searching: The Art of Com-\nputer Programming Volume 3 . Addison-Wesley.\nKukich, K. 1992. Techniques for automatically correcting\nwords in text. ACM Computing Surveys , 24(4):377\u2013439.\nLapata, M. and F. Keller. 2004. The web as a baseline: Eval-\nuating the performance of unsupervised web-based mod-\nels for a range of NLP tasks. HLT-NAACL .\nMays, E., F. J. Damerau, and R. L. Mercer. 1991. Context\nbased spelling correction. Information Processing and\nManagement , 27(5):517\u2013522.\nMitton, R. 1987. Spelling checkers, spelling correctors and\nthe misspellings of poor spellers. Information processing\n& management , 23(5):495\u2013505.\nNorvig, P. 2007. How to write a spelling corrector. http:\n//www.norvig.com/spell-correct.html .\nNorvig, P. 2009. Natural language corpus data. In T. Segaran\nand J. Hammerbacher, eds, Beautiful data: the stories be-\nhind elegant data solutions . O\u2019Reilly.Odell, M. K. and R. C. Russell. 1918/1922. U.S. Patents\n1261167 (1918), 1435663 (1922). Cited in Knuth (1973).\nPeterson, J. L. 1986. A note on undetected typing errors.\nCACM , 29(7):633\u2013637.\nPhilips, L. 1990. Hanging on the metaphone. Computer\nLanguage , 7(12).\nRaviv, J. 1967. Decision making in Markov chains applied\nto the problem of pattern recognition. IEEE Transactions\non Information Theory , 13(4):536\u2013551.\nShannon, C. E. 1948. A mathematical theory of commu-\nnication. Bell System Technical Journal , 27(3):379\u2013423.\nContinued in the following volume.\nToutanova, K. and R. C. Moore. 2002. Pronunciation mod-\neling for improved spelling correction. ACL.\nVeblen, T. 1899. Theory of the Leisure Class . Macmillan,\nNew York.\nWagner, R. A. and M. J. Fischer. 1974. The string-to-string\ncorrection problem. Journal of the ACM , 21:168\u2013173.\nWhitelaw, C., B. Hutchinson, G. Y . Chung, and G. El-\nlis. 2009. Using the web for language independent\nspellchecking and autocorrection. EMNLP .\nWilcox-O\u2019Hearn, L. A. 2014. Detection is the central prob-",
    "metadata": {
      "source": "B",
      "chunk_id": 19,
      "token_count": 760,
      "chapter_title": ""
    }
  },
  {
    "content": "CACM , 29(7):633\u2013637.\nPhilips, L. 1990. Hanging on the metaphone. Computer\nLanguage , 7(12).\nRaviv, J. 1967. Decision making in Markov chains applied\nto the problem of pattern recognition. IEEE Transactions\non Information Theory , 13(4):536\u2013551.\nShannon, C. E. 1948. A mathematical theory of commu-\nnication. Bell System Technical Journal , 27(3):379\u2013423.\nContinued in the following volume.\nToutanova, K. and R. C. Moore. 2002. Pronunciation mod-\neling for improved spelling correction. ACL.\nVeblen, T. 1899. Theory of the Leisure Class . Macmillan,\nNew York.\nWagner, R. A. and M. J. Fischer. 1974. The string-to-string\ncorrection problem. Journal of the ACM , 21:168\u2013173.\nWhitelaw, C., B. Hutchinson, G. Y . Chung, and G. El-\nlis. 2009. Using the web for language independent\nspellchecking and autocorrection. EMNLP .\nWilcox-O\u2019Hearn, L. A. 2014. Detection is the central prob-\nlem in real-word spelling correction. http://arxiv.\norg/abs/1408.3153 .\nWilcox-O\u2019Hearn, L. A., G. Hirst, and A. Budanitsky. 2008.\nReal-word spelling correction with trigrams: A recon-\nsideration of the Mays, Damerau, and Mercer model.\nCICLing-2008 .\nWinkler, W. E. 2006. Overview of record linkage and current\nresearch directions. Technical report, Statistical Research\nDivision, U.S. Census Bureau.",
    "metadata": {
      "source": "B",
      "chunk_id": 20,
      "token_count": 397,
      "chapter_title": ""
    }
  }
]