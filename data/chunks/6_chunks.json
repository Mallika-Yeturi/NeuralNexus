[
  {
    "content": "# 6\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n6Vector Semantics and\nEmbeddings\n\u8343\u8005\u6240\u4ee5\u5728\u9c7c\uff0c\u5f97\u9c7c\u800c\u5fd8\u8343Nets are for \ufb01sh;\nOnce you get the \ufb01sh, you can forget the net.\n\u8a00\u8005\u6240\u4ee5\u5728\u610f\uff0c\u5f97\u610f\u800c\u5fd8\u8a00Words are for meaning;\nOnce you get the meaning, you can forget the words\n\u5e84\u5b50(Zhuangzi), Chapter 26\nThe asphalt that Los Angeles is famous for occurs mainly on its freeways. But\nin the middle of the city is another patch of asphalt, the La Brea tar pits, and this\nasphalt preserves millions of fossil bones from the last of the Ice Ages of the Pleis-\ntocene Epoch. One of these fossils is the Smilodon , or saber-toothed tiger, instantly\nrecognizable by its long canines. Five million years ago or so, a completely different\nsaber-tooth tiger called Thylacosmilus lived\nin Argentina and other parts of South Amer-\nica. Thylacosmilus was a marsupial whereas\nSmilodon was a placental mammal, but Thy-\nlacosmilus had the same long upper canines\nand, like Smilodon, had a protective bone\n\ufb02ange on the lower jaw. The similarity of\nthese two mammals is one of many examples\nof parallel or convergent evolution, in which particular contexts or environments\nlead to the evolution of very similar structures in different species (Gould, 1980).\nThe role of context is also important in the similarity of a less biological kind\nof organism: the word. Words that occur in similar contexts tend to have similar\nmeanings . This link between similarity in how words are distributed and similarity\nin what they mean is called the distributional hypothesis . The hypothesis wasdistributional\nhypothesis\n\ufb01rst formulated in the 1950s by linguists like Joos (1950), Harris (1954), and Firth\n(1957), who noticed that words which are synonyms (like oculist andeye-doctor )\ntended to occur in the same environment (e.g., near words like eyeorexamined )\nwith the amount of meaning difference between two words \u201ccorresponding roughly\nto the amount of difference in their environments\u201d (Harris, 1954, p. 157).\nIn this chapter we introduce vector semantics , which instantiates this linguisticvector\nsemantics\nhypothesis by learning representations of the meaning of words, called embeddings , embeddings\ndirectly from their distributions in texts. These representations are used in every nat-\nural language processing application that makes use of meaning, and the static em-\nbeddings we introduce here underlie the more powerful dynamic or contextualized\nembeddings likeBERT that we will see in Chapter 11.\nThese word representations are also the \ufb01rst example in this book of repre-\nsentation learning , automatically learning useful representations of the input text.representation\nlearning\nFinding such self-supervised ways to learn representations of the input, instead of\ncreating representations by hand via feature engineering , is an important focus of\nNLP research (Bengio et al., 2013).",
    "metadata": {
      "source": "6",
      "chunk_id": 0,
      "token_count": 715,
      "chapter_title": "6"
    }
  },
  {
    "content": "## Page 2",
    "metadata": {
      "source": "6",
      "chunk_id": 1,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "2CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\n6.1 Lexical Semantics\nLet\u2019s begin by introducing some basic principles of word meaning. How should\nwe represent the meaning of a word? In the n-gram models of Chapter 3, and in\nclassical NLP applications, our only representation of a word is as a string of letters,\nor an index in a vocabulary list. This representation is not that different from a\ntradition in philosophy, perhaps you\u2019ve seen it in introductory logic classes, in which\nthe meaning of words is represented by just spelling the word with small capital\nletters; representing the meaning of \u201cdog\u201d as DOG, and \u201ccat\u201d as CAT, or by using an\napostrophe ( DOG \u2019).\nRepresenting the meaning of a word by capitalizing it is a pretty unsatisfactory\nmodel. You might have seen a version of a joke due originally to semanticist Barbara\nPartee (Carlson, 1977):\nQ: What\u2019s the meaning of life?\nA:LIFE \u2019\nSurely we can do better than this! After all, we\u2019ll want a model of word meaning\nto do all sorts of things for us. It should tell us that some words have similar mean-\nings ( catis similar to dog), others are antonyms ( cold is the opposite of hot), some\nhave positive connotations ( happy ) while others have negative connotations ( sad). It\nshould represent the fact that the meanings of buy,sell, and payoffer differing per-\nspectives on the same underlying purchasing event. (If I buy something from you,\nyou\u2019ve probably sold it to me, and I likely paid you.) More generally, a model of\nword meaning should allow us to draw inferences to address meaning-related tasks\nlike question-answering or dialogue.\nIn this section we summarize some of these desiderata, drawing on results in the\nlinguistic study of word meaning, which is called lexical semantics ; we\u2019ll return tolexical\nsemantics\nand expand on this list in Appendix G and Chapter 21.\nLemmas and Senses Let\u2019s start by looking at how one word (we\u2019ll choose mouse )\nmight be de\ufb01ned in a dictionary (simpli\ufb01ed from the online dictionary WordNet):\nmouse (N)\n1. any of numerous small rodents...\n2. a hand-operated device that controls a cursor...\nHere the form mouse is the lemma , also called the citation form . The form lemma\ncitation form mouse would also be the lemma for the word mice ; dictionaries don\u2019t have separate\nde\ufb01nitions for in\ufb02ected forms like mice . Similarly sing is the lemma for sing,sang ,\nsung . In many languages the in\ufb01nitive form is used as the lemma for the verb, so\nSpanish dormir \u201cto sleep\u201d is the lemma for duermes \u201cyou sleep\u201d. The speci\ufb01c forms\nsung orcarpets orsing orduermes are called wordforms . wordform\nAs the example above shows, each lemma can have multiple meanings; the\nlemma mouse can refer to the rodent or the cursor control device. We call each\nof these aspects of the meaning of mouse aword sense . The fact that lemmas can\nbepolysemous (have multiple senses) can make interpretation dif\ufb01cult (is someone\nwho types \u201cmouse info\u201d into a search engine looking for a pet or a tool?). Chap-\nter 11 and Appendix G will discuss the problem of polysemy, and introduce word\nsense disambiguation , the task of determining which sense of a word is being used\nin a particular context.",
    "metadata": {
      "source": "6",
      "chunk_id": 2,
      "token_count": 774,
      "chapter_title": ""
    }
  },
  {
    "content": "mouse (N)\n1. any of numerous small rodents...\n2. a hand-operated device that controls a cursor...\nHere the form mouse is the lemma , also called the citation form . The form lemma\ncitation form mouse would also be the lemma for the word mice ; dictionaries don\u2019t have separate\nde\ufb01nitions for in\ufb02ected forms like mice . Similarly sing is the lemma for sing,sang ,\nsung . In many languages the in\ufb01nitive form is used as the lemma for the verb, so\nSpanish dormir \u201cto sleep\u201d is the lemma for duermes \u201cyou sleep\u201d. The speci\ufb01c forms\nsung orcarpets orsing orduermes are called wordforms . wordform\nAs the example above shows, each lemma can have multiple meanings; the\nlemma mouse can refer to the rodent or the cursor control device. We call each\nof these aspects of the meaning of mouse aword sense . The fact that lemmas can\nbepolysemous (have multiple senses) can make interpretation dif\ufb01cult (is someone\nwho types \u201cmouse info\u201d into a search engine looking for a pet or a tool?). Chap-\nter 11 and Appendix G will discuss the problem of polysemy, and introduce word\nsense disambiguation , the task of determining which sense of a word is being used\nin a particular context.\nSynonymy One important component of word meaning is the relationship be-\ntween word senses. For example when one word has a sense whose meaning is",
    "metadata": {
      "source": "6",
      "chunk_id": 3,
      "token_count": 321,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3",
    "metadata": {
      "source": "6",
      "chunk_id": 4,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "6.1 \u2022 L EXICAL SEMANTICS 3\nidentical to a sense of another word, or nearly identical, we say the two senses of\nthose two words are synonyms . Synonyms include such pairs as synonym\ncouch/sofa vomit/throw up \ufb01lbert/hazelnut car/automobile\nA more formal de\ufb01nition of synonymy (between words rather than senses) is that\ntwo words are synonymous if they are substitutable for one another in any sentence\nwithout changing the truth conditions of the sentence, the situations in which the\nsentence would be true.\nWhile substitutions between some pairs of words like car/automobile orwa-\nter/H2Oare truth preserving, the words are still not identical in meaning. Indeed,\nprobably no two words are absolutely identical in meaning. One of the fundamental\ntenets of semantics, called the principle of contrast (Girard 1718, Br \u00b4eal 1897, Clarkprinciple of\ncontrast\n1987), states that a difference in linguistic form is always associated with some dif-\nference in meaning. For example, the word H2Ois used in scienti\ufb01c contexts and\nwould be inappropriate in a hiking guide\u2014 water would be more appropriate\u2014 and\nthis genre difference is part of the meaning of the word. In practice, the word syn-\nonym is therefore used to describe a relationship of approximate or rough synonymy.\nWord Similarity While words don\u2019t have many synonyms, most words do have\nlots of similar words. Catis not a synonym of dog, but cats anddogs are certainly\nsimilar words. In moving from synonymy to similarity, it will be useful to shift from\ntalking about relations between word senses (like synonymy) to relations between\nwords (like similarity). Dealing with words avoids having to commit to a particular\nrepresentation of word senses, which will turn out to simplify our task.\nThe notion of word similarity is very useful in larger semantic tasks. Knowing similarity\nhow similar two words are can help in computing how similar the meaning of two\nphrases or sentences are, a very important component of tasks like question answer-\ning, paraphrasing, and summarization. One way of getting values for word similarity\nis to ask humans to judge how similar one word is to another. A number of datasets\nhave resulted from such experiments. For example the SimLex-999 dataset (Hill\net al., 2015) gives values on a scale from 0 to 10, like the examples below, which\nrange from near-synonyms ( vanish ,disappear ) to pairs that scarcely seem to have\nanything in common ( hole,agreement ):\nvanish disappear 9.8\nbelief impression 5.95\nmuscle bone 3.65\nmodest \ufb02exible 0.98\nhole agreement 0.3\nWord Relatedness The meaning of two words can be related in ways other than\nsimilarity. One such class of connections is called word relatedness (Budanitsky relatedness\nand Hirst, 2006), also traditionally called word association in psychology. association\nConsider the meanings of the words coffee andcup. Coffee is not similar to cup;\nthey share practically no features (coffee is a plant or a beverage, while a cup is a\nmanufactured object with a particular shape). But coffee and cup are clearly related;\nthey are associated by co-participating in an everyday event (the event of drinking\ncoffee out of a cup). Similarly scalpel andsurgeon are not similar but are related\neventively (a surgeon tends to make use of a scalpel).\nOne common kind of relatedness between words is if they belong to the same",
    "metadata": {
      "source": "6",
      "chunk_id": 5,
      "token_count": 778,
      "chapter_title": ""
    }
  },
  {
    "content": "have resulted from such experiments. For example the SimLex-999 dataset (Hill\net al., 2015) gives values on a scale from 0 to 10, like the examples below, which\nrange from near-synonyms ( vanish ,disappear ) to pairs that scarcely seem to have\nanything in common ( hole,agreement ):\nvanish disappear 9.8\nbelief impression 5.95\nmuscle bone 3.65\nmodest \ufb02exible 0.98\nhole agreement 0.3\nWord Relatedness The meaning of two words can be related in ways other than\nsimilarity. One such class of connections is called word relatedness (Budanitsky relatedness\nand Hirst, 2006), also traditionally called word association in psychology. association\nConsider the meanings of the words coffee andcup. Coffee is not similar to cup;\nthey share practically no features (coffee is a plant or a beverage, while a cup is a\nmanufactured object with a particular shape). But coffee and cup are clearly related;\nthey are associated by co-participating in an everyday event (the event of drinking\ncoffee out of a cup). Similarly scalpel andsurgeon are not similar but are related\neventively (a surgeon tends to make use of a scalpel).\nOne common kind of relatedness between words is if they belong to the same\nsemantic \ufb01eld . A semantic \ufb01eld is a set of words which cover a particular semantic semantic \ufb01eld\ndomain and bear structured relations with each other. For example, words might be",
    "metadata": {
      "source": "6",
      "chunk_id": 6,
      "token_count": 332,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4",
    "metadata": {
      "source": "6",
      "chunk_id": 7,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "4CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\nrelated by being in the semantic \ufb01eld of hospitals ( surgeon ,scalpel ,nurse ,anes-\nthetic ,hospital ), restaurants ( waiter ,menu ,plate ,food,chef), or houses ( door ,roof,\nkitchen ,family ,bed). Semantic \ufb01elds are also related to topic models , like Latent topic models\nDirichlet Allocation ,LDA , which apply unsupervised learning on large sets of texts\nto induce sets of associated words from text. Semantic \ufb01elds and topic models are\nvery useful tools for discovering topical structure in documents.\nIn Appendix G we\u2019ll introduce more relations between senses like hypernymy\norIS-A ,antonymy (opposites) and meronymy (part-whole relations).\nSemantic Frames and Roles Closely related to semantic \ufb01elds is the idea of a\nsemantic frame . A semantic frame is a set of words that denote perspectives or semantic frame\nparticipants in a particular type of event. A commercial transaction, for example,\nis a kind of event in which one entity trades money to another entity in return for\nsome good or service, after which the good changes hands or perhaps the service is\nperformed. This event can be encoded lexically by using verbs like buy(the event\nfrom the perspective of the buyer), sell(from the perspective of the seller), pay\n(focusing on the monetary aspect), or nouns like buyer . Frames have semantic roles\n(like buyer ,seller ,goods ,money ), and words in a sentence can take on these roles.\nKnowing that buyandsellhave this relation makes it possible for a system to\nknow that a sentence like Sam bought the book from Ling could be paraphrased as\nLing sold the book to Sam , and that Sam has the role of the buyer in the frame and\nLing the seller . Being able to recognize such paraphrases is important for question\nanswering, and can help in shifting perspective for machine translation.\nConnotation Finally, words have affective meanings orconnotations . The word connotations\nconnotation has different meanings in different \ufb01elds, but here we use it to mean the\naspects of a word\u2019s meaning that are related to a writer or reader\u2019s emotions, senti-\nment, opinions, or evaluations. For example some words have positive connotations\n(wonderful ) while others have negative connotations ( dreary ). Even words whose\nmeanings are similar in other ways can vary in connotation; consider the difference\nin connotations between fake,knockoff ,forgery , on the one hand, and copy ,replica ,\nreproduction on the other, or innocent (positive connotation) and naive (negative\nconnotation). Some words describe positive evaluation ( great ,love) and others neg-\native evaluation ( terrible ,hate). Positive or negative evaluation language is called\nsentiment , as we saw in Chapter 4, and word sentiment plays a role in important sentiment\ntasks like sentiment analysis, stance detection, and applications of NLP to the lan-\nguage of politics and consumer reviews.\nEarly work on affective meaning (Osgood et al., 1957) found that words varied\nalong three important dimensions of affective meaning:\nvalence: the pleasantness of the stimulus\narousal: the intensity of emotion provoked by the stimulus\ndominance: the degree of control exerted by the stimulus\nThus words like happy orsatis\ufb01ed are high on valence, while unhappy oran-\nnoyed are low on valence. Excited is high on arousal, while calm is low on arousal.",
    "metadata": {
      "source": "6",
      "chunk_id": 8,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "ment, opinions, or evaluations. For example some words have positive connotations\n(wonderful ) while others have negative connotations ( dreary ). Even words whose\nmeanings are similar in other ways can vary in connotation; consider the difference\nin connotations between fake,knockoff ,forgery , on the one hand, and copy ,replica ,\nreproduction on the other, or innocent (positive connotation) and naive (negative\nconnotation). Some words describe positive evaluation ( great ,love) and others neg-\native evaluation ( terrible ,hate). Positive or negative evaluation language is called\nsentiment , as we saw in Chapter 4, and word sentiment plays a role in important sentiment\ntasks like sentiment analysis, stance detection, and applications of NLP to the lan-\nguage of politics and consumer reviews.\nEarly work on affective meaning (Osgood et al., 1957) found that words varied\nalong three important dimensions of affective meaning:\nvalence: the pleasantness of the stimulus\narousal: the intensity of emotion provoked by the stimulus\ndominance: the degree of control exerted by the stimulus\nThus words like happy orsatis\ufb01ed are high on valence, while unhappy oran-\nnoyed are low on valence. Excited is high on arousal, while calm is low on arousal.\nControlling is high on dominance, while awed orin\ufb02uenced are low on dominance.\nEach word is thus represented by three numbers, corresponding to its value on each\nof the three dimensions:",
    "metadata": {
      "source": "6",
      "chunk_id": 9,
      "token_count": 325,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\n6.2 \u2022 V ECTOR SEMANTICS 5\nValence Arousal Dominance\ncourageous 8.05 5.5 7.38\nmusic 7.67 5.57 6.5\nheartbreak 2.45 5.65 3.58\ncub 6.71 3.95 4.24\nOsgood et al. (1957) noticed that in using these 3 numbers to represent the\nmeaning of a word, the model was representing each word as a point in a three-\ndimensional space, a vector whose three dimensions corresponded to the word\u2019s\nrating on the three scales. This revolutionary idea that word meaning could be rep-\nresented as a point in space (e.g., that part of the meaning of heartbreak can be\nrepresented as the point [2:45;5:65;3:58]) was the \ufb01rst expression of the vector se-\nmantics models that we introduce next.\n6.2 Vector Semantics\nVector semantics is the standard way to represent word meaning in NLP, helpingvector\nsemantics\nus model many of the aspects of word meaning we saw in the previous section. The\nroots of the model lie in the 1950s when two big ideas converged: Osgood\u2019s 1957\nidea mentioned above to use a point in three-dimensional space to represent the\nconnotation of a word, and the proposal by linguists like Joos (1950), Harris (1954),\nand Firth (1957) to de\ufb01ne the meaning of a word by its distribution in language\nuse, meaning its neighboring words or grammatical environments. Their idea was\nthat two words that occur in very similar distributions (whose neighboring words are\nsimilar) have similar meanings.\nFor example, suppose you didn\u2019t know the meaning of the word ongchoi (a re-\ncent borrowing from Cantonese) but you see it in the following contexts:\n(6.1) Ongchoi is delicious sauteed with garlic.\n(6.2) Ongchoi is superb over rice.\n(6.3) ...ongchoi leaves with salty sauces...\nAnd suppose that you had seen many of these context words in other contexts:\n(6.4) ...spinach sauteed with garlic over rice...\n(6.5) ...chard stems and leaves are delicious...\n(6.6) ...collard greens and other salty leafy greens\nThe fact that ongchoi occurs with words like riceandgarlic anddelicious and\nsalty , as do words like spinach ,chard , and collard greens might suggest that ongchoi\nis a leafy green similar to these other leafy greens.1We can do the same thing\ncomputationally by just counting words in the context of ongchoi .\nThe idea of vector semantics is to represent a word as a point in a multidimen-\nsional semantic space that is derived (in ways we\u2019ll see) from the distributions of\nword neighbors. Vectors for representing words are called embeddings (although embeddings\nthe term is sometimes more strictly applied only to dense vectors like word2vec\n(Section 6.8), rather than sparse tf-idf or PPMI vectors (Section 6.3-Section 6.6)).\nThe word \u201cembedding\u201d derives from its mathematical sense as a mapping from one\nspace or structure to another, although the meaning has shifted; see the end of the\nchapter.\n1It\u2019s in fact Ipomoea aquatica , a relative of morning glory sometimes called water spinach in English.",
    "metadata": {
      "source": "6",
      "chunk_id": 10,
      "token_count": 766,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\ngoodnicebadworstnot good\nwonderfulamazingterri\ufb01cdislikeworsevery goodincredibly goodfantasticincredibly badnowyouithatwithbyto\u2019sareisathan\nFigure 6.1 A two-dimensional (t-SNE) projection of embeddings for some words and\nphrases, showing that words with similar meanings are nearby in space. The original 60-\ndimensional embeddings were trained for sentiment analysis. Simpli\ufb01ed from Li et al. (2015)\nwith colors added for explanation.\nFig. 6.1 shows a visualization of embeddings learned for sentiment analysis,\nshowing the location of selected words projected down from 60-dimensional space\ninto a two dimensional space. Notice the distinct regions containing positive words,\nnegative words, and neutral function words.\nThe \ufb01ne-grained model of word similarity of vector semantics offers enormous\npower to NLP applications. NLP applications like the sentiment classi\ufb01ers of Chap-\nter 4 or Chapter 5 depend on the same words appearing in the training and test sets.\nBut by representing words as embeddings, a classi\ufb01er can assign sentiment as long\nas it sees some words with similar meanings . And as we\u2019ll see, vector semantic\nmodels can be learned automatically from text without supervision.\nIn this chapter we\u2019ll introduce the two most commonly used models. In the tf-idf\nmodel, an important baseline, the meaning of a word is de\ufb01ned by a simple function\nof the counts of nearby words. We will see that this method results in very long\nvectors that are sparse , i.e. mostly zeros (since most words simply never occur in\nthe context of others). We\u2019ll introduce the word2vec model family for construct-\ning short, dense vectors that have useful semantic properties. We\u2019ll also introduce\nthecosine , the standard way to use embeddings to compute semantic similarity , be-\ntween two words, two sentences, or two documents, an important tool in practical\napplications like question answering, summarization, or automatic essay grading.\n6.3 Words and Vectors\n\u201cThe most important attributes of a vector in 3-space are fLocation, Location, Location g\u201d\nRandall Munroe, https://xkcd.com/2358/\nVector or distributional models of meaning are generally based on a co-occurrence\nmatrix , a way of representing how often words co-occur. We\u2019ll look at two popular\nmatrices: the term-document matrix and the term-term matrix.\n6.3.1 Vectors and documents\nIn aterm-document matrix , each row represents a word in the vocabulary and eachterm-document\nmatrix\ncolumn represents a document from some collection of documents. Fig. 6.2 shows a\nsmall selection from a term-document matrix showing the occurrence of four words\nin four plays by Shakespeare. Each cell in this matrix represents the number of times",
    "metadata": {
      "source": "6",
      "chunk_id": 11,
      "token_count": 627,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7",
    "metadata": {
      "source": "6",
      "chunk_id": 12,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "6.3 \u2022 W ORDS AND VECTORS 7\na particular word (de\ufb01ned by the row) occurs in a particular document (de\ufb01ned by\nthe column). Thus foolappeared 58 times in Twelfth Night .\nAs You Like It Twelfth Night Julius Caesar Henry V\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure 6.2 The term-document matrix for four words in four Shakespeare plays. Each cell\ncontains the number of times the (row) word occurs in the (column) document.\nThe term-document matrix of Fig. 6.2 was \ufb01rst de\ufb01ned as part of the vector\nspace model of information retrieval (Salton, 1971). In this model, a document isvector space\nmodel\nrepresented as a count vector, a column in Fig. 6.3.\nTo review some basic linear algebra, a vector is, at heart, just a list or array of vector\nnumbers. So As You Like It is represented as the list [1,114,36,20] (the \ufb01rst column\nvector in Fig. 6.3) and Julius Caesar is represented as the list [7,62,1,2] (the third\ncolumn vector). A vector space is a collection of vectors, and is characterized by vector space\nitsdimension . Vectors in a 3-dimensional vector space have an element for each dimension\ndimension of the space. We will loosely refer to a vector in a 4-dimensional space\nas a 4-dimensional vector, with one element along each dimension. In the example\nin Fig. 6.3, we\u2019ve chosen to make the document vectors of dimension 4, just so they\n\ufb01t on the page; in real term-document matrices, the document vectors would have\ndimensionalityjVj, the vocabulary size.\nThe ordering of the numbers in a vector space indicates the different dimensions\non which documents vary. The \ufb01rst dimension for both these vectors corresponds to\nthe number of times the word battle occurs, and we can compare each dimension,\nnoting for example that the vectors for As You Like It andTwelfth Night have similar\nvalues (1 and 0, respectively) for the \ufb01rst dimension.\nAs You Like It Twelfth Night Julius Caesar Henry V\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure 6.3 The term-document matrix for four words in four Shakespeare plays. The red\nboxes show that each document is represented as a column vector of length four.\nWe can think of the vector for a document as a point in jVj-dimensional space;\nthus the documents in Fig. 6.3 are points in 4-dimensional space. Since 4-dimensional\nspaces are hard to visualize, Fig. 6.4 shows a visualization in two dimensions; we\u2019ve\narbitrarily chosen the dimensions corresponding to the words battle andfool.\nTerm-document matrices were originally de\ufb01ned as a means of \ufb01nding similar\ndocuments for the task of document information retrieval . Two documents that are\nsimilar will tend to have similar words, and if two documents have similar words\ntheir column vectors will tend to be similar. The vectors for the comedies As You\nLike It [1,114,36,20] and Twelfth Night [0,80,58,15] look a lot more like each other",
    "metadata": {
      "source": "6",
      "chunk_id": 13,
      "token_count": 766,
      "chapter_title": ""
    }
  },
  {
    "content": "values (1 and 0, respectively) for the \ufb01rst dimension.\nAs You Like It Twelfth Night Julius Caesar Henry V\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure 6.3 The term-document matrix for four words in four Shakespeare plays. The red\nboxes show that each document is represented as a column vector of length four.\nWe can think of the vector for a document as a point in jVj-dimensional space;\nthus the documents in Fig. 6.3 are points in 4-dimensional space. Since 4-dimensional\nspaces are hard to visualize, Fig. 6.4 shows a visualization in two dimensions; we\u2019ve\narbitrarily chosen the dimensions corresponding to the words battle andfool.\nTerm-document matrices were originally de\ufb01ned as a means of \ufb01nding similar\ndocuments for the task of document information retrieval . Two documents that are\nsimilar will tend to have similar words, and if two documents have similar words\ntheir column vectors will tend to be similar. The vectors for the comedies As You\nLike It [1,114,36,20] and Twelfth Night [0,80,58,15] look a lot more like each other\n(more fools and wit than battles) than they look like Julius Caesar [7,62,1,2] or\nHenry V [13,89,4,3]. This is clear with the raw numbers; in the \ufb01rst dimension\n(battle) the comedies have low numbers and the others have high numbers, and we\ncan see it visually in Fig. 6.4; we\u2019ll see very shortly how to quantify this intuition\nmore formally.",
    "metadata": {
      "source": "6",
      "chunk_id": 14,
      "token_count": 379,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\n51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540\n354045505560\nFigure 6.4 A spatial visualization of the document vectors for the four Shakespeare play\ndocuments, showing just two of the dimensions, corresponding to the words battle andfool.\nThe comedies have high values for the fooldimension and low values for the battle dimension.\nA real term-document matrix, of course, wouldn\u2019t just have 4 rows and columns,\nlet alone 2. More generally, the term-document matrix has jVjrows (one for each\nword type in the vocabulary) and Dcolumns (one for each document in the collec-\ntion); as we\u2019ll see, vocabulary sizes are generally in the tens of thousands, and the\nnumber of documents can be enormous (think about all the pages on the web).\nInformation retrieval (IR) is the task of \ufb01nding the document dfrom the Dinformation\nretrieval\ndocuments in some collection that best matches a query q. For IR we\u2019ll therefore also\nrepresent a query by a vector, also of length jVj, and we\u2019ll need a way to compare\ntwo vectors to \ufb01nd how similar they are. (Doing IR will also require ef\ufb01cient ways\nto store and manipulate these vectors by making use of the convenient fact that these\nvectors are sparse, i.e., mostly zeros).\nLater in the chapter we\u2019ll introduce some of the components of this vector com-\nparison process: the tf-idf term weighting, and the cosine similarity metric.\n6.3.2 Words as vectors: document dimensions\nWe\u2019ve seen that documents can be represented as vectors in a vector space. But\nvector semantics can also be used to represent the meaning of words . We do this\nby associating each word with a word vector\u2014 a row vector rather than a column row vector\nvector, hence with different dimensions, as shown in Fig. 6.5. The four dimensions\nof the vector for fool, [36,58,1,4], correspond to the four Shakespeare plays. Word\ncounts in the same four dimensions are used to form the vectors for the other 3\nwords: wit, [20,15,2,3]; battle , [1,0,7,13]; and good [114,80,62,89].\nAs You Like It Twelfth Night Julius Caesar Henry V\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure 6.5 The term-document matrix for four words in four Shakespeare plays. The red\nboxes show that each word is represented as a row vector of length four.\nFor documents, we saw that similar documents had similar vectors, because sim-\nilar documents tend to have similar words. This same principle applies to words:\nsimilar words have similar vectors because they tend to occur in similar documents.\nThe term-document matrix thus lets us represent the meaning of a word by the doc-\numents it tends to occur in.",
    "metadata": {
      "source": "6",
      "chunk_id": 15,
      "token_count": 688,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\n6.3 \u2022 W ORDS AND VECTORS 9\n6.3.3 Words as vectors: word dimensions\nAn alternative to using the term-document matrix to represent words as vectors of\ndocument counts, is to use the term-term matrix , also called the word-word ma-\ntrixor the term-context matrix , in which the columns are labeled by words ratherword-word\nmatrix\nthan documents. This matrix is thus of dimensionality jVj\u0002jVjand each cell records\nthe number of times the row (target) word and the column (context) word co-occur\nin some context in some training corpus. The context could be the document, in\nwhich case the cell represents the number of times the two words appear in the same\ndocument. It is most common, however, to use smaller contexts, generally a win-\ndow around the word, for example of 4 words to the left and 4 words to the right,\nin which case the cell represents the number of times (in some training corpus) the\ncolumn word occurs in such a \u00064 word window around the row word. Here are four\nexamples of words in their windows:\nis traditionally followed by cherry pie, a traditional dessert\noften mixed, such as strawberry rhubarb pie. Apple pie\ncomputer peripherals and personal digital assistants. These devices usually\na computer. This includes information available on the internet\nIf we then take every occurrence of each word (say strawberry ) and count the\ncontext words around it, we get a word-word co-occurrence matrix. Fig. 6.6 shows a\nsimpli\ufb01ed subset of the word-word co-occurrence matrix for these four words com-\nputed from the Wikipedia corpus (Davies, 2015).\naardvark ... computer data result pie sugar ...\ncherry 0 ... 2 8 9 442 25 ...\nstrawberry 0 ... 0 0 1 60 19 ...\ndigital 0 ... 1670 1683 85 5 4 ...\ninformation 0 ... 3325 3982 378 5 13 ...\nFigure 6.6 Co-occurrence vectors for four words in the Wikipedia corpus, showing six of\nthe dimensions (hand-picked for pedagogical purposes). The vector for digital is outlined in\nred. Note that a real vector would have vastly more dimensions and thus be much sparser.\nNote in Fig. 6.6 that the two words cherry andstrawberry are more similar to\neach other (both pieandsugar tend to occur in their window) than they are to other\nwords like digital ; conversely, digital andinformation are more similar to each other\nthan, say, to strawberry . Fig. 6.7 shows a spatial visualization.\n100020003000400010002000digital [1683,1670]computer datainformation [3982,3325] 30004000\nFigure 6.7 A spatial visualization of word vectors for digital andinformation , showing just\ntwo of the dimensions, corresponding to the words data andcomputer .\nNote thatjVj, the dimensionality of the vector, is generally the size of the vo-\ncabulary, often between 10,000 and 50,000 words (using the most frequent words",
    "metadata": {
      "source": "6",
      "chunk_id": 16,
      "token_count": 696,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\nin the training corpus; keeping words after about the most frequent 50,000 or so is\ngenerally not helpful). Since most of these numbers are zero these are sparse vector\nrepresentations; there are ef\ufb01cient algorithms for storing and computing with sparse\nmatrices.\nNow that we have some intuitions, let\u2019s move on to examine the details of com-\nputing word similarity. Afterwards we\u2019ll discuss methods for weighting cells.\n6.4 Cosine for measuring similarity\nTo measure similarity between two target words vandw, we need a metric that\ntakes two vectors (of the same dimensionality, either both with words as dimensions,\nhence of lengthjVj, or both with documents as dimensions, of length jDj) and gives\na measure of their similarity. By far the most common similarity metric is the cosine\nof the angle between the vectors.\nThe cosine\u2014like most measures for vector similarity used in NLP\u2014is based on\nthedot product operator from linear algebra, also called the inner product : dot product\ninner product\ndot product (v;w) =v\u0001w=NX\ni=1viwi=v1w1+v2w2+:::+vNwN (6.7)\nThe dot product acts as a similarity metric because it will tend to be high just when\nthe two vectors have large values in the same dimensions. Alternatively, vectors that\nhave zeros in different dimensions\u2014orthogonal vectors\u2014will have a dot product of\n0, representing their strong dissimilarity.\nThis raw dot product, however, has a problem as a similarity metric: it favors\nlong vectors. The vector length is de\ufb01ned as vector length\njvj=vuutNX\ni=1v2\ni(6.8)\nThe dot product is higher if a vector is longer, with higher values in each dimension.\nMore frequent words have longer vectors, since they tend to co-occur with more\nwords and have higher co-occurrence values with each of them. The raw dot product\nthus will be higher for frequent words. But this is a problem; we\u2019d like a similarity\nmetric that tells us how similar two words are regardless of their frequency.\nWe modify the dot product to normalize for the vector length by dividing the\ndot product by the lengths of each of the two vectors. This normalized dot product\nturns out to be the same as the cosine of the angle between the two vectors, following\nfrom the de\ufb01nition of the dot product between two vectors aandb:\na\u0001b=jajjbjcosq\na\u0001b\njajjbj=cosq (6.9)\nThecosine similarity metric between two vectors vandwthus can be computed as: cosine",
    "metadata": {
      "source": "6",
      "chunk_id": 17,
      "token_count": 595,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\n6.5 \u2022 TF-IDF: W EIGHING TERMS IN THE VECTOR 11\ncosine (v;w) =v\u0001w\njvjjwj=NX\ni=1viwi\nvuutNX\ni=1v2\nivuutNX\ni=1w2\ni(6.10)\nFor some applications we pre-normalize each vector, by dividing it by its length,\ncreating a unit vector of length 1. Thus we could compute a unit vector from aby unit vector\ndividing it byjaj. For unit vectors, the dot product is the same as the cosine.\nThe cosine value ranges from 1 for vectors pointing in the same direction, through\n0 for orthogonal vectors, to -1 for vectors pointing in opposite directions. But since\nraw frequency values are non-negative, the cosine for these vectors ranges from 0\u20131.\nLet\u2019s see how the cosine computes which of the words cherry ordigital is closer\nin meaning to information , just using raw counts from the following shortened table:\npie data computer\ncherry 442 8 2\ndigital 5 1683 1670\ninformation 5 3982 3325\ncos(cherry ;information ) =442\u00035+8\u00033982+2\u00033325p\n4422+82+22p\n52+39822+33252=:018\ncos(digital ;information ) =5\u00035+1683\u00033982+1670\u00033325p\n52+16832+16702p\n52+39822+33252=:996\nThe model decides that information is way closer to digital than it is to cherry , a\nresult that seems sensible. Fig. 6.8 shows a visualization.\n50010001500200025003000500digitalcherryinformationDimension 1: \u2018pie\u2019\nDimension 2: \u2018computer\u2019\nFigure 6.8 A (rough) graphical demonstration of cosine similarity, showing vectors for\nthree words ( cherry ,digital , and information ) in the two dimensional space de\ufb01ned by counts\nof the words computer andpienearby. The \ufb01gure doesn\u2019t show the cosine, but it highlights the\nangles; note that the angle between digital andinformation is smaller than the angle between\ncherry andinformation . When two vectors are more similar, the cosine is larger but the angle\nis smaller; the cosine has its maximum (1) when the angle between two vectors is smallest\n(0\u000e); the cosine of all other angles is less than 1.\n6.5 TF-IDF: Weighing terms in the vector\nThe co-occurrence matrices above represent each cell by frequencies, either of words\nwith documents (Fig. 6.5), or words with other words (Fig. 6.6). But raw frequency",
    "metadata": {
      "source": "6",
      "chunk_id": 18,
      "token_count": 597,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12",
    "metadata": {
      "source": "6",
      "chunk_id": 19,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "12 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\nis not the best measure of association between words. Raw frequency is very skewed\nand not very discriminative. If we want to know what kinds of contexts are shared\nbycherry andstrawberry but not by digital andinformation , we\u2019re not going to get\ngood discrimination from words like the,it, orthey, which occur frequently with\nall sorts of words and aren\u2019t informative about any particular word. We saw this\nalso in Fig. 6.3 for the Shakespeare corpus; the dimension for the word good is not\nvery discriminative between plays; good is simply a frequent word and has roughly\nequivalent high frequencies in each of the plays.\nIt\u2019s a bit of a paradox. Words that occur nearby frequently (maybe pienearby\ncherry ) are more important than words that only appear once or twice. Yet words\nthat are too frequent\u2014ubiquitous, like theorgood \u2014 are unimportant. How can we\nbalance these two con\ufb02icting constraints?\nThere are two common solutions to this problem: in this section we\u2019ll describe\nthetf-idf weighting, usually used when the dimensions are documents. In the next\nsection we introduce the PPMI algorithm (usually used when the dimensions are\nwords).\nThetf-idf weighting (the \u2018-\u2019 here is a hyphen, not a minus sign) is the product\nof two terms, each term capturing one of these two intuitions:\nThe \ufb01rst is the term frequency (Luhn, 1957): the frequency of the word tin the term frequency\ndocument d. We can just use the raw count as the term frequency:\ntft;d=count (t;d) (6.11)\nMore commonly we squash the raw frequency a bit, by using the log 10of the fre-\nquency instead. The intuition is that a word appearing 100 times in a document\ndoesn\u2019t make that word 100 times more likely to be relevant to the meaning of the\ndocument. We also need to do something special with counts of 0, since we can\u2019t\ntake the log of 0.2\ntft;d=(\n1+log10count (t;d) if count (t;d)>0\n0 otherwise(6.12)\nIf we use log weighting, terms which occur 0 times in a document would have tf =0,\n1 times in a document tf =1+log10(1) =1+0=1, 10 times in a document tf =\n1+log10(10) =2, 100 times tf =1+log10(100) =3, 1000 times tf =4, and so on.\nThe second factor in tf-idf is used to give a higher weight to words that occur\nonly in a few documents. Terms that are limited to a few documents are useful\nfor discriminating those documents from the rest of the collection; terms that occur\nfrequently across the entire collection aren\u2019t as helpful. The document frequencydocument\nfrequency\ndftof a term tis the number of documents it occurs in. Document frequency is\nnot the same as the collection frequency of a term, which is the total number of\ntimes the word appears in the whole collection in any document. Consider in the\ncollection of Shakespeare\u2019s 37 plays the two words Romeo andaction . The words\nhave identical collection frequencies (they both occur 113 times in all the plays) but\nvery different document frequencies, since Romeo only occurs in a single play. If\nour goal is to \ufb01nd documents about the romantic tribulations of Romeo, the word\nRomeo should be highly weighted, but not action :\nCollection Frequency Document Frequency",
    "metadata": {
      "source": "6",
      "chunk_id": 20,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "1 times in a document tf =1+log10(1) =1+0=1, 10 times in a document tf =\n1+log10(10) =2, 100 times tf =1+log10(100) =3, 1000 times tf =4, and so on.\nThe second factor in tf-idf is used to give a higher weight to words that occur\nonly in a few documents. Terms that are limited to a few documents are useful\nfor discriminating those documents from the rest of the collection; terms that occur\nfrequently across the entire collection aren\u2019t as helpful. The document frequencydocument\nfrequency\ndftof a term tis the number of documents it occurs in. Document frequency is\nnot the same as the collection frequency of a term, which is the total number of\ntimes the word appears in the whole collection in any document. Consider in the\ncollection of Shakespeare\u2019s 37 plays the two words Romeo andaction . The words\nhave identical collection frequencies (they both occur 113 times in all the plays) but\nvery different document frequencies, since Romeo only occurs in a single play. If\nour goal is to \ufb01nd documents about the romantic tribulations of Romeo, the word\nRomeo should be highly weighted, but not action :\nCollection Frequency Document Frequency\nRomeo 113 1\naction 113 31\n2We can also use this alternative formulation, which we have used in earlier editions: tf t;d=\nlog10(count (t;d)+1)",
    "metadata": {
      "source": "6",
      "chunk_id": 21,
      "token_count": 320,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\n6.5 \u2022 TF-IDF: W EIGHING TERMS IN THE VECTOR 13\nWe emphasize discriminative words like Romeo via the inverse document fre-\nquency oridfterm weight (Sparck Jones, 1972). The idf is de\ufb01ned using the frac- idf\ntionN=dft, where Nis the total number of documents in the collection, and df tis\nthe number of documents in which term toccurs. The fewer documents in which a\nterm occurs, the higher this weight. The lowest weight of 1 is assigned to terms that\noccur in all the documents. It\u2019s usually clear what counts as a document: in Shake-\nspeare we would use a play; when processing a collection of encyclopedia articles\nlike Wikipedia, the document is a Wikipedia page; in processing newspaper articles,\nthe document is a single article. Occasionally your corpus might not have appropri-\nate document divisions and you might need to break up the corpus into documents\nyourself for the purposes of computing idf.\nBecause of the large number of documents in many collections, this measure\ntoo is usually squashed with a log function. The resulting de\ufb01nition for inverse\ndocument frequency (idf) is thus\nidft=log10\u0012N\ndft\u0013\n(6.13)\nHere are some idf values for some words in the Shakespeare corpus, (along with\nthe document frequency df values on which they are based) ranging from extremely\ninformative words which occur in only one play like Romeo , to those that occur in a\nfew like salad orFalstaff , to those which are very common like foolor so common\nas to be completely non-discriminative since they occur in all 37 plays like good or\nsweet .3\nWord df idf\nRomeo 1 1.57\nsalad 2 1.27\nFalstaff 4 0.967\nforest 12 0.489\nbattle 21 0.246\nwit 34 0.037\nfool 36 0.012\ngood 37 0\nsweet 37 0\nThe tf-idf weighted value wt;dfor word tin document dthus combines term tf-idf\nfrequency tf t;d(de\ufb01ned either by Eq. 6.11 or by Eq. 6.12) with idf from Eq. 6.13:\nwt;d=tft;d\u0002idft (6.14)\nFig. 6.9 applies tf-idf weighting to the Shakespeare term-document matrix in Fig. 6.2,\nusing the tf equation Eq. 6.12. Note that the tf-idf values for the dimension corre-\nsponding to the word good have now all become 0; since this word appears in every\ndocument, the tf-idf weighting leads it to be ignored. Similarly, the word fool, which\nappears in 36 out of the 37 plays, has a much lower weight.\nThe tf-idf weighting is the way for weighting co-occurrence matrices in infor-\nmation retrieval, but also plays a role in many other aspects of natural language\nprocessing. It\u2019s also a great baseline, the simple thing to try \ufb01rst. We\u2019ll look at other\nweightings like PPMI (Positive Pointwise Mutual Information) in Section 6.6.\n3Sweet was one of Shakespeare\u2019s favorite adjectives, a fact probably related to the increased use of\nsugar in European recipes around the turn of the 16th century (Jurafsky, 2014, p. 175).",
    "metadata": {
      "source": "6",
      "chunk_id": 22,
      "token_count": 758,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14",
    "metadata": {
      "source": "6",
      "chunk_id": 23,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "14 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\nAs You Like It Twelfth Night Julius Caesar Henry V\nbattle 0.246 0 0.454 0.520\ngood 0 0 0 0\nfool 0.030 0.033 0.0012 0.0019\nwit 0.085 0.081 0.048 0.054\nFigure 6.9 A portion of the tf-idf weighted term-document matrix for four words in Shake-\nspeare plays, showing a selection of 4 plays, using counts from Fig. 6.2. For example the\n0:085 value for witinAs You Like It is the product of tf =1+log10(20) =2:301 and idf =:037.\nNote that the idf weighting has eliminated the importance of the ubiquitous word good and\nvastly reduced the impact of the almost-ubiquitous word fool.\n6.6 Pointwise Mutual Information (PMI)\nAn alternative weighting function to tf-idf, PPMI (positive pointwise mutual infor-\nmation), is used for term-term-matrices, when the vector dimensions correspond to\nwords rather than documents. PPMI draws on the intuition that the best way to weigh\nthe association between two words is to ask how much more the two words co-occur\nin our corpus than we would have a priori expected them to appear by chance.\nPointwise mutual information (Fano, 1961)4is one of the most important con-pointwise\nmutual\ninformationcepts in NLP. It is a measure of how often two events xandyoccur, compared with\nwhat we would expect if they were independent:\nI(x;y) =log2P(x;y)\nP(x)P(y)(6.16)\nThe pointwise mutual information between a target word wand a context word\nc(Church and Hanks 1989, Church and Hanks 1990) is then de\ufb01ned as:\nPMI(w;c) =log2P(w;c)\nP(w)P(c)(6.17)\nThe numerator tells us how often we observed the two words together (assuming\nwe compute probability by using the MLE). The denominator tells us how often\nwe would expect the two words to co-occur assuming they each occurred indepen-\ndently; recall that the probability of two independent events both occurring is just\nthe product of the probabilities of the two events. Thus, the ratio gives us an esti-\nmate of how much more the two words co-occur than we expect by chance. PMI is\na useful tool whenever we need to \ufb01nd words that are strongly associated.\nPMI values range from negative to positive in\ufb01nity. But negative PMI values\n(which imply things are co-occurring less often than we would expect by chance)\ntend to be unreliable unless our corpora are enormous. To distinguish whether\ntwo words whose individual probability is each 10\u00006occur together less often than\nchance, we would need to be certain that the probability of the two occurring to-\ngether is signi\ufb01cantly less than 10\u000012, and this kind of granularity would require an\nenormous corpus. Furthermore it\u2019s not clear whether it\u2019s even possible to evaluate\nsuch scores of \u2018unrelatedness\u2019 with human judgments. For this reason it is more\n4PMI is based on the mutual information between two random variables XandY, de\ufb01ned as:\nI(X;Y) =X\nxX\nyP(x;y)log2P(x;y)",
    "metadata": {
      "source": "6",
      "chunk_id": 24,
      "token_count": 774,
      "chapter_title": ""
    }
  },
  {
    "content": "we would expect the two words to co-occur assuming they each occurred indepen-\ndently; recall that the probability of two independent events both occurring is just\nthe product of the probabilities of the two events. Thus, the ratio gives us an esti-\nmate of how much more the two words co-occur than we expect by chance. PMI is\na useful tool whenever we need to \ufb01nd words that are strongly associated.\nPMI values range from negative to positive in\ufb01nity. But negative PMI values\n(which imply things are co-occurring less often than we would expect by chance)\ntend to be unreliable unless our corpora are enormous. To distinguish whether\ntwo words whose individual probability is each 10\u00006occur together less often than\nchance, we would need to be certain that the probability of the two occurring to-\ngether is signi\ufb01cantly less than 10\u000012, and this kind of granularity would require an\nenormous corpus. Furthermore it\u2019s not clear whether it\u2019s even possible to evaluate\nsuch scores of \u2018unrelatedness\u2019 with human judgments. For this reason it is more\n4PMI is based on the mutual information between two random variables XandY, de\ufb01ned as:\nI(X;Y) =X\nxX\nyP(x;y)log2P(x;y)\nP(x)P(y)(6.15)\nIn a confusion of terminology, Fano used the phrase mutual information to refer to what we now call\npointwise mutual information and the phrase expectation of the mutual information for what we now call\nmutual information",
    "metadata": {
      "source": "6",
      "chunk_id": 25,
      "token_count": 339,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\n6.6 \u2022 P OINTWISE MUTUAL INFORMATION (PMI) 15\ncommon to use Positive PMI (called PPMI ) which replaces all negative PMI values PPMI\nwith zero (Church and Hanks 1989, Dagan et al. 1993, Niwa and Nitta 1994)5:\nPPMI (w;c) =max(log2P(w;c)\nP(w)P(c);0) (6.18)\nMore formally, let\u2019s assume we have a co-occurrence matrix F with W rows (words)\nand C columns (contexts), where fi jgives the number of times word wioccurs with\ncontext cj. This can be turned into a PPMI matrix where PPMI i jgives the PPMI\nvalue of word wiwith context cj(which we can also express as PPMI( wi;cj) or\nPPMI( w=i;c=j)) as follows:\npi j=fi jPW\ni=1PC\nj=1fi j;pi\u0003=PC\nj=1fi jPW\ni=1PC\nj=1fi j;p\u0003j=PW\ni=1fi jPW\ni=1PC\nj=1fi j(6.19)\nPPMI i j=max(log2pi j\npi\u0003p\u0003j;0) (6.20)\nLet\u2019s see some PPMI calculations. We\u2019ll use Fig. 6.10, which repeats Fig. 6.6 plus\nall the count marginals, and let\u2019s pretend for ease of calculation that these are the\nonly words/contexts that matter.\ncomputer data result pie sugar count(w)\ncherry 2 8 9 442 25 486\nstrawberry 0 0 1 60 19 80\ndigital 1670 1683 85 5 4 3447\ninformation 3325 3982 378 5 13 7703\ncount(context) 4997 5673 473 512 61 11716\nFigure 6.10 Co-occurrence counts for four words in 5 contexts in the Wikipedia corpus,\ntogether with the marginals, pretending for the purpose of this calculation that no other\nwords/contexts matter.\nThus for example we could compute PPMI(information,data), assuming we pre-\ntended that Fig. 6.6 encompassed all the relevant word contexts/dimensions, as fol-\nlows:\nP(w=information, c=data ) =3982\n11716=:3399\nP(w=information ) =7703\n11716=:6575\nP(c=data ) =5673\n11716=:4842\nPPMI (information,data ) = log2(:3399 =(:6575\u0003:4842)) = :0944\nFig. 6.11 shows the joint probabilities computed from the counts in Fig. 6.10, and\nFig. 6.12 shows the PPMI values. Not surprisingly, cherry andstrawberry are highly\nassociated with both pieandsugar , and data is mildly associated with information .\nPMI has the problem of being biased toward infrequent events; very rare words\ntend to have very high PMI values. One way to reduce this bias toward low frequency\n5Positive PMI also cleanly solves the problem of what to do with zero counts, using 0 to replace the\n\u0000\u00a5from log (0).",
    "metadata": {
      "source": "6",
      "chunk_id": 26,
      "token_count": 736,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\n16 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\np(w,context) p(w)\ncomputer data result pie sugar p(w)\ncherry 0.0002 0.0007 0.0008 0.0377 0.0021 0.0415\nstrawberry 0.0000 0.0000 0.0001 0.0051 0.0016 0.0068\ndigital 0.1425 0.1436 0.0073 0.0004 0.0003 0.2942\ninformation 0.2838 0.3399 0.0323 0.0004 0.0011 0.6575\np(context) 0.4265 0.4842 0.0404 0.0437 0.0052\nFigure 6.11 Replacing the counts in Fig. 6.6 with joint probabilities, showing the marginals\nin the right column and the bottom row.\ncomputer data result pie sugar\ncherry 0 0 0 4.38 3.30\nstrawberry 0 0 0 4.10 5.51\ndigital 0.18 0.01 0 0 0\ninformation 0.02 0.09 0.28 0 0\nFigure 6.12 The PPMI matrix showing the association between words and context words,\ncomputed from the counts in Fig. 6.11. Note that most of the 0 PPMI values are ones that had\na negative PMI; for example PMI( cherry,computer ) = -6.7, meaning that cherry andcomputer\nco-occur on Wikipedia less often than we would expect by chance, and with PPMI we replace\nnegative values by zero.\nevents is to slightly change the computation for P(c), using a different function Pa(c)\nthat raises the probability of the context word to the power of a:\nPPMI a(w;c) =max(log2P(w;c)\nP(w)Pa(c);0) (6.21)\nPa(c) =count (c)a\nP\nccount (c)a(6.22)\nLevy et al. (2015) found that a setting of a=0:75 improved performance of\nembeddings on a wide range of tasks (drawing on a similar weighting used for skip-\ngrams described below in Eq. 6.32). This works because raising the count to a=\n0:75 increases the probability assigned to rare contexts, and hence lowers their PMI\n(Pa(c)>P(c)when cis rare).\nAnother possible solution is Laplace smoothing: Before computing PMI, a small\nconstant k(values of 0.1-3 are common) is added to each of the counts, shrinking\n(discounting) all the non-zero values. The larger the k, the more the non-zero counts\nare discounted.\n6.7 Applications of the tf-idf or PPMI vector models\nIn summary, the vector semantics model we\u2019ve described so far represents a target\nword as a vector with dimensions corresponding either to the documents in a large\ncollection (the term-document matrix) or to the counts of words in some neighboring\nwindow (the term-term matrix). The values in each dimension are counts, weighted\nby tf-idf (for term-document matrices) or PPMI (for term-term matrices), and the\nvectors are sparse (since most values are zero).\nThe model computes the similarity between two words xandyby taking the\ncosine of their tf-idf or PPMI vectors; high cosine, high similarity. This entire model",
    "metadata": {
      "source": "6",
      "chunk_id": 27,
      "token_count": 792,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\n6.8 \u2022 W ORD2VEC 17\nis sometimes referred to as the tf-idf model or the PPMI model, after the weighting\nfunction.\nThe tf-idf model of meaning is often used for document functions like deciding\nif two documents are similar. We represent a document by taking the vectors of\nall the words in the document, and computing the centroid of all those vectors. centroid\nThe centroid is the multidimensional version of the mean; the centroid of a set of\nvectors is a single vector that has the minimum sum of squared distances to each of\nthe vectors in the set. Given kword vectors w1;w2;:::;wk, the centroid document\nvector dis:document\nvector\nd=w1+w2+:::+wk\nk(6.23)\nGiven two documents, we can then compute their document vectors d1andd2, and\nestimate the similarity between the two documents by cos (d1;d2). Document sim-\nilarity is also useful for all sorts of applications; information retrieval, plagiarism\ndetection, news recommender systems, and even for digital humanities tasks like\ncomparing different versions of a text to see which are similar to each other.\nEither the PPMI model or the tf-idf model can be used to compute word simi-\nlarity, for tasks like \ufb01nding word paraphrases, tracking changes in word meaning, or\nautomatically discovering meanings of words in different corpora. For example, we\ncan \ufb01nd the 10 most similar words to any target word wby computing the cosines\nbetween wand each of the V\u00001 other words, sorting, and looking at the top 10.\n6.8 Word2vec\nIn the previous sections we saw how to represent a word as a sparse, long vector with\ndimensions corresponding to words in the vocabulary or documents in a collection.\nWe now introduce a more powerful word representation: embeddings , short dense\nvectors. Unlike the vectors we\u2019ve seen so far, embeddings are short , with number\nof dimensions dranging from 50-1000, rather than the much larger vocabulary size\njVjor number of documents Dwe\u2019ve seen. These ddimensions don\u2019t have a clear\ninterpretation. And the vectors are dense : instead of vector entries being sparse,\nmostly-zero counts or functions of counts, the values will be real-valued numbers\nthat can be negative.\nIt turns out that dense vectors work better in every NLP task than sparse vectors.\nWhile we don\u2019t completely understand all the reasons for this, we have some intu-\nitions. Representing words as 300-dimensional dense vectors requires our classi\ufb01ers\nto learn far fewer weights than if we represented words as 50,000-dimensional vec-\ntors, and the smaller parameter space possibly helps with generalization and avoid-\ning over\ufb01tting. Dense vectors may also do a better job of capturing synonymy.\nFor example, in a sparse vector representation, dimensions for synonyms like car\nandautomobile dimension are distinct and unrelated; sparse vectors may thus fail\nto capture the similarity between a word with caras a neighbor and a word with\nautomobile as a neighbor.\nIn this section we introduce one method for computing embeddings: skip-gram skip-gram\nwith negative sampling , sometimes called SGNS . The skip-gram algorithm is one SGNS\nof two algorithms in a software package called word2vec , and so sometimes the word2vec\nalgorithm is loosely referred to as word2vec (Mikolov et al. 2013a, Mikolov et al.\n2013b). The word2vec methods are fast, ef\ufb01cient to train, and easily available on-",
    "metadata": {
      "source": "6",
      "chunk_id": 28,
      "token_count": 777,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18",
    "metadata": {
      "source": "6",
      "chunk_id": 29,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "18 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\nline with code and pretrained embeddings. Word2vec embeddings are static em-\nbeddings , meaning that the method learns one \ufb01xed embedding for each word in thestatic\nembeddings\nvocabulary. In Chapter 11 we\u2019ll introduce methods for learning dynamic contextual\nembeddings like the popular family of BERT representations, in which the vector\nfor each word is different in different contexts.\nThe intuition of word2vec is that instead of counting how often each word woc-\ncurs near, say, apricot , we\u2019ll instead train a classi\ufb01er on a binary prediction task: \u201cIs\nword wlikely to show up near apricot ?\u201d We don\u2019t actually care about this prediction\ntask; instead we\u2019ll take the learned classi\ufb01er weights as the word embeddings.\nThe revolutionary intuition here is that we can just use running text as implicitly\nsupervised training data for such a classi\ufb01er; a word cthat occurs near the target\nword apricot acts as gold \u2018correct answer\u2019 to the question \u201cIs word clikely to show\nup near apricot ?\u201d This method, often called self-supervision , avoids the need for self-supervision\nany sort of hand-labeled supervision signal. This idea was \ufb01rst proposed in the task\nof neural language modeling, when Bengio et al. (2003) and Collobert et al. (2011)\nshowed that a neural language model (a neural network that learned to predict the\nnext word from prior words) could just use the next word in running text as its\nsupervision signal, and could be used to learn an embedding representation for each\nword as part of doing this prediction task.\nWe\u2019ll see how to do neural networks in the next chapter, but word2vec is a\nmuch simpler model than the neural network language model, in two ways. First,\nword2vec simpli\ufb01es the task (making it binary classi\ufb01cation instead of word pre-\ndiction). Second, word2vec simpli\ufb01es the architecture (training a logistic regression\nclassi\ufb01er instead of a multi-layer neural network with hidden layers that demand\nmore sophisticated training algorithms). The intuition of skip-gram is:\n1. Treat the target word and a neighboring context word as positive examples.\n2. Randomly sample other words in the lexicon to get negative samples.\n3. Use logistic regression to train a classi\ufb01er to distinguish those two cases.\n4. Use the learned weights as the embeddings.\n6.8.1 The classi\ufb01er\nLet\u2019s start by thinking about the classi\ufb01cation task, and then turn to how to train.\nImagine a sentence like the following, with a target word apricot , and assume we\u2019re\nusing a window of \u00062 context words:\n... lemon, a [tablespoon of apricot jam, a] pinch ...\nc1 c2 w c3 c4\nOur goal is to train a classi\ufb01er such that, given a tuple (w;c)of a target word\nwpaired with a candidate context word c(for example ( apricot ,jam), or perhaps\n(apricot ,aardvark )) it will return the probability that cis a real context word (true\nforjam, false for aardvark ):\nP(+jw;c) (6.24)\nThe probability that word cis not a real context word for wis just 1 minus\nEq. 6.24:\nP(\u0000jw;c) =1\u0000P(+jw;c) (6.25)",
    "metadata": {
      "source": "6",
      "chunk_id": 30,
      "token_count": 767,
      "chapter_title": ""
    }
  },
  {
    "content": "1. Treat the target word and a neighboring context word as positive examples.\n2. Randomly sample other words in the lexicon to get negative samples.\n3. Use logistic regression to train a classi\ufb01er to distinguish those two cases.\n4. Use the learned weights as the embeddings.\n6.8.1 The classi\ufb01er\nLet\u2019s start by thinking about the classi\ufb01cation task, and then turn to how to train.\nImagine a sentence like the following, with a target word apricot , and assume we\u2019re\nusing a window of \u00062 context words:\n... lemon, a [tablespoon of apricot jam, a] pinch ...\nc1 c2 w c3 c4\nOur goal is to train a classi\ufb01er such that, given a tuple (w;c)of a target word\nwpaired with a candidate context word c(for example ( apricot ,jam), or perhaps\n(apricot ,aardvark )) it will return the probability that cis a real context word (true\nforjam, false for aardvark ):\nP(+jw;c) (6.24)\nThe probability that word cis not a real context word for wis just 1 minus\nEq. 6.24:\nP(\u0000jw;c) =1\u0000P(+jw;c) (6.25)\nHow does the classi\ufb01er compute the probability P? The intuition of the skip-\ngram model is to base this probability on embedding similarity: a word is likely to",
    "metadata": {
      "source": "6",
      "chunk_id": 31,
      "token_count": 319,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\n6.8 \u2022 W ORD2VEC 19\noccur near the target if its embedding vector is similar to the target embedding. To\ncompute similarity between these dense embeddings, we rely on the intuition that\ntwo vectors are similar if they have a high dot product (after all, cosine is just a\nnormalized dot product). In other words:\nSimilarity (w;c)\u0019c\u0001w (6.26)\nThe dot product c\u0001wis not a probability, it\u2019s just a number ranging from \u0000\u00a5to\u00a5\n(since the elements in word2vec embeddings can be negative, the dot product can be\nnegative). To turn the dot product into a probability, we\u2019ll use the logistic orsigmoid\nfunction s(x), the fundamental core of logistic regression:\ns(x) =1\n1+exp(\u0000x)(6.27)\nWe model the probability that word cis a real context word for target word was:\nP(+jw;c) = s(c\u0001w) =1\n1+exp(\u0000c\u0001w)(6.28)\nThe sigmoid function returns a number between 0 and 1, but to make it a probability\nwe\u2019ll also need the total probability of the two possible events ( cis a context word,\nandcisn\u2019t a context word) to sum to 1. We thus estimate the probability that word c\nis not a real context word for was:\nP(\u0000jw;c) = 1\u0000P(+jw;c)\n=s(\u0000c\u0001w) =1\n1+exp(c\u0001w)(6.29)\nEquation 6.28 gives us the probability for one word, but there are many context\nwords in the window. Skip-gram makes the simplifying assumption that all context\nwords are independent, allowing us to just multiply their probabilities:\nP(+jw;c1:L) =LY\ni=1s(ci\u0001w) (6.30)\nlogP(+jw;c1:L) =LX\ni=1logs(ci\u0001w) (6.31)\nIn summary, skip-gram trains a probabilistic classi\ufb01er that, given a test target word\nwand its context window of Lwords c1:L, assigns a probability based on how similar\nthis context window is to the target word. The probability is based on applying the\nlogistic (sigmoid) function to the dot product of the embeddings of the target word\nwith each context word. To compute this probability, we just need embeddings for\neach target word and context word in the vocabulary.\nFig. 6.13 shows the intuition of the parameters we\u2019ll need. Skip-gram actually\nstores two embeddings for each word, one for the word as a target, and one for the\nword considered as context. Thus the parameters we need to learn are two matrices\nWandC, each containing an embedding for every one of the jVjwords in the\nvocabulary V.6Let\u2019s now turn to learning these embeddings (which is the real goal\nof training this classi\ufb01er in the \ufb01rst place).\n6In principle the target matrix and the context matrix could use different vocabularies, but we\u2019ll simplify\nby assuming one shared vocabulary V.",
    "metadata": {
      "source": "6",
      "chunk_id": 32,
      "token_count": 675,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\n20 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\n1WCaardvark\nzebrazebraaardvarkapricotapricot|V||V|+12V& =target wordscontext & noisewords\u2026\n\u20261..d\u2026\n\u2026\nFigure 6.13 The embeddings learned by the skipgram model. The algorithm stores two\nembeddings for each word, the target embedding (sometimes called the input embedding)\nand the context embedding (sometimes called the output embedding). The parameter qthat\nthe algorithm learns is thus a matrix of 2 jVjvectors, each of dimension d, formed by concate-\nnating two matrices, the target embeddings Wand the context+noise embeddings C.\n6.8.2 Learning skip-gram embeddings\nThe learning algorithm for skip-gram embeddings takes as input a corpus of text,\nand a chosen vocabulary size N. It begins by assigning a random embedding vector\nfor each of the N vocabulary words, and then proceeds to iteratively shift the em-\nbedding of each word wto be more like the embeddings of words that occur nearby\nin texts, and less like the embeddings of words that don\u2019t occur nearby. Let\u2019s start\nby considering a single piece of training data:\n... lemon, a [tablespoon of apricot jam, a] pinch ...\nc1 c2 w c3 c4\nThis example has a target word w(apricot), and 4 context words in the L=\u00062\nwindow, resulting in 4 positive training instances (on the left below):\npositive examples +\nw c pos\napricot tablespoon\napricot of\napricot jam\napricot anegative examples -\nw c neg w c neg\napricot aardvark apricot seven\napricot my apricot forever\napricot where apricot dear\napricot coaxial apricot if\nFor training a binary classi\ufb01er we also need negative examples. In fact skip-\ngram with negative sampling (SGNS) uses more negative examples than positive\nexamples (with the ratio between them set by a parameter k). So for each of these\n(w;cpos)training instances we\u2019ll create knegative samples, each consisting of the\ntarget wplus a \u2018noise word\u2019 cneg. A noise word is a random word from the lexicon,\nconstrained not to be the target word w. The right above shows the setting where\nk=2, so we\u2019ll have 2 negative examples in the negative training set \u0000for each\npositive example w;cpos.\nThe noise words are chosen according to their weighted unigram frequency\npa(w), where ais a weight. If we were sampling according to unweighted fre-\nquency p(w), it would mean that with unigram probability p(\u201cthe\u201d)we would choose\nthe word theas a noise word, with unigram probability p(\u201caardvark \u201d)we would\nchoose aardvark , and so on. But in practice it is common to set a=0:75, i.e. use",
    "metadata": {
      "source": "6",
      "chunk_id": 33,
      "token_count": 651,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21\n\n6.8 \u2022 W ORD2VEC 21\nthe weighting p3\n4(w):\nPa(w) =count (w)a\nP\nw0count (w0)a(6.32)\nSetting a=:75 gives better performance because it gives rare noise words slightly\nhigher probability: for rare words, Pa(w)>P(w). To illustrate this intuition, it\nmight help to work out the probabilities for an example with a=:75 and two events,\nP(a) =0:99 and P(b) =0:01:\nPa(a) =:99:75\n:99:75+:01:75=0:97\nPa(b) =:01:75\n:99:75+:01:75=0:03 (6.33)\nThus using a=:75 increases the probability of the rare event bfrom 0.01 to 0.03.\nGiven the set of positive and negative training instances, and an initial set of\nembeddings, the goal of the learning algorithm is to adjust those embeddings to\n\u2022 Maximize the similarity of the target word, context word pairs (w;cpos)drawn\nfrom the positive examples\n\u2022 Minimize the similarity of the (w;cneg)pairs from the negative examples.\nIf we consider one word/context pair (w;cpos)with its knoise words cneg1:::cnegk,\nwe can express these two goals as the following loss function Lto be minimized\n(hence the\u0000); here the \ufb01rst term expresses that we want the classi\ufb01er to assign the\nreal context word cposa high probability of being a neighbor, and the second term\nexpresses that we want to assign each of the noise words cnegia high probability of\nbeing a non-neighbor, all multiplied because we assume independence:\nL=\u0000log\"\nP(+jw;cpos)kY\ni=1P(\u0000jw;cnegi)#\n=\u0000\"\nlogP(+jw;cpos)+kX\ni=1logP(\u0000jw;cnegi)#\n=\u0000\"\nlogP(+jw;cpos)+kX\ni=1log\u0000\n1\u0000P(+jw;cnegi)\u0001#\n=\u0000\"\nlogs(cpos\u0001w)+kX\ni=1logs(\u0000cnegi\u0001w)#\n(6.34)\nThat is, we want to maximize the dot product of the word with the actual context\nwords, and minimize the dot products of the word with the knegative sampled non-\nneighbor words.\nWe minimize this loss function using stochastic gradient descent. Fig. 6.14\nshows the intuition of one step of learning.\nTo get the gradient, we need to take the derivative of Eq. 6.34 with respect to\nthe different embeddings. It turns out the derivatives are the following (we leave the",
    "metadata": {
      "source": "6",
      "chunk_id": 34,
      "token_count": 605,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22\n\n22 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\nWCmove apricot and jam closer,increasing cpos z waardvark\nmove apricot and matrix apartdecreasing cneg1 z w\u201c\u2026apricot jam\u2026\u201dw\nzebrazebraaardvarkjamapricot\ncposmatrixTolstoymove apricot and Tolstoy apartdecreasing cneg2 z w!cneg1cneg2k=2\nFigure 6.14 Intuition of one step of gradient descent. The skip-gram model tries to shift\nembeddings so the target embeddings (here for apricot ) are closer to (have a higher dot prod-\nuct with) context embeddings for nearby words (here jam) and further from (lower dot product\nwith) context embeddings for noise words that don\u2019t occur nearby (here Tolstoy andmatrix ).\nproof as an exercise at the end of the chapter):\n\u00b6L\n\u00b6cpos= [s(cpos\u0001w)\u00001]w (6.35)\n\u00b6L\n\u00b6cneg= [s(cneg\u0001w)]w (6.36)\n\u00b6L\n\u00b6w= [s(cpos\u0001w)\u00001]cpos+kX\ni=1[s(cnegi\u0001w)]cnegi(6.37)\nThe update equations going from time step ttot+1 in stochastic gradient descent\nare thus:\nct+1\npos=ct\npos\u0000h[s(ct\npos\u0001wt)\u00001]wt(6.38)\nct+1\nneg=ct\nneg\u0000h[s(ct\nneg\u0001wt)]wt(6.39)\nwt+1=wt\u0000h\"\n[s(ct\npos\u0001wt)\u00001]ct\npos+kX\ni=1[s(ct\nnegi\u0001wt)]ct\nnegi#\n(6.40)\nJust as in logistic regression, then, the learning algorithm starts with randomly ini-\ntialized WandCmatrices, and then walks through the training corpus using gradient\ndescent to move WandCso as to minimize the loss in Eq. 6.34 by making the up-\ndates in (Eq. 6.38)-(Eq. 6.40).\nRecall that the skip-gram model learns twoseparate embeddings for each word i:\nthetarget embedding wiand the context embedding ci, stored in two matrices, thetarget\nembeddingcontext\nembedding target matrix Wand the context matrix C. It\u2019s common to just add them together,\nrepresenting word iwith the vector wi+ci. Alternatively we can throw away the C\nmatrix and just represent each word iby the vector wi.\nAs with the simple count-based methods like tf-idf, the context window size L\naffects the performance of skip-gram embeddings, and experiments often tune the\nparameter Lon a devset.",
    "metadata": {
      "source": "6",
      "chunk_id": 35,
      "token_count": 604,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23",
    "metadata": {
      "source": "6",
      "chunk_id": 36,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "6.9 \u2022 V ISUALIZING EMBEDDINGS 23\n6.8.3 Other kinds of static embeddings\nThere are many kinds of static embeddings. An extension of word2vec, fasttext fasttext\n(Bojanowski et al., 2017), addresses a problem with word2vec as we have presented\nit so far: it has no good way to deal with unknown words \u2014words that appear in\na test corpus but were unseen in the training corpus. A related problem is word\nsparsity, such as in languages with rich morphology, where some of the many forms\nfor each noun and verb may only occur rarely. Fasttext deals with these problems\nby using subword models, representing each word as itself plus a bag of constituent\nn-grams, with special boundary symbols <and>added to each word. For example,\nwith n=3 the word where would be represented by the sequence <where> plus the\ncharacter n-grams:\n<wh, whe, her, ere, re>\nThen a skipgram embedding is learned for each constituent n-gram, and the word\nwhere is represented by the sum of all of the embeddings of its constituent n-grams.\nUnknown words can then be presented only by the sum of the constituent n-grams.\nA fasttext open-source library, including pretrained embeddings for 157 languages,\nis available at https://fasttext.cc .\nAnother very widely used static embedding model is GloVe (Pennington et al.,\n2014), short for Global Vectors, because the model is based on capturing global\ncorpus statistics. GloVe is based on ratios of probabilities from the word-word co-\noccurrence matrix, combining the intuitions of count-based models like PPMI while\nalso capturing the linear structures used by methods like word2vec.\nIt turns out that dense embeddings like word2vec actually have an elegant math-\nematical relationship with sparse embeddings like PPMI, in which word2vec can\nbe seen as implicitly optimizing a function of a PPMI matrix (Levy and Goldberg,\n2014c).\n6.9 Visualizing Embeddings\n\u201cI see well in many dimensions as long as the dimensions are around two.\u201d\nThe late economist Martin Shubik\nVisualizing embeddings is an important goal in helping understand, apply, and\nimprove these models of word meaning. But how can we visualize a (for example)\n100-dimensional vector?\nRohde, Gonnerman, Plaut Modeling Word Meaning Using Lexical Co-Occurrence\nHEADHANDFACE\nDOGAMERICA\nCATEYEEUROPE\nFOOTCHINAFRANCE\nCHICAGOARM\nFINGER\nNOSELEGRUSSIA\nMOUSEAFRICA\nATLANTAEARSHOULDERASIA\nCOW\nBULLPUPPYLIONHAWAII\nMONTREALTOKYOTOEMOSCOW\nTOOTH\nNASHVILLEBRAZILWRIST\nKITTENANKLE\nTURTLE\nOYSTER\nFigure 8: Multidimensional scaling for three noun classes.WRIST\nANKLE\nSHOULDER\nARM\nLEG\nHAND\nFOOT\nHEAD\nNOSE\nFINGER\nTOE\nFACE\nEAR\nEYE\nTOOTH\nDOG\nCAT\nPUPPY\nKITTEN\nCOW\nMOUSE\nTURTLE\nOYSTER\nLION\nBULL\nCHICAGO\nATLANTA\nMONTREAL\nNASHVILLE\nTOKYOCHINA\nRUSSIA\nAFRICA\nASIA\nEUROPE\nAMERICA\nBRAZIL\nMOSCOW",
    "metadata": {
      "source": "6",
      "chunk_id": 37,
      "token_count": 766,
      "chapter_title": ""
    }
  },
  {
    "content": "improve these models of word meaning. But how can we visualize a (for example)\n100-dimensional vector?\nRohde, Gonnerman, Plaut Modeling Word Meaning Using Lexical Co-Occurrence\nHEADHANDFACE\nDOGAMERICA\nCATEYEEUROPE\nFOOTCHINAFRANCE\nCHICAGOARM\nFINGER\nNOSELEGRUSSIA\nMOUSEAFRICA\nATLANTAEARSHOULDERASIA\nCOW\nBULLPUPPYLIONHAWAII\nMONTREALTOKYOTOEMOSCOW\nTOOTH\nNASHVILLEBRAZILWRIST\nKITTENANKLE\nTURTLE\nOYSTER\nFigure 8: Multidimensional scaling for three noun classes.WRIST\nANKLE\nSHOULDER\nARM\nLEG\nHAND\nFOOT\nHEAD\nNOSE\nFINGER\nTOE\nFACE\nEAR\nEYE\nTOOTH\nDOG\nCAT\nPUPPY\nKITTEN\nCOW\nMOUSE\nTURTLE\nOYSTER\nLION\nBULL\nCHICAGO\nATLANTA\nMONTREAL\nNASHVILLE\nTOKYOCHINA\nRUSSIA\nAFRICA\nASIA\nEUROPE\nAMERICA\nBRAZIL\nMOSCOW\nFRANCEHAWAIIFigure 9: Hierarchical clustering for three noun classes using distances based on vector correlations.\n20\nThe simplest way to visualize the meaning of a word\nwembedded in a space is to list the most similar words to\nwby sorting the vectors for all words in the vocabulary by\ntheir cosine with the vector for w. For example the 7 closest\nwords to frogusing a particular embeddings computed with\nthe GloVe algorithm are: frogs ,toad,litoria ,leptodactyli-\ndae,rana,lizard , and eleutherodactylus (Pennington et al.,\n2014).\nYet another visualization method is to use a clustering\nalgorithm to show a hierarchical representation of which\nwords are similar to others in the embedding space. The\nuncaptioned \ufb01gure on the left uses hierarchical clustering\nof some embedding vectors for nouns as a visualization",
    "metadata": {
      "source": "6",
      "chunk_id": 38,
      "token_count": 476,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24",
    "metadata": {
      "source": "6",
      "chunk_id": 39,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "24 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\nmethod (Rohde et al., 2006).\nProbably the most common visualization method, how-\never, is to project the 100 dimensions of a word down into 2\ndimensions. Fig. 6.1 showed one such visualization, as does\nFig. 6.16, using a projection method called t-SNE (van der\nMaaten and Hinton, 2008).\n6.10 Semantic properties of embeddings\nIn this section we brie\ufb02y summarize some of the semantic properties of embeddings\nthat have been studied.\nDifferent types of similarity or association: One parameter of vector semantic\nmodels that is relevant to both sparse PPMI vectors and dense word2vec vectors is\nthe size of the context window used to collect counts. This is generally between 1\nand 10 words on each side of the target word (for a total context of 2-20 words).\nThe choice depends on the goals of the representation. Shorter context windows\ntend to lead to representations that are a bit more syntactic, since the information is\ncoming from immediately nearby words. When the vectors are computed from short\ncontext windows, the most similar words to a target word wtend to be semantically\nsimilar words with the same parts of speech. When vectors are computed from long\ncontext windows, the highest cosine words to a target word wtend to be words that\nare topically related but not similar.\nFor example Levy and Goldberg (2014a) showed that using skip-gram with a\nwindow of\u00062, the most similar words to the word Hogwarts (from the Harry Potter\nseries) were names of other \ufb01ctional schools: Sunnydale (from Buffy the Vampire\nSlayer ) orEvernight (from a vampire series). With a window of \u00065, the most similar\nwords to Hogwarts were other words topically related to the Harry Potter series:\nDumbledore ,Malfoy , and half-blood .\nIt\u2019s also often useful to distinguish two kinds of similarity or association between\nwords (Sch \u00a8utze and Pedersen, 1993). Two words have \ufb01rst-order co-occurrence\ufb01rst-order\nco-occurrence\n(sometimes called syntagmatic association ) if they are typically nearby each other.\nThus wrote is a \ufb01rst-order associate of book orpoem . Two words have second-order\nco-occurrence (sometimes called paradigmatic association ) if they have similarsecond-order\nco-occurrence\nneighbors. Thus wrote is a second-order associate of words like said orremarked .\nAnalogy/Relational Similarity: Another semantic property of embeddings is their\nability to capture relational meanings. In an important early vector space model of\ncognition, Rumelhart and Abrahamson (1973) proposed the parallelogram modelparallelogram\nmodel\nfor solving simple analogy problems of the form a is to b as a* is to what? . In\nsuch problems, a system is given a problem like apple:tree::grape:? , i.e., apple is\nto tree as grape is to , and must \ufb01ll in the word vine. In the parallelogram\nmodel, illustrated in Fig. 6.15, the vector from the word apple to the word tree(=#   \u0014tree\u0000#       \u0014apple) is added to the vector for grape (#        \u0014grape); the nearest word to that point\nis returned.\nIn early work with sparse embeddings, scholars showed that sparse vector mod-\nels of meaning could solve such analogy problems (Turney and Littman, 2005),\nbut the parallelogram method received more modern attention because of its suc-",
    "metadata": {
      "source": "6",
      "chunk_id": 40,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "Thus wrote is a \ufb01rst-order associate of book orpoem . Two words have second-order\nco-occurrence (sometimes called paradigmatic association ) if they have similarsecond-order\nco-occurrence\nneighbors. Thus wrote is a second-order associate of words like said orremarked .\nAnalogy/Relational Similarity: Another semantic property of embeddings is their\nability to capture relational meanings. In an important early vector space model of\ncognition, Rumelhart and Abrahamson (1973) proposed the parallelogram modelparallelogram\nmodel\nfor solving simple analogy problems of the form a is to b as a* is to what? . In\nsuch problems, a system is given a problem like apple:tree::grape:? , i.e., apple is\nto tree as grape is to , and must \ufb01ll in the word vine. In the parallelogram\nmodel, illustrated in Fig. 6.15, the vector from the word apple to the word tree(=#   \u0014tree\u0000#       \u0014apple) is added to the vector for grape (#        \u0014grape); the nearest word to that point\nis returned.\nIn early work with sparse embeddings, scholars showed that sparse vector mod-\nels of meaning could solve such analogy problems (Turney and Littman, 2005),\nbut the parallelogram method received more modern attention because of its suc-\ncess with word2vec or GloVe vectors (Mikolov et al. 2013c, Levy and Goldberg\n2014b, Pennington et al. 2014). For example, the result of the expression#     \u0014king\u0000",
    "metadata": {
      "source": "6",
      "chunk_id": 41,
      "token_count": 348,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 25\n\n6.10 \u2022 S EMANTIC PROPERTIES OF EMBEDDINGS 25\ntreeapplegrapevine\nFigure 6.15 The parallelogram model for analogy problems (Rumelhart and Abrahamson,\n1973): the location of#     \u0014vine can be found by subtracting#       \u0014apple from#   \u0014tree and adding#       \u0014grape.\n#     \u0014man+#            \u0014woman is a vector close to#         \u0014queen. Similarly,#      \u0014Paris\u0000#           \u0014France +#     \u0014Italy results\nin a vector that is close to#         \u0014Rome. The embedding model thus seems to be extract-\ning representations of relations like MALE -FEMALE , or CAPITAL -CITY -OF, or even\nCOMPARATIVE /SUPERLATIVE , as shown in Fig. 6.16 from GloVe.\n(a) (b)\nFigure 6.16 Relational properties of the GloVe vector space, shown by projecting vectors onto two dimen-\nsions. (a)#     \u0014king\u0000#     \u0014man+#            \u0014woman is close to#        \u0014queen. (b) offsets seem to capture comparative and superlative\nmorphology (Pennington et al., 2014).\nFor a a:b::a\u0003:b\u0003problem, meaning the algorithm is given vectors a,b, and\na\u0003and must \ufb01nd b\u0003, the parallelogram method is thus:\n\u02c6b\u0003=argmin\nxdistance (x;b\u0000a+a\u0003) (6.41)\nwith some distance function, such as Euclidean distance.\nThere are some caveats. For example, the closest value returned by the paral-\nlelogram algorithm in word2vec or GloVe embedding spaces is usually not in fact\nb* but one of the 3 input words or their morphological variants (i.e., cherry:red ::\npotato:x returns potato orpotatoes instead of brown ), so these must be explicitly\nexcluded. Furthermore while embedding spaces perform well if the task involves\nfrequent words, small distances, and certain relations (like relating countries with\ntheir capitals or verbs/nouns with their in\ufb02ected forms), the parallelogram method\nwith embeddings doesn\u2019t work as well for other relations (Linzen 2016, Gladkova\net al. 2016, Schluter 2018, Ethayarajh et al. 2019a), and indeed Peterson et al. (2020)\nargue that the parallelogram method is in general too simple to model the human\ncognitive process of forming analogies of this kind.",
    "metadata": {
      "source": "6",
      "chunk_id": 42,
      "token_count": 567,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 26",
    "metadata": {
      "source": "6",
      "chunk_id": 43,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "26 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\n6.10.1 Embeddings and Historical Semantics\nEmbeddings can also be a useful tool for studying how meaning changes over time,\nby computing multiple embedding spaces, each from texts written in a particular\ntime period. For example Fig. 6.17 shows a visualization of changes in meaning in\nEnglish words over the last two centuries, computed by building separate embedding\nspaces for each decade from historical corpora like Google n-grams (Lin et al., 2012)\nand the Corpus of Historical American English (Davies, 2012).\nCHAPTER 5. DYNAMIC SOCIAL REPRESENTATIONS OF WORD MEANING 79\nFigure 5.1: Two-dimensional visualization of semantic change in English using SGNS\nvectors (see Section 5.8 for the visualization algorithm). A,T h ew o r d gay shifted\nfrom meaning \u201ccheerful\u201d or \u201cfrolicsome\u201d to referring to homosexuality. A,I nt h ee a r l y\n20th century broadcast referred to \u201ccasting out seeds\u201d; with the rise of television and\nradio its meaning shifted to \u201ctransmitting signals\u201d. C,Awful underwent a process of\npejoration, as it shifted from meaning \u201cfull of awe\u201d to meaning \u201cterrible or appalling\u201d\n[212].\nthat adverbials (e.g., actually )h a v eag e n e r a lt e n d e n c yt ou n d e r g os u b j e c t i \ufb01 c a t i o n\nwhere they shift from objective statements about the world (e.g., \u201cSorry, the car is\nactually broken\u201d) to subjective statements (e.g., \u201cI can\u2019t believe he actually did that\u201d,\nindicating surprise/disbelief).\n5.2.2 Computational linguistic studies\nThere are also a number of recent works analyzing semantic change using computational\nmethods. [ 200] use latent semantic analysis to analyze how word meanings broaden\nand narrow over time. [ 113]u s er a wc o - o c c u r r e n c ev e c t o r st op e r f o r man u m b e ro f\nhistorical case-studies on semantic change, and [ 252] perform a similar set of small-\nscale case-studies using temporal topic models. [ 87]c o n s t r u c tp o i n t - w i s em u t u a l\ninformation-based embeddings and found that semantic changes uncovered by their\nmethod had reasonable agreement with human judgments. [ 129]a n d[ 119]u s e\u201c n e u r a l \u201d\nword-embedding methods to detect linguistic change points. Finally, [ 257]a n a l y z e\nhistorical co-occurrences to test whether synonyms tend to change in similar ways.\nFigure 6.17 A t-SNE visualization of the semantic change of 3 words in English using\nword2vec vectors. The modern sense of each word, and the grey context words, are com-\nputed from the most recent (modern) time-point embedding space. Earlier points are com-\nputed from earlier historical embedding spaces. The visualizations show the changes in the\nword gayfrom meanings related to \u201ccheerful\u201d or \u201cfrolicsome\u201d to referring to homosexuality,\nthe development of the modern \u201ctransmission\u201d sense of broadcast from its original sense of\nsowing seeds, and the pejoration of the word awful as it shifted from meaning \u201cfull of awe\u201d\nto meaning \u201cterrible or appalling\u201d (Hamilton et al., 2016).\n6.11 Bias and Embeddings",
    "metadata": {
      "source": "6",
      "chunk_id": 44,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "scale case-studies using temporal topic models. [ 87]c o n s t r u c tp o i n t - w i s em u t u a l\ninformation-based embeddings and found that semantic changes uncovered by their\nmethod had reasonable agreement with human judgments. [ 129]a n d[ 119]u s e\u201c n e u r a l \u201d\nword-embedding methods to detect linguistic change points. Finally, [ 257]a n a l y z e\nhistorical co-occurrences to test whether synonyms tend to change in similar ways.\nFigure 6.17 A t-SNE visualization of the semantic change of 3 words in English using\nword2vec vectors. The modern sense of each word, and the grey context words, are com-\nputed from the most recent (modern) time-point embedding space. Earlier points are com-\nputed from earlier historical embedding spaces. The visualizations show the changes in the\nword gayfrom meanings related to \u201ccheerful\u201d or \u201cfrolicsome\u201d to referring to homosexuality,\nthe development of the modern \u201ctransmission\u201d sense of broadcast from its original sense of\nsowing seeds, and the pejoration of the word awful as it shifted from meaning \u201cfull of awe\u201d\nto meaning \u201cterrible or appalling\u201d (Hamilton et al., 2016).\n6.11 Bias and Embeddings\nIn addition to their ability to learn word meaning from text, embeddings, alas,\nalso reproduce the implicit biases and stereotypes that were latent in the text. As\nthe prior section just showed, embeddings can roughly model relational similar-\nity: \u2018queen\u2019 as the closest word to \u2018king\u2019 - \u2018man\u2019 + \u2018woman\u2019 implies the analogy\nman:woman::king:queen . But these same embedding analogies also exhibit gender\nstereotypes. For example Bolukbasi et al. (2016) \ufb01nd that the closest occupation\nto \u2018computer programmer\u2019 - \u2018man\u2019 + \u2018woman\u2019 in word2vec embeddings trained on\nnews text is \u2018homemaker\u2019, and that the embeddings similarly suggest the analogy\n\u2018father\u2019 is to \u2018doctor\u2019 as \u2018mother\u2019 is to \u2018nurse\u2019. This could result in what Crawford\n(2017) and Blodgett et al. (2020) call an allocational harm , when a system allo-allocational\nharm\ncates resources (jobs or credit) unfairly to different groups. For example algorithms\nthat use embeddings as part of a search for hiring potential programmers or doctors\nmight thus incorrectly downweight documents with women\u2019s names.\nIt turns out that embeddings don\u2019t just re\ufb02ect the statistics of their input, but also\namplify bias; gendered terms become more gendered in embedding space than theybias\nampli\ufb01cation\nwere in the input text statistics (Zhao et al. 2017, Ethayarajh et al. 2019b, Jia et al.\n2020), and biases are more exaggerated than in actual labor employment statistics\n(Garg et al., 2018).\nEmbeddings also encode the implicit associations that are a property of human\nreasoning. The Implicit Association Test (Greenwald et al., 1998) measures peo-",
    "metadata": {
      "source": "6",
      "chunk_id": 45,
      "token_count": 677,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 27",
    "metadata": {
      "source": "6",
      "chunk_id": 46,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "6.12 \u2022 E VALUATING VECTOR MODELS 27\nple\u2019s associations between concepts (like \u2018\ufb02owers\u2019 or \u2018insects\u2019) and attributes (like\n\u2018pleasantness\u2019 and \u2018unpleasantness\u2019) by measuring differences in the latency with\nwhich they label words in the various categories.7Using such methods, people\nin the United States have been shown to associate African-American names with\nunpleasant words (more than European-American names), male names more with\nmathematics and female names with the arts, and old people\u2019s names with unpleas-\nant words (Greenwald et al. 1998, Nosek et al. 2002a, Nosek et al. 2002b). Caliskan\net al. (2017) replicated all these \ufb01ndings of implicit associations using GloVe vectors\nand cosine similarity instead of human latencies. For example African-American\nnames like \u2018Leroy\u2019 and \u2018Shaniqua\u2019 had a higher GloVe cosine with unpleasant words\nwhile European-American names (\u2018Brad\u2019, \u2018Greg\u2019, \u2018Courtney\u2019) had a higher cosine\nwith pleasant words. These problems with embeddings are an example of a repre-\nsentational harm (Crawford 2017, Blodgett et al. 2020), which is a harm caused byrepresentational\nharm\na system demeaning or even ignoring some social groups. Any embedding-aware al-\ngorithm that made use of word sentiment could thus exacerbate bias against African\nAmericans.\nRecent research focuses on ways to try to remove these kinds of biases, for\nexample by developing a transformation of the embedding space that removes gen-\nder stereotypes but preserves de\ufb01nitional gender (Bolukbasi et al. 2016, Zhao et al.\n2017) or changing the training procedure (Zhao et al., 2018). However, although\nthese sorts of debiasing may reduce bias in embeddings, they do not eliminate it debiasing\n(Gonen and Goldberg, 2019), and this remains an open problem.\nHistorical embeddings are also being used to measure biases in the past. Garg\net al. (2018) used embeddings from historical texts to measure the association be-\ntween embeddings for occupations and embeddings for names of various ethnici-\nties or genders (for example the relative cosine similarity of women\u2019s names versus\nmen\u2019s to occupation words like \u2018librarian\u2019 or \u2018carpenter\u2019) across the 20th century.\nThey found that the cosines correlate with the empirical historical percentages of\nwomen or ethnic groups in those occupations. Historical embeddings also repli-\ncated old surveys of ethnic stereotypes; the tendency of experimental participants in\n1933 to associate adjectives like \u2018industrious\u2019 or \u2018superstitious\u2019 with, e.g., Chinese\nethnicity, correlates with the cosine between Chinese last names and those adjectives\nusing embeddings trained on 1930s text. They also were able to document historical\ngender biases, such as the fact that embeddings for adjectives related to competence\n(\u2018smart\u2019, \u2018wise\u2019, \u2018thoughtful\u2019, \u2018resourceful\u2019) had a higher cosine with male than fe-\nmale words, and showed that this bias has been slowly decreasing since 1960. We\nreturn in later chapters to this question about the role of bias in natural language\nprocessing.\n6.12 Evaluating Vector Models\nThe most important evaluation metric for vector models is extrinsic evaluation on\ntasks, i.e., using vectors in an NLP task and seeing whether this improves perfor-\nmance over some other model.\n7Roughly speaking, if humans associate \u2018\ufb02owers\u2019 with \u2018pleasantness\u2019 and \u2018insects\u2019 with \u2018unpleasant-",
    "metadata": {
      "source": "6",
      "chunk_id": 47,
      "token_count": 777,
      "chapter_title": ""
    }
  },
  {
    "content": "men\u2019s to occupation words like \u2018librarian\u2019 or \u2018carpenter\u2019) across the 20th century.\nThey found that the cosines correlate with the empirical historical percentages of\nwomen or ethnic groups in those occupations. Historical embeddings also repli-\ncated old surveys of ethnic stereotypes; the tendency of experimental participants in\n1933 to associate adjectives like \u2018industrious\u2019 or \u2018superstitious\u2019 with, e.g., Chinese\nethnicity, correlates with the cosine between Chinese last names and those adjectives\nusing embeddings trained on 1930s text. They also were able to document historical\ngender biases, such as the fact that embeddings for adjectives related to competence\n(\u2018smart\u2019, \u2018wise\u2019, \u2018thoughtful\u2019, \u2018resourceful\u2019) had a higher cosine with male than fe-\nmale words, and showed that this bias has been slowly decreasing since 1960. We\nreturn in later chapters to this question about the role of bias in natural language\nprocessing.\n6.12 Evaluating Vector Models\nThe most important evaluation metric for vector models is extrinsic evaluation on\ntasks, i.e., using vectors in an NLP task and seeing whether this improves perfor-\nmance over some other model.\n7Roughly speaking, if humans associate \u2018\ufb02owers\u2019 with \u2018pleasantness\u2019 and \u2018insects\u2019 with \u2018unpleasant-\nness\u2019, when they are instructed to push a green button for \u2018\ufb02owers\u2019 (daisy, iris, lilac) and \u2018pleasant words\u2019\n(love, laughter, pleasure) and a red button for \u2018insects\u2019 (\ufb02ea, spider, mosquito) and \u2018unpleasant words\u2019\n(abuse, hatred, ugly) they are faster than in an incongruous condition where they push a red button for\n\u2018\ufb02owers\u2019 and \u2018unpleasant words\u2019 and a green button for \u2018insects\u2019 and \u2018pleasant words\u2019.",
    "metadata": {
      "source": "6",
      "chunk_id": 48,
      "token_count": 397,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 28",
    "metadata": {
      "source": "6",
      "chunk_id": 49,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "28 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\nNonetheless it is useful to have intrinsic evaluations. The most common metric\nis to test their performance on similarity , computing the correlation between an\nalgorithm\u2019s word similarity scores and word similarity ratings assigned by humans.\nWordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0\nto 10 for 353 noun pairs; for example ( plane ,car) had an average score of 5.77.\nSimLex-999 (Hill et al., 2015) is a more complex dataset that quanti\ufb01es similarity\n(cup, mug ) rather than relatedness ( cup, coffee ), and includes concrete and abstract\nadjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions, each\nconsisting of a target word with 4 additional word choices; the task is to choose\nwhich is the correct synonym, as in the example: Levied is closest in meaning to:\nimposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these\ndatasets present words without context.\nSlightly more realistic are intrinsic similarity tasks that include context. The\nStanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) and the\nWord-in-Context (WiC) dataset (Pilehvar and Camacho-Collados, 2019) offer richer\nevaluation scenarios. SCWS gives human judgments on 2,003 pairs of words in\ntheir sentential context, while WiC gives target words in two sentential contexts that\nare either in the same or different senses; see Appendix G. The semantic textual\nsimilarity task (Agirre et al. 2012, Agirre et al. 2015) evaluates the performance of\nsentence-level similarity algorithms, consisting of a set of pairs of sentences, each\npair with human-labeled similarity scores.\nAnother task used for evaluation is the analogy task, discussed on page 24, where\nthe system has to solve problems of the form a is to b as a* is to b* , given a, b, anda*\nand having to \ufb01nd b*(Turney and Littman, 2005). A number of sets of tuples have\nbeen created for this task (Mikolov et al. 2013a, Mikolov et al. 2013c, Gladkova\net al. 2016), covering morphology ( city:cities::child:children ), lexicographic rela-\ntions ( leg:table::spout:teapot ) and encyclopedia relations ( Beijing:China::Dublin:Ireland ),\nsome drawing from the SemEval-2012 Task 2 dataset of 79 different relations (Jur-\ngens et al., 2012).\nAll embedding algorithms suffer from inherent variability. For example because\nof randomness in the initialization and the random negative sampling, algorithms\nlike word2vec may produce different results even from the same dataset, and in-\ndividual documents in a collection may strongly impact the resulting embeddings\n(Tian et al. 2016, Hellrich and Hahn 2016, Antoniak and Mimno 2018). When em-\nbeddings are used to study word associations in particular corpora, therefore, it is\nbest practice to train multiple embeddings with bootstrap sampling over documents\nand average the results (Antoniak and Mimno, 2018).\n6.13 Summary\n\u2022 In vector semantics, a word is modeled as a vector\u2014a point in high-dimensional\nspace, also called an embedding . In this chapter we focus on static embed-",
    "metadata": {
      "source": "6",
      "chunk_id": 50,
      "token_count": 777,
      "chapter_title": ""
    }
  },
  {
    "content": "been created for this task (Mikolov et al. 2013a, Mikolov et al. 2013c, Gladkova\net al. 2016), covering morphology ( city:cities::child:children ), lexicographic rela-\ntions ( leg:table::spout:teapot ) and encyclopedia relations ( Beijing:China::Dublin:Ireland ),\nsome drawing from the SemEval-2012 Task 2 dataset of 79 different relations (Jur-\ngens et al., 2012).\nAll embedding algorithms suffer from inherent variability. For example because\nof randomness in the initialization and the random negative sampling, algorithms\nlike word2vec may produce different results even from the same dataset, and in-\ndividual documents in a collection may strongly impact the resulting embeddings\n(Tian et al. 2016, Hellrich and Hahn 2016, Antoniak and Mimno 2018). When em-\nbeddings are used to study word associations in particular corpora, therefore, it is\nbest practice to train multiple embeddings with bootstrap sampling over documents\nand average the results (Antoniak and Mimno, 2018).\n6.13 Summary\n\u2022 In vector semantics, a word is modeled as a vector\u2014a point in high-dimensional\nspace, also called an embedding . In this chapter we focus on static embed-\ndings , where each word is mapped to a \ufb01xed embedding.\n\u2022 Vector semantic models fall into two classes: sparse anddense . In sparse\nmodels each dimension corresponds to a word in the vocabulary Vand cells\nare functions of co-occurrence counts . The term-document matrix has a\nrow for each word ( term ) in the vocabulary and a column for each document.\nTheword-context orterm-term matrix has a row for each (target) word in",
    "metadata": {
      "source": "6",
      "chunk_id": 51,
      "token_count": 377,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 29\n\nBIBLIOGRAPHICAL AND HISTORICAL NOTES 29\nthe vocabulary and a column for each context term in the vocabulary. Two\nsparse weightings are common: the tf-idf weighting which weights each cell\nby its term frequency andinverse document frequency , and PPMI (point-\nwise positive mutual information), which is most common for word-context\nmatrices.\n\u2022 Dense vector models have dimensionality 50\u20131000. Word2vec algorithms\nlikeskip-gram are a popular way to compute dense embeddings. Skip-gram\ntrains a logistic regression classi\ufb01er to compute the probability that two words\nare \u2018likely to occur nearby in text\u2019. This probability is computed from the dot\nproduct between the embeddings for the two words.\n\u2022 Skip-gram uses stochastic gradient descent to train the classi\ufb01er, by learning\nembeddings that have a high dot product with embeddings of words that occur\nnearby and a low dot product with noise words.\n\u2022 Other important embedding algorithms include GloVe , a method based on\nratios of word co-occurrence probabilities.\n\u2022 Whether using sparse or dense vectors, word and document similarities are\ncomputed by some function of the dot product between vectors. The cosine\nof two vectors\u2014a normalized dot product\u2014is the most popular such metric.\nBibliographical and Historical Notes\nThe idea of vector semantics arose out of research in the 1950s in three distinct\n\ufb01elds: linguistics, psychology, and computer science, each of which contributed a\nfundamental aspect of the model.\nThe idea that meaning is related to the distribution of words in context was\nwidespread in linguistic theory of the 1950s, among distributionalists like Zellig\nHarris, Martin Joos, and J. R. Firth, and semioticians like Thomas Sebeok. As Joos\n(1950) put it,\nthe linguist\u2019s \u201cmeaning\u201d of a morpheme. . . is by de\ufb01nition the set of conditional\nprobabilities of its occurrence in context with all other morphemes.\nThe idea that the meaning of a word might be modeled as a point in a multi-\ndimensional semantic space came from psychologists like Charles E. Osgood, who\nhad been studying how people responded to the meaning of words by assigning val-\nues along scales like happy/sad orhard/soft . Osgood et al. (1957) proposed that the\nmeaning of a word in general could be modeled as a point in a multidimensional\nEuclidean space, and that the similarity of meaning between two words could be\nmodeled as the distance between these points in the space.\nA \ufb01nal intellectual source in the 1950s and early 1960s was the \ufb01eld then called\nmechanical indexing , now known as information retrieval . In what became knownmechanical\nindexing\nas the vector space model for information retrieval (Salton 1971, Sparck Jones\n1986), researchers demonstrated new ways to de\ufb01ne the meaning of words in terms\nof vectors (Switzer, 1965), and re\ufb01ned methods for word similarity based on mea-\nsures of statistical association between words like mutual information (Giuliano,\n1965) and idf (Sparck Jones, 1972), and showed that the meaning of documents\ncould be represented in the same vector spaces used for words. Around the same\ntime, (Cordier, 1965) showed that factor analysis of word association probabilities\ncould be used to form dense vector representations of words.",
    "metadata": {
      "source": "6",
      "chunk_id": 52,
      "token_count": 755,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 30",
    "metadata": {
      "source": "6",
      "chunk_id": 53,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "30 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\nSome of the philosophical underpinning of the distributional way of thinking\ncame from the late writings of the philosopher Wittgenstein, who was skeptical of\nthe possibility of building a completely formal theory of meaning de\ufb01nitions for\neach word. Wittgenstein suggested instead that \u201cthe meaning of a word is its use in\nthe language\u201d (Wittgenstein, 1953, PI 43). That is, instead of using some logical lan-\nguage to de\ufb01ne each word, or drawing on denotations or truth values, Wittgenstein\u2019s\nidea is that we should de\ufb01ne a word by how it is used by people in speaking and un-\nderstanding in their day-to-day interactions, thus pre\ufb01guring the movement toward\nembodied and experiential models in linguistics and NLP (Glenberg and Robertson\n2000, Lake and Murphy 2021, Bisk et al. 2020, Bender and Koller 2020).\nMore distantly related is the idea of de\ufb01ning words by a vector of discrete fea-\ntures, which has roots at least as far back as Descartes and Leibniz (Wierzbicka 1992,\nWierzbicka 1996). By the middle of the 20th century, beginning with the work of\nHjelmslev (Hjelmslev, 1969) (originally 1943) and \ufb02eshed out in early models of\ngenerative grammar (Katz and Fodor, 1963), the idea arose of representing mean-\ning with semantic features , symbols that represent some sort of primitive meaning.semantic\nfeature\nFor example words like hen,rooster , orchick , have something in common (they all\ndescribe chickens) and something different (their age and sex), representable as:\nhen +female, +chicken, +adult\nrooster -female, +chicken, +adult\nchick +chicken, -adult\nThe dimensions used by vector models of meaning to de\ufb01ne words, however, are\nonly abstractly related to this idea of a small \ufb01xed number of hand-built dimensions.\nNonetheless, there has been some attempt to show that certain dimensions of em-\nbedding models do contribute some speci\ufb01c compositional aspect of meaning like\nthese early semantic features.\nThe use of dense vectors to model word meaning, and indeed the term embed-\nding , grew out of the latent semantic indexing (LSI) model (Deerwester et al.,\n1988) recast as LSA (latent semantic analysis ) (Deerwester et al., 1990). In LSA\nsingular value decomposition \u2014SVD \u2014 is applied to a term-document matrix (each SVD\ncell weighted by log frequency and normalized by entropy), and then the \ufb01rst 300\ndimensions are used as the LSA embedding. Singular Value Decomposition (SVD)\nis a method for \ufb01nding the most important dimensions of a data set, those dimen-\nsions along which the data varies the most. LSA was then quickly widely applied:\nas a cognitive model Landauer and Dumais (1997), and for tasks like spell checking\n(Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and Ju-\nrafsky 1998, Bellegarda 2000), morphology induction (Schone and Jurafsky 2000,\nSchone and Jurafsky 2001b), multiword expressions (MWEs) (Schone and Jurafsky,",
    "metadata": {
      "source": "6",
      "chunk_id": 54,
      "token_count": 780,
      "chapter_title": ""
    }
  },
  {
    "content": "bedding models do contribute some speci\ufb01c compositional aspect of meaning like\nthese early semantic features.\nThe use of dense vectors to model word meaning, and indeed the term embed-\nding , grew out of the latent semantic indexing (LSI) model (Deerwester et al.,\n1988) recast as LSA (latent semantic analysis ) (Deerwester et al., 1990). In LSA\nsingular value decomposition \u2014SVD \u2014 is applied to a term-document matrix (each SVD\ncell weighted by log frequency and normalized by entropy), and then the \ufb01rst 300\ndimensions are used as the LSA embedding. Singular Value Decomposition (SVD)\nis a method for \ufb01nding the most important dimensions of a data set, those dimen-\nsions along which the data varies the most. LSA was then quickly widely applied:\nas a cognitive model Landauer and Dumais (1997), and for tasks like spell checking\n(Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and Ju-\nrafsky 1998, Bellegarda 2000), morphology induction (Schone and Jurafsky 2000,\nSchone and Jurafsky 2001b), multiword expressions (MWEs) (Schone and Jurafsky,\n2001a), and essay grading (Rehder et al., 1998). Related models were simultane-\nously developed and applied to word sense disambiguation by Sch \u00a8utze (1992). LSA\nalso led to the earliest use of embeddings to represent words in a probabilistic clas-\nsi\ufb01er, in the logistic regression document router of Sch \u00a8utze et al. (1995). The idea of\nSVD on the term-term matrix (rather than the term-document matrix) as a model of\nmeaning for NLP was proposed soon after LSA by Sch \u00a8utze (1992). Sch \u00a8utze applied\nthe low-rank (97-dimensional) embeddings produced by SVD to the task of word\nsense disambiguation, analyzed the resulting semantic space, and also suggested\npossible techniques like dropping high-order dimensions. See Sch \u00a8utze (1997).\nA number of alternative matrix models followed on from the early SVD work,\nincluding Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Latent\nDirichlet Allocation (LDA) (Blei et al., 2003), and Non-negative Matrix Factoriza-",
    "metadata": {
      "source": "6",
      "chunk_id": 55,
      "token_count": 541,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 31\n\nEXERCISES 31\ntion (NMF) (Lee and Seung, 1999).\nThe LSA community seems to have \ufb01rst used the word \u201cembedding\u201d in Landauer\net al. (1997), in a variant of its mathematical meaning as a mapping from one space\nor mathematical structure to another. In LSA, the word embedding seems to have\ndescribed the mapping from the space of sparse count vectors to the latent space of\nSVD dense vectors. Although the word thus originally meant the mapping from one\nspace to another, it has metonymically shifted to mean the resulting dense vector in\nthe latent space, and it is in this sense that we currently use the word.\nBy the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that\nneural language models could also be used to develop embeddings as part of the task\nof word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and\nCollobert et al. (2011) then demonstrated that embeddings could be used to represent\nword meanings for a number of NLP tasks. Turian et al. (2010) compared the value\nof different kinds of embeddings for different NLP tasks. Mikolov et al. (2011)\nshowed that recurrent neural nets could be used as language models. The idea of\nsimplifying the hidden layer of these neural net language models to create the skip-\ngram (and also CBOW) algorithms was proposed by Mikolov et al. (2013a). The\nnegative sampling training algorithm was proposed in Mikolov et al. (2013b). There\nare numerous surveys of static embeddings and their parameterizations (Bullinaria\nand Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark\n2014, Levy et al. 2015).\nSee Manning et al. (2008) and Chapter 14 for a deeper understanding of the role\nof vectors in information retrieval, including how to compare queries with docu-\nments, more details on tf-idf, and issues of scaling to very large datasets. See Kim\n(2019) for a clear and comprehensive tutorial on word2vec. Cruse (2004) is a useful\nintroductory linguistic text on lexical semantics.\nExercises",
    "metadata": {
      "source": "6",
      "chunk_id": 56,
      "token_count": 512,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 32",
    "metadata": {
      "source": "6",
      "chunk_id": 57,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "32 Chapter 6 \u2022 Vector Semantics and Embeddings\nAgirre, E., C. Banea, C. Cardie, D. Cer, M. Diab,\nA. Gonzalez-Agirre, W. Guo, I. Lopez-Gazpio, M. Mar-\nitxalar, R. Mihalcea, G. Rigau, L. Uria, and J. Wiebe.\n2015. SemEval-2015 task 2: Semantic textual similarity,\nEnglish, Spanish and pilot on interpretability. SemEval-\n15.\nAgirre, E., M. Diab, D. Cer, and A. Gonzalez-Agirre. 2012.\nSemEval-2012 task 6: A pilot on semantic textual simi-\nlarity. SemEval-12 .\nAntoniak, M. and D. Mimno. 2018. Evaluating the stability\nof embedding-based word similarities. TACL , 6:107\u2013119.\nBellegarda, J. R. 1997. A latent semantic analysis framework\nfor large-span language modeling. EUROSPEECH .\nBellegarda, J. R. 2000. Exploiting latent semantic informa-\ntion in statistical language modeling. Proceedings of the\nIEEE , 89(8):1279\u20131296.\nBender, E. M. and A. Koller. 2020. Climbing towards NLU:\nOn meaning, form, and understanding in the age of data.\nACL.\nBengio, Y ., A. Courville, and P. Vincent. 2013. Represen-\ntation learning: A review and new perspectives. IEEE\nTransactions on Pattern Analysis and Machine Intelli-\ngence , 35(8):1798\u20131828.\nBengio, Y ., R. Ducharme, P. Vincent, and C. Jauvin. 2003.\nA neural probabilistic language model. JMLR , 3:1137\u2013\n1155.\nBengio, Y ., H. Schwenk, J.-S. Sen \u00b4ecal, F. Morin, and J.-L.\nGauvain. 2006. Neural probabilistic language models. In\nInnovations in Machine Learning , 137\u2013186. Springer.\nBisk, Y ., A. Holtzman, J. Thomason, J. Andreas, Y . Bengio,\nJ. Chai, M. Lapata, A. Lazaridou, J. May, A. Nisnevich,\nN. Pinto, and J. Turian. 2020. Experience grounds lan-\nguage. EMNLP .\nBlei, D. M., A. Y . Ng, and M. I. Jordan. 2003. Latent Dirich-\nlet allocation. JMLR , 3(5):993\u20131022.\nBlodgett, S. L., S. Barocas, H. Daum \u00b4e III, and H. Wallach.\n2020. Language (technology) is power: A critical survey\nof \u201cbias\u201d in NLP. ACL.\nBojanowski, P., E. Grave, A. Joulin, and T. Mikolov. 2017.\nEnriching word vectors with subword information. TACL ,\n5:135\u2013146.\nBolukbasi, T., K.-W. Chang, J. Zou, V . Saligrama, and A. T.\nKalai. 2016. Man is to computer programmer as woman",
    "metadata": {
      "source": "6",
      "chunk_id": 58,
      "token_count": 754,
      "chapter_title": ""
    }
  },
  {
    "content": "Innovations in Machine Learning , 137\u2013186. Springer.\nBisk, Y ., A. Holtzman, J. Thomason, J. Andreas, Y . Bengio,\nJ. Chai, M. Lapata, A. Lazaridou, J. May, A. Nisnevich,\nN. Pinto, and J. Turian. 2020. Experience grounds lan-\nguage. EMNLP .\nBlei, D. M., A. Y . Ng, and M. I. Jordan. 2003. Latent Dirich-\nlet allocation. JMLR , 3(5):993\u20131022.\nBlodgett, S. L., S. Barocas, H. Daum \u00b4e III, and H. Wallach.\n2020. Language (technology) is power: A critical survey\nof \u201cbias\u201d in NLP. ACL.\nBojanowski, P., E. Grave, A. Joulin, and T. Mikolov. 2017.\nEnriching word vectors with subword information. TACL ,\n5:135\u2013146.\nBolukbasi, T., K.-W. Chang, J. Zou, V . Saligrama, and A. T.\nKalai. 2016. Man is to computer programmer as woman\nis to homemaker? Debiasing word embeddings. NeurIPS .\nBr\u00b4eal, M. 1897. Essai de S \u00b4emantique: Science des signi\ufb01ca-\ntions . Hachette.\nBudanitsky, A. and G. Hirst. 2006. Evaluating WordNet-\nbased measures of lexical semantic relatedness. Compu-\ntational Linguistics , 32(1):13\u201347.\nBullinaria, J. A. and J. P. Levy. 2007. Extracting seman-\ntic representations from word co-occurrence statistics:\nA computational study. Behavior research methods ,\n39(3):510\u2013526.\nBullinaria, J. A. and J. P. Levy. 2012. Extracting semantic\nrepresentations from word co-occurrence statistics: stop-\nlists, stemming, and SVD. Behavior research methods ,\n44(3):890\u2013907.\nCaliskan, A., J. J. Bryson, and A. Narayanan. 2017. Seman-\ntics derived automatically from language corpora contain\nhuman-like biases. Science , 356(6334):183\u2013186.Carlson, G. N. 1977. Reference to kinds in English . Ph.D.\nthesis, University of Massachusetts, Amherst. Forward.\nChurch, K. W. and P. Hanks. 1989. Word association norms,\nmutual information, and lexicography. ACL.\nChurch, K. W. and P. Hanks. 1990. Word association norms,\nmutual information, and lexicography. Computational\nLinguistics , 16(1):22\u201329.\nClark, E. 1987. The principle of contrast: A constraint on\nlanguage acquisition. In B. MacWhinney, ed., Mecha-\nnisms of language acquisition , 1\u201333. LEA.\nCoccaro, N. and D. Jurafsky. 1998. Towards better integra-\ntion of semantic predictors in statistical language model-\ning. ICSLP .\nCollobert, R. and J. Weston. 2007. Fast semantic extraction\nusing a novel neural network architecture. ACL.",
    "metadata": {
      "source": "6",
      "chunk_id": 59,
      "token_count": 759,
      "chapter_title": ""
    }
  },
  {
    "content": "44(3):890\u2013907.\nCaliskan, A., J. J. Bryson, and A. Narayanan. 2017. Seman-\ntics derived automatically from language corpora contain\nhuman-like biases. Science , 356(6334):183\u2013186.Carlson, G. N. 1977. Reference to kinds in English . Ph.D.\nthesis, University of Massachusetts, Amherst. Forward.\nChurch, K. W. and P. Hanks. 1989. Word association norms,\nmutual information, and lexicography. ACL.\nChurch, K. W. and P. Hanks. 1990. Word association norms,\nmutual information, and lexicography. Computational\nLinguistics , 16(1):22\u201329.\nClark, E. 1987. The principle of contrast: A constraint on\nlanguage acquisition. In B. MacWhinney, ed., Mecha-\nnisms of language acquisition , 1\u201333. LEA.\nCoccaro, N. and D. Jurafsky. 1998. Towards better integra-\ntion of semantic predictors in statistical language model-\ning. ICSLP .\nCollobert, R. and J. Weston. 2007. Fast semantic extraction\nusing a novel neural network architecture. ACL.\nCollobert, R. and J. Weston. 2008. A uni\ufb01ed architecture for\nnatural language processing: Deep neural networks with\nmultitask learning. ICML .\nCollobert, R., J. Weston, L. Bottou, M. Karlen,\nK. Kavukcuoglu, and P. Kuksa. 2011. Natural language\nprocessing (almost) from scratch. JMLR , 12:2493\u20132537.\nCordier, B. 1965. Factor-analysis of correspondences. COL-\nING 1965 .\nCrawford, K. 2017. The trouble with bias. Keynote at\nNeurIPS.\nCruse, D. A. 2004. Meaning in Language: an Introduction\nto Semantics and Pragmatics . Oxford University Press.\nSecond edition.\nDagan, I., S. Marcus, and S. Markovitch. 1993. Contextual\nword similarity and estimation from sparse data. ACL.\nDavies, M. 2012. Expanding horizons in historical lin-\nguistics with the 400-million word Corpus of Historical\nAmerican English. Corpora , 7(2):121\u2013157.\nDavies, M. 2015. The Wikipedia Corpus: 4.6 million arti-\ncles, 1.9 billion words. Adapted from Wikipedia. https:\n//www.english-corpora.org/wiki/ .\nDeerwester, S. C., S. T. Dumais, G. W. Furnas, R. A. Harsh-\nman, T. K. Landauer, K. E. Lochbaum, and L. Streeter.\n1988. Computer information retrieval using latent seman-\ntic structure: US Patent 4,839,853.\nDeerwester, S. C., S. T. Dumais, T. K. Landauer, G. W. Fur-\nnas, and R. A. Harshman. 1990. Indexing by latent se-\nmantics analysis. JASIS , 41(6):391\u2013407.\nEthayarajh, K., D. Duvenaud, and G. Hirst. 2019a. Towards",
    "metadata": {
      "source": "6",
      "chunk_id": 60,
      "token_count": 757,
      "chapter_title": ""
    }
  },
  {
    "content": "word similarity and estimation from sparse data. ACL.\nDavies, M. 2012. Expanding horizons in historical lin-\nguistics with the 400-million word Corpus of Historical\nAmerican English. Corpora , 7(2):121\u2013157.\nDavies, M. 2015. The Wikipedia Corpus: 4.6 million arti-\ncles, 1.9 billion words. Adapted from Wikipedia. https:\n//www.english-corpora.org/wiki/ .\nDeerwester, S. C., S. T. Dumais, G. W. Furnas, R. A. Harsh-\nman, T. K. Landauer, K. E. Lochbaum, and L. Streeter.\n1988. Computer information retrieval using latent seman-\ntic structure: US Patent 4,839,853.\nDeerwester, S. C., S. T. Dumais, T. K. Landauer, G. W. Fur-\nnas, and R. A. Harshman. 1990. Indexing by latent se-\nmantics analysis. JASIS , 41(6):391\u2013407.\nEthayarajh, K., D. Duvenaud, and G. Hirst. 2019a. Towards\nunderstanding linear word analogies. ACL.\nEthayarajh, K., D. Duvenaud, and G. Hirst. 2019b. Under-\nstanding undesirable word embedding associations. ACL.\nFano, R. M. 1961. Transmission of Information: A Statistical\nTheory of Communications . MIT Press.\nFinkelstein, L., E. Gabrilovich, Y . Matias, E. Rivlin,\nZ. Solan, G. Wolfman, and E. Ruppin. 2002. Placing\nsearch in context: The concept revisited. ACM Trans-\nactions on Information Systems , 20(1):116\u2014-131.\nFirth, J. R. 1957. A synopsis of linguistic theory 1930\u2013\n1955. In Studies in Linguistic Analysis . Philological So-\nciety. Reprinted in Palmer, F. (ed.) 1968. Selected Papers\nof J. R. Firth. Longman, Harlow.",
    "metadata": {
      "source": "6",
      "chunk_id": 61,
      "token_count": 482,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 33",
    "metadata": {
      "source": "6",
      "chunk_id": 62,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Exercises 33\nGarg, N., L. Schiebinger, D. Jurafsky, and J. Zou. 2018.\nWord embeddings quantify 100 years of gender and eth-\nnic stereotypes. Proceedings of the National Academy of\nSciences , 115(16):E3635\u2013E3644.\nGirard, G. 1718. La justesse de la langue franc \u00b8oise: ou les\ndiff\u00b4erentes signi\ufb01cations des mots qui passent pour syn-\nonimes . Laurent d\u2019Houry, Paris.\nGiuliano, V . E. 1965. The interpretation of word\nassociations. Statistical Association Methods For\nMechanized Documentation. Symposium Proceed-\nings. Washington, D.C., USA, March 17, 1964 .\nhttps://nvlpubs.nist.gov/nistpubs/Legacy/\nMP/nbsmiscellaneouspub269.pdf .\nGladkova, A., A. Drozd, and S. Matsuoka. 2016. Analogy-\nbased detection of morphological and semantic relations\nwith word embeddings: what works and what doesn\u2019t.\nNAACL Student Research Workshop .\nGlenberg, A. M. and D. A. Robertson. 2000. Symbol ground-\ning and meaning: A comparison of high-dimensional and\nembodied theories of meaning. Journal of memory and\nlanguage , 43(3):379\u2013401.\nGonen, H. and Y . Goldberg. 2019. Lipstick on a pig: Debi-\nasing methods cover up systematic gender biases in word\nembeddings but do not remove them. NAACL HLT .\nGould, S. J. 1980. The Panda\u2019s Thumb . Penguin Group.\nGreenwald, A. G., D. E. McGhee, and J. L. K. Schwartz.\n1998. Measuring individual differences in implicit cogni-\ntion: the implicit association test. Journal of personality\nand social psychology , 74(6):1464\u20131480.\nHamilton, W. L., J. Leskovec, and D. Jurafsky. 2016. Di-\nachronic word embeddings reveal statistical laws of se-\nmantic change. ACL.\nHarris, Z. S. 1954. Distributional structure. Word , 10:146\u2013\n162.\nHellrich, J. and U. Hahn. 2016. Bad company\u2014\nNeighborhoods in neural embedding spaces considered\nharmful. COLING .\nHill, F., R. Reichart, and A. Korhonen. 2015. Simlex-999:\nEvaluating semantic models with (genuine) similarity es-\ntimation. Computational Linguistics , 41(4):665\u2013695.\nHjelmslev, L. 1969. Prologomena to a Theory of Language .\nUniversity of Wisconsin Press. Translated by Francis J.\nWhit\ufb01eld; original Danish edition 1943.\nHofmann, T. 1999. Probabilistic latent semantic indexing.\nSIGIR-99 .\nHuang, E. H., R. Socher, C. D. Manning, and A. Y . Ng. 2012.\nImproving word representations via global context and\nmultiple word prototypes. ACL.\nJia, S., T. Meng, J. Zhao, and K.-W. Chang. 2020. Mitigat-\ning gender bias ampli\ufb01cation in distribution by posterior\nregularization. ACL.",
    "metadata": {
      "source": "6",
      "chunk_id": 63,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "mantic change. ACL.\nHarris, Z. S. 1954. Distributional structure. Word , 10:146\u2013\n162.\nHellrich, J. and U. Hahn. 2016. Bad company\u2014\nNeighborhoods in neural embedding spaces considered\nharmful. COLING .\nHill, F., R. Reichart, and A. Korhonen. 2015. Simlex-999:\nEvaluating semantic models with (genuine) similarity es-\ntimation. Computational Linguistics , 41(4):665\u2013695.\nHjelmslev, L. 1969. Prologomena to a Theory of Language .\nUniversity of Wisconsin Press. Translated by Francis J.\nWhit\ufb01eld; original Danish edition 1943.\nHofmann, T. 1999. Probabilistic latent semantic indexing.\nSIGIR-99 .\nHuang, E. H., R. Socher, C. D. Manning, and A. Y . Ng. 2012.\nImproving word representations via global context and\nmultiple word prototypes. ACL.\nJia, S., T. Meng, J. Zhao, and K.-W. Chang. 2020. Mitigat-\ning gender bias ampli\ufb01cation in distribution by posterior\nregularization. ACL.\nJones, M. P. and J. H. Martin. 1997. Contextual spelling cor-\nrection using latent semantic analysis. ANLP .\nJoos, M. 1950. Description of language design. JASA ,\n22:701\u2013708.\nJurafsky, D. 2014. The Language of Food . W. W. Norton,\nNew York.Jurgens, D., S. M. Mohammad, P. Turney, and K. Holyoak.\n2012. SemEval-2012 task 2: Measuring degrees of rela-\ntional similarity. *SEM 2012 .\nKatz, J. J. and J. A. Fodor. 1963. The structure of a semantic\ntheory. Language , 39:170\u2013210.\nKiela, D. and S. Clark. 2014. A systematic study of semantic\nvector space model parameters. EACL 2nd Workshop on\nContinuous Vector Space Models and their Composition-\nality (CVSC) .\nKim, E. 2019. Optimize computational ef\ufb01ciency\nof skip-gram with negative sampling. https://\naegis4048.github.io/optimize_computational_\nefficiency_of_skip-gram_with_negative_\nsampling .\nLake, B. M. and G. L. Murphy. 2021. Word meaning in\nminds and machines. Psychological Review . In press.\nLandauer, T. K. and S. T. Dumais. 1997. A solution to Plato\u2019s\nproblem: The Latent Semantic Analysis theory of acqui-\nsition, induction, and representation of knowledge. Psy-\nchological Review , 104:211\u2013240.\nLandauer, T. K., D. Laham, B. Rehder, and M. E. Schreiner.\n1997. How well can passage meaning be derived with-\nout using word order? A comparison of Latent Semantic\nAnalysis and humans. COGSCI .\nLapesa, G. and S. Evert. 2014. A large scale evaluation of\ndistributional semantic models: Parameters, interactions\nand model selection. TACL , 2:531\u2013545.\nLee, D. D. and H. S. Seung. 1999. Learning the parts of",
    "metadata": {
      "source": "6",
      "chunk_id": 64,
      "token_count": 761,
      "chapter_title": ""
    }
  },
  {
    "content": "Continuous Vector Space Models and their Composition-\nality (CVSC) .\nKim, E. 2019. Optimize computational ef\ufb01ciency\nof skip-gram with negative sampling. https://\naegis4048.github.io/optimize_computational_\nefficiency_of_skip-gram_with_negative_\nsampling .\nLake, B. M. and G. L. Murphy. 2021. Word meaning in\nminds and machines. Psychological Review . In press.\nLandauer, T. K. and S. T. Dumais. 1997. A solution to Plato\u2019s\nproblem: The Latent Semantic Analysis theory of acqui-\nsition, induction, and representation of knowledge. Psy-\nchological Review , 104:211\u2013240.\nLandauer, T. K., D. Laham, B. Rehder, and M. E. Schreiner.\n1997. How well can passage meaning be derived with-\nout using word order? A comparison of Latent Semantic\nAnalysis and humans. COGSCI .\nLapesa, G. and S. Evert. 2014. A large scale evaluation of\ndistributional semantic models: Parameters, interactions\nand model selection. TACL , 2:531\u2013545.\nLee, D. D. and H. S. Seung. 1999. Learning the parts of\nobjects by non-negative matrix factorization. Nature ,\n401(6755):788\u2013791.\nLevy, O. and Y . Goldberg. 2014a. Dependency-based word\nembeddings. ACL.\nLevy, O. and Y . Goldberg. 2014b. Linguistic regularities in\nsparse and explicit word representations. CoNLL .\nLevy, O. and Y . Goldberg. 2014c. Neural word embedding\nas implicit matrix factorization. NeurIPS .\nLevy, O., Y . Goldberg, and I. Dagan. 2015. Improving dis-\ntributional similarity with lessons learned from word em-\nbeddings. TACL , 3:211\u2013225.\nLi, J., X. Chen, E. H. Hovy, and D. Jurafsky. 2015. Visual-\nizing and understanding neural models in NLP. NAACL\nHLT.\nLin, Y ., J.-B. Michel, E. Lieberman Aiden, J. Orwant,\nW. Brockman, and S. Petrov. 2012. Syntactic annotations\nfor the Google Books NGram corpus. ACL.\nLinzen, T. 2016. Issues in evaluating semantic spaces us-\ning word analogies. 1st Workshop on Evaluating Vector-\nSpace Representations for NLP .\nLuhn, H. P. 1957. A statistical approach to the mechanized\nencoding and searching of literary information. IBM\nJournal of Research and Development , 1(4):309\u2013317.\nManning, C. D., P. Raghavan, and H. Sch \u00a8utze. 2008. Intro-\nduction to Information Retrieval . Cambridge.\nMikolov, T., K. Chen, G. S. Corrado, and J. Dean. 2013a. Ef-\n\ufb01cient estimation of word representations in vector space.\nICLR 2013 .\nMikolov, T., S. Kombrink, L. Burget, J. H. \u02c7Cernock `y, and\nS. Khudanpur. 2011. Extensions of recurrent neural net-\nwork language model. ICASSP .",
    "metadata": {
      "source": "6",
      "chunk_id": 65,
      "token_count": 749,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 34",
    "metadata": {
      "source": "6",
      "chunk_id": 66,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "34 Chapter 6 \u2022 Vector Semantics and Embeddings\nMikolov, T., I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean. 2013b. Distributed representations of words and\nphrases and their compositionality. NeurIPS .\nMikolov, T., W.-t. Yih, and G. Zweig. 2013c. Linguis-\ntic regularities in continuous space word representations.\nNAACL HLT .\nNiwa, Y . and Y . Nitta. 1994. Co-occurrence vectors from\ncorpora vs. distance vectors from dictionaries. COLING .\nNosek, B. A., M. R. Banaji, and A. G. Greenwald. 2002a.\nHarvesting implicit group attitudes and beliefs from a\ndemonstration web site. Group Dynamics: Theory, Re-\nsearch, and Practice , 6(1):101.\nNosek, B. A., M. R. Banaji, and A. G. Greenwald. 2002b.\nMath=male, me=female, therefore math 6=me.Journal of\npersonality and social psychology , 83(1):44.\nOsgood, C. E., G. J. Suci, and P. H. Tannenbaum. 1957. The\nMeasurement of Meaning . University of Illinois Press.\nPennington, J., R. Socher, and C. D. Manning. 2014. GloVe:\nGlobal vectors for word representation. EMNLP .\nPeterson, J. C., D. Chen, and T. L. Grif\ufb01ths. 2020. Parallelo-\ngrams revisited: Exploring the limitations of vector space\nmodels for simple analogies. Cognition , 205.\nPilehvar, M. T. and J. Camacho-Collados. 2019. WiC: the\nword-in-context dataset for evaluating context-sensitive\nmeaning representations. NAACL HLT .\nRehder, B., M. E. Schreiner, M. B. W. Wolfe, D. Laham,\nT. K. Landauer, and W. Kintsch. 1998. Using Latent\nSemantic Analysis to assess knowledge: Some technical\nconsiderations. Discourse Processes , 25(2-3):337\u2013354.\nRohde, D. L. T., L. M. Gonnerman, and D. C. Plaut. 2006.\nAn improved model of semantic similarity based on lexi-\ncal co-occurrence. CACM , 8:627\u2013633.\nRumelhart, D. E. and A. A. Abrahamson. 1973. A model for\nanalogical reasoning. Cognitive Psychology , 5(1):1\u201328.\nSalton, G. 1971. The SMART Retrieval System: Experiments\nin Automatic Document Processing . Prentice Hall.\nSchluter, N. 2018. The word analogy testing caveat. NAACL\nHLT.\nSchone, P. and D. Jurafsky. 2000. Knowlege-free induction\nof morphology using latent semantic analysis. CoNLL .\nSchone, P. and D. Jurafsky. 2001a. Is knowledge-free in-\nduction of multiword unit dictionary headwords a solved\nproblem? EMNLP .\nSchone, P. and D. Jurafsky. 2001b. Knowledge-free induc-\ntion of in\ufb02ectional morphologies. NAACL .",
    "metadata": {
      "source": "6",
      "chunk_id": 67,
      "token_count": 769,
      "chapter_title": ""
    }
  },
  {
    "content": "Semantic Analysis to assess knowledge: Some technical\nconsiderations. Discourse Processes , 25(2-3):337\u2013354.\nRohde, D. L. T., L. M. Gonnerman, and D. C. Plaut. 2006.\nAn improved model of semantic similarity based on lexi-\ncal co-occurrence. CACM , 8:627\u2013633.\nRumelhart, D. E. and A. A. Abrahamson. 1973. A model for\nanalogical reasoning. Cognitive Psychology , 5(1):1\u201328.\nSalton, G. 1971. The SMART Retrieval System: Experiments\nin Automatic Document Processing . Prentice Hall.\nSchluter, N. 2018. The word analogy testing caveat. NAACL\nHLT.\nSchone, P. and D. Jurafsky. 2000. Knowlege-free induction\nof morphology using latent semantic analysis. CoNLL .\nSchone, P. and D. Jurafsky. 2001a. Is knowledge-free in-\nduction of multiword unit dictionary headwords a solved\nproblem? EMNLP .\nSchone, P. and D. Jurafsky. 2001b. Knowledge-free induc-\ntion of in\ufb02ectional morphologies. NAACL .\nSch\u00a8utze, H. 1992. Dimensions of meaning. Proceedings of\nSupercomputing \u201992 . IEEE Press.\nSch\u00a8utze, H. 1997. Ambiguity Resolution in Language Learn-\ning \u2013 Computational and Cognitive Models . CSLI, Stan-\nford, CA.\nSch\u00a8utze, H., D. A. Hull, and J. Pedersen. 1995. A compar-\nison of classi\ufb01ers and document representations for the\nrouting problem. SIGIR-95 .\nSch\u00a8utze, H. and J. Pedersen. 1993. A vector model for syn-\ntagmatic and paradigmatic relatedness. 9th Annual Con-\nference of the UW Centre for the New OED and Text Re-\nsearch .Sparck Jones, K. 1972. A statistical interpretation of term\nspeci\ufb01city and its application in retrieval. Journal of Doc-\numentation , 28(1):11\u201321.\nSparck Jones, K. 1986. Synonymy and Semantic Classi\ufb01ca-\ntion. Edinburgh University Press, Edinburgh. Republica-\ntion of 1964 PhD Thesis.\nSwitzer, P. 1965. Vector images in document retrieval.\nStatistical Association Methods For Mechanized Docu-\nmentation. Symposium Proceedings. Washington, D.C.,\nUSA, March 17, 1964 .https://nvlpubs.nist.gov/\nnistpubs/Legacy/MP/nbsmiscellaneouspub269.\npdf.\nTian, Y ., V . Kulkarni, B. Perozzi, and S. Skiena. 2016. On\nthe convergent properties of word embedding methods.\nArXiv preprint arXiv:1605.03956.\nTurian, J., L. Ratinov, and Y . Bengio. 2010. Word represen-\ntations: a simple and general method for semi-supervised\nlearning. ACL.\nTurney, P. D. and M. L. Littman. 2005. Corpus-based learn-\ning of analogies and semantic relations. Machine Learn-\ning, 60(1-3):251\u2013278.",
    "metadata": {
      "source": "6",
      "chunk_id": 68,
      "token_count": 748,
      "chapter_title": ""
    }
  },
  {
    "content": "umentation , 28(1):11\u201321.\nSparck Jones, K. 1986. Synonymy and Semantic Classi\ufb01ca-\ntion. Edinburgh University Press, Edinburgh. Republica-\ntion of 1964 PhD Thesis.\nSwitzer, P. 1965. Vector images in document retrieval.\nStatistical Association Methods For Mechanized Docu-\nmentation. Symposium Proceedings. Washington, D.C.,\nUSA, March 17, 1964 .https://nvlpubs.nist.gov/\nnistpubs/Legacy/MP/nbsmiscellaneouspub269.\npdf.\nTian, Y ., V . Kulkarni, B. Perozzi, and S. Skiena. 2016. On\nthe convergent properties of word embedding methods.\nArXiv preprint arXiv:1605.03956.\nTurian, J., L. Ratinov, and Y . Bengio. 2010. Word represen-\ntations: a simple and general method for semi-supervised\nlearning. ACL.\nTurney, P. D. and M. L. Littman. 2005. Corpus-based learn-\ning of analogies and semantic relations. Machine Learn-\ning, 60(1-3):251\u2013278.\nvan der Maaten, L. and G. E. Hinton. 2008. Visualizing high-\ndimensional data using t-SNE. JMLR , 9:2579\u20132605.\nWierzbicka, A. 1992. Semantics, Culture, and Cognition:\nUniversity Human Concepts in Culture-Speci\ufb01c Con\ufb01gu-\nrations . Oxford University Press.\nWierzbicka, A. 1996. Semantics: Primes and Universals .\nOxford University Press.\nWittgenstein, L. 1953. Philosophical Investigations. (Trans-\nlated by Anscombe, G.E.M.) . Blackwell.\nZhao, J., T. Wang, M. Yatskar, V . Ordonez, and K.-\nW. Chang. 2017. Men also like shopping: Reducing\ngender bias ampli\ufb01cation using corpus-level constraints.\nEMNLP .\nZhao, J., Y . Zhou, Z. Li, W. Wang, and K.-W. Chang. 2018.\nLearning gender-neutral word embeddings. EMNLP .",
    "metadata": {
      "source": "6",
      "chunk_id": 69,
      "token_count": 511,
      "chapter_title": ""
    }
  }
]