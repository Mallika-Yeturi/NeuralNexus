[
  {
    "content": "# 18\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n18Context-Free Grammars and\nConstituency Parsing\nBecause the Night by Bruce Springsteen and Patti Smith\nThe Fire Next Time by James Baldwin\nIf on a winter\u2019s night a traveler by Italo Calvino\nLove Actually by Richard Curtis\nSuddenly Last Summer by Tennessee Williams\nA Scanner Darkly by Philip K. Dick\nSix titles that are not constituents, from Geoffrey K. Pullum on\nLanguage Log (who was pointing out their incredible rarity).\nOne morning I shot an elephant in my pajamas.\nHow he got into my pajamas I don\u2019t know.\nGroucho Marx, Animal Crackers , 1930\nThe study of grammar has an ancient pedigree. The grammar of Sanskrit was\ndescribed by the Indian grammarian P \u00afan.ini sometime between the 7th and 4th cen-\nturies BCE, in his famous treatise the As .t.\u00afadhy \u00afay\u00af\u0131 (\u20188 books\u2019). And our word syntax syntax\ncomes from the Greek s\u00b4yntaxis , meaning \u201csetting out together or arrangement\u201d, and\nrefers to the way words are arranged together. We have seen syntactic notions in pre-\nvious chapters like the use of part-of-speech categories (Chapter 17). In this chapter\nand the next one we introduce formal models for capturing more sophisticated no-\ntions of grammatical structure and algorithms for parsing these structures.\nOur focus in this chapter is context-free grammars and the CKY algorithm\nfor parsing them. Context-free grammars are the backbone of many formal mod-\nels of the syntax of natural language (and, for that matter, of computer languages).\nSyntactic parsing is the task of assigning a syntactic structure to a sentence. Parse\ntrees (whether for context-free grammars or for the dependency or CCG formalisms\nwe introduce in following chapters) can be used in applications such as grammar\nchecking : sentence that cannot be parsed may have grammatical errors (or at least\nbe hard to read). Parse trees can be an intermediate stage of representation for for-\nmal semantic analysis . And parsers and the grammatical structure they assign a\nsentence are a useful text analysis tool for text data science applications that require\nmodeling the relationship of elements in sentences.\nIn this chapter we introduce context-free grammars, give a small sample gram-\nmar of English, introduce more formal de\ufb01nitions of context-free grammars and\ngrammar normal form, and talk about treebanks : corpora that have been anno-\ntated with syntactic structure. We then discuss parse ambiguity and the problems\nit presents, and turn to parsing itself, giving the famous Cocke-Kasami-Younger\n(CKY) algorithm (Kasami 1965, Younger 1967), the standard dynamic program-\nming approach to syntactic parsing. The CKY algorithm returns an ef\ufb01cient repre-\nsentation of the set of parse trees for a sentence, but doesn\u2019t tell us which parse tree\nis the right one. For that, we need to augment CKY with scores for each possible\nconstituent. We\u2019ll see how to do this with neural span-based parsers. Finally, we\u2019ll\nintroduce the standard set of metrics for evaluating parser accuracy.",
    "metadata": {
      "source": "18",
      "chunk_id": 0,
      "token_count": 722,
      "chapter_title": "18"
    }
  },
  {
    "content": "## Page 2\n\n2CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\n18.1 Constituency\nSyntactic constituency is the idea that groups of words can behave as single units,\nor constituents. Part of developing a grammar involves building an inventory of the\nconstituents in the language. How do words group together in English? Consider\nthenoun phrase , a sequence of words surrounding at least one noun. Here are some noun phrase\nexamples of noun phrases (thanks to Damon Runyon):\nHarry the Horse a high-class spot such as Mindy\u2019s\nthe Broadway coppers the reason he comes into the Hot Box\nthey three parties from Brooklyn\nWhat evidence do we have that these words group together (or \u201cform constituents\u201d)?\nOne piece of evidence is that they can all appear in similar syntactic environments,\nfor example, before a verb.\nthree parties from Brooklyn arrive . . .\na high-class spot such as Mindy\u2019s attracts . . .\nthe Broadway coppers love. . .\nthey sit\nBut while the whole noun phrase can occur before a verb, this is not true of each\nof the individual words that make up a noun phrase. The following are not grammat-\nical sentences of English (recall that we use an asterisk (*) to mark fragments that\nare not grammatical English sentences):\n*from arrive . . .*asattracts . . .\n*the is. . . *spot sat. . .\nThus, to correctly describe facts about the ordering of these words in English, we\nmust be able to say things like \u201c Noun Phrases can occur before verbs \u201d. Let\u2019s now\nsee how to do this in a more formal way!\n18.2 Context-Free Grammars\nA widely used formal system for modeling constituent structure in natural lan-\nguage is the context-free grammar , orCFG . Context-free grammars are also called CFG\nphrase-structure grammars , and the formalism is equivalent to Backus-Naur form ,\norBNF . The idea of basing a grammar on constituent structure dates back to the psy-\nchologist Wilhelm Wundt (1900) but was not formalized until Chomsky (1956) and,\nindependently, Backus (1959).\nA context-free grammar consists of a set of rules orproductions , each of which rules\nexpresses the ways that symbols of the language can be grouped and ordered to-\ngether, and a lexicon of words and symbols. For example, the following productions lexicon\nexpress that an NP(ornoun phrase ) can be composed of either a ProperNoun or NP\na determiner ( Det) followed by a Nominal ; aNominal in turn can consist of one or",
    "metadata": {
      "source": "18",
      "chunk_id": 1,
      "token_count": 568,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\n18.2 \u2022 C ONTEXT -FREE GRAMMARS 3\nmore Noun s.1\nNP!Det Nominal\nNP!ProperNoun\nNominal!NounjNominal Noun\nContext-free rules can be hierarchically embedded, so we can combine the previous\nrules with others, like the following, that express facts about the lexicon:\nDet!a\nDet!the\nNoun!\ufb02ight\nThe symbols that are used in a CFG are divided into two classes. The symbols\nthat correspond to words in the language (\u201cthe\u201d, \u201cnightclub\u201d) are called terminal terminal\nsymbols; the lexicon is the set of rules that introduce these terminal symbols. The\nsymbols that express abstractions over these terminals are called non-terminals . In non-terminal\neach context-free rule, the item to the right of the arrow ( !) is an ordered list of one\nor more terminals and non-terminals; to the left of the arrow is a single non-terminal\nsymbol expressing some cluster or generalization. The non-terminal associated with\neach word in the lexicon is its lexical category, or part of speech.\nA CFG can be thought of in two ways: as a device for generating sentences\nand as a device for assigning a structure to a given sentence. Viewing a CFG as a\ngenerator, we can read the !arrow as \u201crewrite the symbol on the left with the string\nof symbols on the right\u201d.\nSo starting from the symbol: NP\nwe can use our \ufb01rst rule to rewrite NPas: Det Nominal\nand then rewrite Nominal as: Noun\nand \ufb01nally rewrite these parts-of-speech as: a \ufb02ight\nWe say the string a \ufb02ight can be derived from the non-terminal NP. Thus, a CFG\ncan be used to generate a set of strings. This sequence of rule expansions is called a\nderivation of the string of words. It is common to represent a derivation by a parse derivation\ntree (commonly shown inverted with the root at the top). Figure 18.1 shows the tree parse tree\nrepresentation of this derivation.\nNP\nNom\nNoun\n\ufb02ightDet\na\nFigure 18.1 A parse tree for \u201ca \ufb02ight\u201d.\nIn the parse tree shown in Fig. 18.1, we can say that the node NPdominates dominates\nall the nodes in the tree ( Det,Nom ,Noun ,a,\ufb02ight ). We can say further that it\nimmediately dominates the nodes DetandNom .\nThe formal language de\ufb01ned by a CFG is the set of strings that are derivable\nfrom the designated start symbol . Each grammar must have one designated start start symbol\n1When talking about these rules we can pronounce the rightarrow !as \u201cgoes to\u201d, and so we might\nread the \ufb01rst rule above as \u201cNP goes to Det Nominal\u201d.",
    "metadata": {
      "source": "18",
      "chunk_id": 2,
      "token_count": 624,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\nsymbol, which is often called S. Since context-free grammars are often used to de\ufb01ne\nsentences, Sis usually interpreted as the \u201csentence\u201d node, and the set of strings that\nare derivable from Sis the set of sentences in some simpli\ufb01ed version of English.\nLet\u2019s add a few additional rules to our inventory. The following rule expresses\nthe fact that a sentence can consist of a noun phrase followed by a verb phrase : verb phrase\nS!NP VP I prefer a morning \ufb02ight\nA verb phrase in English consists of a verb followed by assorted other things;\nfor example, one kind of verb phrase consists of a verb followed by a noun phrase:\nVP!Verb NP prefer a morning \ufb02ight\nOr the verb may be followed by a noun phrase and a prepositional phrase:\nVP!Verb NP PP leave Boston in the morning\nOr the verb phrase may have a verb followed by a prepositional phrase alone:\nVP!Verb PP leaving on Thursday\nA prepositional phrase generally has a preposition followed by a noun phrase.\nFor example, a common type of prepositional phrase in the ATIS corpus is used to\nindicate location or direction:\nPP!Preposition NP from Los Angeles\nTheNPinside a PPneed not be a location; PPs are often used with times and\ndates, and with other nouns as well; they can be arbitrarily complex. Here are ten\nexamples from the ATIS corpus:\nto Seattle on these \ufb02ights\nin Minneapolis about the ground transportation in Chicago\non Wednesday of the round trip \ufb02ight on United Airlines\nin the evening of the AP \ufb01fty seven \ufb02ight\non the ninth of July with a stopover in Nashville\nFigure 18.2 gives a sample lexicon, and Fig. 18.3 summarizes the grammar rules\nwe\u2019ve seen so far, which we\u2019ll call L0. Note that we can use the or-symbol jto\nindicate that a non-terminal has alternate possible expansions.\nNoun!\ufb02ightsj\ufb02ightjbreezejtripjmorning\nVerb!isjpreferjlikejneedjwantj\ufb02yjdo\nAdjective!cheapestjnon-stopj\ufb01rstjlatest\njotherjdirect\nPronoun!mejIjyoujit\nProper-Noun!AlaskajBaltimorejLos Angeles\njChicagojUnitedjAmerican\nDeterminer!thejajanjthisjthesejthat\nPreposition!fromjtojonjnearjin\nConjunction!andjorjbut\nFigure 18.2 The lexicon for L0.\nWe can use this grammar to generate sentences of this \u201cATIS-language\u201d. We\nstart with S, expand it to NP VP , then choose a random expansion of NP(let\u2019s say, to",
    "metadata": {
      "source": "18",
      "chunk_id": 3,
      "token_count": 628,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\n18.2 \u2022 C ONTEXT -FREE GRAMMARS 5\nGrammar Rules Examples\nS!NP VP I + want a morning \ufb02ight\nNP!Pronoun I\njProper-Noun Los Angeles\njDet Nominal a + \ufb02ight\nNominal!Nominal Noun morning + \ufb02ight\njNoun \ufb02ights\nVP!Verb do\njVerb NP want + a \ufb02ight\njVerb NP PP leave + Boston + in the morning\njVerb PP leaving + on Thursday\nPP!Preposition NP from + Los Angeles\nFigure 18.3 The grammar for L0, with example phrases for each rule.\nS\nVP\nNP\nNom\nNoun\n\ufb02ightNom\nNoun\nmorningDet\naVerb\npreferNP\nPro\nI\nFigure 18.4 The parse tree for \u201cI prefer a morning \ufb02ight\u201d according to grammar L0.\nI), and a random expansion of VP(let\u2019s say, to Verb NP ), and so on until we generate\nthe string I prefer a morning \ufb02ight . Figure 18.4 shows a parse tree that represents a\ncomplete derivation of I prefer a morning \ufb02ight .\nWe can also represent a parse tree in a more compact format called bracketed\nnotation ; here is the bracketed representation of the parse tree of Fig. 18.4:bracketed\nnotation\n(18.1) [ S[NP[ProI]] [VP[Vprefer] [ NP[Deta] [Nom [Nmorning] [ Nom [N\ufb02ight]]]]]]\nA CFG like that of L0de\ufb01nes a formal language. Sentences (strings of words)\nthat can be derived by a grammar are in the formal language de\ufb01ned by that gram-\nmar, and are called grammatical sentences. Sentences that cannot be derived by grammatical\na given formal grammar are not in the language de\ufb01ned by that grammar and are\nreferred to as ungrammatical . This hard line between \u201cin\u201d and \u201cout\u201d characterizes ungrammatical\nall formal languages but is only a very simpli\ufb01ed model of how natural languages\nreally work. This is because determining whether a given sentence is part of a given\nnatural language (say, English) often depends on the context. In linguistics, the use\nof formal languages to model natural languages is called generative grammar sincegenerative\ngrammar\nthe language is de\ufb01ned by the set of possible sentences \u201cgenerated\u201d by the grammar.\n(Note that this is a different sense of the word \u2018generate\u2019 than when we talk about",
    "metadata": {
      "source": "18",
      "chunk_id": 4,
      "token_count": 571,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\nlanguage models generating text.)\n18.2.1 Formal De\ufb01nition of Context-Free Grammar\nWe conclude this section with a quick, formal description of a context-free gram-\nmar and the language it generates. A context-free grammar Gis de\ufb01ned by four\nparameters: N;S;R;S(technically it is a \u201c4-tuple\u201d).\nNa set of non-terminal symbols (orvariables )\nSa set of terminal symbols (disjoint from N)\nRa set of rules or productions, each of the form A!b,\nwhere Ais a non-terminal,\nbis a string of symbols from the in\ufb01nite set of strings (S[N)\u0003\nSa designated start symbol and a member of N\nFor the remainder of the book we adhere to the following conventions when dis-\ncussing the formal properties of context-free grammars (as opposed to explaining\nparticular facts about English or other languages).\nCapital letters like A,B, and S Non-terminals\nS The start symbol\nLower-case Greek letters like a,b, and g Strings drawn from (S[N)\u0003\nLower-case Roman letters like u,v, and w Strings of terminals\nA language is de\ufb01ned through the concept of derivation. One string derives an-\nother one if it can be rewritten as the second one by some series of rule applications.\nMore formally, following Hopcroft and Ullman (1979),\nifA!bis a production of Randaandgare any strings in the set\n(S[N)\u0003, then we say that aAgdirectly derives abg , oraAg)abg . directly derives\nDerivation is then a generalization of direct derivation:\nLeta1;a2;:::; ambe strings in (S[N)\u0003;m\u00151, such that\na1)a2;a2)a3;:::; am\u00001)am\nWe say that a1derives am, ora1\u0003)am. derives\nWe can then formally de\ufb01ne the language LGgenerated by a grammar Gas the\nset of strings composed of terminal symbols that can be derived from the designated\nstart symbol S.\nLG=fwjwis inS\u0003andS\u0003)wg\nThe problem of mapping from a string of words to its parse tree is called syn-\ntactic parsing , as we\u2019ll see in Section 18.6.syntactic\nparsing\n18.3 Treebanks\nA corpus in which every sentence is annotated with a parse tree is called a treebank . treebank",
    "metadata": {
      "source": "18",
      "chunk_id": 5,
      "token_count": 550,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\n18.3 \u2022 T REEBANKS 7\nTreebanks play an important role in parsing as well as in linguistic investigations of\nsyntactic phenomena.\nTreebanks are generally made by running a parser over each sentence and then\nhaving the resulting parse hand-corrected by human linguists. Figure 18.5 shows\nsentences from the Penn Treebank project, which includes various treebanks in Penn Treebank\nEnglish, Arabic, and Chinese. The Penn Treebank part-of-speech tagset was de\ufb01ned\nin Chapter 17, but we\u2019ll see minor formatting differences across treebanks. The use\nof LISP-style parenthesized notation for trees is extremely common and resembles\nthe bracketed notation we saw earlier in (18.1). For those who are not familiar with\nit we show a standard node-and-line tree representation in Fig. 18.6.\n((S\n(NP-SBJ (DT That)\n(JJ cold) (, ,)\n(JJ empty) (NN sky) )\n(VP (VBD was)\n(ADJP-PRD (JJ full)\n(PP (IN of)\n(NP (NN fire)\n(CC and)\n(NN light) ))))\n(. .) ))((S\n(NP-SBJ The/DT flight/NN )\n(VP should/MD\n(VP arrive/VB\n(PP-TMP at/IN\n(NP eleven/CD a.m/RB ))\n(NP-TMP tomorrow/NN )))))\n(a) (b)\nFigure 18.5 Parses from the LDC Treebank3 for (a) Brown and (b) ATIS sentences.\nS\n.\n.VP\nADJP-PRD\nPP\nNP\nNN\nlightCC\nandNN\n\ufb01reIN\nofJJ\nfullVBD\nwasNP-SBJ\nNN\nskyJJ\nempty,\n,JJ\ncoldDT\nThat\nFigure 18.6 The tree corresponding to the Brown corpus sentence in the previous \ufb01gure.\nThe sentences in a treebank implicitly constitute a grammar of the language. For\nexample, from the parsed sentences in Fig. 18.5 we can extract the CFG rules shown\nin Fig. 18.7 (with rule suf\ufb01xes ( -SBJ ) stripped for simplicity). The grammar used\nto parse the Penn Treebank is very \ufb02at, resulting in very many rules. For example,",
    "metadata": {
      "source": "18",
      "chunk_id": 6,
      "token_count": 505,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\nGrammar Lexicon\nS!NP VP . DT!thejthat\nS!NP VP JJ!coldjemptyjfull\nNP!CD RB NN!skyj\ufb01rejlightj\ufb02ightjtomorrow\nNP!DT NN CC!and\nNP!NN CC NN IN!ofjat\nNP!DT JJ , JJ NN CD!eleven\nNP!NN RB!a.m.\nVP!MD VP VB!arrive\nVP!VBD ADJP VBD!wasjsaid\nVP!MD VP MD!shouldjwould\nVP!VB PP NP\nADJP!JJ PP\nPP!IN NP\nFigure 18.7 CFG grammar rules and lexicon from the treebank sentences in Fig. 18.5.\namong the approximately 4,500 different rules for expanding VPs are separate rules\nfor PP sequences of any length and every possible arrangement of verb arguments:\nVP!VBD PP\nVP!VBD PP PP\nVP!VBD PP PP PP\nVP!VBD PP PP PP PP\nVP!VB ADVP PP\nVP!VB PP ADVP\nVP!ADVP VB PP\n18.4 Grammar Equivalence and Normal Form\nA formal language is de\ufb01ned as a (possibly in\ufb01nite) set of strings of words. This sug-\ngests that we could ask if two grammars are equivalent by asking if they generate the\nsame set of strings. In fact, it is possible to have two distinct context-free grammars\ngenerate the same language. We say that two grammars are strongly equivalent ifstrongly\nequivalent\nthey generate the same set of strings andif they assign the same phrase structure\nto each sentence (allowing merely for renaming of the non-terminal symbols). Two\ngrammars are weakly equivalent if they generate the same set of strings but do notweakly\nequivalent\nassign the same phrase structure to each sentence.\nIt is sometimes useful to have a normal form for grammars, in which each of normal form\nthe productions takes a particular form. For example, a context-free grammar is in\nChomsky normal form (CNF) (Chomsky, 1963) if it is \u000f-free and if in additionChomsky\nnormal form\neach production is either of the form A!B C orA!a. That is, the right-hand side\nof each rule either has two non-terminal symbols or one terminal symbol. Chomsky\nnormal form grammars are binary branching , that is they have binary trees (downbinary\nbranching\nto the prelexical nodes). We make use of this binary branching property in the CKY\nparsing algorithm in Section 18.6.\nAny context-free grammar can be converted into a weakly equivalent Chomsky\nnormal form grammar. For example, a rule of the form\nA!B C D\ncan be converted into the following two CNF rules (Exercise 18.1 asks the reader to",
    "metadata": {
      "source": "18",
      "chunk_id": 7,
      "token_count": 652,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\n18.5 \u2022 A MBIGUITY 9\nGrammar Lexicon\nS!NP VP Det!thatjthisjtheja\nS!Aux NP VP Noun!bookj\ufb02ightjmealjmoney\nS!VP Verb!bookjincludejprefer\nNP!Pronoun Pronoun!Ijshejme\nNP!Proper-Noun Proper-Noun!HoustonjUnited\nNP!Det Nominal Aux!does\nNominal!Noun Preposition!fromjtojonjnearjthrough\nNominal!Nominal Noun\nNominal!Nominal PP\nVP!Verb\nVP!Verb NP\nVP!Verb NP PP\nVP!Verb PP\nVP!VP PP\nPP!Preposition NP\nFigure 18.8 TheL1miniature English grammar and lexicon.\nformulate the complete algorithm):\nA!B X\nX!C D\nSometimes using binary branching can actually produce smaller grammars. For\nexample, the sentences that might be characterized as\nVP -> VBD NP PP*\nare represented in the Penn Treebank by this series of rules:\nVP!VBD NP PP\nVP!VBD NP PP PP\nVP!VBD NP PP PP PP\nVP!VBD NP PP PP PP PP\n...\nbut could also be generated by the following two-rule grammar:\nVP!VBD NP PP\nVP!VP PP\nThe generation of a symbol A with a potentially in\ufb01nite sequence of symbols B with\na rule of the form A!A Bis known as Chomsky-adjunction .Chomsky-\nadjunction\n18.5 Ambiguity\nAmbiguity is the most serious problem faced by syntactic parsers. Chapter 17 intro-\nduced the notions of part-of-speech ambiguity andpart-of-speech disambigua-\ntion. Here, we introduce a new kind of ambiguity, called structural ambiguity ,structural\nambiguity\nillustrated with a new toy grammar L1, shown in Figure 18.8, which adds a few\nrules to the L0grammar.\nStructural ambiguity occurs when the grammar can assign more than one parse\nto a sentence. Groucho Marx\u2019s well-known line as Captain Spaulding in Animal",
    "metadata": {
      "source": "18",
      "chunk_id": 8,
      "token_count": 470,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\nS\nVP\nNP\nNominal\nPP\nin my pajamasNominal\nNoun\nelephantDet\nanVerb\nshotNP\nPronoun\nIS\nVP\nPP\nin my pajamasVP\nNP\nNominal\nNoun\nelephantDet\nanVerb\nshotNP\nPronoun\nI\nFigure 18.9 Two parse trees for an ambiguous sentence. The parse on the left corresponds to the humorous\nreading in which the elephant is in the pajamas, the parse on the right corresponds to the reading in which\nCaptain Spaulding did the shooting in his pajamas.\nCrackers is ambiguous because the phrase in my pajamas can be part of the NP\nheaded by elephant or a part of the verb phrase headed by shot. Figure 18.9 illus-\ntrates these two analyses of Marx\u2019s line using rules from L1.\nStructural ambiguity, appropriately enough, comes in many forms. Two common\nkinds of ambiguity are attachment ambiguity andcoordination ambiguity . A\nsentence has an attachment ambiguity if a particular constituent can be attached toattachment\nambiguity\nthe parse tree at more than one place. The Groucho Marx sentence is an example\nofPP-attachment ambiguity : the preposition phrase can be attached either as partPP-attachment\nambiguity\nof the NP or as part of the VP. Various kinds of adverbial phrases are also subject\nto this kind of ambiguity. For instance, in the following example the gerundive-VP\n\ufb02ying to Paris can be part of a gerundive sentence whose subject is the Eiffel Tower\nor it can be an adjunct modifying the VP headed by saw:\n(18.2) We saw the Eiffel Tower \ufb02ying to Paris.\nIncoordination ambiguity phrases can be conjoined by a conjunction like and.coordination\nambiguity\nFor example, the phrase old men and women can be bracketed as [old [men and\nwomen]] , referring to old men andold women , or as [old men] and [women] , in\nwhich case it is only the men who are old. These ambiguities combine in complex\nways in real sentences, like the following news sentence from the Brown corpus:\n(18.3) President Kennedy today pushed aside other White House business to\ndevote all his time and attention to working on the Berlin crisis address he\nwill deliver tomorrow night to the American people over nationwide\ntelevision and radio.\nThis sentence has a number of ambiguities, although since they are semantically\nunreasonable, it requires a careful reading to see them. The last noun phrase could be\nparsed [nationwide [television and radio]] or[[nationwide television] and radio] .\nThe direct object of pushed aside should be other White House business but could\nalso be the bizarre phrase [other White House business to devote all his time and\nattention to working] (i.e., a structure like Kennedy af\ufb01rmed [his intention to propose\na new budget to address the de\ufb01cit] ). Then the phrase on the Berlin crisis address he",
    "metadata": {
      "source": "18",
      "chunk_id": 9,
      "token_count": 677,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\n18.6 \u2022 CKY P ARSING : A D YNAMIC PROGRAMMING APPROACH 11\nwill deliver tomorrow night to the American people could be an adjunct modifying\nthe verb pushed . APPlikeover nationwide television and radio could be attached\nto any of the higher VPs orNPs (e.g., it could modify people ornight ).\nThe fact that there are many grammatically correct but semantically unreason-\nable parses for naturally occurring sentences is an irksome problem that affects all\nparsers. Fortunately, the CKY algorithm below is designed to ef\ufb01ciently handle\nstructural ambiguities. And as we\u2019ll see in the following section, we can augment\nCKY with neural methods to choose a single correct parse by syntactic disambigua-\ntion.syntactic\ndisambiguation\n18.6 CKY Parsing: A Dynamic Programming Approach\nDynamic programming provides a powerful framework for addressing the prob-\nlems caused by ambiguity in grammars. Recall that a dynamic programming ap-\nproach systematically \ufb01lls in a table of solutions to subproblems. The complete\ntable has the solution to all the subproblems needed to solve the problem as a whole.\nIn the case of syntactic parsing, these subproblems represent parse trees for all the\nconstituents detected in the input.\nThe dynamic programming advantage arises from the context-free nature of our\ngrammar rules\u2014once a constituent has been discovered in a segment of the input we\ncan record its presence and make it available for use in any subsequent derivation\nthat might require it. This provides both time and storage ef\ufb01ciencies since subtrees\ncan be looked up in a table, not reanalyzed. This section presents the Cocke-Kasami-\nYounger (CKY) algorithm, the most widely used dynamic-programming based ap-\nproach to parsing. Chart parsing (Kaplan 1973, Kay 1982) is a related approach,\nand dynamic programming methods are often referred to as chart parsing methods. chart parsing\n18.6.1 Conversion to Chomsky Normal Form\nThe CKY algorithm requires grammars to \ufb01rst be in Chomsky Normal Form (CNF).\nRecall from Section 18.4 that grammars in CNF are restricted to rules of the form\nA!B CorA!w. That is, the right-hand side of each rule must expand either to\ntwo non-terminals or to a single terminal. Restricting a grammar to CNF does not\nlead to any loss in expressiveness, since any context-free grammar can be converted\ninto a corresponding CNF grammar that accepts exactly the same set of strings as\nthe original grammar.\nLet\u2019s start with the process of converting a generic CFG into one represented in\nCNF. Assuming we\u2019re dealing with an \u000f-free grammar, there are three situations we\nneed to address in any generic grammar: rules that mix terminals with non-terminals\non the right-hand side, rules that have a single non-terminal on the right-hand side,\nand rules in which the length of the right-hand side is greater than 2.\nThe remedy for rules that mix terminals and non-terminals is to simply introduce\na new dummy non-terminal that covers only the original terminal. For example, a\nrule for an in\ufb01nitive verb phrase such as INF-VP!to VP would be replaced by the\ntwo rules INF-VP!TO VP andTO!to.\nRules with a single non-terminal on the right are called unit productions . WeUnit\nproductions\ncan eliminate unit productions by rewriting the right-hand side of the original rules\nwith the right-hand side of all the non-unit production rules that they ultimately lead\nto. More formally, if A\u0003)Bby a chain of one or more unit productions and B!g",
    "metadata": {
      "source": "18",
      "chunk_id": 10,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\nis a non-unit production in our grammar, then we add A!gfor each such rule in\nthe grammar and discard all the intervening unit productions. As we demonstrate\nwith our toy grammar, this can lead to a substantial \ufb02attening of the grammar and a\nconsequent promotion of terminals to fairly high levels in the resulting trees.\nRules with right-hand sides longer than 2 are normalized through the introduc-\ntion of new non-terminals that spread the longer sequences over several new rules.\nFormally, if we have a rule like\nA!B Cg\nwe replace the leftmost pair of non-terminals with a new non-terminal and introduce\na new production, resulting in the following new rules:\nA!X1g\nX1!B C\nIn the case of longer right-hand sides, we simply iterate this process until the of-\nfending rule has been replaced by rules of length 2. The choice of replacing the\nleftmost pair of non-terminals is purely arbitrary; any systematic scheme that results\nin binary rules would suf\ufb01ce.\nIn our current grammar, the rule S!Aux NP VP would be replaced by the two\nrules S!X1 VP andX1!Aux NP .\nThe entire conversion process can be summarized as follows:\n1. Copy all conforming rules to the new grammar unchanged.\n2. Convert terminals within rules to dummy non-terminals.\n3. Convert unit productions.\n4. Make all rules binary and add them to new grammar.\nFigure 18.10 shows the results of applying this entire conversion procedure to\ntheL1grammar introduced earlier on page 9. Note that this \ufb01gure doesn\u2019t show\nthe original lexical rules; since these original lexical rules are already in CNF, they\nall carry over unchanged to the new grammar. Figure 18.10 does, however, show\nthe various places where the process of eliminating unit productions has, in effect,\ncreated new lexical rules. For example, all the original verbs have been promoted to\nboth VPs and to Ss in the converted grammar.\n18.6.2 CKY Recognition\nWith our grammar now in CNF, each non-terminal node above the part-of-speech\nlevel in a parse tree will have exactly two daughters. A two-dimensional matrix can\nbe used to encode the structure of an entire tree. For a sentence of length n, we will\nwork with the upper-triangular portion of an (n+1)\u0002(n+1)matrix. Each cell [i;j]\nin this matrix contains the set of non-terminals that represent all the constituents that\nspan positions ithrough jof the input. Since our indexing scheme begins with 0, it\u2019s\nnatural to think of the indexes as pointing at the gaps between the input words (as in\n0Book 1that 2\ufb02ight 3). These gaps are often called fenceposts , on the metaphor of fenceposts\nthe posts between segments of fencing. It follows then that the cell that represents\nthe entire input resides in position [0;n]in the matrix.\nSince each non-terminal entry in our table has two daughters in the parse, it fol-\nlows that for each constituent represented by an entry [i;j], there must be a position\nin the input, k, where it can be split into two parts such that i<k<j. Given such",
    "metadata": {
      "source": "18",
      "chunk_id": 11,
      "token_count": 728,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\n18.6 \u2022 CKY P ARSING : A D YNAMIC PROGRAMMING APPROACH 13\nL1Grammar L1in CNF\nS!NP VP S!NP VP\nS!Aux NP VP S!X1 VP\nX1!Aux NP\nS!VP S!bookjincludejprefer\nS!Verb NP\nS!X2 PP\nS!Verb PP\nS!VP PP\nNP!Pronoun NP!Ijshejme\nNP!Proper-Noun NP!UnitedjHouston\nNP!Det Nominal NP!Det Nominal\nNominal!Noun Nominal!bookj\ufb02ightjmealjmoney\nNominal!Nominal Noun Nominal!Nominal Noun\nNominal!Nominal PP Nominal!Nominal PP\nVP!Verb VP!bookjincludejprefer\nVP!Verb NP VP!Verb NP\nVP!Verb NP PP VP!X2 PP\nX2!Verb NP\nVP!Verb PP VP!Verb PP\nVP!VP PP VP!VP PP\nPP!Preposition NP PP!Preposition NP\nFigure 18.10 L1Grammar and its conversion to CNF. Note that although they aren\u2019t shown\nhere, all the original lexical entries from L1carry over unchanged as well.\na position k, the \ufb01rst constituent [i;k]must lie to the left of entry [i;j]somewhere\nalong row i, and the second entry [k;j]must lie beneath it, along column j.\nTo make this more concrete, consider the following example with its completed\nparse matrix, shown in Fig. 18.11.\n(18.4) Book the \ufb02ight through Houston.\nThe superdiagonal row in the matrix contains the parts of speech for each word in\nthe input. The subsequent diagonals above that superdiagonal contain constituents\nthat cover all the spans of increasing length in the input.\nGiven this setup, CKY recognition consists of \ufb01lling the parse table in the right\nway. To do this, we\u2019ll proceed in a bottom-up fashion so that at the point where we\nare \ufb01lling any cell [i;j], the cells containing the parts that could contribute to this\nentry (i.e., the cells to the left and the cells below) have already been \ufb01lled. The\nalgorithm given in Fig. 18.12 \ufb01lls the upper-triangular matrix a column at a time\nworking from left to right, with each column \ufb01lled from bottom to top, as the right\nside of Fig. 18.11 illustrates. This scheme guarantees that at each point in time we\nhave all the information we need (to the left, since all the columns to the left have\nalready been \ufb01lled, and below since we\u2019re \ufb01lling bottom to top). It also mirrors on-\nline processing, since \ufb01lling the columns from left to right corresponds to processing\neach word one at a time.\nThe outermost loop of the algorithm given in Fig. 18.12 iterates over the columns,\nand the second loop iterates over the rows, from the bottom up. The purpose of the\ninnermost loop is to range over all the places where a substring spanning itojin\nthe input might be split in two. As kranges over the places where the string can be\nsplit, the pairs of cells we consider move, in lockstep, to the right along row iand\ndown along column j. Figure 18.13 illustrates the general case of \ufb01lling cell [i;j].",
    "metadata": {
      "source": "18",
      "chunk_id": 12,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n14 CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\nBookthe flight throughHoustonS, VP, Verb, Nominal, NounS,VP,X2S,VP,X2DetNPNPNominal,NounNominalPrepPPNP,Proper-Noun[0,1][0,2][0,3][0,4][0,5][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]\nFigure 18.11 Completed parse table for Book the \ufb02ight through Houston.\nfunction CKY-P ARSE (words, grammar )returns table\nforj from 1toLENGTH (words )do\nfor all fAjA!words [j]2grammar g\ntable [j\u00001;j] table [j\u00001;j][A\nfori from j\u00002down to 0do\nfork i+1toj\u00001do\nfor all fAjA!BC2grammar andB2table[i;k]andC2table[k;j]g\ntable [i,j] table [i,j][A\nFigure 18.12 The CKY algorithm.\nAt each such split, the algorithm considers whether the contents of the two cells can\nbe combined in a way that is sanctioned by a rule in the grammar. If such a rule\nexists, the non-terminal on its left-hand side is entered into the table.\nFigure 18.14 shows how the \ufb01ve cells of column 5 of the table are \ufb01lled after the\nword Houston is read. The arrows point out the two spans that are being used to add\nan entry to the table. Note that the action in cell [0;5]indicates the presence of three\nalternative parses for this input, one where the PPmodi\ufb01es the \ufb02ight , one where\nit modi\ufb01es the booking, and one that captures the second argument in the original\nVP!Verb NP PP rule, now captured indirectly with the VP!X2 PP rule.\n18.6.3 CKY Parsing\nThe algorithm given in Fig. 18.12 is a recognizer, not a parser. That is, it can tell\nus whether a valid parse exists for a given sentence based on whether or not if \ufb01nds\nanSin cell [0;n], but it can\u2019t provide the derivation, which is the actual job for a\nparser. To turn it into a parser capable of returning all possible parses for a given\ninput, we can make two simple changes to the algorithm: the \ufb01rst change is to\naugment the entries in the table so that each non-terminal is paired with pointers to\nthe table entries from which it was derived (more or less as shown in Fig. 18.14), the\nsecond change is to permit multiple versions of the same non-terminal to be entered\ninto the table (again as shown in Fig. 18.14). With these changes, the completed\ntable contains all the possible parses for a given input. Returning an arbitrary single",
    "metadata": {
      "source": "18",
      "chunk_id": 13,
      "token_count": 673,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\n18.6 \u2022 CKY P ARSING : A D YNAMIC PROGRAMMING APPROACH 15\n...\n...[0,n]\n[i,i+1][i,i+2][i,j-2][i,j-1][i+1,j][i+2,j]\n[j-1,j][j-2,j][i,j]...[0,1]\n[n-1, n]\nFigure 18.13 All the ways to \ufb01ll the [ i,j]th cell in the CKY table.\nparse consists of choosing an Sfrom cell [0;n]and then recursively retrieving its\ncomponent constituents from the table. Of course, instead of returning every parse\nfor a sentence, we usually want just the best parse; we\u2019ll see how to do that in the\nnext section.\n18.6.4 CKY in Practice\nFinally, we should note that while the restriction to CNF does not pose a problem\ntheoretically, it does pose some non-trivial problems in practice. The returned CNF\ntrees may not be consistent with the original grammar built by the grammar devel-\nopers, and will complicate any syntax-driven approach to semantic analysis.\nOne approach to getting around these problems is to keep enough information\naround to transform our trees back to the original grammar as a post-processing step\nof the parse. This is trivial in the case of the transformation used for rules with length\ngreater than 2. Simply deleting the new dummy non-terminals and promoting their\ndaughters restores the original tree.\nIn the case of unit productions, it turns out to be more convenient to alter the ba-\nsic CKY algorithm to handle them directly than it is to store the information needed\nto recover the correct trees. Exercise 18.3 asks you to make this change. Many of\nthe probabilistic parsers presented in Appendix C use the CKY algorithm altered in",
    "metadata": {
      "source": "18",
      "chunk_id": 14,
      "token_count": 394,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\n16 CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\nBookthe flight throughHoustonS, VP, Verb, Nominal, NounS,VP,X2DetNPNominal,NounNominalPrepNP,Proper-Noun[0,1][0,2][0,3][0,4][0,5][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]Bookthe flight throughHoustonS, VP, Verb, Nominal, NounS,VP,X2DetNPNPNominal,NounPrepPPNP,Proper-Noun[0,1][0,2][0,3][0,4][0,5][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]\nBookthe flight throughHoustonS, VP, Verb, Nominal, NounS,VP,X2DetNPNPNominal,NounNominalPrepPPNP,Proper-Noun[0,1][0,2][0,3][0,4][0,5][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]Bookthe flight throughHoustonS, VP, Verb, Nominal, NounS,VP,X2DetNPNPNominal,NounNominalPrepPPNP,Proper-Noun[0,1][0,2][0,3][0,4][0,5][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]\nBookthe flight throughHoustonS, VP, Verb, Nominal, NounS,VP,X2DetNPNPNominal,NounNominalPrepPPNP,Proper-Noun[0,1][0,2][0,3][0,4][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]S2, VPS3S1,VP, X2\nFigure 18.14 Filling the cells of column 5 after reading the word Houston .",
    "metadata": {
      "source": "18",
      "chunk_id": 15,
      "token_count": 558,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\n18.7 \u2022 S PAN-BASED NEURAL CONSTITUENCY PARSING 17\njust this manner.\n18.7 Span-Based Neural Constituency Parsing\nWhile the CKY parsing algorithm we\u2019ve seen so far does great at enumerating all\nthe possible parse trees for a sentence, it has a large problem: it doesn\u2019t tell us which\nparse is the correct one! That is, it doesn\u2019t disambiguate among the possible parses.\nTo solve the disambiguation problem we\u2019ll use a simple neural extension of the\nCKY algorithm. The intuition of such parsing algorithms (often called span-based\nconstituency parsing , orneural CKY ), is to train a neural classi\ufb01er to assign a\nscore to each constituent, and then use a modi\ufb01ed version of CKY to combine these\nconstituent scores to \ufb01nd the best-scoring parse tree.\nHere we\u2019ll describe a version of the algorithm from Kitaev et al. (2019). This\nparser learns to map a span of words to a constituent, and, like CKY , hierarchically\ncombines larger and larger spans to build the parse-tree bottom-up. But unlike clas-\nsic CKY , this parser doesn\u2019t use the hand-written grammar to constrain what con-\nstituents can be combined, instead just relying on the learned neural representations\nof spans to encode likely combinations.\n18.7.1 Computing Scores for a Span\nLet\u2019s begin by considering just the constituent (we\u2019ll call it a span ) that lies between span\nfencepost positions iand jwith non-terminal symbol label l. We\u2019ll build a system\nto assign a score s(i;j;l)to this constituent span.\nENCODER[START]BooktheflightthroughHouston[END]map to subwordsmap back to words013245MLPi=1hj-hij=3NPCompute score for spanRepresent spanCKY for computing best parse\npostprocessing layers\nFigure 18.15 A simpli\ufb01ed outline of computing the span score for the span the \ufb02ight with\nthe label NP.\nFig. 18.15 sketches the architecture. The input word tokens are embedded by",
    "metadata": {
      "source": "18",
      "chunk_id": 16,
      "token_count": 467,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\n18 CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\npassing them through a pretrained language model like BERT. Because BERT oper-\nates on the level of subword (wordpiece) tokens rather than words, we\u2019ll \ufb01rst need to\nconvert the BERT outputs to word representations. One standard way of doing this\nis to simply use the \ufb01rst subword unit as the representation for the entire word; us-\ning the last subword unit, or the sum of all the subword units are also common. The\nembeddings can then be passed through some postprocessing layers; Kitaev et al.\n(2019), for example, use 8 Transformer layers.\nThe resulting word encoder outputs ytare then used to compute a span score.\nFirst, we must map the word encodings (indexed by word positions) to span encod-\nings (indexed by fenceposts). We do this by representing each fencepost with two\nseparate values; the intuition is that a span endpoint to the right of a word represents\ndifferent information than a span endpoint to the left of a word. We convert each\nword output ytinto a (leftward-pointing) value for spans ending at this fencepost, \u0000yt, and a (rightward-pointing) value\u0000 !ytfor spans beginning at this fencepost, by\nsplitting ytinto two halves. Each span then stretches from one double-vector fence-\npost to another, as in the following representation of the \ufb02ight , which is span (1;3):\nSTART 0 Book the \ufb02ight through\ny0\u0000 !y0 \u0000y1y1\u0000 !y1 \u0000y2 y2\u0000 !y2 \u0000y3y3\u0000 !y3 \u0000y4 y4\u0000 !y4 \u0000y5:::\n0\n 1\n 2\n 3\n 4\nspan(1,3)\nA traditional way to represent a span, developed originally for RNN-based models\n(Wang and Chang, 2016), but extended also to Transformers, is to take the differ-\nence between the embeddings of its start and end, i.e., representing span (i;j)by\nsubtracting the embedding of ifrom the embedding of j. Here we represent a span\nby concatenating the difference of each of its fencepost components:\nv(i;j) = [\u0000 !yj\u0000\u0000 !yi; \u0000\u0000yj+1\u0000 \u0000\u0000yi+1] (18.5)\nThe span vector vis then passed through an MLP span classi\ufb01er, with two fully-\nconnected layers and one ReLU activation function, whose output dimensionality is\nthe number of possible non-terminal labels:\ns(i;j;\u0001) =W2ReLU (LayerNorm (W1v(i;j))) (18.6)\nThe MLP then outputs a score for each possible non-terminal.\n18.7.2 Integrating Span Scores into a Parse\nNow we have a score for each labeled constituent span s(i;j;l). But we need a score\nfor an entire parse tree. Formally a tree Tis represented as a set of jTjsuch labeled\nspans, with the tthspan starting at position itand ending at position jt, with label lt:\nT=f(it;jt;lt):t=1;:::;jTjg (18.7)\nThus once we have a score for each span, the parser can compute a score for the\nwhole tree s(T)simply by summing over the scores of its constituent spans:\ns(T) =X\n(i;j;l)2Ts(i;j;l) (18.8)",
    "metadata": {
      "source": "18",
      "chunk_id": 17,
      "token_count": 783,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\n18.8 \u2022 E VALUATING PARSERS 19\nAnd we can choose the \ufb01nal parse tree as the tree with the maximum score:\n\u02c6T=argmax\nTs(T) (18.9)\nThe simplest method to produce the most likely parse is to greedily choose the\nhighest scoring label for each span. This greedy method is not guaranteed to produce\na tree, since the best label for a span might not \ufb01t into a complete tree. In practice,\nhowever, the greedy method tends to \ufb01nd trees; in their experiments Gaddy et al.\n(2018) \ufb01nds that 95% of predicted bracketings form valid trees.\nNonetheless it is more common to use a variant of the CKY algorithm to \ufb01nd the\nfull parse. The variant de\ufb01ned in Gaddy et al. (2018) works as follows. Let\u2019s de\ufb01ne\nsbest(i;j)as the score of the best subtree spanning (i;j). For spans of length one, we\nchoose the best label:\nsbest(i;i+1) =max\nls(i;i+1;l) (18.10)\nFor other spans (i;j), the recursion is:\nsbest(i;j) = max\nls(i;j;l)\n+max\nk[sbest(i;k)+sbest(k;j)] (18.11)\nNote that the parser is using the max label for span (i;j)+ the max labels for spans\n(i;k)and(k;j)without worrying about whether those decisions make sense given a\ngrammar. The role of the grammar in classical parsing is to help constrain possible\ncombinations of constituents (NPs like to be followed by VPs). By contrast, the\nneural model seems to learn these kinds of contextual constraints during its mapping\nfrom spans to non-terminals.\nFor more details on span-based parsing, including the margin-based training al-\ngorithm, see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and\nKitaev et al. (2019).\n18.8 Evaluating Parsers\nThe standard tool for evaluating parsers that assign a single parse tree to a sentence\nis the PARSEV AL metrics (Black et al., 1991). The PARSEV AL metric measures PARSEV AL\nhow much the constituents in the hypothesis parse tree look like the constituents in a\nhand-labeled, reference parse. PARSEV AL thus requires a human-labeled reference\n(or \u201cgold standard\u201d) parse tree for each sentence in the test set; we generally draw\nthese reference parses from a treebank like the Penn Treebank.\nA constituent in a hypothesis parse Chof a sentence sis labeled correct if there\nis a constituent in the reference parse Crwith the same starting point, ending point,\nand non-terminal symbol. We can then measure the precision and recall just as for\ntasks we\u2019ve seen already like named entity tagging:\nlabeled recall: =# of correct constituents in hypothesis parse of s\n# of total constituents in reference parse of s\nlabeled precision: =# of correct constituents in hypothesis parse of s\n# of total constituents in hypothesis parse of s",
    "metadata": {
      "source": "18",
      "chunk_id": 18,
      "token_count": 673,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\n20 CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\nS(dumped)\nVP(dumped)\nPP(into)\nNP(bin)\nNN(bin)\nbinDT(a)\naP\nintoNP(sacks)\nNNS(sacks)\nsacksVBD(dumped)\ndumpedNP(workers)\nNNS(workers)\nworkers\nFigure 18.16 A lexicalized tree from Collins (1999).\nAs usual, we often report a combination of the two, F 1:\nF1=2PR\nP+R(18.12)\nWe additionally use a new metric, crossing brackets, for each sentence s:\ncross-brackets: the number of constituents for which the reference parse has a\nbracketing such as ((A B) C) but the hypothesis parse has a bracketing such\nas (A (B C)).\nFor comparing parsers that use different grammars, the PARSEV AL metric in-\ncludes a canonicalization algorithm for removing information likely to be grammar-\nspeci\ufb01c (auxiliaries, pre-in\ufb01nitival \u201cto\u201d, etc.) and for computing a simpli\ufb01ed score\n(Black et al., 1991). The canonical implementation of the PARSEV AL metrics is\ncalled evalb (Sekine and Collins, 1997). evalb\n18.9 Heads and Head-Finding\nSyntactic constituents can be associated with a lexical head ;Nis the head of an NP,\nVis the head of a VP. This idea of a head for each constituent dates back to Bloom-\n\ufb01eld 1914, and is central to the dependency grammars and dependency parsing we\u2019ll\nintroduce in Chapter 19. Indeed, heads can be used as a way to map between con-\nstituency and dependency parses. Heads are also important in probabilistic pars-\ning (Appendix C) and in constituent-based grammar formalisms like Head-Driven\nPhrase Structure Grammar (Pollard and Sag, 1994)..\nIn one simple model of lexical heads, each context-free rule is associated with\na head (Charniak 1997, Collins 1999). The head is the word in the phrase that is\ngrammatically the most important. Heads are passed up the parse tree; thus, each\nnon-terminal in a parse tree is annotated with a single word, which is its lexical head.\nFigure 18.16 shows an example of such a tree from Collins (1999), in which each\nnon-terminal is annotated with its head.\nFor the generation of such a tree, each CFG rule must be augmented to identify\none right-side constituent to be the head child. The headword for a node is then set to\nthe headword of its head child. Choosing these head children is simple for textbook\nexamples ( NNis the head of NP) but is complicated and indeed controversial for",
    "metadata": {
      "source": "18",
      "chunk_id": 19,
      "token_count": 608,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21\n\n18.10 \u2022 S UMMARY 21\nmost phrases. (Should the complementizer toor the verb be the head of an in\ufb01nite\nverb phrase?) Modern linguistic theories of syntax generally include a component\nthat de\ufb01nes heads (see, e.g., (Pollard and Sag, 1994)).\nAn alternative approach to \ufb01nding a head is used in most practical computational\nsystems. Instead of specifying head rules in the grammar itself, heads are identi\ufb01ed\ndynamically in the context of trees for speci\ufb01c sentences. In other words, once\na sentence is parsed, the resulting tree is walked to decorate each node with the\nappropriate head. Most current systems rely on a simple set of handwritten rules,\nsuch as a practical one for Penn Treebank grammars given in Collins (1999) but\ndeveloped originally by Magerman (1995). For example, the rule for \ufb01nding the\nhead of an NPis as follows (Collins, 1999, p. 238):\n\u2022 If the last word is tagged POS, return last-word.\n\u2022 Else search from right to left for the \ufb01rst child which is an NN, NNP, NNPS, NX, POS,\nor JJR.\n\u2022 Else search from left to right for the \ufb01rst child which is an NP.\n\u2022 Else search from right to left for the \ufb01rst child which is a $, ADJP, or PRN.\n\u2022 Else search from right to left for the \ufb01rst child which is a CD.\n\u2022 Else search from right to left for the \ufb01rst child which is a JJ, JJS, RB or QP.\n\u2022 Else return the last word\nSelected other rules from this set are shown in Fig. 18.17. For example, for VP\nrules of the form VP!Y1\u0001\u0001\u0001Yn, the algorithm would start from the left of Y1\u0001\u0001\u0001\nYnlooking for the \ufb01rst Yiof type TO; if no TOs are found, it would search for the\n\ufb01rstYiof type VBD; if no VBDs are found, it would search for a VBN, and so on.\nSee Collins (1999) for more details.\nParent Direction Priority List\nADJP Left NNS QP NN $ ADVP JJ VBN VBG ADJP JJR NP JJS DT FW RBR RBS\nSBAR RB\nADVP Right RB RBR RBS FW ADVP TO CD JJR JJ IN NP JJS NN\nPRN Left\nPRT Right RP\nQP Left $ IN NNS NN JJ RB DT CD NCD QP JJR JJS\nS Left TO IN VP S SBAR ADJP UCP NP\nSBAR Left WHNP WHPP WHADVP WHADJP IN DT S SQ SINV SBAR FRAG\nVP Left TO VBD VBN MD VBZ VB VBG VBP VP ADJP NN NNS NP\nFigure 18.17 Some head rules from Collins (1999). The head rules are also called a head percolation table .\n18.10 Summary\nThis chapter introduced constituency parsing. Here\u2019s a summary of the main points:\n\u2022 In many languages, groups of consecutive words act as a group or a con-\nstituent , which can be modeled by context-free grammars (which are also\nknown as phrase-structure grammars ).\n\u2022 A context-free grammar consists of a set of rules orproductions , expressed\nover a set of non-terminal symbols and a set of terminal symbols. Formally,\na particular context-free language is the set of strings that can be derived\nfrom a particular context-free grammar .",
    "metadata": {
      "source": "18",
      "chunk_id": 20,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22\n\n22 CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\n\u2022Structural ambiguity is a signi\ufb01cant problem for parsers. Common sources\nof structural ambiguity include PP-attachment andcoordination ambiguity .\n\u2022Dynamic programming parsing algorithms, such as CKY , use a table of\npartial parses to ef\ufb01ciently parse ambiguous sentences.\n\u2022CKY restricts the form of the grammar to Chomsky normal form (CNF).\n\u2022 The basic CKY algorithm compactly represents all possible parses of the sen-\ntence but doesn\u2019t choose a single best parse.\n\u2022 Choosing a single parse from all possible parses ( disambiguation ) can be\ndone by neural constituency parsers .\n\u2022 Span-based neural constituency parses train a neural classi\ufb01er to assign a score\nto each constituent, and then use a modi\ufb01ed version of CKY to combine these\nconstituent scores to \ufb01nd the best-scoring parse tree.\n\u2022 Parsers are evaluated with three metrics: labeled recall ,labeled precision ,\nandcross-brackets .\n\u2022Partial parsing andchunking are methods for identifying shallow syntac-\ntic constituents in a text. They are solved by sequence models trained on\nsyntactically-annotated data.\nBibliographical and Historical Notes\nAccording to Percival (1976), the idea of breaking up a sentence into a hierarchy of\nconstituents appeared in the V\u00a8olkerpsychologie of the groundbreaking psychologist\nWilhelm Wundt (Wundt, 1900):\n...den sprachlichen Ausdruck f \u00a8ur die willk \u00a8urliche Gliederung einer Ge-\nsammtvorstellung in ihre in logische Beziehung zueinander gesetzten\nBestandteile\n[the linguistic expression for the arbitrary division of a total idea\ninto its constituent parts placed in logical relations to one another]\nWundt\u2019s idea of constituency was taken up into linguistics by Leonard Bloom-\n\ufb01eld in his early book An Introduction to the Study of Language (Bloom\ufb01eld, 1914).\nBy the time of his later book, Language (Bloom\ufb01eld, 1933), what was then called\n\u201cimmediate-constituent analysis\u201d was a well-established method of syntactic study\nin the United States. By contrast, traditional European grammar, dating from the\nClassical period, de\ufb01ned relations between words rather than constituents, and Eu-\nropean syntacticians retained this emphasis on such dependency grammars, the sub-\nject of Chapter 19. (And indeed, both dependency and constituency grammars have\nbeen in vogue in computational linguistics at different times).\nAmerican Structuralism saw a number of speci\ufb01c de\ufb01nitions of the immediate\nconstituent, couched in terms of their search for a \u201cdiscovery procedure\u201d: a method-\nological algorithm for describing the syntax of a language. In general, these attempt\nto capture the intuition that \u201cThe primary criterion of the immediate constituent\nis the degree in which combinations behave as simple units\u201d (Bazell, 1952/1966, p.\n284). The most well known of the speci\ufb01c de\ufb01nitions is Harris\u2019 idea of distributional\nsimilarity to individual units, with the substitutability test. Essentially, the method\nproceeded by breaking up a construction into constituents by attempting to substitute\nsimple structures for possible constituents\u2014if a substitution of a simple form, say,",
    "metadata": {
      "source": "18",
      "chunk_id": 21,
      "token_count": 742,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23",
    "metadata": {
      "source": "18",
      "chunk_id": 22,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 23\nman, was substitutable in a construction for a more complex set (like intense young\nman), then the form intense young man was probably a constituent. Harris\u2019s test was\nthe beginning of the intuition that a constituent is a kind of equivalence class.\nThe context-free grammar was a formalization of this idea of hierarchical\nconstituency de\ufb01ned in Chomsky (1956) and further expanded upon (and argued\nagainst) in Chomsky (1957) and Chomsky (1956/1975). Shortly after Chomsky\u2019s\ninitial work, the context-free grammar was reinvented by Backus (1959) and inde-\npendently by Naur et al. (1960) in their descriptions of the ALGOL programming\nlanguage; Backus (1996) noted that he was in\ufb02uenced by the productions of Emil\nPost and that Naur\u2019s work was independent of his (Backus\u2019) own. After this early\nwork, a great number of computational models of natural language processing were\nbased on context-free grammars because of the early development of ef\ufb01cient pars-\ning algorithms.\nDynamic programming parsing has a history of independent discovery. Ac-\ncording to the late Martin Kay (personal communication), a dynamic programming\nparser containing the roots of the CKY algorithm was \ufb01rst implemented by John\nCocke in 1960. Later work extended and formalized the algorithm, as well as prov-\ning its time complexity (Kay 1967, Younger 1967, Kasami 1965). The related well-\nformed substring table (WFST ) seems to have been independently proposed by WFST\nKuno (1965) as a data structure that stores the results of all previous computations\nin the course of the parse. Based on a generalization of Cocke\u2019s work, a similar\ndata structure had been independently described in Kay (1967) (and Kay 1973). The\ntop-down application of dynamic programming to parsing was described in Earley\u2019s\nPh.D. dissertation (Earley 1968, Earley 1970). Sheil (1976) showed the equivalence\nof the WFST and the Earley algorithm. Norvig (1991) shows that the ef\ufb01ciency of-\nfered by dynamic programming can be captured in any language with a memoization\nfunction (such as in LISP) simply by wrapping the memoization operation around a\nsimple top-down parser.\nThe earliest disambiguation algorithms for parsing were based on probabilistic\ncontext-free grammars , \ufb01rst worked out by Booth (1969) and Salomaa (1969); seeprobabilistic\ncontext-free\ngrammarsAppendix C for more history. Neural methods were \ufb01rst applied to parsing at around\nthe same time as statistical parsing methods were developed (Henderson, 1994). In\nthe earliest work neural networks were used to estimate some of the probabilities for\nstatistical constituency parsers (Henderson, 2003, 2004; Emami and Jelinek, 2005)\n. The next decades saw a wide variety of neural parsing algorithms, including re-\ncursive neural architectures (Socher et al., 2011, 2013), encoder-decoder models\n(Vinyals et al., 2015; Choe and Charniak, 2016), and the idea of focusing on spans\n(Cross and Huang, 2016). For more on the span-based self-attention approach we\ndescribe in this chapter see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein",
    "metadata": {
      "source": "18",
      "chunk_id": 23,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "fered by dynamic programming can be captured in any language with a memoization\nfunction (such as in LISP) simply by wrapping the memoization operation around a\nsimple top-down parser.\nThe earliest disambiguation algorithms for parsing were based on probabilistic\ncontext-free grammars , \ufb01rst worked out by Booth (1969) and Salomaa (1969); seeprobabilistic\ncontext-free\ngrammarsAppendix C for more history. Neural methods were \ufb01rst applied to parsing at around\nthe same time as statistical parsing methods were developed (Henderson, 1994). In\nthe earliest work neural networks were used to estimate some of the probabilities for\nstatistical constituency parsers (Henderson, 2003, 2004; Emami and Jelinek, 2005)\n. The next decades saw a wide variety of neural parsing algorithms, including re-\ncursive neural architectures (Socher et al., 2011, 2013), encoder-decoder models\n(Vinyals et al., 2015; Choe and Charniak, 2016), and the idea of focusing on spans\n(Cross and Huang, 2016). For more on the span-based self-attention approach we\ndescribe in this chapter see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein\n(2018), and Kitaev et al. (2019). See Chapter 20 for the parallel history of neural\ndependency parsing.\nThe classic reference for parsing algorithms is Aho and Ullman (1972); although\nthe focus of that book is on computer languages, most of the algorithms have been\napplied to natural language.",
    "metadata": {
      "source": "18",
      "chunk_id": 24,
      "token_count": 360,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24\n\n24 CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\nExercises\n18.1 Implement the algorithm to convert arbitrary context-free grammars to CNF.\nApply your program to the L1grammar.\n18.2 Implement the CKY algorithm and test it with your converted L1grammar.\n18.3 Rewrite the CKY algorithm given in Fig. 18.12 on page 14 so that it can accept\ngrammars that contain unit productions.\n18.4 Discuss how to augment a parser to deal with input that may be incorrect, for\nexample, containing spelling errors or mistakes arising from automatic speech\nrecognition.\n18.5 Implement the PARSEV AL metrics described in Section 18.8. Next, use a\nparser and a treebank, compare your metrics against a standard implementa-\ntion. Analyze the errors in your approach.",
    "metadata": {
      "source": "18",
      "chunk_id": 25,
      "token_count": 191,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 25",
    "metadata": {
      "source": "18",
      "chunk_id": 26,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Exercises 25\nAho, A. V . and J. D. Ullman. 1972. The Theory of Parsing,\nTranslation, and Compiling , volume 1. Prentice Hall.\nBackus, J. W. 1959. The syntax and semantics of the\nproposed international algebraic language of the Zurich\nACM-GAMM Conference. Information Processing: Pro-\nceedings of the International Conference on Information\nProcessing, Paris . UNESCO.\nBackus, J. W. 1996. Transcript of question and answer ses-\nsion. In R. L. Wexelblat, ed., History of Programming\nLanguages , page 162. Academic Press.\nBazell, C. E. 1952/1966. The correspondence fallacy in\nstructural linguistics. In E. P. Hamp, F. W. Householder,\nand R. Austerlitz, eds, Studies by Members of the En-\nglish Department, Istanbul University (3), reprinted in\nReadings in Linguistics II (1966) , 271\u2013298. University of\nChicago Press.\nBlack, E., S. P. Abney, D. Flickinger, C. Gdaniec, R. Gr-\nishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,\nJ. L. Klavans, M. Y . Liberman, M. P. Marcus, S. Roukos,\nB. Santorini, and T. Strzalkowski. 1991. A procedure for\nquantitatively comparing the syntactic coverage of En-\nglish grammars. Speech and Natural Language Work-\nshop .\nBloom\ufb01eld, L. 1914. An Introduction to the Study of Lan-\nguage . Henry Holt and Company.\nBloom\ufb01eld, L. 1933. Language . University of Chicago\nPress.\nBooth, T. L. 1969. Probabilistic representation of formal\nlanguages. IEEE Conference Record of the 1969 Tenth\nAnnual Symposium on Switching and Automata Theory .\nCharniak, E. 1997. Statistical parsing with a context-free\ngrammar and word statistics. AAAI .\nChoe, D. K. and E. Charniak. 2016. Parsing as language\nmodeling. EMNLP .\nChomsky, N. 1956. Three models for the description of\nlanguage. IRE Transactions on Information Theory ,\n2(3):113\u2013124.\nChomsky, N. 1956/1975. The Logical Structure of Linguistic\nTheory . Plenum.\nChomsky, N. 1957. Syntactic Structures . Mouton.\nChomsky, N. 1963. Formal properties of grammars. In R. D.\nLuce, R. Bush, and E. Galanter, eds, Handbook of Math-\nematical Psychology , volume 2, 323\u2013418. Wiley.\nCollins, M. 1999. Head-Driven Statistical Models for Natu-\nral Language Parsing . Ph.D. thesis, University of Penn-\nsylvania, Philadelphia.\nCross, J. and L. Huang. 2016. Span-based constituency pars-\ning with a structure-label system and provably optimal\ndynamic oracles. EMNLP .\nEarley, J. 1968. An Ef\ufb01cient Context-Free Parsing Algorithm .\nPh.D. thesis, Carnegie Mellon University, Pittsburgh, PA.\nEarley, J. 1970. An ef\ufb01cient context-free parsing algorithm.",
    "metadata": {
      "source": "18",
      "chunk_id": 27,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "modeling. EMNLP .\nChomsky, N. 1956. Three models for the description of\nlanguage. IRE Transactions on Information Theory ,\n2(3):113\u2013124.\nChomsky, N. 1956/1975. The Logical Structure of Linguistic\nTheory . Plenum.\nChomsky, N. 1957. Syntactic Structures . Mouton.\nChomsky, N. 1963. Formal properties of grammars. In R. D.\nLuce, R. Bush, and E. Galanter, eds, Handbook of Math-\nematical Psychology , volume 2, 323\u2013418. Wiley.\nCollins, M. 1999. Head-Driven Statistical Models for Natu-\nral Language Parsing . Ph.D. thesis, University of Penn-\nsylvania, Philadelphia.\nCross, J. and L. Huang. 2016. Span-based constituency pars-\ning with a structure-label system and provably optimal\ndynamic oracles. EMNLP .\nEarley, J. 1968. An Ef\ufb01cient Context-Free Parsing Algorithm .\nPh.D. thesis, Carnegie Mellon University, Pittsburgh, PA.\nEarley, J. 1970. An ef\ufb01cient context-free parsing algorithm.\nCACM , 6(8):451\u2013455.\nEmami, A. and F. Jelinek. 2005. A neural syntactic language\nmodel. Machine learning , 60(1):195\u2013227.\nGaddy, D., M. Stern, and D. Klein. 2018. What\u2019s going on\nin neural constituency parsers? an analysis. NAACL HLT .Harris, Z. S. 1946. From morpheme to utterance. Language ,\n22(3):161\u2013183.\nHenderson, J. 1994. Description Based Parsing in a Connec-\ntionist Network . Ph.D. thesis, University of Pennsylvania,\nPhiladelphia, PA.\nHenderson, J. 2003. Inducing history representations for\nbroad coverage statistical parsing. HLT-NAACL-03 .\nHenderson, J. 2004. Discriminative training of a neural net-\nwork statistical parser. ACL.\nHopcroft, J. E. and J. D. Ullman. 1979. Introduction to Au-\ntomata Theory, Languages, and Computation . Addison-\nWesley.\nKaplan, R. M. 1973. A general syntactic processor. In\nR. Rustin, ed., Natural Language Processing , 193\u2013241.\nAlgorithmics Press.\nKasami, T. 1965. An ef\ufb01cient recognition and syntax anal-\nysis algorithm for context-free languages. Technical\nReport AFCRL-65-758, Air Force Cambridge Research\nLaboratory, Bedford, MA.\nKay, M. 1967. Experiments with a powerful parser. COL-\nING.\nKay, M. 1973. The MIND system. In R. Rustin, ed., Natural\nLanguage Processing , 155\u2013188. Algorithmics Press.\nKay, M. 1982. Algorithm schemata and data structures in\nsyntactic processing. In S. All \u00b4en, ed., Text Processing:\nText Analysis and Generation, Text Typology and Attribu-\ntion, 327\u2013358. Almqvist and Wiksell, Stockholm.\nKitaev, N., S. Cao, and D. Klein. 2019. Multilingual\nconstituency parsing with self-attention and pre-training.\nACL.",
    "metadata": {
      "source": "18",
      "chunk_id": 28,
      "token_count": 749,
      "chapter_title": ""
    }
  },
  {
    "content": "Hopcroft, J. E. and J. D. Ullman. 1979. Introduction to Au-\ntomata Theory, Languages, and Computation . Addison-\nWesley.\nKaplan, R. M. 1973. A general syntactic processor. In\nR. Rustin, ed., Natural Language Processing , 193\u2013241.\nAlgorithmics Press.\nKasami, T. 1965. An ef\ufb01cient recognition and syntax anal-\nysis algorithm for context-free languages. Technical\nReport AFCRL-65-758, Air Force Cambridge Research\nLaboratory, Bedford, MA.\nKay, M. 1967. Experiments with a powerful parser. COL-\nING.\nKay, M. 1973. The MIND system. In R. Rustin, ed., Natural\nLanguage Processing , 155\u2013188. Algorithmics Press.\nKay, M. 1982. Algorithm schemata and data structures in\nsyntactic processing. In S. All \u00b4en, ed., Text Processing:\nText Analysis and Generation, Text Typology and Attribu-\ntion, 327\u2013358. Almqvist and Wiksell, Stockholm.\nKitaev, N., S. Cao, and D. Klein. 2019. Multilingual\nconstituency parsing with self-attention and pre-training.\nACL.\nKitaev, N. and D. Klein. 2018. Constituency parsing with a\nself-attentive encoder. ACL.\nKuno, S. 1965. The predictive analyzer and a path elimina-\ntion technique. CACM , 8(7):453\u2013462.\nMagerman, D. M. 1995. Statistical decision-tree models for\nparsing. ACL.\nNaur, P., J. W. Backus, F. L. Bauer, J. Green, C. Katz,\nJ. McCarthy, A. J. Perlis, H. Rutishauser, K. Samelson,\nB. Vauquois, J. H. Wegstein, A. van Wijnagaarden, and\nM. Woodger. 1960. Report on the algorithmic language\nALGOL 60. CACM , 3(5):299\u2013314. Revised in CACM\n6:1, 1-17, 1963.\nNorvig, P. 1991. Techniques for automatic memoization with\napplications to context-free parsing. Computational Lin-\nguistics , 17(1):91\u201398.\nPercival, W. K. 1976. On the historical source of immedi-\nate constituent analysis. In J. D. McCawley, ed., Syntax\nand Semantics Volume 7, Notes from the Linguistic Un-\nderground , 229\u2013242. Academic Press.\nPollard, C. and I. A. Sag. 1994. Head-Driven Phrase Struc-\nture Grammar . University of Chicago Press.\nSalomaa, A. 1969. Probabilistic and weighted grammars.\nInformation and Control , 15:529\u2013544.\nSekine, S. and M. Collins. 1997. The evalb software. http:\n//cs.nyu.edu/cs/projects/proteus/evalb .\nSheil, B. A. 1976. Observations on context free parsing.\nSMIL: Statistical Methods in Linguistics , 1:71\u2013109.\nSocher, R., J. Bauer, C. D. Manning, and A. Y . Ng. 2013.\nParsing with compositional vector grammars. ACL.",
    "metadata": {
      "source": "18",
      "chunk_id": 29,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 26\n\n26 Chapter 18 \u2022 Context-Free Grammars and Constituency Parsing\nSocher, R., C. C.-Y . Lin, A. Y . Ng, and C. D. Manning. 2011.\nParsing natural scenes and natural language with recur-\nsive neural networks. ICML .\nStern, M., J. Andreas, and D. Klein. 2017. A minimal span-\nbased neural constituency parser. ACL.\nVinyals, O., \u0141. Kaiser, T. Koo, S. Petrov, I. Sutskever,\nand G. Hinton. 2015. Grammar as a foreign language.\nNeurIPS .\nWang, W. and B. Chang. 2016. Graph-based dependency\nparsing with bidirectional LSTM. ACL.\nWundt, W. 1900. V\u00a8olkerpsychologie: eine Untersuchung der\nEntwicklungsgesetze von Sprache, Mythus, und Sitte . W.\nEngelmann, Leipzig. Band II: Die Sprache, Zweiter Teil.\nYounger, D. H. 1967. Recognition and parsing of context-\nfree languages in time n3.Information and Control ,\n10:189\u2013208.",
    "metadata": {
      "source": "18",
      "chunk_id": 30,
      "token_count": 263,
      "chapter_title": ""
    }
  }
]