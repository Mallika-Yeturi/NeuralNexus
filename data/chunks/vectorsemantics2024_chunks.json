[
  {
    "content": "# vectorsemantics2024\n\n## Page 1\n\nVector Semantics & EmbeddingsWord Meaning\n\n## Page 2\n\nWhat do words mean?N-gram or text classification methods we've seen so far\u25e6Words are just strings (or indices wiin a vocabulary list)\u25e6That's not very satisfactory!Introductory logic classes:\u25e6The meaning of \"dog\" is DOG;  cat is CAT\u2200x DOG(x) \u27f6MAMMAL(x)Old linguistics joke by Barbara Partee in 1967:\u25e6Q: What's the meaning of life?\u25e6A: LIFEThat seems hardly better!\n\n## Page 3\n\nDesiderataWhat should a theory of word meaning do for us?Let's look at some desiderataFrom lexical semantics, the linguistic study of word meaning\n\n## Page 4\n\nmouse (N)1. any of numerous small rodents...2. a hand-operated device that controls a cursor... Lemmas and sensessenselemma\nA sense or \u201cconcept\u201d is the meaning component of a wordLemmas can be polysemous (have multiple senses)Modified from the online thesaurus WordNet\n\n## Page 5\n\nRelations between senses: SynonymySynonyms have the same meaning in some or all contexts.\u25e6filbert / hazelnut\u25e6couch / sofa\u25e6big / large\u25e6automobile / car\u25e6vomit / throw up\u25e6water / H20\n\n## Page 6\n\nRelations between senses: SynonymyNote that there are probably no examples of perfect synonymy.\u25e6Even if many aspects of meaning are identical\u25e6Still may differ based on politeness, slang, register, genre, etc.\n\n## Page 7\n\nRelation: Synonymy?water/H20\"H20\" in a surfing guide?big/largemy big sister != my large sister\n\n## Page 8\n\nThe Linguistic Principle of ContrastDifference in form \u00e0difference in meaning\n\n## Page 9\n\nAbb\u00e9Gabriel Girard 1718\n [I do not believe that there is a synonymous word in any language]\n\"\"Re: \"exact\" synonyms\nThanks to Mark Aronoff!\n\n## Page 10\n\nRelation: SimilarityWords with similar meanings.  Not synonyms, but sharing some element of meaningcar, bicyclecow, horse\n\n## Page 11\n\nAsk humans how similar 2 words areword1word2similarityvanishdisappear9.8 behaveobey7.3 beliefimpression 5.95 musclebone 3.65 modestflexible0.98 holeagreement0.3 SimLex-999 dataset (Hill et al., 2015) \n\n## Page 12\n\nRelation: Word relatednessAlso called \"word association\"Words can be related in any way, perhaps via a semantic frame or field\u25e6coffee, tea:    similar\u25e6coffee, cup:   related, not similar\n\n## Page 13\n\nSemantic fieldWords that \u25e6cover a particular semantic domain \u25e6bear structured relations with each other. hospitalssurgeon, scalpel, nurse, anaesthetic, hospitalrestaurantswaiter, menu, plate, food, menu,chefhousesdoor, roof, kitchen, family, bed\n\n## Page 14\n\nRelation: AntonymySenses that are opposites with respect to only one feature of meaningOtherwise, they are very similar!dark/light   short/longfast/slowrise/fallhot/coldup/downin/outMore formally: antonyms can\u25e6define a binary opposition or be at opposite ends of a scale\u25e6long/short, fast/slow\u25e6Be reversives:\u25e6rise/fall, up/down",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 0,
      "token_count": 768,
      "chapter_title": "vectorsemantics2024"
    }
  },
  {
    "content": "## Page 11\n\nAsk humans how similar 2 words areword1word2similarityvanishdisappear9.8 behaveobey7.3 beliefimpression 5.95 musclebone 3.65 modestflexible0.98 holeagreement0.3 SimLex-999 dataset (Hill et al., 2015) \n\n## Page 12\n\nRelation: Word relatednessAlso called \"word association\"Words can be related in any way, perhaps via a semantic frame or field\u25e6coffee, tea:    similar\u25e6coffee, cup:   related, not similar\n\n## Page 13\n\nSemantic fieldWords that \u25e6cover a particular semantic domain \u25e6bear structured relations with each other. hospitalssurgeon, scalpel, nurse, anaesthetic, hospitalrestaurantswaiter, menu, plate, food, menu,chefhousesdoor, roof, kitchen, family, bed\n\n## Page 14\n\nRelation: AntonymySenses that are opposites with respect to only one feature of meaningOtherwise, they are very similar!dark/light   short/longfast/slowrise/fallhot/coldup/downin/outMore formally: antonyms can\u25e6define a binary opposition or be at opposite ends of a scale\u25e6long/short, fast/slow\u25e6Be reversives:\u25e6rise/fall, up/down\n\n## Page 15\n\nConnotation (sentiment)\u2022Words have affectivemeanings\u2022Positive connotations (happy) \u2022Negative connotations (sad)\u2022Connotations can be subtle:\u2022Positive connotation: copy, replica, reproduction \u2022Negative connotation: fake, knockoff, forgery\u2022Evaluation (sentiment!)\u2022Positive evaluation (great, love) \u2022Negative evaluation (terrible, hate)\n\n## Page 16\n\nConnotationWords seem to vary along 3 affective dimensions:\u25e6valence: the pleasantness of the stimulus\u25e6arousal: the intensity of emotion provoked by the stimulus\u25e6dominance: the degree of control exerted by the stimulusOsgood et al. (1957)\nWordScoreWordScoreValencelove1.000toxic0.008happy1.000nightmare0.005Arousalelated0.960mellow0.069frenzy0.965napping0.046Dominancepowerful0.991weak0.045leadership0.983empty0.081Values from NRC VAD Lexicon  (Mohammad 2018)\n\n## Page 17\n\nSo farConceptsor word senses\u25e6Have a complex many-to-many association with words(homonymy, multiple senses)Have relations with each other\u25e6Synonymy\u25e6Antonymy\u25e6Similarity\u25e6Relatedness\u25e6Connotation\n\n## Page 18\n\nVector Semantics & EmbeddingsWord Meaning\n\n## Page 19\n\nVector Semantics & EmbeddingsVector Semantics\n\n## Page 20\n\nComputational models of word meaningCan we build a theory of how to represent word meaning, that accounts for at least some of the desiderata?We'll introduce vector semanticsThe standard model in language processing!Handles many of our goals!\n\n## Page 21\n\nLudwig WittgensteinPI #43: \"The meaning of a word is its use in the language\"\n\n## Page 22\n\nLet's define words by their usagesOne way to define \"usage\": words are defined by their environments (the words around them)ZelligHarris (1954): If A and B have almost identical environments we say that they are synonyms.",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 1,
      "token_count": 739,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\nSo farConceptsor word senses\u25e6Have a complex many-to-many association with words(homonymy, multiple senses)Have relations with each other\u25e6Synonymy\u25e6Antonymy\u25e6Similarity\u25e6Relatedness\u25e6Connotation\n\n## Page 18\n\nVector Semantics & EmbeddingsWord Meaning\n\n## Page 19\n\nVector Semantics & EmbeddingsVector Semantics\n\n## Page 20\n\nComputational models of word meaningCan we build a theory of how to represent word meaning, that accounts for at least some of the desiderata?We'll introduce vector semanticsThe standard model in language processing!Handles many of our goals!\n\n## Page 21\n\nLudwig WittgensteinPI #43: \"The meaning of a word is its use in the language\"\n\n## Page 22\n\nLet's define words by their usagesOne way to define \"usage\": words are defined by their environments (the words around them)ZelligHarris (1954): If A and B have almost identical environments we say that they are synonyms.\n\n## Page 23\n\nWhat does recent English borrowing ongchoimean?Suppose you see these sentences:\u2022Ong choiis delicious saut\u00e9edwithgarlic. \u2022Ong choiis superb over rice\u2022Ong choileaveswith salty saucesAnd you've also seen these:\u2022\u2026spinach saut\u00e9edwithgarlicover rice\u2022Chard stems and leavesare delicious\u2022Collard greens and other saltyleafy greensConclusion:\u25e6Ongchoiis a leafy green like spinach, chard, or collard greens\u25e6We could conclude this based on words like \"leaves\" and \"delicious\" and \"sauteed\" \n\n## Page 24\n\nOngchoi: Ipomoea aquatica \"Water Spinach\"\nYamaguchi, Wikimedia Commons, public domain!\"#kangkongrau mu\u1ed1ng\u2026\n\n## Page 25\n\nIdea 1: Defining meaning by linguistic distributionLet's define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. \n\n## Page 26\n\nIdea 2: Meaning as a point in space (Osgood et al. 1957)3 affective dimensions for a word\u25e6valence: pleasantness \u25e6arousal: intensity of emotion \u25e6dominance: the degree of control exerted\u25e6Hence the connotation of a word is a vector in 3-spaceWordScoreWordScoreValencelove1.000toxic0.008happy1.000nightmare0.005Arousalelated0.960mellow0.069frenzy0.965napping0.046Dominancepowerful0.991weak0.045leadership0.983empty0.081NRC VAD Lexicon  (Mohammad 2018)\n\n## Page 27\n\nIdea 1: Defining meaning by linguistic distributionIdea 2: Meaning as a point in multidimensional space",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 2,
      "token_count": 640,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 27\n\nIdea 1: Defining meaning by linguistic distributionIdea 2: Meaning as a point in multidimensional space\n\n## Page 28\n\n6CHAPTER6\u2022VECTORSEMANTICS ANDEMBEDDINGS\ngoodnicebadworstnot good\nwonderfulamazingterri\ufb01cdislikeworsevery goodincredibly goodfantasticincredibly badnowyouithatwithbyto\u2019sareisathanFigure 6.1A two-dimensional (t-SNE) projection of embeddings for some words andphrases, showing that words with similar meanings are nearby in space. The original 60-dimensional embeddings were trained for sentiment analysis. Simpli\ufb01ed fromLi et al. (2015)with colors added for explanation.The \ufb01ne-grained model of word similarity of vector semantics offers enormouspower to NLP applications. NLP applications like the sentiment classi\ufb01ers of Chap-ter 4 or Chapter 5 depend on the same words appearing in the training and test sets.But by representing words as embeddings, classi\ufb01ers can assign sentiment as long asit sees some words withsimilar meanings. And as we\u2019ll see, vector semantic modelscan be learned automatically from text without supervision.In this chapter we\u2019ll introduce the two most commonly used models. In thetf-idfmodel, an important baseline, the meaning of a word is de\ufb01ned by a simple functionof the counts of nearby words. We will see that this method results in very longvectors that aresparse, i.e. mostly zeros (since most words simply never occur inthe context of others). We\u2019ll introduce theword2vecmodel family for construct-ing short,densevectors that have useful semantic properties. We\u2019ll also introducethecosine, the standard way to use embeddings to computesemantic similarity, be-tween two words, two sentences, or two documents, an important tool in practicalapplications like question answering, summarization, or automatic essay grading.6.3 Words and Vectors\u201cThe most important attributes of a vector in 3-space are{Location, Location, Location}\u201dRandall Munroe,https://xkcd.com/2358/Vector or distributional models of meaning are generally based on aco-occurrencematrix, a way of representing how often words co-occur. We\u2019ll look at two popularmatrices: the term-document matrix and the term-term matrix.6.3.1 Vectors and documentsIn aterm-document matrix, each row represents a word in the vocabulary and eachterm-documentmatrixcolumn represents a document from some collection of documents. Fig.6.2shows asmall selection from a term-document matrix showing the occurrence of four wordsin four plays by Shakespeare. Each cell in this matrix represents the number of timesa particular word (de\ufb01ned by the row) occurs in a particular document (de\ufb01ned bythe column). Thusfoolappeared 58 times inTwelfth Night.The term-document matrix of Fig.6.2was \ufb01rst de\ufb01ned as part of thevectorspace modelof information retrieval(Salton, 1971). In this model, a document isvector spacemodelDefining meaning as a point in space based on distributionEach word = a vector   (not just \"good\" or \"w45\")Similar words are \"nearby in semantic space\"We build this space automatically by seeing which words are nearby in text\n\n## Page 29\n\nWe define meaning of a word as a vectorCalled an \"embedding\" because it's embedded into a space (see textbook)The standard way to represent meaning in NLPEvery modern NLP algorithm uses embeddings as the representation of word meaningFine-grained model of meaning for similarity",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 3,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 29\n\nWe define meaning of a word as a vectorCalled an \"embedding\" because it's embedded into a space (see textbook)The standard way to represent meaning in NLPEvery modern NLP algorithm uses embeddings as the representation of word meaningFine-grained model of meaning for similarity \n\n## Page 30\n\nIntuition: why vectors?Consider sentiment analysis:\u25e6With words,  a feature is a word identity\u25e6Feature 5: 'The previous word was \"terrible\"'\u25e6requires exactsamewordto be in training and test\u25e6With embeddings: \u25e6Feature is a word vector\u25e6'The previous word was vector [35,22,17\u2026]\u25e6Now in the test set we might see a similar vector [34,21,14]\u25e6We can generalize to similar but unseenwords!!! \n\n## Page 31\n\nWe'll discuss 2 kinds of embeddingstf-idf\u25e6Information Retrieval workhorse!\u25e6A common baseline model\u25e6Sparsevectors\u25e6Words are represented by (a simple function of) the counts of nearby wordsWord2vec\u25e6Densevectors\u25e6Representation is created by training a classifier to predictwhether a word is likely to appear nearby\u25e6Later we'll discuss extensions called  contextual embeddings",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 4,
      "token_count": 268,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 31\n\nWe'll discuss 2 kinds of embeddingstf-idf\u25e6Information Retrieval workhorse!\u25e6A common baseline model\u25e6Sparsevectors\u25e6Words are represented by (a simple function of) the counts of nearby wordsWord2vec\u25e6Densevectors\u25e6Representation is created by training a classifier to predictwhether a word is likely to appear nearby\u25e6Later we'll discuss extensions called  contextual embeddings\n\n## Page 32\n\nFrom now on:Computing with meaning representationsinstead of string representationsSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright\u00a92020. Allrights reserved. Draft of January 13, 2021.CHAPTER6Vector Semantics andEmbeddingsC\u21e7@\u00c2(|\u0000\u00f3|\u0000\u00ffCNets are for \ufb01sh;Once you get the \ufb01sh, you can forget the net.\u0000\u21e7@\u00c2(\u270f\u0000\u00f3\u270f\u0000\u00ff\u0000Words are for meaning;Once you get the meaning, you can forget the words\u00d1P(Zhuangzi), Chapter 26The asphalt that Los Angeles is famous for occurs mainly on its freeways. Butin the middle of the city is another patch of asphalt, the La Brea tar pits, and thisasphalt preserves millions of fossil bones from the last of the Ice Ages of the Pleis-tocene Epoch. One of these fossils is theSmilodon, or saber-toothed tiger, instantlyrecognizable by its long canines. Five million years ago or so, a completely different\nsabre-tooth tiger calledThylacosmiluslivedin Argentina and other parts of South Amer-ica. Thylacosmilus was a marsupial whereasSmilodon was a placental mammal, but Thy-lacosmilus had the same long upper caninesand, like Smilodon, had a protective bone\ufb02ange on the lower jaw. The similarity ofthese two mammals is one of many examplesof parallel or convergent evolution, in which particular contexts or environmentslead to the evolution of very similar structures in different species(Gould, 1980).The role of context is also important in the similarity of a less biological kindof organism: the word. Words that occur insimilar contextstend to havesimilarmeanings. This link between similarity in how words are distributed and similarityin what they mean is called thedistributional hypothesis. The hypothesis wasdistributionalhypothesis\ufb01rst formulated in the 1950s by linguists likeJoos (1950),Harris (1954), andFirth(1957), who noticed that words which are synonyms (likeoculistandeye-doctor)tended to occur in the same environment (e.g., near words likeeyeorexamined)with the amount of meaning difference between two words \u201ccorresponding roughlyto the amount of difference in their environments\u201d(Harris, 1954, 157).In this chapter we introducevector semantics, which instantiates this linguisticvectorsemanticshypothesis by learning representations of the meaning of words, calledembeddings,embeddingsdirectly from their distributions in texts. These representations are used in every nat-ural language processing application that makes use of meaning, and thestatic em-beddingswe introduce here underlie the more powerful dynamic orcontextualizedembeddingslikeBERTthat we will see in Chapter 10.These word representations are also the \ufb01rst example in this book ofrepre-sentation learning, automatically learning useful representations of the input text.representationlearningFinding suchself-supervisedways to learn representations of the input, instead ofcreating representations by hand viafeature engineering, is an important focus ofNLP research(Bengio et al., 2013).",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 5,
      "token_count": 783,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 33\n\nVector Semantics & EmbeddingsVector Semantics\n\n## Page 34\n\nVector Semantics & EmbeddingsWords and Vectors",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 6,
      "token_count": 30,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 35",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 7,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Term-document matrix6.3\u2022WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like Itis represented as the list [1,114,36,20] (the \ufb01rstcolumnvectorin Fig.6.3) andJulius Caesaris represented as the list [7,62,1,2] (the thirdcolumn vector). Avector spaceis a collection of vectors, characterized by theirvector spacedimension. In the example in Fig.6.3, the document vectors are of dimension 4,dimensionjust so they \ufb01t on the page; in real term-document matrices, the vectors representingeach document would have dimensionality|V|, the vocabulary size.The ordering of the numbers in a vector space indicates different meaningful di-mensions on which documents vary. Thus the \ufb01rst dimension for both these vectorscorresponds to the number of times the wordbattleoccurs, and we can compareeach dimension, noting for example that the vectors forAs You Like ItandTwelfthNighthave similar values (1 and 0, respectively) for the \ufb01rst dimension.As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.3The term-document matrix for four words in four Shakespeare plays. The redboxes show that each document is represented as a column vector of length four.We can think of the vector for a document as a point in|V|-dimensional space;thus the documents in Fig.6.3are points in 4-dimensional space. Since 4-dimensionalspaces are hard to visualize, Fig.6.4shows a visualization in two dimensions; we\u2019vearbitrarily chosen the dimensions corresponding to the wordsbattleandfool.\n51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 8,
      "token_count": 538,
      "chapter_title": ""
    }
  },
  {
    "content": "51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540\n354045505560Figure 6.4A spatial visualization of the document vectors for the four Shakespeare playdocuments, showing just two of the dimensions, corresponding to the wordsbattleandfool.The comedies have high values for thefooldimension and low values for thebattledimension.Term-document matrices were originally de\ufb01ned as a means of \ufb01nding similardocuments for the task of documentinformation retrieval. Two documents that are6.3\u2022WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like Itis represented as the list [1,114,36,20] (the \ufb01rstcolumnvectorin Fig.6.3) andJulius Caesaris represented as the list [7,62,1,2] (the thirdcolumn vector). Avector spaceis a collection of vectors, characterized by theirvector spacedimension. In the example in Fig.6.3, the document vectors are of dimension 4,dimensionjust so they \ufb01t on the page; in real term-document matrices, the vectors representingeach document would have dimensionality|V|, the vocabulary size.The ordering of the numbers in a vector space indicates different meaningful di-mensions on which documents vary. Thus the \ufb01rst dimension for both these vectorscorresponds to the number of times the wordbattleoccurs, and we can compareeach dimension, noting for example that the vectors forAs You Like ItandTwelfthNighthave similar values (1 and 0, respectively) for the \ufb01rst dimension.As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.3The term-document matrix for four words in four Shakespeare plays. The redboxes show that each document is represented as a column vector of length four.We can think of the vector for a document as a point in|V|-dimensional space;thus the documents in Fig.6.3are points in 4-dimensional space. Since 4-dimensionalspaces are hard to visualize, Fig.6.4shows a visualization in two dimensions; we\u2019vearbitrarily chosen the dimensions corresponding to the wordsbattleandfool.\n51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540\n354045505560Figure 6.4A spatial visualization of the document vectors for the four Shakespeare playdocuments, showing just two of the dimensions, corresponding to the wordsbattleandfool.The comedies have high values for thefooldimension and low values for thebattledimension.Term-document matrices were originally de\ufb01ned as a means of \ufb01nding similardocuments for the task of documentinformation retrieval. Two documents that areEach document is represented by a vector of words",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 9,
      "token_count": 769,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 36\n\nVisualizing document vectors6.3\u2022WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like Itis represented as the list [1,114,36,20] (the \ufb01rstcolumnvectorin Fig.6.3) andJulius Caesaris represented as the list [7,62,1,2] (the thirdcolumn vector). Avector spaceis a collection of vectors, characterized by theirvector spacedimension. In the example in Fig.6.3, the document vectors are of dimension 4,dimensionjust so they \ufb01t on the page; in real term-document matrices, the vectors representingeach document would have dimensionality|V|, the vocabulary size.The ordering of the numbers in a vector space indicates different meaningful di-mensions on which documents vary. Thus the \ufb01rst dimension for both these vectorscorresponds to the number of times the wordbattleoccurs, and we can compareeach dimension, noting for example that the vectors forAs You Like ItandTwelfthNighthave similar values (1 and 0, respectively) for the \ufb01rst dimension.As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.3The term-document matrix for four words in four Shakespeare plays. The redboxes show that each document is represented as a column vector of length four.We can think of the vector for a document as a point in|V|-dimensional space;thus the documents in Fig.6.3are points in 4-dimensional space. Since 4-dimensionalspaces are hard to visualize, Fig.6.4shows a visualization in two dimensions; we\u2019vearbitrarily chosen the dimensions corresponding to the wordsbattleandfool.\n51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540\n354045505560Figure 6.4A spatial visualization of the document vectors for the four Shakespeare playdocuments, showing just two of the dimensions, corresponding to the wordsbattleandfool.The comedies have high values for thefooldimension and low values for thebattledimension.Term-document matrices were originally de\ufb01ned as a means of \ufb01nding similardocuments for the task of documentinformation retrieval. Two documents that are",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 10,
      "token_count": 636,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 37\n\nVectors are the basis of information retrieval6.3\u2022WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like Itis represented as the list [1,114,36,20] (the \ufb01rstcolumnvectorin Fig.6.3) andJulius Caesaris represented as the list [7,62,1,2] (the thirdcolumn vector). Avector spaceis a collection of vectors, characterized by theirvector spacedimension. In the example in Fig.6.3, the document vectors are of dimension 4,dimensionjust so they \ufb01t on the page; in real term-document matrices, the vectors representingeach document would have dimensionality|V|, the vocabulary size.The ordering of the numbers in a vector space indicates different meaningful di-mensions on which documents vary. Thus the \ufb01rst dimension for both these vectorscorresponds to the number of times the wordbattleoccurs, and we can compareeach dimension, noting for example that the vectors forAs You Like ItandTwelfthNighthave similar values (1 and 0, respectively) for the \ufb01rst dimension.As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.3The term-document matrix for four words in four Shakespeare plays. The redboxes show that each document is represented as a column vector of length four.We can think of the vector for a document as a point in|V|-dimensional space;thus the documents in Fig.6.3are points in 4-dimensional space. Since 4-dimensionalspaces are hard to visualize, Fig.6.4shows a visualization in two dimensions; we\u2019vearbitrarily chosen the dimensions corresponding to the wordsbattleandfool.\n51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540\n354045505560Figure 6.4A spatial visualization of the document vectors for the four Shakespeare playdocuments, showing just two of the dimensions, corresponding to the wordsbattleandfool.The comedies have high values for thefooldimension and low values for thebattledimension.Term-document matrices were originally de\ufb01ned as a means of \ufb01nding similardocuments for the task of documentinformation retrieval. Two documents that areVectors are similar for the two comediesBut comedies are different than the other two  Comedies have more fools and wit and fewer battles.",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 11,
      "token_count": 669,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 38",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 12,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Idea for word meaning: Words can be vectors too!!!6.3\u2022WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like Itis represented as the list [1,114,36,20] (the \ufb01rstcolumnvectorin Fig.6.3) andJulius Caesaris represented as the list [7,62,1,2] (the thirdcolumn vector). Avector spaceis a collection of vectors, characterized by theirvector spacedimension. In the example in Fig.6.3, the document vectors are of dimension 4,dimensionjust so they \ufb01t on the page; in real term-document matrices, the vectors representingeach document would have dimensionality|V|, the vocabulary size.The ordering of the numbers in a vector space indicates different meaningful di-mensions on which documents vary. Thus the \ufb01rst dimension for both these vectorscorresponds to the number of times the wordbattleoccurs, and we can compareeach dimension, noting for example that the vectors forAs You Like ItandTwelfthNighthave similar values (1 and 0, respectively) for the \ufb01rst dimension.As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.3The term-document matrix for four words in four Shakespeare plays. The redboxes show that each document is represented as a column vector of length four.We can think of the vector for a document as a point in|V|-dimensional space;thus the documents in Fig.6.3are points in 4-dimensional space. Since 4-dimensionalspaces are hard to visualize, Fig.6.4shows a visualization in two dimensions; we\u2019vearbitrarily chosen the dimensions corresponding to the wordsbattleandfool.\n51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 13,
      "token_count": 547,
      "chapter_title": ""
    }
  },
  {
    "content": "354045505560Figure 6.4A spatial visualization of the document vectors for the four Shakespeare playdocuments, showing just two of the dimensions, corresponding to the wordsbattleandfool.The comedies have high values for thefooldimension and low values for thebattledimension.Term-document matrices were originally de\ufb01ned as a means of \ufb01nding similardocuments for the task of documentinformation retrieval. Two documents that arebattle is \"the kind of word that occurs in Julius Caesar and Henry V\"fool is \"the kind of word that occurs  in comedies, especially Twelfth Night\"8CHAPTER6\u2022VECTORSEMANTICS ANDEMBEDDINGSsimilar will tend to have similar words, and if two documents have similar wordstheir column vectors will tend to be similar. The vectors for the comediesAs YouLike It[1,114,36,20] andTwelfth Night[0,80,58,15] look a lot more like each other(more fools and wit than battles) than they look likeJulius Caesar[7,62,1,2] orHenry V[13,89,4,3]. This is clear with the raw numbers; in the \ufb01rst dimension(battle) the comedies have low numbers and the others have high numbers, and wecan see it visually in Fig.6.4; we\u2019ll see very shortly how to quantify this intuitionmore formally.A real term-document matrix, of course, wouldn\u2019t just have 4 rows and columns,let alone 2. More generally, the term-document matrix has|V|rows (one for eachword type in the vocabulary) andDcolumns (one for each document in the collec-tion); as we\u2019ll see, vocabulary sizes are generally in the tens of thousands, and thenumber of documents can be enormous (think about all the pages on the web).Information retrieval(IR) is the task of \ufb01nding the documentdfrom theDinformationretrievaldocuments in some collection that best matches a queryq. For IR we\u2019ll therefore alsorepresent a query by a vector, also of length|V|, and we\u2019ll need a way to comparetwo vectors to \ufb01nd how similar they are. (Doing IR will also require ef\ufb01cient waysto store and manipulate these vectors by making use of the convenient fact that thesevectors are sparse, i.e., mostly zeros).Later in the chapter we\u2019ll introduce some of the components of this vector com-parison process: the tf-idf term weighting, and the cosine similarity metric.6.3.2 Words as vectors: document dimensionsWe\u2019ve seen that documents can be represented as vectors in a vector space. Butvector semantics can also be used to represent the meaning ofwords. We do thisby associating each word with a word vector\u2014 arow vectorrather than a columnrow vectorvector, hence with different dimensions, as shown in Fig.6.5. The four dimensionsof the vector forfool, [36,58,1,4], correspond to the four Shakespeare plays. Wordcounts in the same four dimensions are used to form the vectors for the other 3words:wit, [20,15,2,3];battle, [1,0,7,13]; andgood[114,80,62,89].As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.5The term-document matrix for four words in four Shakespeare plays. The redboxes show that each word is represented as a row vector of length four.For documents, we saw that similar documents had similar vectors, because sim-ilar documents tend to have similar words. This same principle applies to words:similar words",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 14,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "in the chapter we\u2019ll introduce some of the components of this vector com-parison process: the tf-idf term weighting, and the cosine similarity metric.6.3.2 Words as vectors: document dimensionsWe\u2019ve seen that documents can be represented as vectors in a vector space. Butvector semantics can also be used to represent the meaning ofwords. We do thisby associating each word with a word vector\u2014 arow vectorrather than a columnrow vectorvector, hence with different dimensions, as shown in Fig.6.5. The four dimensionsof the vector forfool, [36,58,1,4], correspond to the four Shakespeare plays. Wordcounts in the same four dimensions are used to form the vectors for the other 3words:wit, [20,15,2,3];battle, [1,0,7,13]; andgood[114,80,62,89].As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.5The term-document matrix for four words in four Shakespeare plays. The redboxes show that each word is represented as a row vector of length four.For documents, we saw that similar documents had similar vectors, because sim-ilar documents tend to have similar words. This same principle applies to words:similar words have similar vectors because they tend to occur in similar documents.The term-document matrix thus lets us represent the meaning of a word by the doc-uments it tends to occur in.6.3.3 Words as vectors: word dimensionsAn alternative to using the term-document matrix to represent words as vectors ofdocument counts, is to use theterm-term matrix, also called theword-word ma-trixor theterm-context matrix, in which the columns are labeled by words ratherword-wordmatrixthan documents. This matrix is thus of dimensionality|V|\u21e5|V|and each cell records",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 15,
      "token_count": 418,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 39",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 16,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "More common: word-word matrix(or \"term-context matrix\")Two wordsare similar in meaning if their context vectors are similar6.3\u2022WORDS ANDVECTORS9Information retrieval(IR) is the task of \ufb01nding the documentdfrom theDinformationretrievaldocuments in some collection that best matches a queryq. For IR we\u2019ll therefore alsorepresent a query by a vector, also of length|V|, and we\u2019ll need a way to comparetwo vectors to \ufb01nd how similar they are. (Doing IR will also require ef\ufb01cient waysto store and manipulate these vectors by making use of the convenient fact that thesevectors are sparse, i.e., mostly zeros).Later in the chapter we\u2019ll introduce some of the components of this vector com-parison process: the tf-idf term weighting, and the cosine similarity metric.6.3.2 Words as vectorsWe\u2019ve seen that documents can be represented as vectors in a vector space. Butvector semantics can also be used to represent the meaning ofwords, by associatingeach word with a vector.The word vector is now arow vectorrather than a column vector, and hence therow vectordimensions of the vector are different. The four dimensions of the vector forfool,[36,58,1,4], correspond to the four Shakespeare plays. The same four dimensionsare used to form the vectors for the other 3 words:wit, [20,15,2,3];battle, [1,0,7,13];andgood[114,80,62,89]. Each entry in the vector thus represents the counts of theword\u2019s occurrence in the document corresponding to that dimension.For documents, we saw that similar documents had similar vectors, because sim-ilar documents tend to have similar words. This same principle applies to words:similar words have similar vectors because they tend to occur in similar documents.The term-document matrix thus lets us represent the meaning of a word by the doc-uments it tends to occur in.However, it is most common to use a different kind of context for the dimensionsof a word\u2019s vector representation. Rather than the term-document matrix we use theterm-term matrix, more commonly called theword-word matrixor theterm-word-wordmatrixcontext matrix, in which the columns are labeled by words rather than documents.This matrix is thus of dimensionality|V|\u21e5|V|and each cell records the number oftimes the row (target) word and the column (context) word co-occur in some contextin some training corpus. The context could be the document, in which case the cellrepresents the number of times the two words appear in the same document. It ismost common, however, to use smaller contexts, generally a window around theword, for example of 4 words to the left and 4 words to the right, in which casethe cell represents the number of times (in some training corpus) the column wordoccurs in such a\u00b14 word window around the row word. For example here is oneexample each of some words in their windows:is traditionally followed bycherrypie, a traditional dessertoften mixed, such asstrawberryrhubarb pie. Apple piecomputer peripherals and personaldigitalassistants. These devices usuallya computer. This includesinformationavailable on the internetIf we then take every occurrence of each word (saystrawberry) and count the con-text words around it, we get a word-word co-occurrence matrix. Fig.6.5shows asimpli\ufb01ed subset of the word-word co-occurrence matrix for these four words com-puted from the Wikipedia corpus(Davies, 2015).Note in Fig.6.5that the two wordscherryandstrawberryare more similar toeach other (bothpieandsugartend to occur in their window) than they are",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 17,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "the row (target) word and the column (context) word co-occur in some contextin some training corpus. The context could be the document, in which case the cellrepresents the number of times the two words appear in the same document. It ismost common, however, to use smaller contexts, generally a window around theword, for example of 4 words to the left and 4 words to the right, in which casethe cell represents the number of times (in some training corpus) the column wordoccurs in such a\u00b14 word window around the row word. For example here is oneexample each of some words in their windows:is traditionally followed bycherrypie, a traditional dessertoften mixed, such asstrawberryrhubarb pie. Apple piecomputer peripherals and personaldigitalassistants. These devices usuallya computer. This includesinformationavailable on the internetIf we then take every occurrence of each word (saystrawberry) and count the con-text words around it, we get a word-word co-occurrence matrix. Fig.6.5shows asimpli\ufb01ed subset of the word-word co-occurrence matrix for these four words com-puted from the Wikipedia corpus(Davies, 2015).Note in Fig.6.5that the two wordscherryandstrawberryare more similar toeach other (bothpieandsugartend to occur in their window) than they are to otherwords likedigital; conversely,digitalandinformationare more similar to each otherthan, say, tostrawberry. Fig.6.6shows a spatial visualization.6.3\u2022WORDS ANDVECTORS9the number of times the row (target) word and the column (context) word co-occurin some context in some training corpus. The context could be the document, inwhich case the cell represents the number of times the two words appear in the samedocument. It is most common, however, to use smaller contexts, generally a win-dow around the word, for example of 4 words to the left and 4 words to the right,in which case the cell represents the number of times (in some training corpus) thecolumn word occurs in such a\u00b14 word window around the row word. For examplehere is one example each of some words in their windows:is traditionally followed bycherrypie, a traditional dessertoften mixed, such asstrawberryrhubarb pie. Apple piecomputer peripherals and personaldigitalassistants. These devices usuallya computer. This includesinformationavailable on the internetIf we then take every occurrence of each word (saystrawberry) and count thecontext words around it, we get a word-word co-occurrence matrix. Fig.6.6shows asimpli\ufb01ed subset of the word-word co-occurrence matrix for these four words com-puted from the Wikipedia corpus(Davies, 2015).aardvark...computer data result pie sugar...cherry0 ... 2 8 9 442 25 ...strawberry0 ... 0 0 1 60 19 ...digital0 ... 1670 1683 85 5 4 ...information0 ... 3325 3982 378 5 13 ...Figure 6.6Co-occurrence vectors for four words in the Wikipedia corpus, showing six ofthe dimensions (hand-picked for pedagogical purposes). The vector fordigitalis outlined inred. Note that a real vector would have vastly more dimensions and thus be much sparser.Note in Fig.6.6that the two wordscherryandstrawberryare more similar toeach other (bothpieandsugartend to occur in their window) than they are to otherwords likedigital; conversely,digitalandinformationare more similar to each otherthan,",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 18,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "such asstrawberryrhubarb pie. Apple piecomputer peripherals and personaldigitalassistants. These devices usuallya computer. This includesinformationavailable on the internetIf we then take every occurrence of each word (saystrawberry) and count thecontext words around it, we get a word-word co-occurrence matrix. Fig.6.6shows asimpli\ufb01ed subset of the word-word co-occurrence matrix for these four words com-puted from the Wikipedia corpus(Davies, 2015).aardvark...computer data result pie sugar...cherry0 ... 2 8 9 442 25 ...strawberry0 ... 0 0 1 60 19 ...digital0 ... 1670 1683 85 5 4 ...information0 ... 3325 3982 378 5 13 ...Figure 6.6Co-occurrence vectors for four words in the Wikipedia corpus, showing six ofthe dimensions (hand-picked for pedagogical purposes). The vector fordigitalis outlined inred. Note that a real vector would have vastly more dimensions and thus be much sparser.Note in Fig.6.6that the two wordscherryandstrawberryare more similar toeach other (bothpieandsugartend to occur in their window) than they are to otherwords likedigital; conversely,digitalandinformationare more similar to each otherthan, say, tostrawberry. Fig.6.7shows a spatial visualization.",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 19,
      "token_count": 316,
      "chapter_title": ""
    }
  },
  {
    "content": "100020003000400010002000digital [1683,1670]computer datainformation [3982,3325] 30004000\nFigure 6.7A spatial visualization of word vectors fordigitalandinformation, showing justtwo of the dimensions, corresponding to the wordsdataandcomputer.Note that|V|, the length of the vector, is generally the size of the vocabulary, of-ten between 10,000 and 50,000 words (using the most frequent words in the trainingcorpus; keeping words after about the most frequent 50,000 or so is generally nothelpful). Since most of these numbers are zero these aresparsevector representa-tions; there are ef\ufb01cient algorithms for storing and computing with sparse matrices.Now that we have some intuitions, let\u2019s move on to examine the details of com-puting word similarity. Afterwards we\u2019ll discuss methods for weighting cells.",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 20,
      "token_count": 191,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 40\n\n10CHAPTER6\u2022VECTORSEMANTICS ANDEMBEDDINGSaardvark...computer data result pie sugar...cherry0 ... 2 8 9 442 25strawberry0 ... 0 0 1 60 19digital0 ... 1670 1683 85 5 4information0 ... 3325 3982 378 5 13Figure 6.5Co-occurrence vectors for four words in the Wikipedia corpus, showing six ofthe dimensions (hand-picked for pedagogical purposes). The vector fordigitalis outlined inred. Note that a real vector would have vastly more dimensions and thus be much sparser.\n100020003000400010002000digital [1683,1670]computer datainformation [3982,3325] 30004000\nFigure 6.6A spatial visualization of word vectors fordigitalandinformation, showing justtwo of the dimensions, corresponding to the wordsdataandcomputer.Note that|V|, the length of the vector, is generally the size of the vocabulary,usually between 10,000 and 50,000 words (using the most frequent words in thetraining corpus; keeping words after about the most frequent 50,000 or so is gener-ally not helpful). But of course since most of these numbers are zero these aresparsevector representations, and there are ef\ufb01cient algorithms for storing and computingwith sparse matrices.Now that we have some intuitions, let\u2019s move on to examine the details of com-puting word similarity. Afterwards we\u2019ll discuss the tf-idf method of weightingcells.6.4 Cosine for measuring similarityTo de\ufb01ne similarity between two target wordsvandw, we need a measure for takingtwo such vectors and giving a measure of vector similarity. By far the most commonsimilarity metric is thecosineof the angle between the vectors.The cosine\u2014like most measures for vector similarity used in NLP\u2014is based onthedot productoperator from linear algebra, also called theinner product:dot productinner productdot product(v,w)=v\u00b7w=NXi=1viwi=v1w1+v2w2+...+vNwN(6.7)As we will see, most metrics for similarity between vectors are based on the dotproduct. The dot product acts as a similarity metric because it will tend to be highjust when the two vectors have large values in the same dimensions. Alternatively,vectors that have zeros in different dimensions\u2014orthogonal vectors\u2014will have adot product of 0, representing their strong dissimilarity.\n\n## Page 41\n\nVector Semantics & EmbeddingsWords and Vectors\n\n## Page 42\n\nVector Semantics & EmbeddingsCosine for computing word similarity",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 21,
      "token_count": 576,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 41\n\nVector Semantics & EmbeddingsWords and Vectors\n\n## Page 42\n\nVector Semantics & EmbeddingsCosine for computing word similarity\n\n## Page 43\n\nComputing word similarity: Dot product and cosineThe dot product between two vectors is a scalar:The dot product tends to be high when the two vectors have large values in the same dimensionsDot product can thus be a useful similarity metric between vectors10CHAPTER6\u2022VECTORSEMANTICS ANDEMBEDDINGS6.4 Cosine for measuring similarityTo measure similarity between two target wordsvandw, we need a metric thattakes two vectors (of the same dimensionality, either both with words as dimensions,hence of length|V|, or both with documents as dimensions as documents, of length|D|) and gives a measure of their similarity. By far the most common similaritymetric is thecosineof the angle between the vectors.The cosine\u2014like most measures for vector similarity used in NLP\u2014is based onthedot productoperator from linear algebra, also called theinner product:dot productinner productdot product(v,w)=v\u00b7w=NXi=1viwi=v1w1+v2w2+...+vNwN(6.7)As we will see, most metrics for similarity between vectors are based on the dotproduct. The dot product acts as a similarity metric because it will tend to be highjust when the two vectors have large values in the same dimensions. Alternatively,vectors that have zeros in different dimensions\u2014orthogonal vectors\u2014will have adot product of 0, representing their strong dissimilarity.This raw dot product, however, has a problem as a similarity metric: it favorslongvectors. Thevector lengthis de\ufb01ned asvector length|v|=vuutNXi=1v2i(6.8)The dot product is higher if a vector is longer, with higher values in each dimension.More frequent words have longer vectors, since they tend to co-occur with morewords and have higher co-occurrence values with each of them. The raw dot productthus will be higher for frequent words. But this is a problem; we\u2019d like a similaritymetric that tells us how similar two words are regardless of their frequency.We modify the dot product to normalize for the vector length by dividing thedot product by the lengths of each of the two vectors. Thisnormalized dot productturns out to be the same as the cosine of the angle between the two vectors, followingfrom the de\ufb01nition of the dot product between two vectorsaandb:a\u00b7b=|a||b|cosqa\u00b7b|a||b|=cosq(6.9)Thecosinesimilarity metric between two vectorsvandwthus can be computed as:cosinecosine(v,w)=v\u00b7w|v||w|=NXi=1viwivuutNXi=1v2ivuutNXi=1w2i(6.10)For some applications we pre-normalize each vector, by dividing it by its length,creating aunit vectorof length 1. Thus we could compute a unit vector fromabyunit vectordividing it by|a|. For unit vectors, the dot product is the same as the cosine.",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 22,
      "token_count": 677,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 44\n\nProblem with raw dot-productDot product favors long vectorsDot product is higher if a vector is longer (has higher values in many dimension)Vector length:Frequent words (of, the, you) have long vectors (since they occur many times with other words).So dot product overly favors frequent words10CHAPTER6\u2022VECTORSEMANTICS ANDEMBEDDINGS6.4 Cosine for measuring similarityTo measure similarity between two target wordsvandw, we need a metric thattakes two vectors (of the same dimensionality, either both with words as dimensions,hence of length|V|, or both with documents as dimensions as documents, of length|D|) and gives a measure of their similarity. By far the most common similaritymetric is thecosineof the angle between the vectors.The cosine\u2014like most measures for vector similarity used in NLP\u2014is based onthedot productoperator from linear algebra, also called theinner product:dot productinner productdot product(v,w)=v\u00b7w=NXi=1viwi=v1w1+v2w2+...+vNwN(6.7)As we will see, most metrics for similarity between vectors are based on the dotproduct. The dot product acts as a similarity metric because it will tend to be highjust when the two vectors have large values in the same dimensions. Alternatively,vectors that have zeros in different dimensions\u2014orthogonal vectors\u2014will have adot product of 0, representing their strong dissimilarity.This raw dot product, however, has a problem as a similarity metric: it favorslongvectors. Thevector lengthis de\ufb01ned asvector length|v|=vuutNXi=1v2i(6.8)The dot product is higher if a vector is longer, with higher values in each dimension.More frequent words have longer vectors, since they tend to co-occur with morewords and have higher co-occurrence values with each of them. The raw dot productthus will be higher for frequent words. But this is a problem; we\u2019d like a similaritymetric that tells us how similar two words are regardless of their frequency.We modify the dot product to normalize for the vector length by dividing thedot product by the lengths of each of the two vectors. Thisnormalized dot productturns out to be the same as the cosine of the angle between the two vectors, followingfrom the de\ufb01nition of the dot product between two vectorsaandb:a\u00b7b=|a||b|cosqa\u00b7b|a||b|=cosq(6.9)Thecosinesimilarity metric between two vectorsvandwthus can be computed as:cosinecosine(v,w)=v\u00b7w|v||w|=NXi=1viwivuutNXi=1v2ivuutNXi=1w2i(6.10)For some applications we pre-normalize each vector, by dividing it by its length,creating aunit vectorof length 1. Thus we could compute a unit vector fromabyunit vectordividing it by|a|. For unit vectors, the dot product is the same as the cosine.",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 23,
      "token_count": 655,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 45",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 24,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Alternative: cosine for computing word similarity12CHAPTER6\u2022VECTORSEMANTICS~a\u00b7~b=|~a||~b|cosq~a\u00b7~b|~a||~b|=cosq(6.9)Thecosinesimilarity metric between two vectors~vand~wthus can be computedcosineas:cosine(~v,~w)=~v\u00b7~w|~v||~w|=NXi=1viwivuutNXi=1v2ivuutNXi=1w2i(6.10)For some applications we pre-normalize each vector, by dividing it by its length,creating aunit vectorof length 1. Thus we could compute a unit vector from~abyunit vectordividing it by|~a|. For unit vectors, the dot product is the same as the cosine.The cosine value ranges from 1 for vectors pointing in the same direction, through0 for vectors that are orthogonal, to -1 for vectors pointing in opposite directions.But raw frequency values are non-negative, so the cosine for these vectors rangesfrom 0\u20131.Let\u2019s see how the cosine computes which of the wordsapricotordigitalis closerin meaning toinformation, just using raw counts from the following simpli\ufb01ed table:large data computerapricot20 0digital01 2information16 1cos(apricot,information)=2+0+0p4+0+0p1+36+1=22p38=.16cos(digital,information)=0+6+2p0+1+4p1+36+1=8p38p5=.58(6.11)The model decides thatinformationis closer todigitalthan it is toapricot,aresult that seems sensible. Fig.6.7shows a visualization.6.5 TF-IDF: Weighing terms in the vectorThe co-occurrence matrix in Fig.6.5represented each cell by the raw frequency ofthe co-occurrence of two words.It turns out, however, that simple frequency isn\u2019t the best measure of associationbetween words. One problem is that raw frequency is very skewed and not verydiscriminative. If we want to know what kinds of contexts are shared byapricotandpineapplebut not bydigitalandinformation, we\u2019re not going to get good discrimi-nation from words likethe,it, orthey, which occur frequently with all sorts of wordsand aren\u2019t informative about any particular word.It\u2019s a bit of a paradox. Word that occur nearby frequently (maybesugarappearsoften in our corpus nearapricot) are more important than words that only appear10CHAPTER6\u2022VECTORSEMANTICS ANDEMBEDDINGS6.4 Cosine for measuring similarityTo measure similarity between two target wordsvandw, we need a metric thattakes two vectors (of the same dimensionality, either both with words as dimensions,hence of length|V|, or both with documents as dimensions as documents, of length|D|) and gives a measure of their similarity. By far the most common similaritymetric is thecosineof the angle between the vectors.The cosine\u2014like most measures for vector similarity used in NLP\u2014is based onthedot productoperator from linear algebra, also called theinner product:dot productinner productdot product(v,w)=v\u00b7w=NXi=1viwi=v1w1+v2w2+...+vNwN(6.7)As we will see, most metrics for similarity between vectors are based on the dotproduct. The dot product acts as a similarity metric because it will tend to be highjust when the two vectors have large values in the same dimensions. Alternatively,vectors that have zeros in different dimensions\u2014orthogonal vectors\u2014will have adot product of",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 25,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "from words likethe,it, orthey, which occur frequently with all sorts of wordsand aren\u2019t informative about any particular word.It\u2019s a bit of a paradox. Word that occur nearby frequently (maybesugarappearsoften in our corpus nearapricot) are more important than words that only appear10CHAPTER6\u2022VECTORSEMANTICS ANDEMBEDDINGS6.4 Cosine for measuring similarityTo measure similarity between two target wordsvandw, we need a metric thattakes two vectors (of the same dimensionality, either both with words as dimensions,hence of length|V|, or both with documents as dimensions as documents, of length|D|) and gives a measure of their similarity. By far the most common similaritymetric is thecosineof the angle between the vectors.The cosine\u2014like most measures for vector similarity used in NLP\u2014is based onthedot productoperator from linear algebra, also called theinner product:dot productinner productdot product(v,w)=v\u00b7w=NXi=1viwi=v1w1+v2w2+...+vNwN(6.7)As we will see, most metrics for similarity between vectors are based on the dotproduct. The dot product acts as a similarity metric because it will tend to be highjust when the two vectors have large values in the same dimensions. Alternatively,vectors that have zeros in different dimensions\u2014orthogonal vectors\u2014will have adot product of 0, representing their strong dissimilarity.This raw dot product, however, has a problem as a similarity metric: it favorslongvectors. Thevector lengthis de\ufb01ned asvector length|v|=vuutNXi=1v2i(6.8)The dot product is higher if a vector is longer, with higher values in each dimension.More frequent words have longer vectors, since they tend to co-occur with morewords and have higher co-occurrence values with each of them. The raw dot productthus will be higher for frequent words. But this is a problem; we\u2019d like a similaritymetric that tells us how similar two words are regardless of their frequency.We modify the dot product to normalize for the vector length by dividing thedot product by the lengths of each of the two vectors. Thisnormalized dot productturns out to be the same as the cosine of the angle between the two vectors, followingfrom the de\ufb01nition of the dot product between two vectorsaandb:a\u00b7b=|a||b|cosqa\u00b7b|a||b|=cosq(6.9)Thecosinesimilarity metric between two vectorsvandwthus can be computed as:cosinecosine(v,w)=v\u00b7w|v||w|=NXi=1viwivuutNXi=1v2ivuutNXi=1w2i(6.10)For some applications we pre-normalize each vector, by dividing it by its length,creating aunit vectorof length 1. Thus we could compute a unit vector fromabyunit vectordividing it by|a|. For unit vectors, the dot product is the same as the cosine.Based on the definition of the dot product between two vectors a and b",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 26,
      "token_count": 666,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 46\n\nCosine as a similarity metric-1: vectors point in opposite directions +1:  vectors point in same directions0: vectors are orthogonalBut since raw frequency values are non-negative, the cosine for term-term matrix vectors ranges from 0\u20131 46",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 27,
      "token_count": 57,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 47",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 28,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Cosine examplespiedatacomputercherry44282digital516831670information539823325",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 29,
      "token_count": 19,
      "chapter_title": ""
    }
  },
  {
    "content": "47cos(\uf072v,\uf072w)=\uf072v\u2022\uf072w\uf072v\uf072w=\uf072v\uf072v\u2022\uf072w\uf072w=viwii=1N\u2211vi2i=1N\u2211wi2i=1N\u22116.4\u2022COSINE FOR MEASURING SIMILARITY11This raw dot product, however, has a problem as a similarity metric: it favorslongvectors. Thevector lengthis de\ufb01ned asvector length|v|=vuutNXi=1v2i(6.8)The dot product is higher if a vector is longer, with higher values in each dimension.More frequent words have longer vectors, since they tend to co-occur with morewords and have higher co-occurrence values with each of them. The raw dot productthus will be higher for frequent words. But this is a problem; we\u2019d like a similaritymetric that tells us how similar two words are regardless of their frequency.The simplest way to modify the dot product to normalize for the vector length isto divide the dot product by the lengths of each of the two vectors. Thisnormalizeddot productturns out to be the same as the cosine of the angle between the twovectors, following from the de\ufb01nition of the dot product between two vectorsaandb:a\u00b7b=|a||b|cosqa\u00b7b|a||b|=cosq(6.9)Thecosinesimilarity metric between two vectorsvandwthus can be computed as:cosinecosine(v,w)=v\u00b7w|v||w|=NXi=1viwivuutNXi=1v2ivuutNXi=1w2i(6.10)For some applications we pre-normalize each vector, by dividing it by its length,creating aunit vectorof length 1. Thus we could compute a unit vector fromabyunit vectordividing it by|a|. For unit vectors, the dot product is the same as the cosine.The cosine value ranges from 1 for vectors pointing in the same direction, through0 for vectors that are orthogonal, to -1 for vectors pointing in opposite directions.But raw frequency values are non-negative, so the cosine for these vectors rangesfrom 0\u20131.Let\u2019s see how the cosine computes which of the wordscherryordigitalis closerin meaning toinformation, just using raw counts from the following shortened table:pie data computercherry442 8 2digital5 1683 1670information5 3982 3325cos(cherry,information)=442\u21e45+8\u21e43982+2\u21e43325p4422+82+22p52+39822+33252=.017cos(digital,information)=5\u21e45+1683\u21e43982+1670\u21e43325p52+16832+16702p52+39822+33252=.996The model decides thatinformationis way closer todigitalthan it is tocherry,aresult that seems sensible. Fig.6.7shows a visualization.6.4\u2022COSINE FOR MEASURING SIMILARITY11This raw dot product, however, has a problem as a similarity metric: it favorslongvectors. Thevector lengthis de\ufb01ned asvector length|v|=vuutNXi=1v2i(6.8)The dot product is higher if a vector is longer, with higher values in each dimension.More frequent words have longer vectors, since they tend to co-occur with morewords and have higher co-occurrence values with each of them. The raw dot productthus will be higher for frequent words. But this is a problem;",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 30,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "which of the wordscherryordigitalis closerin meaning toinformation, just using raw counts from the following shortened table:pie data computercherry442 8 2digital5 1683 1670information5 3982 3325cos(cherry,information)=442\u21e45+8\u21e43982+2\u21e43325p4422+82+22p52+39822+33252=.017cos(digital,information)=5\u21e45+1683\u21e43982+1670\u21e43325p52+16832+16702p52+39822+33252=.996The model decides thatinformationis way closer todigitalthan it is tocherry,aresult that seems sensible. Fig.6.7shows a visualization.6.4\u2022COSINE FOR MEASURING SIMILARITY11This raw dot product, however, has a problem as a similarity metric: it favorslongvectors. Thevector lengthis de\ufb01ned asvector length|v|=vuutNXi=1v2i(6.8)The dot product is higher if a vector is longer, with higher values in each dimension.More frequent words have longer vectors, since they tend to co-occur with morewords and have higher co-occurrence values with each of them. The raw dot productthus will be higher for frequent words. But this is a problem; we\u2019d like a similaritymetric that tells us how similar two words are regardless of their frequency.The simplest way to modify the dot product to normalize for the vector length isto divide the dot product by the lengths of each of the two vectors. Thisnormalizeddot productturns out to be the same as the cosine of the angle between the twovectors, following from the de\ufb01nition of the dot product between two vectorsaandb:a\u00b7b=|a||b|cosqa\u00b7b|a||b|=cosq(6.9)Thecosinesimilarity metric between two vectorsvandwthus can be computed as:cosinecosine(v,w)=v\u00b7w|v||w|=NXi=1viwivuutNXi=1v2ivuutNXi=1w2i(6.10)For some applications we pre-normalize each vector, by dividing it by its length,creating aunit vectorof length 1. Thus we could compute a unit vector fromabyunit vectordividing it by|a|. For unit vectors, the dot product is the same as the cosine.The cosine value ranges from 1 for vectors pointing in the same direction, through0 for vectors that are orthogonal, to -1 for vectors pointing in opposite directions.But raw frequency values are non-negative, so the cosine for these vectors rangesfrom 0\u20131.Let\u2019s see how the cosine computes which of the wordscherryordigitalis closerin meaning toinformation, just using raw counts from the following shortened table:pie data computercherry442 8 2digital5 1683 1670information5 3982 3325cos(cherry,information)=442\u21e45+8\u21e43982+2\u21e43325p4422+82+22p52+39822+33252=.017cos(digital,information)=5\u21e45+1683\u21e43982+1670\u21e43325p52+16832+16702p52+39822+33252=.996The model decides thatinformationis way closer todigitalthan it is tocherry,aresult that seems sensible. Fig.6.7shows a visualization.6.4\u2022COSINE FOR MEASURING SIMILARITY11This raw dot product, however, has a problem as a similarity metric: it",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 31,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "vectorof length 1. Thus we could compute a unit vector fromabyunit vectordividing it by|a|. For unit vectors, the dot product is the same as the cosine.The cosine value ranges from 1 for vectors pointing in the same direction, through0 for vectors that are orthogonal, to -1 for vectors pointing in opposite directions.But raw frequency values are non-negative, so the cosine for these vectors rangesfrom 0\u20131.Let\u2019s see how the cosine computes which of the wordscherryordigitalis closerin meaning toinformation, just using raw counts from the following shortened table:pie data computercherry442 8 2digital5 1683 1670information5 3982 3325cos(cherry,information)=442\u21e45+8\u21e43982+2\u21e43325p4422+82+22p52+39822+33252=.017cos(digital,information)=5\u21e45+1683\u21e43982+1670\u21e43325p52+16832+16702p52+39822+33252=.996The model decides thatinformationis way closer todigitalthan it is tocherry,aresult that seems sensible. Fig.6.7shows a visualization.6.4\u2022COSINE FOR MEASURING SIMILARITY11This raw dot product, however, has a problem as a similarity metric: it favorslongvectors. Thevector lengthis de\ufb01ned asvector length|v|=vuutNXi=1v2i(6.8)The dot product is higher if a vector is longer, with higher values in each dimension.More frequent words have longer vectors, since they tend to co-occur with morewords and have higher co-occurrence values with each of them. The raw dot productthus will be higher for frequent words. But this is a problem; we\u2019d like a similaritymetric that tells us how similar two words are regardless of their frequency.The simplest way to modify the dot product to normalize for the vector length isto divide the dot product by the lengths of each of the two vectors. Thisnormalizeddot productturns out to be the same as the cosine of the angle between the twovectors, following from the de\ufb01nition of the dot product between two vectorsaandb:a\u00b7b=|a||b|cosqa\u00b7b|a||b|=cosq(6.9)Thecosinesimilarity metric between two vectorsvandwthus can be computed as:cosinecosine(v,w)=v\u00b7w|v||w|=NXi=1viwivuutNXi=1v2ivuutNXi=1w2i(6.10)For some applications we pre-normalize each vector, by dividing it by its length,creating aunit vectorof length 1. Thus we could compute a unit vector fromabyunit vectordividing it by|a|. For unit vectors, the dot product is the same as the cosine.The cosine value ranges from 1 for vectors pointing in the same direction, through0 for vectors that are orthogonal, to -1 for vectors pointing in opposite directions.But raw frequency values are non-negative, so the cosine for these vectors rangesfrom 0\u20131.Let\u2019s see how the cosine computes which of the wordscherryordigitalis closerin meaning toinformation, just using raw counts from the following shortened table:pie data computercherry442 8 2digital5 1683 1670information5 3982",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 32,
      "token_count": 741,
      "chapter_title": ""
    }
  },
  {
    "content": "of the two vectors. Thisnormalizeddot productturns out to be the same as the cosine of the angle between the twovectors, following from the de\ufb01nition of the dot product between two vectorsaandb:a\u00b7b=|a||b|cosqa\u00b7b|a||b|=cosq(6.9)Thecosinesimilarity metric between two vectorsvandwthus can be computed as:cosinecosine(v,w)=v\u00b7w|v||w|=NXi=1viwivuutNXi=1v2ivuutNXi=1w2i(6.10)For some applications we pre-normalize each vector, by dividing it by its length,creating aunit vectorof length 1. Thus we could compute a unit vector fromabyunit vectordividing it by|a|. For unit vectors, the dot product is the same as the cosine.The cosine value ranges from 1 for vectors pointing in the same direction, through0 for vectors that are orthogonal, to -1 for vectors pointing in opposite directions.But raw frequency values are non-negative, so the cosine for these vectors rangesfrom 0\u20131.Let\u2019s see how the cosine computes which of the wordscherryordigitalis closerin meaning toinformation, just using raw counts from the following shortened table:pie data computercherry442 8 2digital5 1683 1670information5 3982 3325cos(cherry,information)=442\u21e45+8\u21e43982+2\u21e43325p4422+82+22p52+39822+33252=.017cos(digital,information)=5\u21e45+1683\u21e43982+1670\u21e43325p52+16832+16702p52+39822+33252=.996The model decides thatinformationis way closer todigitalthan it is tocherry,aresult that seems sensible. Fig.6.7shows a visualization.6.4\u2022COSINE FOR MEASURING SIMILARITY11This raw dot product, however, has a problem as a similarity metric: it favorslongvectors. Thevector lengthis de\ufb01ned asvector length|v|=vuutNXi=1v2i(6.8)The dot product is higher if a vector is longer, with higher values in each dimension.More frequent words have longer vectors, since they tend to co-occur with morewords and have higher co-occurrence values with each of them. The raw dot productthus will be higher for frequent words. But this is a problem; we\u2019d like a similaritymetric that tells us how similar two words are regardless of their frequency.The simplest way to modify the dot product to normalize for the vector length isto divide the dot product by the lengths of each of the two vectors. Thisnormalizeddot productturns out to be the same as the cosine of the angle between the twovectors, following from the de\ufb01nition of the dot product between two vectorsaandb:a\u00b7b=|a||b|cosqa\u00b7b|a||b|=cosq(6.9)Thecosinesimilarity metric between two vectorsvandwthus can be computed as:cosinecosine(v,w)=v\u00b7w|v||w|=NXi=1viwivuutNXi=1v2ivuutNXi=1w2i(6.10)For some applications we pre-normalize each vector, by dividing it by its length,creating aunit vectorof length 1. Thus we could compute a unit vector fromabyunit vectordividing it by|a|. For unit vectors, the dot product is the same as the cosine.The cosine value ranges from 1 for vectors pointing in the same",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 33,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "in each dimension.More frequent words have longer vectors, since they tend to co-occur with morewords and have higher co-occurrence values with each of them. The raw dot productthus will be higher for frequent words. But this is a problem; we\u2019d like a similaritymetric that tells us how similar two words are regardless of their frequency.The simplest way to modify the dot product to normalize for the vector length isto divide the dot product by the lengths of each of the two vectors. Thisnormalizeddot productturns out to be the same as the cosine of the angle between the twovectors, following from the de\ufb01nition of the dot product between two vectorsaandb:a\u00b7b=|a||b|cosqa\u00b7b|a||b|=cosq(6.9)Thecosinesimilarity metric between two vectorsvandwthus can be computed as:cosinecosine(v,w)=v\u00b7w|v||w|=NXi=1viwivuutNXi=1v2ivuutNXi=1w2i(6.10)For some applications we pre-normalize each vector, by dividing it by its length,creating aunit vectorof length 1. Thus we could compute a unit vector fromabyunit vectordividing it by|a|. For unit vectors, the dot product is the same as the cosine.The cosine value ranges from 1 for vectors pointing in the same direction, through0 for vectors that are orthogonal, to -1 for vectors pointing in opposite directions.But raw frequency values are non-negative, so the cosine for these vectors rangesfrom 0\u20131.Let\u2019s see how the cosine computes which of the wordscherryordigitalis closerin meaning toinformation, just using raw counts from the following shortened table:pie data computercherry442 8 2digital5 1683 1670information5 3982 3325cos(cherry,information)=442\u21e45+8\u21e43982+2\u21e43325p4422+82+22p52+39822+33252=.017cos(digital,information)=5\u21e45+1683\u21e43982+1670\u21e43325p52+16832+16702p52+39822+33252=.996The model decides thatinformationis way closer todigitalthan it is tocherry,aresult that seems sensible. Fig.6.7shows a visualization.",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 34,
      "token_count": 516,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 48\n\nVisualizing cosines (well, angles)12CHAPTER6\u2022VECTORSEMANTICS ANDEMBEDDINGS\n50010001500200025003000500digitalcherryinformationDimension 1: \u2018pie\u2019\nDimension 2: \u2018computer\u2019Figure 6.7A (rough) graphical demonstration of cosine similarity, showing vectors forthree words (cherry,digital, andinformation) in the two dimensional space de\ufb01ned by countsof the wordscomputerandpienearby. Note that the angle betweendigitalandinformationissmaller than the angle betweencherryandinformation. When two vectors are more similar,the cosine is larger but the angle is smaller; the cosine has its maximum (1) when the anglebetween two vectors is smallest (0\u0000); the cosine of all other angles is less than 1.6.5 TF-IDF: Weighing terms in the vectorThe co-occurrence matrix in Fig.6.5represented each cell by the raw frequency ofthe co-occurrence of two words.It turns out, however, that simple frequency isn\u2019t the best measure of associationbetween words. One problem is that raw frequency is very skewed and not verydiscriminative. If we want to know what kinds of contexts are shared bycherryandstrawberrybut not bydigitalandinformation, we\u2019re not going to get good discrimi-nation from words likethe,it, orthey, which occur frequently with all sorts of wordsand aren\u2019t informative about any particular word. We saw this also in Fig.6.3forthe Shakespeare corpus; the dimension for the wordgoodis not very discrimina-tive between plays;goodis simply a frequent word and has roughly equivalent highfrequencies in each of the plays.It\u2019s a bit of a paradox. Words that occur nearby frequently (maybepienearbycherry) are more important than words that only appear once or twice. Yet wordsthat are too frequent\u2014ubiquitous, liketheorgood\u2014 are unimportant. How can webalance these two con\ufb02icting constraints?Thetf-idf algorithm(the \u2018-\u2019 here is a hyphen, not a minus sign) is the productof two terms, each term capturing one of these two intuitions:The \ufb01rst is theterm frequency(Luhn, 1957): the frequency of the wordtin theterm frequencydocumentd. We can just use the raw count as the term frequency:tft,d=count(t,d)(6.11)Alternatively we can squash the raw frequency a bit, by using the log10of the fre-quency instead. The intuition is that a word appearing 100 times in a documentdoesn\u2019t make that word 100 times more likely to be relevant to the meaning of thedocument. Because we can\u2019t take the log of 0, we normally add 1 to the count:3tft,d=log10(count(t,d)+1)(6.12)If we use log weighting, terms which occur 10 times in a document would have atf=2, 100 times in a document tf=3, 1000 times tf=4, and so on.3Or we can use this alternative: tft,d=\u21e21+log10count(t,d)if count(t,d)>00 otherwise\n\n## Page 49\n\nVector Semantics & EmbeddingsCosine for computing word similarity\n\n## Page 50\n\nVector Semantics & EmbeddingsTF-IDF",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 35,
      "token_count": 721,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 49\n\nVector Semantics & EmbeddingsCosine for computing word similarity\n\n## Page 50\n\nVector Semantics & EmbeddingsTF-IDF\n\n## Page 51\n\nBut raw frequency is a bad representation\u2022The co-occurrence matrices we have seen represent each cell by word frequencies.\u2022Frequency is clearly useful; if sugarappears a lot near apricot, that's useful information.\u2022But overly frequent words like the, it,or theyare not very informative about the context\u2022It's a paradox! How can we balance these two conflicting constraints?",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 36,
      "token_count": 115,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 52\n\nTwo common solutions for word weightingtf-idf:     tf-idfvalue for word t in document d:PMI: (Pointwise mutual information)\u25e6PMI\ud835\udc98\ud835\udfcf,\ud835\udc98\ud835\udfd0=\ud835\udc8d\ud835\udc90\ud835\udc88\ud835\udc91(\ud835\udc98\ud835\udfcf,\ud835\udc98\ud835\udfd0)\ud835\udc91\ud835\udc98\ud835\udfcf\ud835\udc91(\ud835\udc98\ud835\udfd0)14CHAPTER6\u2022VECTORSEMANTICSCollection FrequencyDocument FrequencyRomeo1131action11331We assign importance to these more discriminative words likeRomeoviatheinverse document frequencyoridfterm weight(Sparck Jones, 1972).idfThe idf is de\ufb01ned using the fractionN/dft, whereNis the total number ofdocuments in the collection, and dftis the number of documents in whichtermtoccurs. The fewer documents in which a term occurs, the higher thisweight. The lowest weight of 1 is assigned to terms that occur in all thedocuments. It\u2019s usually clear what counts as a document: in Shakespearewe would use a play; when processing a collection of encyclopedia articleslike Wikipedia, the document is a Wikipedia page; in processing newspaperarticles, the document is a single article. Occasionally your corpus mightnot have appropriate document divisions and you might need to break up thecorpus into documents yourself for the purposes of computing idf.Because of the large number of documents in many collections, this mea-sure is usually squashed with a log function. The resulting de\ufb01nition for in-verse document frequency (idf) is thusidft=log10\u2713Ndft\u25c6(6.12)Here are some idf values for some words in the Shakespeare corpus, rangingfrom extremely informative words which occur in only one play likeRomeo, tothose that occur in a few likesaladorFalstaff, to those which are very common likefoolor so common as to be completely non-discriminative since they occur in all 37plays likegoodorsweet.3WorddfidfRomeo11.57salad21.27Falstaff40.967forest120.489battle210.074fool360.012good370sweet370Thetf-idfweighting of the value for wordtin documentd,wt,dthus combinestf-idfterm frequency with idf:wt,d=tft,d\u21e5idft(6.13)Fig.6.8applies tf-idf weighting to the Shakespeare term-document matrix in Fig.6.2.Note that the tf-idf values for the dimension corresponding to the wordgoodhavenow all become 0; since this word appears in every document, the tf-idf algorithmleads it to be ignored in any comparison of the plays. Similarly, the wordfool, whichappears in 36 out of the 37 plays, has a much lower weight.The tf-idf weighting is by far the dominant way of weighting co-occurrence ma-trices in information retrieval, but also plays a role in many other aspects of natural3Sweetwas one of Shakespeare\u2019s favorite adjectives, a fact probably related to the increased use ofsugar in European recipes around the turn of the 16th century(Jurafsky, 2014, p. 175).Words like \"the\" or \"it\" have very low idfSee if words like \"good\" appear more often with \"great\" than we would expect by chance",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 37,
      "token_count": 733,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 53",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 38,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Term frequency (tf) in the tf-idfalgorithmWe could imagine using raw count:tft,d= count(t,d)But instead of using raw count, we usually squash a bit:12CHAPTER6\u2022VECTORSEMANTICS ANDEMBEDDINGSis not the best measure of association between words. Raw frequency is very skewedand not very discriminative. If we want to know what kinds of contexts are sharedbycherryandstrawberrybut not bydigitalandinformation, we\u2019re not going to getgood discrimination from words likethe,it, orthey, which occur frequently withall sorts of words and aren\u2019t informative about any particular word. We saw thisalso in Fig.6.3for the Shakespeare corpus; the dimension for the wordgoodis notvery discriminative between plays;goodis simply a frequent word and has roughlyequivalent high frequencies in each of the plays.It\u2019s a bit of a paradox. Words that occur nearby frequently (maybepienearbycherry) are more important than words that only appear once or twice. Yet wordsthat are too frequent\u2014ubiquitous, liketheorgood\u2014 are unimportant. How can webalance these two con\ufb02icting constraints?There are two common solutions to this problem: in this section we\u2019ll describethetf-idfweighting, usually used when the dimensions are documents. In the nextwe introduce thePPMIalgorithm (usually used when the dimensions are words).Thetf-idf weighting(the \u2018-\u2019 here is a hyphen, not a minus sign) is the productof two terms, each term capturing one of these two intuitions:The \ufb01rst is theterm frequency(Luhn,1957): the frequency of the wordtin theterm frequencydocumentd. We can just use the raw count as the term frequency:tft,d=count(t,d)(6.11)More commonly we squash the raw frequency a bit, by using the log10of the fre-quency instead. The intuition is that a word appearing 100 times in a documentdoesn\u2019t make that word 100 times more likely to be relevant to the meaning of thedocument. We also need to do something special with counts of 0, since we can\u2019ttake the log of 0.2tft,d=(1+log10count(t,d)if count(t,d)>00 otherwise(6.12)If we use log weighting, terms which occur 0 times in a document would have tf=0,1 times in a document tf=1+log10(1)=1+0=1, 10 times in a document tf=1+log10(10)=2, 100 times tf=1+log10(100)=3, 1000 times tf=4, and so on.The second factor in tf-idf is used to give a higher weight to words that occuronly in a few documents. Terms that are limited to a few documents are usefulfor discriminating those documents from the rest of the collection; terms that occurfrequently across the entire collection aren\u2019t as helpful. Thedocument frequencydocumentfrequencydftof a termtis the number of documents it occurs in. Document frequency isnot the same as thecollection frequencyof a term, which is the total number oftimes the word appears in the whole collection in any document. Consider in thecollection of Shakespeare\u2019s 37 plays the two wordsRomeoandaction. The wordshave identical collection frequencies (they both occur 113 times in all the plays) butvery different document frequencies, since Romeo only occurs in a single play. Ifour goal is to \ufb01nd documents about the romantic tribulations of Romeo, the wordRomeoshould be highly weighted, but notaction:Collection FrequencyDocument FrequencyRomeo1131action113312We can also use this alternative formulation, which we have used in earlier editions:",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 39,
      "token_count": 795,
      "chapter_title": ""
    }
  },
  {
    "content": "occur 0 times in a document would have tf=0,1 times in a document tf=1+log10(1)=1+0=1, 10 times in a document tf=1+log10(10)=2, 100 times tf=1+log10(100)=3, 1000 times tf=4, and so on.The second factor in tf-idf is used to give a higher weight to words that occuronly in a few documents. Terms that are limited to a few documents are usefulfor discriminating those documents from the rest of the collection; terms that occurfrequently across the entire collection aren\u2019t as helpful. Thedocument frequencydocumentfrequencydftof a termtis the number of documents it occurs in. Document frequency isnot the same as thecollection frequencyof a term, which is the total number oftimes the word appears in the whole collection in any document. Consider in thecollection of Shakespeare\u2019s 37 plays the two wordsRomeoandaction. The wordshave identical collection frequencies (they both occur 113 times in all the plays) butvery different document frequencies, since Romeo only occurs in a single play. Ifour goal is to \ufb01nd documents about the romantic tribulations of Romeo, the wordRomeoshould be highly weighted, but notaction:Collection FrequencyDocument FrequencyRomeo1131action113312We can also use this alternative formulation, which we have used in earlier editions: tft,d=log10(count(t,d)+1)",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 40,
      "token_count": 312,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 54",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 41,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Document frequency (df)dftis the number of documents toccurs in.(note this is not collection frequency: total count across all documents)\"Romeo\" is very distinctive for one Shakespeare play:12CHAPTER6\u2022VECTORSEMANTICS ANDEMBEDDINGSthat are too frequent\u2014ubiquitous, liketheorgood\u2014 are unimportant. How can webalance these two con\ufb02icting constraints?There are two common solutions to this problem: in this section we\u2019ll describethetf-idfalgorithm, usually used when the dimensions are documents. In the nextwe introduce thePPMIalgorithm (usually used when the dimensions are words).Thetf-idf algorithm(the \u2018-\u2019 here is a hyphen, not a minus sign) is the productof two terms, each term capturing one of these two intuitions:The \ufb01rst is theterm frequency(Luhn, 1957): the frequency of the wordtin theterm frequencydocumentd. We can just use the raw count as the term frequency:tft,d=count(t,d)(6.11)More commonly we squash the raw frequency a bit, by using the log10of the fre-quency instead. The intuition is that a word appearing 100 times in a documentdoesn\u2019t make that word 100 times more likely to be relevant to the meaning of thedocument. Because we can\u2019t take the log of 0, we normally add 1 to the count:2tft,d=log10(count(t,d)+1)(6.12)If we use log weighting, terms which occur 0 times in a document would havetf=log10(1)=0, 10 times in a document tf=log10(11)=1.4, 100 times tf=log10(101)=2.004, 1000 times tf=3.00044, and so on.The second factor in tf-idf is used to give a higher weight to words that occuronly in a few documents. Terms that are limited to a few documents are usefulfor discriminating those documents from the rest of the collection; terms that occurfrequently across the entire collection aren\u2019t as helpful. Thedocument frequencydocumentfrequencydftof a termtis the number of documents it occurs in. Document frequency isnot the same as thecollection frequencyof a term, which is the total number oftimes the word appears in the whole collection in any document. Consider in thecollection of Shakespeare\u2019s 37 plays the two wordsRomeoandaction. The wordshave identical collection frequencies (they both occur 113 times in all the plays) butvery different document frequencies, since Romeo only occurs in a single play. Ifour goal is to \ufb01nd documents about the romantic tribulations of Romeo, the wordRomeoshould be highly weighted, but notaction:Collection FrequencyDocument FrequencyRomeo1131action11331We emphasize discriminative words likeRomeovia theinverse document fre-quencyoridfterm weight(Sparck Jones, 1972). The idf is de\ufb01ned using the frac-idftionN/dft, whereNis the total number of documents in the collection, and dftisthe number of documents in which termtoccurs. The fewer documents in which aterm occurs, the higher this weight. The lowest weight of 1 is assigned to terms thatoccur in all the documents. It\u2019s usually clear what counts as a document: in Shake-speare we would use a play; when processing a collection of encyclopedia articleslike Wikipedia, the document is a Wikipedia page; in processing newspaper articles,the document is a single article. Occasionally your corpus might not have appropri-ate document divisions and you might need to break up the corpus into documentsyourself for the purposes of computing idf.2Or we can use this alternative:",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 42,
      "token_count": 785,
      "chapter_title": ""
    }
  },
  {
    "content": "appears in the whole collection in any document. Consider in thecollection of Shakespeare\u2019s 37 plays the two wordsRomeoandaction. The wordshave identical collection frequencies (they both occur 113 times in all the plays) butvery different document frequencies, since Romeo only occurs in a single play. Ifour goal is to \ufb01nd documents about the romantic tribulations of Romeo, the wordRomeoshould be highly weighted, but notaction:Collection FrequencyDocument FrequencyRomeo1131action11331We emphasize discriminative words likeRomeovia theinverse document fre-quencyoridfterm weight(Sparck Jones, 1972). The idf is de\ufb01ned using the frac-idftionN/dft, whereNis the total number of documents in the collection, and dftisthe number of documents in which termtoccurs. The fewer documents in which aterm occurs, the higher this weight. The lowest weight of 1 is assigned to terms thatoccur in all the documents. It\u2019s usually clear what counts as a document: in Shake-speare we would use a play; when processing a collection of encyclopedia articleslike Wikipedia, the document is a Wikipedia page; in processing newspaper articles,the document is a single article. Occasionally your corpus might not have appropri-ate document divisions and you might need to break up the corpus into documentsyourself for the purposes of computing idf.2Or we can use this alternative: tft,d=\u21e21+log10count(t,d)if count(t,d)>00 otherwise",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 43,
      "token_count": 323,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 55",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 44,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Inverse document frequency (idf)6.5\u2022TF-IDF: WEIGHING TERMS IN THE VECTOR13Because of the large number of documents in many collections, this measuretoo is usually squashed with a log function. The resulting de\ufb01nition for inversedocument frequency (idf) is thusidft=log10\u2713Ndft\u25c6(6.13)Here are some idf values for some words in the Shakespeare corpus, ranging fromextremely informative words which occur in only one play likeRomeo, to those thatoccur in a few likesaladorFalstaff, to those which are very common likefoolor socommon as to be completely non-discriminative since they occur in all 37 plays likegoodorsweet.3WorddfidfRomeo11.57salad21.27Falstaff40.967forest120.489battle210.246wit340.037fool360.012good370sweet370Thetf-idfweighted valuewt,dfor wordtin documentdthus combines termtf-idffrequency tft,d(de\ufb01ned either by Eq.6.11or by Eq.6.12) with idf from Eq.6.13:wt,d=tft,d\u21e5idft(6.14)Fig.6.9applies tf-idf weighting to the Shakespeare term-document matrix in Fig.6.2,using the tf equation Eq.6.12. Note that the tf-idf values for the dimension corre-sponding to the wordgoodhave now all become 0; since this word appears in everydocument, the tf-idf algorithm leads it to be ignored. Similarly, the wordfool, whichappears in 36 out of the 37 plays, has a much lower weight.As You Like It Twelfth Night Julius Caesar Henry Vbattle0.074 0 0.22 0.28good00 0 0fool0.019 0.021 0.0036 0.0083wit0.049 0.044 0.018 0.022Figure 6.9A tf-idf weighted term-document matrix for four words in four Shakespeareplays, using the counts in Fig.6.2. For example the 0.049 value forwitinAs You Like Itisthe product of tf=log10(20+1)=1.322 and idf=.037. Note that the idf weighting haseliminated the importance of the ubiquitous wordgoodand vastly reduced the impact of thealmost-ubiquitous wordfool.The tf-idf weighting is the way for weighting co-occurrence matrices in infor-mation retrieval, but also plays a role in many other aspects of natural languageprocessing. It\u2019s also a great baseline, the simple thing to try \ufb01rst. We\u2019ll look at otherweightings like PPMI (Positive Pointwise Mutual Information) in Section6.6.3Sweetwas one of Shakespeare\u2019s favorite adjectives, a fact probably related to the increased use ofsugar in European recipes around the turn of the 16th century(Jurafsky, 2014, p. 175).6.5\u2022TF-IDF: WEIGHING TERMS IN THE VECTOR13Because of the large number of documents in many collections, this measuretoo is usually squashed with a log function. The resulting de\ufb01nition for inversedocument frequency (idf) is thusidft=log10\u2713Ndft\u25c6(6.13)Here are some idf values for some words in the Shakespeare corpus, ranging fromextremely informative words which occur in only one play likeRomeo, to those thatoccur in a few likesaladorFalstaff, to those which are very common likefoolor socommon as to be completely non-discriminative since they occur in all",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 45,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "Note that the idf weighting haseliminated the importance of the ubiquitous wordgoodand vastly reduced the impact of thealmost-ubiquitous wordfool.The tf-idf weighting is the way for weighting co-occurrence matrices in infor-mation retrieval, but also plays a role in many other aspects of natural languageprocessing. It\u2019s also a great baseline, the simple thing to try \ufb01rst. We\u2019ll look at otherweightings like PPMI (Positive Pointwise Mutual Information) in Section6.6.3Sweetwas one of Shakespeare\u2019s favorite adjectives, a fact probably related to the increased use ofsugar in European recipes around the turn of the 16th century(Jurafsky, 2014, p. 175).6.5\u2022TF-IDF: WEIGHING TERMS IN THE VECTOR13Because of the large number of documents in many collections, this measuretoo is usually squashed with a log function. The resulting de\ufb01nition for inversedocument frequency (idf) is thusidft=log10\u2713Ndft\u25c6(6.13)Here are some idf values for some words in the Shakespeare corpus, ranging fromextremely informative words which occur in only one play likeRomeo, to those thatoccur in a few likesaladorFalstaff, to those which are very common likefoolor socommon as to be completely non-discriminative since they occur in all 37 plays likegoodorsweet.3WorddfidfRomeo11.57salad21.27Falstaff40.967forest120.489battle210.246wit340.037fool360.012good370sweet370Thetf-idfweighted valuewt,dfor wordtin documentdthus combines termtf-idffrequency tft,d(de\ufb01ned either by Eq.6.11or by Eq.6.12) with idf from Eq.6.13:wt,d=tft,d\u21e5idft(6.14)Fig.6.9applies tf-idf weighting to the Shakespeare term-document matrix in Fig.6.2,using the tf equation Eq.6.12. Note that the tf-idf values for the dimension corre-sponding to the wordgoodhave now all become 0; since this word appears in everydocument, the tf-idf algorithm leads it to be ignored. Similarly, the wordfool, whichappears in 36 out of the 37 plays, has a much lower weight.As You Like It Twelfth Night Julius Caesar Henry Vbattle0.074 0 0.22 0.28good00 0 0fool0.019 0.021 0.0036 0.0083wit0.049 0.044 0.018 0.022Figure 6.9A tf-idf weighted term-document matrix for four words in four Shakespeareplays, using the counts in Fig.6.2. For example the 0.049 value forwitinAs You Like Itisthe product of tf=log10(20+1)=1.322 and idf=.037. Note that the idf weighting haseliminated the importance of the ubiquitous wordgoodand vastly reduced the impact of thealmost-ubiquitous wordfool.The tf-idf weighting is the way for weighting co-occurrence matrices in infor-mation retrieval, but also plays a role in many other aspects of natural languageprocessing. It\u2019s also a great baseline, the simple thing to try \ufb01rst. We\u2019ll look at otherweightings like PPMI (Positive Pointwise Mutual Information) in Section6.6.3Sweetwas one of Shakespeare\u2019s favorite adjectives, a fact probably related to the increased use ofsugar in European recipes around the turn of the 16th century(Jurafsky, 2014,",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 46,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "whichappears in 36 out of the 37 plays, has a much lower weight.As You Like It Twelfth Night Julius Caesar Henry Vbattle0.074 0 0.22 0.28good00 0 0fool0.019 0.021 0.0036 0.0083wit0.049 0.044 0.018 0.022Figure 6.9A tf-idf weighted term-document matrix for four words in four Shakespeareplays, using the counts in Fig.6.2. For example the 0.049 value forwitinAs You Like Itisthe product of tf=log10(20+1)=1.322 and idf=.037. Note that the idf weighting haseliminated the importance of the ubiquitous wordgoodand vastly reduced the impact of thealmost-ubiquitous wordfool.The tf-idf weighting is the way for weighting co-occurrence matrices in infor-mation retrieval, but also plays a role in many other aspects of natural languageprocessing. It\u2019s also a great baseline, the simple thing to try \ufb01rst. We\u2019ll look at otherweightings like PPMI (Positive Pointwise Mutual Information) in Section6.6.3Sweetwas one of Shakespeare\u2019s favorite adjectives, a fact probably related to the increased use ofsugar in European recipes around the turn of the 16th century(Jurafsky, 2014, p. 175).N is the total number of documents in the collection",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 47,
      "token_count": 315,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 56\n\nWhat is a document?Could be a play or a Wikipedia articleBut for the purposes of tf-idf, documents can be anything; we often call each paragraph a document!",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 48,
      "token_count": 40,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 57",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 49,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Final tf-idfweighted value for a wordRaw counts:tf-idf:6.5\u2022TF-IDF: WEIGHING TERMS IN THE VECTOR13Because of the large number of documents in many collections, this measuretoo is usually squashed with a log function. The resulting de\ufb01nition for inversedocument frequency (idf) is thusidft=log10\u2713Ndft\u25c6(6.13)Here are some idf values for some words in the Shakespeare corpus, ranging fromextremely informative words which occur in only one play likeRomeo, to those thatoccur in a few likesaladorFalstaff, to those which are very common likefoolor socommon as to be completely non-discriminative since they occur in all 37 plays likegoodorsweet.3WorddfidfRomeo11.57salad21.27Falstaff40.967forest120.489battle210.246wit340.037fool360.012good370sweet370Thetf-idfweighted valuewt,dfor wordtin documentdthus combines termtf-idffrequency tft,d(de\ufb01ned either by Eq.6.11or by Eq.6.12) with idf from Eq.6.13:wt,d=tft,d\u21e5idft(6.14)Fig.6.9applies tf-idf weighting to the Shakespeare term-document matrix in Fig.6.2,using the tf equation Eq.6.12. Note that the tf-idf values for the dimension corre-sponding to the wordgoodhave now all become 0; since this word appears in everydocument, the tf-idf algorithm leads it to be ignored. Similarly, the wordfool, whichappears in 36 out of the 37 plays, has a much lower weight.As You Like It Twelfth Night Julius Caesar Henry Vbattle0.074 0 0.22 0.28good00 0 0fool0.019 0.021 0.0036 0.0083wit0.049 0.044 0.018 0.022Figure 6.9A tf-idf weighted term-document matrix for four words in four Shakespeareplays, using the counts in Fig.6.2. For example the 0.049 value forwitinAs You Like Itisthe product of tf=log10(20+1)=1.322 and idf=.037. Note that the idf weighting haseliminated the importance of the ubiquitous wordgoodand vastly reduced the impact of thealmost-ubiquitous wordfool.The tf-idf weighting is the way for weighting co-occurrence matrices in infor-mation retrieval, but also plays a role in many other aspects of natural languageprocessing. It\u2019s also a great baseline, the simple thing to try \ufb01rst. We\u2019ll look at otherweightings like PPMI (Positive Pointwise Mutual Information) in Section6.6.3Sweetwas one of Shakespeare\u2019s favorite adjectives, a fact probably related to the increased use ofsugar in European recipes around the turn of the 16th century(Jurafsky, 2014, p. 175).6.3\u2022WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 50,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "and idf=.037. Note that the idf weighting haseliminated the importance of the ubiquitous wordgoodand vastly reduced the impact of thealmost-ubiquitous wordfool.The tf-idf weighting is the way for weighting co-occurrence matrices in infor-mation retrieval, but also plays a role in many other aspects of natural languageprocessing. It\u2019s also a great baseline, the simple thing to try \ufb01rst. We\u2019ll look at otherweightings like PPMI (Positive Pointwise Mutual Information) in Section6.6.3Sweetwas one of Shakespeare\u2019s favorite adjectives, a fact probably related to the increased use ofsugar in European recipes around the turn of the 16th century(Jurafsky, 2014, p. 175).6.3\u2022WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like Itis represented as the list [1,114,36,20] (the \ufb01rstcolumnvectorin Fig.6.3) andJulius Caesaris represented as the list [7,62,1,2] (the thirdcolumn vector). Avector spaceis a collection of vectors, characterized by theirvector spacedimension. In the example in Fig.6.3, the document vectors are of dimension 4,dimensionjust so they \ufb01t on the page; in real term-document matrices, the vectors representingeach document would have dimensionality|V|, the vocabulary size.The ordering of the numbers in a vector space indicates different meaningful di-mensions on which documents vary. Thus the \ufb01rst dimension for both these vectorscorresponds to the number of times the wordbattleoccurs, and we can compareeach dimension, noting for example that the vectors forAs You Like ItandTwelfthNighthave similar values (1 and 0, respectively) for the \ufb01rst dimension.As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.3The term-document matrix for four words in four Shakespeare plays. The redboxes show that each document is represented as a column vector of length four.We can think of the vector for a document as a point in|V|-dimensional space;thus the documents in Fig.6.3are points in 4-dimensional space. Since 4-dimensionalspaces are hard to visualize, Fig.6.4shows a visualization in two dimensions; we\u2019vearbitrarily chosen the dimensions corresponding to the wordsbattleandfool.",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 51,
      "token_count": 654,
      "chapter_title": ""
    }
  },
  {
    "content": "51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 52,
      "token_count": 41,
      "chapter_title": ""
    }
  },
  {
    "content": "354045505560Figure 6.4A spatial visualization of the document vectors for the four Shakespeare playdocuments, showing just two of the dimensions, corresponding to the wordsbattleandfool.The comedies have high values for thefooldimension and low values for thebattledimension.Term-document matrices were originally de\ufb01ned as a means of \ufb01nding similardocuments for the task of documentinformation retrieval. Two documents that are14CHAPTER6\u2022VECTORSEMANTICS ANDEMBEDDINGSAs You Like It Twelfth Night Julius Caesar Henry Vbattle0.246 0 0.454 0.520good00 0 0fool0.030 0.033 0.0012 0.0019wit0.085 0.081 0.048 0.054Figure 6.9A tf-idf weighted term-document matrix for four words in four Shakespeareplays, using the counts in Fig.6.2. For example the 0.085 value forwitinAs You Like Itisthe product of tf=1+log10(20)=2.301 and idf=.037. Note that the idf weighting haseliminated the importance of the ubiquitous wordgoodand vastly reduced the impact of thealmost-ubiquitous wordfool.6.6 Pointwise Mutual Information (PMI)An alternative weighting function to tf-idf, PPMI (positive pointwise mutual infor-mation), is used for term-term-matrices, when the vector dimensions correspond towords rather than documents. PPMI draws on the intuition that the best way to weighthe association between two words is to ask how muchmorethe two words co-occurin our corpus than we would have a priori expected them to appear by chance.Pointwise mutual information(Fano,1961)4is one of the most important con-pointwisemutualinformationcepts in NLP. It is a measure of how often two eventsxandyoccur, compared withwhat we would expect if they were independent:I(x,y)=log2P(x,y)P(x)P(y)(6.16)The pointwise mutual information between a target wordwand a context wordc(Church and Hanks1989,Church and Hanks1990) is then de\ufb01ned as:PMI(w,c)=log2P(w,c)P(w)P(c)(6.17)The numerator tells us how often we observed the two words together (assumingwe compute probability by using the MLE). The denominator tells us how oftenwe wouldexpectthe two words to co-occur assuming they each occurred indepen-dently; recall that the probability of two independent events both occurring is justthe product of the probabilities of the two events. Thus, the ratio gives us an esti-mate of how much more the two words co-occur than we expect by chance. PMI isa useful tool whenever we need to \ufb01nd words that are strongly associated.PMI values range from negative to positive in\ufb01nity. But negative PMI values(which imply things are co-occurringless oftenthan we would expect by chance)tend to be unreliable unless our corpora are enormous. To distinguish whethertwo words whose individual probability is each 10\u00006occur together less often thanchance, we would need to be certain that the probability of the two occurring to-gether is signi\ufb01cantly less than 10\u000012, and this kind of granularity would require anenormous corpus. Furthermore it\u2019s not clear whether it\u2019s even possible to evaluatesuch scores of \u2018unrelatedness\u2019 with human judgments. For this reason it is more4PMI is based on themutual informationbetween two random variablesXandY, de\ufb01ned",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 53,
      "token_count": 785,
      "chapter_title": ""
    }
  },
  {
    "content": "numerator tells us how often we observed the two words together (assumingwe compute probability by using the MLE). The denominator tells us how oftenwe wouldexpectthe two words to co-occur assuming they each occurred indepen-dently; recall that the probability of two independent events both occurring is justthe product of the probabilities of the two events. Thus, the ratio gives us an esti-mate of how much more the two words co-occur than we expect by chance. PMI isa useful tool whenever we need to \ufb01nd words that are strongly associated.PMI values range from negative to positive in\ufb01nity. But negative PMI values(which imply things are co-occurringless oftenthan we would expect by chance)tend to be unreliable unless our corpora are enormous. To distinguish whethertwo words whose individual probability is each 10\u00006occur together less often thanchance, we would need to be certain that the probability of the two occurring to-gether is signi\ufb01cantly less than 10\u000012, and this kind of granularity would require anenormous corpus. Furthermore it\u2019s not clear whether it\u2019s even possible to evaluatesuch scores of \u2018unrelatedness\u2019 with human judgments. For this reason it is more4PMI is based on themutual informationbetween two random variablesXandY, de\ufb01ned as:I(X,Y)=XxXyP(x,y)log2P(x,y)P(x)P(y)(6.15)In a confusion of terminology, Fano used the phrasemutual informationto refer to what we now callpointwise mutual informationand the phraseexpectation of the mutual informationfor what we now callmutual information",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 54,
      "token_count": 357,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 58\n\nVector Semantics & EmbeddingsTF-IDF\n\n## Page 59\n\nVector Semantics & EmbeddingsWord2vec\n\n## Page 60\n\nSparse versus dense vectorstf-idf(or PMI) vectors are\u25e6long(length |V|= 20,000 to 50,000)\u25e6sparse (most elements are zero)Alternative: learn vectors which are\u25e6short(length 50-1000)\u25e6dense(most elements are non-zero)\n\n## Page 61\n\nSparse versus dense vectorsWhy dense vectors?\u25e6Short vectors may be easier to use as featuresin machine learning (fewer weights to tune)\u25e6Dense vectors may generalizebetter than explicit counts\u25e6Dense vectors may do better at capturing synonymy:\u25e6carand automobileare synonyms; but are distinct dimensions\u25e6a word with caras a neighbor and a word with automobileas a neighbor should be similar, but aren't\u25e6In practice, they work better61\n\n## Page 62\n\nCommon methods for getting short dense vectors\u201cNeural Language Model\u201d-inspired models\u25e6Word2vec (skipgram, CBOW), GloVeSingular Value Decomposition (SVD)\u25e6A special case of this is called LSA \u2013Latent Semantic AnalysisAlternative to these \"static embeddings\":\u2022Contextual Embeddings (ELMo, BERT)\u2022Compute distinct embeddings for a word in its context\u2022Separate embeddings for each token of a word\n\n## Page 63\n\nSimple static embeddings you can download!Word2vec (Mikolovet al)https://code.google.com/archive/p/word2vec/GloVe(Pennington, Socher, Manning)http://nlp.stanford.edu/projects/glove/\n\n## Page 64\n\nWord2vecPopular embedding methodVery fast to trainCode available on the webIdea: predictrather than countWord2vec provides various options. We'll do:skip-gram with negative sampling (SGNS)\n\n## Page 65\n\nWord2vecInstead of countinghow often each word woccurs near \"apricot\"\u25e6Train a classifier on a binary predictiontask:\u25e6Is w likely to show up near \"apricot\"?We don\u2019t actually care about this task\u25e6But we'll take the learned classifier weights as the word embeddingsBig idea:  self-supervision: \u25e6A word c that occurs near apricot in the corpus cats as the gold \"correct answer\" for supervised learning\u25e6No need for human labels\u25e6Bengioet al. (2003); Collobertet al. (2011) \n\n## Page 66\n\nApproach: predict if candidate word cis a \"neighbor\"1.Treat the target word tand a neighboring context word cas positive examples.2.Randomly sample other words in the lexicon to get negative examples3.Use logistic regression to train a classifier to distinguish those two cases4.Use the learned weights as the embeddings\n\n## Page 67\n\nSkip-Gram Training DataAssume a +/-2 word window, given training sentence:\u2026lemon, a [tablespoon of  apricot  jam,   a]  pinch\u2026c1                   c2 c3      c4                                 [target]",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 55,
      "token_count": 673,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 65\n\nWord2vecInstead of countinghow often each word woccurs near \"apricot\"\u25e6Train a classifier on a binary predictiontask:\u25e6Is w likely to show up near \"apricot\"?We don\u2019t actually care about this task\u25e6But we'll take the learned classifier weights as the word embeddingsBig idea:  self-supervision: \u25e6A word c that occurs near apricot in the corpus cats as the gold \"correct answer\" for supervised learning\u25e6No need for human labels\u25e6Bengioet al. (2003); Collobertet al. (2011) \n\n## Page 66\n\nApproach: predict if candidate word cis a \"neighbor\"1.Treat the target word tand a neighboring context word cas positive examples.2.Randomly sample other words in the lexicon to get negative examples3.Use logistic regression to train a classifier to distinguish those two cases4.Use the learned weights as the embeddings\n\n## Page 67\n\nSkip-Gram Training DataAssume a +/-2 word window, given training sentence:\u2026lemon, a [tablespoon of  apricot  jam,   a]  pinch\u2026c1                   c2 c3      c4                                 [target]\n\n## Page 68\n\nSkip-Gram Classifier(assuming a +/-2 word window)\u2026lemon, a [tablespoon of  apricot  jam,   a]  pinch\u2026c1                   c2 [target]c3      c4Goal: train a classifier that is given a candidate (word, context) pair(apricot, jam)(apricot, aardvark)\u2026And assigns each pair a probability:P(+|w, c) P(\u2212|w, c) = 1 \u2212 P(+|w, c) \n\n## Page 69\n\nSimilarity is computed from dot productRemember: two vectors are similar if they have a high dot product\u25e6Cosine is just a normalized dot productSo:\u25e6Similarity(w,c)  \u221dw\u00b7 cWe\u2019ll need to normalize to get a probability \u25e6(cosine isn't a probability either)69",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 56,
      "token_count": 448,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 70",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 57,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Turning dot products into probabilitiesSim(w,c) \u2248 w\u00b7 cTo turn this into a probability We'll use the sigmoid from logistic regression:6.8\u2022WORD2VEC19We model the probability that wordcis a real context word for target wordwas:P(+|w,c)=s(c\u00b7w)=11+exp(\u0000c\u00b7w)(6.28)The sigmoid function returns a number between 0 and 1, but to make it a probabilitywe\u2019ll also need the total probability of the two possible events (cis a context word,andcisn\u2019t a context word) to sum to 1. We thus estimate the probability that wordcis not a real context word forwas:P(\u0000|w,c)=1\u0000P(+|w,c)=s(\u0000c\u00b7w)=11+exp(c\u00b7w)(6.29)Equation6.28gives us the probability for one word, but there are many contextwords in the window. Skip-gram makes the simplifying assumption that all contextwords are independent, allowing us to just multiply their probabilities:P(+|w,c1:L)=LYi=1s(\u0000ci\u00b7w)(6.30)logP(+|w,c1:L)=LXi=1logs(\u0000ci\u00b7w)(6.31)In summary, skip-gram trains a probabilistic classi\ufb01er that, given a test target wordwand its context window ofLwordsc1:L, assigns a probability based on how similarthis context window is to the target word. The probability is based on applying thelogistic (sigmoid) function to the dot product of the embeddings of the target wordwith each context word. To compute this probability, we just need embeddings foreach target word and context word in the vocabulary.1WCaardvark\nzebrazebraaardvarkapricotapricot|V||V|+12V& =target wordscontext & noisewords\u2026\n\u20261..d\u2026",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 58,
      "token_count": 409,
      "chapter_title": ""
    }
  },
  {
    "content": "zebrazebraaardvarkapricotapricot|V||V|+12V& =target wordscontext & noisewords\u2026\n\u20261..d\u2026\n\u2026Figure 6.13The embeddings learned by the skipgram model. The algorithm stores twoembeddings for each word, the target embedding (sometimes called the input embedding)and the context embedding (sometimes called the output embedding). The parameterqthatthe algorithm learns is thus a matrix of 2|V|vectors, each of dimensiond, formed by concate-nating two matrices, the target embeddingsWand the context+noise embeddingsC.Fig.6.13shows the intuition of the parameters we\u2019ll need. Skip-gram actuallystores two embeddings for each word, one for the word as a target, and one for the6.8\u2022WORD2VEC19We model the probability that wordcis a real context word for target wordwas:P(+|w,c)=s(c\u00b7w)=11+exp(\u0000c\u00b7w)(6.28)The sigmoid function returns a number between 0 and 1, but to make it a probabilitywe\u2019ll also need the total probability of the two possible events (cis a context word,andcisn\u2019t a context word) to sum to 1. We thus estimate the probability that wordcis not a real context word forwas:P(\u0000|w,c)=1\u0000P(+|w,c)=s(\u0000c\u00b7w)=11+exp(c\u00b7w)(6.29)Equation6.28gives us the probability for one word, but there are many contextwords in the window. Skip-gram makes the simplifying assumption that all contextwords are independent, allowing us to just multiply their probabilities:P(+|w,c1:L)=LYi=1s(\u0000ci\u00b7w)(6.30)logP(+|w,c1:L)=LXi=1logs(\u0000ci\u00b7w)(6.31)In summary, skip-gram trains a probabilistic classi\ufb01er that, given a test target wordwand its context window ofLwordsc1:L, assigns a probability based on how similarthis context window is to the target word. The probability is based on applying thelogistic (sigmoid) function to the dot product of the embeddings of the target wordwith each context word. To compute this probability, we just need embeddings foreach target word and context word in the vocabulary.1WCaardvark\nzebrazebraaardvarkapricotapricot|V||V|+12V& =target wordscontext & noisewords\u2026\n\u20261..d\u2026\n\u2026Figure 6.13The embeddings learned by the skipgram model. The algorithm stores twoembeddings for each word, the target embedding (sometimes called the input embedding)and the context embedding (sometimes called the output embedding). The parameterqthatthe algorithm learns is thus a matrix of 2|V|vectors, each of dimensiond, formed by concate-nating two matrices, the target embeddingsWand the context+noise embeddingsC.Fig.6.13shows the intuition of the parameters we\u2019ll need. Skip-gram actuallystores two embeddings for each word, one for the word as a target, and one for the",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 59,
      "token_count": 678,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 71",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 60,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "How Skip-Gram Classifier computes P(+|w, c) This is for one context word, but we have lots of context words.We'll assume independence and just multiply them:6.8\u2022WORD2VEC19We model the probability that wordcis a real context word for target wordwas:P(+|w,c)=s(c\u00b7w)=11+exp(\u0000c\u00b7w)(6.28)The sigmoid function returns a number between 0 and 1, but to make it a probabilitywe\u2019ll also need the total probability of the two possible events (cis a context word,andcisn\u2019t a context word) to sum to 1. We thus estimate the probability that wordcis not a real context word forwas:P(\u0000|w,c)=1\u0000P(+|w,c)=s(\u0000c\u00b7w)=11+exp(c\u00b7w)(6.29)Equation6.28gives us the probability for one word, but there are many contextwords in the window. Skip-gram makes the simplifying assumption that all contextwords are independent, allowing us to just multiply their probabilities:P(+|w,c1:L)=LYi=1s(\u0000ci\u00b7w)(6.30)logP(+|w,c1:L)=LXi=1logs(\u0000ci\u00b7w)(6.31)In summary, skip-gram trains a probabilistic classi\ufb01er that, given a test target wordwand its context window ofLwordsc1:L, assigns a probability based on how similarthis context window is to the target word. The probability is based on applying thelogistic (sigmoid) function to the dot product of the embeddings of the target wordwith each context word. To compute this probability, we just need embeddings foreach target word and context word in the vocabulary.1WCaardvark\nzebrazebraaardvarkapricotapricot|V||V|+12V& =target wordscontext & noisewords\u2026\n\u20261..d\u2026",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 61,
      "token_count": 416,
      "chapter_title": ""
    }
  },
  {
    "content": "zebrazebraaardvarkapricotapricot|V||V|+12V& =target wordscontext & noisewords\u2026\n\u20261..d\u2026\n\u2026Figure 6.13The embeddings learned by the skipgram model. The algorithm stores twoembeddings for each word, the target embedding (sometimes called the input embedding)and the context embedding (sometimes called the output embedding). The parameterqthatthe algorithm learns is thus a matrix of 2|V|vectors, each of dimensiond, formed by concate-nating two matrices, the target embeddingsWand the context+noise embeddingsC.Fig.6.13shows the intuition of the parameters we\u2019ll need. Skip-gram actuallystores two embeddings for each word, one for the word as a target, and one for the6.8\u2022WORD2VEC19We model the probability that wordcis a real context word for target wordwas:P(+|w,c)=s(c\u00b7w)=11+exp(\u0000c\u00b7w)(6.28)The sigmoid function returns a number between 0 and 1, but to make it a probabilitywe\u2019ll also need the total probability of the two possible events (cis a context word,andcisn\u2019t a context word) to sum to 1. We thus estimate the probability that wordcis not a real context word forwas:P(\u0000|w,c)=1\u0000P(+|w,c)=s(\u0000c\u00b7w)=11+exp(c\u00b7w)(6.29)Equation6.28gives us the probability for one word, but there are many contextwords in the window. Skip-gram makes the simplifying assumption that all contextwords are independent, allowing us to just multiply their probabilities:P(+|w,c1:L)=LYi=1s(ci\u00b7w)(6.30)logP(+|w,c1:L)=LXi=1logs(ci\u00b7w)(6.31)In summary, skip-gram trains a probabilistic classi\ufb01er that, given a test target wordwand its context window ofLwordsc1:L, assigns a probability based on how similarthis context window is to the target word. The probability is based on applying thelogistic (sigmoid) function to the dot product of the embeddings of the target wordwith each context word. To compute this probability, we just need embeddings foreach target word and context word in the vocabulary.1WCaardvark\nzebrazebraaardvarkapricotapricot|V||V|+12V& =target wordscontext & noisewords\u2026\n\u20261..d\u2026\n\u2026Figure 6.13The embeddings learned by the skipgram model. The algorithm stores twoembeddings for each word, the target embedding (sometimes called the input embedding)and the context embedding (sometimes called the output embedding). The parameterqthatthe algorithm learns is thus a matrix of 2|V|vectors, each of dimensiond, formed by concate-nating two matrices, the target embeddingsWand the context+noise embeddingsC.Fig.6.13shows the intuition of the parameters we\u2019ll need. Skip-gram actuallystores two embeddings for each word, one for the word as a target, and one for the",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 62,
      "token_count": 674,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 72\n\nSkip-gram classifier: summaryA probabilistic classifier, given \u2022a test target word w \u2022its context window of L words c1:LEstimates probability that w occurs in this window based on similarity of w (embeddings) to c1:L(embeddings).To compute this, we just need embeddings for all the words.\n\n## Page 73\n\nThese embeddings we'll need: a set for w, a set for c1WCaardvark\nzebrazebraaardvarkapricotapricot|V||V|+12V& =target wordscontext & noisewords\u2026\n\u20261..d\u2026\n\u2026\n\n## Page 74\n\nVector Semantics & EmbeddingsWord2vec\n\n## Page 75\n\nVector Semantics & EmbeddingsWord2vec: Learning the embeddings",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 63,
      "token_count": 171,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 73\n\nThese embeddings we'll need: a set for w, a set for c1WCaardvark\nzebrazebraaardvarkapricotapricot|V||V|+12V& =target wordscontext & noisewords\u2026\n\u20261..d\u2026\n\u2026\n\n## Page 74\n\nVector Semantics & EmbeddingsWord2vec\n\n## Page 75\n\nVector Semantics & EmbeddingsWord2vec: Learning the embeddings\n\n## Page 76\n\nSkip-Gram Training data\u2026lemon, a [tablespoon of  apricot  jam,   a]  pinch\u2026c1                   c2 [target]c3      c4\n7620CHAPTER6\u2022VECTORSEMANTICS ANDEMBEDDINGSthis context window is to the target word. The probability is based on applying thelogistic (sigmoid) function to the dot product of the embeddings of the target wordwith each context word. We could thus compute this probability if only we hadembeddings for each target word and context word in the vocabulary. Let\u2019s now turnto learning these embeddings (which is the real goal of training this classi\ufb01er in the\ufb01rst place).6.8.2 Learning skip-gram embeddingsWord2vec learns embeddings by starting with an initial set of embedding vectorsand then iteratively shifting the embedding of each wordwto be more like the em-beddings of words that occur nearby in texts, and less like the embeddings of wordsthat don\u2019t occur nearby. Let\u2019s start by considering a single piece of training data:... lemon, a [tablespoon of apricot jam, a] pinch ...c1 c2 t c3 c4This example has a target wordt(apricot), and 4 context words in theL=\u00b12window, resulting in 4 positive training instances (on the left below):positive examples +tcapricot tablespoonapricot ofapricot jamapricot anegative examples -tc tcapricot aardvark apricot sevenapricot my apricot foreverapricot where apricot dearapricot coaxial apricot ifFor training a binary classi\ufb01er we also need negative examples. In fact skip-gram uses more negative examples than positive examples (with the ratio betweenthem set by a parameterk). So for each of these(t,c)training instances we\u2019ll createknegative samples, each consisting of the targettplus a \u2018noise word\u2019. A noise wordis a random word from the lexicon, constrained not to be the target wordt. Theright above shows the setting wherek=2, so we\u2019ll have 2 negative examples in thenegative training set\u0000for each positive examplet,c.The noise words are chosen according to their weighted unigram frequencypa(w), whereais a weight. If we were sampling according to unweighted fre-quencyp(w), it would mean that with unigram probabilityp(\u201cthe\u201d)we would choosethe wordtheas a noise word, with unigram probabilityp(\u201caardvark\u201d)we wouldchooseaardvark, and so on. But in practice it is common to seta=.75, i.e. use theweightingp34(w):Pa(w)=count(w)aPw0count(w0)a(6.32)Settinga=.75 gives better performance because it gives rare noise words slightlyhigher probability: for rare words,Pa(w)>P(w). To visualize this intuition, itmight help to work out the probabilities for an example with two events,P(a)=.99andP(b)=.01:Pa(a)=.99.75.99.75+.01.75=.97Pa(b)=.01.75.99.75+.01.75=.03(6.33)",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 64,
      "token_count": 790,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 77\n\nSkip-Gram Training data\u2026lemon, a [tablespoon of  apricot  jam,   a]  pinch\u2026c1                   c2 [target]c3      c4\n77For each positive example we'll grab k negative examples, sampling by frequency20CHAPTER6\u2022VECTORSEMANTICS ANDEMBEDDINGSthis context window is to the target word. The probability is based on applying thelogistic (sigmoid) function to the dot product of the embeddings of the target wordwith each context word. We could thus compute this probability if only we hadembeddings for each target word and context word in the vocabulary. Let\u2019s now turnto learning these embeddings (which is the real goal of training this classi\ufb01er in the\ufb01rst place).6.8.2 Learning skip-gram embeddingsWord2vec learns embeddings by starting with an initial set of embedding vectorsand then iteratively shifting the embedding of each wordwto be more like the em-beddings of words that occur nearby in texts, and less like the embeddings of wordsthat don\u2019t occur nearby. Let\u2019s start by considering a single piece of training data:... lemon, a [tablespoon of apricot jam, a] pinch ...c1 c2 t c3 c4This example has a target wordt(apricot), and 4 context words in theL=\u00b12window, resulting in 4 positive training instances (on the left below):positive examples +tcapricot tablespoonapricot ofapricot jamapricot anegative examples -tc tcapricot aardvark apricot sevenapricot my apricot foreverapricot where apricot dearapricot coaxial apricot ifFor training a binary classi\ufb01er we also need negative examples. In fact skip-gram uses more negative examples than positive examples (with the ratio betweenthem set by a parameterk). So for each of these(t,c)training instances we\u2019ll createknegative samples, each consisting of the targettplus a \u2018noise word\u2019. A noise wordis a random word from the lexicon, constrained not to be the target wordt. Theright above shows the setting wherek=2, so we\u2019ll have 2 negative examples in thenegative training set\u0000for each positive examplet,c.The noise words are chosen according to their weighted unigram frequencypa(w), whereais a weight. If we were sampling according to unweighted fre-quencyp(w), it would mean that with unigram probabilityp(\u201cthe\u201d)we would choosethe wordtheas a noise word, with unigram probabilityp(\u201caardvark\u201d)we wouldchooseaardvark, and so on. But in practice it is common to seta=.75, i.e. use theweightingp34(w):Pa(w)=count(w)aPw0count(w0)a(6.32)Settinga=.75 gives better performance because it gives rare noise words slightlyhigher probability: for rare words,Pa(w)>P(w). To visualize this intuition, itmight help to work out the probabilities for an example with two events,P(a)=.99andP(b)=.01:Pa(a)=.99.75.99.75+.01.75=.97Pa(b)=.01.75.99.75+.01.75=.03(6.33)",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 65,
      "token_count": 704,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 78",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 66,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Skip-Gram Training data\u2026lemon, a [tablespoon of  apricot  jam,   a]  pinch\u2026c1                   c2 [target]c3      c4",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 67,
      "token_count": 40,
      "chapter_title": ""
    }
  },
  {
    "content": "7820CHAPTER6\u2022VECTORSEMANTICS ANDEMBEDDINGSthis context window is to the target word. The probability is based on applying thelogistic (sigmoid) function to the dot product of the embeddings of the target wordwith each context word. We could thus compute this probability if only we hadembeddings for each target word and context word in the vocabulary. Let\u2019s now turnto learning these embeddings (which is the real goal of training this classi\ufb01er in the\ufb01rst place).6.8.2 Learning skip-gram embeddingsWord2vec learns embeddings by starting with an initial set of embedding vectorsand then iteratively shifting the embedding of each wordwto be more like the em-beddings of words that occur nearby in texts, and less like the embeddings of wordsthat don\u2019t occur nearby. Let\u2019s start by considering a single piece of training data:... lemon, a [tablespoon of apricot jam, a] pinch ...c1 c2 t c3 c4This example has a target wordt(apricot), and 4 context words in theL=\u00b12window, resulting in 4 positive training instances (on the left below):positive examples +tcapricot tablespoonapricot ofapricot jamapricot anegative examples -tc tcapricot aardvark apricot sevenapricot my apricot foreverapricot where apricot dearapricot coaxial apricot ifFor training a binary classi\ufb01er we also need negative examples. In fact skip-gram uses more negative examples than positive examples (with the ratio betweenthem set by a parameterk). So for each of these(t,c)training instances we\u2019ll createknegative samples, each consisting of the targettplus a \u2018noise word\u2019. A noise wordis a random word from the lexicon, constrained not to be the target wordt. Theright above shows the setting wherek=2, so we\u2019ll have 2 negative examples in thenegative training set\u0000for each positive examplet,c.The noise words are chosen according to their weighted unigram frequencypa(w), whereais a weight. If we were sampling according to unweighted fre-quencyp(w), it would mean that with unigram probabilityp(\u201cthe\u201d)we would choosethe wordtheas a noise word, with unigram probabilityp(\u201caardvark\u201d)we wouldchooseaardvark, and so on. But in practice it is common to seta=.75, i.e. use theweightingp34(w):Pa(w)=count(w)aPw0count(w0)a(6.32)Settinga=.75 gives better performance because it gives rare noise words slightlyhigher probability: for rare words,Pa(w)>P(w). To visualize this intuition, itmight help to work out the probabilities for an example with two events,P(a)=.99andP(b)=.01:Pa(a)=.99.75.99.75+.01.75=.97Pa(b)=.01.75.99.75+.01.75=.03(6.33)20CHAPTER6\u2022VECTORSEMANTICS ANDEMBEDDINGSthis context window is to the target word. The probability is based on applying thelogistic (sigmoid) function to the dot product of the embeddings of the target wordwith each context word. We could thus compute this probability if only we hadembeddings for each target word and context word in the vocabulary. Let\u2019s now turnto learning these embeddings (which is the real goal of training this classi\ufb01er in the\ufb01rst place).6.8.2 Learning skip-gram embeddingsWord2vec learns embeddings by starting with an initial set of embedding vectorsand then iteratively shifting the embedding of each wordwto be more like the em-beddings of words that occur nearby in texts, and",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 68,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "and so on. But in practice it is common to seta=.75, i.e. use theweightingp34(w):Pa(w)=count(w)aPw0count(w0)a(6.32)Settinga=.75 gives better performance because it gives rare noise words slightlyhigher probability: for rare words,Pa(w)>P(w). To visualize this intuition, itmight help to work out the probabilities for an example with two events,P(a)=.99andP(b)=.01:Pa(a)=.99.75.99.75+.01.75=.97Pa(b)=.01.75.99.75+.01.75=.03(6.33)20CHAPTER6\u2022VECTORSEMANTICS ANDEMBEDDINGSthis context window is to the target word. The probability is based on applying thelogistic (sigmoid) function to the dot product of the embeddings of the target wordwith each context word. We could thus compute this probability if only we hadembeddings for each target word and context word in the vocabulary. Let\u2019s now turnto learning these embeddings (which is the real goal of training this classi\ufb01er in the\ufb01rst place).6.8.2 Learning skip-gram embeddingsWord2vec learns embeddings by starting with an initial set of embedding vectorsand then iteratively shifting the embedding of each wordwto be more like the em-beddings of words that occur nearby in texts, and less like the embeddings of wordsthat don\u2019t occur nearby. Let\u2019s start by considering a single piece of training data:... lemon, a [tablespoon of apricot jam, a] pinch ...c1 c2 t c3 c4This example has a target wordt(apricot), and 4 context words in theL=\u00b12window, resulting in 4 positive training instances (on the left below):positive examples +tcapricot tablespoonapricot ofapricot jamapricot anegative examples -tc tcapricot aardvark apricot sevenapricot my apricot foreverapricot where apricot dearapricot coaxial apricot ifFor training a binary classi\ufb01er we also need negative examples. In fact skip-gram uses more negative examples than positive examples (with the ratio betweenthem set by a parameterk). So for each of these(t,c)training instances we\u2019ll createknegative samples, each consisting of the targettplus a \u2018noise word\u2019. A noise wordis a random word from the lexicon, constrained not to be the target wordt. Theright above shows the setting wherek=2, so we\u2019ll have 2 negative examples in thenegative training set\u0000for each positive examplet,c.The noise words are chosen according to their weighted unigram frequencypa(w), whereais a weight. If we were sampling according to unweighted fre-quencyp(w), it would mean that with unigram probabilityp(\u201cthe\u201d)we would choosethe wordtheas a noise word, with unigram probabilityp(\u201caardvark\u201d)we wouldchooseaardvark, and so on. But in practice it is common to seta=.75, i.e. use theweightingp34(w):Pa(w)=count(w)aPw0count(w0)a(6.32)Settinga=.75 gives better performance because it gives rare noise words slightlyhigher probability: for rare words,Pa(w)>P(w). To visualize this intuition, itmight help to work out the probabilities for an example with two events,P(a)=.99andP(b)=.01:Pa(a)=.99.75.99.75+.01.75=.97Pa(b)=.01.75.99.75+.01.75=.03(6.33)",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 69,
      "token_count": 785,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 79\n\nWord2vec: how to learn vectorsGiven the set of positive and negative training instances, and an initial set of embedding vectors The goal of learning is to adjust those word vectors such that we:\u25e6Maximizethe similarity of the target word, context wordpairs (w , cpos) drawn from the positive data\u25e6Minimizethe similarity of the (w , cneg) pairs drawn from the negative data. 2/3/2479\n\n## Page 80\n\nLoss function for one w with cpos, cneg1...cnegkMaximize the similarity of the target with the actual context words, and minimize the similarity of the target with the k negative sampled non-neighbor words. 6.8\u2022WORD2VEC21Given the set of positive and negative training instances, and an initial set of embed-dings, the goal of the learning algorithm is to adjust those embeddings to\u2022Maximize the similarity of the target word, context word pairs(w,cpos)drawnfrom the positive examples\u2022Minimize the similarity of the(w,cneg)pairs from the negative examples.If we consider one word/context pair(w,cpos)with itsknoise wordscneg1...cnegk,we can express these two goals as the following loss functionLto be minimized(hence the\u0000); here the \ufb01rst term expresses that we want the classi\ufb01er to assign thereal context wordcposa high probability of being a neighbor, and the second termexpresses that we want to assign each of the noise wordscnegia high probability ofbeing a non-neighbor, all multiplied because we assume independence:LCE=\u0000log\"P(+|w,cpos)kYi=1P(\u0000|w,cnegi)#=\u0000\"logP(+|w,cpos)+kXi=1logP(\u0000|w,cnegi)#=\u0000\"logP(+|w,cpos)+kXi=1log\u00001\u0000P(+|w,cnegi)\u0000#=\u0000\"logs(cpos\u00b7w)+kXi=1logs(\u0000cnegi\u00b7w)#(6.34)That is, we want to maximize the dot product of the word with the actual contextwords, and minimize the dot products of the word with theknegative sampled non-neighbor words.We minimize this loss function using stochastic gradient descent. Fig.6.14shows the intuition of one step of learning.WCmove apricot and jam closer,increasing cpos z waardvark\nmove apricot and matrix apartdecreasing cneg1 z w\u201c\u2026apricot jam\u2026\u201dw\nzebrazebraaardvarkjamapricot\ncposmatrixTolstoymove apricot and Tolstoy apartdecreasing cneg2 z w!cneg1cneg2k=2Figure 6.14Intuition of one step of gradient descent. The skip-gram model tries to shiftembeddings so the target embeddings (here forapricot) are closer to (have a higher dot prod-uct with) context embeddings for nearby words (herejam) and further from (lower dot productwith) context embeddings for noise words that don\u2019t occur nearby (hereTolstoyandmatrix).To get the gradient, we need to take the derivative of Eq.6.34with respect tothe different embeddings. It turns out the derivatives are the following (we leave the\n\n## Page 81\n\nLearning the classifierHow to learn?\u25e6Stochastic gradient descent!We\u2019ll adjust the word weights to\u25e6make the positive pairs more likely \u25e6and the negative pairs less likely, \u25e6over the entire training set.",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 70,
      "token_count": 766,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 81\n\nLearning the classifierHow to learn?\u25e6Stochastic gradient descent!We\u2019ll adjust the word weights to\u25e6make the positive pairs more likely \u25e6and the negative pairs less likely, \u25e6over the entire training set.\n\n## Page 82\n\nIntuition of one step of gradient descentWCmove apricot and jam closer,increasing cpos z waardvark\nmove apricot and matrix apartdecreasing cneg1 z w\u201c\u2026apricot jam\u2026\u201dw\nzebrazebraaardvarkjamapricot\ncposmatrixTolstoymove apricot and Tolstoy apartdecreasing cneg2 z w!cneg1cneg2k=2\n\n## Page 83\n\nReminder: gradient descent\u2022At each step\u2022Direction: We move in the reverse direction from the gradient of the loss function\u2022Magnitude: we move the value of this gradient !!\"\ud835\udc3f(\ud835\udc53\ud835\udc65;\ud835\udc64,\ud835\udc66)weighted by a learning rate \u03b7 \u2022Higher learning rate means move wfaster10CHAPTER5\u2022LOGISTICREGRESSIONexample):wt+1=wt\u0000hddwL(f(x;w),y)(5.14)Now let\u2019s extend the intuition from a function of one scalar variablewto manyvariables, because we don\u2019t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If we\u2019re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.\nCost(w,b)\nwbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially we\u2019reasking: \u201cHow much would a small change in that variablewiin\ufb02uence the total lossfunctionL?\u201dIn each dimensionwi, we express the slope as a partial derivative\u2202\u2202wiof the lossfunction. The gradient is then de\ufb01ned as a vector of these partials. We\u2019ll represent \u02c6yasf(x;q)to make the dependence onqmore obvious:\u2014qL(f(x;q),y)) =266664\u2202\u2202w1L(f(x;q),y)\u2202\u2202w2L(f(x;q),y)...\u2202\u2202wnL(f(x;q),y)377775(5.15)The \ufb01nal equation for updatingqbased on the gradient is thusqt+1=qt\u0000h\u2014L(f(x;q),y)(5.16)",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 71,
      "token_count": 692,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 84",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 72,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "The derivatives of the loss function22CHAPTER6\u2022VECTORSEMANTICS ANDEMBEDDINGSproof as an exercise at the end of the chapter):\u2202LCE\u2202cpos=[s(cpos\u00b7w)\u00001]w(6.35)\u2202LCE\u2202cneg=[s(cneg\u00b7w)]w(6.36)\u2202LCE\u2202w=[s(cpos\u00b7w)\u00001]cpos+kXi=1[s(cnegi\u00b7w)]cnegi(6.37)The update equations going from time stepttot+1 in stochastic gradient descentare thus:ct+1pos=ctpos\u0000h[s(ctpos\u00b7w)\u00001]w(6.38)ct+1neg=ctneg\u0000h[s(ctneg\u00b7w)]w(6.39)wt+1=wt\u0000h[s(cpos\u00b7wt)\u00001]cpos+kXi=1[s(cnegi\u00b7wt)]cnegi(6.40)Just as in logistic regression, then, the learning algorithm starts with randomly ini-tializedWandCmatrices, and then walks through the training corpus using gradientdescent to moveWandCso as to maximize the objective in Eq.6.34by making theupdates in (Eq.6.39)-(Eq.6.40).Recall that the skip-gram model learnstwoseparate embeddings for each wordi:thetarget embeddingwiand thecontext embeddingci, stored in two matrices, thetargetembeddingcontextembeddingtarget matrixWand thecontext matrixC. It\u2019s common to just add them together,representing wordiwith the vectorwi+ci. Alternatively we can throw away theCmatrix and just represent each wordiby the vectorwi.As with the simple count-based methods like tf-idf, the context window sizeLaffects the performance of skip-gram embeddings, and experiments often tune theparameterLon a devset.6.8.3 Other kinds of static embeddingsThere are many kinds of static embeddings. An extension of word2vec,fasttextfasttext(Bojanowski et al., 2017), deals with unknown words and sparsity in languages withrich morphology, by using subword models. Each word in fasttext is represented asitself plus a bag of constituent n-grams, with special boundary symbols<and>added to each word. For example, withn=3 the wordwherewould be representedby the sequence<where>plus the character n-grams:<wh, whe, her, ere, re>Then a skipgram embedding is learned for each constituent n-gram, and the wordwhereis represented by the sum of all of the embeddings of its constituent n-grams.A fasttext open-source library, including pretrained embeddings for 157 languages,is available athttps://fasttext.cc.The most widely used static embedding model besides word2vec is GloVe(Pen-nington et al., 2014), short for Global Vectors, because the model is based on cap-turing global corpus statistics. GloVe is based on ratios of probabilities from theword-word co-occurrence matrix, combining the intuitions of count-based modelslike PPMI while also capturing the linear structures used by methods like word2vec.6.8\u2022WORD2VEC21Given the set of positive and negative training instances, and an initial set of embed-dings, the goal of the learning algorithm is to adjust those embeddings to\u2022Maximize the similarity of the target word, context word pairs(w,cpos)drawnfrom the positive examples\u2022Minimize the similarity of the(w,cneg)pairs from the negative examples.If we consider one word/context pair(w,cpos)with itsknoise wordscneg1...cnegk,we can express these two goals as the following loss functionLto be",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 73,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "example, withn=3 the wordwherewould be representedby the sequence<where>plus the character n-grams:<wh, whe, her, ere, re>Then a skipgram embedding is learned for each constituent n-gram, and the wordwhereis represented by the sum of all of the embeddings of its constituent n-grams.A fasttext open-source library, including pretrained embeddings for 157 languages,is available athttps://fasttext.cc.The most widely used static embedding model besides word2vec is GloVe(Pen-nington et al., 2014), short for Global Vectors, because the model is based on cap-turing global corpus statistics. GloVe is based on ratios of probabilities from theword-word co-occurrence matrix, combining the intuitions of count-based modelslike PPMI while also capturing the linear structures used by methods like word2vec.6.8\u2022WORD2VEC21Given the set of positive and negative training instances, and an initial set of embed-dings, the goal of the learning algorithm is to adjust those embeddings to\u2022Maximize the similarity of the target word, context word pairs(w,cpos)drawnfrom the positive examples\u2022Minimize the similarity of the(w,cneg)pairs from the negative examples.If we consider one word/context pair(w,cpos)with itsknoise wordscneg1...cnegk,we can express these two goals as the following loss functionLto be minimized(hence the\u0000); here the \ufb01rst term expresses that we want the classi\ufb01er to assign thereal context wordcposa high probability of being a neighbor, and the second termexpresses that we want to assign each of the noise wordscnegia high probability ofbeing a non-neighbor, all multiplied because we assume independence:LCE=\u0000log\"P(+|w,cpos)kYi=1P(\u0000|w,cnegi)#=\u0000\"logP(+|w,cpos)+kXi=1logP(\u0000|w,cnegi)#=\u0000\"logP(+|w,cpos)+kXi=1log\u00001\u0000P(+|w,cnegi)\u0000#=\u0000\"logs(cpos\u00b7w)+kXi=1logs(\u0000cnegi\u00b7w)#(6.34)That is, we want to maximize the dot product of the word with the actual contextwords, and minimize the dot products of the word with theknegative sampled non-neighbor words.We minimize this loss function using stochastic gradient descent. Fig.6.14shows the intuition of one step of learning.WCmove apricot and jam closer,increasing cpos z waardvark",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 74,
      "token_count": 555,
      "chapter_title": ""
    }
  },
  {
    "content": "move apricot and matrix apartdecreasing cneg1 z w\u201c\u2026apricot jam\u2026\u201dw\nzebrazebraaardvarkjamapricot\ncposmatrixTolstoymove apricot and Tolstoy apartdecreasing cneg2 z w!cneg1cneg2k=2Figure 6.14Intuition of one step of gradient descent. The skip-gram model tries to shiftembeddings so the target embeddings (here forapricot) are closer to (have a higher dot prod-uct with) context embeddings for nearby words (herejam) and further from (lower dot productwith) context embeddings for noise words that don\u2019t occur nearby (hereTolstoyandmatrix).To get the gradient, we need to take the derivative of Eq.6.34with respect tothe different embeddings. It turns out the derivatives are the following (we leave the6.8\u2022WORD2VEC21Given the set of positive and negative training instances, and an initial set of embed-dings, the goal of the learning algorithm is to adjust those embeddings to\u2022Maximize the similarity of the target word, context word pairs(w,cpos)drawnfrom the positive examples\u2022Minimize the similarity of the(w,cneg)pairs from the negative examples.If we consider one word/context pair(w,cpos)with itsknoise wordscneg1...cnegk,we can express these two goals as the following loss functionLto be minimized(hence the\u0000); here the \ufb01rst term expresses that we want the classi\ufb01er to assign thereal context wordcposa high probability of being a neighbor, and the second termexpresses that we want to assign each of the noise wordscnegia high probability ofbeing a non-neighbor, all multiplied because we assume independence:LCE=\u0000log\"P(+|w,cpos)kYi=1P(\u0000|w,cnegi)#=\u0000\"logP(+|w,cpos)+kXi=1logP(\u0000|w,cnegi)#=\u0000\"logP(+|w,cpos)+kXi=1log\u00001\u0000P(+|w,cnegi)\u0000#=\u0000\"logs(cpos\u00b7w)+kXi=1logs(\u0000cnegi\u00b7w)#(6.34)That is, we want to maximize the dot product of the word with the actual contextwords, and minimize the dot products of the word with theknegative sampled non-neighbor words.We minimize this loss function using stochastic gradient descent. Fig.6.14shows the intuition of one step of learning.WCmove apricot and jam closer,increasing cpos z waardvark\nmove apricot and matrix apartdecreasing cneg1 z w\u201c\u2026apricot jam\u2026\u201dw\nzebrazebraaardvarkjamapricotcposmatrixTolstoymove apricot and Tolstoy apartdecreasing cneg2 z w!cneg1cneg2k=2Figure 6.14Intuition of one step of gradient descent. The skip-gram model tries to shiftembeddings so the target embeddings (here forapricot) are closer to (have a higher dot prod-uct with) context embeddings for nearby words (herejam) and further from (lower dot productwith) context embeddings for noise words that don\u2019t occur nearby (hereTolstoyandmatrix).To get the gradient, we need to take the derivative of Eq.6.34with respect tothe different embeddings. It turns out the derivatives are the following (we leave the",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 75,
      "token_count": 751,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 85\n\nUpdate equation in SGD22CHAPTER6\u2022VECTORSEMANTICS ANDEMBEDDINGSproof as an exercise at the end of the chapter):\u2202LCE\u2202cpos=[s(cpos\u00b7w)\u00001]w(6.35)\u2202LCE\u2202cneg=[s(cneg\u00b7w)]w(6.36)\u2202LCE\u2202w=[s(cpos\u00b7w)\u00001]cpos+kXi=1[s(cnegi\u00b7w)]cnegi(6.37)The update equations going from time stepttot+1 in stochastic gradient descentare thus:ct+1pos=ctpos\u0000h[s(ctpos\u00b7wt)\u00001]wt(6.38)ct+1neg=ctneg\u0000h[s(ctneg\u00b7wt)]wt(6.39)wt+1=wt\u0000h\"[s(cpos\u00b7wt)\u00001]cpos+kXi=1[s(cnegi\u00b7wt)]cnegi#(6.40)Just as in logistic regression, then, the learning algorithm starts with randomly ini-tializedWandCmatrices, and then walks through the training corpus using gradientdescent to moveWandCso as to maximize the objective in Eq.6.34by making theupdates in (Eq.6.39)-(Eq.6.40).Recall that the skip-gram model learnstwoseparate embeddings for each wordi:thetarget embeddingwiand thecontext embeddingci, stored in two matrices, thetargetembeddingcontextembeddingtarget matrixWand thecontext matrixC. It\u2019s common to just add them together,representing wordiwith the vectorwi+ci. Alternatively we can throw away theCmatrix and just represent each wordiby the vectorwi.As with the simple count-based methods like tf-idf, the context window sizeLaffects the performance of skip-gram embeddings, and experiments often tune theparameterLon a devset.6.8.3 Other kinds of static embeddingsThere are many kinds of static embeddings. An extension of word2vec,fasttextfasttext(Bojanowski et al., 2017), deals with unknown words and sparsity in languages withrich morphology, by using subword models. Each word in fasttext is represented asitself plus a bag of constituent n-grams, with special boundary symbols<and>added to each word. For example, withn=3 the wordwherewould be representedby the sequence<where>plus the character n-grams:<wh, whe, her, ere, re>Then a skipgram embedding is learned for each constituent n-gram, and the wordwhereis represented by the sum of all of the embeddings of its constituent n-grams.A fasttext open-source library, including pretrained embeddings for 157 languages,is available athttps://fasttext.cc.The most widely used static embedding model besides word2vec is GloVe(Pen-nington et al., 2014), short for Global Vectors, because the model is based on cap-turing global corpus statistics. GloVe is based on ratios of probabilities from theword-word co-occurrence matrix, combining the intuitions of count-based modelslike PPMI while also capturing the linear structures used by methods like word2vec.Start with randomly initialized C and W matrices, then incrementally do updates\n\n## Page 86\n\nTwo sets of embeddingsSGNS learns two sets of embeddingsTarget embeddings matrix WContext embedding matrix C It's common to just add them together, representing word ias the vector  wi+ ci",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 76,
      "token_count": 743,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 86\n\nTwo sets of embeddingsSGNS learns two sets of embeddingsTarget embeddings matrix WContext embedding matrix C It's common to just add them together, representing word ias the vector  wi+ ci\n\n## Page 87\n\nSummary: How to learn word2vec (skip-gram) embeddingsStart with V random d-dimensional vectors as initial embeddingsTrain a classifier based on embedding similarity\u25e6Take a corpus and take pairs of words that co-occur as positive examples\u25e6Take pairs of words that don't co-occur as negative examples\u25e6Train the classifier to distinguish these by slowly adjusting all the embeddingsto improve the classifier performance\u25e6Throw away the classifier code and keep the embeddings.\n\n## Page 88\n\nVector Semantics & EmbeddingsWord2vec: Learning the embeddings\n\n## Page 89\n\nVector Semantics & EmbeddingsProperties of Embeddings\n\n## Page 90\n\nThe kinds of neighbors depend on window sizeSmall windows (C= +/-2): nearest words are syntactically similar words in same taxonomy\u25e6Hogwartsnearest neighbors are other fictional schools\u25e6Sunnydale, Evernight, BlandingsLarge windows (C= +/-5):  nearest words are related words in same semantic field\u25e6Hogwartsnearest neighbors are Harry Potter world:\u25e6Dumbledore, half-blood,  Malfoy",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 77,
      "token_count": 276,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 91",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 78,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "24CHAPTER6\u2022VECTORSEMANTICS ANDEMBEDDINGSFor exampleLevy and Goldberg (2014a)showed that using skip-gram with awindow of\u00b12, the most similar words to the wordHogwarts(from theHarry Potterseries) were names of other \ufb01ctional schools:Sunnydale(fromBuffy the VampireSlayer) orEvernight(from a vampire series). With a window of\u00b15, the most similarwords toHogwartswere other words topically related to theHarry Potterseries:Dumbledore,Malfoy, andhalf-blood.It\u2019s also often useful to distinguish two kinds of similarity or association betweenwords(Sch\u00a8utze and Pedersen, 1993). Two words have\ufb01rst-order co-occurrence\ufb01rst-orderco-occurrence(sometimes calledsyntagmatic association) if they are typically nearby each other.Thuswroteis a \ufb01rst-order associate ofbookorpoem. Two words havesecond-orderco-occurrence(sometimes calledparadigmatic association) if they have similarsecond-orderco-occurrenceneighbors. Thuswroteis a second-order associate of words likesaidorremarked.Analogy/Relational Similarity:Another semantic property of embeddings is theirability to capture relational meanings. In an important early vector space model ofcognition,Rumelhart and Abrahamson (1973)proposed theparallelogram modelparallelogrammodelfor solving simple analogy problems of the forma is to b as a* is to what?. In suchproblems, a system given a problem likeapple:tree::grape:?, i.e.,apple is to tree asgrape is to, and must \ufb01ll in the wordvine. In the parallelogram model, illus-trated in Fig.6.15, the vector from the wordappleto the wordtree(=#       \u00bbapple\u0000#   \u00bbtree)is added to the vector forgrape(#        \u00bbgrape); the nearest word to that point is returned.treeapplegrapevineFigure 6.15The parallelogram model for analogy problems(Rumelhart and Abrahamson,1973): the location of#     \u00bbvine can be found by subtracting#   \u00bbtree from#       \u00bbapple and adding#       \u00bbgrape.In early work with sparse embeddings, scholars showed that sparse vector mod-els of meaning could solve such analogy problems(Turney and Littman, 2005), butthe parallelogram method received more modern attention because of its successwith word2vec or GloVe vectors (Mikolov et al. 2013b,Levy and Goldberg 2014b,Pennington et al. 2014). For example, the result of the expression (#     \u00bbking)\u0000#     \u00bbman+#            \u00bbwoman is a vector close to#         \u00bbqueen. Similarly,#      \u00bbParis\u0000#           \u00bbFrance+#     \u00bbItaly) results in avector that is close to#         \u00bbRome. The embedding model thus seems to be extracting rep-resentations of relations likeMALE-FEMALE, orCAPITAL-CITY-OF, or evenCOM-PARATIVE/SUPERLATIVE, as shown in Fig.6.16from GloVe.For aa:b::a*:b*problem, meaning the algorithm is givena, b,anda*and must\ufb01ndb*, the parallelogram method is",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 79,
      "token_count": 713,
      "chapter_title": ""
    }
  },
  {
    "content": "\u00bbapple and adding#       \u00bbgrape.In early work with sparse embeddings, scholars showed that sparse vector mod-els of meaning could solve such analogy problems(Turney and Littman, 2005), butthe parallelogram method received more modern attention because of its successwith word2vec or GloVe vectors (Mikolov et al. 2013b,Levy and Goldberg 2014b,Pennington et al. 2014). For example, the result of the expression (#     \u00bbking)\u0000#     \u00bbman+#            \u00bbwoman is a vector close to#         \u00bbqueen. Similarly,#      \u00bbParis\u0000#           \u00bbFrance+#     \u00bbItaly) results in avector that is close to#         \u00bbRome. The embedding model thus seems to be extracting rep-resentations of relations likeMALE-FEMALE, orCAPITAL-CITY-OF, or evenCOM-PARATIVE/SUPERLATIVE, as shown in Fig.6.16from GloVe.For aa:b::a*:b*problem, meaning the algorithm is givena, b,anda*and must\ufb01ndb*, the parallelogram method is thus:\u02c6b\u21e4=argmaxxdistance(x,a\u21e4\u0000a+b)(6.41)with the distance function de\ufb01ned either as cosine or as Euclidean distance.There are some caveats. For example, the closest value returned by the paral-lelogram algorithm in word2vec or GloVe embedding spaces is usually not in factb* but one of the 3 input words or their morphological variants (i.e.,cherry:red ::Analogical relationsThe classic parallelogram model of analogical reasoning (Rumelhartand Abrahamson 1973)To solve: \"apple is to tree as grape is to  _____\"Add tree \u2013apple  to grape to get vine",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 80,
      "token_count": 393,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 92",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 81,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Analogical relations via parallelogramThe parallelogram method can solve analogies with both sparse and dense embeddings (Turney and Littman 2005, Mikolovet al. 2013b)king \u2013man + woman is close to queenParis \u2013France + Italy is close to RomeFor a problem a:a*::b:b*, the parallelogram method is:24CHAPTER6\u2022VECTORSEMANTICS ANDEMBEDDINGSFor exampleLevy and Goldberg (2014a)showed that using skip-gram with awindow of\u00b12, the most similar words to the wordHogwarts(from theHarry Potterseries) were names of other \ufb01ctional schools:Sunnydale(fromBuffy the VampireSlayer) orEvernight(from a vampire series). With a window of\u00b15, the most similarwords toHogwartswere other words topically related to theHarry Potterseries:Dumbledore,Malfoy, andhalf-blood.It\u2019s also often useful to distinguish two kinds of similarity or association betweenwords(Sch\u00a8utze and Pedersen, 1993). Two words have\ufb01rst-order co-occurrence\ufb01rst-orderco-occurrence(sometimes calledsyntagmatic association) if they are typically nearby each other.Thuswroteis a \ufb01rst-order associate ofbookorpoem. Two words havesecond-orderco-occurrence(sometimes calledparadigmatic association) if they have similarsecond-orderco-occurrenceneighbors. Thuswroteis a second-order associate of words likesaidorremarked.Analogy/Relational Similarity:Another semantic property of embeddings is theirability to capture relational meanings. In an important early vector space model ofcognition,Rumelhart and Abrahamson (1973)proposed theparallelogram modelparallelogrammodelfor solving simple analogy problems of the forma is to b as a* is to what?. In suchproblems, a system given a problem likeapple:tree::grape:?, i.e.,apple is to tree asgrape is to, and must \ufb01ll in the wordvine. In the parallelogram model, illus-trated in Fig.6.15, the vector from the wordappleto the wordtree(=#       \u00bbapple\u0000#   \u00bbtree)is added to the vector forgrape(#        \u00bbgrape); the nearest word to that point is returned.treeapplegrapevineFigure 6.15The parallelogram model for analogy problems(Rumelhart and Abrahamson,1973): the location of#     \u00bbvine can be found by subtracting#   \u00bbtree from#       \u00bbapple and adding#       \u00bbgrape.In early work with sparse embeddings, scholars showed that sparse vector mod-els of meaning could solve such analogy problems(Turney and Littman, 2005), butthe parallelogram method received more modern attention because of its successwith word2vec or GloVe vectors (Mikolov et al. 2013b,Levy and Goldberg 2014b,Pennington et al. 2014). For example, the result of the expression (#     \u00bbking)\u0000#     \u00bbman+#            \u00bbwoman is a vector close to#         \u00bbqueen. Similarly,#      \u00bbParis\u0000#           \u00bbFrance+#     \u00bbItaly) results in avector that is close to#         \u00bbRome. The embedding model thus seems to be extracting rep-resentations of relations",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 82,
      "token_count": 723,
      "chapter_title": ""
    }
  },
  {
    "content": "\u00bbgrape); the nearest word to that point is returned.treeapplegrapevineFigure 6.15The parallelogram model for analogy problems(Rumelhart and Abrahamson,1973): the location of#     \u00bbvine can be found by subtracting#   \u00bbtree from#       \u00bbapple and adding#       \u00bbgrape.In early work with sparse embeddings, scholars showed that sparse vector mod-els of meaning could solve such analogy problems(Turney and Littman, 2005), butthe parallelogram method received more modern attention because of its successwith word2vec or GloVe vectors (Mikolov et al. 2013b,Levy and Goldberg 2014b,Pennington et al. 2014). For example, the result of the expression (#     \u00bbking)\u0000#     \u00bbman+#            \u00bbwoman is a vector close to#         \u00bbqueen. Similarly,#      \u00bbParis\u0000#           \u00bbFrance+#     \u00bbItaly) results in avector that is close to#         \u00bbRome. The embedding model thus seems to be extracting rep-resentations of relations likeMALE-FEMALE, orCAPITAL-CITY-OF, or evenCOM-PARATIVE/SUPERLATIVE, as shown in Fig.6.16from GloVe.For aa:b::a*:b*problem, meaning the algorithm is givena, b,anda*and must\ufb01ndb*, the parallelogram method is thus:\u02c6b\u21e4=argmaxxdistance(x,a\u21e4\u0000a+b)(6.41)with the distance function de\ufb01ned either as cosine or as Euclidean distance.There are some caveats. For example, the closest value returned by the paral-lelogram algorithm in word2vec or GloVe embedding spaces is usually not in factb* but one of the 3 input words or their morphological variants (i.e.,cherry:red ::",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 83,
      "token_count": 403,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 93\n\nStructure in GloVE Embedding space\n\n## Page 94\n\nCaveats with the parallelogram methodIt only seems to work for frequent words, small distances and certain relations (relating countries to capitals, or parts of speech), but not others. (Linzen2016, Gladkovaet al. 2016, Ethayarajhet al. 2019a) Understanding analogy is an open area of research (Peterson et al. 2020)\n\n## Page 95\n\nTrain embeddings on different decades of historical text to see meanings shift~30 million books, 1850-1990, Google Books dataEmbeddings as a window onto historical semantics\nWilliam L. Hamilton, Jure Leskovec, and Dan Jurafsky. 2016. Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change. Proceedings of ACL.\n\n## Page 96\n\nEmbeddings reflect cultural bias!Ask \u201cParis : France :: Tokyo : x\u201d \u25e6x = JapanAsk \u201cfather : doctor :: mother : x\u201d \u25e6x = nurseAsk \u201cman : computer programmer :: woman : x\u201d \u25e6x = homemakerBolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. \"Man is to computer programmer as woman is to homemaker? debiasing word embeddings.\" InNeurIPS, pp. 4349-4357. 2016.\nAlgorithms that use embeddings as part of e.g., hiring searches for programmers, might lead to bias in hiring\n\n## Page 97\n\nHistorical embedding as a tool to study cultural biases\u2022Compute a gender or ethnic bias for each adjective: e.g., how much closer the adjective is to \"woman\" synonyms than \"man\" synonyms, or names of particular ethnicities\u2022Embeddings for competence adjective (smart, wise, brilliant, resourceful, thoughtful, logical) are biased toward men, a bias slowly decreasing 1960-1990\u2022Embeddings for dehumanizing adjectives (barbaric, monstrous, bizarre)  were biased toward Asians in the 1930s, bias decreasing over the 20th century.\u2022These match the results of old surveys done in the 1930sGarg, N., Schiebinger, L., Jurafsky, D., and Zou, J. (2018). Word embeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences 115(16), E3635\u2013E3644.\n\n## Page 98\n\nVector Semantics & EmbeddingsProperties of Embeddings",
    "metadata": {
      "source": "vectorsemantics2024",
      "chunk_id": 84,
      "token_count": 551,
      "chapter_title": ""
    }
  }
]