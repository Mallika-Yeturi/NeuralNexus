[
  {
    "content": "# 8_POSNER_intro_May_6_2021\n\n## Page 1\n\nSequence Labeling for Part of Speech and Named EntitiesPart of Speech Tagging\n\n## Page 2\n\nParts of SpeechFrom the earliest linguistic traditions (Yaskaand Panini 5thC. BCE, Aristotle 4thC. BCE), the idea that words can be classified into grammatical categories\u2022part of speech, word classes, POS, POS tags8 parts of speech attributed to Dionysius Thraxof Alexandria (c. 1stC. BCE): \u2022noun, verb, pronoun, preposition, adverb, conjunction, participle, article \u2022These categories are relevant for NLP today.\n\n## Page 3\n\nTwo classes of words: Open vs. ClosedClosed class words\u2022Relatively fixed membership\u2022Usually functionwords: short, frequent words with grammatical function\u2022determiners: a, an, the\u2022pronouns: she, he, I\u2022prepositions: on, under, over, near, by, \u2026Open class words\u2022Usually contentwords: Nouns, Verbs, Adjectives, Adverbs\u2022Plus interjections: oh, ouch, uh-huh, yes, hello\u2022New nouns and verbs like iPhone or to fax\n\n## Page 4\n\nOpen class (\"content\") words\nClosed class (\"function\")Nouns\nVerbsProperCommonAuxiliaryMainAdjectivesAdverbsPrepositionsParticlesDeterminersConjunctionsPronouns\u2026 more\u2026 moreJanetItalycat,  catsmangoeatwentcanhadold   green   tastyslowly yesterday\nto withoff   upthe someand orthey itsNumbers122,312oneInterjectionsOw  hello\n\n## Page 5\n\nPart-of-Speech TaggingAssigning a part-of-speech to each word in a text. Words often have more than one POS. book:\u2022VERB: (Bookthat flight) \u2022NOUN: (Hand me that book).",
    "metadata": {
      "source": "8_POSNER_intro_May_6_2021",
      "chunk_id": 0,
      "token_count": 411,
      "chapter_title": "8_POSNER_intro_May_6_2021"
    }
  },
  {
    "content": "## Page 5\n\nPart-of-Speech TaggingAssigning a part-of-speech to each word in a text. Words often have more than one POS. book:\u2022VERB: (Bookthat flight) \u2022NOUN: (Hand me that book).\n\n## Page 6\n\nPart-of-Speech Tagging8.2\u2022PART-OF-SPEECHTAGGING5\nwillNOUNAUXVERBDETNOUNJanetbackthebillPart of Speech Taggerx1x2x3x4x5y1y2y3y4y5\nFigure 8.3The task of part-of-speech tagging: mapping from input wordsx1,x2,. . . ,xntooutput POS tagsy1,y2,. . . ,yn.thought thatyour \ufb02ight was earlier). The goal of POS-tagging is toresolvetheseambiguityresolutionambiguities, choosing the proper tag for the context.Theaccuracyof part-of-speech tagging algorithms (the percentage of test setaccuracytags that match human gold labels) is extremely high. One study found accuraciesover 97% across 15 languages from the Universal Dependency (UD) treebank(Wuand Dredze, 2019). Accuracies on various English treebanks are also 97% (no matterthe algorithm; HMMs, CRFs, BERT perform similarly). This 97% number is alsoabout the human performance on this task, at least for English(Manning, 2011).Types: WSJ BrownUnambiguous(1 tag) 44,432 (86%) 45,799 (85%)Ambiguous(2+ tags) 7,025 (14%) 8,050 (15%)Tokens:Unambiguous(1 tag) 577,421 (45%) 384,349 (33%)Ambiguous(2+ tags) 711,780 (55%) 786,646 (67%)Figure 8.4Tag ambiguity in the Brown and WSJ corpora (Treebank-3 45-tag tagset).We\u2019ll introduce algorithms for the task in the next few sections, but \ufb01rst let\u2019sexplore the task. Exactly how hard is it? Fig.8.4shows that most word types(85-86%) are unambiguous (Janetis always NNP,hesitantlyis always RB). But theambiguous words, though accounting for only 14-15% of the vocabulary, are verycommon, and 55-67% of word tokens in running text are ambiguous. Particularlyambiguous common words includethat,back,down,putandset; here are someexamples of the 6 different parts of speech for the wordback:earnings growth took aback/JJseata small building in theback/NNa clear majority of senatorsback/VBPthe billDave began toback/VBtoward the doorenable the country to buyback/RPdebtI was twenty-oneback/RBthenNonetheless, many words are easy to disambiguate, because their different tagsaren\u2019t equally likely. For example,acan be a determiner or the lettera, but thedeterminer sense is much more likely.This idea suggests a usefulbaseline: given an ambiguous word, choose the tagwhich ismost frequentin the training corpus. This is a key concept:Most Frequent Class Baseline:Always compare a classi\ufb01er against a baseline atleast as good as the most frequent class baseline (assigning each token to the classit occurred in most often in the training set).Map from sequence x1,\u2026,xnof words to y1,\u2026,ynof POS tags",
    "metadata": {
      "source": "8_POSNER_intro_May_6_2021",
      "chunk_id": 1,
      "token_count": 761,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7",
    "metadata": {
      "source": "8_POSNER_intro_May_6_2021",
      "chunk_id": 2,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "\"Universal Dependencies\" Tagset2CHAPTER8\u2022SEQUENCELABELING FORPARTS OFSPEECH ANDNAMEDENTITIES8.1 (Mostly) English Word ClassesUntil now we have been using part-of-speech terms likenounandverbrather freely.In this section we give more complete de\ufb01nitions. While word classes do havesemantic tendencies\u2014adjectives, for example, often describepropertiesand nounspeople\u2014 parts of speech are de\ufb01ned instead based on their grammatical relationshipwith neighboring words or the morphological properties about their af\ufb01xes.Tag Description ExampleOpen ClassADJAdjective: noun modi\ufb01ers describing propertiesred,young,awesomeADVAdverb: verb modi\ufb01ers of time, place, mannervery,slowly,home,yesterdayNOUNwords for persons, places, things, etc.algorithm,cat,mango,beautyVERBwords for actions and processesdraw,provide,goPROPNProper noun: name of a person, organization, place, etc..Regina,IBM,ColoradoINTJInterjection: exclamation, greeting, yes/no response, etc.oh,um,yes,helloClosed Class WordsADPAdposition (Preposition/Postposition): marks a noun\u2019sspacial, temporal, or other relationin, on, by underAUXAuxiliary: helping verb marking tense, aspect, mood, etc.,can, may, should, areCCONJCoordinating Conjunction: joins two phrases/clausesand,or,butDETDeterminer: marks noun phrase propertiesa, an, the, thisNUMNumeralone, two, \ufb01rst, secondPARTParticle: a preposition-like form used together with a verbup, down, on, off, in, out, at, byPRONPronoun: a shorthand for referring to an entity or eventshe, who, I, othersSCONJSubordinating Conjunction: joins a main clause with asubordinate clause such as a sentential complementthat,whichOtherPUNCTPunctuation\u02d9, , ()SYMSymbols like $ or emoji $, %XOther asdf, qwfgFigure 8.1The 17 parts of speech in the Universal Dependencies tagset(Nivre et al., 2016a). Features canbe added to make \ufb01ner-grained distinctions (with properties like number, case, de\ufb01niteness, and so on).Parts of speech fall into two broad categories:closed classandopen class.closed classopen classClosed classes are those with relatively \ufb01xed membership, such as prepositions\u2014new prepositions are rarely coined. By contrast, nouns and verbs are open classes\u2014new nouns and verbs likeiPhoneorto faxare continually being created or borrowed.Closed class words are generallyfunction wordslikeof,it,and, oryou, which tendfunction wordto be very short, occur frequently, and often have structuring uses in grammar.Four major open classes occur in the languages of the world:nouns(includingproper nouns),verbs,adjectives, andadverbs, as well as the smaller open class ofinterjections. English has all \ufb01ve, although not every language does.Nounsare words for people, places, or things, but include others as well.Com-nounmon nounsinclude concrete terms likecatandmango, abstractions likealgorithmcommon nounandbeauty, and verb-like terms likepacingas inHis pacing to and fro became quiteannoying. Nouns in English can occur with determiners (a goat, its bandwidth)take possessives (IBM\u2019s annual revenue), and may occur in the plural (goats, abaci).Many languages, including English, divide common nouns intocount nounsandcount nounmass nouns. Count nouns can occur in the singular and plural (goat/goats, rela-mass",
    "metadata": {
      "source": "8_POSNER_intro_May_6_2021",
      "chunk_id": 3,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "de\ufb01niteness, and so on).Parts of speech fall into two broad categories:closed classandopen class.closed classopen classClosed classes are those with relatively \ufb01xed membership, such as prepositions\u2014new prepositions are rarely coined. By contrast, nouns and verbs are open classes\u2014new nouns and verbs likeiPhoneorto faxare continually being created or borrowed.Closed class words are generallyfunction wordslikeof,it,and, oryou, which tendfunction wordto be very short, occur frequently, and often have structuring uses in grammar.Four major open classes occur in the languages of the world:nouns(includingproper nouns),verbs,adjectives, andadverbs, as well as the smaller open class ofinterjections. English has all \ufb01ve, although not every language does.Nounsare words for people, places, or things, but include others as well.Com-nounmon nounsinclude concrete terms likecatandmango, abstractions likealgorithmcommon nounandbeauty, and verb-like terms likepacingas inHis pacing to and fro became quiteannoying. Nouns in English can occur with determiners (a goat, its bandwidth)take possessives (IBM\u2019s annual revenue), and may occur in the plural (goats, abaci).Many languages, including English, divide common nouns intocount nounsandcount nounmass nouns. Count nouns can occur in the singular and plural (goat/goats, rela-mass nountionship/relationships) and can be counted (one goat, two goats). Mass nouns areused when something is conceptualized as a homogeneous group. Sosnow, salt, andcommunismare not counted (i.e.,*two snowsor*two communisms).Proper nouns,proper nounlikeRegina,Colorado, andIBM, are names of speci\ufb01c persons or entities.Nivreet al. 2016",
    "metadata": {
      "source": "8_POSNER_intro_May_6_2021",
      "chunk_id": 4,
      "token_count": 393,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\nSample \"Tagged\" English sentencesThere/PROwere/VERB70/NUMchildren/NOUNthere/ADV./PUNCPreliminary/ADJfindings/NOUNwere/AUXreported/VERBin/ADPtoday/NOUN\u2019s/PARTNew/PROPNEngland/PROPNJournal/PROPNof/ADPMedicine/PROPN\n\n## Page 9\n\nWhy Part of Speech Tagging?\u25e6Can be useful for other NLP tasks\u25e6Parsing: POS tagging can improve syntactic parsing\u25e6MT: reordering of adjectives and nouns (say from Spanish to English)\u25e6Sentiment or affective tasks: may want to distinguish adjectives or other POS\u25e6Text-to-speech (how do we pronounce \u201clead\u201dor \"object\"?)\u25e6Or linguistic or language-analytic computational tasks\u25e6Need to control for POS when studying linguistic change like creation of new words, or meaning shift\u25e6Or control for POS in measuring meaning similarity or difference\n\n## Page 10\n\nHow difficult is POS tagging in English?Roughly 15% of word types are ambiguous\u2022Hence 85% of word types are unambiguous\u2022Janetis always PROPN, hesitantlyis always ADV But those 15% tend to be very common. So ~60% of word tokens are ambiguousE.g., backearnings growth took a back/ADJ seata small building in the back/NOUNa clear majority of senators back/VERB the bill enable the country to buy back/PART debtI was twenty-one back/ADV then \n\n## Page 11\n\nPOS tagging performance in EnglishHow many tags are correct?  (Tag accuracy)\u25e6About 97%\u25e6Hasn't changed in the last 10+ years\u25e6HMMs, CRFs, BERT perform similarly .\u25e6Human accuracy about the sameBut baseline is 92%!\u25e6Baseline is performance of stupidest possible method\u25e6\"Most frequent class baseline\" is an important baseline for many tasks\u25e6Tag every word with its most frequent tag\u25e6(and tag unknown words as nouns)\u25e6Partly easy because\u25e6Many words are unambiguous\n\n## Page 12\n\nSources of information for POS taggingJanet willback the billAUX/NOUN/VERB?           NOUN/VERB?Prior probabilities of word/tag\u2022\"will\" is usually an AUXIdentity of neighboring words\u2022\"the\" means the next word is probably not a verbMorphology and wordshape:\u25e6Prefixesunable: un-\u00aeADJ\u25e6Suffixesimportantly: -ly\u00aeADJ\u25e6CapitalizationJanet: CAP\u00aePROPN\n\n## Page 13\n\nStandard algorithms for POS taggingSupervised Machine Learning Algorithms:\u2022Hidden Markov Models\u2022Conditional Random Fields (CRF)/ Maximum Entropy Markov Models (MEMM)\u2022Neural sequence models (RNNs or Transformers)\u2022Large Language Models (like BERT), finetunedAll required a hand-labeled training set, all about equal performance (97% on English)All make use of information sources we discussed\u2022Via human created features: HMMs and CRFs\u2022Via representation learning:  Neural LMs\n\n## Page 14\n\nSequence Labeling for Part of Speech and Named EntitiesPart of Speech Tagging\n\n## Page 15\n\nSequence Labeling for Part of Speech and Named EntitiesNamed Entity Recognition (NER)",
    "metadata": {
      "source": "8_POSNER_intro_May_6_2021",
      "chunk_id": 5,
      "token_count": 716,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\nSources of information for POS taggingJanet willback the billAUX/NOUN/VERB?           NOUN/VERB?Prior probabilities of word/tag\u2022\"will\" is usually an AUXIdentity of neighboring words\u2022\"the\" means the next word is probably not a verbMorphology and wordshape:\u25e6Prefixesunable: un-\u00aeADJ\u25e6Suffixesimportantly: -ly\u00aeADJ\u25e6CapitalizationJanet: CAP\u00aePROPN\n\n## Page 13\n\nStandard algorithms for POS taggingSupervised Machine Learning Algorithms:\u2022Hidden Markov Models\u2022Conditional Random Fields (CRF)/ Maximum Entropy Markov Models (MEMM)\u2022Neural sequence models (RNNs or Transformers)\u2022Large Language Models (like BERT), finetunedAll required a hand-labeled training set, all about equal performance (97% on English)All make use of information sources we discussed\u2022Via human created features: HMMs and CRFs\u2022Via representation learning:  Neural LMs\n\n## Page 14\n\nSequence Labeling for Part of Speech and Named EntitiesPart of Speech Tagging\n\n## Page 15\n\nSequence Labeling for Part of Speech and Named EntitiesNamed Entity Recognition (NER)\n\n## Page 16\n\nNamed Entities\u25e6Named entity, in its core usage, means anything that can be referred to with a proper name. Most common 4 tags:\u25e6PER(Person): \u201cMarie Curie\u201d\u25e6LOC(Location): \u201cNew York City\u201d \u25e6ORG(Organization): \u201cStanford University\u201d\u25e6GPE(Geo-Political Entity): \"Boulder, Colorado\"\u25e6Often multi-word phrases\u25e6But the term is also extended to things that aren't entities:\u25e6dates, times, prices\n\n## Page 17\n\nNamed Entity taggingThe task of named entity recognition (NER):\u2022find spans of text that constitute proper names\u2022tag the type of the entity.",
    "metadata": {
      "source": "8_POSNER_intro_May_6_2021",
      "chunk_id": 6,
      "token_count": 405,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\nNamed Entity taggingThe task of named entity recognition (NER):\u2022find spans of text that constitute proper names\u2022tag the type of the entity. \n\n## Page 18\n\nNER output6CHAPTER8\u2022SEQUENCELABELING FORPARTS OFSPEECH ANDNAMEDENTITIESThe most-frequent-tag baseline has an accuracy of about 92%1. The baselinethus differs from the state-of-the-art and human ceiling (97%) by only 5%.8.3 Named Entities and Named Entity TaggingPart of speech tagging can tell us that words likeJanet,Stanford University, andColoradoare all proper nouns; being a proper noun is a grammatical property ofthese words. But viewed from a semantic perspective, these proper nouns refer todifferent kinds of entities: Janet is a person, Stanford University is an organization,..and Colorado is a location.Anamed entityis, roughly speaking, anything that can be referred to with anamed entityproper name: a person, a location, an organization. The task ofnamed entity recog-nition(NER) is to \ufb01nd spans of text that constitute proper names and tag the type ofnamed entityrecognitionNERthe entity. Four entity tags are most common:PER(person),LOC(location),ORG(organization), orGPE(geo-political entity). However, the termnamed entityiscommonly extended to include things that aren\u2019t entities per se, including dates,times, and other kinds of temporal expressions, and even numerical expressions likeprices. Here\u2019s an example of the output of an NER tagger:Citing high fuel prices,[ORGUnited Airlines]said[TIMEFriday]ithas increased fares by[MONEY$6]per round trip on \ufb02ights to somecities also served by lower-cost carriers.[ORGAmerican Airlines],aunit of[ORGAMR Corp.], immediately matched the move, spokesman[PERTim Wagner]said.[ORGUnited], a unit of[ORGUAL Corp.],said the increase took effect[TIMEThursday]and applies to mostroutes where it competes against discount carriers, such as[LOCChicago]to[LOCDallas]and[LOCDenver]to[LOCSan Francisco].The text contains 13 mentions of named entities including 5 organizations, 4 loca-tions, 2 times, 1 person, and 1 mention of money. Figure8.5shows typical genericnamed entity types. Many applications will also need to use speci\ufb01c entity types likeproteins, genes, commercial products, or works of art.Type Tag Sample Categories Example sentencesPeoplePERpeople, charactersTuringis a giant of computer science.OrganizationORGcompanies, sports teams TheIPCCwarned about the cyclone.LocationLOCregions, mountains, seasMt. Sanitasis inSunshine Canyon.Geo-Political EntityGPEcountries, statesPalo Altois raising the fees for parking.Figure 8.5A list of generic named entity types with the kinds of entities they refer to.Named entity tagging is a useful \ufb01rst step in lots of natural language understand-ing tasks. In sentiment analysis we might want to know a consumer\u2019s sentimenttoward a particular entity. Entities are a useful \ufb01rst stage in question answering,or for linking text to information in structured knowledge sources like Wikipedia.And named entity tagging is also central to natural language understanding tasksof building semantic representations, like extracting events and the relationship be-tween participants.Unlike part-of-speech tagging, where there is no segmentation problem sinceeach word gets one tag, the task of named entity recognition is to \ufb01nd and label1In English, on the WSJ corpus, tested on sections 22-24.",
    "metadata": {
      "source": "8_POSNER_intro_May_6_2021",
      "chunk_id": 7,
      "token_count": 767,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\nWhy NER?Sentiment analysis: consumer\u2019s sentiment toward a particular company or person?Question Answering: answer questions about an entity?Information Extraction: Extracting facts about entities from text.\n\n## Page 20\n\nWhy NER is hard1)Segmentation\u2022In POS tagging, no segmentation problem since each word gets one tag.\u2022In NER we have to find and segment the entities!2)Type ambiguity8.3\u2022NAMEDENTITIES ANDNAMEDENTITYTAGGING7spansof text, and is dif\ufb01cult partly because of the ambiguity of segmentation; weneed to decide what\u2019s an entity and what isn\u2019t, and where the boundaries are. Indeed,most words in a text will not be named entities. Another dif\ufb01culty is caused by typeambiguity. The mentionJFKcan refer to a person, the airport in New York, or anynumber of schools, bridges, and streets around the United States. Some examples ofthis kind of cross-type confusion are given in Figure8.6.[PERWashington] was born into slavery on the farm of James Burroughs.[ORGWashington] went up 2 games to 1 in the four-game series.Blair arrived in [LOCWashington] for what may well be his last state visit.In June, [GPEWashington] passed a primary seatbelt law.Figure 8.6Examples of type ambiguities in the use of the nameWashington.The standard approach to sequence labeling for a span-recognition problem likeNER isBIOtagging(Ramshaw and Marcus, 1995). This is a method that allows usto treat NER like a word-by-word sequence labeling task, via tags that capture boththe boundary and the named entity type. Consider the following sentence:[PERJane Villanueva]of[ORGUnited], a unit of[ORGUnited AirlinesHolding], said the fare applies to the[LOCChicago]route.Figure8.7shows the same excerpt represented withBIOtagging, as well asBIOvariants calledIOtagging andBIOEStagging. In BIO tagging we label any tokenthatbeginsa span of interest with the labelB, tokens that occurinsidea span aretagged with anI, and any tokens outside of any span of interest are labeledO. Whilethere is only oneOtag, we\u2019ll have distinctBandItags for each named entity class.The number of tags is thus 2n+1 tags, wherenis the number of entity types. BIOtagging can represent exactly the same information as the bracketed notation, but hasthe advantage that we can represent the task in the same simple sequence modelingway as part-of-speech tagging: assigning a single labelyito each input wordxi:WordsIO LabelBIO LabelBIOES LabelJaneI-PERB-PERB-PERVillanuevaI-PERI-PERE-PERofOOOUnitedI-ORGB-ORGB-ORGAirlinesI-ORGI-ORGI-ORGHoldingI-ORGI-ORGE-ORGdiscussedOOOtheOOOChicagoI-LOCB-LOCS-LOCrouteOOO.OOOFigure 8.7NER as a sequence model, showing IO, BIO, and BIOES taggings.We\u2019ve also shown two variant tagging schemes: IO tagging, which loses someinformation by eliminating the B tag, and BIOES tagging, which adds an end tagEfor the end of a span, and a span tagSfor a span consisting of only one word.A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label eachtoken in a text with tags that indicate the presence (or absence) of particular kindsof named entities.",
    "metadata": {
      "source": "8_POSNER_intro_May_6_2021",
      "chunk_id": 8,
      "token_count": 778,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21\n\nBIO TaggingHow can we turn this structured problem into a sequence problem like POS tagging, with one label per word?[PER Jane Villanueva] of [ORG United] , a unit of [ORG United Airlines Holding] , said the fare applies to the [LOC Chicago ] route.",
    "metadata": {
      "source": "8_POSNER_intro_May_6_2021",
      "chunk_id": 9,
      "token_count": 64,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22",
    "metadata": {
      "source": "8_POSNER_intro_May_6_2021",
      "chunk_id": 10,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIO Tagging[PER Jane Villanueva] of [ORG United] , a unit of [ORG United Airlines Holding] , said the fare applies to the [LOC Chicago ] route. 8.3\u2022NAMEDENTITIES ANDNAMEDENTITYTAGGING7spansof text, and is dif\ufb01cult partly because of the ambiguity of segmentation; weneed to decide what\u2019s an entity and what isn\u2019t, and where the boundaries are. Indeed,most words in a text will not be named entities. Another dif\ufb01culty is caused by typeambiguity. The mentionJFKcan refer to a person, the airport in New York, or anynumber of schools, bridges, and streets around the United States. Some examples ofthis kind of cross-type confusion are given in Figure8.6.[PERWashington] was born into slavery on the farm of James Burroughs.[ORGWashington] went up 2 games to 1 in the four-game series.Blair arrived in [LOCWashington] for what may well be his last state visit.In June, [GPEWashington] passed a primary seatbelt law.Figure 8.6Examples of type ambiguities in the use of the nameWashington.The standard approach to sequence labeling for a span-recognition problem likeNER isBIOtagging(Ramshaw and Marcus, 1995). This is a method that allows usto treat NER like a word-by-word sequence labeling task, via tags that capture boththe boundary and the named entity type. Consider the following sentence:[PERJane Villanueva]of[ORGUnited], a unit of[ORGUnited AirlinesHolding], said the fare applies to the[LOCChicago]route.Figure8.7shows the same excerpt represented withBIOtagging, as well asBIOvariants calledIOtagging andBIOEStagging. In BIO tagging we label any tokenthatbeginsa span of interest with the labelB, tokens that occurinsidea span aretagged with anI, and any tokens outside of any span of interest are labeledO. Whilethere is only oneOtag, we\u2019ll have distinctBandItags for each named entity class.The number of tags is thus 2n+1 tags, wherenis the number of entity types. BIOtagging can represent exactly the same information as the bracketed notation, but hasthe advantage that we can represent the task in the same simple sequence modelingway as part-of-speech tagging: assigning a single labelyito each input wordxi:WordsIO LabelBIO LabelBIOES LabelJaneI-PERB-PERB-PERVillanuevaI-PERI-PERE-PERofOOOUnitedI-ORGB-ORGB-ORGAirlinesI-ORGI-ORGI-ORGHoldingI-ORGI-ORGE-ORGdiscussedOOOtheOOOChicagoI-LOCB-LOCS-LOCrouteOOO.OOOFigure 8.7NER as a sequence model, showing IO, BIO, and BIOES taggings.We\u2019ve also shown two variant tagging schemes: IO tagging, which loses someinformation by eliminating the B tag, and BIOES tagging, which adds an end tagEfor the end of a span, and a span tagSfor a span consisting of only one word.A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label eachtoken in a text with tags that indicate the presence (or absence) of particular kindsof named entities.8.3\u2022NAMEDENTITIES ANDNAMEDENTITYTAGGING7spansof text, and is dif\ufb01cult partly because of the ambiguity of segmentation; weneed to decide what\u2019s an entity and what isn\u2019t, and where the boundaries are. Indeed,most words in a text will not be named entities. Another",
    "metadata": {
      "source": "8_POSNER_intro_May_6_2021",
      "chunk_id": 11,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "the same simple sequence modelingway as part-of-speech tagging: assigning a single labelyito each input wordxi:WordsIO LabelBIO LabelBIOES LabelJaneI-PERB-PERB-PERVillanuevaI-PERI-PERE-PERofOOOUnitedI-ORGB-ORGB-ORGAirlinesI-ORGI-ORGI-ORGHoldingI-ORGI-ORGE-ORGdiscussedOOOtheOOOChicagoI-LOCB-LOCS-LOCrouteOOO.OOOFigure 8.7NER as a sequence model, showing IO, BIO, and BIOES taggings.We\u2019ve also shown two variant tagging schemes: IO tagging, which loses someinformation by eliminating the B tag, and BIOES tagging, which adds an end tagEfor the end of a span, and a span tagSfor a span consisting of only one word.A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label eachtoken in a text with tags that indicate the presence (or absence) of particular kindsof named entities.8.3\u2022NAMEDENTITIES ANDNAMEDENTITYTAGGING7spansof text, and is dif\ufb01cult partly because of the ambiguity of segmentation; weneed to decide what\u2019s an entity and what isn\u2019t, and where the boundaries are. Indeed,most words in a text will not be named entities. Another dif\ufb01culty is caused by typeambiguity. The mentionJFKcan refer to a person, the airport in New York, or anynumber of schools, bridges, and streets around the United States. Some examples ofthis kind of cross-type confusion are given in Figure8.6.[PERWashington] was born into slavery on the farm of James Burroughs.[ORGWashington] went up 2 games to 1 in the four-game series.Blair arrived in [LOCWashington] for what may well be his last state visit.In June, [GPEWashington] passed a primary seatbelt law.Figure 8.6Examples of type ambiguities in the use of the nameWashington.The standard approach to sequence labeling for a span-recognition problem likeNER isBIOtagging(Ramshaw and Marcus, 1995). This is a method that allows usto treat NER like a word-by-word sequence labeling task, via tags that capture boththe boundary and the named entity type. Consider the following sentence:[PERJane Villanueva]of[ORGUnited], a unit of[ORGUnited AirlinesHolding], said the fare applies to the[LOCChicago]route.Figure8.7shows the same excerpt represented withBIOtagging, as well asBIOvariants calledIOtagging andBIOEStagging. In BIO tagging we label any tokenthatbeginsa span of interest with the labelB, tokens that occurinsidea span aretagged with anI, and any tokens outside of any span of interest are labeledO. Whilethere is only oneOtag, we\u2019ll have distinctBandItags for each named entity class.The number of tags is thus 2n+1 tags, wherenis the number of entity types. BIOtagging can represent exactly the same information as the bracketed notation, but hasthe advantage that we can represent the task in the same simple sequence modelingway as part-of-speech tagging: assigning a single labelyito each input wordxi:WordsIO LabelBIO LabelBIOES",
    "metadata": {
      "source": "8_POSNER_intro_May_6_2021",
      "chunk_id": 12,
      "token_count": 722,
      "chapter_title": ""
    }
  },
  {
    "content": "law.Figure 8.6Examples of type ambiguities in the use of the nameWashington.The standard approach to sequence labeling for a span-recognition problem likeNER isBIOtagging(Ramshaw and Marcus, 1995). This is a method that allows usto treat NER like a word-by-word sequence labeling task, via tags that capture boththe boundary and the named entity type. Consider the following sentence:[PERJane Villanueva]of[ORGUnited], a unit of[ORGUnited AirlinesHolding], said the fare applies to the[LOCChicago]route.Figure8.7shows the same excerpt represented withBIOtagging, as well asBIOvariants calledIOtagging andBIOEStagging. In BIO tagging we label any tokenthatbeginsa span of interest with the labelB, tokens that occurinsidea span aretagged with anI, and any tokens outside of any span of interest are labeledO. Whilethere is only oneOtag, we\u2019ll have distinctBandItags for each named entity class.The number of tags is thus 2n+1 tags, wherenis the number of entity types. BIOtagging can represent exactly the same information as the bracketed notation, but hasthe advantage that we can represent the task in the same simple sequence modelingway as part-of-speech tagging: assigning a single labelyito each input wordxi:WordsIO LabelBIO LabelBIOES LabelJaneI-PERB-PERB-PERVillanuevaI-PERI-PERE-PERofOOOUnitedI-ORGB-ORGB-ORGAirlinesI-ORGI-ORGI-ORGHoldingI-ORGI-ORGE-ORGdiscussedOOOtheOOOChicagoI-LOCB-LOCS-LOCrouteOOO.OOOFigure 8.7NER as a sequence model, showing IO, BIO, and BIOES taggings.We\u2019ve also shown two variant tagging schemes: IO tagging, which loses someinformation by eliminating the B tag, and BIOES tagging, which adds an end tagEfor the end of a span, and a span tagSfor a span consisting of only one word.A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label eachtoken in a text with tags that indicate the presence (or absence) of particular kindsof named entities.Now we have one tag per token!!!",
    "metadata": {
      "source": "8_POSNER_intro_May_6_2021",
      "chunk_id": 13,
      "token_count": 505,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23",
    "metadata": {
      "source": "8_POSNER_intro_May_6_2021",
      "chunk_id": 14,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIO TaggingB: token that begins a spanI: tokens inside a spanO: tokens outside of any span# of tags (where n is #entity types):1 O tag, nB tags, nI tagstotal of 2n+18.3\u2022NAMEDENTITIES ANDNAMEDENTITYTAGGING7spansof text, and is dif\ufb01cult partly because of the ambiguity of segmentation; weneed to decide what\u2019s an entity and what isn\u2019t, and where the boundaries are. Indeed,most words in a text will not be named entities. Another dif\ufb01culty is caused by typeambiguity. The mentionJFKcan refer to a person, the airport in New York, or anynumber of schools, bridges, and streets around the United States. Some examples ofthis kind of cross-type confusion are given in Figure8.6.[PERWashington] was born into slavery on the farm of James Burroughs.[ORGWashington] went up 2 games to 1 in the four-game series.Blair arrived in [LOCWashington] for what may well be his last state visit.In June, [GPEWashington] passed a primary seatbelt law.Figure 8.6Examples of type ambiguities in the use of the nameWashington.The standard approach to sequence labeling for a span-recognition problem likeNER isBIOtagging(Ramshaw and Marcus, 1995). This is a method that allows usto treat NER like a word-by-word sequence labeling task, via tags that capture boththe boundary and the named entity type. Consider the following sentence:[PERJane Villanueva]of[ORGUnited], a unit of[ORGUnited AirlinesHolding], said the fare applies to the[LOCChicago]route.Figure8.7shows the same excerpt represented withBIOtagging, as well asBIOvariants calledIOtagging andBIOEStagging. In BIO tagging we label any tokenthatbeginsa span of interest with the labelB, tokens that occurinsidea span aretagged with anI, and any tokens outside of any span of interest are labeledO. Whilethere is only oneOtag, we\u2019ll have distinctBandItags for each named entity class.The number of tags is thus 2n+1 tags, wherenis the number of entity types. BIOtagging can represent exactly the same information as the bracketed notation, but hasthe advantage that we can represent the task in the same simple sequence modelingway as part-of-speech tagging: assigning a single labelyito each input wordxi:WordsIO LabelBIO LabelBIOES LabelJaneI-PERB-PERB-PERVillanuevaI-PERI-PERE-PERofOOOUnitedI-ORGB-ORGB-ORGAirlinesI-ORGI-ORGI-ORGHoldingI-ORGI-ORGE-ORGdiscussedOOOtheOOOChicagoI-LOCB-LOCS-LOCrouteOOO.OOOFigure 8.7NER as a sequence model, showing IO, BIO, and BIOES taggings.We\u2019ve also shown two variant tagging schemes: IO tagging, which loses someinformation by eliminating the B tag, and BIOES tagging, which adds an end tagEfor the end of a span, and a span tagSfor a span consisting of only one word.A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label eachtoken in a text with tags that indicate the presence (or absence) of particular kindsof named entities.8.3\u2022NAMEDENTITIES ANDNAMEDENTITYTAGGING7spansof text, and is dif\ufb01cult partly because of the ambiguity of segmentation; weneed to decide what\u2019s an entity and what isn\u2019t, and where the boundaries are. Indeed,most",
    "metadata": {
      "source": "8_POSNER_intro_May_6_2021",
      "chunk_id": 15,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "but hasthe advantage that we can represent the task in the same simple sequence modelingway as part-of-speech tagging: assigning a single labelyito each input wordxi:WordsIO LabelBIO LabelBIOES LabelJaneI-PERB-PERB-PERVillanuevaI-PERI-PERE-PERofOOOUnitedI-ORGB-ORGB-ORGAirlinesI-ORGI-ORGI-ORGHoldingI-ORGI-ORGE-ORGdiscussedOOOtheOOOChicagoI-LOCB-LOCS-LOCrouteOOO.OOOFigure 8.7NER as a sequence model, showing IO, BIO, and BIOES taggings.We\u2019ve also shown two variant tagging schemes: IO tagging, which loses someinformation by eliminating the B tag, and BIOES tagging, which adds an end tagEfor the end of a span, and a span tagSfor a span consisting of only one word.A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label eachtoken in a text with tags that indicate the presence (or absence) of particular kindsof named entities.8.3\u2022NAMEDENTITIES ANDNAMEDENTITYTAGGING7spansof text, and is dif\ufb01cult partly because of the ambiguity of segmentation; weneed to decide what\u2019s an entity and what isn\u2019t, and where the boundaries are. Indeed,most words in a text will not be named entities. Another dif\ufb01culty is caused by typeambiguity. The mentionJFKcan refer to a person, the airport in New York, or anynumber of schools, bridges, and streets around the United States. Some examples ofthis kind of cross-type confusion are given in Figure8.6.[PERWashington] was born into slavery on the farm of James Burroughs.[ORGWashington] went up 2 games to 1 in the four-game series.Blair arrived in [LOCWashington] for what may well be his last state visit.In June, [GPEWashington] passed a primary seatbelt law.Figure 8.6Examples of type ambiguities in the use of the nameWashington.The standard approach to sequence labeling for a span-recognition problem likeNER isBIOtagging(Ramshaw and Marcus, 1995). This is a method that allows usto treat NER like a word-by-word sequence labeling task, via tags that capture boththe boundary and the named entity type. Consider the following sentence:[PERJane Villanueva]of[ORGUnited], a unit of[ORGUnited AirlinesHolding], said the fare applies to the[LOCChicago]route.Figure8.7shows the same excerpt represented withBIOtagging, as well asBIOvariants calledIOtagging andBIOEStagging. In BIO tagging we label any tokenthatbeginsa span of interest with the labelB, tokens that occurinsidea span aretagged with anI, and any tokens outside of any span of interest are labeledO. Whilethere is only oneOtag, we\u2019ll have distinctBandItags for each named entity class.The number of tags is thus 2n+1 tags, wherenis the number of entity types. BIOtagging can represent exactly the same information as the bracketed notation, but hasthe advantage that we can represent the task in the same simple sequence modelingway as part-of-speech tagging: assigning a single labelyito each input wordxi:WordsIO LabelBIO LabelBIOES",
    "metadata": {
      "source": "8_POSNER_intro_May_6_2021",
      "chunk_id": 16,
      "token_count": 733,
      "chapter_title": ""
    }
  },
  {
    "content": "law.Figure 8.6Examples of type ambiguities in the use of the nameWashington.The standard approach to sequence labeling for a span-recognition problem likeNER isBIOtagging(Ramshaw and Marcus, 1995). This is a method that allows usto treat NER like a word-by-word sequence labeling task, via tags that capture boththe boundary and the named entity type. Consider the following sentence:[PERJane Villanueva]of[ORGUnited], a unit of[ORGUnited AirlinesHolding], said the fare applies to the[LOCChicago]route.Figure8.7shows the same excerpt represented withBIOtagging, as well asBIOvariants calledIOtagging andBIOEStagging. In BIO tagging we label any tokenthatbeginsa span of interest with the labelB, tokens that occurinsidea span aretagged with anI, and any tokens outside of any span of interest are labeledO. Whilethere is only oneOtag, we\u2019ll have distinctBandItags for each named entity class.The number of tags is thus 2n+1 tags, wherenis the number of entity types. BIOtagging can represent exactly the same information as the bracketed notation, but hasthe advantage that we can represent the task in the same simple sequence modelingway as part-of-speech tagging: assigning a single labelyito each input wordxi:WordsIO LabelBIO LabelBIOES LabelJaneI-PERB-PERB-PERVillanuevaI-PERI-PERE-PERofOOOUnitedI-ORGB-ORGB-ORGAirlinesI-ORGI-ORGI-ORGHoldingI-ORGI-ORGE-ORGdiscussedOOOtheOOOChicagoI-LOCB-LOCS-LOCrouteOOO.OOOFigure 8.7NER as a sequence model, showing IO, BIO, and BIOES taggings.We\u2019ve also shown two variant tagging schemes: IO tagging, which loses someinformation by eliminating the B tag, and BIOES tagging, which adds an end tagEfor the end of a span, and a span tagSfor a span consisting of only one word.A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label eachtoken in a text with tags that indicate the presence (or absence) of particular kindsof named entities.",
    "metadata": {
      "source": "8_POSNER_intro_May_6_2021",
      "chunk_id": 17,
      "token_count": 498,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24\n\nBIO Tagging variants: IO and BIOES[PER Jane Villanueva] of [ORG United] , a unit of [ORG United Airlines Holding] , said the fare applies to the [LOC Chicago ] route. 8.3\u2022NAMEDENTITIES ANDNAMEDENTITYTAGGING7spansof text, and is dif\ufb01cult partly because of the ambiguity of segmentation; weneed to decide what\u2019s an entity and what isn\u2019t, and where the boundaries are. Indeed,most words in a text will not be named entities. Another dif\ufb01culty is caused by typeambiguity. The mentionJFKcan refer to a person, the airport in New York, or anynumber of schools, bridges, and streets around the United States. Some examples ofthis kind of cross-type confusion are given in Figure8.6.[PERWashington] was born into slavery on the farm of James Burroughs.[ORGWashington] went up 2 games to 1 in the four-game series.Blair arrived in [LOCWashington] for what may well be his last state visit.In June, [GPEWashington] passed a primary seatbelt law.Figure 8.6Examples of type ambiguities in the use of the nameWashington.The standard approach to sequence labeling for a span-recognition problem likeNER isBIOtagging(Ramshaw and Marcus, 1995). This is a method that allows usto treat NER like a word-by-word sequence labeling task, via tags that capture boththe boundary and the named entity type. Consider the following sentence:[PERJane Villanueva]of[ORGUnited], a unit of[ORGUnited AirlinesHolding], said the fare applies to the[LOCChicago]route.Figure8.7shows the same excerpt represented withBIOtagging, as well asBIOvariants calledIOtagging andBIOEStagging. In BIO tagging we label any tokenthatbeginsa span of interest with the labelB, tokens that occurinsidea span aretagged with anI, and any tokens outside of any span of interest are labeledO. Whilethere is only oneOtag, we\u2019ll have distinctBandItags for each named entity class.The number of tags is thus 2n+1 tags, wherenis the number of entity types. BIOtagging can represent exactly the same information as the bracketed notation, but hasthe advantage that we can represent the task in the same simple sequence modelingway as part-of-speech tagging: assigning a single labelyito each input wordxi:WordsIO LabelBIO LabelBIOES LabelJaneI-PERB-PERB-PERVillanuevaI-PERI-PERE-PERofOOOUnitedI-ORGB-ORGB-ORGAirlinesI-ORGI-ORGI-ORGHoldingI-ORGI-ORGE-ORGdiscussedOOOtheOOOChicagoI-LOCB-LOCS-LOCrouteOOO.OOOFigure 8.7NER as a sequence model, showing IO, BIO, and BIOES taggings.We\u2019ve also shown two variant tagging schemes: IO tagging, which loses someinformation by eliminating the B tag, and BIOES tagging, which adds an end tagEfor the end of a span, and a span tagSfor a span consisting of only one word.A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label eachtoken in a text with tags that indicate the presence (or absence) of particular kindsof named entities.",
    "metadata": {
      "source": "8_POSNER_intro_May_6_2021",
      "chunk_id": 18,
      "token_count": 740,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 25\n\nStandard algorithms for NERSupervised Machine Learning given a human-labeled training set of text annotated with tags\u2022Hidden Markov Models\u2022Conditional Random Fields (CRF)/ Maximum Entropy Markov Models (MEMM)\u2022Neural sequence models (RNNs or Transformers)\u2022Large Language Models (like BERT), finetuned\n\n## Page 26\n\nSequence Labeling for Part of Speech and Named EntitiesNamed Entity Recognition (NER)",
    "metadata": {
      "source": "8_POSNER_intro_May_6_2021",
      "chunk_id": 19,
      "token_count": 95,
      "chapter_title": ""
    }
  }
]