[
  {
    "content": "# 19\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n19Dependency Parsing\nTout mot qui fait partie d\u2019une phrase... Entre lui et ses voisins, l\u2019esprit aperc \u00b8oit\ndes connexions, dont l\u2019ensemble forme la charpente de la phrase.\n[Between each word in a sentence and its neighbors, the mind perceives con-\nnections . These connections together form the scaffolding of the sentence.]\nLucien Tesni `ere. 1959. \u00b4El\u00b4ements de syntaxe structurale, A.1.\u00a74\nThe focus of the last chapter was on context-free grammars and constituent-\nbased representations. Here we present another important family of grammar for-\nmalisms called dependency grammars . In dependency formalisms, phrasal con-dependency\ngrammars\nstituents and phrase-structure rules do not play a direct role. Instead, the syntactic\nstructure of a sentence is described solely in terms of directed binary grammatical\nrelations between the words , as in the following dependency parse:\nIprefer themorning \ufb02ight through Denvernsubjobj\ndet\ncompoundnmod\ncaseroot\n(19.1)\nRelations among the words are illustrated above the sentence with directed, labeled\narcs from heads todependents . We call this a typed dependency structure becausetyped\ndependency\nthe labels are drawn from a \ufb01xed inventory of grammatical relations. A root node\nexplicitly marks the root of the tree, the head of the entire structure.\nFigure 19.1 on the next page shows the dependency analysis from Eq. 19.1 but\nvisualized as a tree, alongside its corresponding phrase-structure analysis of the kind\ngiven in the prior chapter. Note the absence of nodes corresponding to phrasal con-\nstituents or lexical categories in the dependency parse; the internal structure of the\ndependency parse consists solely of directed relations between words. These head-\ndependent relationships directly encode important information that is often buried in\nthe more complex phrase-structure parses. For example, the arguments to the verb\nprefer are directly linked to it in the dependency structure, while their connection\nto the main verb is more distant in the phrase-structure tree. Similarly, morning\nandDenver , modi\ufb01ers of \ufb02ight , are linked to it directly in the dependency structure.\nThis fact that the head-dependent relations are a good proxy for the semantic rela-\ntionship between predicates and their arguments is an important reason why depen-\ndency grammars are currently more common than constituency grammars in natural\nlanguage processing.\nAnother major advantage of dependency grammars is their ability to deal with\nlanguages that have a relatively free word order . For example, word order in Czech free word order\ncan be much more \ufb02exible than in English; a grammatical object might occur before\nor after a location adverbial . A phrase-structure grammar would need a separate rule",
    "metadata": {
      "source": "19",
      "chunk_id": 0,
      "token_count": 650,
      "chapter_title": "19"
    }
  },
  {
    "content": "## Page 2\n\n2CHAPTER 19 \u2022 D EPENDENCY PARSING\nprefer\n\ufb02ight\nDenver\nthroughmorning theIS\nVP\nNP\nNom\nPP\nNP\nPro\nDenverP\nthroughNom\nNoun\n\ufb02ightNom\nNoun\nmorningDet\ntheVerb\npreferNP\nPro\nI\nFigure 19.1 Dependency and constituent analyses for I prefer the morning \ufb02ight through Denver.\nfor each possible place in the parse tree where such an adverbial phrase could occur.\nA dependency-based approach can have just one link type representing this particu-\nlar adverbial relation; dependency grammar approaches can thus abstract away a bit\nmore from word order information.\nIn the following sections, we\u2019ll give an inventory of relations used in dependency\nparsing, discuss two families of parsing algorithms (transition-based, and graph-\nbased), and discuss evaluation.\n19.1 Dependency Relations\nThe traditional linguistic notion of grammatical relation provides the basis for thegrammatical\nrelation\nbinary relations that comprise these dependency structures. The arguments to these\nrelations consist of a head and a dependent . The head plays the role of the central head\ndependent organizing word, and the dependent as a kind of modi\ufb01er. The head-dependent rela-\ntionship is made explicit by directly linking heads to the words that are immediately\ndependent on them.\nIn addition to specifying the head-dependent pairs, dependency grammars allow\nus to classify the kinds of grammatical relations, or grammatical function that thegrammatical\nfunction\ndependent plays with respect to its head. These include familiar notions such as\nsubject ,direct object andindirect object . In English these notions strongly corre-\nlate with, but by no means determine, both position in a sentence and constituent\ntype and are therefore somewhat redundant with the kind of information found in\nphrase-structure trees. However, in languages with more \ufb02exible word order, the\ninformation encoded directly in these grammatical relations is critical since phrase-\nbased constituent syntax provides little help.\nLinguists have developed taxonomies of relations that go well beyond the famil-\niar notions of subject and object. While there is considerable variation from theory",
    "metadata": {
      "source": "19",
      "chunk_id": 1,
      "token_count": 461,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\n19.1 \u2022 D EPENDENCY RELATIONS 3\nClausal Argument Relations Description\nNSUBJ Nominal subject\nOBJ Direct object\nIOBJ Indirect object\nCCOMP Clausal complement\nNominal Modi\ufb01er Relations Description\nNMOD Nominal modi\ufb01er\nAMOD Adjectival modi\ufb01er\nAPPOS Appositional modi\ufb01er\nDET Determiner\nCASE Prepositions, postpositions and other case markers\nOther Notable Relations Description\nCONJ Conjunct\nCC Coordinating conjunction\nFigure 19.2 Some of the Universal Dependency relations (de Marneffe et al., 2021).\nto theory, there is enough commonality that cross-linguistic standards have been\ndeveloped. The Universal Dependencies (UD) project (de Marneffe et al., 2021),Universal\nDependencies\nan open community effort to annotate dependencies and other aspects of grammar\nacross more than 100 languages, provides an inventory of 37 dependency relations.\nFig. 19.2 shows a subset of the UD relations and Fig. 19.3 provides some examples.\nThe motivation for all of the relations in the Universal Dependency scheme is\nbeyond the scope of this chapter, but the core set of frequently used relations can be\nbroken into two sets: clausal relations that describe syntactic roles with respect to a\npredicate (often a verb), and modi\ufb01er relations that categorize the ways that words\ncan modify their heads.\nConsider, for example, the following sentence:\nUnited canceled themorning \ufb02ights toHoustonnsubjobj\ndet\ncompoundnmod\ncaseroot\n(19.2)\nHere the clausal relations NSUBJ and OBJidentify the subject and direct object of\nthe predicate cancel , while the NMOD ,DET, and CASE relations denote modi\ufb01ers of\nthe nouns \ufb02ights andHouston .\n19.1.1 Dependency Formalisms\nA dependency structure can be represented as a directed graph G= (V;A), consisting\nof a set of vertices V, and a set of ordered pairs of vertices A, which we\u2019ll call arcs.\nFor the most part we will assume that the set of vertices, V, corresponds exactly\nto the set of words in a given sentence. However, they might also correspond to\npunctuation, or when dealing with morphologically complex languages the set of\nvertices might consist of stems and af\ufb01xes. The set of arcs, A, captures the head-\ndependent and grammatical function relationships between the elements in V.\nDifferent grammatical theories or formalisms may place further constraints on\nthese dependency structures. Among the more frequent restrictions are that the struc-\ntures must be connected, have a designated root node, and be acyclic or planar. Of\nmost relevance to the parsing approaches discussed in this chapter is the common,",
    "metadata": {
      "source": "19",
      "chunk_id": 2,
      "token_count": 604,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4CHAPTER 19 \u2022 D EPENDENCY PARSING\nRelation Examples with head anddependent\nNSUBJ United canceled the \ufb02ight.\nOBJ United diverted the\ufb02ight to Reno.\nWebooked her the \ufb01rst \ufb02ight to Miami.\nIOBJ Webooked herthe \ufb02ight to Miami.\nCOMPOUND We took the morning \ufb02ight .\nNMOD \ufb02ight toHouston .\nAMOD Book the cheapest \ufb02ight .\nAPPOS United , aunit of UAL, matched the fares.\nDET The \ufb02ight was canceled.\nWhich \ufb02ight was delayed?\nCONJ We\ufb02ewto Denver and drove to Steamboat.\nCC We \ufb02ew to Denver anddrove to Steamboat.\nCASE Book the \ufb02ight through Houston .\nFigure 19.3 Examples of some Universal Dependency relations.\ncomputationally-motivated, restriction to rooted trees. That is, a dependency treedependency\ntree\nis a directed graph that satis\ufb01es the following constraints:\n1. There is a single designated root node that has no incoming arcs.\n2. With the exception of the root node, each vertex has exactly one incoming arc.\n3. There is a unique path from the root node to each vertex in V.\nTaken together, these constraints ensure that each word has a single head, that the\ndependency structure is connected, and that there is a single root node from which\none can follow a unique directed path to each of the words in the sentence.\n19.1.2 Projectivity\nThe notion of projectivity imposes an additional constraint that is derived from the\norder of the words in the input. An arc from a head to a dependent is said to be\nprojective if there is a path from the head to every word that lies between the head projective\nand the dependent in the sentence. A dependency tree is then said to be projective if\nall the arcs that make it up are projective. All the dependency trees we\u2019ve seen thus\nfar have been projective. There are, however, many valid constructions which lead\nto non-projective trees, particularly in languages with relatively \ufb02exible word order.\nConsider the following example.\nJetBlue canceled our \ufb02ight this morning which was already latensubjobjobl\ndetacl:relcl\ndet nsubjcop\nadvroot\n(19.3)\nIn this example, the arc from \ufb02ight to its modi\ufb01er lateis non-projective since there\nis no path from \ufb02ight to the intervening words thisandmorning . As we can see from\nthis diagram, projectivity (and non-projectivity) can be detected in the way we\u2019ve\nbeen drawing our trees. A dependency tree is projective if it can be drawn with\nno crossing edges. Here there is no way to link \ufb02ight to its dependent latewithout\ncrossing the arc that links morning to its head.",
    "metadata": {
      "source": "19",
      "chunk_id": 3,
      "token_count": 630,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\n19.1 \u2022 D EPENDENCY RELATIONS 5\nOur concern with projectivity arises from two related issues. First, the most\nwidely used English dependency treebanks were automatically derived from phrase-\nstructure treebanks through the use of head-\ufb01nding rules. The trees generated in such\na fashion will always be projective, and hence will be incorrect when non-projective\nexamples like this one are encountered.\nSecond, there are computational limitations to the most widely used families of\nparsing algorithms. The transition-based approaches discussed in Section 19.2 can\nonly produce projective trees, hence any sentences with non-projective structures\nwill necessarily contain some errors. This limitation is one of the motivations for\nthe more \ufb02exible graph-based parsing approach described in Section 19.3.\n19.1.3 Dependency Treebanks\nTreebanks play a critical role in the development and evaluation of dependency\nparsers. They are used for training parsers, they act as the gold labels for evaluating\nparsers, and they also provide useful information for corpus linguistics studies.\nDependency treebanks are created by having human annotators directly generate\ndependency structures for a given corpus, or by hand-correcting the output of an\nautomatic parser. A few early treebanks were also based on using a deterministic\nprocess to translate existing constituent-based treebanks into dependency trees.\nThe largest open community project for building dependency trees is the Univer-\nsal Dependencies project at https://universaldependencies.org/ introduced\nabove, which currently has almost 200 dependency treebanks in more than 100 lan-\nguages (de Marneffe et al., 2021). Here are a few UD examples showing dependency\ntrees for sentences in Spanish, Basque, and Mandarin Chinese:\nVERB ADP DET NOUN ADP DET NUM PUNCT\nSubiremos a el tren a las cinco .\nwe-will-board on the train at the \ufb01ve .obl\ndetcase\ndetobl:tmod\ncasepunct\n[Spanish] Subiremos al tren a las cinco. \u201cWe will be boarding the train at \ufb01ve.\u201d (19.4)\nNOUN NOUN VERB AUX PUNCT\nEkaitzak itsasontzia hondoratu du .\nstorm (Erg.) ship (Abs.) sunk has .nsubj\nobj auxpunct\n[Basque] Ekaitzak itsasontzia hondoratu du. \u201cThe storm has sunk the ship.\u201d (19.5)",
    "metadata": {
      "source": "19",
      "chunk_id": 4,
      "token_count": 522,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6CHAPTER 19 \u2022 D EPENDENCY PARSING\nADV PRON NOUN ADV VERB VERB NOUN\n\u4f46\u6211\u6628\u5929 \u624d \u6536 \u5230\u4fe1\nbut I yesterday only-then receive arrive letter .adv\nnsubj\nobj:tmod\nadvmod compound:vvobj\n[Chinese] \u4f46\u6211\u6628\u5929\u624d\u6536\u5230\u4fe1\u201cBut I didn\u2019t receive the letter until yesterday\u201d (19.6)\n19.2 Transition-Based Dependency Parsing\nOur \ufb01rst approach to dependency parsing is called transition-based parsing. This transition-based\narchitecture draws on shift-reduce parsing , a paradigm originally developed for\nanalyzing programming languages (Aho and Ullman, 1972). In transition-based\nparsing we\u2019ll have a stack on which we build the parse, a buffer of tokens to be\nparsed, and a parser which takes actions on the parse via a predictor called an oracle ,\nas illustrated in Fig. 19.4.\nwnw1w2s2...s1snParserInput bu\ufb00erStackOracleLEFTARCRIGHTARCSHIFTActionDependencyRelationsw3w2\nFigure 19.4 Basic transition-based parser. The parser examines the top two elements of the\nstack and selects an action by consulting an oracle that examines the current con\ufb01guration.\nThe parser walks through the sentence left-to-right, successively shifting items\nfrom the buffer onto the stack. At each time point we examine the top two elements\non the stack, and the oracle makes a decision about what transition to apply to build\nthe parse. The possible transitions correspond to the intuitive actions one might take\nin creating a dependency tree by examining the words in a single pass over the input\nfrom left to right (Covington, 2001):\n\u2022 Assign the current word as the head of some previously seen word,\n\u2022 Assign some previously seen word as the head of the current word,\n\u2022 Postpone dealing with the current word, storing it for later processing.\nWe\u2019ll formalize this intuition with the following three transition operators that\nwill operate on the top two elements of the stack:\n\u2022LEFT ARC: Assert a head-dependent relation between the word at the top of\nthe stack and the second word; remove the second word from the stack.\n\u2022RIGHT ARC: Assert a head-dependent relation between the second word on\nthe stack and the word at the top; remove the top word from the stack;",
    "metadata": {
      "source": "19",
      "chunk_id": 5,
      "token_count": 511,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\n19.2 \u2022 T RANSITION -BASED DEPENDENCY PARSING 7\n\u2022SHIFT : Remove the word from the front of the input buffer and push it onto\nthe stack.\nWe\u2019ll sometimes call operations like LEFT ARCand RIGHT ARCreduce operations,\nbased on a metaphor from shift-reduce parsing, in which reducing means combin-\ning elements on the stack. There are some preconditions for using operators. The\nLEFT ARCoperator cannot be applied when ROOT is the second element of the stack\n(since by de\ufb01nition the ROOT node cannot have any incoming arcs). And both the\nLEFT ARCand RIGHT ARCoperators require two elements to be on the stack to be\napplied.\nThis particular set of operators implements what is known as the arc standard arc standard\napproach to transition-based parsing (Covington 2001, Nivre 2003). In arc standard\nparsing the transition operators only assert relations between elements at the top of\nthe stack, and once an element has been assigned its head it is removed from the\nstack and is not available for further processing. As we\u2019ll see, there are alterna-\ntive transition systems which demonstrate different parsing behaviors, but the arc\nstandard approach is quite effective and is simple to implement.\nThe speci\ufb01cation of a transition-based parser is quite simple, based on repre-\nsenting the current state of the parse as a con\ufb01guration : the stack, an input buffer con\ufb01guration\nof words or tokens, and a set of relations representing a dependency tree. Parsing\nmeans making a sequence of transitions through the space of possible con\ufb01gura-\ntions. We start with an initial con\ufb01guration in which the stack contains the ROOT\nnode, the buffer has the tokens in the sentence, and an empty set of relations repre-\nsents the parse. In the \ufb01nal goal state, the stack and the word list should be empty,\nand the set of relations will represent the \ufb01nal parse. Fig. 19.5 gives the algorithm.\nfunction DEPENDENCY PARSE (words )returns dependency tree\nstate f[root], [ words ], []g; initial con\ufb01guration\nwhile state not \ufb01nal\nt ORACLE (state ) ; choose a transition operator to apply\nstate APPLY (t,state ) ; apply it, creating a new state\nreturn state\nFigure 19.5 A generic transition-based dependency parser\nAt each step, the parser consults an oracle (we\u2019ll come back to this shortly) that\nprovides the correct transition operator to use given the current con\ufb01guration. It then\napplies that operator to the current con\ufb01guration, producing a new con\ufb01guration.\nThe process ends when all the words in the sentence have been consumed and the\nROOT node is the only element remaining on the stack.\nThe ef\ufb01ciency of transition-based parsers should be apparent from the algorithm.\nThe complexity is linear in the length of the sentence since it is based on a single\nleft to right pass through the words in the sentence. (Each word must \ufb01rst be shifted\nonto the stack and then later reduced.)\nNote that unlike the dynamic programming and search-based approaches dis-\ncussed in Chapter 18, this approach is a straightforward greedy algorithm\u2014the or-\nacle provides a single choice at each step and the parser proceeds with that choice,\nno other options are explored, no backtracking is employed, and a single parse is\nreturned in the end.\nFigure 19.6 illustrates the operation of the parser with the sequence of transitions",
    "metadata": {
      "source": "19",
      "chunk_id": 6,
      "token_count": 756,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8CHAPTER 19 \u2022 D EPENDENCY PARSING\nleading to a parse for the following example.\nBook methemorning \ufb02ightiobjobj\ndet\ncompoundroot\n(19.7)\nLet\u2019s consider the state of the con\ufb01guration at Step 2, after the word mehas been\npushed onto the stack.\nStack Word List Relations\n[root, book, me] [the, morning, \ufb02ight]\nThe correct operator to apply here is RIGHT ARCwhich assigns book as the head of\nmeand pops mefrom the stack resulting in the following con\ufb01guration.\nStack Word List Relations\n[root, book] [the, morning, \ufb02ight] (book!me)\nAfter several subsequent applications of the SHIFT operator, the con\ufb01guration in\nStep 6 looks like the following:\nStack Word List Relations\n[root, book, the, morning, \ufb02ight] [] (book!me)\nHere, all the remaining words have been passed onto the stack and all that is left\nto do is to apply the appropriate reduce operators. In the current con\ufb01guration, we\nemploy the LEFT ARCoperator resulting in the following state.\nStack Word List Relations\n[root, book, the, \ufb02ight] [] (book!me)\n(morning \ufb02ight)\nAt this point, the parse for this sentence consists of the following structure.\nBook methemorning \ufb02ightiobj compound\n(19.8)\nThere are several important things to note when examining sequences such as\nthe one in Figure 19.6. First, the sequence given is not the only one that might lead\nto a reasonable parse. In general, there may be more than one path that leads to the\nsame result, and due to ambiguity, there may be other transition sequences that lead\nto different equally valid parses.\nSecond, we are assuming that the oracle always provides the correct operator\nat each point in the parse\u2014an assumption that is unlikely to be true in practice.\nAs a result, given the greedy nature of this algorithm, incorrect choices will lead to\nincorrect parses since the parser has no opportunity to go back and pursue alternative\nchoices. Section 19.2.4 will introduce several techniques that allow transition-based\napproaches to explore the search space more fully.",
    "metadata": {
      "source": "19",
      "chunk_id": 7,
      "token_count": 489,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9",
    "metadata": {
      "source": "19",
      "chunk_id": 8,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "19.2 \u2022 T RANSITION -BASED DEPENDENCY PARSING 9\nStep Stack Word List Action Relation Added\n0 [root] [book, me, the, morning, \ufb02ight] SHIFT\n1 [root, book] [me, the, morning, \ufb02ight] SHIFT\n2 [root, book, me] [the, morning, \ufb02ight] RIGHT ARC (book!me)\n3 [root, book] [the, morning, \ufb02ight] SHIFT\n4 [root, book, the] [morning, \ufb02ight] SHIFT\n5 [root, book, the, morning] [\ufb02ight] SHIFT\n6 [root, book, the, morning, \ufb02ight] [] LEFT ARC (morning \ufb02ight)\n7 [root, book, the, \ufb02ight] [] LEFT ARC (the \ufb02ight)\n8 [root, book, \ufb02ight] [] RIGHT ARC (book!\ufb02ight)\n9 [root, book] [] RIGHT ARC (root!book)\n10 [root] [] Done\nFigure 19.6 Trace of a transition-based parse.\nFinally, for simplicity, we have illustrated this example without the labels on\nthe dependency relations. To produce labeled trees, we can parameterize the LEFT -\nARCand RIGHT ARCoperators with dependency labels, as in LEFT ARC(NSUBJ ) or\nRIGHT ARC(OBJ). This is equivalent to expanding the set of transition operators from\nour original set of three to a set that includes LEFT ARCand RIGHT ARCoperators for\neach relation in the set of dependency relations being used, plus an additional one\nfor the SHIFT operator. This, of course, makes the job of the oracle more dif\ufb01cult\nsince it now has a much larger set of operators from which to choose.\n19.2.1 Creating an Oracle\nThe oracle for greedily selecting the appropriate transition is trained by supervised\nmachine learning. As with all supervised machine learning methods, we will need\ntraining data: con\ufb01gurations annotated with the correct transition to take. We can\ndraw these from dependency trees. And we need to extract features of the con-\n\ufb01guration. We\u2019ll introduce neural classi\ufb01ers that represent the con\ufb01guration via\nembeddings, as well as classic systems that use hand-designed features.\nGenerating Training Data\nThe oracle from the algorithm in Fig. 19.5 takes as input a con\ufb01guration and returns a\ntransition operator. Therefore, to train a classi\ufb01er, we will need con\ufb01gurations paired\nwith transition operators (i.e., LEFT ARC,RIGHT ARC, or SHIFT ). Unfortunately,\ntreebanks pair entire sentences with their corresponding trees, not con\ufb01gurations\nwith transitions.\nTo generate the required training data, we employ the oracle-based parsing algo-\nrithm in a clever way. We supply our oracle with the training sentences to be parsed\nalong with their corresponding reference parses from the treebank. To produce train-\ning instances, we then simulate the operation of the parser by running the algorithm\nand relying on a new training oracle to give us correct transition operators for each training oracle\nsuccessive con\ufb01guration.\nTo see how this works, let\u2019s \ufb01rst review the operation of our parser. It begins with\na default initial con\ufb01guration where the stack contains the ROOT , the input list is just\nthe list of words, and the set of relations is empty. The LEFT ARCand RIGHT ARC\noperators each add relations between the words at the top of the stack to the set of\nrelations being accumulated for a given sentence. Since we have a gold-standard",
    "metadata": {
      "source": "19",
      "chunk_id": 9,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "embeddings, as well as classic systems that use hand-designed features.\nGenerating Training Data\nThe oracle from the algorithm in Fig. 19.5 takes as input a con\ufb01guration and returns a\ntransition operator. Therefore, to train a classi\ufb01er, we will need con\ufb01gurations paired\nwith transition operators (i.e., LEFT ARC,RIGHT ARC, or SHIFT ). Unfortunately,\ntreebanks pair entire sentences with their corresponding trees, not con\ufb01gurations\nwith transitions.\nTo generate the required training data, we employ the oracle-based parsing algo-\nrithm in a clever way. We supply our oracle with the training sentences to be parsed\nalong with their corresponding reference parses from the treebank. To produce train-\ning instances, we then simulate the operation of the parser by running the algorithm\nand relying on a new training oracle to give us correct transition operators for each training oracle\nsuccessive con\ufb01guration.\nTo see how this works, let\u2019s \ufb01rst review the operation of our parser. It begins with\na default initial con\ufb01guration where the stack contains the ROOT , the input list is just\nthe list of words, and the set of relations is empty. The LEFT ARCand RIGHT ARC\noperators each add relations between the words at the top of the stack to the set of\nrelations being accumulated for a given sentence. Since we have a gold-standard\nreference parse for each training sentence, we know which dependency relations are\nvalid for a given sentence. Therefore, we can use the reference parse to guide the",
    "metadata": {
      "source": "19",
      "chunk_id": 10,
      "token_count": 324,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 CHAPTER 19 \u2022 D EPENDENCY PARSING\nStep Stack Word List Predicted Action\n0 [root] [book, the, \ufb02ight, through, houston] SHIFT\n1 [root, book] [the, \ufb02ight, through, houston] SHIFT\n2 [root, book, the] [\ufb02ight, through, houston] SHIFT\n3 [root, book, the, \ufb02ight] [through, houston] LEFT ARC\n4 [root, book, \ufb02ight] [through, houston] SHIFT\n5 [root, book, \ufb02ight, through] [houston] SHIFT\n6 [root, book, \ufb02ight, through, houston] [] LEFT ARC\n7 [root, book, \ufb02ight, houston ] [] RIGHT ARC\n8 [root, book, \ufb02ight] [] RIGHT ARC\n9 [root, book] [] RIGHT ARC\n10 [root] [] Done\nFigure 19.7 Generating training items consisting of con\ufb01guration/predicted action pairs by simulating a parse\nwith a given reference parse.\nselection of operators as the parser steps through a sequence of con\ufb01gurations.\nTo be more precise, given a reference parse and a con\ufb01guration, the training\noracle proceeds as follows:\n\u2022 Choose LEFT ARCif it produces a correct head-dependent relation given the\nreference parse and the current con\ufb01guration,\n\u2022 Otherwise, choose RIGHT ARCif (1) it produces a correct head-dependent re-\nlation given the reference parse and (2) all of the dependents of the word at\nthe top of the stack have already been assigned,\n\u2022 Otherwise, choose SHIFT .\nThe restriction on selecting the RIGHT ARCoperator is needed to ensure that a\nword is not popped from the stack, and thus lost to further processing, before all its\ndependents have been assigned to it.\nMore formally, during training the oracle has access to the following:\n\u2022 A current con\ufb01guration with a stack Sand a set of dependency relations Rc\n\u2022 A reference parse consisting of a set of vertices Vand a set of dependency\nrelations Rp\nGiven this information, the oracle chooses transitions as follows:\nLEFT ARC(r):if(S1r S2)2Rp\nRIGHT ARC(r):if(S2r S1)2Rpand8r0;w s:t:(S1r0w)2Rpthen(S1r0w)2Rc\nSHIFT :otherwise\nLet\u2019s walk through the processing of the following example as shown in Fig. 19.7.\nBook the\ufb02ight through Houstonobj\ndetnmod\ncaseroot\n(19.9)\nAt Step 1, LEFT ARCis not applicable in the initial con\ufb01guration since it asserts\na relation, (root book), not in the reference answer; RIGHT ARCdoes assert a\nrelation contained in the \ufb01nal answer (root !book), however book has not been\nattached to any of its dependents yet, so we have to defer, leaving SHIFT as the only",
    "metadata": {
      "source": "19",
      "chunk_id": 11,
      "token_count": 658,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\n19.2 \u2022 T RANSITION -BASED DEPENDENCY PARSING 11\npossible action. The same conditions hold in the next two steps. In step 3, LEFT ARC\nis selected to link theto its head.\nNow consider the situation in Step 4.\nStack Word buffer Relations\n[root, book, \ufb02ight] [through, Houston] (the \ufb02ight)\nHere, we might be tempted to add a dependency relation between book and\ufb02ight ,\nwhich is present in the reference parse. But doing so now would prevent the later\nattachment of Houston since \ufb02ight would have been removed from the stack. For-\ntunately, the precondition on choosing RIGHT ARCprevents this choice and we\u2019re\nagain left with SHIFT as the only viable option. The remaining choices complete the\nset of operators needed for this example.\nTo recap, we derive appropriate training instances consisting of con\ufb01guration-\ntransition pairs from a treebank by simulating the operation of a parser in the con-\ntext of a reference dependency tree. We can deterministically record correct parser\nactions at each step as we progress through each training example, thereby creating\nthe training set we require.\n19.2.2 A feature-based classi\ufb01er\nWe\u2019ll now introduce two classi\ufb01ers for choosing transitions, here a classic feature-\nbased algorithm and in the next section a neural classi\ufb01er using embedding features.\nFeatured-based classi\ufb01ers generally use the same features we\u2019ve seen with part-\nof-speech tagging and partial parsing: Word forms, lemmas, parts of speech, the\nhead, and the dependency relation to the head. Other features may be relevant for\nsome languages, for example morphosyntactic features like case marking on subjects\nor objects. The features are extracted from the training con\ufb01gurations , which consist\nof the stack, the buffer and the current set of relations. Most useful are features\nreferencing the top levels of the stack, the words near the front of the buffer, and the\ndependency relations already associated with any of those elements.\nWe\u2019ll use a feature template as we did for sentiment analysis and part-of-speechfeature\ntemplate\ntagging. Feature templates allow us to automatically generate large numbers of spe-\nci\ufb01c features from a training set. For example, consider the following feature tem-\nplates that are based on single positions in a con\ufb01guration.\nhs1:w;opi;hs2:w;opihs1:t;opi;hs2:t;opi\nhb1:w;opi;hb1:t;opihs1:wt;opi (19.10)\nHere features are denoted as location :property , where s= stack, b= the word\nbuffer, w= word forms, t= part-of-speech, and op= operator. Thus the feature for\nthe word form at the top of the stack would be s1:w, the part of speech tag at the\nfront of the buffer b1:t, and the concatenated feature s1:wtrepresents the word form\nconcatenated with the part of speech of the word at the top of the stack. Consider\napplying these templates to the following intermediate con\ufb01guration derived from a\ntraining oracle for (19.2).\nStack Word buffer Relations\n[root, canceled, \ufb02ights] [to Houston] (canceled!United)\n(\ufb02ights!morning)\n(\ufb02ights!the)",
    "metadata": {
      "source": "19",
      "chunk_id": 12,
      "token_count": 736,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 CHAPTER 19 \u2022 D EPENDENCY PARSING\nThe correct transition here is SHIFT (you should convince yourself of this before\nproceeding). The application of our set of feature templates to this con\ufb01guration\nwould result in the following set of instantiated features.\nhs1:w=\ufb02ights ;op=shifti (19.11)\nhs2:w=canceled ;op=shifti\nhs1:t=NNS;op=shifti\nhs2:t=VBD;op=shifti\nhb1:w=to;op=shifti\nhb1:t=TO;op=shifti\nhs1:wt=\ufb02ightsNNS ;op=shifti\nGiven that the left and right arc transitions operate on the top two elements of the\nstack, features that combine properties from these positions are even more useful.\nFor example, a feature like s1:t\u000es2:tconcatenates the part of speech tag of the word\nat the top of the stack with the tag of the word beneath it.\nhs1:t\u000es2:t=NNSVBD ;op=shifti (19.12)\nGiven the training data and features, any classi\ufb01er, like multinomial logistic re-\ngression or support vector machines, can be used.\n19.2.3 A neural classi\ufb01er\nThe oracle can also be implemented by a neural classi\ufb01er. A standard architecture\nis simply to pass the sentence through an encoder, then take the presentation of the\ntop 2 words on the stack and the \ufb01rst word of the buffer, concatenate them, and\npresent to a feedforward network that predicts the transition to take (Kiperwasser\nand Goldberg, 2016; Kulmizev et al., 2019). Fig. 19.8 sketches this model. Learning\ncan be done with cross-entropy loss.\nw\u2026s2...s1Input bu\ufb00erStackLEFTARCRIGHTARCSHIFTActionDependencyRelationsw3w2ENCODERw1w2w3w4w5w6Parser OracleSoftmaxFFNws1s2e(w)e(s1)e(s2)\nFigure 19.8 Neural classi\ufb01er for the oracle for the transition-based parser. The parser takes\nthe top 2 words on the stack and the \ufb01rst word of the buffer, represents them by their encodings\n(from running the whole sentence through the encoder), concatenates the embeddings and\npasses through a softmax to choose a parser action (transition).",
    "metadata": {
      "source": "19",
      "chunk_id": 13,
      "token_count": 543,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\n19.2 \u2022 T RANSITION -BASED DEPENDENCY PARSING 13\n19.2.4 Advanced Methods in Transition-Based Parsing\nThe basic transition-based approach can be elaborated in a number of ways to im-\nprove performance by addressing some of the most obvious \ufb02aws in the approach.\nAlternative Transition Systems\nThe arc-standard transition system described above is only one of many possible sys-\ntems. A frequently used alternative is the arc eager transition system. The arc eager arc eager\napproach gets its name from its ability to assert rightward relations much sooner\nthan in the arc standard approach. To see this, let\u2019s revisit the arc standard trace of\nExample 19.9, repeated here.\nBook the\ufb02ight through Houstonobj\ndetnmod\ncaseroot\nConsider the dependency relation between book and\ufb02ight in this analysis. As\nis shown in Fig. 19.7, an arc-standard approach would assert this relation at Step 8,\ndespite the fact that book and\ufb02ight \ufb01rst come together on the stack much earlier at\nStep 4. The reason this relation can\u2019t be captured at this point is due to the presence\nof the postnominal modi\ufb01er through Houston . In an arc-standard approach, depen-\ndents are removed from the stack as soon as they are assigned their heads. If \ufb02ight\nhad been assigned book as its head in Step 4, it would no longer be available to serve\nas the head of Houston .\nWhile this delay doesn\u2019t cause any issues in this example, in general the longer\na word has to wait to get assigned its head the more opportunities there are for\nsomething to go awry. The arc-eager system addresses this issue by allowing words\nto be attached to their heads as early as possible, before all the subsequent words\ndependent on them have been seen. This is accomplished through minor changes to\ntheLEFT ARCand RIGHT ARCoperators and the addition of a new REDUCE operator.\n\u2022LEFT ARC: Assert a head-dependent relation between the word at the front of\nthe input buffer and the word at the top of the stack; pop the stack.\n\u2022RIGHT ARC: Assert a head-dependent relation between the word on the top of\nthe stack and the word at the front of the input buffer; shift the word at the\nfront of the input buffer to the stack.\n\u2022SHIFT : Remove the word from the front of the input buffer and push it onto\nthe stack.\n\u2022REDUCE : Pop the stack.\nThe LEFT ARCand RIGHT ARCoperators are applied to the top of the stack and\nthe front of the input buffer, instead of the top two elements of the stack as in the\narc-standard approach. The RIGHT ARCoperator now moves the dependent to the\nstack from the buffer rather than removing it, thus making it available to serve as the\nhead of following words. The new REDUCE operator removes the top element from\nthe stack. Together these changes permit a word to be eagerly assigned its head and\nstill allow it to serve as the head for later dependents. The trace shown in Fig. 19.9\nillustrates the new decision sequence for this example.\nIn addition to demonstrating the arc-eager transition system, this example demon-\nstrates the power and \ufb02exibility of the overall transition-based approach. We were\nable to swap in a new transition system without having to make any changes to the",
    "metadata": {
      "source": "19",
      "chunk_id": 14,
      "token_count": 724,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14",
    "metadata": {
      "source": "19",
      "chunk_id": 15,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "14 CHAPTER 19 \u2022 D EPENDENCY PARSING\nStep Stack Word List Action Relation Added\n0 [root] [book, the, \ufb02ight, through, houston] RIGHT ARC (root!book)\n1 [root, book] [the, \ufb02ight, through, houston] SHIFT\n2 [root, book, the] [\ufb02ight, through, houston] LEFT ARC (the \ufb02ight)\n3 [root, book] [\ufb02ight, through, houston] RIGHT ARC (book!\ufb02ight)\n4 [root, book, \ufb02ight] [through, houston] SHIFT\n5 [root, book, \ufb02ight, through] [houston] LEFT ARC (through houston)\n6 [root, book, \ufb02ight] [houston] RIGHT ARC (\ufb02ight!houston)\n7 [root, book, \ufb02ight, houston] [] REDUCE\n8 [root, book, \ufb02ight] [] REDUCE\n9 [root, book] [] REDUCE\n10 [root] [] Done\nFigure 19.9 A processing trace of Book the \ufb02ight through Houston using the arc-eager transition operators.\nunderlying parsing algorithm. This \ufb02exibility has led to the development of a di-\nverse set of transition systems that address different aspects of syntax and semantics\nincluding: assigning part of speech tags (Choi and Palmer, 2011a), allowing the\ngeneration of non-projective dependency structures (Nivre, 2009), assigning seman-\ntic roles (Choi and Palmer, 2011b), and parsing texts containing multiple languages\n(Bhat et al., 2017).\nBeam Search\nThe computational ef\ufb01ciency of the transition-based approach discussed earlier de-\nrives from the fact that it makes a single pass through the sentence, greedily making\ndecisions without considering alternatives. Of course, this is also a weakness \u2013 once\na decision has been made it can not be undone, even in the face of overwhelming\nevidence arriving later in a sentence. We can use beam search to explore alterna- beam search\ntive decision sequences. Recall from Chapter 9 that beam search uses a breadth-\ufb01rst\nsearch strategy with a heuristic \ufb01lter that prunes the search frontier to stay within a\n\ufb01xed-size beam width . beam width\nIn applying beam search to transition-based parsing, we\u2019ll elaborate on the al-\ngorithm given in Fig. 19.5. Instead of choosing the single best transition operator\nat each iteration, we\u2019ll apply all applicable operators to each state on an agenda and\nthen score the resulting con\ufb01gurations. We then add each of these new con\ufb01gura-\ntions to the frontier, subject to the constraint that there has to be room within the\nbeam. As long as the size of the agenda is within the speci\ufb01ed beam width, we can\nadd new con\ufb01gurations to the agenda. Once the agenda reaches the limit, we only\nadd new con\ufb01gurations that are better than the worst con\ufb01guration on the agenda\n(removing the worst element so that we stay within the limit). Finally, to insure that\nwe retrieve the best possible state on the agenda, the while loop continues as long as\nthere are non-\ufb01nal states on the agenda.\nThe beam search approach requires a more elaborate notion of scoring than we\nused with the greedy algorithm. There, we assumed that the oracle would be a\nsupervised classi\ufb01er that chose the best transition operator based on features of the",
    "metadata": {
      "source": "19",
      "chunk_id": 16,
      "token_count": 776,
      "chapter_title": ""
    }
  },
  {
    "content": "search strategy with a heuristic \ufb01lter that prunes the search frontier to stay within a\n\ufb01xed-size beam width . beam width\nIn applying beam search to transition-based parsing, we\u2019ll elaborate on the al-\ngorithm given in Fig. 19.5. Instead of choosing the single best transition operator\nat each iteration, we\u2019ll apply all applicable operators to each state on an agenda and\nthen score the resulting con\ufb01gurations. We then add each of these new con\ufb01gura-\ntions to the frontier, subject to the constraint that there has to be room within the\nbeam. As long as the size of the agenda is within the speci\ufb01ed beam width, we can\nadd new con\ufb01gurations to the agenda. Once the agenda reaches the limit, we only\nadd new con\ufb01gurations that are better than the worst con\ufb01guration on the agenda\n(removing the worst element so that we stay within the limit). Finally, to insure that\nwe retrieve the best possible state on the agenda, the while loop continues as long as\nthere are non-\ufb01nal states on the agenda.\nThe beam search approach requires a more elaborate notion of scoring than we\nused with the greedy algorithm. There, we assumed that the oracle would be a\nsupervised classi\ufb01er that chose the best transition operator based on features of the\ncurrent con\ufb01guration. This choice can be viewed as assigning a score to all the\npossible transitions and picking the best one.\n\u02c6T(c) =argmaxScore (t;c)\nWith beam search we are now searching through the space of decision sequences,\nso it makes sense to base the score for a con\ufb01guration on its entire history. So we\ncan de\ufb01ne the score for a new con\ufb01guration as the score of its predecessor plus the",
    "metadata": {
      "source": "19",
      "chunk_id": 17,
      "token_count": 393,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\n19.3 \u2022 G RAPH -BASED DEPENDENCY PARSING 15\nscore of the operator used to produce it.\nCon\ufb01gScore (c0) = 0:0\nCon\ufb01gScore (ci) = Con\ufb01gScore (ci\u00001)+Score (ti;ci\u00001)\nThis score is used both in \ufb01ltering the agenda and in selecting the \ufb01nal answer. The\nnew beam search version of transition-based parsing is given in Fig. 19.10.\nfunction DEPENDENCY BEAM PARSE (words ,width )returns dependency tree\nstate f[root], [ words ], [], 0.0 g;initial con\ufb01guration\nagenda hstatei ;initial agenda\nwhile agenda contains non-\ufb01nal states\nnewagenda hi\nfor each state2agenda do\nfor all ftjt2VALID OPERATORS (state )gdo\nchild APPLY (t,state )\nnewagenda ADDTOBEAM (child ,newagenda ,width )\nagenda newagenda\nreturn BESTOF(agenda )\nfunction ADDTOBEAM (state ,agenda ,width )returns updated agenda\nifLENGTH (agenda )<width then\nagenda INSERT (state ,agenda )\nelse if SCORE (state )>SCORE (WORST OF(agenda ))\nagenda REMOVE (WORST OF(agenda ))\nagenda INSERT (state ,agenda )\nreturn agenda\nFigure 19.10 Beam search applied to transition-based dependency parsing.\n19.3 Graph-Based Dependency Parsing\nGraph-based methods are the second important family of dependency parsing algo-\nrithms. Graph-based parsers are more accurate than transition-based parsers, espe-\ncially on long sentences; transition-based methods have trouble when the heads are\nvery far from the dependents (McDonald and Nivre, 2011). Graph-based methods\navoid this dif\ufb01culty by scoring entire trees, rather than relying on greedy local de-\ncisions. Furthermore, unlike transition-based approaches, graph-based parsers can\nproduce non-projective trees. Although projectivity is not a signi\ufb01cant issue for\nEnglish, it is de\ufb01nitely a problem for many of the world\u2019s languages.\nGraph-based dependency parsers search through the space of possible trees for a\ngiven sentence for a tree (or trees) that maximize some score. These methods encode\nthe search space as directed graphs and employ methods drawn from graph theory\nto search the space for optimal solutions. More formally, given a sentence Swe\u2019re\nlooking for the best dependency tree in Gs, the space of all possible trees for that\nsentence, that maximizes some score.\n\u02c6T(S) =argmax\nt2GSScore (t;S)",
    "metadata": {
      "source": "19",
      "chunk_id": 18,
      "token_count": 568,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\n16 CHAPTER 19 \u2022 D EPENDENCY PARSING\nWe\u2019ll make the simplifying assumption that this score can be edge-factored , edge-factored\nmeaning that the overall score for a tree is the sum of the scores of each of the scores\nof the edges that comprise the tree.\nScore (t;S) =X\ne2tScore (e)\nGraph-based algorithms have to solve two problems: (1) assigning a score to\neach edge, and (2) \ufb01nding the best parse tree given the scores of all potential edges.\nIn the next few sections we\u2019ll introduce solutions to these two problems, beginning\nwith the second problem of \ufb01nding trees, and then giving a feature-based and a\nneural algorithm for solving the \ufb01rst problem of assigning scores.\n19.3.1 Parsing via \ufb01nding the maximum spanning tree\nIn graph-based parsing, given a sentence Swe start by creating a graph Gwhich is a\nfully-connected, weighted, directed graph where the vertices are the input words and\nthe directed edges represent all possible head-dependent assignments. We\u2019ll include\nan additional ROOT node with outgoing edges directed at all of the other vertices.\nThe weights of each edge in Gre\ufb02ect the score for each possible head-dependent\nrelation assigned by some scoring algorithm.\nIt turns out that \ufb01nding the best dependency parse for Sis equivalent to \ufb01nding\nthemaximum spanning tree over G. A spanning tree over a graph Gis a subsetmaximum\nspanning tree\nofGthat is a tree and covers all the vertices in G; a spanning tree over Gthat starts\nfrom the ROOT is a valid parse of S. A maximum spanning tree is the spanning tree\nwith the highest score. Thus a maximum spanning tree of Gemanating from the\nROOT is the optimal dependency parse for the sentence.\nA directed graph for the example Book that \ufb02ight is shown in Fig. 19.11, with the\nmaximum spanning tree corresponding to the desired parse shown in blue. For ease\nof exposition, we\u2019ll describe here the algorithm for unlabeled dependency parsing.\nrootBookthat\ufb02ight1244568757\nFigure 19.11 Initial rooted, directed graph for Book that \ufb02ight .\nBefore describing the algorithm it\u2019s useful to consider two intuitions about di-\nrected graphs and their spanning trees. The \ufb01rst intuition begins with the fact that\nevery vertex in a spanning tree has exactly one incoming edge. It follows from this\nthat every connected component of a spanning tree (i.e., every set of vertices that\nare linked to each other by paths over edges) will also have one incoming edge.\nThe second intuition is that the absolute values of the edge scores are not critical\nto determining its maximum spanning tree. Instead, it is the relative weights of the\nedges entering each vertex that matters. If we were to subtract a constant amount\nfrom each edge entering a given vertex it would have no impact on the choice of",
    "metadata": {
      "source": "19",
      "chunk_id": 19,
      "token_count": 631,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17",
    "metadata": {
      "source": "19",
      "chunk_id": 20,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "19.3 \u2022 G RAPH -BASED DEPENDENCY PARSING 17\nthe maximum spanning tree since every possible spanning tree would decrease by\nexactly the same amount.\nThe \ufb01rst step of the algorithm itself is quite straightforward. For each vertex\nin the graph, an incoming edge (representing a possible head assignment) with the\nhighest score is chosen. If the resulting set of edges produces a spanning tree then\nwe\u2019re done. More formally, given the original fully-connected graph G= (V;E), a\nsubgraph T= (V;F)is a spanning tree if it has no cycles and each vertex (other than\nthe root) has exactly one edge entering it. If the greedy selection process produces\nsuch a tree then it is the best possible one.\nUnfortunately, this approach doesn\u2019t always lead to a tree since the set of edges\nselected may contain cycles. Fortunately, in yet another case of multiple discovery,\nthere is a straightforward way to eliminate cycles generated during the greedy se-\nlection phase. Chu and Liu (1965) and Edmonds (1967) independently developed\nan approach that begins with greedy selection and follows with an elegant recursive\ncleanup phase that eliminates cycles.\nThe cleanup phase begins by adjusting all the weights in the graph by subtracting\nthe score of the maximum edge entering each vertex from the score of all the edges\nentering that vertex. This is where the intuitions mentioned earlier come into play.\nWe have scaled the values of the edges so that the weights of the edges in the cycle\nhave no bearing on the weight of anyof the possible spanning trees. Subtracting the\nvalue of the edge with maximum weight from each edge entering a vertex results\nin a weight of zero for all of the edges selected during the greedy selection phase,\nincluding all of the edges involved in the cycle .\nHaving adjusted the weights, the algorithm creates a new graph by selecting a\ncycle and collapsing it into a single new node. Edges that enter or leave the cycle\nare altered so that they now enter or leave the newly collapsed node. Edges that do\nnot touch the cycle are included and edges within the cycle are dropped.\nNow, if we knew the maximum spanning tree of this new graph, we would have\nwhat we need to eliminate the cycle. The edge of the maximum spanning tree di-\nrected towards the vertex representing the collapsed cycle tells us which edge to\ndelete in order to eliminate the cycle. How do we \ufb01nd the maximum spanning tree\nof this new graph? We recursively apply the algorithm to the new graph. This will\neither result in a spanning tree or a graph with a cycle. The recursions can continue\nas long as cycles are encountered. When each recursion completes we expand the\ncollapsed vertex, restoring all the vertices and edges from the cycle with the excep-\ntion of the single edge to be deleted .\nPutting all this together, the maximum spanning tree algorithm consists of greedy\nedge selection, re-scoring of edge costs and a recursive cleanup phase when needed.\nThe full algorithm is shown in Fig. 19.12.\nFig. 19.13 steps through the algorithm with our Book that \ufb02ight example. The\n\ufb01rst row of the \ufb01gure illustrates greedy edge selection with the edges chosen shown\nin blue (corresponding to the set Fin the algorithm). This results in a cycle between\nthatand\ufb02ight . The scaled weights using the maximum value entering each node are\nshown in the graph to the right.\nCollapsing the cycle between that and\ufb02ight to a single node (labelled tf) and\nrecursing with the newly scaled costs is shown in the second row. The greedy selec-",
    "metadata": {
      "source": "19",
      "chunk_id": 21,
      "token_count": 771,
      "chapter_title": ""
    }
  },
  {
    "content": "rected towards the vertex representing the collapsed cycle tells us which edge to\ndelete in order to eliminate the cycle. How do we \ufb01nd the maximum spanning tree\nof this new graph? We recursively apply the algorithm to the new graph. This will\neither result in a spanning tree or a graph with a cycle. The recursions can continue\nas long as cycles are encountered. When each recursion completes we expand the\ncollapsed vertex, restoring all the vertices and edges from the cycle with the excep-\ntion of the single edge to be deleted .\nPutting all this together, the maximum spanning tree algorithm consists of greedy\nedge selection, re-scoring of edge costs and a recursive cleanup phase when needed.\nThe full algorithm is shown in Fig. 19.12.\nFig. 19.13 steps through the algorithm with our Book that \ufb02ight example. The\n\ufb01rst row of the \ufb01gure illustrates greedy edge selection with the edges chosen shown\nin blue (corresponding to the set Fin the algorithm). This results in a cycle between\nthatand\ufb02ight . The scaled weights using the maximum value entering each node are\nshown in the graph to the right.\nCollapsing the cycle between that and\ufb02ight to a single node (labelled tf) and\nrecursing with the newly scaled costs is shown in the second row. The greedy selec-\ntion step in this recursion yields a spanning tree that links roottobook , as well as an\nedge that links book to the contracted node. Expanding the contracted node, we can\nsee that this edge corresponds to the edge from book to\ufb02ight in the original graph.\nThis in turn tells us which edge to drop to eliminate the cycle.",
    "metadata": {
      "source": "19",
      "chunk_id": 22,
      "token_count": 360,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\n18 CHAPTER 19 \u2022 D EPENDENCY PARSING\nfunction MAXSPANNING TREE(G=(V ,E) ,root,score )returns spanning tree\nF []\nT\u2019 []\nscore\u2019 []\nfor each v2Vdo\nbestInEdge argmaxe=(u;v)2Escore[e]\nF F[bestInEdge\nfor each e=(u,v)2Edo\nscore\u2019[e] score[e]\u0000score[bestInEdge]\nifT=(V ,F) is a spanning tree then return it\nelse\nC a cycle in F\nG\u2019 CONTRACT (G,C)\nT\u2019 MAXSPANNING TREE(G\u2019,root,score\u2019 )\nT EXPAND (T\u2019,C)\nreturn T\nfunction CONTRACT (G,C)returns contracted graph\nfunction EXPAND (T,C)returns expanded graph\nFigure 19.12 The Chu-Liu Edmonds algorithm for \ufb01nding a maximum spanning tree in a\nweighted directed graph.\nOn arbitrary directed graphs, this version of the CLE algorithm runs in O(mn)\ntime, where mis the number of edges and nis the number of nodes. Since this par-\nticular application of the algorithm begins by constructing a fully connected graph\nm=n2yielding a running time of O(n3). Gabow et al. (1986) present a more ef\ufb01-\ncient implementation with a running time of O(m+nlogn ).\n19.3.2 A feature-based algorithm for assigning scores\nRecall that given a sentence, S, and a candidate tree, T, edge-factored parsing models\nmake the simpli\ufb01cation that the score for the tree is the sum of the scores of the edges\nthat comprise the tree:\nscore(S;T) =X\ne2Tscore(S;e)\nIn a feature-based algorithm we compute the edge score as a weighted sum of fea-\ntures extracted from it:\nscore(S;e) =NX\ni=1wifi(S;e)\nOr more succinctly.\nscore(S;e) = w\u0001f\nGiven this formulation, we need to identify relevant features and train the weights.\nThe features (and feature combinations) used to train edge-factored models mir-\nror those used in training transition-based parsers, such as",
    "metadata": {
      "source": "19",
      "chunk_id": 23,
      "token_count": 478,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\n19.3 \u2022 G RAPH -BASED DEPENDENCY PARSING 19\nrootBooktf\nrootBookthat\ufb02ight0-3-4\n-7-1-6-2rootBook12that7\ufb02ight8-4-30-2-6-1-700\nrootBook0tf-10-3-4\n-7-1-6-2rootBook12that7\ufb02ight81244568757\nDeleted from cycle\nFigure 19.13 Chu-Liu-Edmonds graph-based example for Book that \ufb02ight\n\u2022 Wordforms, lemmas, and parts of speech of the headword and its dependent.\n\u2022 Corresponding features from the contexts before, after and between the words.\n\u2022 Word embeddings.\n\u2022 The dependency relation itself.\n\u2022 The direction of the relation (to the right or left).\n\u2022 The distance from the head to the dependent.\nGiven a set of features, our next problem is to learn a set of weights correspond-\ning to each. Unlike many of the learning problems discussed in earlier chapters,\nhere we are not training a model to associate training items with class labels, or\nparser actions. Instead, we seek to train a model that assigns higher scores to cor-\nrect trees than to incorrect ones. An effective framework for problems like this is to\nuseinference-based learning combined with the perceptron learning rule. In thisinference-based\nlearning\nframework, we parse a sentence (i.e, perform inference) from the training set using\nsome initially random set of initial weights. If the resulting parse matches the cor-\nresponding tree in the training data, we do nothing to the weights. Otherwise, we\n\ufb01nd those features in the incorrect parse that are notpresent in the reference parse\nand we lower their weights by a small amount based on the learning rate. We do this\nincrementally for each sentence in our training data until the weights converge.",
    "metadata": {
      "source": "19",
      "chunk_id": 24,
      "token_count": 409,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\n20 CHAPTER 19 \u2022 D EPENDENCY PARSING\n19.3.3 A neural algorithm for assigning scores\nState-of-the-art graph-based multilingual parsers are based on neural networks. In-\nstead of extracting hand-designed features to represent each edge between words wi\nandwj, these parsers run the sentence through an encoder, and then pass the encoded\nrepresentation of the two words wiandwjthrough a network that estimates a score\nfor the edge i!j.\nbookthat\ufb02ightr1score(h1head, h3dep)Bia\ufb03neb\nENCODERUh1 headFFNheadFFNheadFFNdepFFNdeph1 depFFNheadFFNdeph2 headh2 deph3 headh3 depWr2r3\u2211+\nFigure 19.14 Computing scores for a single edge (book !\ufb02ight) in the biaf\ufb01ne parser of\nDozat and Manning (2017); Dozat et al. (2017). The parser uses distinct feedforward net-\nworks to turn the encoder output for each word into a head and dependent representation for\nthe word. The biaf\ufb01ne function turns the head embedding of the head and the dependent\nembedding of the dependent into a score for the dependency edge.\nHere we\u2019ll sketch the biaf\ufb01ne algorithm of Dozat and Manning (2017) and Dozat\net al. (2017) shown in Fig. 19.14, drawing on the work of Gr \u00a8unewald et al. (2021)\nwho tested many versions of the algorithm via their STEPS system. The algorithm\n\ufb01rst runs the sentence X=x1;:::;xnthrough an encoder to produce a contextual\nembedding representation for each token R=r1;:::;rn. The embedding for each\ntoken is now passed through two separate feedforward networks, one to produce a\nrepresentation of this token as a head, and one to produce a representation of this\ntoken as a dependent:\nhhead\ni=FFNhead(ri) (19.13)\nhdep\ni=FFNdep(ri) (19.14)\nNow to assign a score to the directed edge i!j, (wiis the head and wjis the depen-\ndent), we feed the head representation of i,hhead\ni, and the dependent representation\nofj,hdep\nj, into a biaf\ufb01ne scoring function:\nScore (i!j) = Biaff(hhead\ni;hdep\nj) (19.15)\nBiaff(x;y) = x|Uy+W(x\by)+b (19.16)",
    "metadata": {
      "source": "19",
      "chunk_id": 25,
      "token_count": 578,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21\n\n19.4 \u2022 E VALUATION 21\nwhere U,W, and bare weights learned by the model. The idea of using a biaf\ufb01ne\nfunction is to allow the system to learn multiplicative interactions between the vec-\ntorsxandy.\nIf we pass Score (i!j)through a softmax, we end up with a probability distri-\nbution, for each token j, over potential heads i(all other tokens in the sentence):\np(i!j) =softmax ([Score (k!j);8k6=j;1\u0014k\u0014n]) (19.17)\nThis probability can then be passed to the maximum spanning tree algorithm of\nSection 19.3.1 to \ufb01nd the best tree.\nThis p(i!j)classi\ufb01er is trained by optimizing the cross-entropy loss.\nNote that the algorithm as we\u2019ve described it is unlabeled. To make this into\na labeled algorithm, the Dozat and Manning (2017) algorithm actually trains two\nclassi\ufb01ers. The \ufb01rst classi\ufb01er, the edge-scorer , the one we described above, assigns\na probability p(i!j)to each word wiandwj. Then the Maximum Spanning Tree\nalgorithm is run to get a single best dependency parse tree for the second. We then\napply a second classi\ufb01er, the label-scorer , whose job is to \ufb01nd the maximum prob-\nability label for each edge in this parse. This second classi\ufb01er has the same form\nas (19.15-19.17), but instead of being trained to predict with binary softmax the\nprobability of an edge existing between two words, it is trained with a softmax over\ndependency labels to predict the dependency label between the words.\n19.4 Evaluation\nAs with phrase structure-based parsing, the evaluation of dependency parsers pro-\nceeds by measuring how well they work on a test set. An obvious metric would be\nexact match (EM)\u2014how many sentences are parsed correctly. This metric is quite\npessimistic, with most sentences being marked wrong. Such measures are not \ufb01ne-\ngrained enough to guide the development process. Our metrics need to be sensitive\nenough to tell if actual improvements are being made.\nFor these reasons, the most common method for evaluating dependency parsers\nare labeled and unlabeled attachment accuracy. Labeled attachment refers to the\nproper assignment of a word to its head along with the correct dependency relation.\nUnlabeled attachment simply looks at the correctness of the assigned head, ignor-\ning the dependency relation. Given a system output and a corresponding reference\nparse, accuracy is simply the percentage of words in an input that are assigned the\ncorrect head with the correct relation. These metrics are usually referred to as the\nlabeled attachment score (LAS) and unlabeled attachment score (UAS). Finally, we\ncan make use of a label accuracy score (LS), the percentage of tokens with correct\nlabels, ignoring where the relations are coming from.\nAs an example, consider the reference parse and system parse for the following\nexample shown in Fig. 19.15.\n(19.18) Book me the \ufb02ight through Houston.\nThe system correctly \ufb01nds 4 of the 6 dependency relations present in the reference\nparse and receives an LAS of 2/3. However, one of the 2 incorrect relations found\nby the system holds between book and\ufb02ight , which are in a head-dependent relation\nin the reference parse; the system therefore achieves a UAS of 5/6.\nBeyond attachment scores, we may also be interested in how well a system is\nperforming on a particular kind of dependency relation, for example NSUBJ , across",
    "metadata": {
      "source": "19",
      "chunk_id": 26,
      "token_count": 790,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22\n\n22 CHAPTER 19 \u2022 D EPENDENCY PARSING\nBook methe \ufb02ight through Houston\n(a) Referenceiobjobj\ndetnmod\ncaseroot\nBook methe \ufb02ight through Houston\n(b) Systemxcomp\nnsubj\ndetnmod\ncaseroot\nFigure 19.15 Reference and system parses for Book me the \ufb02ight through Houston , resulting in an LAS of\n2/3 and an UAS of 5/6.\na development corpus. Here we can make use of the notions of precision and recall\nintroduced in Chapter 17, measuring the percentage of relations labeled NSUBJ by\nthe system that were correct (precision), and the percentage of the NSUBJ relations\npresent in the development set that were in fact discovered by the system (recall).\nWe can employ a confusion matrix to keep track of how often each dependency type\nwas confused for another.\n19.5 Summary\nThis chapter has introduced the concept of dependency grammars and dependency\nparsing. Here\u2019s a summary of the main points that we covered:\n\u2022 In dependency-based approaches to syntax, the structure of a sentence is de-\nscribed in terms of a set of binary relations that hold between the words in a\nsentence. Larger notions of constituency are not directly encoded in depen-\ndency analyses.\n\u2022 The relations in a dependency structure capture the head-dependent relation-\nship among the words in a sentence.\n\u2022 Dependency-based analysis provides information directly useful in further\nlanguage processing tasks including information extraction, semantic parsing\nand question answering.\n\u2022 Transition-based parsing systems employ a greedy stack-based algorithm to\ncreate dependency structures.\n\u2022 Graph-based methods for creating dependency structures are based on the use\nof maximum spanning tree methods from graph theory.\n\u2022 Both transition-based and graph-based approaches are developed using super-\nvised machine learning techniques.\n\u2022 Treebanks provide the data needed to train these systems. Dependency tree-\nbanks can be created directly by human annotators or via automatic transfor-\nmation from phrase-structure treebanks.\n\u2022 Evaluation of dependency parsers is based on labeled and unlabeled accuracy\nscores as measured against withheld development and test corpora.",
    "metadata": {
      "source": "19",
      "chunk_id": 27,
      "token_count": 451,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23",
    "metadata": {
      "source": "19",
      "chunk_id": 28,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 23\nBibliographical and Historical Notes\nThe dependency-based approach to grammar is much older than the relatively recent\nphrase-structure or constituency grammars, which date only to the 20th century. De-\npendency grammar dates back to the Indian grammarian P \u00afan.ini sometime between\nthe 7th and 4th centuries BCE, as well as the ancient Greek linguistic traditions.\nContemporary theories of dependency grammar all draw heavily on the 20th cen-\ntury work of Tesni `ere (1959).\nAutomatic parsing using dependency grammars was \ufb01rst introduced into compu-\ntational linguistics by early work on machine translation at the RAND Corporation\nled by David Hays. This work on dependency parsing closely paralleled work on\nconstituent parsing and made explicit use of grammars to guide the parsing process.\nAfter this early period, computational work on dependency parsing remained inter-\nmittent over the following decades. Notable implementations of dependency parsers\nfor English during this period include Link Grammar (Sleator and Temperley, 1993),\nConstraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003).\nDependency parsing saw a major resurgence in the late 1990\u2019s with the appear-\nance of large dependency-based treebanks and the associated advent of data driven\napproaches described in this chapter. Eisner (1996) developed an ef\ufb01cient dynamic\nprogramming approach to dependency parsing based on bilexical grammars derived\nfrom the Penn Treebank. Covington (2001) introduced the deterministic word by\nword approach underlying current transition-based approaches. Yamada and Mat-\nsumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce\nparadigm and the use of supervised machine learning in the form of support vector\nmachines to dependency parsing.\nTransition-based parsing is based on the shift-reduce parsing algorithm orig-\ninally developed for analyzing programming languages (Aho and Ullman, 1972).\nShift-reduce parsing also makes use of a context-free grammar. Input tokens are\nsuccessively shifted onto the stack and the top two elements of the stack are matched\nagainst the right-hand side of the rules in the grammar; when a match is found the\nmatched elements are replaced on the stack (reduced) by the non-terminal from the\nleft-hand side of the rule being matched. In transition-based dependency parsing\nwe skip the grammar, and alter the reduce operation to add a dependency relation\nbetween a word and its head.\nNivre (2003) de\ufb01ned the modern, deterministic, transition-based approach to\ndependency parsing. Subsequent work by Nivre and his colleagues formalized and\nanalyzed the performance of numerous transition systems, training methods, and\nmethods for dealing with non-projective language (Nivre and Scholz 2004, Nivre\n2006, Nivre and Nilsson 2005, Nivre et al. 2007b, Nivre 2007). The neural ap-\nproach was pioneered by Chen and Manning (2014) and extended by Kiperwasser\nand Goldberg (2016); Kulmizev et al. (2019).\nThe graph-based maximum spanning tree approach to dependency parsing was\nintroduced by McDonald et al. 2005a, McDonald et al. 2005b. The neural classi\ufb01er\nwas introduced by (Kiperwasser and Goldberg, 2016).\nThe long-running Prague Dependency Treebank project (Haji \u02c7c, 1998) is the most",
    "metadata": {
      "source": "19",
      "chunk_id": 29,
      "token_count": 763,
      "chapter_title": ""
    }
  },
  {
    "content": "against the right-hand side of the rules in the grammar; when a match is found the\nmatched elements are replaced on the stack (reduced) by the non-terminal from the\nleft-hand side of the rule being matched. In transition-based dependency parsing\nwe skip the grammar, and alter the reduce operation to add a dependency relation\nbetween a word and its head.\nNivre (2003) de\ufb01ned the modern, deterministic, transition-based approach to\ndependency parsing. Subsequent work by Nivre and his colleagues formalized and\nanalyzed the performance of numerous transition systems, training methods, and\nmethods for dealing with non-projective language (Nivre and Scholz 2004, Nivre\n2006, Nivre and Nilsson 2005, Nivre et al. 2007b, Nivre 2007). The neural ap-\nproach was pioneered by Chen and Manning (2014) and extended by Kiperwasser\nand Goldberg (2016); Kulmizev et al. (2019).\nThe graph-based maximum spanning tree approach to dependency parsing was\nintroduced by McDonald et al. 2005a, McDonald et al. 2005b. The neural classi\ufb01er\nwas introduced by (Kiperwasser and Goldberg, 2016).\nThe long-running Prague Dependency Treebank project (Haji \u02c7c, 1998) is the most\nsigni\ufb01cant effort to directly annotate a corpus with multiple layers of morphological,\nsyntactic and semantic information. PDT 3.0 contains over 1.5 M tokens (Bej \u02c7cek\net al., 2013).\nUniversal Dependencies (UD) (de Marneffe et al., 2021) is an open community",
    "metadata": {
      "source": "19",
      "chunk_id": 30,
      "token_count": 370,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24\n\n24 CHAPTER 19 \u2022 D EPENDENCY PARSING\nproject to create a framework for dependency treebank annotation, with nearly 200\ntreebanks in over 100 languages. The UD annotation scheme evolved out of several\ndistinct efforts including Stanford dependencies (de Marneffe et al. 2006, de Marn-\neffe and Manning 2008, de Marneffe et al. 2014), Google\u2019s universal part-of-speech\ntags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets\n(Zeman, 2008).\nThe Conference on Natural Language Learning (CoNLL) has conducted an in-\n\ufb02uential series of shared tasks related to dependency parsing over the years (Buch-\nholz and Marsi 2006, Nivre et al. 2007a, Surdeanu et al. 2008, Haji \u02c7c et al. 2009).\nMore recent evaluations have focused on parser robustness with respect to morpho-\nlogically rich languages (Seddah et al., 2013), and non-canonical language forms\nsuch as social media, texts, and spoken language (Petrov and McDonald, 2012).\nChoi et al. (2015) presents a performance analysis of 10 dependency parsers across\na range of metrics, as well as DEPEND ABLE, a robust parser evaluation tool.\nExercises",
    "metadata": {
      "source": "19",
      "chunk_id": 31,
      "token_count": 312,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 25",
    "metadata": {
      "source": "19",
      "chunk_id": 32,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Exercises 25\nAho, A. V . and J. D. Ullman. 1972. The Theory of Parsing,\nTranslation, and Compiling , volume 1. Prentice Hall.\nBej\u02c7cek, E., E. Haji \u02c7cov\u00b4a, J. Haji \u02c7c, P. J \u00b4\u0131nov\u00b4a, V . Kettnerov \u00b4a,\nV . Kol \u00b4a\u02c7rov\u00b4a, M. Mikulov \u00b4a, J. M \u00b4\u0131rovsk \u00b4y, A. Nedoluzhko,\nJ. Panevov \u00b4a, L. Pol \u00b4akov \u00b4a, M. \u02c7Sev\u02c7c\u00b4\u0131kov\u00b4a, J. \u02c7St\u02c7ep\u00b4anek,\nand \u02c7S. Zik \u00b4anov \u00b4a. 2013. Prague dependency treebank\n3.0. Technical report, Institute of Formal and Ap-\nplied Linguistics, Charles University in Prague. LIN-\nDAT/CLARIN digital library at Institute of Formal and\nApplied Linguistics, Charles University in Prague.\nBhat, I., R. A. Bhat, M. Shrivastava, and D. Sharma. 2017.\nJoining hands: Exploiting monolingual treebanks for\nparsing of code-mixing data. EACL .\nBuchholz, S. and E. Marsi. 2006. Conll-x shared task on\nmultilingual dependency parsing. CoNLL .\nChen, D. and C. Manning. 2014. A fast and accurate depen-\ndency parser using neural networks. EMNLP .\nChoi, J. D. and M. Palmer. 2011a. Getting the most out of\ntransition-based dependency parsing. ACL.\nChoi, J. D. and M. Palmer. 2011b. Transition-based semantic\nrole labeling using predicate argument clustering. Pro-\nceedings of the ACL 2011 Workshop on Relational Mod-\nels of Semantics .\nChoi, J. D., J. Tetreault, and A. Stent. 2015. It depends:\nDependency parser comparison using a web-based evalu-\nation tool. ACL.\nChu, Y .-J. and T.-H. Liu. 1965. On the shortest arborescence\nof a directed graph. Science Sinica , 14:1396\u20131400.\nCovington, M. 2001. A fundamental algorithm for depen-\ndency parsing. Proceedings of the 39th Annual ACM\nSoutheast Conference .\nDozat, T. and C. D. Manning. 2017. Deep biaf\ufb01ne attention\nfor neural dependency parsing. ICLR .\nDozat, T. and C. D. Manning. 2018. Simpler but more accu-\nrate semantic dependency parsing. ACL.\nDozat, T., P. Qi, and C. D. Manning. 2017. Stanford\u2019s\ngraph-based neural dependency parser at the CoNLL\n2017 shared task. Proceedings of the CoNLL 2017 Shared\nTask: Multilingual Parsing from Raw Text to Universal\nDependencies .\nEdmonds, J. 1967. Optimum branchings. Journal of Re-\nsearch of the National Bureau of Standards B , 71(4):233\u2013\n240.\nEisner, J. 1996. Three new probabilistic models for depen-\ndency parsing: An exploration. COLING .",
    "metadata": {
      "source": "19",
      "chunk_id": 33,
      "token_count": 765,
      "chapter_title": ""
    }
  },
  {
    "content": "ation tool. ACL.\nChu, Y .-J. and T.-H. Liu. 1965. On the shortest arborescence\nof a directed graph. Science Sinica , 14:1396\u20131400.\nCovington, M. 2001. A fundamental algorithm for depen-\ndency parsing. Proceedings of the 39th Annual ACM\nSoutheast Conference .\nDozat, T. and C. D. Manning. 2017. Deep biaf\ufb01ne attention\nfor neural dependency parsing. ICLR .\nDozat, T. and C. D. Manning. 2018. Simpler but more accu-\nrate semantic dependency parsing. ACL.\nDozat, T., P. Qi, and C. D. Manning. 2017. Stanford\u2019s\ngraph-based neural dependency parser at the CoNLL\n2017 shared task. Proceedings of the CoNLL 2017 Shared\nTask: Multilingual Parsing from Raw Text to Universal\nDependencies .\nEdmonds, J. 1967. Optimum branchings. Journal of Re-\nsearch of the National Bureau of Standards B , 71(4):233\u2013\n240.\nEisner, J. 1996. Three new probabilistic models for depen-\ndency parsing: An exploration. COLING .\nGabow, H. N., Z. Galil, T. Spencer, and R. E. Tarjan.\n1986. Ef\ufb01cient algorithms for \ufb01nding minimum spanning\ntrees in undirected and directed graphs. Combinatorica ,\n6(2):109\u2013122.\nGr\u00a8unewald, S., A. Friedrich, and J. Kuhn. 2021. Applying\nOccam\u2019s razor to transformer-based dependency parsing:\nWhat works, what doesn\u2019t, and what is really necessary.\nIWPT .\nHaji\u02c7c, J. 1998. Building a Syntactically Annotated Corpus:\nThe Prague Dependency Treebank , 106\u2013132. Karolinum.Haji\u02c7c, J., M. Ciaramita, R. Johansson, D. Kawahara, M. A.\nMart \u00b4\u0131, L. M `arquez, A. Meyers, J. Nivre, S. Pad \u00b4o,\nJ.\u02c7St\u02c7ep\u00b4anek, P. Stran \u02c7a\u00b4k, M. Surdeanu, N. Xue, and\nY . Zhang. 2009. The conll-2009 shared task: Syntac-\ntic and semantic dependencies in multiple languages.\nCoNLL .\nKarlsson, F., A. V outilainen, J. Heikkil \u00a8a, and A. Anttila, eds.\n1995. Constraint Grammar: A Language-Independent\nSystem for Parsing Unrestricted Text . Mouton de Gruyter.\nKiperwasser, E. and Y . Goldberg. 2016. Simple and accu-\nrate dependency parsing using bidirectional LSTM fea-\nture representations. TACL , 4:313\u2013327.\nKudo, T. and Y . Matsumoto. 2002. Japanese dependency\nanalysis using cascaded chunking. CoNLL .\nKulmizev, A., M. de Lhoneux, J. Gontrum, E. Fano, and\nJ. Nivre. 2019. Deep contextualized word embeddings\nin transition-based and graph-based dependency parsing\n- a tale of two parsers revisited. EMNLP .\nLin, D. 2003. Dependency-based evaluation of minipar.",
    "metadata": {
      "source": "19",
      "chunk_id": 34,
      "token_count": 767,
      "chapter_title": ""
    }
  },
  {
    "content": "J.\u02c7St\u02c7ep\u00b4anek, P. Stran \u02c7a\u00b4k, M. Surdeanu, N. Xue, and\nY . Zhang. 2009. The conll-2009 shared task: Syntac-\ntic and semantic dependencies in multiple languages.\nCoNLL .\nKarlsson, F., A. V outilainen, J. Heikkil \u00a8a, and A. Anttila, eds.\n1995. Constraint Grammar: A Language-Independent\nSystem for Parsing Unrestricted Text . Mouton de Gruyter.\nKiperwasser, E. and Y . Goldberg. 2016. Simple and accu-\nrate dependency parsing using bidirectional LSTM fea-\nture representations. TACL , 4:313\u2013327.\nKudo, T. and Y . Matsumoto. 2002. Japanese dependency\nanalysis using cascaded chunking. CoNLL .\nKulmizev, A., M. de Lhoneux, J. Gontrum, E. Fano, and\nJ. Nivre. 2019. Deep contextualized word embeddings\nin transition-based and graph-based dependency parsing\n- a tale of two parsers revisited. EMNLP .\nLin, D. 2003. Dependency-based evaluation of minipar.\nWorkshop on the Evaluation of Parsing Systems .\nde Marneffe, M.-C., T. Dozat, N. Silveira, K. Haverinen,\nF. Ginter, J. Nivre, and C. D. Manning. 2014. Univer-\nsal Stanford dependencies: A cross-linguistic typology.\nLREC .\nde Marneffe, M.-C., B. MacCartney, and C. D. Manning.\n2006. Generating typed dependency parses from phrase\nstructure parses. LREC .\nde Marneffe, M.-C. and C. D. Manning. 2008. The Stanford\ntyped dependencies representation. COLING Workshop\non Cross-Framework and Cross-Domain Parser Evalua-\ntion.\nde Marneffe, M.-C., C. D. Manning, J. Nivre, and D. Zeman.\n2021. Universal Dependencies. Computational Linguis-\ntics, 47(2):255\u2013308.\nMcDonald, R., K. Crammer, and F. C. N. Pereira. 2005a. On-\nline large-margin training of dependency parsers. ACL.\nMcDonald, R. and J. Nivre. 2011. Analyzing and inte-\ngrating dependency parsers. Computational Linguistics ,\n37(1):197\u2013230.\nMcDonald, R., F. C. N. Pereira, K. Ribarov, and J. Haji \u02c7c.\n2005b. Non-projective dependency parsing using span-\nning tree algorithms. HLT-EMNLP .\nNivre, J. 2007. Incremental non-projective dependency pars-\ning. NAACL-HLT .\nNivre, J. 2003. An ef\ufb01cient algorithm for projective depen-\ndency parsing. Proceedings of the 8th International Work-\nshop on Parsing Technologies (IWPT) .\nNivre, J. 2006. Inductive Dependency Parsing . Springer.\nNivre, J. 2009. Non-projective dependency parsing in ex-\npected linear time. ACL IJCNLP .\nNivre, J., J. Hall, S. K \u00a8ubler, R. McDonald, J. Nilsson,",
    "metadata": {
      "source": "19",
      "chunk_id": 35,
      "token_count": 752,
      "chapter_title": ""
    }
  },
  {
    "content": "tics, 47(2):255\u2013308.\nMcDonald, R., K. Crammer, and F. C. N. Pereira. 2005a. On-\nline large-margin training of dependency parsers. ACL.\nMcDonald, R. and J. Nivre. 2011. Analyzing and inte-\ngrating dependency parsers. Computational Linguistics ,\n37(1):197\u2013230.\nMcDonald, R., F. C. N. Pereira, K. Ribarov, and J. Haji \u02c7c.\n2005b. Non-projective dependency parsing using span-\nning tree algorithms. HLT-EMNLP .\nNivre, J. 2007. Incremental non-projective dependency pars-\ning. NAACL-HLT .\nNivre, J. 2003. An ef\ufb01cient algorithm for projective depen-\ndency parsing. Proceedings of the 8th International Work-\nshop on Parsing Technologies (IWPT) .\nNivre, J. 2006. Inductive Dependency Parsing . Springer.\nNivre, J. 2009. Non-projective dependency parsing in ex-\npected linear time. ACL IJCNLP .\nNivre, J., J. Hall, S. K \u00a8ubler, R. McDonald, J. Nilsson,\nS. Riedel, and D. Yuret. 2007a. The conll 2007 shared\ntask on dependency parsing. EMNLP/CoNLL .\nNivre, J., J. Hall, J. Nilsson, A. Chanev, G. Eryigit,\nS. K \u00a8ubler, S. Marinov, and E. Marsi. 2007b. Malt-\nparser: A language-independent system for data-driven\ndependency parsing. Natural Language Engineering ,\n13(02):95\u2013135.",
    "metadata": {
      "source": "19",
      "chunk_id": 36,
      "token_count": 391,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 26\n\n26 Chapter 19 \u2022 Dependency Parsing\nNivre, J. and J. Nilsson. 2005. Pseudo-projective dependency\nparsing. ACL.\nNivre, J. and M. Scholz. 2004. Deterministic dependency\nparsing of english text. COLING .\nPetrov, S., D. Das, and R. McDonald. 2012. A universal\npart-of-speech tagset. LREC .\nPetrov, S. and R. McDonald. 2012. Overview of the 2012\nshared task on parsing the web. Notes of the First Work-\nshop on Syntactic Analysis of Non-Canonical Language\n(SANCL) , volume 59.\nSeddah, D., R. Tsarfaty, S. K \u00a8ubler, M. Candito, J. D. Choi,\nR. Farkas, J. Foster, I. Goenaga, K. Gojenola, Y . Gold-\nberg, S. Green, N. Habash, M. Kuhlmann, W. Maier,\nJ. Nivre, A. Przepi \u00b4orkowski, R. Roth, W. Seeker, Y . Vers-\nley, V . Vincze, M. Woli \u00b4nski, A. Wr \u00b4oblewska, and E. Ville-\nmonte de la Cl \u00b4ergerie. 2013. Overview of the SPMRL\n2013 shared task: cross-framework evaluation of parsing\nmorphologically rich languages. 4th Workshop on Statis-\ntical Parsing of Morphologically-Rich Languages .\nSleator, D. and D. Temperley. 1993. Parsing English with a\nlink grammar. IWPT-93 .\nSurdeanu, M., R. Johansson, A. Meyers, L. M `arquez, and\nJ. Nivre. 2008. The CoNLL 2008 shared task on joint\nparsing of syntactic and semantic dependencies. CoNLL .\nTesni `ere, L. 1959. \u00b4El\u00b4ements de Syntaxe Structurale . Librairie\nC. Klincksieck, Paris.\nYamada, H. and Y . Matsumoto. 2003. Statistical dependency\nanalysis with support vector machines. IWPT-03 .\nZeman, D. 2008. Reusable tagset conversion using tagset\ndrivers. LREC .",
    "metadata": {
      "source": "19",
      "chunk_id": 37,
      "token_count": 531,
      "chapter_title": ""
    }
  }
]