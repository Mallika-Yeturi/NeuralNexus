[
  {
    "content": "# ed3book_Jan25\n\n## Page 1\n\nSpeech and Language Processing\nAn Introduction to Natural Language Processing,\nComputational Linguistics, and Speech Recognition\nwith Language Models\nThird Edition draft\nDaniel Jurafsky\nStanford University\nJames H. Martin\nUniversity of Colorado at Boulder\nCopyright \u00a92024. All rights reserved.\nDraft of January 12, 2025. Comments and typos welcome!",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 0,
      "token_count": 87,
      "chapter_title": "ed3book_Jan25"
    }
  },
  {
    "content": "## Page 2",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Summary of Contents\nI Fundamental Algorithms for NLP 1\n1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2 Regular Expressions, Tokenization, Edit Distance . . . . . . . . . . . . . . . 4\n3 N-gram Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n4 Naive Bayes, Text Classi\ufb01cation, and Sentiment . . . . . . . . . . . . . . . . . 56\n5 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n6 Vector Semantics and Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n7 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132\n8 RNNs and LSTMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\n9 The Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184\n10 Large Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\n11 Masked Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n12 Model Alignment, Prompting, and In-Context Learning . . . . . . . . . 242\nII NLP Applications 261\n13 Machine Translation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\n14 Question Answering, Information Retrieval, and RAG . . . . . . . . . . 289\n15 Chatbots & Dialogue Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309\n16 Automatic Speech Recognition and Text-to-Speech . . . . . . . . . . . . . . 331\nIII Annotating Linguistic Structure 359\n17 Sequence Labeling for Parts of Speech and Named Entities . . . . . . 362\n18 Context-Free Grammars and Constituency Parsing . . . . . . . . . . . . . 387\n19 Dependency Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 2,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "12 Model Alignment, Prompting, and In-Context Learning . . . . . . . . . 242\nII NLP Applications 261\n13 Machine Translation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\n14 Question Answering, Information Retrieval, and RAG . . . . . . . . . . 289\n15 Chatbots & Dialogue Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309\n16 Automatic Speech Recognition and Text-to-Speech . . . . . . . . . . . . . . 331\nIII Annotating Linguistic Structure 359\n17 Sequence Labeling for Parts of Speech and Named Entities . . . . . . 362\n18 Context-Free Grammars and Constituency Parsing . . . . . . . . . . . . . 387\n19 Dependency Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411\n20 Information Extraction: Relations, Events, and Time. . . . . . . . . . . . 435\n21 Semantic Role Labeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461\n22 Lexicons for Sentiment, Affect, and Connotation . . . . . . . . . . . . . . . . 481\n23 Coreference Resolution and Entity Linking . . . . . . . . . . . . . . . . . . . . . 501\n24 Discourse Coherence. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 531\nBibliography. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 553\nSubject Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585\n2",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 3,
      "token_count": 586,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 4,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Contents\nI Fundamental Algorithms for NLP 1\n1 Introduction 3\n2 Regular Expressions, Tokenization, Edit Distance 4\n2.1 Regular Expressions . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2 Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.3 Corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.4 Simple Unix Tools for Word Tokenization . . . . . . . . . . . . . 16\n2.5 Word and Subword Tokenization . . . . . . . . . . . . . . . . . . 18\n2.6 Word Normalization, Lemmatization and Stemming . . . . . . . . 23\n2.7 Sentence Segmentation . . . . . . . . . . . . . . . . . . . . . . . 24\n2.8 Minimum Edit Distance . . . . . . . . . . . . . . . . . . . . . . . 25\n2.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 30\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n3 N-gram Language Models 32\n3.1 N-Grams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n3.2 Evaluating Language Models: Training and Test Sets . . . . . . . 38\n3.3 Evaluating Language Models: Perplexity . . . . . . . . . . . . . . 40\n3.4 Sampling sentences from a language model . . . . . . . . . . . . . 42\n3.5 Generalizing vs. over\ufb01tting the training set . . . . . . . . . . . . . 43\n3.6 Smoothing, Interpolation, and Backoff . . . . . . . . . . . . . . . 45\n3.7 Advanced: Perplexity\u2019s Relation to Entropy . . . . . . . . . . . . 49\n3.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 52\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n4 Naive Bayes, Text Classi\ufb01cation, and Sentiment 56\n4.1 Naive Bayes Classi\ufb01ers . . . . . . . . . . . . . . . . . . . . . . . 57",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 5,
      "token_count": 766,
      "chapter_title": ""
    }
  },
  {
    "content": "3.4 Sampling sentences from a language model . . . . . . . . . . . . . 42\n3.5 Generalizing vs. over\ufb01tting the training set . . . . . . . . . . . . . 43\n3.6 Smoothing, Interpolation, and Backoff . . . . . . . . . . . . . . . 45\n3.7 Advanced: Perplexity\u2019s Relation to Entropy . . . . . . . . . . . . 49\n3.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 52\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n4 Naive Bayes, Text Classi\ufb01cation, and Sentiment 56\n4.1 Naive Bayes Classi\ufb01ers . . . . . . . . . . . . . . . . . . . . . . . 57\n4.2 Training the Naive Bayes Classi\ufb01er . . . . . . . . . . . . . . . . . 60\n4.3 Worked example . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.4 Optimizing for Sentiment Analysis . . . . . . . . . . . . . . . . . 62\n4.5 Naive Bayes for other text classi\ufb01cation tasks . . . . . . . . . . . 64\n4.6 Naive Bayes as a Language Model . . . . . . . . . . . . . . . . . 65\n4.7 Evaluation: Precision, Recall, F-measure . . . . . . . . . . . . . . 66\n4.8 Test sets and Cross-validation . . . . . . . . . . . . . . . . . . . . 69\n4.9 Statistical Signi\ufb01cance Testing . . . . . . . . . . . . . . . . . . . 70\n4.10 Avoiding Harms in Classi\ufb01cation . . . . . . . . . . . . . . . . . . 73\n4.11 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 75\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\n5 Logistic Regression 77\n5.1 The sigmoid function . . . . . . . . . . . . . . . . . . . . . . . . 78\n5.2 Classi\ufb01cation with Logistic Regression . . . . . . . . . . . . . . . 80\n5.3 Multinomial logistic regression . . . . . . . . . . . . . . . . . . . 84\n5.4 Learning in Logistic Regression . . . . . . . . . . . . . . . . . . . 87\n3",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 6,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 7,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "4CONTENTS\n5.5 The cross-entropy loss function . . . . . . . . . . . . . . . . . . . 88\n5.6 Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n5.7 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n5.8 Learning in Multinomial Logistic Regression . . . . . . . . . . . . 97\n5.9 Interpreting models . . . . . . . . . . . . . . . . . . . . . . . . . 98\n5.10 Advanced: Deriving the Gradient Equation . . . . . . . . . . . . . 98\n5.11 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 100\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n6 Vector Semantics and Embeddings 101\n6.1 Lexical Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n6.2 Vector Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n6.3 Words and Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n6.4 Cosine for measuring similarity . . . . . . . . . . . . . . . . . . . 110\n6.5 TF-IDF: Weighing terms in the vector . . . . . . . . . . . . . . . 111\n6.6 Pointwise Mutual Information (PMI) . . . . . . . . . . . . . . . . 114\n6.7 Applications of the tf-idf or PPMI vector models . . . . . . . . . . 116\n6.8 Word2vec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n6.9 Visualizing Embeddings . . . . . . . . . . . . . . . . . . . . . . . 123\n6.10 Semantic properties of embeddings . . . . . . . . . . . . . . . . . 124\n6.11 Bias and Embeddings . . . . . . . . . . . . . . . . . . . . . . . . 126\n6.12 Evaluating Vector Models . . . . . . . . . . . . . . . . . . . . . . 127\n6.13 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 129",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 8,
      "token_count": 766,
      "chapter_title": ""
    }
  },
  {
    "content": "6.6 Pointwise Mutual Information (PMI) . . . . . . . . . . . . . . . . 114\n6.7 Applications of the tf-idf or PPMI vector models . . . . . . . . . . 116\n6.8 Word2vec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n6.9 Visualizing Embeddings . . . . . . . . . . . . . . . . . . . . . . . 123\n6.10 Semantic properties of embeddings . . . . . . . . . . . . . . . . . 124\n6.11 Bias and Embeddings . . . . . . . . . . . . . . . . . . . . . . . . 126\n6.12 Evaluating Vector Models . . . . . . . . . . . . . . . . . . . . . . 127\n6.13 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 129\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\n7 Neural Networks 132\n7.1 Units . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\n7.2 The XOR problem . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n7.3 Feedforward Neural Networks . . . . . . . . . . . . . . . . . . . . 138\n7.4 Feedforward networks for NLP: Classi\ufb01cation . . . . . . . . . . . 142\n7.5 Training Neural Nets . . . . . . . . . . . . . . . . . . . . . . . . 145\n7.6 Feedforward Neural Language Modeling . . . . . . . . . . . . . . 152\n7.7 Training the neural language model . . . . . . . . . . . . . . . . . 155\n7.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 157\n8 RNNs and LSTMs 158\n8.1 Recurrent Neural Networks . . . . . . . . . . . . . . . . . . . . . 158\n8.2 RNNs as Language Models . . . . . . . . . . . . . . . . . . . . . 162\n8.3 RNNs for other NLP tasks . . . . . . . . . . . . . . . . . . . . . . 165\n8.4 Stacked and Bidirectional RNN architectures . . . . . . . . . . . . 168\n8.5 The LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 9,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "7.6 Feedforward Neural Language Modeling . . . . . . . . . . . . . . 152\n7.7 Training the neural language model . . . . . . . . . . . . . . . . . 155\n7.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 157\n8 RNNs and LSTMs 158\n8.1 Recurrent Neural Networks . . . . . . . . . . . . . . . . . . . . . 158\n8.2 RNNs as Language Models . . . . . . . . . . . . . . . . . . . . . 162\n8.3 RNNs for other NLP tasks . . . . . . . . . . . . . . . . . . . . . . 165\n8.4 Stacked and Bidirectional RNN architectures . . . . . . . . . . . . 168\n8.5 The LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n8.6 Summary: Common RNN NLP Architectures . . . . . . . . . . . 174\n8.7 The Encoder-Decoder Model with RNNs . . . . . . . . . . . . . . 174\n8.8 Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\n8.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 182\n9 The Transformer 184\n9.1 Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 10,
      "token_count": 498,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 11,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "CONTENTS 5\n9.2 Transformer Blocks . . . . . . . . . . . . . . . . . . . . . . . . . 191\n9.3 Parallelizing computation using a single matrix X. . . . . . . . . 194\n9.4 The input: embeddings for token and position . . . . . . . . . . . 197\n9.5 The Language Modeling Head . . . . . . . . . . . . . . . . . . . 199\n9.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 202\n10 Large Language Models 203\n10.1 Large Language Models with Transformers . . . . . . . . . . . . . 204\n10.2 Sampling for LLM Generation . . . . . . . . . . . . . . . . . . . 207\n10.3 Pretraining Large Language Models . . . . . . . . . . . . . . . . 210\n10.4 Evaluating Large Language Models . . . . . . . . . . . . . . . . . 214\n10.5 Dealing with Scale . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n10.6 Potential Harms from Language Models . . . . . . . . . . . . . . 219\n10.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 220\n11 Masked Language Models 223\n11.1 Bidirectional Transformer Encoders . . . . . . . . . . . . . . . . . 223\n11.2 Training Bidirectional Encoders . . . . . . . . . . . . . . . . . . . 226\n11.3 Contextual Embeddings . . . . . . . . . . . . . . . . . . . . . . . 231\n11.4 Fine-Tuning for Classi\ufb01cation . . . . . . . . . . . . . . . . . . . . 235\n11.5 Fine-Tuning for Sequence Labelling: Named Entity Recognition . 237\n11.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 241\n12 Model Alignment, Prompting, and In-Context Learning 242\n12.1 Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243\n12.2 Post-training and Model Alignment . . . . . . . . . . . . . . . . . 248\n12.3 Model Alignment: Instruction Tuning . . . . . . . . . . . . . . . . 249\n12.4 Chain-of-Thought Prompting . . . . . . . . . . . . . . . . . . . . 254",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 12,
      "token_count": 786,
      "chapter_title": ""
    }
  },
  {
    "content": "11.3 Contextual Embeddings . . . . . . . . . . . . . . . . . . . . . . . 231\n11.4 Fine-Tuning for Classi\ufb01cation . . . . . . . . . . . . . . . . . . . . 235\n11.5 Fine-Tuning for Sequence Labelling: Named Entity Recognition . 237\n11.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 241\n12 Model Alignment, Prompting, and In-Context Learning 242\n12.1 Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243\n12.2 Post-training and Model Alignment . . . . . . . . . . . . . . . . . 248\n12.3 Model Alignment: Instruction Tuning . . . . . . . . . . . . . . . . 249\n12.4 Chain-of-Thought Prompting . . . . . . . . . . . . . . . . . . . . 254\n12.5 Automatic Prompt Optimization . . . . . . . . . . . . . . . . . . . 254\n12.6 Evaluating Prompted Language Models . . . . . . . . . . . . . . . 258\n12.7 Model Alignment with Human Preferences: RLHF and DPO . . . 258\n12.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 259\nII NLP Applications 261\n13 Machine Translation 263\n13.1 Language Divergences and Typology . . . . . . . . . . . . . . . . 264\n13.2 Machine Translation using Encoder-Decoder . . . . . . . . . . . . 268\n13.3 Details of the Encoder-Decoder Model . . . . . . . . . . . . . . . 272\n13.4 Decoding in MT: Beam Search . . . . . . . . . . . . . . . . . . . 274\n13.5 Translating in low-resource situations . . . . . . . . . . . . . . . . 278\n13.6 MT Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280\n13.7 Bias and Ethical Issues . . . . . . . . . . . . . . . . . . . . . . . 284\n13.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 286\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288\n14 Question Answering, Information Retrieval, and RAG 289",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 13,
      "token_count": 782,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 14,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "6CONTENTS\n14.1 Information Retrieval . . . . . . . . . . . . . . . . . . . . . . . . 290\n14.2 Information Retrieval with Dense Vectors . . . . . . . . . . . . . . 298\n14.3 Answering Questions with RAG . . . . . . . . . . . . . . . . . . 301\n14.4 Evaluating Question Answering . . . . . . . . . . . . . . . . . . . 304\n14.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 306\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308\n15 Chatbots & Dialogue Systems 309\n15.1 Properties of Human Conversation . . . . . . . . . . . . . . . . . 311\n15.2 Frame-Based Dialogue Systems . . . . . . . . . . . . . . . . . . . 314\n15.3 Dialogue Acts and Dialogue State . . . . . . . . . . . . . . . . . . 317\n15.4 Chatbots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321\n15.5 Dialogue System Design . . . . . . . . . . . . . . . . . . . . . . . 325\n15.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 328\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330\n16 Automatic Speech Recognition and Text-to-Speech 331\n16.1 The Automatic Speech Recognition Task . . . . . . . . . . . . . . 332\n16.2 Feature Extraction for ASR: Log Mel Spectrum . . . . . . . . . . 334\n16.3 Speech Recognition Architecture . . . . . . . . . . . . . . . . . . 339\n16.4 CTC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\n16.5 ASR Evaluation: Word Error Rate . . . . . . . . . . . . . . . . . 346\n16.6 TTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348\n16.7 Other Speech Tasks . . . . . . . . . . . . . . . . . . . . . . . . . 353\n16.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 15,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "16 Automatic Speech Recognition and Text-to-Speech 331\n16.1 The Automatic Speech Recognition Task . . . . . . . . . . . . . . 332\n16.2 Feature Extraction for ASR: Log Mel Spectrum . . . . . . . . . . 334\n16.3 Speech Recognition Architecture . . . . . . . . . . . . . . . . . . 339\n16.4 CTC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\n16.5 ASR Evaluation: Word Error Rate . . . . . . . . . . . . . . . . . 346\n16.6 TTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348\n16.7 Other Speech Tasks . . . . . . . . . . . . . . . . . . . . . . . . . 353\n16.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 354\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357\nIII Annotating Linguistic Structure 359\n17 Sequence Labeling for Parts of Speech and Named Entities 362\n17.1 (Mostly) English Word Classes . . . . . . . . . . . . . . . . . . . 363\n17.2 Part-of-Speech Tagging . . . . . . . . . . . . . . . . . . . . . . . 365\n17.3 Named Entities and Named Entity Tagging . . . . . . . . . . . . . 367\n17.4 HMM Part-of-Speech Tagging . . . . . . . . . . . . . . . . . . . 369\n17.5 Conditional Random Fields (CRFs) . . . . . . . . . . . . . . . . . 376\n17.6 Evaluation of Named Entity Recognition . . . . . . . . . . . . . . 381\n17.7 Further Details . . . . . . . . . . . . . . . . . . . . . . . . . . . 381\n17.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 384\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385\n18 Context-Free Grammars and Constituency Parsing 387\n18.1 Constituency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388\n18.2 Context-Free Grammars . . . . . . . . . . . . . . . . . . . . . . . 388",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 16,
      "token_count": 770,
      "chapter_title": ""
    }
  },
  {
    "content": "17.5 Conditional Random Fields (CRFs) . . . . . . . . . . . . . . . . . 376\n17.6 Evaluation of Named Entity Recognition . . . . . . . . . . . . . . 381\n17.7 Further Details . . . . . . . . . . . . . . . . . . . . . . . . . . . 381\n17.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 384\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385\n18 Context-Free Grammars and Constituency Parsing 387\n18.1 Constituency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388\n18.2 Context-Free Grammars . . . . . . . . . . . . . . . . . . . . . . . 388\n18.3 Treebanks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392\n18.4 Grammar Equivalence and Normal Form . . . . . . . . . . . . . . 394\n18.5 Ambiguity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395\n18.6 CKY Parsing: A Dynamic Programming Approach . . . . . . . . 397\n18.7 Span-Based Neural Constituency Parsing . . . . . . . . . . . . . . 403",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 17,
      "token_count": 431,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 18,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "CONTENTS 7\n18.8 Evaluating Parsers . . . . . . . . . . . . . . . . . . . . . . . . . . 405\n18.9 Heads and Head-Finding . . . . . . . . . . . . . . . . . . . . . . 406\n18.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 408\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409\n19 Dependency Parsing 411\n19.1 Dependency Relations . . . . . . . . . . . . . . . . . . . . . . . . 412\n19.2 Transition-Based Dependency Parsing . . . . . . . . . . . . . . . 416\n19.3 Graph-Based Dependency Parsing . . . . . . . . . . . . . . . . . 425\n19.4 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431\n19.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 433\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 434\n20 Information Extraction: Relations, Events, and Time 435\n20.1 Relation Extraction . . . . . . . . . . . . . . . . . . . . . . . . . 436\n20.2 Relation Extraction Algorithms . . . . . . . . . . . . . . . . . . . 438\n20.3 Extracting Events . . . . . . . . . . . . . . . . . . . . . . . . . . 446\n20.4 Representing Time . . . . . . . . . . . . . . . . . . . . . . . . . . 447\n20.5 Representing Aspect . . . . . . . . . . . . . . . . . . . . . . . . . 450\n20.6 Temporally Annotated Datasets: TimeBank . . . . . . . . . . . . . 451\n20.7 Automatic Temporal Analysis . . . . . . . . . . . . . . . . . . . . 452\n20.8 Template Filling . . . . . . . . . . . . . . . . . . . . . . . . . . . 456\n20.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 459\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 19,
      "token_count": 791,
      "chapter_title": ""
    }
  },
  {
    "content": "20.4 Representing Time . . . . . . . . . . . . . . . . . . . . . . . . . . 447\n20.5 Representing Aspect . . . . . . . . . . . . . . . . . . . . . . . . . 450\n20.6 Temporally Annotated Datasets: TimeBank . . . . . . . . . . . . . 451\n20.7 Automatic Temporal Analysis . . . . . . . . . . . . . . . . . . . . 452\n20.8 Template Filling . . . . . . . . . . . . . . . . . . . . . . . . . . . 456\n20.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 459\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460\n21 Semantic Role Labeling 461\n21.1 Semantic Roles . . . . . . . . . . . . . . . . . . . . . . . . . . . 462\n21.2 Diathesis Alternations . . . . . . . . . . . . . . . . . . . . . . . . 462\n21.3 Semantic Roles: Problems with Thematic Roles . . . . . . . . . . 464\n21.4 The Proposition Bank . . . . . . . . . . . . . . . . . . . . . . . . 465\n21.5 FrameNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466\n21.6 Semantic Role Labeling . . . . . . . . . . . . . . . . . . . . . . . 468\n21.7 Selectional Restrictions . . . . . . . . . . . . . . . . . . . . . . . 472\n21.8 Primitive Decomposition of Predicates . . . . . . . . . . . . . . . 476\n21.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 478\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 480\n22 Lexicons for Sentiment, Affect, and Connotation 481\n22.1 De\ufb01ning Emotion . . . . . . . . . . . . . . . . . . . . . . . . . . 482\n22.2 Available Sentiment and Affect Lexicons . . . . . . . . . . . . . . 484\n22.3 Creating Affect Lexicons by Human Labeling . . . . . . . . . . . 485\n22.4 Semi-supervised Induction of Affect Lexicons . . . . . . . . . . . 487",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 20,
      "token_count": 775,
      "chapter_title": ""
    }
  },
  {
    "content": "21.7 Selectional Restrictions . . . . . . . . . . . . . . . . . . . . . . . 472\n21.8 Primitive Decomposition of Predicates . . . . . . . . . . . . . . . 476\n21.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 478\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 480\n22 Lexicons for Sentiment, Affect, and Connotation 481\n22.1 De\ufb01ning Emotion . . . . . . . . . . . . . . . . . . . . . . . . . . 482\n22.2 Available Sentiment and Affect Lexicons . . . . . . . . . . . . . . 484\n22.3 Creating Affect Lexicons by Human Labeling . . . . . . . . . . . 485\n22.4 Semi-supervised Induction of Affect Lexicons . . . . . . . . . . . 487\n22.5 Supervised Learning of Word Sentiment . . . . . . . . . . . . . . 490\n22.6 Using Lexicons for Sentiment Recognition . . . . . . . . . . . . . 495\n22.7 Using Lexicons for Affect Recognition . . . . . . . . . . . . . . . 496\n22.8 Lexicon-based methods for Entity-Centric Affect . . . . . . . . . . 497\n22.9 Connotation Frames . . . . . . . . . . . . . . . . . . . . . . . . . 497\n22.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 21,
      "token_count": 478,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8CONTENTS\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 500\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 500\n23 Coreference Resolution and Entity Linking 501\n23.1 Coreference Phenomena: Linguistic Background . . . . . . . . . . 504\n23.2 Coreference Tasks and Datasets . . . . . . . . . . . . . . . . . . . 509\n23.3 Mention Detection . . . . . . . . . . . . . . . . . . . . . . . . . . 510\n23.4 Architectures for Coreference Algorithms . . . . . . . . . . . . . 513\n23.5 Classi\ufb01ers using hand-built features . . . . . . . . . . . . . . . . . 515\n23.6 A neural mention-ranking algorithm . . . . . . . . . . . . . . . . 517\n23.7 Entity Linking . . . . . . . . . . . . . . . . . . . . . . . . . . . . 520\n23.8 Evaluation of Coreference Resolution . . . . . . . . . . . . . . . . 524\n23.9 Winograd Schema problems . . . . . . . . . . . . . . . . . . . . . 525\n23.10 Gender Bias in Coreference . . . . . . . . . . . . . . . . . . . . . 526\n23.11 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 528\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 530\n24 Discourse Coherence 531\n24.1 Coherence Relations . . . . . . . . . . . . . . . . . . . . . . . . . 533\n24.2 Discourse Structure Parsing . . . . . . . . . . . . . . . . . . . . . 536\n24.3 Centering and Entity-Based Coherence . . . . . . . . . . . . . . . 540\n24.4 Representation learning models for local coherence . . . . . . . . 544\n24.5 Global Coherence . . . . . . . . . . . . . . . . . . . . . . . . . . 546\n24.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 549\nBibliographical and Historical Notes . . . . . . . . . . . . . . . . . . . . 550\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552\nBibliography 553\nSubject Index 585",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 22,
      "token_count": 767,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\nPart I\nFUNDAMENTAL ALGORITHMS FOR\nNLP\nIn the \ufb01rst part of the book we introduce the fundamental suite of algorithmic\ntools that make up the modern neural language model that is the heart of end-to-end\nNLP systems. We begin with tokenization and preprocessing, as well as useful algo-\nrithms like computing edit distance, and then proceed to the tasks of classi\ufb01cation,\nlogistic regression, neural networks, proceeding through feedforward networks, re-\ncurrent networks, and then transformers. We\u2019ll also see the role of embeddings as a\nmodel of word meaning.\n\n## Page 11\n\nCHAPTER\n1Introduction\nLa derni `ere chose qu\u2019on trouve en faisant un ouvrage est de savoir celle qu\u2019il faut\nmettre la premi `ere.\n[The last thing you \ufb01gure out in writing a book is what to put \ufb01rst.]\nPascal\n3",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 23,
      "token_count": 202,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n4CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\nCHAPTER\n2Regular Expressions, Tok-\nenization, Edit Distance\nUser: I am unhappy.\nELIZA: DO YOU THINK COMING HERE WILL HELP YOU NOT TO BE UNHAPPY\nUser: I need some help, that much seems certain.\nELIZA: WHAT WOULD IT MEAN TO YOU IF YOU GOT SOME HELP\nUser: Perhaps I could learn to get along with my mother.\nELIZA: TELL ME MORE ABOUT YOUR FAMILY\nUser: My mother takes care of me.\nELIZA: WHO ELSE IN YOU FAMILY TAKES CARE OF YOU\nUser: My father.\nELIZA: YOUR FATHER\nUser: You are like my father in some ways.\nWeizenbaum (1966)\nThe dialogue above is from ELIZA , an early natural language processing system ELIZA\nthat could carry on a limited conversation with a user by imitating the responses of\na Rogerian psychotherapist (Weizenbaum, 1966). ELIZA is a surprisingly simple\nprogram that uses pattern matching to recognize phrases like \u201cI need X\u201d and translate\nthem into suitable outputs like \u201cWhat would it mean to you if you got X?\u201d. This\nsimple technique succeeds in this domain because ELIZA doesn\u2019t actually need to\nknow anything to mimic a Rogerian psychotherapist. As Weizenbaum notes, this is\none of the few dialogue genres where listeners can act as if they know nothing of the\nworld. ELIZA\u2019s mimicry of human conversation was remarkably successful: many\npeople who interacted with ELIZA came to believe that it really understood them\nand their problems, many continued to believe in ELIZA\u2019s abilities even after the\nprogram\u2019s operation was explained to them (Weizenbaum, 1976), and even today\nsuch chatbots are a fun diversion. chatbots\nOf course modern conversational agents are much more than a diversion; they\ncan answer questions, book \ufb02ights, or \ufb01nd restaurants, functions for which they rely\non a much more sophisticated understanding of the user\u2019s intent, as we will see in\nChapter 15. Nonetheless, the simple pattern-based methods that powered ELIZA\nand other chatbots play a crucial role in natural language processing.\nWe\u2019ll begin with the most important tool for describing text patterns: the regular\nexpression . Regular expressions can be used to specify strings we might want to\nextract from a document, from transforming \u201cI need X\u201d in ELIZA above, to de\ufb01ning\nstrings like $199 or$24.99 for extracting tables of prices from a document.\nWe\u2019ll then turn to a set of tasks collectively called text normalization , in whichtext\nnormalization\nregular expressions play an important part. Normalizing text means converting it\nto a more convenient, standard form. For example, most of what we are going to\ndo with language relies on \ufb01rst separating out or tokenizing words or word parts\nfrom running text, the task of tokenization . English words are often separated from tokenization\neach other by whitespace, but whitespace is not always suf\ufb01cient. New York and\nrock \u2019n\u2019 roll are sometimes treated as large words despite the fact that they contain\nspaces, while sometimes we\u2019ll need to separate I\u2019minto the two words Iandam.\nFor processing tweets or texts we\u2019ll need to tokenize emoticons like:)orhashtags",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 24,
      "token_count": 732,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\n2.1 \u2022 R EGULAR EXPRESSIONS 5\nlike#nlproc . Some languages, like Japanese, don\u2019t have spaces between words,\nso word tokenization becomes more dif\ufb01cult. And as we\u2019ll see, for large language\nmodels we\u2019ll use tokens that range greatly in size, from letters to subwords (parts of\nwords) to words and even sometimes short phrases.\nAnother part of text normalization is lemmatization , the task of determining lemmatization\nthat two words have the same root, despite their surface differences. For example,\nthe words sang ,sung , and sings are forms of the verb sing. The word sing is the\ncommon lemma of these words, and a lemmatizer maps from all of these to sing.\nLemmatization is essential for processing morphologically complex languages like\nArabic. Stemming refers to a simpler version of lemmatization in which we mainly stemming\njust strip suf\ufb01xes from the end of the word. Text normalization also includes sen-\ntence segmentation : breaking up a text into individual sentences, using cues likesentence\nsegmentation\nperiods or exclamation points.\nFinally, we\u2019ll need to compare words and other strings. We\u2019ll introduce a metric\ncalled edit distance that measures how similar two strings are based on the number\nof edits (insertions, deletions, substitutions) it takes to change one string into the\nother. Edit distance is an algorithm with applications throughout language process-\ning, from spelling correction to speech recognition to coreference resolution.\n2.1 Regular Expressions\nOne of the most useful tools for text processing in computer science has been the\nregular expression (often shortened to regex ), a language for specifying text searchregular\nexpression\nstrings. This practical language is used in every computer language, in text process-\ning tools like the Unix tools grep, and in editors like vim or Emacs. Formally, a\nregular expression is an algebraic notation for characterizing a set of strings. Reg-\nular expressions are particularly useful for searching in texts, when we have a pat-\ntern to search for and a corpus of texts to search through. A regular expression corpus\nsearch function will search through the corpus, returning all texts that match the\npattern. The corpus can be a single document or a collection. For example, the\nUnix command-line tool grep takes a regular expression and returns every line of\nthe input document that matches the expression.\nA search can be designed to return every match on a line, if there are more than\none, or just the \ufb01rst match. In the following examples we generally underline the\nexact string that matches the regular expression and show only the \ufb01rst match. We\u2019ll\nshow regular expressions delimited by slashes but note that slashes are notpart of\nthe regular expressions.\nRegular expressions come in many variants. We\u2019ll be describing extended regu-\nlar expressions ; different regular expression parsers may only recognize subsets of\nthese, or treat some expressions slightly differently. Using an online regular expres-\nsion tester is a handy way to test out your expressions and explore these variations.\n2.1.1 Basic Regular Expression Patterns\nThe simplest kind of regular expression is a sequence of simple characters; putting\ncharacters in sequence is called concatenation . To search for woodchuck , we type concatenation\n/woodchuck/ . The expression /Buttercup/ matches any string containing the\nsubstring Buttercup ;grep with that expression would return the line I\u2019m called lit-\ntle Buttercup . The search string can consist of a single character (like /!/) or a",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 25,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n6CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\nsequence of characters (like /urgl/ ) (see Fig. 2.1).\nRegex Example Patterns Matched\n/woodchucks/ \u201cinteresting links to woodchucks and lemurs\u201d\n/a/ \u201cMary Ann stopped by Mona\u2019s\u201d\n/!/ \u201cYou\u2019ve left the burglar behind again! \u201d said Nori\nFigure 2.1 Some simple regex searches.\nRegular expressions are case sensitive ; lower case /s/ is distinct from upper\ncase/S/ (/s/ matches a lower case sbut not an upper case S). This means that\nthe pattern /woodchucks/ will not match the string Woodchucks . We can solve this\nproblem with the use of the square braces [and]. The string of characters inside the\nbraces speci\ufb01es a disjunction of characters to match. For example, Fig. 2.2 shows\nthat the pattern /[wW]/ matches patterns containing either worW.\nRegex Match Example Patterns\n/[wW]oodchuck/ Woodchuck or woodchuck \u201cWoodchuck \u201d\n/[abc]/ \u2018a\u2019, \u2018b\u2019, or\u2018c\u2019 \u201cIn uomini, in solda ti\u201d\n/[1234567890]/ any digit \u201cplenty of 7 to 5\u201d\nFigure 2.2 The use of the brackets []to specify a disjunction of characters.\nThe regular expression /[1234567890]/ speci\ufb01es any single digit. While such\nclasses of characters as digits or letters are important building blocks in expressions,\nthey can get awkward (e.g., it\u2019s inconvenient to specify\n/[ABCDEFGHIJKLMNOPQRSTUVWXYZ]/ (2.1)\nto mean \u201cany capital letter\u201d). In cases where there is a well-de\ufb01ned sequence asso-\nciated with a set of characters, the brackets can be used with the dash ( -) to specify\nany one character in a range . The pattern /[2-5]/ speci\ufb01es any one of the charac- range\nters2,3,4, or5. The pattern /[b-g]/ speci\ufb01es one of the characters b,c,d,e,f, or\ng. Some other examples are shown in Fig. 2.3.\nRegex Match Example Patterns Matched\n/[A-Z]/ an upper case letter \u201cwe should call it \u2018D renched Blossoms\u2019 \u201d\n/[a-z]/ a lower case letter \u201cmy beans were impatient to be hoed!\u201d\n/[0-9]/ a single digit \u201cChapter 1 : Down the Rabbit Hole\u201d\nFigure 2.3 The use of the brackets []plus the dash -to specify a range.\nThe square braces can also be used to specify what a single character cannot be,\nby use of the caret ^. If the caret ^is the \ufb01rst symbol after the open square brace [,\nthe resulting pattern is negated. For example, the pattern /[^a]/ matches any single\ncharacter (including special characters) except a. This is only true when the caret\nis the \ufb01rst symbol after the open square brace. If it occurs anywhere else, it usually\nstands for a caret; Fig. 2.4 shows some examples.\nHow can we talk about optional elements, like an optional sinwoodchuck and\nwoodchucks ? We can\u2019t use the square brackets, because while they allow us to say\n\u201cs or S\u201d, they don\u2019t allow us to say \u201cs or nothing\u201d. For this we use the question mark\n/?/, which means \u201cthe preceding character or nothing\u201d, as shown in Fig. 2.5.\nWe can think of the question mark as meaning \u201czero or one instances of the\nprevious character\u201d. That is, it\u2019s a way of specifying how many of something that",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 26,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\n2.1 \u2022 R EGULAR EXPRESSIONS 7\nRegex Match (single characters) Example Patterns Matched\n/[^A-Z]/ not an upper case letter \u201cOyfn pripetchik\u201d\n/[^Ss]/ neither \u2018S\u2019 nor \u2018s\u2019 \u201cIhave no exquisite reason for\u2019t\u201d\n/[^.]/ not a period \u201cour resident Djinn\u201d\n/[e^]/ either \u2018e\u2019 or \u2018 ^\u2019 \u201clook up \u02c6 now\u201d\n/a^b/ the pattern \u2018 a^b\u2019 \u201clook up a\u02c6 b now\u201d\nFigure 2.4 The caret ^for negation or just to mean ^. See below re: the backslash for escaping the period.\nRegex Match Example Patterns Matched\n/woodchucks?/ woodchuck or woodchucks \u201cwoodchuck \u201d\n/colou?r/ color or colour \u201ccolor \u201d\nFigure 2.5 The question mark ?marks optionality of the previous expression.\nwe want, something that is very important in regular expressions. For example,\nconsider the language of certain sheep, which consists of strings that look like the\nfollowing:\nbaa!\nbaaa!\nbaaaa!\n. . .\nThis language consists of strings with a b, followed by at least two a\u2019s, followed\nby an exclamation point. The set of operators that allows us to say things like \u201csome\nnumber of as\u201d are based on the asterisk or *, commonly called the Kleene * (gen- Kleene *\nerally pronounced \u201ccleany star\u201d). The Kleene star means \u201czero or more occurrences\nof the immediately previous character or regular expression\u201d. So /a*/ means \u201cany\nstring of zero or more as\u201d. This will match aoraaaaaa , but it will also match the\nempty string at the start of Off Minor since the string Off Minor starts with zero a\u2019s.\nSo the regular expression for matching one or more ais/aa*/ , meaning one afol-\nlowed by zero or more as. More complex patterns can also be repeated. So /[ab]*/\nmeans \u201czero or more a\u2019s or b\u2019s\u201d (not \u201czero or more right square braces\u201d). This will\nmatch strings like aaaa orababab orbbbb , as well as the empty string.\nFor specifying multiple digits (useful for \ufb01nding prices) we can extend /[0-9]/ ,\nthe regular expression for a single digit. An integer (a string of digits) is thus\n/[0-9][0-9]*/ . (Why isn\u2019t it just /[0-9]*/ ?)\nSometimes it\u2019s annoying to have to write the regular expression for digits twice,\nso there is a shorter way to specify \u201cat least one\u201d of some character. This is the\nKleene + , which means \u201cone or more occurrences of the immediately preceding Kleene +\ncharacter or regular expression\u201d. Thus, the expression /[0-9]+/ is the normal way\nto specify \u201ca sequence of digits\u201d. There are thus two ways to specify the sheep\nlanguage: /baaa*!/ or/baa+!/ .\nOne very important special character is the period ( /./), awildcard expression\nthat matches any single character ( except a carriage return), as shown in Fig. 2.6.\nRegex Match Example Matches\n/beg.n/ any character between begandn begin , beg\u2019n , begun\nFigure 2.6 The use of the period .to specify any character.\nThe wildcard is often used together with the Kleene star to mean \u201cany string of\ncharacters\u201d. For example, suppose we want to \ufb01nd any line in which a particular",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 27,
      "token_count": 760,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 28,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "8CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\nword, for example, aardvark , appears twice. We can specify this with the regular\nexpression /aardvark.*aardvark/ .\nAnchors are special characters that anchor regular expressions to particular places anchors\nin a string. The most common anchors are the caret ^and the dollar sign $. The caret\n^matches the start of a line. The pattern /^The/ matches the word Theonly at the\nstart of a line. Thus, the caret ^has three uses: to match the start of a line, to in-\ndicate a negation inside of square brackets, and just to mean a caret. (What are the\ncontexts that allow grep or Python to know which function a given caret is supposed\nto have?) The dollar sign $matches the end of a line. So the pattern  $is a useful\npattern for matching a space at the end of a line, and /^The dog\\.$/ matches a\nline that contains only the phrase The dog. (We have to use the backslash here since\nwe want the .to mean \u201cperiod\u201d and not the wildcard.)\nRegex Match\n^ start of line\n$ end of line\n\\b word boundary\n\\B non-word boundary\nFigure 2.7 Anchors in regular expressions.\nThere are also two other anchors: \\bmatches a word boundary, and \\Bmatches\na non word-boundary. Thus, /\\bthe\\b/ matches the word thebut not the word\nother . A \u201cword\u201d for the purposes of a regular expression is de\ufb01ned based on the\nde\ufb01nition of words in programming languages as a sequence of digits, underscores,\nor letters. Thus /\\b99\\b/ will match the string 99inThere are 99 bottles of beer on\nthe wall (because 99 follows a space) but not 99inThere are 299 bottles of beer on\nthe wall (since 99 follows a number). But it will match 99in$99(since 99follows\na dollar sign ($), which is not a digit, underscore, or letter).\n2.1.2 Disjunction, Grouping, and Precedence\nSuppose we need to search for texts about pets; perhaps we are particularly interested\nin cats and dogs. In such a case, we might want to search for either the string cator\nthe string dog. Since we can\u2019t use the square brackets to search for \u201ccat or dog\u201d (why\ncan\u2019t we say /[catdog]/ ?), we need a new operator, the disjunction operator, also disjunction\ncalled the pipe symbol|. The pattern /cat|dog/ matches either the string cator\nthe string dog.\nSometimes we need to use this disjunction operator in the midst of a larger se-\nquence. For example, suppose I want to search for information about pet \ufb01sh for\nmy cousin David. How can I specify both guppy andguppies ? We cannot simply\nsay/guppy|ies/ , because that would match only the strings guppy andies. This\nis because sequences like guppy take precedence over the disjunction operator |. precedence\nTo make the disjunction operator apply only to a speci\ufb01c pattern, we need to use the\nparenthesis operators (and). Enclosing a pattern in parentheses makes it act like\na single character for the purposes of neighboring operators like the pipe |and the\nKleene*. So the pattern /gupp(y|ies)/ would specify that we meant the disjunc-\ntion only to apply to the suf\ufb01xes yandies.\nThe parenthesis operator (is also useful when we are using counters like the",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 29,
      "token_count": 784,
      "chapter_title": ""
    }
  },
  {
    "content": "in cats and dogs. In such a case, we might want to search for either the string cator\nthe string dog. Since we can\u2019t use the square brackets to search for \u201ccat or dog\u201d (why\ncan\u2019t we say /[catdog]/ ?), we need a new operator, the disjunction operator, also disjunction\ncalled the pipe symbol|. The pattern /cat|dog/ matches either the string cator\nthe string dog.\nSometimes we need to use this disjunction operator in the midst of a larger se-\nquence. For example, suppose I want to search for information about pet \ufb01sh for\nmy cousin David. How can I specify both guppy andguppies ? We cannot simply\nsay/guppy|ies/ , because that would match only the strings guppy andies. This\nis because sequences like guppy take precedence over the disjunction operator |. precedence\nTo make the disjunction operator apply only to a speci\ufb01c pattern, we need to use the\nparenthesis operators (and). Enclosing a pattern in parentheses makes it act like\na single character for the purposes of neighboring operators like the pipe |and the\nKleene*. So the pattern /gupp(y|ies)/ would specify that we meant the disjunc-\ntion only to apply to the suf\ufb01xes yandies.\nThe parenthesis operator (is also useful when we are using counters like the\nKleene*. Unlike the |operator, the Kleene *operator applies by default only to\na single character, not to a whole sequence. Suppose we want to match repeated\ninstances of a string. Perhaps we have a line that has column labels of the form",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 30,
      "token_count": 349,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\n2.1 \u2022 R EGULAR EXPRESSIONS 9\nColumn 1 Column 2 Column 3 . The expression /Column [0-9]+ */ will not\nmatch any number of columns; instead, it will match a single column followed by\nany number of spaces! The star here applies only to the space  that precedes it,\nnot to the whole sequence. With the parentheses, we could write the expression\n/(Column [0-9]+ *)*/ to match the word Column , followed by a number and\noptional spaces, the whole pattern repeated zero or more times.\nThis idea that one operator may take precedence over another, requiring us to\nsometimes use parentheses to specify what we mean, is formalized by the operator\nprecedence hierarchy for regular expressions. The following table gives the orderoperator\nprecedence\nof RE operator precedence, from highest precedence to lowest precedence.\nParenthesis ()\nCounters * + ? {}\nSequences and anchors the ^my end$\nDisjunction |\nThus, because counters have a higher precedence than sequences,\n/the*/ matches theeeee but not thethe . Because sequences have a higher prece-\ndence than disjunction, /the|any/ matches theoranybut not thany ortheny .\nPatterns can be ambiguous in another way. Consider the expression /[a-z]*/\nwhen matching against the text once upon a time . Since/[a-z]*/ matches zero or\nmore letters, this expression could match nothing, or just the \ufb01rst letter o,on,onc,\noronce . In these cases regular expressions always match the largest string they can;\nwe say that patterns are greedy , expanding to cover as much of a string as they can. greedy\nThere are, however, ways to enforce non-greedy matching, using another mean- non-greedy\ning of the ?quali\ufb01er. The operator *?is a Kleene star that matches as little text as *?\npossible. The operator +?is a Kleene plus that matches as little text as possible. +?\n2.1.3 A Simple Example\nSuppose we wanted to write a RE to \ufb01nd cases of the English article the. A simple\n(but incorrect) pattern might be:\n/the/ (2.2)\nOne problem is that this pattern will miss the word when it begins a sentence and\nhence is capitalized (i.e., The). This might lead us to the following pattern:\n/[tT]he/ (2.3)\nBut we will still overgeneralize, incorrectly return texts with theembedded in other\nwords (e.g., other orthere ). So we need to specify that we want instances with a\nword boundary on both sides:\n/\\b[tT]he\\b/ (2.4)\nThe simple process we just went through was based on \ufb01xing two kinds of errors:\nfalse positives , strings that we incorrectly matched like other orthere , and false false positives\nnegatives , strings that we incorrectly missed, like The. Addressing these two kinds false negatives\nof errors comes up again and again in language processing. Reducing the overall\nerror rate for an application thus involves two antagonistic efforts:\n\u2022 Increasing precision (minimizing false positives)\n\u2022 Increasing recall (minimizing false negatives)\nWe\u2019ll come back to precision and recall with more precise de\ufb01nitions in Chapter 4.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 31,
      "token_count": 699,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\n10 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\n2.1.4 More Operators\nFigure 2.8 shows some aliases for common ranges, which can be used mainly to\nsave typing. Besides the Kleene * and Kleene + we can also use explicit numbers as\ncounters, by enclosing them in curly brackets. The operator /{3}/ means \u201cexactly\n3 occurrences of the previous character or expression\u201d. So /a\\.{24}z/ will match\nafollowed by 24 dots followed by z(but not afollowed by 23 or 25 dots followed\nby a z).\nRegex Expansion Match First Matches\n\\d [0-9] any digit Party of 5\n\\D [^0-9] any non-digit Blue moon\n\\w [a-zA-Z0-9_] any alphanumeric/underscore Daiyu\n\\W [^\\w] a non-alphanumeric !!!!\n\\s [ \\r\\t\\n\\f] whitespace (space, tab) inConcord\n\\S [^\\s] Non-whitespace in Concord\nFigure 2.8 Aliases for common sets of characters.\nA range of numbers can also be speci\ufb01ed. So /{n,m}/ speci\ufb01es from ntom\noccurrences of the previous char or expression, and /{n,}/ means at least noccur-\nrences of the previous expression. REs for counting are summarized in Fig. 2.9.\nRegex Match\n* zero or more occurrences of the previous char or expression\n+ one or more occurrences of the previous char or expression\n? zero or one occurrence of the previous char or expression\n{n} exactly noccurrences of the previous char or expression\n{n,m} from ntomoccurrences of the previous char or expression\n{n,} at least noccurrences of the previous char or expression\n{,m} up to moccurrences of the previous char or expression\nFigure 2.9 Regular expression operators for counting.\nFinally, certain special characters are referred to by special notation based on the\nbackslash ( \\) (see Fig. 2.10). The most common of these are the newline character newline\n\\nand the tabcharacter\\t. To refer to characters that are special themselves (like\n.,*,[, and\\), precede them with a backslash, (i.e., /\\./ ,/\\*/ ,/\\[/ , and/\\\\/ ).\nRegex Match First Patterns Matched\n\\* an asterisk \u201c*\u201d \u201cK* A*P*L*A*N\u201d\n\\. a period \u201c.\u201d \u201cDr. Livingston, I presume\u201d\n\\? a question mark \u201cWhy don\u2019t they come and lend a hand? \u201d\n\\n a newline\n\\t a tab\nFigure 2.10 Some characters that need to be escaped (via backslash).\n2.1.5 A More Complex Example\nLet\u2019s try out a more signi\ufb01cant example of the power of REs. Suppose our goal is\nhelp a user buy a computer on the Web who wants \u201cat least 6 GHz and 500 GB of\ndisk space for less than $1000\u201d. To do this kind of retrieval, we \ufb01rst need to be",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 32,
      "token_count": 674,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\n2.1 \u2022 R EGULAR EXPRESSIONS 11\nable to look for expressions like 6 GHz or500 GB or$999.99 . Let\u2019s work out some\nregular expressions for this task.\nFirst, let\u2019s complete our regular expression for prices. Here\u2019s a regular expres-\nsion for a dollar sign followed by a string of digits:\n/$[0-9]+/ (2.5)\nNote that the $character has a different function here than the end-of-line function\nwe discussed earlier. Most regular expression parsers are smart enough to realize\nthat$here doesn\u2019t mean end-of-line. (As a thought experiment, think about how\nregex parsers might \ufb01gure out the function of $from the context.)\nNow we just need to deal with fractions of dollars. We\u2019ll add a decimal point\nand two digits afterwards:\n/$[0-9]+\\.[0-9][0-9]/ (2.6)\nThis pattern only allows $199.99 but not $199 . We need to make the cents optional\nand to make sure we\u2019re at a word boundary:\n/(^|\\W)$[0-9]+(\\.[0-9][0-9])?\\b/ (2.7)\nOne last catch! This pattern allows prices like $199999.99 which would be far too\nexpensive! We need to limit the dollars:\n/(^|\\W)$[0-9]{0,3}(\\.[0-9][0-9])?\\b/ (2.8)\nFurther \ufb01xes (like avoiding matching a dollar sign with no price after it) are left as\nan exercise for the reader.\nHow about disk space? We\u2019ll need to allow for optional fractions again ( 5.5 GB );\nnote the use of ?for making the \ufb01nal soptional, and the use of / */ to mean \u201czero\nor more spaces\u201d since there might always be extra spaces lying around:\n/\\b[0-9]+(\\.[0-9]+)? *(GB|[Gg]igabytes?)\\b/ (2.9)\nModifying this regular expression so that it only matches more than 500 GB is left\nas an exercise for the reader.\n2.1.6 Substitution, Capture Groups, and ELIZA\nAn important use of regular expressions is in substitutions . For example, the substi- substitution\ntution operator s/regexp1/pattern/ used in Python and in Unix commands like\nvimorsedallows a string characterized by a regular expression to be replaced by\nanother string:\ns/colour/color/ (2.10)\nIt is often useful to be able to refer to a particular subpart of the string matching\nthe \ufb01rst pattern. For example, suppose we wanted to put angle brackets around all\nintegers in a text, for example, changing the 35 boxes tothe<35>boxes . We\u2019d\nlike a way to refer to the integer we\u2019ve found so that we can easily add the brackets.\nTo do this, we put parentheses (and)around the \ufb01rst pattern and use the number\noperator\\1in the second pattern to refer back. Here\u2019s how it looks:\ns/([0-9]+)/<\\1>/ (2.11)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 33,
      "token_count": 685,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 34,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "12 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\nThe parenthesis and number operators can also specify that a certain string or ex-\npression must occur twice in the text. For example, suppose we are looking for the\npattern \u201cthe Xer they were, the Xer they will be\u201d, where we want to constrain the two\nX\u2019s to be the same string. We do this by surrounding the \ufb01rst X with the parenthesis\noperator, and replacing the second X with the number operator \\1, as follows:\n/the (.*)er they were, the \\1er they will be/ (2.12)\nHere the\\1will be replaced by whatever string matched the \ufb01rst item in parentheses.\nSo this will match the bigger they were, the bigger they will be but not the bigger\nthey were, the faster they will be .\nThis use of parentheses to store a pattern in memory is called a capture group . capture group\nEvery time a capture group is used (i.e., parentheses surround a pattern), the re-\nsulting match is stored in a numbered register . If you match two different sets of register\nparentheses, \\2means whatever matched the second capture group. Thus\n/the (.*)er they (.*), the \\1er we \\2/ (2.13)\nwill match the faster they ran, the faster we ran but not the faster they ran, the faster\nwe ate . Similarly, the third capture group is stored in \\3, the fourth is \\4, and so on.\nParentheses thus have a double function in regular expressions; they are used\nto group terms for specifying the order in which operators should apply, and they\nare used to capture something in a register. Occasionally we might want to use\nparentheses for grouping, but don\u2019t want to capture the resulting pattern in a register.\nIn that case we use a non-capturing group , which is speci\ufb01ed by putting the specialnon-capturing\ngroup\ncommands ?:after the open parenthesis, in the form (?: pattern ) .\n/(?:some|a few) (people|cats) like some \\1/ (2.14)\nwill match some cats like some cats but not some cats like some some .\nSubstitutions and capture groups are very useful in implementing simple chat-\nbots like ELIZA (Weizenbaum, 1966). Recall that ELIZA simulates a Rogerian\npsychologist by carrying on conversations like the following:\nUser 1: Men are all alike.\nELIZA 1:IN WHAT WAY\nUser 2: They\u2019re always bugging us about something or other.\nELIZA 2:CAN YOU THINK OF A SPECIFIC EXAMPLE\nUser 3: Well, my boyfriend made me come here.\nELIZA 3:YOUR BOYFRIEND MADE YOU COME HERE\nUser 4: He says I\u2019m depressed much of the time.\nELIZA 4:I AM SORRY TO HEAR YOU ARE DEPRESSED\nELIZA works by having a series or cascade of regular expression substitutions\neach of which matches and changes some part of the input lines. After the input\nis uppercased, substitutions change all instances of MYtoYOUR , and I\u2019MtoYOU\nARE, and so on. That way when ELIZA repeats back part of the user utterance, it\nwill seem to be referring correctly to the user. The next set of substitutions matches\nand replaces other patterns in the input. Here are some examples:\ns/.* YOU ARE (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE \\1/",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 35,
      "token_count": 759,
      "chapter_title": ""
    }
  },
  {
    "content": "Substitutions and capture groups are very useful in implementing simple chat-\nbots like ELIZA (Weizenbaum, 1966). Recall that ELIZA simulates a Rogerian\npsychologist by carrying on conversations like the following:\nUser 1: Men are all alike.\nELIZA 1:IN WHAT WAY\nUser 2: They\u2019re always bugging us about something or other.\nELIZA 2:CAN YOU THINK OF A SPECIFIC EXAMPLE\nUser 3: Well, my boyfriend made me come here.\nELIZA 3:YOUR BOYFRIEND MADE YOU COME HERE\nUser 4: He says I\u2019m depressed much of the time.\nELIZA 4:I AM SORRY TO HEAR YOU ARE DEPRESSED\nELIZA works by having a series or cascade of regular expression substitutions\neach of which matches and changes some part of the input lines. After the input\nis uppercased, substitutions change all instances of MYtoYOUR , and I\u2019MtoYOU\nARE, and so on. That way when ELIZA repeats back part of the user utterance, it\nwill seem to be referring correctly to the user. The next set of substitutions matches\nand replaces other patterns in the input. Here are some examples:\ns/.* YOU ARE (depressed|sad) .*/I AM SORRY TO HEAR YOU ARE \\1/\ns/.* YOU ARE (depressed|sad) .*/WHY DO YOU THINK YOU ARE \\1/\ns/.* all .*/IN WHAT WAY/\ns/.* always .*/CAN YOU THINK OF A SPECIFIC EXAMPLE/",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 36,
      "token_count": 339,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21\n\n2.2 \u2022 W ORDS 13\nSince multiple substitutions can apply to a given input, substitutions are assigned\na rank and applied in order. Creating patterns is the topic of Exercise 2.3, and we\nreturn to the details of the ELIZA architecture in Chapter 15.\n2.1.7 Lookahead Assertions\nFinally, there will be times when we need to predict the future: look ahead in the\ntext to see if some pattern matches, but not yet advance the pointer we always keep\nto where we are in the text, so that we can then deal with the pattern if it occurs, but\nif it doesn\u2019t we can check for something else instead.\nThese lookahead assertions make use of the (?syntax that we saw in the previ- lookahead\nous section for non-capture groups. The operator (?= pattern) is true ifpattern\noccurs, but is zero-width , i.e. the match pointer doesn\u2019t advance. The operator zero-width\n(?! pattern) only returns true if a pattern does not match, but again is zero-width\nand doesn\u2019t advance the pointer. Negative lookahead is commonly used when we\nare parsing some complex pattern but want to rule out a special case. For example\nsuppose we want to match, at the beginning of a line, any single word that doesn\u2019t\nstart with \u201cV olcano\u201d. We can use negative lookahead to do this:\n/^(?!Volcano)[A-Za-z]+/ (2.15)\n2.2 Words\nBefore we talk about processing words, we need to decide what counts as a word.\nLet\u2019s start by looking at one particular corpus (plural corpora ), a computer-readable corpus\ncorpora collection of text or speech. For example the Brown corpus is a million-word col-\nlection of samples from 500 written English texts from different genres (newspa-\nper, \ufb01ction, non-\ufb01ction, academic, etc.), assembled at Brown University in 1963\u201364\n(Ku\u02c7cera and Francis, 1967). How many words are in the following Brown sentence?\nHe stepped out into the hall, was delighted to encounter\na water brother.\nThis sentence has 13 words if we don\u2019t count punctuation marks as words, 15\nif we count punctuation. Whether we treat period (\u201c .\u201d), comma (\u201c ,\u201d), and so on as\nwords depends on the task. Punctuation is critical for \ufb01nding boundaries of things\n(commas, periods, colons) and for identifying some aspects of meaning (question\nmarks, exclamation marks, quotation marks). For some tasks, like part-of-speech\ntagging or parsing or speech synthesis, we sometimes treat punctuation marks as if\nthey were separate words.\nThe Switchboard corpus of American English telephone conversations between\nstrangers was collected in the early 1990s; it contains 2430 conversations averaging\n6 minutes each, totaling 240 hours of speech and about 3 million words (Godfrey\net al., 1992). Such corpora of spoken language introduce other complications with\nregard to de\ufb01ning words. Let\u2019s look at one utterance from Switchboard; an utter-\nance is the spoken correlate of a sentence: utterance\nI do uh main- mainly business data processing\nThis utterance has two kinds of dis\ufb02uencies . The broken-off word main- is dis\ufb02uency\ncalled a fragment . Words like uhandumare called \ufb01llers or\ufb01lled pauses . Should fragment\n\ufb01lled pause we consider these to be words? Again, it depends on the application. If we are\nbuilding a speech transcription system, we might want to eventually strip out the\ndis\ufb02uencies.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 37,
      "token_count": 785,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 38,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "14 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\nBut we also sometimes keep dis\ufb02uencies around. Dis\ufb02uencies like uhorum\nare actually helpful in speech recognition in predicting the upcoming word, because\nthey may signal that the speaker is restarting the clause or idea, and so for speech\nrecognition they are treated as regular words. Because different people use differ-\nent dis\ufb02uencies they can also be a cue to speaker identi\ufb01cation. In fact Clark and\nFox Tree (2002) showed that uhandumhave different meanings. What do you think\nthey are?\nPerhaps most important, in thinking about what is a word, we need to distinguish\ntwo ways of talking about words that will be useful throughout the book. Word types word type\nare the number of distinct words in a corpus; if the set of words in the vocabulary is\nV, the number of types is the vocabulary size jVj. Word instances are the total num- word instance\nberNof running words.1If we ignore punctuation, the following Brown sentence\nhas 14 types and 16 instances:\nThey picnicked by the pool, then lay back on the grass and\nlooked at the stars.\nWe still have decisions to make! For example, should we consider a capitalized\nstring (like They ) and one that is uncapitalized (like they) to be the same word type?\nThe answer is that it depends on the task! They andthey might be lumped together\nas the same type in some tasks, like speech recognition, where we care more about\nthe sequence of words and less about the formatting, while for other tasks, such\nas deciding whether a particular word is a name of a person or location (named-\nentity tagging), capitalization is a useful feature and is retained. Sometimes we keep\naround two versions of a particular NLP model, one with capitalization and one\nwithout capitalization.\nCorpus Types =jVjInstances = N\nShakespeare 31 thousand 884 thousand\nBrown corpus 38 thousand 1 million\nSwitchboard telephone conversations 20 thousand 2.4 million\nCOCA 2 million 440 million\nGoogle n-grams 13 million 1 trillion\nFigure 2.11 Rough numbers of wordform types and instances for some English language\ncorpora. The largest, the Google n-grams corpus, contains 13 million types, but this count\nonly includes types appearing 40 or more times, so the true number would be much larger.\nHow many words are there in English? When we speak about the number of\nwords in the language, we are generally referring to word types. Fig. 2.11 shows\nthe rough numbers of types and instances computed from some English corpora.\nThe larger the corpora we look at, the more word types we \ufb01nd, and in fact this\nrelationship between the number of types jVjand number of instances Nis called\nHerdan\u2019s Law (Herdan, 1960) or Heaps\u2019 Law (Heaps, 1978) after its discoverers Herdan\u2019s Law\nHeaps\u2019 Law (in linguistics and information retrieval respectively). It is shown in Eq. 2.16, where\nkandbare positive constants, and 0 <b<1.\njVj=kNb(2.16)\nThe value of bdepends on the corpus size and the genre, but at least for the large\ncorpora in Fig. 2.11, branges from .67 to .75. Roughly then we can say that the\n1In earlier tradition, and occasionally still, you might see word instances referred to as word tokens , but",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 39,
      "token_count": 783,
      "chapter_title": ""
    }
  },
  {
    "content": "corpora. The largest, the Google n-grams corpus, contains 13 million types, but this count\nonly includes types appearing 40 or more times, so the true number would be much larger.\nHow many words are there in English? When we speak about the number of\nwords in the language, we are generally referring to word types. Fig. 2.11 shows\nthe rough numbers of types and instances computed from some English corpora.\nThe larger the corpora we look at, the more word types we \ufb01nd, and in fact this\nrelationship between the number of types jVjand number of instances Nis called\nHerdan\u2019s Law (Herdan, 1960) or Heaps\u2019 Law (Heaps, 1978) after its discoverers Herdan\u2019s Law\nHeaps\u2019 Law (in linguistics and information retrieval respectively). It is shown in Eq. 2.16, where\nkandbare positive constants, and 0 <b<1.\njVj=kNb(2.16)\nThe value of bdepends on the corpus size and the genre, but at least for the large\ncorpora in Fig. 2.11, branges from .67 to .75. Roughly then we can say that the\n1In earlier tradition, and occasionally still, you might see word instances referred to as word tokens , but\nwe now try to reserve the word token instead to mean the output of subword tokenization algorithms.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 40,
      "token_count": 309,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 41,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "2.3 \u2022 C ORPORA 15\nvocabulary size for a text goes up signi\ufb01cantly faster than the square root of its\nlength in words.\nIt\u2019s sometimes useful to make a further distinction. Consider in\ufb02ected forms like\ncatsversus cat. We say these two words are different wordforms but have the same\nlemma . Alemma is a set of lexical forms having the same stem, and usually the lemma\nsame major part-of-speech. The wordform is the full in\ufb02ected or derived form of wordform\nthe word. The two wordforms catandcatsthus have the same lemma, which we can\nrepresent as cat.\nFor morphologically complex languages like Arabic, we often need to deal with\nlemmatization. For most tasks in English, however, wordforms are suf\ufb01cient, and\nwhen we talk about words in this book we almost always mean wordforms (although\nwe will discuss basic algorithms for lemmatization and the related task of stemming\nbelow in Section 2.6). One of the situations even in English where we talk about\nlemmas is when we measure the number of words in a dictionary. Dictionary en-\ntries orboldface forms are a very rough approximation to (an upper bound on) the\nnumber of lemmas (since some lemmas have multiple boldface forms). The 1989\nedition of the Oxford English Dictionary had 615,000 entries.\nFinally, we should note that in practice, for many NLP applications (for example\nfor neural language modeling) we don\u2019t actually use words as our internal unit of\nrepresentation at all! We instead tokenize the input strings into tokens , which can\nbe words but can also be only parts of words. We\u2019ll return to this tokenization\nquestion when we introduce the BPE algorithm in Section 2.5.2.\n2.3 Corpora\nWords don\u2019t appear out of nowhere. Any particular piece of text that we study\nis produced by one or more speci\ufb01c speakers or writers, in a speci\ufb01c dialect of a\nspeci\ufb01c language, at a speci\ufb01c time, in a speci\ufb01c place, for a speci\ufb01c function.\nPerhaps the most important dimension of variation is the language. NLP algo-\nrithms are most useful when they apply across many languages. The world has 7097\nlanguages at the time of this writing, according to the online Ethnologue catalog\n(Simons and Fennig, 2018). It is important to test algorithms on more than one lan-\nguage, and particularly on languages with different properties; by contrast there is\nan unfortunate current tendency for NLP algorithms to be developed or tested just\non English (Bender, 2019). Even when algorithms are developed beyond English,\nthey tend to be developed for the of\ufb01cial languages of large industrialized nations\n(Chinese, Spanish, Japanese, German etc.), but we don\u2019t want to limit tools to just\nthese few languages. Furthermore, most languages also have multiple varieties, of-\nten spoken in different regions or by different social groups. Thus, for example,\nif we\u2019re processing text that uses features of African American English ( AAE ) or AAE\nAfrican American Vernacular English (AA VE)\u2014the variations of English used by\nmillions of people in African American communities (King 2020)\u2014we must use\nNLP tools that function with features of those varieties. Twitter posts might use fea-\ntures often used by speakers of African American English, such as constructions like\niont(I don\u2019t in Mainstream American English ( MAE )), or talmbout corresponding MAE",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 42,
      "token_count": 780,
      "chapter_title": ""
    }
  },
  {
    "content": "rithms are most useful when they apply across many languages. The world has 7097\nlanguages at the time of this writing, according to the online Ethnologue catalog\n(Simons and Fennig, 2018). It is important to test algorithms on more than one lan-\nguage, and particularly on languages with different properties; by contrast there is\nan unfortunate current tendency for NLP algorithms to be developed or tested just\non English (Bender, 2019). Even when algorithms are developed beyond English,\nthey tend to be developed for the of\ufb01cial languages of large industrialized nations\n(Chinese, Spanish, Japanese, German etc.), but we don\u2019t want to limit tools to just\nthese few languages. Furthermore, most languages also have multiple varieties, of-\nten spoken in different regions or by different social groups. Thus, for example,\nif we\u2019re processing text that uses features of African American English ( AAE ) or AAE\nAfrican American Vernacular English (AA VE)\u2014the variations of English used by\nmillions of people in African American communities (King 2020)\u2014we must use\nNLP tools that function with features of those varieties. Twitter posts might use fea-\ntures often used by speakers of African American English, such as constructions like\niont(I don\u2019t in Mainstream American English ( MAE )), or talmbout corresponding MAE\nto MAE talking about , both examples that in\ufb02uence word segmentation (Blodgett\net al. 2016, Jones 2015).\nIt\u2019s also quite common for speakers or writers to use multiple languages in a\nsingle communicative act, a phenomenon called code switching . Code switching code switching",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 43,
      "token_count": 356,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24\n\n16 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\nis enormously common across the world; here are examples showing Spanish and\n(transliterated) Hindi code switching with English (Solorio et al. 2014, Jurgens et al.\n2017):\n(2.17) Por primera vez veo a @username actually being hateful! it was beautiful:)\n[For the \ufb01rst time I get to see @username actually being hateful! it was\nbeautiful:) ]\n(2.18) dost tha or ra- hega ... dont wory ... but dherya rakhe\n[\u201che was and will remain a friend ... don\u2019t worry ... but have faith\u201d]\nAnother dimension of variation is the genre. The text that our algorithms must\nprocess might come from newswire, \ufb01ction or non-\ufb01ction books, scienti\ufb01c articles,\nWikipedia, or religious texts. It might come from spoken genres like telephone\nconversations, business meetings, police body-worn cameras, medical interviews,\nor transcripts of television shows or movies. It might come from work situations\nlike doctors\u2019 notes, legal text, or parliamentary or congressional proceedings.\nText also re\ufb02ects the demographic characteristics of the writer (or speaker): their\nage, gender, race, socioeconomic class can all in\ufb02uence the linguistic properties of\nthe text we are processing.\nAnd \ufb01nally, time matters too. Language changes over time, and for some lan-\nguages we have good corpora of texts from different historical periods.\nBecause language is so situated, when developing computational models for lan-\nguage processing from a corpus, it\u2019s important to consider who produced the lan-\nguage, in what context, for what purpose. How can a user of a dataset know all these\ndetails? The best way is for the corpus creator to build a datasheet (Gebru et al., datasheet\n2020) or data statement (Bender et al., 2021) for each corpus. A datasheet speci\ufb01es\nproperties of a dataset like:\nMotivation : Why was the corpus collected, by whom, and who funded it?\nSituation : When and in what situation was the text written/spoken? For example,\nwas there a task? Was the language originally spoken conversation, edited\ntext, social media communication, monologue vs. dialogue?\nLanguage variety : What language (including dialect/region) was the corpus in?\nSpeaker demographics : What was, e.g., the age or gender of the text\u2019s authors?\nCollection process : How big is the data? If it is a subsample how was it sampled?\nWas the data collected with consent? How was the data pre-processed, and\nwhat metadata is available?\nAnnotation process : What are the annotations, what are the demographics of the\nannotators, how were they trained, how was the data annotated?\nDistribution : Are there copyright or other intellectual property restrictions?\n2.4 Simple Unix Tools for Word Tokenization\nBefore almost any natural language processing of a text, the text has to be normal-\nized, a task called text normalization . At least three tasks are commonly applied astext\nnormalization\npart of any normalization process:\n1. Tokenizing (segmenting) words\n2. Normalizing word formats\n3. Segmenting sentences",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 44,
      "token_count": 701,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 25\n\n2.4 \u2022 S IMPLE UNIXTOOLS FOR WORD TOKENIZATION 17\nIn the next sections we walk through each of these tasks, but we\u2019ll \ufb01rst start with\nan easy, if somewhat naive version of word tokenization and normalization (and fre-\nquency computation) that can be accomplished for English solely in a single Unix\ncommand-line, inspired by Church (1994). We\u2019ll make use of some Unix com-\nmands:tr, used to systematically change particular characters in the input; sort ,\nwhich sorts input lines in alphabetical order; and uniq , which collapses and counts\nadjacent identical lines.\nFor example let\u2019s begin with the \u2018complete words\u2019 of Shakespeare in one \ufb01le,\nsh.txt . We can use trto tokenize the words by changing every sequence of non-\nalphabetic characters to a newline (\u2019A-Za-z\u2019 means alphabetic and the -c option\ncomplements to non-alphabet, so together they mean to change every non-alphabetic\ncharacter into a newline. The -s (\u2018squeeze\u2019) option is used to replace the result\nof multiple consecutive changes into a single output, so a series of non-alphabetic\ncharacters in a row would all be \u2018squeezed\u2019 into a single newline):\ntr -sc 'A-Za-z' '\\n' < sh.txt\nThe output of this command will be:\nTHE\nSONNETS\nby\nWilliam\nShakespeare\nFrom\nfairest\ncreatures\nWe\n...\nNow that there is one word per line, we can sort the lines, and pass them to uniq\n-cwhich will collapse and count them:\ntr -sc 'A-Za-z' '\\n' < sh.txt | sort | uniq -c\nwith the following output:\n1945 A\n72 AARON\n19 ABBESS\n25 Aaron\n6 Abate\n1 Abates\n5 Abbess\n6 Abbey\n3 Abbot\n...\nAlternatively, we can collapse all the upper case to lower case:\ntr -sc 'A-Za-z' '\\n' < sh.txt | tr A-Z a-z | sort | uniq -c\nwhose output is\n14725 a\n97 aaron\n1 abaissiez\n10 abandon",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 45,
      "token_count": 464,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 26\n\n18 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\n2 abandoned\n2 abase\n1 abash\n14 abate\n3 abated\n3 abatement\n...\nNow we can sort again to \ufb01nd the frequent words. The -noption tosort means\nto sort numerically rather than alphabetically, and the -roption means to sort in\nreverse order (highest-to-lowest):\ntr -sc 'A-Za-z' '\\n' < sh.txt | tr A-Z a-z | sort | uniq -c | sort -n -r\nThe results show that the most frequent words in Shakespeare, as in any other\ncorpus, are the short function words like articles, pronouns, prepositions:\n27378 the\n26084 and\n22538 i\n19771 to\n17481 of\n14725 a\n13826 you\n12489 my\n11318 that\n11112 in\n...\nUnix tools of this sort can be very handy in building quick word count statistics\nfor any corpus in English. While in some versions of Unix these command-line tools\nalso correctly handle Unicode characters and so can be used for many languages,\nin general for handling most languages outside English we use more sophisticated\ntokenization algorithms.\n2.5 Word and Subword Tokenization\nThe simple Unix tools above were \ufb01ne for getting rough word statistics but more\nsophisticated algorithms are generally necessary for tokenization , the task of seg- tokenization\nmenting running text into words. There are roughly two classes of tokenization\nalgorithms. In top-down tokenization, we de\ufb01ne a standard and implement rules to\nimplement that kind of tokenization.\nBut more commonly instead of using words as the input to NLP algorithms we\nbreak up words into subword tokens , which can be words or parts of words or subword tokens\neven individual letters. These are derived via bottom-up tokenization, in which we\nuse simple statistics of letter sequences to come up with the vocabulary of subword\ntokens, and break up the input into those subwords.\n2.5.1 Top-down (rule-based) tokenization\nWhile the Unix command sequence just removed all the numbers and punctuation,\nfor most NLP applications we\u2019ll need to keep these in our tokenization. We often",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 46,
      "token_count": 485,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 27",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 47,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "2.5 \u2022 W ORD AND SUBWORD TOKENIZATION 19\nwant to break off punctuation as a separate token; commas are a useful piece of infor-\nmation for parsers, and periods help indicate sentence boundaries. But we\u2019ll often\nwant to keep the punctuation that occurs word internally, in examples like m.p.h. ,\nPh.D. ,AT&T , and cap\u2019n . Special characters and numbers will need to be kept in\nprices ($45.55) and dates ( 01/02/06 ); we don\u2019t want to segment that price into sepa-\nrate tokens of \u201c45\u201d and \u201c55\u201d. And there are URLs ( https://www.stanford.edu ),\nTwitter hashtags ( #nlproc ), or email addresses ( someone@cs.colorado.edu ).\nNumber expressions introduce complications; in addition to appearing at word\nboundaries, commas appear inside numbers in English, every three digits: 555,500.50 .\nTokenization differs by language; languages like Spanish, French, and German, for\nexample, use a comma to mark the decimal point, and spaces (or sometimes periods)\nwhere English puts commas, for example, 555 500,50 .\nA tokenizer can also be used to expand clitic contractions that are marked by clitic\napostrophes, converting what're to the two tokens what are , andwe're towe\nare. A clitic is a part of a word that can\u2019t stand on its own, and can only occur\nwhen it is attached to another word. Such contractions occur in other alphabetic\nlanguages, including French pronouns ( j'ai and articles l'homme ).\nDepending on the application, tokenization algorithms may also tokenize mul-\ntiword expressions like New York orrock 'n' roll as a single token, which re-\nquires a multiword expression dictionary of some sort. Tokenization is thus inti-\nmately tied up with named entity recognition , the task of detecting names, dates,\nand organizations (Chapter 17).\nOne commonly used tokenization standard is known as the Penn Treebank to-\nkenization standard, used for the parsed corpora (treebanks) released by the Lin-Penn Treebank\ntokenization\nguistic Data Consortium (LDC), the source of many useful datasets. This standard\nseparates out clitics ( doesn\u2019t becomes does plus n\u2019t), keeps hyphenated words to-\ngether, and separates out all punctuation (to save space we\u2019re showing visible spaces\n\u2018\u2019 between tokens, although newlines is a more common output):\nInput :\"The San Francisco-based restaurant,\" they said,\n\"doesn't charge $10\".\nOutput :\"TheSanFrancisco-based restaurant ,\"theysaid,\n\"doesn'tcharge $10\".\nIn practice, since tokenization is run before any other language processing, it\nneeds to be very fast. For word tokenization we generally use deterministic algo-\nrithms based on regular expressions compiled into ef\ufb01cient \ufb01nite state automata.\nFor example, Fig. 2.12 shows a basic regular expression that can be used to tok-\nenize English with the nltk.regexp tokenize function of the Python-based Nat-\nural Language Toolkit (NLTK) (Bird et al. 2009; https://www.nltk.org ).\nCarefully designed deterministic algorithms can deal with the ambiguities that\narise, such as the fact that the apostrophe needs to be tokenized differently when used\nas a genitive marker (as in the book\u2019s cover ), a quotative as in \u2018The other class\u2019, she\nsaid, or in clitics like they\u2019re .\nWord tokenization is more complex in languages like written Chinese, Japanese,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 48,
      "token_count": 755,
      "chapter_title": ""
    }
  },
  {
    "content": "separates out clitics ( doesn\u2019t becomes does plus n\u2019t), keeps hyphenated words to-\ngether, and separates out all punctuation (to save space we\u2019re showing visible spaces\n\u2018\u2019 between tokens, although newlines is a more common output):\nInput :\"The San Francisco-based restaurant,\" they said,\n\"doesn't charge $10\".\nOutput :\"TheSanFrancisco-based restaurant ,\"theysaid,\n\"doesn'tcharge $10\".\nIn practice, since tokenization is run before any other language processing, it\nneeds to be very fast. For word tokenization we generally use deterministic algo-\nrithms based on regular expressions compiled into ef\ufb01cient \ufb01nite state automata.\nFor example, Fig. 2.12 shows a basic regular expression that can be used to tok-\nenize English with the nltk.regexp tokenize function of the Python-based Nat-\nural Language Toolkit (NLTK) (Bird et al. 2009; https://www.nltk.org ).\nCarefully designed deterministic algorithms can deal with the ambiguities that\narise, such as the fact that the apostrophe needs to be tokenized differently when used\nas a genitive marker (as in the book\u2019s cover ), a quotative as in \u2018The other class\u2019, she\nsaid, or in clitics like they\u2019re .\nWord tokenization is more complex in languages like written Chinese, Japanese,\nand Thai, which do not use spaces to mark potential word-boundaries. In Chinese,\nfor example, words are composed of characters (called hanzi in Chinese). Each hanzi\ncharacter generally represents a single unit of meaning (called a morpheme ) and is\npronounceable as a single syllable. Words are about 2.4 characters long on average.\nBut deciding what counts as a word in Chinese is complex. For example, consider\nthe following sentence:",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 49,
      "token_count": 381,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 28\n\n20 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\n>>> text = 'That U.S.A. poster-print costs $12.40...'\n>>> pattern = r'''(?x) # set flag to allow verbose regexps\n... (?:[A-Z]\\.)+ # abbreviations, e.g. U.S.A.\n... | \\w+(?:-\\w+)* # words with optional internal hyphens\n... | \\$?\\d+(?:\\.\\d+)?%? # currency, percentages, e.g. $12.40, 82%\n... | \\.\\.\\. # ellipsis\n... | [][.,;\"'?():_`-] # these are separate tokens; includes ], [\n... '''\n>>> nltk.regexp_tokenize(text, pattern)\n['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\nFigure 2.12 A Python trace of regular expression tokenization in the NLTK Python-based\nnatural language processing toolkit (Bird et al., 2009), commented for readability; the (?x)\nverbose \ufb02ag tells Python to strip comments and whitespace. Figure from Chapter 3 of Bird\net al. (2009).\n(2.19)\u59da\u660e\u8fdb\u5165\u603b\u51b3\u8d5b y\u00b4ao m \u00b4\u0131ng j`\u0131n r`u z\u02c7ong ju \u00b4e s`ai\n\u201cYao Ming reaches the \ufb01nals\u201d\nAs Chen et al. (2017b) point out, this could be treated as 3 words (\u2018Chinese Tree-\nbank\u2019 segmentation):\n(2.20)\u59da\u660e\nYaoMing\u8fdb\u5165\nreaches\u603b\u51b3\u8d5b\n\ufb01nals\nor as 5 words (\u2018Peking University\u2019 segmentation):\n(2.21)\u59da\nYao\u660e\nMing\u8fdb\u5165\nreaches\u603b\noverall\u51b3\u8d5b\n\ufb01nals\nFinally, it is possible in Chinese simply to ignore words altogether and use characters\nas the basic elements, treating the sentence as a series of 7 characters:\n(2.22)\u59da\nYao\u660e\nMing\u8fdb\nenter\u5165\nenter\u603b\noverall\u51b3\ndecision\u8d5b\ngame\nIn fact, for most Chinese NLP tasks it turns out to work better to take characters\nrather than words as input, since characters are at a reasonable semantic level for\nmost applications, and since most word standards, by contrast, result in a huge vo-\ncabulary with large numbers of very rare words (Li et al., 2019b).\nHowever, for Japanese and Thai the character is too small a unit, and so algo-\nrithms for word segmentation are required. These can also be useful for Chineseword\nsegmentation\nin the rare situations where word rather than character boundaries are required. For\nthese situations we can use the subword tokenization algorithms introduced in the\nnext section.\n2.5.2 Byte-Pair Encoding: A Bottom-up Tokenization Algorithm\nThere is a third option to tokenizing text, one that is most commonly used by large\nlanguage models. Instead of de\ufb01ning tokens as words (whether delimited by spaces\nor more complex algorithms), or as characters (as in Chinese), we can use our data to\nautomatically tell us what the tokens should be. This is especially useful in dealing\nwith unknown words, an important problem in language processing. As we will\nsee in the next chapter, NLP algorithms often learn some facts about language from\none corpus (a training corpus) and then use these facts to make decisions about a\nseparate testcorpus and its language. Thus if our training corpus contains, say the",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 50,
      "token_count": 787,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 29",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 51,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "2.5 \u2022 W ORD AND SUBWORD TOKENIZATION 21\nwords low,new,newer , but not lower , then if the word lower appears in our test\ncorpus, our system will not know what to do with it.\nTo deal with this unknown word problem, modern tokenizers automatically in-\nduce sets of tokens that include tokens smaller than words, called subwords . Sub- subwords\nwords can be arbitrary substrings, or they can be meaning-bearing units like the\nmorphemes -estor-er. (A morpheme is the smallest meaning-bearing unit of a lan-\nguage; for example the word unwashable has the morphemes un-,wash , and -able .)\nIn modern tokenization schemes, most tokens are words, but some tokens are fre-\nquently occurring morphemes or other subwords like -er. Every unseen word like\nlower can thus be represented by some sequence of known subword units, such as\nlowander, or even as a sequence of individual letters if necessary.\nMost tokenization schemes have two parts: a token learner , and a token seg-\nmenter . The token learner takes a raw training corpus (sometimes roughly pre-\nseparated into words, for example by whitespace) and induces a vocabulary, a set\nof tokens. The token segmenter takes a raw test sentence and segments it into the\ntokens in the vocabulary. Two algorithms are widely used: byte-pair encoding\n(Sennrich et al., 2016), and unigram language modeling (Kudo, 2018), There is\nalso a SentencePiece library that includes implementations of both of these (Kudo\nand Richardson, 2018a), and people often use the name SentencePiece to simply\nmean unigram language modeling tokenization.\nIn this section we introduce the simplest of the three, the byte-pair encoding or\nBPE algorithm (Sennrich et al., 2016); see Fig. 2.13. The BPE token learner begins BPE\nwith a vocabulary that is just the set of all individual characters. It then examines the\ntraining corpus, chooses the two symbols that are most frequently adjacent (say \u2018A\u2019,\n\u2018B\u2019), adds a new merged symbol \u2018AB\u2019 to the vocabulary, and replaces every adjacent\n\u2019A\u2019 \u2019B\u2019 in the corpus with the new \u2018AB\u2019. It continues to count and merge, creating\nnew longer and longer character strings, until kmerges have been done creating\nknovel tokens; kis thus a parameter of the algorithm. The resulting vocabulary\nconsists of the original set of characters plus knew symbols.\nThe algorithm is usually run inside words (not merging across word boundaries),\nso the input corpus is \ufb01rst white-space-separated to give a set of strings, each corre-\nsponding to the characters of a word, plus a special end-of-word symbol , and its\ncounts. Let\u2019s see its operation on the following tiny input corpus of 18 word tokens\nwith counts for each word (the word lowappears 5 times, the word newer 6 times,\nand so on), which would have a starting vocabulary of 11 letters:\ncorpus vocabulary\n5l o w , d, e, i, l, n, o, r, s, t, w\n2l o w e s t\n6n e w e r\n3w i d e r\n2n e w\nThe BPE algorithm \ufb01rst counts all pairs of adjacent symbols: the most frequent\nis the pair e rbecause it occurs in newer (frequency of 6) and wider (frequency of\n3) for a total of 9 occurrences.2We then merge these symbols, treating eras one\nsymbol, and count again:",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 52,
      "token_count": 775,
      "chapter_title": ""
    }
  },
  {
    "content": "new longer and longer character strings, until kmerges have been done creating\nknovel tokens; kis thus a parameter of the algorithm. The resulting vocabulary\nconsists of the original set of characters plus knew symbols.\nThe algorithm is usually run inside words (not merging across word boundaries),\nso the input corpus is \ufb01rst white-space-separated to give a set of strings, each corre-\nsponding to the characters of a word, plus a special end-of-word symbol , and its\ncounts. Let\u2019s see its operation on the following tiny input corpus of 18 word tokens\nwith counts for each word (the word lowappears 5 times, the word newer 6 times,\nand so on), which would have a starting vocabulary of 11 letters:\ncorpus vocabulary\n5l o w , d, e, i, l, n, o, r, s, t, w\n2l o w e s t\n6n e w e r\n3w i d e r\n2n e w\nThe BPE algorithm \ufb01rst counts all pairs of adjacent symbols: the most frequent\nis the pair e rbecause it occurs in newer (frequency of 6) and wider (frequency of\n3) for a total of 9 occurrences.2We then merge these symbols, treating eras one\nsymbol, and count again:\n2Note that there can be ties; we could have instead chosen to merge r \ufb01rst, since that also has a\nfrequency of 9.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 53,
      "token_count": 312,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 30\n\n22 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\ncorpus vocabulary\n5l o w , d, e, i, l, n, o, r, s, t, w, er\n2l o w e s t\n6n e w er\n3w i d er\n2n e w\nNow the most frequent pair is er , which we merge; our system has learned\nthat there should be a token for word-\ufb01nal er, represented as er:\ncorpus vocabulary\n5l o w ,d,e,i,l,n,o,r,s,t,w,er,er\n2l o w e s t\n6n e w er\n3w i d er\n2n e w\nNextn e(total count of 8) get merged to ne:\ncorpus vocabulary\n5l o w ,d,e,i,l,n,o,r,s,t,w,er,er,ne\n2l o w e s t\n6ne w er\n3w i d er\n2ne w\nIf we continue, the next merges are:\nmerge current vocabulary\n(ne, w) ,d,e,i,l,n,o,r,s,t,w,er,er,ne,new\n(l, o) ,d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo\n(lo, w) ,d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low\n(new, er ) ,d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer\n(low,) ,d,e,i,l,n,o,r,s,t,w,er,er,ne,new,lo,low,newer ,low\nfunction BYTE-PAIR ENCODING (strings C, number of merges k)returns vocab V\nV all unique characters in C # initial set of tokens is characters\nfori= 1tokdo # merge tokens ktimes\ntL,tR Most frequent pair of adjacent tokens in C\ntNEW tL+tR # make new token by concatenating\nV V+tNEW # update the vocabulary\nReplace each occurrence of tL,tRinCwith tNEW # and update the corpus\nreturn V\nFigure 2.13 The token learner part of the BPE algorithm for taking a corpus broken up\ninto individual characters or bytes, and learning a vocabulary by iteratively merging tokens.\nFigure adapted from Bostrom and Durrett (2020).\nOnce we\u2019ve learned our vocabulary, the token segmenter is used to tokenize a\ntest sentence. The token segmenter just runs on the merges we have learned from\nthe training data on the test data. It runs them greedily, in the order we learned them.\n(Thus the frequencies in the test data don\u2019t play a role, just the frequencies in the\ntraining data). So \ufb01rst we segment each test sentence word into characters. Then\nwe apply the \ufb01rst rule: replace every instance of e rin the test corpus with er, and",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 54,
      "token_count": 643,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 31\n\n2.6 \u2022 W ORD NORMALIZATION , LEMMATIZATION AND STEMMING 23\nthen the second rule: replace every instance of er in the test corpus with er,\nand so on. By the end, if the test corpus contained the character sequence n e w e\nr, it would be tokenized as a full word. But the characters of a new (unknown)\nword like l o w e r would be merged into the two tokens lower .\nOf course in real settings BPE is run with many thousands of merges on a very\nlarge input corpus. The result is that most words will be represented as full symbols,\nand only the very rare words (and unknown words) will have to be represented by\ntheir parts.\n2.6 Word Normalization, Lemmatization and Stemming\nWord normalization is the task of putting words or tokens in a standard format. The normalization\nsimplest case of word normalization is case folding . Mapping everything to lower case folding\ncase means that Woodchuck andwoodchuck are represented identically, which is\nvery helpful for generalization in many tasks, such as information retrieval or speech\nrecognition. For sentiment analysis and other text classi\ufb01cation tasks, information\nextraction, and machine translation, by contrast, case can be quite helpful and case\nfolding is generally not done. This is because maintaining the difference between,\nfor example, USthe country and usthe pronoun can outweigh the advantage in\ngeneralization that case folding would have provided for other words. Sometimes\nwe produce both cased (i.e. including both upper and lower case words or tokens)\nand uncased versions of language models.\nSystems that use BPE or other kinds of bottom-up tokenization may do no fur-\nther word normalization. In other NLP systems, we may want to do further nor-\nmalizations, like choosing a single normal form for words with multiple forms like\nUSA andUSoruh-huh anduhhuh . This standardization may be valuable, despite\nthe spelling information that is lost in the normalization process. For information\nretrieval or information extraction about the US, we might want to see information\nfrom documents whether they mention the USor theUSA.\n2.6.1 Lemmatization\nFor other natural language processing situations we also want two morphologically\ndifferent forms of a word to behave similarly. For example in web search, someone\nmay type the string woodchucks but a useful system might want to also return pages\nthat mention woodchuck with no s. This is especially common in morphologically\ncomplex languages like Polish, where for example the word Warsaw has different\nendings when it is the subject ( Warszawa ), or after a preposition like \u201cin Warsaw\u201d ( w\nWarszawie ), or \u201cto Warsaw\u201d ( do Warszawy ), and so on. Lemmatization is the task lemmatization\nof determining that two words have the same root, despite their surface differences.\nThe words am,are, and ishave the shared lemma be; the words dinner anddinners\nboth have the lemma dinner . Lemmatizing each of these forms to the same lemma\nwill let us \ufb01nd all mentions of words in Polish like Warsaw . The lemmatized form\nof a sentence like He is reading detective stories would thus be He be read detective\nstory .\nHow is lemmatization done? The most sophisticated methods for lemmatization\ninvolve complete morphological parsing of the word. Morphology is the study of\nthe way words are built up from smaller meaning-bearing units called morphemes . morpheme\nTwo broad classes of morphemes can be distinguished: stems \u2014the central mor- stem",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 55,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 32\n\n24 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\npheme of the word, supplying the main meaning\u2014and af\ufb01xes \u2014adding \u201cadditional\u201d af\ufb01x\nmeanings of various kinds. So, for example, the word foxconsists of one morpheme\n(the morpheme fox) and the word cats consists of two: the morpheme catand the\nmorpheme -s. A morphological parser takes a word like cats and parses it into the\ntwo morphemes catands, or parses a Spanish word like amaren (\u2018if in the future\nthey would love\u2019) into the morpheme amar \u2018to love\u2019, and the morphological features\n3PL (third person plural) and future subjunctive .\nStemming: The Porter Stemmer\nLemmatization algorithms can be complex. For this reason we sometimes make\nuse of a simpler but cruder method, which mainly consists of chopping off word-\n\ufb01nal af\ufb01xes. This naive version of morphological analysis is called stemming . For stemming\nexample, the classic Porter stemmer (Porter, 1980), when applied to the following Porter stemmer\nparagraph:\nThis was not the map we found in Billy Bones's chest, but\nan accurate copy, complete in all things-names and heights\nand soundings-with the single exception of the red crosses\nand the written notes.\nproduces the following stemmed output:\nThi wa not the map we found in Billi Bone s chest but an\naccur copi complet in all thing name and height and sound\nwith the singl except of the red cross and the written note\nThe algorithm is based on rewrite rules run in series, with the output of each pass\nfed as input to the next pass. Some sample rules (more at https://tartarus.org/\nmartin/PorterStemmer/ ):\nATIONAL!ATE (e.g., relational !relate)\nING!\u000fif the stem contains a vowel (e.g., motoring !motor)\nSSES!SS (e.g., grasses !grass)\nSimple stemmers can be useful in cases where we need to collapse across dif-\nferent variants of the same lemma. Nonetheless, they are less commonly used in\nmodern systems since they commit errors of both over-generalizing (lemmatizing\npolicy topolice ) and under-generalizing (not lemmatizing European toEurope )\n(Krovetz, 1993).\n2.7 Sentence Segmentation\nSentence segmentation is another important step in text processing. The most use-sentence\nsegmentation\nful cues for segmenting a text into sentences are punctuation, like periods, question\nmarks, and exclamation points. Question marks and exclamation points are rela-\ntively unambiguous markers of sentence boundaries. Periods, on the other hand, are\nmore ambiguous. The period character \u201c.\u201d is ambiguous between a sentence bound-\nary marker and a marker of abbreviations like Mr.orInc.The previous sentence that\nyou just read showed an even more complex case of this ambiguity, in which the \ufb01nal\nperiod of Inc. marked both an abbreviation and the sentence boundary marker. For\nthis reason, sentence tokenization and word tokenization may be addressed jointly.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 56,
      "token_count": 680,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 33\n\n2.8 \u2022 M INIMUM EDITDISTANCE 25\nIn general, sentence tokenization methods work by \ufb01rst deciding (based on rules\nor machine learning) whether a period is part of the word or is a sentence-boundary\nmarker. An abbreviation dictionary can help determine whether the period is part\nof a commonly used abbreviation; the dictionaries can be hand-built or machine-\nlearned (Kiss and Strunk, 2006), as can the \ufb01nal sentence splitter. In the Stanford\nCoreNLP toolkit (Manning et al., 2014), for example sentence splitting is rule-based,\na deterministic consequence of tokenization; a sentence ends when a sentence-ending\npunctuation (., !, or ?) is not already grouped with other characters into a token (such\nas for an abbreviation or number), optionally followed by additional \ufb01nal quotes or\nbrackets.\n2.8 Minimum Edit Distance\nMuch of natural language processing is concerned with measuring how similar two\nstrings are. For example in spelling correction, the user typed some erroneous\nstring\u2014let\u2019s say graffe \u2013and we want to know what the user meant. The user prob-\nably intended a word that is similar to graffe . Among candidate similar words,\nthe wordgiraffe , which differs by only one letter from graffe , seems intuitively\nto be more similar than, say grail orgraf , which differ in more letters. Another\nexample comes from coreference , the task of deciding whether two strings such as\nthe following refer to the same entity:\nStanford Arizona Cactus Garden\nStanford University Arizona Cactus Garden\nAgain, the fact that these two strings are very similar (differing by only one word)\nseems like useful evidence for deciding that they might be coreferent. Finally, string\nsimilarity is commonly used to measure the quality of the transcription produced by\na speech recognition system, by asking how similar (in words) the transcript is to a\nreference transcript. A system whose transcript is off by many words is measurably\nworse than one which is only off by a few words.\nEdit distance gives us a way to quantify these intuitions about string similarity.\nMore formally, the minimum edit distance between two strings is de\ufb01ned as theminimum edit\ndistance\nminimum number of editing operations (operations like insertion, deletion, substitu-\ntion) needed to transform one string into another.\nThe gap between intention andexecution , for example, is 5 (delete an i, substi-\ntuteeforn, substitute xfort, insertc, substitute uforn). It\u2019s much easier to see\nthis by looking at the most important visualization for string distances, an alignment alignment\nbetween the two strings, shown in Fig. 2.14. Given two sequences, an alignment is\na correspondence between substrings of the two sequences. Thus, we say Ialigns\nwith the empty string, NwithE, and so on. Beneath the aligned strings is another\nrepresentation; a series of symbols expressing an operation list for converting the\ntop string into the bottom string: dfor deletion, sfor substitution, ifor insertion.\nWe can also assign a particular cost or weight to each of these operations. The\nLevenshtein distance between two sequences is the simplest weighting factor in\nwhich each of the three operations has a cost of 1 (Levenshtein, 1966)\u2014we assume\nthat the substitution of a letter for itself, for example, tfort, has zero cost. The Lev-\nenshtein distance between intention andexecution is 5. Levenshtein also proposed\nan alternative version of his metric in which each insertion or deletion has a cost of\n1 and substitutions are not allowed. (This is equivalent to allowing substitution, but",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 57,
      "token_count": 792,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 34\n\n26 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\nINTE*NTION\njjjjjjjjjj\n*EXECUTION\nd s s i s\nFigure 2.14 Representing the minimum edit distance between two strings as an alignment .\nThe \ufb01nal row gives the operation list for converting the top string into the bottom string: d for\ndeletion, s for substitution, i for insertion.\ngiving each substitution a cost of 2 since any substitution can be represented by one\ninsertion and one deletion). Using this version, the Levenshtein distance between\nintention andexecution is 8.\n2.8.1 The Minimum Edit Distance Algorithm\nHow do we \ufb01nd the minimum edit distance? We can think of this as a search task, in\nwhich we are searching for the shortest path\u2014a sequence of edits\u2014from one string\nto another.\nn t e n t i o ni n t e c n t i o ni n x e n t i o ndelinssubsti n t e n t i o n\nFigure 2.15 Finding the edit distance viewed as a search problem\nThe space of all possible edits is enormous, so we can\u2019t search naively. However,\nlots of distinct edit paths will end up in the same state (string), so rather than recom-\nputing all those paths, we could just remember the shortest path to a state each time\nwe saw it. We can do this by using dynamic programming . Dynamic programmingdynamic\nprogramming\nis the name for a class of algorithms, \ufb01rst introduced by Bellman (1957), that apply\na table-driven method to solve problems by combining solutions to subproblems.\nSome of the most commonly used algorithms in natural language processing make\nuse of dynamic programming, such as the Viterbi algorithm (Chapter 17) and the\nCKY algorithm for parsing (Chapter 18).\nThe intuition of a dynamic programming problem is that a large problem can\nbe solved by properly combining the solutions to various subproblems. Consider\nthe shortest path of transformed words that represents the minimum edit distance\nbetween the strings intention andexecution shown in Fig. 2.16.\nImagine some string (perhaps it is exention ) that is in this optimal path (whatever\nit is). The intuition of dynamic programming is that if exention is in the optimal\noperation list, then the optimal sequence must also include the optimal path from\nintention toexention . Why? If there were a shorter path from intention toexention ,\nthen we could use it instead, resulting in a shorter overall path, and the optimal\nsequence wouldn\u2019t be optimal, thus leading to a contradiction.\nThe minimum edit distance algorithm was named by Wagner and Fischerminimum edit\ndistance\nalgorithm(1974) but independently discovered by many people (see the Historical Notes sec-\ntion of Chapter 17).\nLet\u2019s \ufb01rst de\ufb01ne the minimum edit distance between two strings. Given two\nstrings, the source string Xof length n, and target string Yof length m, we\u2019ll de\ufb01ne",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 58,
      "token_count": 646,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 35\n\n2.8 \u2022 M INIMUM EDITDISTANCE 27\nn t e n t i o ni n t e n t i o n\ne t e n t i o n\ne x e n t i o n\ne x e n u t i o n\ne x e c u t i o ndelete i\nsubstitute n by e\nsubstitute t by x\ninsert u\nsubstitute n by c\nFigure 2.16 Path from intention toexecution .\nD[i;j]as the edit distance between X[1::i]andY[1::j], i.e., the \ufb01rst icharacters of X\nand the \ufb01rst jcharacters of Y. The edit distance between XandYis thus D[n;m].\nWe\u2019ll use dynamic programming to compute D[n;m]bottom up, combining so-\nlutions to subproblems. In the base case, with a source substring of length ibut an\nempty target string, going from icharacters to 0 requires ideletes. With a target\nsubstring of length jbut an empty source going from 0 characters to jcharacters\nrequires jinserts. Having computed D[i;j]for small i;jwe then compute larger\nD[i;j]based on previously computed smaller values. The value of D[i;j]is com-\nputed by taking the minimum of the three possible paths through the matrix which\narrive there:\nD[i;j] =min8\n<\n:D[i\u00001;j]+del-cost (source [i])\nD[i;j\u00001]+ins-cost (target [j])\nD[i\u00001;j\u00001]+sub-cost (source [i];target [j])(2.23)\nWe mentioned above two versions of Levenshtein distance, one in which substitu-\ntions cost 1 and one in which substitutions cost 2 (i.e., are equivalent to an insertion\nplus a deletion). Let\u2019s here use that second version of Levenshtein distance in which\nthe insertions and deletions each have a cost of 1 (ins-cost( \u0001) = del-cost(\u0001) = 1), and\nsubstitutions have a cost of 2 (except substitution of identical letters has zero cost).\nUnder this version of Levenshtein, the computation for D[i;j]becomes:\nD[i;j] =min8\n>><\n>>:D[i\u00001;j]+1\nD[i;j\u00001]+1\nD[i\u00001;j\u00001]+\u001a2; if source [i]6=target [j]\n0; if source [i] =target [j](2.24)\nThe algorithm is summarized in Fig. 2.17; Fig. 2.18 shows the results of applying\nthe algorithm to the distance between intention andexecution with the version of\nLevenshtein in Eq. 2.24.\nAlignment Knowing the minimum edit distance is useful for algorithms like \ufb01nd-\ning potential spelling error corrections. But the edit distance algorithm is important\nin another way; with a small change, it can also provide the minimum cost align-\nment between two strings. Aligning two strings is useful throughout speech and\nlanguage processing. In speech recognition, minimum edit distance alignment is\nused to compute the word error rate (Chapter 16). Alignment plays a role in ma-\nchine translation, in which sentences in a parallel corpus (a corpus with a text in two\nlanguages) need to be matched to each other.\nTo extend the edit distance algorithm to produce an alignment, we can start by\nvisualizing an alignment as a path through the edit distance matrix. Figure 2.19",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 59,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 36\n\n28 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\nfunction MIN-EDIT-DISTANCE (source ,target )returns min-distance\nn LENGTH (source )\nm LENGTH (target )\nCreate a distance matrix D[n+1,m+1]\n#Initialization: the zeroth row and column is the distance from the empty string\nD[0,0] = 0\nforeach row ifrom 1tondo\nD[i,0] D[i-1,0] + del-cost (source [i])\nforeach column jfrom 1tomdo\nD[0,j] D[0,j-1] + ins-cost (target [j])\n#Recurrence relation:\nforeach row ifrom 1tondo\nforeach column jfrom 1tomdo\nD[i, j] MIN(D[i\u00001,j] +del-cost (source [i]),\nD[i\u00001,j\u00001] + sub-cost (source [i],target [j]),\nD[i,j\u00001] + ins-cost (target [j]))\n#Termination\nreturn D[n,m]\nFigure 2.17 The minimum edit distance algorithm, an example of the class of dynamic\nprogramming algorithms. The various costs can either be \ufb01xed (e.g., 8x;ins-cost (x) =1)\nor can be speci\ufb01c to the letter (to model the fact that some letters are more likely to be in-\nserted than others). We assume that there is no cost for substituting a letter for itself (i.e.,\nsub-cost (x;x) =0).\nSrcnTar # e x e c u t i o n\n# 0 1 2 3 4 5 6 7 8 9\ni 1 2 3 4 5 6 7 6 7 8\nn 2 3 4 5 6 7 8 7 8 7\nt 3 4 5 6 7 8 7 8 9 8\ne 4 3 4 5 6 7 8 9 10 9\nn 5 4 5 6 7 8 9 10 11 10\nt 6 5 6 7 8 9 8 9 10 11\ni 7 6 7 8 9 10 9 8 9 10\no 8 7 8 9 10 11 10 9 8 9\nn 9 8 9 10 11 12 11 10 9 8\nFigure 2.18 Computation of minimum edit distance between intention andexecution with\nthe algorithm of Fig. 2.17, using Levenshtein distance with cost of 1 for insertions or dele-\ntions, 2 for substitutions.\nshows this path with boldfaced cells. Each boldfaced cell represents an alignment\nof a pair of letters in the two strings. If two boldfaced cells occur in the same row,\nthere will be an insertion in going from the source to the target; two boldfaced cells\nin the same column indicate a deletion.\nFigure 2.19 also shows the intuition of how to compute this alignment path. The\ncomputation proceeds in two steps. In the \ufb01rst step, we augment the minimum edit\ndistance algorithm to store backpointers in each cell. The backpointer from a cell\npoints to the previous cell (or cells) that we came from in entering the current cell.\nWe\u2019ve shown a schematic of these backpointers in Fig. 2.19. Some cells have mul-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 60,
      "token_count": 791,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 37",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 61,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "2.9 \u2022 S UMMARY 29\ntiple backpointers because the minimum extension could have come from multiple\nprevious cells. In the second step, we perform a backtrace . In a backtrace, we start backtrace\nfrom the last cell (at the \ufb01nal row and column), and follow the pointers back through\nthe dynamic programming matrix. Each complete path between the \ufb01nal cell and the\ninitial cell is a minimum distance alignment. Exercise 2.7 asks you to modify the\nminimum edit distance algorithm to store the pointers and compute the backtrace to\noutput an alignment.\n# e x e c u t i o n\n# 0 1 2 3 4 5 6 7 8 9\ni\"1- \" 2- \" 3- \" 4- \" 5- \" 6- \" 7-6 7 8\nn\"2- \" 3- \" 4- \" 5- \" 6- \" 7- \" 8\"7- \" 8-7\nt\"3- \" 4- \" 5- \" 6- \" 7- \" 8-7 \"8- \" 9\"8\ne\"4-3 4- 5 6 7 \"8- \" 9- \" 10\"9\nn\"5\"4- \" 5- \" 6- \" 7- \" 8- \" 9- \" 10- \" 11-\"10\nt\"6\"5- \" 6- \" 7- \" 8- \" 9-8 9 10 \"11\ni\"7\"6- \" 7- \" 8- \" 9- \" 10\"9-8 9 10\no\"8\"7- \" 8- \" 9- \" 10- \" 11\"10\"9-8 9\nn\"9\"8- \" 9- \" 10- \" 11- \" 12\"11\"10\"9-8\nFigure 2.19 When entering a value in each cell, we mark which of the three neighboring\ncells we came from with up to three arrows. After the table is full we compute an alignment\n(minimum edit path) by using a backtrace , starting at the 8in the lower-right corner and\nfollowing the arrows back. The sequence of bold cells represents one possible minimum\ncost alignment between the two strings, again using Levenshtein distance with cost of 1 for\ninsertions or deletions, 2 for substitutions. Diagram design after Gus\ufb01eld (1997).\nWhile we worked our example with simple Levenshtein distance, the algorithm\nin Fig. 2.17 allows arbitrary weights on the operations. For spelling correction, for\nexample, substitutions are more likely to happen between letters that are next to\neach other on the keyboard. The Viterbi algorithm is a probabilistic extension of\nminimum edit distance. Instead of computing the \u201cminimum edit distance\u201d between\ntwo strings, Viterbi computes the \u201cmaximum probability alignment\u201d of one string\nwith another. We\u2019ll discuss this more in Chapter 17.\n2.9 Summary\nThis chapter introduced a fundamental tool in language processing, the regular ex-\npression , and showed how to perform basic text normalization tasks including\nword segmentation andnormalization ,sentence segmentation , and stemming .\nWe also introduced the important minimum edit distance algorithm for comparing\nstrings. Here\u2019s a summary of the main points we covered about these ideas:\n\u2022 The regular expression language is a powerful tool for pattern-matching.\n\u2022 Basic operations in regular expressions include concatenation of symbols,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 62,
      "token_count": 780,
      "chapter_title": ""
    }
  },
  {
    "content": "(minimum edit path) by using a backtrace , starting at the 8in the lower-right corner and\nfollowing the arrows back. The sequence of bold cells represents one possible minimum\ncost alignment between the two strings, again using Levenshtein distance with cost of 1 for\ninsertions or deletions, 2 for substitutions. Diagram design after Gus\ufb01eld (1997).\nWhile we worked our example with simple Levenshtein distance, the algorithm\nin Fig. 2.17 allows arbitrary weights on the operations. For spelling correction, for\nexample, substitutions are more likely to happen between letters that are next to\neach other on the keyboard. The Viterbi algorithm is a probabilistic extension of\nminimum edit distance. Instead of computing the \u201cminimum edit distance\u201d between\ntwo strings, Viterbi computes the \u201cmaximum probability alignment\u201d of one string\nwith another. We\u2019ll discuss this more in Chapter 17.\n2.9 Summary\nThis chapter introduced a fundamental tool in language processing, the regular ex-\npression , and showed how to perform basic text normalization tasks including\nword segmentation andnormalization ,sentence segmentation , and stemming .\nWe also introduced the important minimum edit distance algorithm for comparing\nstrings. Here\u2019s a summary of the main points we covered about these ideas:\n\u2022 The regular expression language is a powerful tool for pattern-matching.\n\u2022 Basic operations in regular expressions include concatenation of symbols,\ndisjunction of symbols ( [],|),counters (*,+, and{n,m} ),anchors (^,$)\nand precedence operators ( (,)).\n\u2022Word tokenization and normalization are generally done by cascades of\nsimple regular expression substitutions or \ufb01nite automata.\n\u2022 The Porter algorithm is a simple and ef\ufb01cient way to do stemming , stripping\noff af\ufb01xes. It does not have high accuracy but may be useful for some tasks.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 63,
      "token_count": 386,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 38\n\n30 CHAPTER 2 \u2022 R EGULAR EXPRESSIONS , TOKENIZATION , EDITDISTANCE\n\u2022 The minimum edit distance between two strings is the minimum number of\noperations it takes to edit one into the other. Minimum edit distance can be\ncomputed by dynamic programming , which also results in an alignment of\nthe two strings.\nBibliographical and Historical Notes\nKleene 1951; 1956 \ufb01rst de\ufb01ned regular expressions and the \ufb01nite automaton, based\non the McCulloch-Pitts neuron. Ken Thompson was one of the \ufb01rst to build regular\nexpressions compilers into editors for text searching (Thompson, 1968). His edi-\ntoredincluded a command \u201cg/regular expression/p\u201d, or Global Regular Expression\nPrint, which later became the Unix grep utility.\nText normalization algorithms have been applied since the beginning of the\n\ufb01eld. One of the earliest widely used stemmers was Lovins (1968). Stemming\nwas also applied early to the digital humanities, by Packard (1973), who built an\naf\ufb01x-stripping morphological parser for Ancient Greek. Currently a wide vari-\nety of code for tokenization and normalization is available, such as the Stanford\nTokenizer ( https://nlp.stanford.edu/software/tokenizer.shtml ) or spe-\ncialized tokenizers for Twitter (O\u2019Connor et al., 2010), or for sentiment ( http:\n//sentiment.christopherpotts.net/tokenizing.html ). See Palmer (2012)\nfor a survey of text preprocessing. NLTK is an essential tool that offers both useful\nPython libraries ( https://www.nltk.org ) and textbook descriptions (Bird et al.,\n2009) of many algorithms including text normalization and corpus interfaces.\nFor more on Herdan\u2019s law and Heaps\u2019 Law, see Herdan (1960, p. 28), Heaps\n(1978), Egghe (2007) and Baayen (2001); For more on edit distance, see Gus\ufb01eld\n(1997). Our example measuring the edit distance from \u2018intention\u2019 to \u2018execution\u2019\nwas adapted from Kruskal (1983). There are various publicly available packages to\ncompute edit distance, including Unix diff and the NIST sclite program (NIST,\n2005).\nIn his autobiography Bellman (1984) explains how he originally came up with\nthe term dynamic programming :\n\u201c...The 1950s were not good years for mathematical research. [the]\nSecretary of Defense ...had a pathological fear and hatred of the word,\nresearch... I decided therefore to use the word, \u201cprogramming\u201d. I\nwanted to get across the idea that this was dynamic, this was multi-\nstage... I thought, let\u2019s ... take a word that has an absolutely precise\nmeaning, namely dynamic... it\u2019s impossible to use the word, dynamic,\nin a pejorative sense. Try thinking of some combination that will pos-\nsibly give it a pejorative meaning. It\u2019s impossible. Thus, I thought\ndynamic programming was a good name. It was something not even a\nCongressman could object to.\u201d\nExercises\n2.1 Write regular expressions for the following languages.\n1. the set of all alphabetic strings;\n2. the set of all lower case alphabetic strings ending in a b;",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 64,
      "token_count": 701,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 39\n\nEXERCISES 31\n3. the set of all strings from the alphabet a;bsuch that each ais immedi-\nately preceded by and immediately followed by a b;\n2.2 Write regular expressions for the following languages. By \u201cword\u201d, we mean\nan alphabetic string separated from other words by whitespace, any relevant\npunctuation, line breaks, and so forth.\n1. the set of all strings with two consecutive repeated words (e.g., \u201cHum-\nbert Humbert\u201d and \u201cthe the\u201d but not \u201cthe bug\u201d or \u201cthe big bug\u201d);\n2. all strings that start at the beginning of the line with an integer and that\nend at the end of the line with a word;\n3. all strings that have both the word grotto and the word raven in them\n(but not, e.g., words like grottos that merely contain the word grotto );\n4. write a pattern that places the \ufb01rst word of an English sentence in a\nregister. Deal with punctuation.\n2.3 Implement an ELIZA-like program, using substitutions such as those described\non page 12. You might want to choose a different domain than a Rogerian psy-\nchologist, although keep in mind that you would need a domain in which your\nprogram can legitimately engage in a lot of simple repetition.\n2.4 Compute the edit distance (using insertion cost 1, deletion cost 1, substitution\ncost 1) of \u201cleda\u201d to \u201cdeal\u201d. Show your work (using the edit distance grid).\n2.5 Figure out whether drive is closer to brief or to divers and what the edit dis-\ntance is to each. You may use any version of distance that you like.\n2.6 Now implement a minimum edit distance algorithm and use your hand-computed\nresults to check your code.\n2.7 Augment the minimum edit distance algorithm to output an alignment; you\nwill need to store pointers and add a stage to compute the backtrace.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 65,
      "token_count": 420,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 40\n\n32 CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\nCHAPTER\n3N-gram Language Models\n\u201cYou are uniformly charming!\u201d cried he, with a smile of associating and now\nand then I bowed and they perceived a chaise and four to wish for.\nRandom sentence generated from a Jane Austen trigram model\nPredicting is dif\ufb01cult\u2014especially about the future, as the old quip goes. But how\nabout predicting something that seems much easier, like the next word someone is\ngoing to say? What word, for example, is likely to follow\nThe water of Walden Pond is so beautifully ...\nYou might conclude that a likely word is blue , orgreen , orclear , but probably\nnotrefrigerator northis . In this chapter we formalize this intuition by intro-\nducing language models orLMs . A language model is a machine learning model language model\nLM that predicts upcoming words. More formally, a language model assigns a prob-\nability to each possible next word, or equivalently gives a probability distribution\nover possible next works. Language models can also assign a probability to an entire\nsentence. Thus an LM could tell us that the following sequence has a much higher\nprobability of appearing in a text:\nall of a sudden I notice three guys standing on the sidewalk\nthan does this same set of words in a different order:\non guys all I of notice sidewalk three a sudden standing the\nWhy would we want to predict upcoming words, or know the probability of a sen-\ntence? One reason is for generation: choosing contextually better words. For ex-\nample we can correct grammar or spelling errors like Their are two midterms ,\nin whichThere was mistyped as Their , orEverything has improve , in which\nimprove should have been improved . The phrase There are is more probable\nthanTheir are , andhas improved thanhas improve , so a language model can\nhelp users select the more grammatical variant. Or for a speech system to recognize\nthat you said I will be back soonish and notI will be bassoon dish , it\nhelps to know that back soonish is a more probable sequence. Language models\ncan also help in augmentative and alternative communication (Trnka et al. 2007,\nKane et al. 2017). People can use AAC systems if they are physically unable to AAC\nspeak or sign but can instead use eye gaze or other movements to select words from\na menu. Word prediction can be used to suggest likely words for the menu.\nWord prediction is also central to NLP for another reason: large language mod-\nelsare built just by training them to predict words!! As we\u2019ll see in chapters 7-9,\nlarge language models learn an enormous amount about language solely from being\ntrained to predict upcoming words from neighboring words.\nIn this chapter we introduce the simplest kind of language model: the n-gram n-gram",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 66,
      "token_count": 608,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 41",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 67,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "3.1 \u2022 N-G RAMS 33\nlanguage model. An n-gram is a sequence of nwords: a 2-gram (which we\u2019ll call\nbigram ) is a two-word sequence of words like The water , orwater of , and a 3-\ngram (a trigram ) is a three-word sequence of words like The water of , orwater\nof Walden . But we also (in a bit of terminological ambiguity) use the word \u2018n-\ngram\u2019 to mean a probabilistic model that can estimate the probability of a word given\nthe n-1 previous words, and thereby also to assign probabilities to entire sequences.\nIn later chapters we will introduce the much more powerful neural large lan-\nguage models , based on the transformer architecture of Chapter 9. But because\nn-grams have a remarkably simple and clear formalization, we use them to intro-\nduce some major concepts of large language modeling, including training and test\nsets,perplexity ,sampling , and interpolation .\n3.1 N-Grams\nLet\u2019s begin with the task of computing P(wjh), the probability of a word wgiven\nsome history h. Suppose the history his \u201cThe water of Walden Pond is so\nbeautifully \u201d and we want to know the probability that the next word is blue :\nP(bluejThe water of Walden Pond is so beautifully ) (3.1)\nOne way to estimate this probability is directly from relative frequency counts: take a\nvery large corpus, count the number of times we see The water of Walden Pond\nis so beautifully , and count the number of times this is followed by blue . This\nwould be answering the question \u201cOut of the times we saw the history h, how many\ntimes was it followed by the word w\u201d, as follows:\nP(bluejThe water of Walden Pond is so beautifully ) =\nC(The water of Walden Pond is so beautifully blue )\nC(The water of Walden Pond is so beautifully )(3.2)\nIf we had a large enough corpus, we could compute these two counts and estimate\nthe probability from Eq. 3.2. But even the entire web isn\u2019t big enough to give us\ngood estimates for counts of entire sentences. This is because language is creative ;\nnew sentences are invented all the time, and we can\u2019t expect to get accurate counts\nfor such large objects as entire sentences. For this reason, we\u2019ll need more clever\nways to estimate the probability of a word wgiven a history h, or the probability of\nan entire word sequence W.\nLet\u2019s start with some notation. First, throughout this chapter we\u2019ll continue to\nrefer to words , although in practice we usually compute language models over to-\nkens like the BPE tokens of page 20. To represent the probability of a particular\nrandom variable Xitaking on the value \u201cthe\u201d, or P(Xi=\u201cthe\u201d), we will use the\nsimpli\ufb01cation P(the). We\u2019ll represent a sequence of nwords either as w1:::wnor\nw1:n. Thus the expression w1:n\u00001means the string w1;w2;:::;wn\u00001, but we\u2019ll also\nbe using the equivalent notation w<n, which can be read as \u201call the elements of w\nfrom w1up to and including wn\u00001\u201d. For the joint probability of each word in a se-\nquence having a particular value P(X1=w1;X2=w2;X3=w3;:::;Xn=wn)we\u2019ll\nuseP(w1;w2;:::;wn).\nNow, how can we compute probabilities of entire sequences like P(w1;w2;:::;wn)?\nOne thing we can do is decompose this probability using the chain rule of proba-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 68,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 42\n\n34 CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\nbility :\nP(X1:::Xn) = P(X1)P(X2jX1)P(X3jX1:2):::P(XnjX1:n\u00001)\n=nY\nk=1P(XkjX1:k\u00001) (3.3)\nApplying the chain rule to words, we get\nP(w1:n) = P(w1)P(w2jw1)P(w3jw1:2):::P(wnjw1:n\u00001)\n=nY\nk=1P(wkjw1:k\u00001) (3.4)\nThe chain rule shows the link between computing the joint probability of a sequence\nand computing the conditional probability of a word given previous words. Equa-\ntion 3.4 suggests that we could estimate the joint probability of an entire sequence of\nwords by multiplying together a number of conditional probabilities. But using the\nchain rule doesn\u2019t really seem to help us! We don\u2019t know any way to compute the\nexact probability of a word given a long sequence of preceding words, P(wnjw1:n\u00001).\nAs we said above, we can\u2019t just estimate by counting the number of times every word\noccurs following every long string in some corpus, because language is creative and\nany particular context might have never occurred before!\n3.1.1 The Markov assumption\nThe intuition of the n-gram model is that instead of computing the probability of a\nword given its entire history, we can approximate the history by just the last few\nwords.\nThebigram model, for example, approximates the probability of a word given bigram\nall the previous words P(wnjw1:n\u00001)by using only the conditional probability given\nthe preceding word P(wnjwn\u00001). In other words, instead of computing the probabil-\nity\nP(bluejThe water of Walden Pond is so beautifully ) (3.5)\nwe approximate it with the probability\nP(bluejbeautifully ) (3.6)\nWhen we use a bigram model to predict the conditional probability of the next word,\nwe are thus making the following approximation:\nP(wnjw1:n\u00001)\u0019P(wnjwn\u00001) (3.7)\nThe assumption that the probability of a word depends only on the previous word is\ncalled a Markov assumption. Markov models are the class of probabilistic models Markov\nthat assume we can predict the probability of some future unit without looking too\nfar into the past. We can generalize the bigram (which looks one word into the past)\nto the trigram (which looks two words into the past) and thus to the n-gram (which n-gram\nlooks n\u00001 words into the past).\nLet\u2019s see a general equation for this n-gram approximation to the conditional\nprobability of the next word in a sequence. We\u2019ll use Nhere to mean the n-gram",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 69,
      "token_count": 637,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 43\n\n3.1 \u2022 N-G RAMS 35\nsize, so N=2 means bigrams and N=3 means trigrams. Then we approximate the\nprobability of a word given its entire context as follows:\nP(wnjw1:n\u00001)\u0019P(wnjwn\u0000N+1:n\u00001) (3.8)\nGiven the bigram assumption for the probability of an individual word, we can com-\npute the probability of a complete word sequence by substituting Eq. 3.7 into Eq. 3.4:\nP(w1:n)\u0019nY\nk=1P(wkjwk\u00001) (3.9)\n3.1.2 How to estimate probabilities\nHow do we estimate these bigram or n-gram probabilities? An intuitive way to\nestimate probabilities is called maximum likelihood estimation orMLE . We getmaximum\nlikelihood\nestimationthe MLE estimate for the parameters of an n-gram model by getting counts from\na corpus, and normalizing the counts so that they lie between 0 and 1. For proba- normalize\nbilistic models, normalizing means dividing by some total count so that the resulting\nprobabilities fall between 0 and 1 and sum to 1.\nFor example, to compute a particular bigram probability of a word wngiven a\nprevious word wn\u00001, we\u2019ll compute the count of the bigram C(wn\u00001wn)and normal-\nize by the sum of all the bigrams that share the same \ufb01rst word wn\u00001:\nP(wnjwn\u00001) =C(wn\u00001wn)P\nwC(wn\u00001w)(3.10)\nWe can simplify this equation, since the sum of all bigram counts that start with\na given word wn\u00001must be equal to the unigram count for that word wn\u00001(the reader\nshould take a moment to be convinced of this):\nP(wnjwn\u00001) =C(wn\u00001wn)\nC(wn\u00001)(3.11)\nLet\u2019s work through an example using a mini-corpus of three sentences. We\u2019ll\n\ufb01rst need to augment each sentence with a special symbol <s>at the beginning\nof the sentence, to give us the bigram context of the \ufb01rst word. We\u2019ll also need a\nspecial end-symbol </s> .1\n<s> I am Sam </s>\n<s> Sam I am </s>\n<s> I do not like green eggs and ham </s>\nHere are the calculations for some of the bigram probabilities from this corpus\nP(I|<s> ) =2\n3=0:67 P(Sam|<s> ) =1\n3=0:33 P(am|I) =2\n3=0:67\nP(</s>|Sam ) =1\n2=0:5 P(Sam|am ) =1\n2=0:5 P(do|I) =1\n3=0:33\nFor the general case of MLE n-gram parameter estimation:\nP(wnjwn\u0000N+1:n\u00001) =C(wn\u0000N+1:n\u00001wn)\nC(wn\u0000N+1:n\u00001)(3.12)\n1We need the end-symbol to make the bigram grammar a true probability distribution. Without an end-\nsymbol, instead of the sentence probabilities of all sentences summing to one, the sentence probabilities\nfor all sentences of a given length would sum to one. This model would de\ufb01ne an in\ufb01nite set of probability\ndistributions, with one distribution per sentence length. See Exercise 3.5.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 70,
      "token_count": 777,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 44",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 71,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "36 CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\nEquation 3.12 (like Eq. 3.11) estimates the n-gram probability by dividing the\nobserved frequency of a particular sequence by the observed frequency of a pre\ufb01x.\nThis ratio is called a relative frequency . We said above that this use of relativerelative\nfrequency\nfrequencies as a way to estimate probabilities is an example of maximum likelihood\nestimation or MLE. In MLE, the resulting parameter set maximizes the likelihood of\nthe training set Tgiven the model M(i.e., P(TjM)). For example, suppose the word\nChinese occurs 400 times in a corpus of a million words. What is the probability\nthat a random word selected from some other text of, say, a million words will be the\nword Chinese ? The MLE of its probability is400\n1000000or 0:0004. Now 0 :0004 is not\nthe best possible estimate of the probability of Chinese occurring in all situations; it\nmight turn out that in some other corpus or context Chinese is a very unlikely word.\nBut it is the probability that makes it most likely that Chinese will occur 400 times\nin a million-word corpus. We present ways to modify the MLE estimates slightly to\nget better probability estimates in Section 3.6.\nLet\u2019s move on to some examples from a real but tiny corpus, drawn from the\nnow-defunct Berkeley Restaurant Project, a dialogue system from the last century\nthat answered questions about a database of restaurants in Berkeley, California (Ju-\nrafsky et al., 1994). Here are some sample user queries (text-normalized, by lower\ncasing and with punctuation striped) (a sample of 9332 sentences is on the website):\ncan you tell me about any good cantonese restaurants close by\ntell me about chez panisse\ni\u2019m looking for a good place to eat breakfast\nwhen is caffe venezia open during the day\nFigure 3.1 shows the bigram counts from part of a bigram grammar from text-\nnormalized Berkeley Restaurant Project sentences. Note that the majority of the\nvalues are zero. In fact, we have chosen the sample words to cohere with each other;\na matrix selected from a random set of eight words would be even more sparse.\ni want to eat chinese food lunch spend\ni 5 827 0 9 0 0 0 2\nwant 2 0 608 1 6 6 5 1\nto 2 0 4 686 2 0 6 211\neat 0 0 2 0 16 2 42 0\nchinese 1 0 0 0 0 82 1 0\nfood 15 0 15 0 1 4 0 0\nlunch 2 0 0 0 0 1 0 0\nspend 1 0 1 0 0 0 0 0\nFigure 3.1 Bigram counts for eight of the words (out of V=1446) in the Berkeley Restau-\nrant Project corpus of 9332 sentences. Zero counts are in gray. Each cell shows the count of\nthe column label word following the row label word. Thus the cell in row iand column want\nmeans that want followed i827 times in the corpus.\nFigure 3.2 shows the bigram probabilities after normalization (dividing each cell\nin Fig. 3.1 by the appropriate unigram for its row, taken from the following set of\nunigram counts):\ni want to eat chinese food lunch spend\n2533 927 2417 746 158 1093 341 278",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 72,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 45",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 73,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "3.1 \u2022 N-G RAMS 37\ni want to eat chinese food lunch spend\ni 0.002 0.33 0 0.0036 0 0 0 0.00079\nwant 0.0022 0 0.66 0.0011 0.0065 0.0065 0.0054 0.0011\nto 0.00083 0 0.0017 0.28 0.00083 0 0.0025 0.087\neat 0 0 0.0027 0 0.021 0.0027 0.056 0\nchinese 0.0063 0 0 0 0 0.52 0.0063 0\nfood 0.014 0 0.014 0 0.00092 0.0037 0 0\nlunch 0.0059 0 0 0 0 0.0029 0 0\nspend 0.0036 0 0.0036 0 0 0 0 0\nFigure 3.2 Bigram probabilities for eight words in the Berkeley Restaurant Project corpus\nof 9332 sentences. Zero probabilities are in gray.\nHere are a few other useful probabilities:\nP(i|<s> ) =0:25 P(english|want ) =0:0011\nP(food|english ) =0:5 P(</s>|food ) =0:68\nNow we can compute the probability of sentences like I want English food or\nI want Chinese food by simply multiplying the appropriate bigram probabilities to-\ngether, as follows:\nP(<s> i want english food </s> )\n=P(i|<s> )P(want|i )P(english|want )\nP(food|english )P(</s>|food )\n=0:25\u00020:33\u00020:0011\u00020:5\u00020:68\n=0:000031\nWe leave it as Exercise 3.2 to compute the probability of i want chinese food .\nWhat kinds of linguistic phenomena are captured in these bigram statistics?\nSome of the bigram probabilities above encode some facts that we think of as strictly\nsyntactic in nature, like the fact that what comes after eatis usually a noun or an\nadjective, or that what comes after tois usually a verb. Others might be a fact about\nthe personal assistant task, like the high probability of sentences beginning with\nthe words I. And some might even be cultural rather than linguistic, like the higher\nprobability that people are looking for Chinese versus English food.\n3.1.3 Dealing with scale in large n-gram models\nIn practice, language models can be very large, leading to practical issues.\nLog probabilities Language model probabilities are always stored and computed\nin log space as log probabilities . This is because probabilities are (by de\ufb01nition) lesslog\nprobabilities\nthan or equal to 1, and so the more probabilities we multiply together, the smaller the\nproduct becomes. Multiplying enough n-grams together would result in numerical\nunder\ufb02ow. Adding in log space is equivalent to multiplying in linear space, so we\ncombine log probabilities by adding them. By adding log probabilities instead of\nmultiplying probabilities, we get results that are not as small. We do all computation\nand storage in log space, and just convert back into probabilities if we need to report\nprobabilities at the end by taking the exp of the logprob:",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 74,
      "token_count": 761,
      "chapter_title": ""
    }
  },
  {
    "content": "Some of the bigram probabilities above encode some facts that we think of as strictly\nsyntactic in nature, like the fact that what comes after eatis usually a noun or an\nadjective, or that what comes after tois usually a verb. Others might be a fact about\nthe personal assistant task, like the high probability of sentences beginning with\nthe words I. And some might even be cultural rather than linguistic, like the higher\nprobability that people are looking for Chinese versus English food.\n3.1.3 Dealing with scale in large n-gram models\nIn practice, language models can be very large, leading to practical issues.\nLog probabilities Language model probabilities are always stored and computed\nin log space as log probabilities . This is because probabilities are (by de\ufb01nition) lesslog\nprobabilities\nthan or equal to 1, and so the more probabilities we multiply together, the smaller the\nproduct becomes. Multiplying enough n-grams together would result in numerical\nunder\ufb02ow. Adding in log space is equivalent to multiplying in linear space, so we\ncombine log probabilities by adding them. By adding log probabilities instead of\nmultiplying probabilities, we get results that are not as small. We do all computation\nand storage in log space, and just convert back into probabilities if we need to report\nprobabilities at the end by taking the exp of the logprob:\np1\u0002p2\u0002p3\u0002p4=exp(logp1+logp2+logp3+logp4) (3.13)\nIn practice throughout this book, we\u2019ll use log to mean natural log (ln) when the\nbase is not speci\ufb01ed.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 75,
      "token_count": 353,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 46",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 76,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "38 CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\nLonger context Although for pedagogical purposes we have only described bi-\ngram models, when there is suf\ufb01cient training data we use trigram models, which trigram\ncondition on the previous two words, or 4-gram or5-gram models. For these larger 4-gram\n5-gram n-grams, we\u2019ll need to assume extra contexts to the left and right of the sentence end.\nFor example, to compute trigram probabilities at the very beginning of the sentence,\nwe use two pseudo-words for the \ufb01rst trigram (i.e., P(I|<s><s> ).\nSome large n-gram datasets have been created, like the million most frequent\nn-grams drawn from the Corpus of Contemporary American English (COCA), a\ncurated 1 billion word corpus of American English (Davies, 2020), Google\u2019s Web\n5-gram corpus from 1 trillion words of English web text (Franz and Brants, 2006),\nor the Google Books Ngrams corpora (800 billion tokens from Chinese, English,\nFrench, German, Hebrew, Italian, Russian, and Spanish) (Lin et al., 2012a)).\nIt\u2019s even possible to use extremely long-range n-gram context. The in\ufb01ni-gram\n(\u00a5-gram) project (Liu et al., 2024) allows n-grams of any length. Their idea is to\navoid the expensive (in space and time) pre-computation of huge n-gram count ta-\nbles. Instead, n-gram probabilities with arbitrary n are computed quickly at inference\ntime by using an ef\ufb01cient representation called suf\ufb01x arrays. This allows computing\nof n-grams of every length for enormous corpora of 5 trillion tokens.\nEf\ufb01ciency considerations are important when building large n-gram language\nmodels. It is standard to quantize the probabilities using only 4-8 bits (instead of\n8-byte \ufb02oats), store the word strings on disk and represent them in memory only as\na 64-bit hash, and represent n-grams in special data structures like \u2018reverse tries\u2019.\nIt is also common to prune n-gram language models, for example by only keeping\nn-grams with counts greater than some threshold or using entropy to prune less-\nimportant n-grams (Stolcke, 1998). Ef\ufb01cient language model toolkits like KenLM\n(Hea\ufb01eld 2011, Hea\ufb01eld et al. 2013) use sorted arrays and use merge sorts to ef\ufb01-\nciently build the probability tables in a minimal number of passes through a large\ncorpus.\n3.2 Evaluating Language Models: Training and Test Sets\nThe best way to evaluate the performance of a language model is to embed it in\nan application and measure how much the application improves. Such end-to-end\nevaluation is called extrinsic evaluation . Extrinsic evaluation is the only way toextrinsic\nevaluation\nknow if a particular improvement in the language model (or any component) is really\ngoing to help the task at hand. Thus for evaluating n-gram language models that are\na component of some task like speech recognition or machine translation, we can\ncompare the performance of two candidate language models by running the speech\nrecognizer or machine translator twice, once with each language model, and seeing\nwhich gives the more accurate transcription.\nUnfortunately, running big NLP systems end-to-end is often very expensive. In-\nstead, it\u2019s helpful to have a metric that can be used to quickly evaluate potential",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 77,
      "token_count": 765,
      "chapter_title": ""
    }
  },
  {
    "content": "n-grams with counts greater than some threshold or using entropy to prune less-\nimportant n-grams (Stolcke, 1998). Ef\ufb01cient language model toolkits like KenLM\n(Hea\ufb01eld 2011, Hea\ufb01eld et al. 2013) use sorted arrays and use merge sorts to ef\ufb01-\nciently build the probability tables in a minimal number of passes through a large\ncorpus.\n3.2 Evaluating Language Models: Training and Test Sets\nThe best way to evaluate the performance of a language model is to embed it in\nan application and measure how much the application improves. Such end-to-end\nevaluation is called extrinsic evaluation . Extrinsic evaluation is the only way toextrinsic\nevaluation\nknow if a particular improvement in the language model (or any component) is really\ngoing to help the task at hand. Thus for evaluating n-gram language models that are\na component of some task like speech recognition or machine translation, we can\ncompare the performance of two candidate language models by running the speech\nrecognizer or machine translator twice, once with each language model, and seeing\nwhich gives the more accurate transcription.\nUnfortunately, running big NLP systems end-to-end is often very expensive. In-\nstead, it\u2019s helpful to have a metric that can be used to quickly evaluate potential\nimprovements in a language model. An intrinsic evaluation metric is one that mea-intrinsic\nevaluation\nsures the quality of a model independent of any application. In the next section we\u2019ll\nintroduce perplexity , which is the standard intrinsic metric for measuring language\nmodel performance, both for simple n-gram language models and for the more so-\nphisticated neural large language models of Chapter 9.\nIn order to evaluate any machine learning model, we need to have at least three\ndistinct data sets: the training set , the development set , and the test set . training set\ndevelopment\nset\ntest set",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 78,
      "token_count": 409,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 47",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 79,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "3.2 \u2022 E VALUATING LANGUAGE MODELS : TRAINING AND TESTSETS 39\nThe training set is the data we use to learn the parameters of our model; for\nsimple n-gram language models it\u2019s the corpus from which we get the counts that\nwe normalize into the probabilities of the n-gram language model.\nThetest set is a different, held-out set of data, not overlapping with the training\nset, that we use to evaluate the model. We need a separate test set to give us an\nunbiased estimate of how well the model we trained can generalize when we apply\nit to some new unknown dataset. A machine learning model that perfectly captured\nthe training data, but performed terribly on any other data, wouldn\u2019t be much use\nwhen it comes time to apply it to any new data or problem! We thus measure the\nquality of an n-gram model by its performance on this unseen test set or test corpus.\nHow should we choose a training and test set? The test set should re\ufb02ect the\nlanguage we want to use the model for. If we\u2019re going to use our language model\nfor speech recognition of chemistry lectures, the test set should be text of chemistry\nlectures. If we\u2019re going to use it as part of a system for translating hotel booking re-\nquests from Chinese to English, the test set should be text of hotel booking requests.\nIf we want our language model to be general purpose, then the test set should be\ndrawn from a wide variety of texts. In such cases we might collect a lot of texts\nfrom different sources, and then divide it up into a training set and a test set. It\u2019s\nimportant to do the dividing carefully; if we\u2019re building a general purpose model,\nwe don\u2019t want the test set to consist of only text from one document, or one author,\nsince that wouldn\u2019t be a good measure of general performance.\nThus if we are given a corpus of text and want to compare the performance of\ntwo different n-gram models, we divide the data into training and test sets, and train\nthe parameters of both models on the training set. We can then compare how well\nthe two trained models \ufb01t the test set.\nBut what does it mean to \u201c\ufb01t the test set\u201d? The standard answer is simple:\nwhichever language model assigns a higher probability to the test set\u2014which\nmeans it more accurately predicts the test set\u2014is a better model. Given two proba-\nbilistic models, the better model is the one that better predicts the details of the test\ndata, and hence will assign a higher probability to the test data.\nSince our evaluation metric is based on test set probability, it\u2019s important not to\nlet the test sentences into the training set. Suppose we are trying to compute the\nprobability of a particular \u201ctest\u201d sentence. If our test sentence is part of the training\ncorpus, we will mistakenly assign it an arti\ufb01cially high probability when it occurs\nin the test set. We call this situation training on the test set . Training on the test\nset introduces a bias that makes the probabilities all look too high, and causes huge\ninaccuracies in perplexity , the probability-based metric we introduce below.\nEven if we don\u2019t train on the test set, if we test our language model on the\ntest set many times after making different changes, we might implicitly tune to its\ncharacteristics, by noticing which changes seem to make the model better. For this\nreason, we only want to run our model on the test set once, or a very few number of\ntimes, once we are sure our model is ready.\nFor this reason we normally instead have a third dataset called a developmentdevelopment\ntest",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 80,
      "token_count": 775,
      "chapter_title": ""
    }
  },
  {
    "content": "whichever language model assigns a higher probability to the test set\u2014which\nmeans it more accurately predicts the test set\u2014is a better model. Given two proba-\nbilistic models, the better model is the one that better predicts the details of the test\ndata, and hence will assign a higher probability to the test data.\nSince our evaluation metric is based on test set probability, it\u2019s important not to\nlet the test sentences into the training set. Suppose we are trying to compute the\nprobability of a particular \u201ctest\u201d sentence. If our test sentence is part of the training\ncorpus, we will mistakenly assign it an arti\ufb01cially high probability when it occurs\nin the test set. We call this situation training on the test set . Training on the test\nset introduces a bias that makes the probabilities all look too high, and causes huge\ninaccuracies in perplexity , the probability-based metric we introduce below.\nEven if we don\u2019t train on the test set, if we test our language model on the\ntest set many times after making different changes, we might implicitly tune to its\ncharacteristics, by noticing which changes seem to make the model better. For this\nreason, we only want to run our model on the test set once, or a very few number of\ntimes, once we are sure our model is ready.\nFor this reason we normally instead have a third dataset called a developmentdevelopment\ntest\ntest set or, devset . We do all our testing on this dataset until the very end, and then\nwe test on the test set once to see how good our model is.\nHow do we divide our data into training, development, and test sets? We want\nour test set to be as large as possible, since a small test set may be accidentally un-\nrepresentative, but we also want as much training data as possible. At the minimum,\nwe would want to pick the smallest test set that gives us enough statistical power\nto measure a statistically signi\ufb01cant difference between two potential models. It\u2019s",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 81,
      "token_count": 421,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 48\n\n40 CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\nimportant that the devset be drawn from the same kind of text as the test set, since\nits goal is to measure how we would do on the test set.\n3.3 Evaluating Language Models: Perplexity\nWe said above that we evaluate language models based on which one assigns a\nhigher probability to the test set. A better model is better at predicting upcoming\nwords, and so it will be less surprised by (i.e., assign a higher probability to) each\nword when it occurs in the test set. Indeed, a perfect language model would correctly\nguess each next word in a corpus, assigning it a probability of 1, and all the other\nwords a probability of zero. So given a test corpus, a better language model will\nassign a higher probability to it than a worse language model.\nBut in fact, we do not use raw probability as our metric for evaluating language\nmodels. The reason is that the probability of a test set (or any sequence) depends\non the number of words or tokens in it; the probability of a test set gets smaller the\nlonger the text. We\u2019d prefer a metric that is per-word, normalized by length, so we\ncould compare across texts of different lengths. The metric we use is, a function of\nprobability called perplexity , is one of the most important metrics in NLP, used for\nevaluating large language models as well as n-gram models.\nTheperplexity (sometimes abbreviated as PP or PPL) of a language model on a perplexity\ntest set is the inverse probability of the test set (one over the probability of the test\nset), normalized by the number of words (or tokens). For this reason it\u2019s sometimes\ncalled the per-word or per-token perplexity. We normalize by the number of words\nNby taking the Nth root. For a test set W=w1w2:::wN,:\nperplexity (W) = P(w1w2:::wN)\u00001\nN (3.14)\n=Ns\n1\nP(w1w2:::wN)\nOr we can use the chain rule to expand the probability of W:\nperplexity (W) =NvuutNY\ni=11\nP(wijw1:::wi\u00001)(3.15)\nNote that because of the inverse in Eq. 3.15, the higher the probability of the word\nsequence, the lower the perplexity. Thus the the lower the perplexity of a model on\nthe data, the better the model . Minimizing perplexity is equivalent to maximizing\nthe test set probability according to the language model. Why does perplexity use\nthe inverse probability? It turns out the inverse arises from the original de\ufb01nition\nof perplexity from cross-entropy rate in information theory; for those interested, the\nexplanation is in the advanced Section 3.7. Meanwhile, we just have to remember\nthat perplexity has an inverse relationship with probability.\nThe details of computing the perplexity of a test set Wdepends on which lan-\nguage model we use. Here\u2019s the perplexity of Wwith a unigram language model\n(just the geometric mean of the inverse of the unigram probabilities):\nperplexity (W) =NvuutNY\ni=11\nP(wi)(3.16)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 82,
      "token_count": 719,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 49",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 83,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "3.3 \u2022 E VALUATING LANGUAGE MODELS : PERPLEXITY 41\nThe perplexity of Wcomputed with a bigram language model is still a geometric\nmean, but now of the inverse of the bigram probabilities:\nperplexity (W) =NvuutNY\ni=11\nP(wijwi\u00001)(3.17)\nWhat we generally use for word sequence in Eq. 3.15 or Eq. 3.17 is the entire\nsequence of words in some test set. Since this sequence will cross many sentence\nboundaries, if our vocabulary includes a between-sentence token <EOS> or separate\nbegin- and end-sentence markers <s> and</s> then we can include them in the\nprobability computation. If we do, then we also include one token per sentence in\nthe total count of word tokens N.2\nWe mentioned above that perplexity is a function of both the text and the lan-\nguage model: given a text W, different language models will have different perplex-\nities. Because of this, perplexity can be used to compare different language models.\nFor example, here we trained unigram, bigram, and trigram grammars on 38 million\nwords from the Wall Street Journal newspaper. We then computed the perplexity of\neach of these models on a WSJ test set using Eq. 3.16 for unigrams, Eq. 3.17 for\nbigrams, and the corresponding equation for trigrams. The table below shows the\nperplexity of the 1.5 million word test set according to each of the language models.\nUnigram Bigram Trigram\nPerplexity 962 170 109\nAs we see above, the more information the n-gram gives us about the word\nsequence, the higher the probability the n-gram will assign to the string. A trigram\nmodel is less surprised than a unigram model because it has a better idea of what\nwords might come next, and so it assigns them a higher probability. And the higher\nthe probability, the lower the perplexity (since as Eq. 3.15 showed, perplexity is\nrelated inversely to the probability of the test sequence according to the model). So\na lower perplexity tells us that a language model is a better predictor of the test set.\nNote that in computing perplexities, the language model must be constructed\nwithout any knowledge of the test set, or else the perplexity will be arti\ufb01cially low.\nAnd the perplexity of two language models is only comparable if they use identical\nvocabularies.\nAn (intrinsic) improvement in perplexity does not guarantee an (extrinsic) im-\nprovement in the performance of a language processing task like speech recognition\nor machine translation. Nonetheless, because perplexity usually correlates with task\nimprovements, it is commonly used as a convenient evaluation metric. Still, when\npossible a model\u2019s improvement in perplexity should be con\ufb01rmed by an end-to-end\nevaluation on a real task.\n3.3.1 Perplexity as Weighted Average Branching Factor\nIt turns out that perplexity can also be thought of as the weighted average branch-\ning factor of a language. The branching factor of a language is the number of\npossible next words that can follow any word. For example consider a mini arti\ufb01cial\n2For example if we use both begin and end tokens, we would include the end-of-sentence marker </s>\nbut not the beginning-of-sentence marker <s>in our count of N; This is because the end-sentence token is\nfollowed directly by the begin-sentence token with probability almost 1, so we don\u2019t want the probability\nof that fake transition to in\ufb02uence our perplexity.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 84,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 50\n\n42 CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\nlanguage that is deterministic (no probabilities), any word can follow any word, and\nwhose vocabulary consists of only three colors:\nL=fred;blue;greeng (3.18)\nThe branching factor of this language is 3.\nNow let\u2019s make a probabilistic version of the same LM, let\u2019s call it A, where each\nword follows each other with equal probability1\n3(it was trained on a training set with\nequal counts for the 3 colors), and a test set T= \u201cred red red red blue \u201d.\nLet\u2019s \ufb01rst convince ourselves that if we compute the perplexity of this arti\ufb01cial\ndigit language on this test set (or any such test set) we indeed get 3. By Eq. 3.15, the\nperplexity of AonTis:\nperplexityA(T) = PA(red red red red blue )\u00001\n5\n= \u00121\n3\u00135!\u00001\n5\n=\u00121\n3\u0013\u00001\n=3 (3.19)\nBut now suppose redwas very likely in the training set a different LM B, and so B\nhas the following probabilities:\nP(red) =0:8P(green ) =0:1P(blue) =0:1 (3.20)\nWe should expect the perplexity of the same test set red red red red blue for\nlanguage model Bto be lower since most of the time the next color will be red, which\nis very predictable, i.e. has a high probability. So the probability of the test set will\nbe higher, and since perplexity is inversely related to probability, the perplexity will\nbe lower. Thus, although the branching factor is still 3, the perplexity or weighted\nbranching factor is smaller:\nperplexityB(T) = PB(red red red red blue )\u00001=5\n=0:04096\u00001\n5\n=0:527\u00001=1:89 (3.21)\n3.4 Sampling sentences from a language model\nOne important way to visualize what kind of knowledge a language model embodies\nis to sample from it. Sampling from a distribution means to choose random points sampling\naccording to their likelihood. Thus sampling from a language model\u2014which rep-\nresents a distribution over sentences\u2014means to generate some sentences, choosing\neach sentence according to its likelihood as de\ufb01ned by the model. Thus we are more\nlikely to generate sentences that the model thinks have a high probability and less\nlikely to generate sentences that the model thinks have a low probability.\nThis technique of visualizing a language model by sampling was \ufb01rst suggested\nvery early on by Shannon (1948) and Miller and Selfridge (1950). It\u2019s simplest to\nvisualize how this works for the unigram case. Imagine all the words of the English\nlanguage covering the number line between 0 and 1, each word covering an interval",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 85,
      "token_count": 631,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 51\n\n3.5 \u2022 G ENERALIZING VS .OVERFITTING THE TRAINING SET 43\n010.06the.060.03of0.02a0.02toin.09.11.13.15\u2026however(p=0.0003)polyphonicp=0.0000018\u20260.02.66.99\u2026\nFigure 3.3 A visualization of the sampling distribution for sampling sentences by repeat-\nedly sampling unigrams. The blue bar represents the relative frequency of each word (we\u2019ve\nordered them from most frequent to least frequent, but the choice of order is arbitrary). The\nnumber line shows the cumulative probabilities. If we choose a random number between 0\nand 1, it will fall in an interval corresponding to some word. The expectation for the random\nnumber to fall in the larger intervals of one of the frequent words ( the,of,a) is much higher\nthan in the smaller interval of one of the rare words ( polyphonic ).\nproportional to its frequency. Fig. 3.3 shows a visualization, using a unigram LM\ncomputed from the text of this book. We choose a random value between 0 and 1,\n\ufb01nd that point on the probability line, and print the word whose interval includes this\nchosen value. We continue choosing random numbers and generating words until\nwe randomly generate the sentence-\ufb01nal token </s> .\nWe can use the same technique to generate bigrams by \ufb01rst generating a ran-\ndom bigram that starts with <s>(according to its bigram probability). Let\u2019s say the\nsecond word of that bigram is w. We next choose a random bigram starting with w\n(again, drawn according to its bigram probability), and so on.\n3.5 Generalizing vs. over\ufb01tting the training set\nThe n-gram model, like many statistical models, is dependent on the training corpus.\nOne implication of this is that the probabilities often encode speci\ufb01c facts about a\ngiven training corpus. Another implication is that n-grams do a better and better job\nof modeling the training corpus as we increase the value of N.\nWe can use the sampling method from the prior section to visualize both of\nthese facts! To give an intuition for the increasing power of higher-order n-grams,\nFig. 3.4 shows random sentences generated from unigram, bigram, trigram, and 4-\ngram models trained on Shakespeare\u2019s works.\nThe longer the context, the more coherent the sentences. The unigram sen-\ntences show no coherent relation between words nor any sentence-\ufb01nal punctua-\ntion. The bigram sentences have some local word-to-word coherence (especially\nconsidering punctuation as words). The trigram sentences are beginning to look a\nlot like Shakespeare. Indeed, the 4-gram sentences look a little too much like Shake-\nspeare. The words It cannot be but so are directly from King John . This is because,\nnot to put the knock on Shakespeare, his oeuvre is not very large as corpora go\n(N=884;647;V=29;066), and our n-gram probability matrices are ridiculously\nsparse. There are V2=844;000;000 possible bigrams alone, and the number of\npossible 4-grams is V4=7\u00021017. Thus, once the generator has chosen the \ufb01rst\n3-gram ( It cannot be ), there are only seven possible next words for the 4th element\n(but,I,that,thus,this, and the period).\nTo get an idea of the dependence on the training set, let\u2019s look at LMs trained on a\ncompletely different corpus: the Wall Street Journal (WSJ) newspaper. Shakespeare",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 86,
      "token_count": 793,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 52\n\n44 CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\n1\u2013To him swallowed confess hear both. Which. Of save on trail for are ay device and\nrote life have\ngram \u2013Hill he late speaks; or! a more to leg less \ufb01rst you enter\n2\u2013Why dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live\nking. Follow.\ngram \u2013What means, sir. I confess she? then all sorts, he is trim, captain.\n3\u2013Fly, and will rid me these news of price. Therefore the sadness of parting, as they say,\n\u2019tis done.\ngram \u2013This shall forbid it should be branded, if renown made it empty.\n4\u2013King Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. A\ngreat banquet serv\u2019d in;\ngram \u2013It cannot be but so.\nFigure 3.4 Eight sentences randomly generated from four n-grams computed from Shakespeare\u2019s works. All\ncharacters were mapped to lower-case and punctuation marks were treated as words. Output is hand-corrected\nfor capitalization to improve readability.\nand the WSJ are both English, so we might have expected some overlap between our\nn-grams for the two genres. Fig. 3.5 shows sentences generated by unigram, bigram,\nand trigram grammars trained on 40 million words from WSJ.\n1Months the my and issue of year foreign new exchange\u2019s september\nwere recession exchange new endorsed a acquire to six executivesgram\n2Last December through the way to preserve the Hudson corporation N.\nB. E. C. Taylor would seem to complete the major central planners one\ngram point \ufb01ve percent of U. S. E. has already old M. X. corporation of living\non information such as more frequently \ufb01shing to keep her\n3They also point to ninety nine point six billion dollars from two hundred\nfour oh six three percent of the rates of interest stores as Mexico and\ngram Brazil on market conditions\nFigure 3.5 Three sentences randomly generated from three n-gram models computed from\n40 million words of the Wall Street Journal , lower-casing all characters and treating punctua-\ntion as words. Output was then hand-corrected for capitalization to improve readability.\nCompare these examples to the pseudo-Shakespeare in Fig. 3.4. While they both\nmodel \u201cEnglish-like sentences\u201d, there is no overlap in the generated sentences, and\nlittle overlap even in small phrases. Statistical models are pretty useless as predictors\nif the training sets and the test sets are as different as Shakespeare and the WSJ.\nHow should we deal with this problem when we build n-gram models? One step\nis to be sure to use a training corpus that has a similar genre to whatever task we are\ntrying to accomplish. To build a language model for translating legal documents,\nwe need a training corpus of legal documents. To build a language model for a\nquestion-answering system, we need a training corpus of questions.\nIt is equally important to get training data in the appropriate dialect orvariety ,\nespecially when processing social media posts or spoken transcripts. For exam-\nple some tweets will use features of African American English (AAE)\u2014 the name\nfor the many variations of language used in African American communities (King,\n2020). Such features can include words like \ufb01nna \u2014an auxiliary verb that marks\nimmediate future tense \u2014that don\u2019t occur in other varieties, or spellings like denfor\nthen, in tweets like this one (Blodgett and O\u2019Connor, 2017):",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 87,
      "token_count": 765,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 53\n\n3.6 \u2022 S MOOTHING , INTERPOLATION ,AND BACKOFF 45\n(3.22) Bored af den my phone \ufb01nna die!!!\nwhile tweets from English-based languages like Nigerian Pidgin have markedly dif-\nferent vocabulary and n-gram patterns from American English (Jurgens et al., 2017):\n(3.23) @username R u a wizard or wat gan sef: in d mornin - u tweet, afternoon - u\ntweet, nyt gan u dey tweet. beta get ur IT placement wiv twitter\nIs it possible for the testset nonetheless to have a word we have never seen be-\nfore? What happens if the word Jurafsky never occurs in our training set, but pops up\nin the test set? The answer is that although words might be unseen, we normally run\nour NLP algorithms not on words but on subword tokens . With subword tokeniza-\ntion (like the BPE algorithm of Chapter 2) any word can be modeled as a sequence\nof known smaller subwords, if necessary by a sequence of tokens corresponding to\nindividual letters. So although for convenience we\u2019ve been referring to words in\nthis chapter, the language model vocabulary is normally the set of tokens rather than\nwords, and in this way the test set can never contain unseen tokens.\n3.6 Smoothing, Interpolation, and Backoff\nThere is a problem with using maximum likelihood estimates for probabilities: any\n\ufb01nite training corpus will be missing some perfectly acceptable English word se-\nquences. That is, cases where a particular n-gram never occurs in the training data\nbut appears in the test set. Perhaps our training corpus has the words ruby and\nslippers in it but just happens not to have the phrase ruby slippers .\nThese unseen sequences or zeros \u2014sequences that don\u2019t occur in the training set zeros\nbut do occur in the test set\u2014are a problem for two reasons. First, their presence\nmeans we are underestimating the probability of word sequences that might occur,\nwhich hurts the performance of any application we want to run on this data. Second,\nif the probability of any word in the test set is 0, the probability of the whole test\nset is 0. Perplexity is de\ufb01ned based on the inverse probability of the test set. Thus\nif some words in context have zero probability, we can\u2019t compute perplexity at all,\nsince we can\u2019t divide by 0!\nThe standard way to deal with putative \u201czero probability n-grams\u201d that should re-\nally have some non-zero probability is called smoothing ordiscounting . Smoothing smoothing\ndiscounting algorithms shave off a bit of probability mass from some more frequent events and\ngive it to unseen events. Here we\u2019ll introduce some simple smoothing algorithms:\nLaplace (add-one) smoothing ,stupid backoff , and n-gram interpolation .\n3.6.1 Laplace Smoothing\nThe simplest way to do smoothing is to add one to all the n-gram counts, before\nwe normalize them into probabilities. All the counts that used to be zero will now\nhave a count of 1, the counts of 1 will be 2, and so on. This algorithm is called\nLaplace smoothing . Laplace smoothing does not perform well enough to be usedLaplace\nsmoothing\nin modern n-gram models, but it usefully introduces many of the concepts that we\nsee in other smoothing algorithms, gives a useful baseline, and is also a practical\nsmoothing algorithm for other tasks like text classi\ufb01cation (Chapter 4).\nLet\u2019s start with the application of Laplace smoothing to unigram probabilities.\nRecall that the unsmoothed maximum likelihood estimate of the unigram probability",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 88,
      "token_count": 790,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 54",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 89,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "46 CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\nof the word wiis its count cinormalized by the total number of word tokens N:\nP(wi) =ci\nN\nLaplace smoothing merely adds one to each count (hence its alternate name add-\nonesmoothing). Since there are Vwords in the vocabulary and each one was in- add-one\ncremented, we also need to adjust the denominator to take into account the extra V\nobservations. (What happens to our Pvalues if we don\u2019t increase the denominator?)\nPLaplace (wi) =ci+1\nN+V(3.24)\nNow that we have the intuition for the unigram case, let\u2019s smooth our Berkeley\nRestaurant Project bigrams. Figure 3.6 shows the add-one smoothed counts for the\nbigrams in Fig. 3.1.\ni want to eat chinese food lunch spend\ni 6 828 1 10 1 1 1 3\nwant 3 1 609 2 7 7 6 2\nto 3 1 5 687 3 1 7 212\neat 1 1 3 1 17 3 43 1\nchinese 2 1 1 1 1 83 2 1\nfood 16 1 16 1 2 5 1 1\nlunch 3 1 1 1 1 2 1 1\nspend 2 1 2 1 1 1 1 1\nFigure 3.6 Add-one smoothed bigram counts for eight of the words (out of V=1446) in\nthe Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero counts are in gray.\nFigure 3.7 shows the add-one smoothed probabilities for the bigrams in Fig. 3.2,\ncomputed by Eq. 3.26 below. Recall that normal bigram probabilities are computed\nby normalizing each row of counts by the unigram count:\nPMLE(wnjwn\u00001) =C(wn\u00001wn)\nC(wn\u00001)(3.25)\nFor add-one smoothed bigram counts, we need to augment the unigram count in the\ndenominator by the number of total word types in the vocabulary V. We can see\nwhy this is in the following equation, which makes it explicit that the unigram count\nin the denominator is really the sum over all the bigrams that start with wn\u00001. Since\nwe add one to each of these, and there are Vof them, we add a total of Vto the\ndenominator:\nPLaplace (wnjwn\u00001) =C(wn\u00001wn)+1P\nw(C(wn\u00001w)+1)=C(wn\u00001wn)+1\nC(wn\u00001)+V(3.26)\nThus, each of the unigram counts given on page 36 will need to be augmented by V=\n1446. The result, using Eq. 3.26, is the smoothed bigram probabilities in Fig. 3.7.\nOne useful visualization technique is to reconstruct an adjusted count matrix\nso we can see how much a smoothing algorithm has changed the original counts.\nThis adjusted count C\u0003is the count that, if divided by C(wn\u00001), would result in\nthe smoothed probability. This adjusted count is easier to compare directly with\nthe MLE counts. That is, the Laplace probability can equally be expressed as the\nadjusted count divided by the (non-smoothed) denominator from Eq. 3.25:",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 90,
      "token_count": 770,
      "chapter_title": ""
    }
  },
  {
    "content": "denominator by the number of total word types in the vocabulary V. We can see\nwhy this is in the following equation, which makes it explicit that the unigram count\nin the denominator is really the sum over all the bigrams that start with wn\u00001. Since\nwe add one to each of these, and there are Vof them, we add a total of Vto the\ndenominator:\nPLaplace (wnjwn\u00001) =C(wn\u00001wn)+1P\nw(C(wn\u00001w)+1)=C(wn\u00001wn)+1\nC(wn\u00001)+V(3.26)\nThus, each of the unigram counts given on page 36 will need to be augmented by V=\n1446. The result, using Eq. 3.26, is the smoothed bigram probabilities in Fig. 3.7.\nOne useful visualization technique is to reconstruct an adjusted count matrix\nso we can see how much a smoothing algorithm has changed the original counts.\nThis adjusted count C\u0003is the count that, if divided by C(wn\u00001), would result in\nthe smoothed probability. This adjusted count is easier to compare directly with\nthe MLE counts. That is, the Laplace probability can equally be expressed as the\nadjusted count divided by the (non-smoothed) denominator from Eq. 3.25:\nPLaplace (wnjwn\u00001) =C(wn\u00001wn)+1\nC(wn\u00001)+V=C\u0003(wn\u00001wn)\nC(wn\u00001)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 91,
      "token_count": 329,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 55",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 92,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "3.6 \u2022 S MOOTHING , INTERPOLATION ,AND BACKOFF 47\ni want to eat chinese food lunch spend\ni 0.0015 0.21 0.00025 0.0025 0.00025 0.00025 0.00025 0.00075\nwant 0.0013 0.00042 0.26 0.00084 0.0029 0.0029 0.0025 0.00084\nto 0.00078 0.00026 0.0013 0.18 0.00078 0.00026 0.0018 0.055\neat 0.00046 0.00046 0.0014 0.00046 0.0078 0.0014 0.02 0.00046\nchinese 0.0012 0.00062 0.00062 0.00062 0.00062 0.052 0.0012 0.00062\nfood 0.0063 0.00039 0.0063 0.00039 0.00079 0.002 0.00039 0.00039\nlunch 0.0017 0.00056 0.00056 0.00056 0.00056 0.0011 0.00056 0.00056\nspend 0.0012 0.00058 0.0012 0.00058 0.00058 0.00058 0.00058 0.00058\nFigure 3.7 Add-one smoothed bigram probabilities for eight of the words (out of V=1446) in the BeRP\ncorpus of 9332 sentences computed by Eq. 3.26. Previously-zero probabilities are in gray.\nRearranging terms, we can solve for C\u0003(wn\u00001wn):\nC\u0003(wn\u00001wn) =[C(wn\u00001wn)+1]\u0002C(wn\u00001)\nC(wn\u00001)+V(3.27)\nFigure 3.8 shows the reconstructed counts, computed by Eq. 3.27.\ni want to eat chinese food lunch spend\ni 3.8 527 0.64 6.4 0.64 0.64 0.64 1.9\nwant 1.2 0.39 238 0.78 2.7 2.7 2.3 0.78\nto 1.9 0.63 3.1 430 1.9 0.63 4.4 133\neat 0.34 0.34 1 0.34 5.8 1 15 0.34\nchinese 0.2 0.098 0.098 0.098 0.098 8.2 0.2 0.098\nfood 6.9 0.43 6.9 0.43 0.86 2.2 0.43 0.43\nlunch 0.57 0.19 0.19 0.19 0.19 0.38 0.19 0.19\nspend 0.32 0.16 0.32 0.16 0.16 0.16 0.16 0.16\nFigure 3.8 Add-one reconstituted counts for eight words (of V=1446) in the BeRP corpus",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 93,
      "token_count": 783,
      "chapter_title": ""
    }
  },
  {
    "content": "i want to eat chinese food lunch spend\ni 3.8 527 0.64 6.4 0.64 0.64 0.64 1.9\nwant 1.2 0.39 238 0.78 2.7 2.7 2.3 0.78\nto 1.9 0.63 3.1 430 1.9 0.63 4.4 133\neat 0.34 0.34 1 0.34 5.8 1 15 0.34\nchinese 0.2 0.098 0.098 0.098 0.098 8.2 0.2 0.098\nfood 6.9 0.43 6.9 0.43 0.86 2.2 0.43 0.43\nlunch 0.57 0.19 0.19 0.19 0.19 0.38 0.19 0.19\nspend 0.32 0.16 0.32 0.16 0.16 0.16 0.16 0.16\nFigure 3.8 Add-one reconstituted counts for eight words (of V=1446) in the BeRP corpus\nof 9332 sentences, computed by Eq. 3.27. Previously-zero counts are in gray.\nNote that add-one smoothing has made a very big change to the counts. Com-\nparing Fig. 3.8 to the original counts in Fig. 3.1, we can see that C(want to )changed\nfrom 608 to 238! We can see this in probability space as well: P(tojwant)decreases\nfrom 0.66 in the unsmoothed case to 0.26 in the smoothed case. Looking at the dis-\ncount d, de\ufb01ned as the ratio between new and old counts, shows us how strikingly\nthe counts for each pre\ufb01x word have been reduced; the discount for the bigram want\ntois 0.39, while the discount for Chinese food is 0.10, a factor of 10! The sharp\nchange occurs because too much probability mass is moved to all the zeros.\n3.6.2 Add-k smoothing\nOne alternative to add-one smoothing is to move a bit less of the probability mass\nfrom the seen to the unseen events. Instead of adding 1 to each count, we add a\nfractional count k(0.5? 0.01?). This algorithm is therefore called add-k smoothing . add-k\nP\u0003\nAdd-k(wnjwn\u00001) =C(wn\u00001wn)+k\nC(wn\u00001)+kV(3.28)\nAdd-k smoothing requires that we have a method for choosing k; this can be\ndone, for example, by optimizing on a devset . Although add-k is useful for some\ntasks (including text classi\ufb01cation), it turns out that it still doesn\u2019t work well for",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 94,
      "token_count": 670,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 56",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 95,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "48 CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\nlanguage modeling, generating counts with poor variances and often inappropriate\ndiscounts (Gale and Church, 1994).\n3.6.3 Language Model Interpolation\nThere is an alternative source of knowledge we can draw on to solve the problem\nof zero frequency n-grams. If we are trying to compute P(wnjwn\u00002wn\u00001)but we\nhave no examples of a particular trigram wn\u00002wn\u00001wn, we can instead estimate its\nprobability by using the bigram probability P(wnjwn\u00001). Similarly, if we don\u2019t have\ncounts to compute P(wnjwn\u00001), we can look to the unigram P(wn). In other words,\nsometimes using less context can help us generalize more for contexts that the model\nhasn\u2019t learned much about.\nThe most common way to use this n-gram hierarchy is called interpolation : interpolation\ncomputing a new probability by interpolating (weighting and combining) the tri-\ngram, bigram, and unigram probabilities.3In simple linear interpolation, we com-\nbine different order n-grams by linearly interpolating them. Thus, we estimate the\ntrigram probability P(wnjwn\u00002wn\u00001)by mixing together the unigram, bigram, and\ntrigram probabilities, each weighted by a l:\n\u02c6P(wnjwn\u00002wn\u00001) = l1P(wn)\n+l2P(wnjwn\u00001)\n+l3P(wnjwn\u00002wn\u00001) (3.29)\nThels must sum to 1, making Eq. 3.29 equivalent to a weighted average. In a\nslightly more sophisticated version of linear interpolation, each lweight is com-\nputed by conditioning on the context. This way, if we have particularly accurate\ncounts for a particular bigram, we assume that the counts of the trigrams based on\nthis bigram will be more trustworthy, so we can make the ls for those trigrams\nhigher and thus give that trigram more weight in the interpolation. Equation 3.30\nshows the equation for interpolation with context-conditioned weights, where each\nlambda takes an argument that is the two prior word context:\n\u02c6P(wnjwn\u00002wn\u00001) = l1(wn\u00002:n\u00001)P(wn)\n+l2(wn\u00002:n\u00001)P(wnjwn\u00001)\n+l3(wn\u00002:n\u00001)P(wnjwn\u00002wn\u00001) (3.30)\nHow are these lvalues set? Both the simple interpolation and conditional interpo-\nlation ls are learned from a held-out corpus. A held-out corpus is an additional held-out\ntraining corpus, so-called because we hold it out from the training data, that we use\nto set these lvalues.4We do so by choosing the lvalues that maximize the likeli-\nhood of the held-out corpus. That is, we \ufb01x the n-gram probabilities and then search\nfor the lvalues that\u2014when plugged into Eq. 3.29\u2014give us the highest probability\nof the held-out set. There are various ways to \ufb01nd this optimal set of ls. One way\nis to use the EMalgorithm, an iterative learning algorithm that converges on locally\noptimal ls (Jelinek and Mercer, 1980).\n3We won\u2019t discuss the less-common alternative, called backoff , in which we use the trigram if the\nevidence is suf\ufb01cient for it, but if not we instead just use the bigram, otherwise the unigram. That is, we",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 96,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "+l2(wn\u00002:n\u00001)P(wnjwn\u00001)\n+l3(wn\u00002:n\u00001)P(wnjwn\u00002wn\u00001) (3.30)\nHow are these lvalues set? Both the simple interpolation and conditional interpo-\nlation ls are learned from a held-out corpus. A held-out corpus is an additional held-out\ntraining corpus, so-called because we hold it out from the training data, that we use\nto set these lvalues.4We do so by choosing the lvalues that maximize the likeli-\nhood of the held-out corpus. That is, we \ufb01x the n-gram probabilities and then search\nfor the lvalues that\u2014when plugged into Eq. 3.29\u2014give us the highest probability\nof the held-out set. There are various ways to \ufb01nd this optimal set of ls. One way\nis to use the EMalgorithm, an iterative learning algorithm that converges on locally\noptimal ls (Jelinek and Mercer, 1980).\n3We won\u2019t discuss the less-common alternative, called backoff , in which we use the trigram if the\nevidence is suf\ufb01cient for it, but if not we instead just use the bigram, otherwise the unigram. That is, we\nonly \u201cback off\u201d to a lower-order n-gram if we have zero evidence for a higher-order n-gram.\n4Held-out corpora are generally used to set hyperparameters , which are special parameters, unlike\nregular counts that are learned from the training data; we\u2019ll discuss hyperparameters in Chapter 7.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 97,
      "token_count": 340,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 57",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 98,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "3.7 \u2022 A DVANCED : PERPLEXITY \u2019SRELATION TO ENTROPY 49\n3.6.4 Stupid Backoff\nAn alternative to interpolation is backoff . In a backoff model, if the n-gram we need backoff\nhas zero counts, we approximate it by backing off to the (n-1)-gram. We continue\nbacking off until we reach a history that has some counts. For a backoff model to\ngive a correct probability distribution, we have to discount the higher-order n-grams discount\nto save some probability mass for the lower order n-grams. In practice, instead of\ndiscounting, it\u2019s common to use a much simpler non-discounted backoff algorithm\ncalled stupid backoff (Brants et al., 2007). stupid backoff\nStupid backoff gives up the idea of trying to make the language model a true\nprobability distribution. There is no discounting of the higher-order probabilities. If\na higher-order n-gram has a zero count, we simply backoff to a lower order n-gram,\nweighed by a \ufb01xed (context-independent) weight. This algorithm does not produce\na probability distribution, so we\u2019ll follow Brants et al. (2007) in referring to it as S:\nS(wijwi\u0000N+1:i\u00001) =8\n<\n:count (wi\u0000N+1:i)\ncount (wi\u0000N+1:i\u00001)if count (wi\u0000N+1:i)>0\nlS(wijwi\u0000N+2:i\u00001)otherwise(3.31)\nThe backoff terminates in the unigram, which has score S(w) =count (w)\nN. Brants et al.\n(2007) \ufb01nd that a value of 0.4 worked well for l.\n3.7 Advanced: Perplexity\u2019s Relation to Entropy\nWe introduced perplexity in Section 3.3 as a way to evaluate n-gram models on\na test set. A better n-gram model is one that assigns a higher probability to the\ntest data, and perplexity is a normalized version of the probability of the test set.\nThe perplexity measure actually arises from the information-theoretic concept of\ncross-entropy, which explains otherwise mysterious properties of perplexity (why\nthe inverse probability, for example?) and its relationship to entropy. Entropy is a Entropy\nmeasure of information. Given a random variable Xranging over whatever we are\npredicting (words, letters, parts of speech), the set of which we\u2019ll call c, and with a\nparticular probability function, call it p(x), the entropy of the random variable Xis:\nH(X) =\u0000X\nx2cp(x)log2p(x) (3.32)\nThe log can, in principle, be computed in any base. If we use log base 2, the\nresulting value of entropy will be measured in bits.\nOne intuitive way to think about entropy is as a lower bound on the number of\nbits it would take to encode a certain decision or piece of information in the optimal\ncoding scheme. Consider an example from the standard information theory textbook\nCover and Thomas (1991). Imagine that we want to place a bet on a horse race but\nit is too far to go all the way to Yonkers Racetrack, so we\u2019d like to send a short\nmessage to the bookie to tell him which of the eight horses to bet on. One way to\nencode this message is just to use the binary representation of the horse\u2019s number\nas the code; thus, horse 1 would be 001, horse 2010, horse 3011, and so on, with",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 99,
      "token_count": 785,
      "chapter_title": ""
    }
  },
  {
    "content": "the inverse probability, for example?) and its relationship to entropy. Entropy is a Entropy\nmeasure of information. Given a random variable Xranging over whatever we are\npredicting (words, letters, parts of speech), the set of which we\u2019ll call c, and with a\nparticular probability function, call it p(x), the entropy of the random variable Xis:\nH(X) =\u0000X\nx2cp(x)log2p(x) (3.32)\nThe log can, in principle, be computed in any base. If we use log base 2, the\nresulting value of entropy will be measured in bits.\nOne intuitive way to think about entropy is as a lower bound on the number of\nbits it would take to encode a certain decision or piece of information in the optimal\ncoding scheme. Consider an example from the standard information theory textbook\nCover and Thomas (1991). Imagine that we want to place a bet on a horse race but\nit is too far to go all the way to Yonkers Racetrack, so we\u2019d like to send a short\nmessage to the bookie to tell him which of the eight horses to bet on. One way to\nencode this message is just to use the binary representation of the horse\u2019s number\nas the code; thus, horse 1 would be 001, horse 2010, horse 3011, and so on, with\nhorse 8 coded as 000. If we spend the whole day betting and each horse is coded\nwith 3 bits, on average we would be sending 3 bits per race.\nCan we do better? Suppose that the spread is the actual distribution of the bets\nplaced and that we represent it as the prior probability of each horse as follows:",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 100,
      "token_count": 367,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 58\n\n50 CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\nHorse 11\n2Horse 51\n64\nHorse 21\n4Horse 61\n64\nHorse 31\n8Horse 71\n64\nHorse 41\n16Horse 81\n64\nThe entropy of the random variable Xthat ranges over horses gives us a lower\nbound on the number of bits and is\nH(X) =\u0000i=8X\ni=1p(i)log2p(i)\n=\u00001\n2log21\n2\u00001\n4log21\n4\u00001\n8log21\n8\u00001\n16log21\n16\u00004(1\n64log21\n64)\n=2 bits (3.33)\nA code that averages 2 bits per race can be built with short encodings for more\nprobable horses, and longer encodings for less probable horses. For example, we\ncould encode the most likely horse with the code 0, and the remaining horses as 10,\nthen110,1110 ,111100 ,111101 ,111110 , and111111 .\nWhat if the horses are equally likely? We saw above that if we used an equal-\nlength binary code for the horse numbers, each horse took 3 bits to code, so the\naverage was 3. Is the entropy the same? In this case each horse would have a\nprobability of1\n8. The entropy of the choice of horses is then\nH(X) =\u0000i=8X\ni=11\n8log21\n8=\u0000log21\n8=3 bits (3.34)\nUntil now we have been computing the entropy of a single variable. But most of\nwhat we will use entropy for involves sequences . For a grammar, for example, we\nwill be computing the entropy of some sequence of words W=fw1;w2;:::; wng.\nOne way to do this is to have a variable that ranges over sequences of words. For\nexample we can compute the entropy of a random variable that ranges over all se-\nquences of words of length nin some language Las follows:\nH(w1;w2;:::; wn) =\u0000X\nw1:n2Lp(w1:n)logp(w1:n) (3.35)\nWe could de\ufb01ne the entropy rate (we could also think of this as the per-word entropy rate\nentropy ) as the entropy of this sequence divided by the number of words:\n1\nnH(w1:n) =\u00001\nnX\nw1:n2Lp(w1:n)logp(w1:n) (3.36)\nBut to measure the true entropy of a language, we need to consider sequences of\nin\ufb01nite length. If we think of a language as a stochastic process Lthat produces a\nsequence of words, and allow Wto represent the sequence of words w1;:::; wn, then\nL\u2019s entropy rate H(L)is de\ufb01ned as\nH(L) = lim\nn!\u00a51\nnH(w1:n)\n=\u0000lim\nn!\u00a51\nnX\nW2Lp(w1:n)logp(w1:n) (3.37)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 101,
      "token_count": 690,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 59\n\n3.7 \u2022 A DVANCED : PERPLEXITY \u2019SRELATION TO ENTROPY 51\nThe Shannon-McMillan-Breiman theorem (Algoet and Cover 1988, Cover and Thomas\n1991) states that if the language is regular in certain ways (to be exact, if it is both\nstationary and ergodic),\nH(L) =lim\nn!\u00a5\u00001\nnlogp(w1:n) (3.38)\nThat is, we can take a single sequence that is long enough instead of summing over\nall possible sequences. The intuition of the Shannon-McMillan-Breiman theorem\nis that a long-enough sequence of words will contain in it many other shorter se-\nquences and that each of these shorter sequences will reoccur in the longer sequence\naccording to their probabilities.\nA stochastic process is said to be stationary if the probabilities it assigns to a Stationary\nsequence are invariant with respect to shifts in the time index. In other words, the\nprobability distribution for words at time tis the same as the probability distribution\nat time t+1. Markov models, and hence n-grams, are stationary. For example, in\na bigram, Piis dependent only on Pi\u00001. So if we shift our time index by x,Pi+xis\nstill dependent on Pi+x\u00001. But natural language is not stationary, since as we show\nin Appendix D, the probability of upcoming words can be dependent on events that\nwere arbitrarily distant and time dependent. Thus, our statistical models only give\nan approximation to the correct distributions and entropies of natural language.\nTo summarize, by making some incorrect but convenient simplifying assump-\ntions, we can compute the entropy of some stochastic process by taking a very long\nsample of the output and computing its average log probability.\nNow we are ready to introduce cross-entropy . The cross-entropy is useful when cross-entropy\nwe don\u2019t know the actual probability distribution pthat generated some data. It\nallows us to use some m, which is a model of p(i.e., an approximation to p). The\ncross-entropy of monpis de\ufb01ned by\nH(p;m) =lim\nn!\u00a5\u00001\nnX\nW2Lp(w1;:::; wn)logm(w1;:::; wn) (3.39)\nThat is, we draw sequences according to the probability distribution p, but sum the\nlog of their probabilities according to m.\nAgain, following the Shannon-McMillan-Breiman theorem, for a stationary er-\ngodic process:\nH(p;m) =lim\nn!\u00a5\u00001\nnlogm(w1w2:::wn) (3.40)\nThis means that, as for entropy, we can estimate the cross-entropy of a model m\non some distribution pby taking a single sequence that is long enough instead of\nsumming over all possible sequences.\nWhat makes the cross-entropy useful is that the cross-entropy H(p;m)is an up-\nper bound on the entropy H(p). For any model m:\nH(p)\u0014H(p;m) (3.41)\nThis means that we can use some simpli\ufb01ed model mto help estimate the true en-\ntropy of a sequence of symbols drawn according to probability p. The more accurate\nmis, the closer the cross-entropy H(p;m)will be to the true entropy H(p). Thus,\nthe difference between H(p;m)andH(p)is a measure of how accurate a model is.\nBetween two models m1andm2, the more accurate model will be the one with the",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 102,
      "token_count": 774,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 60\n\n52 CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\nlower cross-entropy. (The cross-entropy can never be lower than the true entropy, so\na model cannot err by underestimating the true entropy.)\nWe are \ufb01nally ready to see the relation between perplexity and cross-entropy\nas we saw it in Eq. 3.40. Cross-entropy is de\ufb01ned in the limit as the length of the\nobserved word sequence goes to in\ufb01nity. We approximate this cross-entropy by\nrelying on a (suf\ufb01ciently long) sequence of \ufb01xed length. This approximation to the\ncross-entropy of a model M=P(wijwi\u0000N+1:i\u00001)on a sequence of words Wis\nH(W) =\u00001\nNlogP(w1w2:::wN) (3.42)\nTheperplexity of a model Pon a sequence of words Wis now formally de\ufb01ned as perplexity\n2 raised to the power of this cross-entropy:\nPerplexity (W) = 2H(W)\n=P(w1w2:::wN)\u00001\nN\n=Ns\n1\nP(w1w2:::wN)\n3.8 Summary\nThis chapter introduced language modeling via the n-gram model, a classic model\nthat allows us to introduce many of the basic concepts in language modeling.\n\u2022 Language models offer a way to assign a probability to a sentence or other\nsequence of words or tokens, and to predict a word or token from preceding\nwords or tokens.\n\u2022N-grams are perhaps the simplest kind of language model. They are Markov\nmodels that estimate words from a \ufb01xed window of previous words. N-gram\nmodels can be trained by counting in a training corpus and normalizing the\ncounts (the maximum likelihood estimate ).\n\u2022 N-gram language models can be evaluated on a test set using perplexity.\n\u2022 The perplexity of a test set according to a language model is a function of\nthe probability of the test set: the inverse test set probability according to the\nmodel, normalized by the length.\n\u2022Sampling from a language model means to generate some sentences, choos-\ning each sentence according to its likelihood as de\ufb01ned by the model.\n\u2022Smoothing algorithms provide a way to estimate probabilities for events that\nwere unseen in training. Commonly used smoothing algorithms for n-grams\ninclude add-1 smoothing, or rely on lower-order n-gram counts through inter-\npolation .\nBibliographical and Historical Notes\nThe underlying mathematics of the n-gram was \ufb01rst proposed by Markov (1913),\nwho used what are now called Markov chains (bigrams and trigrams) to predict\nwhether an upcoming letter in Pushkin\u2019s Eugene Onegin would be a vowel or a con-\nsonant. Markov classi\ufb01ed 20,000 letters as V or C and computed the bigram and",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 103,
      "token_count": 627,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 61",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 104,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 53\ntrigram probability that a given letter would be a vowel given the previous one or\ntwo letters. Shannon (1948) applied n-grams to compute approximations to English\nword sequences. Based on Shannon\u2019s work, Markov models were commonly used in\nengineering, linguistic, and psychological work on modeling word sequences by the\n1950s. In a series of extremely in\ufb02uential papers starting with Chomsky (1956) and\nincluding Chomsky (1957) and Miller and Chomsky (1963), Noam Chomsky argued\nthat \u201c\ufb01nite-state Markov processes\u201d, while a possibly useful engineering heuristic,\nwere incapable of being a complete cognitive model of human grammatical knowl-\nedge. These arguments led many linguists and computational linguists to ignore\nwork in statistical modeling for decades.\nThe resurgence of n-gram language models came from Fred Jelinek and col-\nleagues at the IBM Thomas J. Watson Research Center, who were in\ufb02uenced by\nShannon, and James Baker at CMU, who was in\ufb02uenced by the prior, classi\ufb01ed\nwork of Leonard Baum and colleagues on these topics at labs like the US Institute\nfor Defense Analyses (IDA) after they were declassi\ufb01ed. Independently these two\nlabs successfully used n-grams in their speech recognition systems at the same time\n(Baker 1975b, Jelinek et al. 1975, Baker 1975a, Bahl et al. 1983, Jelinek 1990). The\nterms \u201clanguage model\u201d and \u201cperplexity\u201d were \ufb01rst used for this technology by the\nIBM group. Jelinek and his colleagues used the term language model in a pretty\nmodern way, to mean the entire set of linguistic in\ufb02uences on word sequence prob-\nabilities, including grammar, semantics, discourse, and even speaker characteristics,\nrather than just the particular n-gram model itself.\nAdd-one smoothing derives from Laplace\u2019s 1812 law of succession and was \ufb01rst\napplied as an engineering solution to the zero frequency problem by Jeffreys (1948)\nbased on an earlier Add-K suggestion by Johnson (1932). Problems with the add-\none algorithm are summarized in Gale and Church (1994).\nA wide variety of different language modeling and smoothing techniques were\nproposed in the 80s and 90s, including Good-Turing discounting\u2014\ufb01rst applied to the\nn-gram smoothing at IBM by Katz (N \u00b4adas 1984, Church and Gale 1991)\u2014 Witten-\nBell discounting (Witten and Bell, 1991), and varieties of class-based n-gram mod-class-based\nn-gram\nels that used information about word classes. Starting in the late 1990s, Chen and\nGoodman performed a number of carefully controlled experiments comparing dif-\nferent algorithms and parameters (Chen and Goodman 1999, Goodman 2006, inter\nalia). They showed the advantages of Modi\ufb01ed Interpolated Kneser-Ney , which\nbecame the standard baseline for n-gram language modeling around the turn of the\ncentury, especially because they showed that caches and class-based models pro-\nvided only minor additional improvement. SRILM (Stolcke, 2002) and KenLM\n(Hea\ufb01eld 2011, Hea\ufb01eld et al. 2013) are publicly available toolkits for building n-\ngram language models.\nLarge language models are based on neural networks rather than n-grams, en-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 105,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "one algorithm are summarized in Gale and Church (1994).\nA wide variety of different language modeling and smoothing techniques were\nproposed in the 80s and 90s, including Good-Turing discounting\u2014\ufb01rst applied to the\nn-gram smoothing at IBM by Katz (N \u00b4adas 1984, Church and Gale 1991)\u2014 Witten-\nBell discounting (Witten and Bell, 1991), and varieties of class-based n-gram mod-class-based\nn-gram\nels that used information about word classes. Starting in the late 1990s, Chen and\nGoodman performed a number of carefully controlled experiments comparing dif-\nferent algorithms and parameters (Chen and Goodman 1999, Goodman 2006, inter\nalia). They showed the advantages of Modi\ufb01ed Interpolated Kneser-Ney , which\nbecame the standard baseline for n-gram language modeling around the turn of the\ncentury, especially because they showed that caches and class-based models pro-\nvided only minor additional improvement. SRILM (Stolcke, 2002) and KenLM\n(Hea\ufb01eld 2011, Hea\ufb01eld et al. 2013) are publicly available toolkits for building n-\ngram language models.\nLarge language models are based on neural networks rather than n-grams, en-\nabling them to solve the two major problems with n-grams: (1) the number of param-\neters increases exponentially as the n-gram order increases, and (2) n-grams have no\nway to generalize from training examples to test set examples unless they use iden-\ntical words. Neural language models instead project words into a continuous space\nin which words with similar contexts have similar representations. We\u2019ll introduce\ntransformer-based large language models in Chapter 9, along the way introducing\nfeedforward language models (Bengio et al. 2006, Schwenk 2007) in Chapter 7 and\nrecurrent language models (Mikolov, 2012) in Chapter 8.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 106,
      "token_count": 435,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 62\n\n54 CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\nExercises\n3.1 Write out the equation for trigram probability estimation (modifying Eq. 3.11).\nNow write out all the non-zero trigram probabilities for the I am Sam corpus\non page 35.\n3.2 Calculate the probability of the sentence i want chinese food . Give two\nprobabilities, one using Fig. 3.2 and the \u2018useful probabilities\u2019 just below it on\npage 37, and another using the add-1 smoothed table in Fig. 3.7. Assume the\nadditional add-1 smoothed probabilities P(i|<s> ) =0:19 and P(</s>|food ) =\n0:40.\n3.3 Which of the two probabilities you computed in the previous exercise is higher,\nunsmoothed or smoothed? Explain why.\n3.4 We are given the following corpus, modi\ufb01ed from the one in the chapter:\n<s> I am Sam </s>\n<s> Sam I am </s>\n<s> I am Sam </s>\n<s> I do not like green eggs and Sam </s>\nUsing a bigram language model with add-one smoothing, what is P(Sam j\nam)? Include <s>and</s> in your counts just like any other token.\n3.5 Suppose we didn\u2019t use the end-symbol </s> . Train an unsmoothed bigram\ngrammar on the following training corpus without using the end-symbol </s> :\n<s> a b\n<s> b b\n<s> b a\n<s> a a\nDemonstrate that your bigram model does not assign a single probability dis-\ntribution across all sentence lengths by showing that the sum of the probability\nof the four possible 2 word sentences over the alphabet fa,bgis 1.0, and the\nsum of the probability of all possible 3 word sentences over the alphabet fa,bg\nis also 1.0.\n3.6 Suppose we train a trigram language model with add-one smoothing on a\ngiven corpus. The corpus contains V word types. Express a formula for esti-\nmating P(w3jw1,w2), where w3 is a word which follows the bigram (w1,w2),\nin terms of various n-gram counts and V . Use the notation c(w1,w2,w3) to\ndenote the number of times that trigram (w1,w2,w3) occurs in the corpus, and\nso on for bigrams and unigrams.\n3.7 We are given the following corpus, modi\ufb01ed from the one in the chapter:\n<s> I am Sam </s>\n<s> Sam I am </s>\n<s> I am Sam </s>\n<s> I do not like green eggs and Sam </s>\nIf we use linear interpolation smoothing between a maximum-likelihood bi-\ngram model and a maximum-likelihood unigram model with l1=1\n2andl2=\n1\n2, what is P(Samjam)? Include <s> and</s> in your counts just like any\nother token.\n3.8 Write a program to compute unsmoothed unigrams and bigrams.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 107,
      "token_count": 674,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 63\n\nEXERCISES 55\n3.9 Run your n-gram program on two different small corpora of your choice (you\nmight use email text or newsgroups). Now compare the statistics of the two\ncorpora. What are the differences in the most common unigrams between the\ntwo? How about interesting differences in bigrams?\n3.10 Add an option to your program to generate random sentences.\n3.11 Add an option to your program to compute the perplexity of a test set.\n3.12 You are given a training set of 100 numbers that consists of 91 zeros and 1\neach of the other digits 1-9. Now we see the following test set: 0 0 0 0 0 3 0 0\n0 0. What is the unigram perplexity?",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 108,
      "token_count": 179,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 64\n\n56 CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\nCHAPTER\n4Naive Bayes, Text Classi\ufb01ca-\ntion, and Sentiment\nClassi\ufb01cation lies at the heart of both human and machine intelligence. Deciding\nwhat letter, word, or image has been presented to our senses, recognizing faces\nor voices, sorting mail, assigning grades to homeworks; these are all examples of\nassigning a category to an input. The potential challenges of this task are highlighted\nby the fabulist Jorge Luis Borges (1964), who imagined classifying animals into:\n(a) those that belong to the Emperor, (b) embalmed ones, (c) those that\nare trained, (d) suckling pigs, (e) mermaids, (f) fabulous ones, (g) stray\ndogs, (h) those that are included in this classi\ufb01cation, (i) those that\ntremble as if they were mad, (j) innumerable ones, (k) those drawn with\na very \ufb01ne camel\u2019s hair brush, (l) others, (m) those that have just broken\na \ufb02ower vase, (n) those that resemble \ufb02ies from a distance.\nMany language processing tasks involve classi\ufb01cation, although luckily our classes\nare much easier to de\ufb01ne than those of Borges. In this chapter we introduce the naive\nBayes algorithm and apply it to text categorization , the task of assigning a label ortext\ncategorization\ncategory to an entire text or document.\nWe focus on one common text categorization task, sentiment analysis , the ex-sentiment\nanalysis\ntraction of sentiment , the positive or negative orientation that a writer expresses\ntoward some object. A review of a movie, book, or product on the web expresses the\nauthor\u2019s sentiment toward the product, while an editorial or political text expresses\nsentiment toward a candidate or political action. Extracting consumer or public sen-\ntiment is thus relevant for \ufb01elds from marketing to politics.\nThe simplest version of sentiment analysis is a binary classi\ufb01cation task, and\nthe words of the review provide excellent cues. Consider, for example, the follow-\ning phrases extracted from positive and negative reviews of movies and restaurants.\nWords like great ,richly ,awesome , and pathetic , and awful andridiculously are very\ninformative cues:\n+...zany characters and richly applied satire, and some great plot twists\n\u0000It was pathetic. The worst part about it was the boxing scenes...\n+...awesome caramel sauce and sweet toasty almonds. I love this place!\n\u0000...awful pizza and ridiculously overpriced...\nSpam detection is another important commercial application, the binary clas- spam detection\nsi\ufb01cation task of assigning an email to one of the two classes spam ornot-spam .\nMany lexical and other features can be used to perform this classi\ufb01cation. For ex-\nample you might quite reasonably be suspicious of an email containing phrases like\n\u201conline pharmaceutical\u201d or \u201cWITHOUT ANY COST\u201d or \u201cDear Winner\u201d.\nAnother thing we might want to know about a text is the language it\u2019s written\nin. Texts on social media, for example, can be in any number of languages and\nwe\u2019ll need to apply different processing. The task of language id is thus the \ufb01rst language id\nstep in most language processing pipelines. Related text classi\ufb01cation tasks like au-\nthorship attribution \u2014 determining a text\u2019s author\u2014 are also relevant to the digitalauthorship\nattribution\nhumanities, social sciences, and forensic linguistics.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 109,
      "token_count": 780,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 65",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 110,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "4.1 \u2022 N AIVE BAYES CLASSIFIERS 57\nFinally, one of the oldest tasks in text classi\ufb01cation is assigning a library sub-\nject category or topic label to a text. Deciding whether a research paper concerns\nepidemiology or instead, perhaps, embryology, is an important component of infor-\nmation retrieval. Various sets of subject categories exist, such as the MeSH (Medical\nSubject Headings) thesaurus. In fact, as we will see, subject category classi\ufb01cation\nis the task for which the naive Bayes algorithm was invented in 1961 (Maron, 1961).\nClassi\ufb01cation is essential for tasks below the level of the document as well.\nWe\u2019ve already seen period disambiguation (deciding if a period is the end of a sen-\ntence or part of a word), and word tokenization (deciding if a character should be\na word boundary). Even language modeling can be viewed as classi\ufb01cation: each\nword can be thought of as a class, and so predicting the next word is classifying the\ncontext-so-far into a class for each next word. A part-of-speech tagger (Chapter 17)\nclassi\ufb01es each occurrence of a word in a sentence as, e.g., a noun or a verb.\nThe goal of classi\ufb01cation is to take a single observation, extract some useful\nfeatures, and thereby classify the observation into one of a set of discrete classes.\nOne method for classifying text is to use rules handwritten by humans. Handwrit-\nten rule-based classi\ufb01ers can be components of state-of-the-art systems in language\nprocessing. But rules can be fragile, as situations or data change over time, and for\nsome tasks humans aren\u2019t necessarily good at coming up with the rules.\nThe most common way of doing text classi\ufb01cation in language processing is\ninstead via supervised machine learning , the subject of this chapter. In supervisedsupervised\nmachine\nlearninglearning, we have a data set of input observations, each associated with some correct\noutput (a \u2018supervision signal\u2019). The goal of the algorithm is to learn how to map\nfrom a new observation to a correct output.\nFormally, the task of supervised classi\ufb01cation is to take an input xand a \ufb01xed\nset of output classes Y=fy1;y2;:::;yMgand return a predicted class y2Y. For\ntext classi\ufb01cation, we\u2019ll sometimes talk about c(for \u201cclass\u201d) instead of yas our\noutput variable, and d(for \u201cdocument\u201d) instead of xas our input variable. In the\nsupervised situation we have a training set of Ndocuments that have each been hand-\nlabeled with a class: f(d1;c1);::::;(dN;cN)g. Our goal is to learn a classi\ufb01er that is\ncapable of mapping from a new document dto its correct class c2C, where Cis\nsome set of useful document classes. A probabilistic classi\ufb01er additionally will tell\nus the probability of the observation being in the class. This full distribution over\nthe classes can be useful information for downstream decisions; avoiding making\ndiscrete decisions early on can be useful when combining systems.\nMany kinds of machine learning algorithms are used to build classi\ufb01ers. This\nchapter introduces naive Bayes; the following one introduces logistic regression.\nThese exemplify two ways of doing classi\ufb01cation. Generative classi\ufb01ers like naive\nBayes build a model of how a class could generate some input data. Given an ob-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 111,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "set of output classes Y=fy1;y2;:::;yMgand return a predicted class y2Y. For\ntext classi\ufb01cation, we\u2019ll sometimes talk about c(for \u201cclass\u201d) instead of yas our\noutput variable, and d(for \u201cdocument\u201d) instead of xas our input variable. In the\nsupervised situation we have a training set of Ndocuments that have each been hand-\nlabeled with a class: f(d1;c1);::::;(dN;cN)g. Our goal is to learn a classi\ufb01er that is\ncapable of mapping from a new document dto its correct class c2C, where Cis\nsome set of useful document classes. A probabilistic classi\ufb01er additionally will tell\nus the probability of the observation being in the class. This full distribution over\nthe classes can be useful information for downstream decisions; avoiding making\ndiscrete decisions early on can be useful when combining systems.\nMany kinds of machine learning algorithms are used to build classi\ufb01ers. This\nchapter introduces naive Bayes; the following one introduces logistic regression.\nThese exemplify two ways of doing classi\ufb01cation. Generative classi\ufb01ers like naive\nBayes build a model of how a class could generate some input data. Given an ob-\nservation, they return the class most likely to have generated the observation. Dis-\ncriminative classi\ufb01ers like logistic regression instead learn what features from the\ninput are most useful to discriminate between the different possible classes. While\ndiscriminative systems are often more accurate and hence more commonly used,\ngenerative classi\ufb01ers still have a role.\n4.1 Naive Bayes Classi\ufb01ers\nIn this section we introduce the multinomial naive Bayes classi\ufb01er , so called be-naive Bayes\nclassi\ufb01er\ncause it is a Bayesian classi\ufb01er that makes a simplifying (naive) assumption about",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 112,
      "token_count": 417,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 66\n\n58 CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\nhow the features interact.\nThe intuition of the classi\ufb01er is shown in Fig. 4.1. We represent a text document\nas if it were a bag of words , that is, an unordered set of words with their position bag of words\nignored, keeping only their frequency in the document. In the example in the \ufb01gure,\ninstead of representing the word order in all the phrases like \u201cI love this movie\u201d and\n\u201cI would recommend it\u201d, we simply note that the word Ioccurred 5 times in the\nentire excerpt, the word it6 times, the words love,recommend , and movie once, and\nso on.\nititititititIIII\nIloverecommendmoviethethethetheto\ntotoand\nandandseenseenyetwouldwithwhowhimsical\nwhilewhenevertimessweetseveralscenessatiricalromanticofmanageshumorhavehappyfunfriendfairydialoguebutconventionsareanyoneadventurealwaysagainaboutI love this movie! It's sweet, but with satirical humor. The dialogue is great and the adventure scenes are fun... It manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre. I would recommend it to just about anyone. I've seen it several times, and I'm always happy to see it again whenever I have a friend who hasn't seen it yet!it Ithetoandseenyetwouldwhimsicaltimessweetsatiricaladventuregenrefairyhumorhavegreat\u20266 54332111111111111\u2026\nFigure 4.1 Intuition of the multinomial naive Bayes classi\ufb01er applied to a movie review. The position of the\nwords is ignored (the bag-of-words assumption) and we make use of the frequency of each word.\nNaive Bayes is a probabilistic classi\ufb01er, meaning that for a document d, out of\nall classes c2Cthe classi\ufb01er returns the class \u02c6 cwhich has the maximum posterior\nprobability given the document. In Eq. 4.1 we use the hat notation \u02c6to mean \u201cour \u02c6\nestimate of the correct class\u201d, and we use argmax to mean an operation that selects argmax\nthe argument (in this case the class c) that maximizes a function (in this case the\nprobability P(cjd).\n\u02c6c=argmax\nc2CP(cjd) (4.1)\nThis idea of Bayesian inference has been known since the work of Bayes (1763),Bayesian\ninference\nand was \ufb01rst applied to text classi\ufb01cation by Mosteller and Wallace (1964). The\nintuition of Bayesian classi\ufb01cation is to use Bayes\u2019 rule to transform Eq. 4.1 into\nother probabilities that have some useful properties. Bayes\u2019 rule is presented in\nEq. 4.2; it gives us a way to break down any conditional probability P(xjy)into\nthree other probabilities:\nP(xjy) =P(yjx)P(x)\nP(y)(4.2)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 113,
      "token_count": 692,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 67\n\n4.1 \u2022 N AIVE BAYES CLASSIFIERS 59\nWe can then substitute Eq. 4.2 into Eq. 4.1 to get Eq. 4.3:\n\u02c6c=argmax\nc2CP(cjd) =argmax\nc2CP(djc)P(c)\nP(d)(4.3)\nWe can conveniently simplify Eq. 4.3 by dropping the denominator P(d). This\nis possible because we will be computingP(djc)P(c)\nP(d)for each possible class. But P(d)\ndoesn\u2019t change for each class; we are always asking about the most likely class for\nthe same document d, which must have the same probability P(d). Thus, we can\nchoose the class that maximizes this simpler formula:\n\u02c6c=argmax\nc2CP(cjd) =argmax\nc2CP(djc)P(c) (4.4)\nWe call Naive Bayes a generative model because we can read Eq. 4.4 as stating\na kind of implicit assumption about how a document is generated: \ufb01rst a class is\nsampled from P(c), and then the words are generated by sampling from P(djc). (In\nfact we could imagine generating arti\ufb01cial documents, or at least their word counts,\nby following this process). We\u2019ll say more about this intuition of generative models\nin Chapter 5.\nTo return to classi\ufb01cation: we compute the most probable class \u02c6 cgiven some\ndocument dby choosing the class which has the highest product of two probabilities:\ntheprior probability of the class P(c)and the likelihood of the document P(djc):prior\nprobability\nlikelihood\n\u02c6c=argmax\nc2Clikelihoodz}|{\nP(djc)priorz}|{\nP(c) (4.5)\nWithout loss of generality, we can represent a document das a set of features\nf1;f2;:::;fn:\n\u02c6c=argmax\nc2Clikelihoodz}|{\nP(f1;f2;::::; fnjc)priorz}|{\nP(c) (4.6)\nUnfortunately, Eq. 4.6 is still too hard to compute directly: without some sim-\nplifying assumptions, estimating the probability of every possible combination of\nfeatures (for example, every possible set of words and positions) would require huge\nnumbers of parameters and impossibly large training sets. Naive Bayes classi\ufb01ers\ntherefore make two simplifying assumptions.\nThe \ufb01rst is the bag-of-words assumption discussed intuitively above: we assume\nposition doesn\u2019t matter, and that the word \u201clove\u201d has the same effect on classi\ufb01cation\nwhether it occurs as the 1st, 20th, or last word in the document. Thus we assume\nthat the features f1;f2;:::;fnonly encode word identity and not position.\nThe second is commonly called the naive Bayes assumption : this is the condi-naive Bayes\nassumption\ntional independence assumption that the probabilities P(fijc)are independent given\nthe class cand hence can be \u2018naively\u2019 multiplied as follows:\nP(f1;f2;::::; fnjc) = P(f1jc)\u0001P(f2jc)\u0001:::\u0001P(fnjc) (4.7)\nThe \ufb01nal equation for the class chosen by a naive Bayes classi\ufb01er is thus:\ncNB=argmax\nc2CP(c)Y\nf2FP(fjc) (4.8)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 114,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 68\n\n60 CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\nTo apply the naive Bayes classi\ufb01er to text, we will use each word in the documents\nas a feature, as suggested above, and we consider each of the words in the document\nby walking an index through every word position in the document:\npositions all word positions in test document\ncNB=argmax\nc2CP(c)Y\ni2positionsP(wijc) (4.9)\nNaive Bayes calculations, like calculations for language modeling, are done in log\nspace, to avoid under\ufb02ow and increase speed. Thus Eq. 4.9 is generally instead\nexpressed1as\ncNB=argmax\nc2ClogP(c)+X\ni2positionslogP(wijc) (4.10)\nBy considering features in log space, Eq. 4.10 computes the predicted class as a lin-\near function of input features. Classi\ufb01ers that use a linear combination of the inputs\nto make a classi\ufb01cation decision \u2014like naive Bayes and also logistic regression\u2014\nare called linear classi\ufb01ers .linear\nclassi\ufb01ers\n4.2 Training the Naive Bayes Classi\ufb01er\nHow can we learn the probabilities P(c)andP(fijc)? Let\u2019s \ufb01rst consider the maxi-\nmum likelihood estimate. We\u2019ll simply use the frequencies in the data. For the class\nprior P(c)we ask what percentage of the documents in our training set are in each\nclass c. Let Ncbe the number of documents in our training data with class cand\nNdocbe the total number of documents. Then:\n\u02c6P(c) =Nc\nNdoc(4.11)\nTo learn the probability P(fijc), we\u2019ll assume a feature is just the existence of a word\nin the document\u2019s bag of words, and so we\u2019ll want P(wijc), which we compute as\nthe fraction of times the word wiappears among all words in all documents of topic\nc. We \ufb01rst concatenate all documents with category cinto one big \u201ccategory c\u201d text.\nThen we use the frequency of wiin this concatenated document to give a maximum\nlikelihood estimate of the probability:\n\u02c6P(wijc) =count (wi;c)P\nw2Vcount (w;c)(4.12)\nHere the vocabulary V consists of the union of all the word types in all classes, not\njust the words in one class c.\nThere is a problem, however, with maximum likelihood training. Imagine we\nare trying to estimate the likelihood of the word \u201cfantastic\u201d given class positive , but\nsuppose there are no training documents that both contain the word \u201cfantastic\u201d and\nare classi\ufb01ed as positive . Perhaps the word \u201cfantastic\u201d happens to occur (sarcasti-\ncally?) in the class negative . In such a case the probability for this feature will be\nzero:\n\u02c6P(\u201cfantastic\u201djpositive ) =count (\u201cfantastic\u201d;positive )P\nw2Vcount (w;positive )=0 (4.13)\n1In practice throughout this book, we\u2019ll use log to mean natural log (ln) when the base is not speci\ufb01ed.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 115,
      "token_count": 713,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 69\n\n4.3 \u2022 W ORKED EXAMPLE 61\nBut since naive Bayes naively multiplies all the feature likelihoods together, zero\nprobabilities in the likelihood term for any class will cause the probability of the\nclass to be zero, no matter the other evidence!\nThe simplest solution is the add-one (Laplace) smoothing introduced in Chap-\nter 3. While Laplace smoothing is usually replaced by more sophisticated smoothing\nalgorithms in language modeling, it is commonly used in naive Bayes text catego-\nrization:\n\u02c6P(wijc) =count (wi;c)+1P\nw2V(count (w;c)+1)=count (wi;c)+1\u0000P\nw2Vcount (w;c)\u0001\n+jVj(4.14)\nNote once again that it is crucial that the vocabulary V consists of the union of all the\nword types in all classes, not just the words in one class c(try to convince yourself\nwhy this must be true; see the exercise at the end of the chapter).\nWhat do we do about words that occur in our test data but are not in our vocab-\nulary at all because they did not occur in any training document in any class? The\nsolution for such unknown words is to ignore them\u2014remove them from the test unknown word\ndocument and not include any probability for them at all.\nFinally, some systems choose to completely ignore another class of words: stop\nwords , very frequent words like theanda. This can be done by sorting the vocabu- stop words\nlary by frequency in the training set, and de\ufb01ning the top 10\u2013100 vocabulary entries\nas stop words, or alternatively by using one of the many prede\ufb01ned stop word lists\navailable online. Then each instance of these stop words is simply removed from\nboth training and test documents as if it had never occurred. In most text classi\ufb01ca-\ntion applications, however, using a stop word list doesn\u2019t improve performance, and\nso it is more common to make use of the entire vocabulary and not use a stop word\nlist.\nFig. 4.2 shows the \ufb01nal algorithm.\n4.3 Worked example\nLet\u2019s walk through an example of training and testing naive Bayes with add-one\nsmoothing. We\u2019ll use a sentiment analysis domain with the two classes positive\n(+) and negative (-), and take the following miniature training and test documents\nsimpli\ufb01ed from actual movie reviews.\nCat Documents\nTraining - just plain boring\n- entirely predictable and lacks energy\n- no surprises and very few laughs\n+ very powerful\n+ the most fun \ufb01lm of the summer\nTest ? predictable with no fun\nThe prior P(c)for the two classes is computed via Eq. 4.11 asNc\nNdoc:\nP(\u0000) =3\n5P(+) =2\n5\nThe word with doesn\u2019t occur in the training set, so we drop it completely (as\nmentioned above, we don\u2019t use unknown word models for naive Bayes). The like-\nlihoods from the training set for the remaining three words \u201cpredictable\u201d, \u201cno\u201d, and",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 116,
      "token_count": 669,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 70\n\n62 CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\nfunction TRAIN NAIVE BAYES (D, C) returns V;logP(c), log P(wjc)\nfor each class c2C # Calculate P(c)terms\nNdoc= number of documents in D\nNc= number of documents from D in class c\nlogprior [c] logNc\nNdoc\nV vocabulary of D\nbigdoc [c] append (d)ford2Dwith class c\nfor each word win V # Calculate P(wjc)terms\ncount(w,c) # of occurrences of winbigdoc [c]\nloglikelihood [w,c] logcount (w;c) + 1P\nw0in V(count (w0;c) + 1)\nreturn logprior ,loglikelihood ,V\nfunction TESTNAIVE BAYES (testdoc ,logprior ,loglikelihood , C, V) returns bestc\nfor each class c2C\nsum[c] logprior [c]\nfor each position iintestdoc\nword testdoc[i]\nifword2V\nsum[c] sum[c]+loglikelihood [word ,c]\nreturn argmaxcsum[c]\nFigure 4.2 The naive Bayes algorithm, using add-1 smoothing. To use add- asmoothing\ninstead, change the +1 to+afor loglikelihood counts in training.\n\u201cfun\u201d, are as follows, from Eq. 4.14 (computing the probabilities for the remainder\nof the words in the training set is left as an exercise for the reader):\nP(\u201cpredictable\u201dj\u0000) =1+1\n14+20P(\u201cpredictable\u201dj+) =0+1\n9+20\nP(\u201cno\u201dj\u0000) =1+1\n14+20P(\u201cno\u201dj+) =0+1\n9+20\nP(\u201cfun\u201dj\u0000) =0+1\n14+20P(\u201cfun\u201dj+) =1+1\n9+20\nFor the test sentence S = \u201cpredictable with no fun\u201d, after removing the word \u2018with\u2019,\nthe chosen class, via Eq. 4.9, is therefore computed as follows:\nP(\u0000)P(Sj\u0000) =3\n5\u00022\u00022\u00021\n343=6:1\u000210\u00005\nP(+)P(Sj+) =2\n5\u00021\u00021\u00022\n293=3:2\u000210\u00005\nThe model thus predicts the class negative for the test sentence.\n4.4 Optimizing for Sentiment Analysis\nWhile standard naive Bayes text classi\ufb01cation can work well for sentiment analysis,\nsome small changes are generally employed that improve performance.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 117,
      "token_count": 584,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 71\n\n4.4 \u2022 O PTIMIZING FOR SENTIMENT ANALYSIS 63\nFirst, for sentiment classi\ufb01cation and a number of other text classi\ufb01cation tasks,\nwhether a word occurs or not seems to matter more than its frequency. Thus it often\nimproves performance to clip the word counts in each document at 1 (see the end\nof the chapter for pointers to these results). This variant is called binary multino-\nmial naive Bayes orbinary naive Bayes . The variant uses the same algorithm asbinary naive\nBayes\nin Fig. 4.2 except that for each document we remove all duplicate words before con-\ncatenating them into the single big document during training and we also remove\nduplicate words from test documents. Fig. 4.3 shows an example in which a set\nof four documents (shortened and text-normalized for this example) are remapped\nto binary, with the modi\ufb01ed counts shown in the table on the right. The example\nis worked without add-1 smoothing to make the differences clearer. Note that the\nresults counts need not be 1; the word great has a count of 2 even for binary naive\nBayes, because it appears in multiple documents.\nFour original documents:\n\u0000it was pathetic the worst part was the\nboxing scenes\n\u0000no plot twists or great scenes\n+and satire and great plot twists\n+great scenes great \ufb01lm\nAfter per-document binarization:\n\u0000it was pathetic the worst part boxing\nscenes\n\u0000no plot twists or great scenes\n+and satire great plot twists\n+great scenes \ufb01lmNB Binary\nCounts Counts\n+\u0000+\u0000\nand 2 0 1 0\nboxing 0 1 0 1\n\ufb01lm 1 0 1 0\ngreat 3 1 2 1\nit 0 1 0 1\nno 0 1 0 1\nor 0 1 0 1\npart 0 1 0 1\npathetic 0 1 0 1\nplot 1 1 1 1\nsatire 1 0 1 0\nscenes 1 2 1 2\nthe 0 2 0 1\ntwists 1 1 1 1\nwas 0 2 0 1\nworst 0 1 0 1\nFigure 4.3 An example of binarization for the binary naive Bayes algorithm.\nA second important addition commonly made when doing text classi\ufb01cation for\nsentiment is to deal with negation. Consider the difference between I really like this\nmovie (positive) and I didn\u2019t like this movie (negative). The negation expressed by\ndidn\u2019t completely alters the inferences we draw from the predicate like. Similarly,\nnegation can modify a negative word to produce a positive review ( don\u2019t dismiss this\n\ufb01lm,doesn\u2019t let us get bored ).\nA very simple baseline that is commonly used in sentiment analysis to deal with\nnegation is the following: during text normalization, prepend the pre\ufb01x NOT to\nevery word after a token of logical negation ( n\u2019t, not, no, never ) until the next punc-\ntuation mark. Thus the phrase\ndidn't like this movie , but I\nbecomes\ndidn't NOT_like NOT_this NOT_movie , but I\nNewly formed \u2018words\u2019 like NOT like,NOT recommend will thus occur more\noften in negative document and act as cues for negative sentiment, while words\nlikeNOT bored ,NOT dismiss will acquire positive associations. Syntactic parsing\n(Chapter 18) can be used deal more accurately with the scope relationship between",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 118,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 72",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 119,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "64 CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\nthese negation words and the predicates they modify, but this simple baseline works\nquite well in practice.\nFinally, in some situations we might have insuf\ufb01cient labeled training data to\ntrain accurate naive Bayes classi\ufb01ers using all words in the training set to estimate\npositive and negative sentiment. In such cases we can instead derive the positive\nand negative word features from sentiment lexicons , lists of words that are pre-sentiment\nlexicons\nannotated with positive or negative sentiment. Four popular lexicons are the General\nInquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexiconGeneral\nInquirer\nLIWC of Hu and Liu (2004a) and the MPQA Subjectivity Lexicon (Wilson et al., 2005).\nFor example the MPQA subjectivity lexicon has 6885 words each marked for\nwhether it is strongly or weakly biased positive or negative. Some examples:\n+:admirable, beautiful, con\ufb01dent, dazzling, ecstatic, favor, glee, great\n\u0000:awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate\nA common way to use lexicons in a naive Bayes classi\ufb01er is to add a feature\nthat is counted whenever a word from that lexicon occurs. Thus we might add a\nfeature called \u2018this word occurs in the positive lexicon\u2019, and treat all instances of\nwords in the lexicon as counts for that one feature, instead of counting each word\nseparately. Similarly, we might add as a second feature \u2018this word occurs in the\nnegative lexicon\u2019 of words in the negative lexicon. If we have lots of training data,\nand if the test data matches the training data, using just two features won\u2019t work as\nwell as using all the words. But when training data is sparse or not representative of\nthe test set, using dense lexicon features instead of sparse individual-word features\nmay generalize better.\nWe\u2019ll return to this use of lexicons in Chapter 22, showing how these lexicons\ncan be learned automatically, and how they can be applied to many other tasks be-\nyond sentiment classi\ufb01cation.\n4.5 Naive Bayes for other text classi\ufb01cation tasks\nIn the previous section we pointed out that naive Bayes doesn\u2019t require that our\nclassi\ufb01er use all the words in the training data as features. In fact features in naive\nBayes can express any property of the input text we want.\nConsider the task of spam detection , deciding if a particular piece of email is spam detection\nan example of spam (unsolicited bulk email)\u2014one of the \ufb01rst applications of naive\nBayes to text classi\ufb01cation (Sahami et al., 1998).\nA common solution here, rather than using all the words as individual features,\nis to prede\ufb01ne likely sets of words or phrases as features, combined with features\nthat are not purely linguistic. For example the open-source SpamAssassin tool2\nprede\ufb01nes features like the phrase \u201cone hundred percent guaranteed\u201d, or the feature\nmentions millions of dollars , which is a regular expression that matches suspiciously\nlarge sums of money. But it also includes features like HTML has a low ratio of text\nto image area , that aren\u2019t purely linguistic and might require some sophisticated\ncomputation, or totally non-linguistic features about, say, the path that the email\ntook to arrive. More sample SpamAssassin features:\n\u2022 Email subject line is all capital letters",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 120,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "4.5 Naive Bayes for other text classi\ufb01cation tasks\nIn the previous section we pointed out that naive Bayes doesn\u2019t require that our\nclassi\ufb01er use all the words in the training data as features. In fact features in naive\nBayes can express any property of the input text we want.\nConsider the task of spam detection , deciding if a particular piece of email is spam detection\nan example of spam (unsolicited bulk email)\u2014one of the \ufb01rst applications of naive\nBayes to text classi\ufb01cation (Sahami et al., 1998).\nA common solution here, rather than using all the words as individual features,\nis to prede\ufb01ne likely sets of words or phrases as features, combined with features\nthat are not purely linguistic. For example the open-source SpamAssassin tool2\nprede\ufb01nes features like the phrase \u201cone hundred percent guaranteed\u201d, or the feature\nmentions millions of dollars , which is a regular expression that matches suspiciously\nlarge sums of money. But it also includes features like HTML has a low ratio of text\nto image area , that aren\u2019t purely linguistic and might require some sophisticated\ncomputation, or totally non-linguistic features about, say, the path that the email\ntook to arrive. More sample SpamAssassin features:\n\u2022 Email subject line is all capital letters\n\u2022 Contains phrases of urgency like \u201curgent reply\u201d\n2https://spamassassin.apache.org",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 121,
      "token_count": 306,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 73\n\n4.6 \u2022 N AIVE BAYES AS A LANGUAGE MODEL 65\n\u2022 Email subject line contains \u201conline pharmaceutical\u201d\n\u2022 HTML has unbalanced \u201chead\u201d tags\n\u2022 Claims you can be removed from the list\nFor other tasks, like language id \u2014determining what language a given piece language id\nof text is written in\u2014the most effective naive Bayes features are not words at all,\nbutcharacter n-grams , 2-grams (\u2018zw\u2019) 3-grams (\u2018nya\u2019, \u2018 V o\u2019), or 4-grams (\u2018ie z\u2019,\n\u2018thei\u2019), or, even simpler byte n-grams , where instead of using the multibyte Unicode\ncharacter representations called codepoints, we just pretend everything is a string of\nraw bytes. Because spaces count as a byte, byte n-grams can model statistics about\nthe beginning or ending of words. A widely used naive Bayes system, langid.py\n(Lui and Baldwin, 2012) begins with all possible n-grams of lengths 1-4, using fea-\nture selection to winnow down to the most informative 7000 \ufb01nal features.\nLanguage ID systems are trained on multilingual text, such as Wikipedia (Wiki-\npedia text in 68 different languages was used in (Lui and Baldwin, 2011)), or newswire.\nTo make sure that this multilingual text correctly re\ufb02ects different regions, dialects,\nand socioeconomic classes, systems also add Twitter text in many languages geo-\ntagged to many regions (important for getting world English dialects from countries\nwith large Anglophone populations like Nigeria or India), Bible and Quran transla-\ntions, slang websites like Urban Dictionary, corpora of African American Vernacular\nEnglish (Blodgett et al., 2016), and so on (Jurgens et al., 2017).\n4.6 Naive Bayes as a Language Model\nAs we saw in the previous section, naive Bayes classi\ufb01ers can use any sort of feature:\ndictionaries, URLs, email addresses, network features, phrases, and so on. But if,\nas in Section 4.3, we use only individual word features, and we use all of the words\nin the text (not a subset), then naive Bayes has an important similarity to language\nmodeling. Speci\ufb01cally, a naive Bayes model can be viewed as a set of class-speci\ufb01c\nunigram language models, in which the model for each class instantiates a unigram\nlanguage model.\nSince the likelihood features from the naive Bayes model assign a probability to\neach word P(wordjc), the model also assigns a probability to each sentence:\nP(sjc) =Y\ni2positionsP(wijc) (4.15)\nThus consider a naive Bayes model with the classes positive (+) and negative (-)\nand the following model parameters:\nw P(wj+)P(wj-)\nI 0.1 0.2\nlove 0.1 0.001\nthis 0.01 0.01\nfun 0.05 0.005\n\ufb01lm 0.1 0.1\n... ... ...\nEach of the two columns above instantiates a language model that can assign a\nprobability to the sentence \u201cI love this fun \ufb01lm\u201d:",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 122,
      "token_count": 713,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 74",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 123,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "66 CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\nP(\u201cI love this fun \ufb01lm\u201d j+) = 0:1\u00020:1\u00020:01\u00020:05\u00020:1=5\u000210\u00007\nP(\u201cI love this fun \ufb01lm\u201d j\u0000) = 0:2\u00020:001\u00020:01\u00020:005\u00020:1=1:0\u000210\u00009\nAs it happens, the positive model assigns a higher probability to the sentence:\nP(sjpos)>P(sjneg). Note that this is just the likelihood part of the naive Bayes\nmodel; once we multiply in the prior a full naive Bayes model might well make a\ndifferent classi\ufb01cation decision.\n4.7 Evaluation: Precision, Recall, F-measure\nTo introduce the methods for evaluating text classi\ufb01cation, let\u2019s \ufb01rst consider some\nsimple binary detection tasks. For example, in spam detection, our goal is to label\nevery text as being in the spam category (\u201cpositive\u201d) or not in the spam category\n(\u201cnegative\u201d). For each item (email document) we therefore need to know whether\nour system called it spam or not. We also need to know whether the email is actually\nspam or not, i.e. the human-de\ufb01ned labels for each document that we are trying to\nmatch. We will refer to these human labels as the gold labels . gold labels\nOr imagine you\u2019re the CEO of the Delicious Pie Company and you need to know\nwhat people are saying about your pies on social media, so you build a system that\ndetects tweets concerning Delicious Pie. Here the positive class is tweets about\nDelicious Pie and the negative class is all other tweets.\nIn both cases, we need a metric for knowing how well our spam detector (or\npie-tweet-detector) is doing. To evaluate any system for detecting things, we start\nby building a confusion matrix like the one shown in Fig. 4.4. A confusion matrixconfusion\nmatrix\nis a table for visualizing how an algorithm performs with respect to the human gold\nlabels, using two dimensions (system output and gold labels), and each cell labeling\na set of possible outcomes. In the spam detection case, for example, true positives\nare documents that are indeed spam (indicated by human-created gold labels) that\nour system correctly said were spam. False negatives are documents that are indeed\nspam but our system incorrectly labeled as non-spam.\nTo the bottom right of the table is the equation for accuracy , which asks what\npercentage of all the observations (for the spam or pie examples that means all emails\nor tweets) our system labeled correctly. Although accuracy might seem a natural\nmetric, we generally don\u2019t use it for text classi\ufb01cation tasks. That\u2019s because accuracy\ndoesn\u2019t work well when the classes are unbalanced (as indeed they are with spam,\nwhich is a large majority of email, or with tweets, which are mainly not about pie).\nTo make this more explicit, imagine that we looked at a million tweets, and\nlet\u2019s say that only 100 of them are discussing their love (or hatred) for our pie,\nwhile the other 999,900 are tweets about something completely unrelated. Imagine a\nsimple classi\ufb01er that stupidly classi\ufb01ed every tweet as \u201cnot about pie\u201d. This classi\ufb01er\nwould have 999,900 true negatives and only 100 false negatives for an accuracy of\n999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we should",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 124,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "are documents that are indeed spam (indicated by human-created gold labels) that\nour system correctly said were spam. False negatives are documents that are indeed\nspam but our system incorrectly labeled as non-spam.\nTo the bottom right of the table is the equation for accuracy , which asks what\npercentage of all the observations (for the spam or pie examples that means all emails\nor tweets) our system labeled correctly. Although accuracy might seem a natural\nmetric, we generally don\u2019t use it for text classi\ufb01cation tasks. That\u2019s because accuracy\ndoesn\u2019t work well when the classes are unbalanced (as indeed they are with spam,\nwhich is a large majority of email, or with tweets, which are mainly not about pie).\nTo make this more explicit, imagine that we looked at a million tweets, and\nlet\u2019s say that only 100 of them are discussing their love (or hatred) for our pie,\nwhile the other 999,900 are tweets about something completely unrelated. Imagine a\nsimple classi\ufb01er that stupidly classi\ufb01ed every tweet as \u201cnot about pie\u201d. This classi\ufb01er\nwould have 999,900 true negatives and only 100 false negatives for an accuracy of\n999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we should\nbe happy with this classi\ufb01er? But of course this fabulous \u2018no pie\u2019 classi\ufb01er would\nbe completely useless, since it wouldn\u2019t \ufb01nd a single one of the customer comments\nwe are looking for. In other words, accuracy is not a good metric when the goal is\nto discover something that is rare, or at least not completely balanced in frequency,\nwhich is a very common situation in the world.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 125,
      "token_count": 370,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 75\n\n4.7 \u2022 E VALUATION : PRECISION , RECALL , F- MEASURE 67\ntrue positivefalse negativefalse positivetrue negativegold positivegold negativesystempositivesystemnegativegold standard labelssystemoutputlabelsrecall = tptp+fnprecision = tptp+fpaccuracy = tp+tntp+fp+tn+fn\nFigure 4.4 A confusion matrix for visualizing how well a binary classi\ufb01cation system per-\nforms against gold standard labels.\nThat\u2019s why instead of accuracy we generally turn to two other metrics shown in\nFig. 4.4: precision andrecall .Precision measures the percentage of the items that precision\nthe system detected (i.e., the system labeled as positive) that are in fact positive (i.e.,\nare positive according to the human gold labels). Precision is de\ufb01ned as\nPrecision =true positives\ntrue positives + false positives\nRecall measures the percentage of items actually present in the input that were recall\ncorrectly identi\ufb01ed by the system. Recall is de\ufb01ned as\nRecall =true positives\ntrue positives + false negatives\nPrecision and recall will help solve the problem with the useless \u201cnothing is\npie\u201d classi\ufb01er. This classi\ufb01er, despite having a fabulous accuracy of 99.99%, has\na terrible recall of 0 (since there are no true positives, and 100 false negatives, the\nrecall is 0/100). You should convince yourself that the precision at \ufb01nding relevant\ntweets is equally problematic. Thus precision and recall, unlike accuracy, emphasize\ntrue positives: \ufb01nding the things that we are supposed to be looking for.\nThere are many ways to de\ufb01ne a single metric that incorporates aspects of both\nprecision and recall. The simplest of these combinations is the F-measure (van F-measure\nRijsbergen, 1975) , de\ufb01ned as:\nFb=(b2+1)PR\nb2P+R\nThebparameter differentially weights the importance of recall and precision,\nbased perhaps on the needs of an application. Values of b>1 favor recall, while\nvalues of b<1 favor precision. When b=1, precision and recall are equally bal-\nanced; this is the most frequently used metric, and is called F b=1or just F 1: F1\nF1=2PR\nP+R(4.16)\nF-measure comes from a weighted harmonic mean of precision and recall. The\nharmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-\nrocals:\nHarmonicMean (a1;a2;a3;a4;:::;an) =n\n1\na1+1\na2+1\na3+:::+1\nan(4.17)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 126,
      "token_count": 607,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 76\n\n68 CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\nand hence F-measure is\nF=1\na1\nP+(1\u0000a)1\nRor\u0012\nwithb2=1\u0000a\na\u0013\nF=(b2+1)PR\nb2P+R(4.18)\nHarmonic mean is used because the harmonic mean of two values is closer to the\nminimum of the two values than the arithmetic mean is. Thus it weighs the lower of\nthe two numbers more heavily, which is more conservative in this situation.\n4.7.1 Evaluating with more than two classes\nUp to now we have been describing text classi\ufb01cation tasks with only two classes.\nBut lots of classi\ufb01cation tasks in language processing have more than two classes.\nFor sentiment analysis we generally have 3 classes (positive, negative, neutral) and\neven more classes are common for tasks like part-of-speech tagging, word sense\ndisambiguation, semantic role labeling, emotion detection, and so on. Luckily the\nnaive Bayes algorithm is already a multi-class classi\ufb01cation algorithm.\n851060urgentnormalgold labelssystemoutputrecallu = 88+5+3precisionu= 88+10+115030200spamurgentnormalspam3recalln = recalls = precisionn= 605+60+50precisions= 2003+30+2006010+60+302001+50+200\nFigure 4.5 Confusion matrix for a three-class categorization task, showing for each pair of\nclasses (c1;c2), how many documents from c1were (in)correctly assigned to c2.\nBut we\u2019ll need to slightly modify our de\ufb01nitions of precision and recall. Con-\nsider the sample confusion matrix for a hypothetical 3-way one-of email catego-\nrization decision (urgent, normal, spam) shown in Fig. 4.5. The matrix shows, for\nexample, that the system mistakenly labeled one spam document as urgent, and we\nhave shown how to compute a distinct precision and recall value for each class. In\norder to derive a single metric that tells us how well the system is doing, we can com-\nbine these values in two ways. In macroaveraging , we compute the performance macroaveraging\nfor each class, and then average over classes. In microaveraging , we collect the de- microaveraging\ncisions for all classes into a single confusion matrix, and then compute precision and\nrecall from that table. Fig. 4.6 shows the confusion matrix for each class separately,\nand shows the computation of microaveraged and macroaveraged precision.\nAs the \ufb01gure shows, a microaverage is dominated by the more frequent class (in\nthis case spam), since the counts are pooled. The macroaverage better re\ufb02ects the\nstatistics of the smaller classes, and so is more appropriate when performance on all\nthe classes is equally important.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 127,
      "token_count": 645,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 77\n\n4.8 \u2022 T EST SETS AND CROSS -VALIDATION 69\n8811340trueurgenttruenotsystemurgentsystemnot604055212truenormaltruenotsystemnormalsystemnot200513383truespamtruenotsystemspamsystemnot2689999635trueyestruenosystemyessystemnoprecision =8+118= .42precision =200+33200= .86precision =60+5560= .52microaverageprecision268+99268= .73=macroaverageprecision3.42+.52+.86= .60=PooledClass 3: SpamClass 2: NormalClass 1: Urgent\nFigure 4.6 Separate confusion matrices for the 3 classes from the previous \ufb01gure, showing the pooled confu-\nsion matrix and the microaveraged and macroaveraged precision.\n4.8 Test sets and Cross-validation\nThe training and testing procedure for text classi\ufb01cation follows what we saw with\nlanguage modeling (Section 3.2): we use the training set to train the model, then use\nthedevelopment test set (also called a devset ) to perhaps tune some parameters,development\ntest set\ndevset and in general decide what the best model is. Once we come up with what we think\nis the best model, we run it on the (hitherto unseen) test set to report its performance.\nWhile the use of a devset avoids over\ufb01tting the test set, having a \ufb01xed train-\ning set, devset, and test set creates another problem: in order to save lots of data\nfor training, the test set (or devset) might not be large enough to be representative.\nWouldn\u2019t it be better if we could somehow use all our data for training and still use\nall our data for test? We can do this by cross-validation . cross-validation\nIn cross-validation, we choose a number k, and partition our data into kdisjoint\nsubsets called folds . Now we choose one of those kfolds as a test set, train our folds\nclassi\ufb01er on the remaining k\u00001 folds, and then compute the error rate on the test\nset. Then we repeat with another fold as the test set, again training on the other k\u00001\nfolds. We do this sampling process ktimes and average the test set error rate from\nthese kruns to get an average error rate. If we choose k=10, we would train 10\ndifferent models (each on 90% of our data), test the model 10 times, and average\nthese 10 values. This is called 10-fold cross-validation .10-fold\ncross-validation\nThe only problem with cross-validation is that because all the data is used for\ntesting, we need the whole corpus to be blind; we can\u2019t examine any of the data\nto suggest possible features and in general see what\u2019s going on, because we\u2019d be\npeeking at the test set, and such cheating would cause us to overestimate the perfor-\nmance of our system. However, looking at the corpus to understand what\u2019s going\non is important in designing NLP systems! What to do? For this reason, it is com-\nmon to create a \ufb01xed training set and test set, then do 10-fold cross-validation inside\nthe training set, but compute error rate the normal way in the test set, as shown in\nFig. 4.7.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 128,
      "token_count": 736,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 78\n\n70 CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\nTraining Iterations13452678910DevDevDevDevDevDevDevDevDevDevTrainingTrainingTrainingTrainingTrainingTrainingTrainingTrainingTrainingTrainingTrainingTest SetTesting\nFigure 4.7 10-fold cross-validation\n4.9 Statistical Signi\ufb01cance Testing\nIn building systems we often need to compare the performance of two systems. How\ncan we know if the new system we just built is better than our old one? Or better\nthan some other system described in the literature? This is the domain of statistical\nhypothesis testing, and in this section we introduce tests for statistical signi\ufb01cance\nfor NLP classi\ufb01ers, drawing especially on the work of Dror et al. (2020) and Berg-\nKirkpatrick et al. (2012).\nSuppose we\u2019re comparing the performance of classi\ufb01ers AandBon a metric M\nsuch as F 1, or accuracy. Perhaps we want to know if our logistic regression senti-\nment classi\ufb01er A(Chapter 5) gets a higher F 1score than our naive Bayes sentiment\nclassi\ufb01er Bon a particular test set x. Let\u2019s call M(A;x)the score that system Agets\non test set x, and d(x)the performance difference between AandBonx:\nd(x) =M(A;x)\u0000M(B;x) (4.19)\nWe would like to know if d(x)>0, meaning that our logistic regression classi\ufb01er\nhas a higher F 1than our naive Bayes classi\ufb01er on x.d(x)is called the effect size ; a effect size\nbigger dmeans that Aseems to be way better than B; a small dmeans Aseems to\nbe only a little better.\nWhy don\u2019t we just check if d(x)is positive? Suppose we do, and we \ufb01nd that\nthe F 1score of Ais higher than B\u2019s by .04. Can we be certain that Ais better? We\ncannot! That\u2019s because Amight just be accidentally better than Bon this particular x.\nWe need something more: we want to know if A\u2019s superiority over Bis likely to hold\nagain if we checked another test set x0, or under some other set of circumstances.\nIn the paradigm of statistical hypothesis testing, we test this by formalizing two\nhypotheses.\nH0:d(x)\u00140\nH1:d(x)>0 (4.20)\nThe hypothesis H0, called the null hypothesis , supposes that d(x)is actually nega- null hypothesis\ntive or zero, meaning that Ais not better than B. We would like to know if we can\ncon\ufb01dently rule out this hypothesis, and instead support H1, that Ais better.\nWe do this by creating a random variable Xranging over all test sets. Now we\nask how likely is it, if the null hypothesis H0was correct, that among these test sets",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 129,
      "token_count": 653,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 79",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 130,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "4.9 \u2022 S TATISTICAL SIGNIFICANCE TESTING 71\nwe would encounter the value of d(x)that we found, if we repeated the experiment\na great many times. We formalize this likelihood as the p-value : the probability, p-value\nassuming the null hypothesis H0is true, of seeing the d(x)that we saw or one even\ngreater\nP(d(X)\u0015d(x)jH0is true ) (4.21)\nSo in our example, this p-value is the probability that we would see d(x)assuming\nAisnotbetter than B. Ifd(x)is huge (let\u2019s say Ahas a very respectable F 1of .9\nandBhas a terrible F 1of only .2 on x), we might be surprised, since that would be\nextremely unlikely to occur if H0were in fact true, and so the p-value would be low\n(unlikely to have such a large difAis in fact not better than B). But if d(x)is very\nsmall, it might be less surprising to us even if H0were true and Ais not really better\nthan B, and so the p-value would be higher.\nA very small p-value means that the difference we observed is very unlikely\nunder the null hypothesis, and we can reject the null hypothesis. What counts as very\nsmall? It is common to use values like .05 or .01 as the thresholds. A value of .01\nmeans that if the p-value (the probability of observing the dwe saw assuming H0is\ntrue) is less than .01, we reject the null hypothesis and assume that Ais indeed better\nthan B. We say that a result (e.g., \u201c Ais better than B\u201d) is statistically signi\ufb01cant ifstatistically\nsigni\ufb01cant\nthedwe saw has a probability that is below the threshold and we therefore reject\nthis null hypothesis.\nHow do we compute this probability we need for the p-value? In NLP we gen-\nerally don\u2019t use simple parametric tests like t-tests or ANOV As that you might be\nfamiliar with. Parametric tests make assumptions about the distributions of the test\nstatistic (such as normality) that don\u2019t generally hold in our cases. So in NLP we\nusually use non-parametric tests based on sampling: we arti\ufb01cially create many ver-\nsions of the experimental setup. For example, if we had lots of different test sets x0\nwe could just measure all the d(x0)for all the x0. That gives us a distribution. Now\nwe set a threshold (like .01) and if we see in this distribution that 99% or more of\nthose deltas are smaller than the delta we observed, i.e., that p-value( x)\u2014the proba-\nbility of seeing a d(x)as big as the one we saw\u2014is less than .01, then we can reject\nthe null hypothesis and agree that d(x)was a suf\ufb01ciently surprising difference and\nAis really a better algorithm than B.\nThere are two common non-parametric tests used in NLP: approximate ran-\ndomization (Noreen, 1989) and the bootstrap test . We will describe bootstrapapproximate\nrandomization\nbelow, showing the paired version of the test, which again is most common in NLP.\nPaired tests are those in which we compare two sets of observations that are aligned: paired\neach observation in one set can be paired with an observation in another. This hap-\npens naturally when we are comparing the performance of two systems on the same\ntest set; we can pair the performance of system Aon an individual observation xi",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 131,
      "token_count": 784,
      "chapter_title": ""
    }
  },
  {
    "content": "sions of the experimental setup. For example, if we had lots of different test sets x0\nwe could just measure all the d(x0)for all the x0. That gives us a distribution. Now\nwe set a threshold (like .01) and if we see in this distribution that 99% or more of\nthose deltas are smaller than the delta we observed, i.e., that p-value( x)\u2014the proba-\nbility of seeing a d(x)as big as the one we saw\u2014is less than .01, then we can reject\nthe null hypothesis and agree that d(x)was a suf\ufb01ciently surprising difference and\nAis really a better algorithm than B.\nThere are two common non-parametric tests used in NLP: approximate ran-\ndomization (Noreen, 1989) and the bootstrap test . We will describe bootstrapapproximate\nrandomization\nbelow, showing the paired version of the test, which again is most common in NLP.\nPaired tests are those in which we compare two sets of observations that are aligned: paired\neach observation in one set can be paired with an observation in another. This hap-\npens naturally when we are comparing the performance of two systems on the same\ntest set; we can pair the performance of system Aon an individual observation xi\nwith the performance of system Bon the same xi.\n4.9.1 The Paired Bootstrap Test\nThebootstrap test (Efron and Tibshirani, 1993) can apply to any metric; from pre- bootstrap test\ncision, recall, or F1 to the BLEU metric used in machine translation. The word\nbootstrapping refers to repeatedly drawing large numbers of samples with replace- bootstrapping\nment (called bootstrap samples ) from an original set. The intuition of the bootstrap\ntest is that we can create many virtual test sets from an observed test set by repeat-\nedly sampling from it. The method only makes the assumption that the sample is\nrepresentative of the population.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 132,
      "token_count": 422,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 80",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 133,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "72 CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\nConsider a tiny text classi\ufb01cation example with a test set xof 10 documents. The\n\ufb01rst row of Fig. 4.8 shows the results of two classi\ufb01ers (A and B) on this test set.\nEach document is labeled by one of the four possibilities (A and B both right, both\nwrong, A right and B wrong, A wrong and B right). A slash through a letter ( \u0013B)\nmeans that that classi\ufb01er got the answer wrong. On the \ufb01rst document both A and\nB get the correct class (AB), while on the second document A got it right but B got\nit wrong (A \u0013B). If we assume for simplicity that our metric is accuracy, A has an\naccuracy of .70 and B of .50, so d(x)is .20.\nNow we create a large number b(perhaps 105) of virtual test sets x(i), each of size\nn=10. Fig. 4.8 shows a couple of examples. To create each virtual test set x(i), we\nrepeatedly ( n=10 times) select a cell from row xwith replacement. For example, to\ncreate the \ufb01rst cell of the \ufb01rst virtual test set x(1), if we happened to randomly select\nthe second cell of the xrow; we would copy the value A \u0013B into our new cell, and\nmove on to create the second cell of x(1), each time sampling (randomly choosing)\nfrom the original xwith replacement.\n1 2 3 4 5 6 7 8 910A% B% d()\nx AB A\u0013\u0013BAB\u0000\u0000AB A\u0013\u0013B\u0000\u0000AB A\u0013\u0013BAB\u0000\u0000A\u0013\u0013BA\u0013\u0013B.70 .50 .20\nx(1)A\u0013\u0013BAB A\u0013\u0013B\u0000\u0000AB\u0000\u0000AB A\u0013\u0013B\u0000\u0000AB AB\u0000\u0000A\u0013\u0013BAB .60 .60 .00\nx(2)A\u0013\u0013BAB\u0000\u0000A\u0013\u0013B\u0000\u0000AB\u0000\u0000AB AB\u0000\u0000AB A\u0013\u0013BAB AB .60 .70-.10\n...\nx(b)\nFigure 4.8 The paired bootstrap test: Examples of bpseudo test sets x(i)being created\nfrom an initial true test set x. Each pseudo test set is created by sampling n=10 times with\nreplacement; thus an individual sample is a single cell, a document with its gold label and\nthe correct or incorrect performance of classi\ufb01ers A and B. Of course real test sets don\u2019t have\nonly 10 examples, and bneeds to be large as well.\nNow that we have the btest sets, providing a sampling distribution, we can do\nstatistics on how often Ahas an accidental advantage. There are various ways to\ncompute this advantage; here we follow the version laid out in Berg-Kirkpatrick\net al. (2012). Assuming H0(Aisn\u2019t better than B), we would expect that d(X),\nestimated over many test sets, would be zero or negative; a much higher value would\nbe surprising, since H0speci\ufb01cally assumes Aisn\u2019t better than B. To measure exactly\nhow surprising our observed d(x)is, we would in other circumstances compute the\np-value by counting over many test sets how often d(x(i))exceeds the expected zero\nvalue by d(x)or more:\np-value (x) =1\nbbX\ni=11\u0010\nd(x(i))\u0000d(x)\u00150\u0011",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 134,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 4.8 The paired bootstrap test: Examples of bpseudo test sets x(i)being created\nfrom an initial true test set x. Each pseudo test set is created by sampling n=10 times with\nreplacement; thus an individual sample is a single cell, a document with its gold label and\nthe correct or incorrect performance of classi\ufb01ers A and B. Of course real test sets don\u2019t have\nonly 10 examples, and bneeds to be large as well.\nNow that we have the btest sets, providing a sampling distribution, we can do\nstatistics on how often Ahas an accidental advantage. There are various ways to\ncompute this advantage; here we follow the version laid out in Berg-Kirkpatrick\net al. (2012). Assuming H0(Aisn\u2019t better than B), we would expect that d(X),\nestimated over many test sets, would be zero or negative; a much higher value would\nbe surprising, since H0speci\ufb01cally assumes Aisn\u2019t better than B. To measure exactly\nhow surprising our observed d(x)is, we would in other circumstances compute the\np-value by counting over many test sets how often d(x(i))exceeds the expected zero\nvalue by d(x)or more:\np-value (x) =1\nbbX\ni=11\u0010\nd(x(i))\u0000d(x)\u00150\u0011\n(We use the notation 1(x)to mean \u201c1 if xis true, and 0 otherwise\u201d.) However,\nalthough it\u2019s generally true that the expected value of d(X)over many test sets,\n(again assuming Aisn\u2019t better than B) is 0, this isn\u2019t true for the bootstrapped test\nsets we created. That\u2019s because we didn\u2019t draw these samples from a distribution\nwith 0 mean; we happened to create them from the original test set x, which happens\nto be biased (by .20) in favor of A. So to measure how surprising is our observed\nd(x), we actually compute the p-value by counting over many test sets how often",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 135,
      "token_count": 440,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 81\n\n4.10 \u2022 A VOIDING HARMS IN CLASSIFICATION 73\nd(x(i))exceeds the expected value of d(x)byd(x)or more:\np-value (x) =1\nbbX\ni=11\u0010\nd(x(i))\u0000d(x)\u0015d(x)\u0011\n=1\nbbX\ni=11\u0010\nd(x(i))\u00152d(x)\u0011\n(4.22)\nSo if for example we have 10,000 test sets x(i)and a threshold of .01, and in only 47\nof the test sets do we \ufb01nd that A is accidentally better d(x(i))\u00152d(x), the resulting\np-value of .0047 is smaller than .01, indicating that the delta we found, d(x)is indeed\nsuf\ufb01ciently surprising and unlikely to have happened by accident, and we can reject\nthe null hypothesis and conclude Ais better than B.\nfunction BOOTSTRAP (test set x,num of samples b)returns p-value (x)\nCalculate d(x)# how much better does algorithm A do than B on x\ns= 0\nfori= 1tobdo\nforj= 1tondo # Draw a bootstrap sample x(i)of size n\nSelect a member of xat random and add it to x(i)\nCalculate d(x(i))# how much better does algorithm A do than B on x(i)\ns s+ 1ifd(x(i))\u00152d(x)\np-value( x)\u0019s\nb# on what % of the b samples did algorithm A beat expectations?\nreturn p-value( x) # if very few did, our observed dis probably not accidental\nFigure 4.9 A version of the paired bootstrap algorithm after Berg-Kirkpatrick et al. (2012).\nThe full algorithm for the bootstrap is shown in Fig. 4.9. It is given a test set x, a\nnumber of samples b, and counts the percentage of the bbootstrap test sets in which\nd(x\u0003(i))>2d(x). This percentage then acts as a one-sided empirical p-value.\n4.10 Avoiding Harms in Classi\ufb01cation\nIt is important to avoid harms that may result from classi\ufb01ers, harms that exist both\nfor naive Bayes classi\ufb01ers and for the other classi\ufb01cation algorithms we introduce\nin later chapters.\nOne class of harms is representational harms (Crawford 2017, Blodgett et al.representational\nharms\n2020), harms caused by a system that demeans a social group, for example by per-\npetuating negative stereotypes about them. For example Kiritchenko and Moham-\nmad (2018) examined the performance of 200 sentiment analysis systems on pairs of\nsentences that were identical except for containing either a common African Amer-\nican \ufb01rst name (like Shaniqua ) or a common European American \ufb01rst name (like\nStephanie ), chosen from the Caliskan et al. (2017) study discussed in Chapter 6.\nThey found that most systems assigned lower sentiment and more negative emotion\nto sentences with African American names, re\ufb02ecting and perpetuating stereotypes\nthat associate African Americans with negative emotions (Popp et al., 2003).\nIn other tasks classi\ufb01ers may lead to both representational harms and other\nharms, such as silencing. For example the important text classi\ufb01cation task of tox-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 136,
      "token_count": 750,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 82\n\n74 CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\nicity detection is the task of detecting hate speech, abuse, harassment, or othertoxicity\ndetection\nkinds of toxic language. While the goal of such classi\ufb01ers is to help reduce soci-\netal harm, toxicity classi\ufb01ers can themselves cause harms. For example, researchers\nhave shown that some widely used toxicity classi\ufb01ers incorrectly \ufb02ag as being toxic\nsentences that are non-toxic but simply mention identities like women (Park et al.,\n2018), blind people (Hutchinson et al., 2020) or gay people (Dixon et al., 2018;\nDias Oliva et al., 2021), or simply use linguistic features characteristic of varieties\nlike African-American Vernacular English (Sap et al. 2019, Davidson et al. 2019).\nSuch false positive errors could lead to the silencing of discourse by or about these\ngroups.\nThese model problems can be caused by biases or other problems in the training\ndata; in general, machine learning systems replicate and even amplify the biases\nin their training data. But these problems can also be caused by the labels (for\nexample due to biases in the human labelers), by the resources used (like lexicons,\nor model components like pretrained embeddings), or even by model architecture\n(like what the model is trained to optimize). While the mitigation of these biases\n(for example by carefully considering the training data sources) is an important area\nof research, we currently don\u2019t have general solutions. For this reason it\u2019s important,\nwhen introducing any NLP model, to study these kinds of factors and make them\nclear. One way to do this is by releasing a model card (Mitchell et al., 2019) for model card\neach version of a model. A model card documents a machine learning model with\ninformation like:\n\u2022 training algorithms and parameters\n\u2022 training data sources, motivation, and preprocessing\n\u2022 evaluation data sources, motivation, and preprocessing\n\u2022 intended use and users\n\u2022 model performance across different demographic or other groups and envi-\nronmental situations\n4.11 Summary\nThis chapter introduced the naive Bayes model for classi\ufb01cation and applied it to\nthetext categorization task of sentiment analysis .\n\u2022 Many language processing tasks can be viewed as tasks of classi\ufb01cation .\n\u2022 Text categorization, in which an entire text is assigned a class from a \ufb01nite set,\nincludes such tasks as sentiment analysis ,spam detection , language identi-\n\ufb01cation, and authorship attribution.\n\u2022 Sentiment analysis classi\ufb01es a text as re\ufb02ecting the positive or negative orien-\ntation ( sentiment ) that a writer expresses toward some object.\n\u2022 Naive Bayes is a generative model that makes the bag-of-words assumption\n(position doesn\u2019t matter) and the conditional independence assumption (words\nare conditionally independent of each other given the class)\n\u2022 Naive Bayes with binarized features seems to work better for many text clas-\nsi\ufb01cation tasks.\n\u2022 Classi\ufb01ers are evaluated based on precision andrecall .\n\u2022 Classi\ufb01ers are trained using distinct training, dev, and test sets, including the\nuse of cross-validation in the training set.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 137,
      "token_count": 707,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 83",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 138,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 75\n\u2022 Statistical signi\ufb01cance tests should be used to determine whether we can be\ncon\ufb01dent that one version of a classi\ufb01er is better than another.\n\u2022 Designers of classi\ufb01ers should carefully consider harms that may be caused\nby the model, including its training data and other components, and report\nmodel characteristics in a model card .\nBibliographical and Historical Notes\nMultinomial naive Bayes text classi\ufb01cation was proposed by Maron (1961) at the\nRAND Corporation for the task of assigning subject categories to journal abstracts.\nHis model introduced most of the features of the modern form presented here, ap-\nproximating the classi\ufb01cation task with one-of categorization, and implementing\nadd-dsmoothing and information-based feature selection.\nThe conditional independence assumptions of naive Bayes and the idea of Bayes-\nian analysis of text seems to have arisen multiple times. The same year as Maron\u2019s\npaper, Minsky (1961) proposed a naive Bayes classi\ufb01er for vision and other arti-\n\ufb01cial intelligence problems, and Bayesian techniques were also applied to the text\nclassi\ufb01cation task of authorship attribution by Mosteller and Wallace (1963). It had\nlong been known that Alexander Hamilton, John Jay, and James Madison wrote\nthe anonymously-published Federalist papers in 1787\u20131788 to persuade New York\nto ratify the United States Constitution. Yet although some of the 85 essays were\nclearly attributable to one author or another, the authorship of 12 were in dispute\nbetween Hamilton and Madison. Mosteller and Wallace (1963) trained a Bayesian\nprobabilistic model of the writing of Hamilton and another model on the writings\nof Madison, then computed the maximum-likelihood author for each of the disputed\nessays. Naive Bayes was \ufb01rst applied to spam detection in Heckerman et al. (1998).\nMetsis et al. (2006), Pang et al. (2002), and Wang and Manning (2012) show\nthat using boolean attributes with multinomial naive Bayes works better than full\ncounts. Binary multinomial naive Bayes is sometimes confused with another variant\nof naive Bayes that also uses a binary representation of whether a term occurs in\na document: Multivariate Bernoulli naive Bayes . The Bernoulli variant instead\nestimates P(wjc)as the fraction of documents that contain a term, and includes a\nprobability for whether a term is notin a document. McCallum and Nigam (1998)\nand Wang and Manning (2012) show that the multivariate Bernoulli variant of naive\nBayes doesn\u2019t work as well as the multinomial algorithm for sentiment or other text\ntasks.\nThere are a variety of sources covering the many kinds of text classi\ufb01cation\ntasks. For sentiment analysis see Pang and Lee (2008), and Liu and Zhang (2012).\nStamatatos (2009) surveys authorship attribute algorithms. On language identi\ufb01ca-\ntion see Jauhiainen et al. (2019); Jaech et al. (2016) is an important early neural\nsystem. The task of newswire indexing was often used as a test case for text classi-\n\ufb01cation algorithms, based on the Reuters-21578 collection of newswire articles.\nSee Manning et al. (2008) and Aggarwal and Zhai (2012) on text classi\ufb01cation;\nclassi\ufb01cation in general is covered in machine learning textbooks (Hastie et al. 2001,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 139,
      "token_count": 778,
      "chapter_title": ""
    }
  },
  {
    "content": "a document: Multivariate Bernoulli naive Bayes . The Bernoulli variant instead\nestimates P(wjc)as the fraction of documents that contain a term, and includes a\nprobability for whether a term is notin a document. McCallum and Nigam (1998)\nand Wang and Manning (2012) show that the multivariate Bernoulli variant of naive\nBayes doesn\u2019t work as well as the multinomial algorithm for sentiment or other text\ntasks.\nThere are a variety of sources covering the many kinds of text classi\ufb01cation\ntasks. For sentiment analysis see Pang and Lee (2008), and Liu and Zhang (2012).\nStamatatos (2009) surveys authorship attribute algorithms. On language identi\ufb01ca-\ntion see Jauhiainen et al. (2019); Jaech et al. (2016) is an important early neural\nsystem. The task of newswire indexing was often used as a test case for text classi-\n\ufb01cation algorithms, based on the Reuters-21578 collection of newswire articles.\nSee Manning et al. (2008) and Aggarwal and Zhai (2012) on text classi\ufb01cation;\nclassi\ufb01cation in general is covered in machine learning textbooks (Hastie et al. 2001,\nWitten and Frank 2005, Bishop 2006, Murphy 2012).\nNon-parametric methods for computing statistical signi\ufb01cance were used \ufb01rst in\nNLP in the MUC competition (Chinchor et al., 1993), and even earlier in speech\nrecognition (Gillick and Cox 1989, Bisani and Ney 2004). Our description of the\nbootstrap draws on the description in Berg-Kirkpatrick et al. (2012). Recent work\nhas focused on issues including multiple test sets and multiple metrics (S\u00f8gaard et al.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 140,
      "token_count": 403,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 84\n\n76 CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\n2014, Dror et al. 2017).\nFeature selection is a method of removing features that are unlikely to generalize\nwell. Features are generally ranked by how informative they are about the classi\ufb01ca-\ntion decision. A very common metric, information gain , tells us how many bits ofinformation\ngain\ninformation the presence of the word gives us for guessing the class. Other feature\nselection metrics include c2, pointwise mutual information, and GINI index; see\nYang and Pedersen (1997) for a comparison and Guyon and Elisseeff (2003) for an\nintroduction to feature selection.\nExercises\n4.1 Assume the following likelihoods for each word being part of a positive or\nnegative movie review, and equal prior probabilities for each class.\npos neg\nI 0.09 0.16\nalways 0.07 0.06\nlike 0.29 0.06\nforeign 0.04 0.15\n\ufb01lms 0.08 0.11\nWhat class will Naive bayes assign to the sentence \u201cI always like foreign\n\ufb01lms.\u201d?\n4.2 Given the following short movie reviews, each labeled with a genre, either\ncomedy or action:\n1. fun, couple, love, love comedy\n2. fast, furious, shoot action\n3. couple, \ufb02y, fast, fun, fun comedy\n4. furious, shoot, shoot, fun action\n5. \ufb02y, fast, shoot, love action\nand a new document D:\nfast, couple, shoot, \ufb02y\ncompute the most likely class for D. Assume a naive Bayes classi\ufb01er and use\nadd-1 smoothing for the likelihoods.\n4.3 Train two models, multinomial naive Bayes and binarized naive Bayes, both\nwith add-1 smoothing, on the following document counts for key sentiment\nwords, with positive or negative class assigned as noted.\ndoc \u201cgood\u201d \u201cpoor\u201d \u201cgreat\u201d (class)\nd1. 3 0 3 pos\nd2. 0 1 2 pos\nd3. 1 3 0 neg\nd4. 1 5 2 neg\nd5. 0 2 0 neg\nUse both naive Bayes models to assign a class (pos or neg) to this sentence:\nA good, good plot and great characters, but poor acting.\nRecall from page 61 that with naive Bayes text classi\ufb01cation, we simply ig-\nnore (throw out) any word that never occurred in the training document. (We\ndon\u2019t throw out words that appear in some classes but not others; that\u2019s what\nadd-one smoothing is for.) Do the two models agree or disagree?",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 141,
      "token_count": 620,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 85\n\nCHAPTER\n5Logistic Regression\n\u201cAnd how do you know that these \ufb01ne begonias are not of equal importance?\u201d\nHercule Poirot, in Agatha Christie\u2019s The Mysterious Affair at Styles\nDetective stories are as littered with clues as texts are with words. Yet for the\npoor reader it can be challenging to know how to weigh the author\u2019s clues in order\nto make the crucial classi\ufb01cation task: deciding whodunnit.\nIn this chapter we introduce an algorithm that is admirably suited for discovering\nthe link between features or clues and some particular outcome: logistic regression .logistic\nregression\nIndeed, logistic regression is one of the most important analytic tools in the social\nand natural sciences. In natural language processing, logistic regression is the base-\nline supervised machine learning algorithm for classi\ufb01cation, and also has a very\nclose relationship with neural networks. As we will see in Chapter 7, a neural net-\nwork can be viewed as a series of logistic regression classi\ufb01ers stacked on top of\neach other. Thus the classi\ufb01cation and machine learning techniques introduced here\nwill play an important role throughout the book.\nLogistic regression can be used to classify an observation into one of two classes\n(like \u2018positive sentiment\u2019 and \u2018negative sentiment\u2019), or into one of many classes.\nBecause the mathematics for the two-class case is simpler, we\u2019ll describe this special\ncase of logistic regression \ufb01rst in the next few sections, and then brie\ufb02y summarize\nthe use of multinomial logistic regression for more than two classes in Section 5.3.\nWe\u2019ll introduce the mathematics of logistic regression in the next few sections.\nBut let\u2019s begin with some high-level issues.\nGenerative and Discriminative Classi\ufb01ers: The most important difference be-\ntween naive Bayes and logistic regression is that logistic regression is a discrimina-\ntiveclassi\ufb01er while naive Bayes is a generative classi\ufb01er.\nThese are two very different frameworks for how\nto build a machine learning model. Consider a visual\nmetaphor: imagine we\u2019re trying to distinguish dog\nimages from cat images. A generative model would\nhave the goal of understanding what dogs look like\nand what cats look like. You might literally ask such\na model to \u2018generate\u2019, i.e., draw, a dog. Given a test\nimage, the system then asks whether it\u2019s the cat model or the dog model that better\n\ufb01ts (is less surprised by) the image, and chooses that as its label.\nA discriminative model, by contrast, is only try-\ning to learn to distinguish the classes (perhaps with-\nout learning much about them). So maybe all the\ndogs in the training data are wearing collars and the\ncats aren\u2019t. If that one feature neatly separates the\nclasses, the model is satis\ufb01ed. If you ask such a\nmodel what it knows about cats all it can say is that\nthey don\u2019t wear collars.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 142,
      "token_count": 646,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 86\n\n78 CHAPTER 5 \u2022 L OGISTIC REGRESSION\nMore formally, recall that the naive Bayes assigns a class cto a document dnot\nby directly computing P(cjd)but by computing a likelihood and a prior\n\u02c6c=argmax\nc2Clikelihoodz}|{\nP(djc)priorz}|{\nP(c) (5.1)\nAgenerative model like naive Bayes makes use of this likelihood term, whichgenerative\nmodel\nexpresses how to generate the features of a document if we knew it was of class c .\nBy contrast a discriminative model in this text categorization scenario attemptsdiscriminative\nmodel\ntodirectly compute P(cjd). Perhaps it will learn to assign a high weight to document\nfeatures that directly improve its ability to discriminate between possible classes,\neven if it couldn\u2019t generate an example of one of the classes.\nComponents of a probabilistic machine learning classi\ufb01er: Like naive Bayes,\nlogistic regression is a probabilistic classi\ufb01er that makes use of supervised machine\nlearning. Machine learning classi\ufb01ers require a training corpus of minput/output\npairs(x(i);y(i)). (We\u2019ll use superscripts in parentheses to refer to individual instances\nin the training set\u2014for sentiment classi\ufb01cation each instance might be an individual\ndocument to be classi\ufb01ed.) A machine learning system for classi\ufb01cation then has\nfour components:\n1. A feature representation of the input. For each input observation x(i), this\nwill be a vector of features [x1;x2;:::;xn]. We will generally refer to feature\nifor input x(j)asx(j)\ni, sometimes simpli\ufb01ed as xi, but we will also see the\nnotation fi,fi(x), or, for multiclass classi\ufb01cation, fi(c;x).\n2. A classi\ufb01cation function that computes \u02c6 y, the estimated class, via p(yjx). In\nthe next section we will introduce the sigmoid andsoftmax tools for classi\ufb01-\ncation.\n3. An objective function that we want to optimize for learning, usually involving\nminimizing a loss function corresponding to error on training examples. We\nwill introduce the cross-entropy loss function .\n4. An algorithm for optimizing the objective function. We introduce the stochas-\ntic gradient descent algorithm.\nLogistic regression has two phases:\ntraining: We train the system (speci\ufb01cally the weights wandb, introduced be-\nlow) using stochastic gradient descent and the cross-entropy loss.\ntest: Given a test example xwe compute p(yjx)and return the higher probability\nlabel y=1 ory=0.\n5.1 The sigmoid function\nThe goal of binary logistic regression is to train a classi\ufb01er that can make a binary\ndecision about the class of a new input observation. Here we introduce the sigmoid\nclassi\ufb01er that will help us make this decision.\nConsider a single input observation x, which we will represent by a vector of\nfeatures [x1;x2;:::;xn]. (We\u2019ll show sample features in the next subsection.) The\nclassi\ufb01er output ycan be 1 (meaning the observation is a member of the class) or\n0 (the observation is not a member of the class). We want to know the probability",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 143,
      "token_count": 716,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 87\n\n5.1 \u2022 T HE SIGMOID FUNCTION 79\nP(y=1jx)that this observation is a member of the class. So perhaps the decision\nis \u201cpositive sentiment\u201d versus \u201cnegative sentiment\u201d, the features represent counts of\nwords in a document, P(y=1jx)is the probability that the document has positive\nsentiment, and P(y=0jx)is the probability that the document has negative senti-\nment.\nLogistic regression solves this task by learning, from a training set, a vector of\nweights and a bias term . Each weight wiis a real number, and is associated with one\nof the input features xi. The weight wirepresents how important that input feature\nis to the classi\ufb01cation decision, and can be positive (providing evidence that the in-\nstance being classi\ufb01ed belongs in the positive class) or negative (providing evidence\nthat the instance being classi\ufb01ed belongs in the negative class). Thus we might\nexpect in a sentiment task the word awesome to have a high positive weight, and\nabysmal to have a very negative weight. The bias term , also called the intercept , is bias term\nintercept another real number that\u2019s added to the weighted inputs.\nTo make a decision on a test instance\u2014after we\u2019ve learned the weights in training\u2014\nthe classi\ufb01er \ufb01rst multiplies each xiby its weight wi, sums up the weighted features,\nand adds the bias term b. The resulting single number zexpresses the weighted sum\nof the evidence for the class.\nz= nX\ni=1wixi!\n+b (5.2)\nIn the rest of the book we\u2019ll represent such sums using the dot product notation dot product\nfrom linear algebra. The dot product of two vectors aandb, written as a\u0001b, is the\nsum of the products of the corresponding elements of each vector. (Notice that we\nrepresent vectors using the boldface notation b). Thus the following is an equivalent\nformation to Eq. 5.2:\nz=w\u0001x+b (5.3)\nBut note that nothing in Eq. 5.3 forces zto be a legal probability, that is, to lie\nbetween 0 and 1. In fact, since weights are real-valued, the output might even be\nnegative; zranges from\u0000\u00a5to\u00a5.\nFigure 5.1 The sigmoid function s(z) =1\n1+e\u0000ztakes a real value and maps it to the range\n(0;1). It is nearly linear around 0 but outlier values get squashed toward 0 or 1.\nTo create a probability, we\u2019ll pass zthrough the sigmoid function, s(z). The sigmoid\nsigmoid function (named because it looks like an s) is also called the logistic func-\ntion, and gives logistic regression its name. The sigmoid has the following equation,logistic\nfunction\nshown graphically in Fig. 5.1:\ns(z) =1\n1+e\u0000z=1\n1+exp(\u0000z)(5.4)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 144,
      "token_count": 661,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 88\n\n80 CHAPTER 5 \u2022 L OGISTIC REGRESSION\n(For the rest of the book, we\u2019ll use the notation exp (x)to mean ex.) The sigmoid\nhas a number of advantages; it takes a real-valued number and maps it into the range\n(0;1), which is just what we want for a probability. Because it is nearly linear around\n0 but \ufb02attens toward the ends, it tends to squash outlier values toward 0 or 1. And\nit\u2019s differentiable, which as we\u2019ll see in Section 5.10 will be handy for learning.\nWe\u2019re almost there. If we apply the sigmoid to the sum of the weighted features,\nwe get a number between 0 and 1. To make it a probability, we just need to make\nsure that the two cases, p(y=1)andp(y=0), sum to 1. We can do this as follows:\nP(y=1) = s(w\u0001x+b)\n=1\n1+exp(\u0000(w\u0001x+b))\nP(y=0) = 1\u0000s(w\u0001x+b)\n=1\u00001\n1+exp(\u0000(w\u0001x+b))\n=exp(\u0000(w\u0001x+b))\n1+exp(\u0000(w\u0001x+b))(5.5)\nThe sigmoid function has the property\n1\u0000s(x) =s(\u0000x) (5.6)\nso we could also have expressed P(y=0)ass(\u0000(w\u0001x+b)).\nFinally, one terminological point. The input to the sigmoid function, the score\nz=w\u0001x+bfrom Eq. 5.3, is often called the logit . This is because the logit function logit\nis the inverse of the sigmoid. The logit function is the log of the odds ratiop\n1\u0000p:\nlogit(p) =s\u00001(p) =lnp\n1\u0000p(5.7)\nUsing the term logit forzis a way of reminding us that by using the sigmoid to turn\nz(which ranges from \u0000\u00a5to\u00a5) into a probability, we are implicitly interpreting zas\nnot just any real-valued number, but as speci\ufb01cally a log odds.\n5.2 Classi\ufb01cation with Logistic Regression\nThe sigmoid function from the prior section thus gives us a way to take an instance\nxand compute the probability P(y=1jx).\nHow do we make a decision about which class to apply to a test instance x? For\na given x, we say yes if the probability P(y=1jx)is more than .5, and no otherwise.\nWe call .5 the decision boundary :decision\nboundary\ndecision (x) =\u001a1 if P(y=1jx)>0:5\n0 otherwise\nLet\u2019s have some examples of applying logistic regression as a classi\ufb01er for language\ntasks.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 145,
      "token_count": 623,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 89\n\n5.2 \u2022 C LASSIFICATION WITH LOGISTIC REGRESSION 81\n5.2.1 Sentiment Classi\ufb01cation\nSuppose we are doing binary sentiment classi\ufb01cation on movie review text, and\nwe would like to know whether to assign the sentiment class +or\u0000to a review\ndocument doc. We\u2019ll represent each input observation by the 6 features x1:::x6of\nthe input shown in the following table; Fig. 5.2 shows the features in a sample mini\ntest document.\nVar De\ufb01nition Value in Fig. 5.2\nx1 count(positive lexicon words 2doc) 3\nx2 count(negative lexicon words 2doc) 2\nx3\u001a1 if \u201cno\u201d2doc\n0 otherwise1\nx4 count (1st and 2nd pronouns 2doc) 3\nx5\u001a1 if \u201c!\u201d2doc\n0 otherwise0\nx6 ln(word count of doc ) ln(66) =4:19\n It's hokey . There are virtually no surprises , and the writing is second-rate . So why was it so enjoyable  ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing .  It sucked me in , and it'll do the same to you  .x1=3x6=4.19x3=1x4=3x5=0x2=2\nFigure 5.2 A sample mini test document showing the extracted features in the vector x.\nLet\u2019s assume for the moment that we\u2019ve already learned a real-valued weight\nfor each of these features, and that the 6 weights corresponding to the 6 features\nare[2:5;\u00005:0;\u00001:2;0:5;2:0;0:7], while b= 0.1. (We\u2019ll discuss in the next section\nhow the weights are learned.) The weight w1, for example indicates how important\na feature the number of positive lexicon words ( great ,nice,enjoyable , etc.) is to\na positive sentiment decision, while w2tells us the importance of negative lexicon\nwords. Note that w1=2:5 is positive, while w2=\u00005:0, meaning that negative words\nare negatively associated with a positive sentiment decision, and are about twice as\nimportant as positive words.\nGiven these 6 features and the input review x,P(+jx)andP(\u0000jx)can be com-\nputed using Eq. 5.5:\np(+jx) =P(y=1jx) = s(w\u0001x+b)\n=s([2:5;\u00005:0;\u00001:2;0:5;2:0;0:7]\u0001[3;2;1;3;0;4:19]+0:1)\n=s(:833)\n=0:70 (5.8)\np(\u0000jx) =P(y=0jx) = 1\u0000s(w\u0001x+b)\n=0:30",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 146,
      "token_count": 676,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 90\n\n82 CHAPTER 5 \u2022 L OGISTIC REGRESSION\n5.2.2 Other classi\ufb01cation tasks and features\nLogistic regression is applied to all sorts of NLP tasks, and any property of the input\ncan be a feature. Consider the task of period disambiguation : deciding if a periodperiod\ndisambiguation\nis the end of a sentence or part of a word, by classifying each period into one of two\nclasses, EOS (end-of-sentence) and not-EOS. We might use features like x1below\nexpressing that the current word is lower case, perhaps with a positive weight. Or a\nfeature expressing that the current word is in our abbreviations dictionary (\u201cProf.\u201d),\nperhaps with a negative weight. A feature can also express a combination of proper-\nties. For example a period following an upper case word is likely to be an EOS, but\nif the word itself is St.and the previous word is capitalized then the period is likely\npart of a shortening of the word street following a street name.\nx1=\u001a\n1 if \u201c Case(wi) =Lower\u201d\n0 otherwise\nx2=\u001a1 if \u201c wi2AcronymDict\u201d\n0 otherwise\nx3=\u001a1 if \u201c wi=St. & Case(wi\u00001) =Upper\u201d\n0 otherwise\nDesigning versus learning features: In classic models, features are designed by\nhand by examining the training set with an eye to linguistic intuitions and literature,\nsupplemented by insights from error analysis on the training set of an early version\nof a system. We can also consider ( feature interactions ), complex features that arefeature\ninteractions\ncombinations of more primitive features. We saw such a feature for period disam-\nbiguation above, where a period on the word St.was less likely to be the end of the\nsentence if the previous word was capitalized. Features can be created automatically\nviafeature templates , abstract speci\ufb01cations of features. For example a bigramfeature\ntemplates\ntemplate for period disambiguation might create a feature for every pair of words\nthat occurs before a period in the training set. Thus the feature space is sparse, since\nwe only have to create a feature if that n-gram exists in that position in the training\nset. The feature is generally created as a hash from the string descriptions. A user\ndescription of a feature as, \u201cbigram(American breakfast)\u201d is hashed into a unique\ninteger ithat becomes the feature number fi.\nIt should be clear from the prior paragraph that designing features by hand re-\nquires extensive human effort. For this reason, recent NLP systems avoid hand-\ndesigned features and instead focus on representation learning : ways to learn fea-\ntures automatically in an unsupervised way from the input. We\u2019ll introduce methods\nfor representation learning in Chapter 6 and Chapter 7.\nScaling input features: When different input features have extremely different\nranges of values, it\u2019s common to rescale them so they have comparable ranges. We\nstandardize input values by centering them to result in a zero mean and a standard standardize\ndeviation of one (this transformation is sometimes called the z-score ). That is, if miz-score\nis the mean of the values of feature xiacross the mobservations in the input dataset,\nandsiis the standard deviation of the values of features xiacross the input dataset,\nwe can replace each feature xiby a new feature x0\nicomputed as follows:\nmi=1\nmmX\nj=1x(j)\ni si=vuut1\nmmX\nj=1\u0010\nx(j)\ni\u0000mi\u00112\nx0\ni=xi\u0000mi\nsi(5.9)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 147,
      "token_count": 787,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 91\n\n5.2 \u2022 C LASSIFICATION WITH LOGISTIC REGRESSION 83\nAlternatively, we can normalize the input features values to lie between 0 and 1: normalize\nx0\ni=xi\u0000min(xi)\nmax(xi)\u0000min(xi)(5.10)\nHaving input data with comparable range is useful when comparing values across\nfeatures. Data scaling is especially important in large neural networks, since it helps\nspeed up gradient descent.\n5.2.3 Processing many examples at once\nWe\u2019ve shown the equations for logistic regression for a single example. But in prac-\ntice we\u2019ll of course want to process an entire test set with many examples. Let\u2019s\nsuppose we have a test set consisting of mtest examples each of which we\u2019d like\nto classify. We\u2019ll continue to use the notation from page 78, in which a superscript\nvalue in parentheses refers to the example index in some set of data (either for train-\ning or for test). So in this case each test example x(i)has a feature vector x(i),\n1\u0014i\u0014m. (As usual, we\u2019ll represent vectors and matrices in bold.)\nOne way to compute each output value \u02c6 y(i)is just to have a for-loop, and compute\neach test example one at a time:\nforeach x(i)in input [x(1);x(2);:::;x(m)]\ny(i)=s(w\u0001x(i)+b) (5.11)\nFor the \ufb01rst 3 test examples, then, we would be separately computing the pre-\ndicted \u02c6 y(i)as follows:\nP(y(1)=1jx(1)) = s(w\u0001x(1)+b)\nP(y(2)=1jx(2)) = s(w\u0001x(2)+b)\nP(y(3)=1jx(3)) = s(w\u0001x(3)+b)\nBut it turns out that we can slightly modify our original equation Eq. 5.5 to do\nthis much more ef\ufb01ciently. We\u2019ll use matrix arithmetic to assign a class to all the\nexamples with one matrix operation!\nFirst, we\u2019ll pack all the input feature vectors for each input xinto a single input\nmatrix X, where each row iis a row vector consisting of the feature vector for in-\nput example x(i)(i.e., the vector x(i)). Assuming each example has ffeatures and\nweights, Xwill therefore be a matrix of shape [m\u0002f], as follows:\nX=2\n66664x(1)\n1x(1)\n2:::x(1)\nf\nx(2)\n1x(2)\n2:::x(2)\nf\nx(3)\n1x(3)\n2:::x(3)\nf\n:::3\n77775(5.12)\nNow if we introduce bas a vector of length mwhich consists of the scalar bias\nterm brepeated mtimes, b= [b;b;:::;b], and ^ y= [\u02c6y(1);\u02c6y(2):::;\u02c6y(m)]as the vector of\noutputs (one scalar \u02c6 y(i)for each input x(i)and its feature vector x(i)), and represent\nthe weight vector was a column vector, we can compute all the outputs with a single\nmatrix multiplication and one addition:\ny=Xw+b (5.13)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 148,
      "token_count": 725,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 92\n\n84 CHAPTER 5 \u2022 L OGISTIC REGRESSION\nYou should convince yourself that Eq. 5.13 computes the same thing as our for-loop\nin Eq. 5.11. For example \u02c6 y(1), the \ufb01rst entry of the output vector y, will correctly be:\n\u02c6y(1)= [x(1)\n1;x(1)\n2;:::;x(1)\nf]\u0001[w1;w2;:::;wf]+b (5.14)\nNote that we had to reorder Xandwfrom the order they appeared in in Eq. 5.5 to\nmake the multiplications come out properly. Here is Eq. 5.13 again with the shapes\nshown:\ny=X w +b\n(m\u00021) ( m\u0002f)(f\u00021) (m\u00021) (5.15)\nModern compilers and compute hardware can compute this matrix operation very\nef\ufb01ciently, making the computation much faster, which becomes important when\ntraining or testing on very large datasets.\nNote by the way that we could have kept Xandwin the original order ( y=\nXw+b) if we had chosen to de\ufb01ne Xdifferently as a matrix of column vectors, one\nvector for each input example, instead of row vectors, and then it would have shape\n[f\u0002m]. But we conventionally represent inputs as rows.\n5.2.4 Choosing a classi\ufb01er\nLogistic regression has a number of advantages over naive Bayes. Naive Bayes has\noverly strong conditional independence assumptions. Consider two features which\nare strongly correlated; in fact, imagine that we just add the same feature f1twice.\nNaive Bayes will treat both copies of f1as if they were separate, multiplying them\nboth in, overestimating the evidence. By contrast, logistic regression is much more\nrobust to correlated features; if two features f1and f2are perfectly correlated, re-\ngression will simply assign part of the weight to w1and part to w2. Thus when\nthere are many correlated features, logistic regression will assign a more accurate\nprobability than naive Bayes. So logistic regression generally works better on larger\ndocuments or datasets and is a common default.\nDespite the less accurate probabilities, naive Bayes still often makes the correct\nclassi\ufb01cation decision. Furthermore, naive Bayes can work extremely well (some-\ntimes even better than logistic regression) on very small datasets (Ng and Jordan,\n2002) or short documents (Wang and Manning, 2012). Furthermore, naive Bayes is\neasy to implement and very fast to train (there\u2019s no optimization step). So it\u2019s still a\nreasonable approach to use in some situations.\n5.3 Multinomial logistic regression\nSometimes we need more than two classes. Perhaps we might want to do 3-way\nsentiment classi\ufb01cation (positive, negative, or neutral). Or we could be assigning\nsome of the labels we will introduce in Chapter 17, like the part of speech of a word\n(choosing from 10, 30, or even 50 different parts of speech), or the named entity\ntype of a phrase (choosing from tags like person, location, organization).\nIn such cases we use multinomial logistic regression , also called softmax re-multinomial\nlogistic\nregressiongression (in older NLP literature you will sometimes see the name maxent classi-\n\ufb01er). In multinomial logistic regression we want to label each observation with a\nclass kfrom a set of Kclasses, under the stipulation that only one of these classes is",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 149,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 93\n\n5.3 \u2022 M ULTINOMIAL LOGISTIC REGRESSION 85\nthe correct one (sometimes called hard classi\ufb01cation ; an observation can not be in\nmultiple classes). Let\u2019s use the following representation: the output yfor each input\nxwill be a vector of length K. If class cis the correct class, we\u2019ll set yc=1, and\nset all the other elements of yto be 0, i.e., yc=1 and yj=08j6=c. A vector like\nthisy, with one value=1 and the rest 0, is called a one-hot vector . The job of the\nclassi\ufb01er is to produce an estimate vector ^ y. For each class k, the value \u02c6 ykwill be\nthe classi\ufb01er\u2019s estimate of the probability p(yk=1jx).\n5.3.1 Softmax\nThe multinomial logistic classi\ufb01er uses a generalization of the sigmoid, called the\nsoftmax function, to compute p(yk=1jx). The softmax function takes a vector softmax\nz= [z1;z2;:::;zK]ofKarbitrary values and maps them to a probability distribution,\nwith each value in the range [0,1], and all the values summing to 1. Like the sigmoid,\nit is an exponential function.\nFor a vector zof dimensionality K, the softmax is de\ufb01ned as:\nsoftmax (zi) =exp(zi)PK\nj=1exp(zj)1\u0014i\u0014K (5.16)\nThe softmax of an input vector z= [z1;z2;:::;zK]is thus a vector itself:\nsoftmax (z) =\"\nexp(z1)PK\ni=1exp(zi);exp(z2)PK\ni=1exp(zi);:::;exp(zK)PK\ni=1exp(zi)#\n(5.17)\nThe denominatorPK\ni=1exp(zi)is used to normalize all the values into probabilities.\nThus for example given a vector:\nz= [0:6;1:1;\u00001:5;1:2;3:2;\u00001:1]\nthe resulting (rounded) softmax( z) is\n[0:05;0:09;0:01;0:1;0:74;0:01]\nLike the sigmoid, the softmax has the property of squashing values toward 0 or 1.\nThus if one of the inputs is larger than the others, it will tend to push its probability\ntoward 1, and suppress the probabilities of the smaller inputs.\nFinally, note that, just as for the sigmoid, we refer to z, the vector of scores that\nis the input to the softmax, as logits (see Eq. 5.7).\n5.3.2 Applying softmax in logistic regression\nWhen we apply softmax for logistic regression, the input will (just as for the sig-\nmoid) be the dot product between a weight vector wand an input vector x(plus a\nbias). But now we\u2019ll need separate weight vectors wkand bias bkfor each of the K\nclasses. The probability of each of our output classes \u02c6 ykcan thus be computed as:\np(yk=1jx) =exp(wk\u0001x+bk)\nKX\nj=1exp(wj\u0001x+bj)(5.18)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 150,
      "token_count": 732,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 94\n\n86 CHAPTER 5 \u2022 L OGISTIC REGRESSION\nThe form of Eq. 5.18 makes it seem that we would compute each output sep-\narately. Instead, it\u2019s more common to set up the equation for more ef\ufb01cient com-\nputation by modern vector processing hardware. We\u2019ll do this by representing the\nset of Kweight vectors as a weight matrix Wand a bias vector b. Each row kof\nWcorresponds to the vector of weights wk.Wthus has shape [K\u0002f], for Kthe\nnumber of output classes and fthe number of input features. The bias vector bhas\none value for each of the Koutput classes. If we represent the weights in this way,\nwe can compute \u02c6y, the vector of output probabilities for each of the Kclasses, by a\nsingle elegant equation:\n\u02c6y=softmax (Wx+b) (5.19)\nIf you work out the matrix arithmetic, you can see that the estimated score of\nthe \ufb01rst output class \u02c6 y1(before we take the softmax) will correctly turn out to be\nw1\u0001x+b1.\nOne helpful interpretation of the weight matrix Wis to see each row wkas a\nprototype of class k. The weight vector wkthat is learned represents the class as prototype\na kind of template. Since two vectors that are more similar to each other have a\nhigher dot product with each other, the dot product acts as a similarity function.\nLogistic regression is thus learning an exemplar representation for each class, such\nthat incoming vectors are assigned the class kthey are most similar to from the K\nclasses.\nFig. 5.3 shows the difference between binary and multinomial logistic regression\nby illustrating the weight vector versus weight matrix in the computation of the\noutput class probabilities.\n5.3.3 Features in Multinomial Logistic Regression\nFeatures in multinomial logistic regression act like features in binary logistic regres-\nsion, with the difference mentioned above that we\u2019ll need separate weight vectors\nand biases for each of the Kclasses. Recall our binary exclamation point feature x5\nfrom page 81:\nx5=\u001a\n1 if \u201c!\u201d2doc\n0 otherwise\nIn binary classi\ufb01cation a positive weight w5on a feature in\ufb02uences the classi\ufb01er\ntoward y=1 (positive sentiment) and a negative weight in\ufb02uences it toward y=0\n(negative sentiment) with the absolute value indicating how important the feature\nis. For multinomial logistic regression, by contrast, with separate weights for each\nclass, a feature can be evidence for or against each individual class.\nIn 3-way multiclass sentiment classi\ufb01cation, for example, we must assign each\ndocument one of the 3 classes +,\u0000, or 0 (neutral). Now a feature related to excla-\nmation marks might have a negative weight for 0 documents, and a positive weight\nfor+or\u0000documents:\nFeature De\ufb01nition w5;+w5;\u0000w5;0\nf5(x)\u001a1 if \u201c!\u201d2doc\n0 otherwise3:5 3:1\u00005:3\nBecause these feature weights are dependent both on the input text and the output\nclass, we sometimes make this dependence explicit and represent the features them-\nselves as f(x;y): a function of both the input and the class. Using such a notation",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 151,
      "token_count": 727,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 95\n\n5.4 \u2022 L EARNING IN LOGISTIC REGRESSION 87\nBinary Logistic Regression\nw[f \u2a091]Outputsigmoid[1\u2a09f]Input wordsp(+) = 1- p(-)\u2026y^xyInput featurevector [scalar]positive lexiconwords = 1count of \u201cno\u201d = 0wordcount=3x1x2x3xfdessert   was    greatWeight vector\nMultinomial Logistic Regression\nW[f\u2a091]Outputsoftmax[K\u2a09f]Input wordsp(+)\u2026y1^y2^y3^xyInput featurevector [K\u2a091]positive lexiconwords = 1count of \u201cno\u201d = 0wordcount=3x1x2x3xfdessert   was    greatp(-)p(neut)Weight matrixThese f red weightsare a row of W correspondingto weight vector w3,(= weights for class 3,= a prototype of class 3)\nFigure 5.3 Binary versus multinomial logistic regression. Binary logistic regression uses a\nsingle weight vector w, and has a scalar output \u02c6 y. In multinomial logistic regression we have\nKseparate weight vectors corresponding to the Kclasses, all packed into a single weight\nmatrix W, and a vector output \u02c6y. We omit the biases from both \ufb01gures for clarity.\nf5(x)above could be represented as three features f5(x;+),f5(x;\u0000), and f5(x;0),\neach of which has a single weight. We\u2019ll use this kind of notation in our description\nof the CRF in Chapter 17.\n5.4 Learning in Logistic Regression\nHow are the parameters of the model, the weights wand bias b, learned? Logistic\nregression is an instance of supervised classi\ufb01cation in which we know the correct\nlabel y(either 0 or 1) for each observation x. What the system produces via Eq. 5.5\nis \u02c6y, the system\u2019s estimate of the true y. We want to learn parameters (meaning w\nandb) that make \u02c6 yfor each training observation as close as possible to the true y.\nThis requires two components that we foreshadowed in the introduction to the\nchapter. The \ufb01rst is a metric for how close the current label ( \u02c6 y) is to the true gold",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 152,
      "token_count": 518,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 96\n\n88 CHAPTER 5 \u2022 L OGISTIC REGRESSION\nlabel y. Rather than measure similarity, we usually talk about the opposite of this:\nthedistance between the system output and the gold output, and we call this distance\nthelossfunction or the cost function . In the next section we\u2019ll introduce the loss loss\nfunction that is commonly used for logistic regression and also for neural networks,\nthecross-entropy loss .\nThe second thing we need is an optimization algorithm for iteratively updating\nthe weights so as to minimize this loss function. The standard algorithm for this is\ngradient descent ; we\u2019ll introduce the stochastic gradient descent algorithm in the\nfollowing section.\nWe\u2019ll describe these algorithms for the simpler case of binary logistic regres-\nsion in the next two sections, and then turn to multinomial logistic regression in\nSection 5.8.\n5.5 The cross-entropy loss function\nWe need a loss function that expresses, for an observation x, how close the classi\ufb01er\noutput ( \u02c6 y=s(w\u0001x+b)) is to the correct output ( y, which is 0 or 1). We\u2019ll call this:\nL(\u02c6y;y) = How much \u02c6 ydiffers from the true y (5.20)\nWe do this via a loss function that prefers the correct class labels of the train-\ning examples to be more likely . This is called conditional maximum likelihood\nestimation : we choose the parameters w;bthatmaximize the log probability of\nthe true ylabels in the training data given the observations x. The resulting loss\nfunction is the negative log likelihood loss , generally called the cross-entropy loss .cross-entropy\nloss\nLet\u2019s derive this loss function, applied to a single observation x. We\u2019d like to\nlearn weights that maximize the probability of the correct label p(yjx). Since there\nare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we can\nexpress the probability p(yjx)that our classi\ufb01er produces for one observation as the\nfollowing (keeping in mind that if y=1, Eq. 5.21 simpli\ufb01es to \u02c6 y; ify=0, Eq. 5.21\nsimpli\ufb01es to 1\u0000\u02c6y):\np(yjx) = \u02c6yy(1\u0000\u02c6y)1\u0000y(5.21)\nNow we take the log of both sides. This will turn out to be handy mathematically,\nand doesn\u2019t hurt us; whatever values maximize a probability will also maximize the\nlog of the probability:\nlogp(yjx) = log\u0002\n\u02c6yy(1\u0000\u02c6y)1\u0000y\u0003\n=ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y) (5.22)\nEq. 5.22 describes a log likelihood that should be maximized. In order to turn this\ninto a loss function (something that we need to minimize), we\u2019ll just \ufb02ip the sign on\nEq. 5.22. The result is the cross-entropy loss LCE:\nLCE(\u02c6y;y) =\u0000logp(yjx) =\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)] (5.23)\nFinally, we can plug in the de\ufb01nition of \u02c6 y=s(w\u0001x+b):\nLCE(\u02c6y;y) =\u0000[ylogs(w\u0001x+b)+(1\u0000y)log(1\u0000s(w\u0001x+b))] (5.24)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 153,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 97",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 154,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "5.6 \u2022 G RADIENT DESCENT 89\nLet\u2019s see if this loss function does the right thing for our example from Fig. 5.2. We\nwant the loss to be smaller if the model\u2019s estimate is close to correct, and bigger if\nthe model is confused. So \ufb01rst let\u2019s suppose the correct gold label for the sentiment\nexample in Fig. 5.2 is positive, i.e., y=1. In this case our model is doing well, since\nfrom Eq. 5.8 it indeed gave the example a higher probability of being positive (.70)\nthan negative (.30). If we plug s(w\u0001x+b) =:70 and y=1 into Eq. 5.24, the right\nside of the equation drops out, leading to the following loss (we\u2019ll use log to mean\nnatural log when the base is not speci\ufb01ed):\nLCE(\u02c6y;y) =\u0000[ylogs(w\u0001x+b)+(1\u0000y)log(1\u0000s(w\u0001x+b))]\n=\u0000[logs(w\u0001x+b)]\n=\u0000log(:70)\n= :36\nBy contrast, let\u2019s pretend instead that the example in Fig. 5.2 was actually negative,\ni.e., y=0 (perhaps the reviewer went on to say \u201cBut bottom line, the movie is\nterrible! I beg you not to see it!\u201d). In this case our model is confused and we\u2019d want\nthe loss to be higher. Now if we plug y=0 and 1\u0000s(w\u0001x+b) =:30 from Eq. 5.8\ninto Eq. 5.24, the left side of the equation drops out:\nLCE(\u02c6y;y) =\u0000[ylogs(w\u0001x+b)+(1\u0000y)log(1\u0000s(w\u0001x+b))]\n= \u0000[log(1\u0000s(w\u0001x+b))]\n= \u0000log(:30)\n= 1:2\nSure enough, the loss for the \ufb01rst classi\ufb01er (.36) is less than the loss for the second\nclassi\ufb01er (1.2).\nWhy does minimizing this negative log probability do what we want? A perfect\nclassi\ufb01er would assign probability 1 to the correct outcome ( y=1 or y=0) and\nprobability 0 to the incorrect outcome. That means if yequals 1, the higher \u02c6 yis (the\ncloser it is to 1), the better the classi\ufb01er; the lower \u02c6 yis (the closer it is to 0), the\nworse the classi\ufb01er. If yequals 0, instead, the higher 1 \u0000\u02c6yis (closer to 1), the better\nthe classi\ufb01er. The negative log of \u02c6 y(if the true yequals 1) or 1\u0000\u02c6y(if the true y\nequals 0) is a convenient loss metric since it goes from 0 (negative log of 1, no loss)\nto in\ufb01nity (negative log of 0, in\ufb01nite loss). This loss function also ensures that as\nthe probability of the correct answer is maximized, the probability of the incorrect\nanswer is minimized; since the two sum to one, any increase in the probability of the\ncorrect answer is coming at the expense of the incorrect answer. It\u2019s called the cross-\nentropy loss, because Eq. 5.22 is also the formula for the cross-entropy between the\ntrue probability distribution yand our estimated distribution \u02c6 y.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 155,
      "token_count": 770,
      "chapter_title": ""
    }
  },
  {
    "content": "probability 0 to the incorrect outcome. That means if yequals 1, the higher \u02c6 yis (the\ncloser it is to 1), the better the classi\ufb01er; the lower \u02c6 yis (the closer it is to 0), the\nworse the classi\ufb01er. If yequals 0, instead, the higher 1 \u0000\u02c6yis (closer to 1), the better\nthe classi\ufb01er. The negative log of \u02c6 y(if the true yequals 1) or 1\u0000\u02c6y(if the true y\nequals 0) is a convenient loss metric since it goes from 0 (negative log of 1, no loss)\nto in\ufb01nity (negative log of 0, in\ufb01nite loss). This loss function also ensures that as\nthe probability of the correct answer is maximized, the probability of the incorrect\nanswer is minimized; since the two sum to one, any increase in the probability of the\ncorrect answer is coming at the expense of the incorrect answer. It\u2019s called the cross-\nentropy loss, because Eq. 5.22 is also the formula for the cross-entropy between the\ntrue probability distribution yand our estimated distribution \u02c6 y.\nNow we know what we want to minimize; in the next section, we\u2019ll see how to\n\ufb01nd the minimum.\n5.6 Gradient Descent\nOur goal with gradient descent is to \ufb01nd the optimal weights: minimize the loss\nfunction we\u2019ve de\ufb01ned for the model. In Eq. 5.25 below, we\u2019ll explicitly represent\nthe fact that the cross-entropy loss function LCEis parameterized by the weights. In",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 156,
      "token_count": 368,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 98\n\n90 CHAPTER 5 \u2022 L OGISTIC REGRESSION\nmachine learning in general we refer to the parameters being learned as q; in the\ncase of logistic regression q=fw;bg. So the goal is to \ufb01nd the set of weights which\nminimizes the loss function, averaged over all examples:\n\u02c6q=argmin\nq1\nmmX\ni=1LCE(f(x(i);q);y(i)) (5.25)\nHow shall we \ufb01nd the minimum of this (or any) loss function? Gradient descent is\na method that \ufb01nds a minimum of a function by \ufb01guring out in which direction (in\nthe space of the parameters q) the function\u2019s slope is rising the most steeply, and\nmoving in the opposite direction. The intuition is that if you are hiking in a canyon\nand trying to descend most quickly down to the river at the bottom, you might look\naround yourself in all directions, \ufb01nd the direction where the ground is sloping the\nsteepest, and walk downhill in that direction.\nFor logistic regression, this loss function is conveniently convex . A convex func- convex\ntion has at most one minimum; there are no local minima to get stuck in, so gradient\ndescent starting from any point is guaranteed to \ufb01nd the minimum. (By contrast,\nthe loss for multi-layer neural networks is non-convex, and gradient descent may\nget stuck in local minima for neural network training and never \ufb01nd the global opti-\nmum.)\nAlthough the algorithm (and the concept of gradient) are designed for direction\nvectors , let\u2019s \ufb01rst consider a visualization of the case where the parameter of our\nsystem is just a single scalar w, shown in Fig. 5.4.\nGiven a random initialization of wat some value w1, and assuming the loss\nfunction Lhappened to have the shape in Fig. 5.4, we need the algorithm to tell us\nwhether at the next iteration we should move left (making w2smaller than w1) or\nright (making w2bigger than w1) to reach the minimum.\nwLoss\n0w1wminslope of loss at w1 is negative(goal)one stepof gradientdescent\nFigure 5.4 The \ufb01rst step in iteratively \ufb01nding the minimum of this loss function, by moving\nwin the reverse direction from the slope of the function. Since the slope is negative, we need\nto move win a positive direction, to the right. Here superscripts are used for learning steps,\nsow1means the initial value of w(which is 0), w2the value at the second step, and so on.\nThe gradient descent algorithm answers this question by \ufb01nding the gradient gradient\nof the loss function at the current point and moving in the opposite direction. The\ngradient of a function of many variables is a vector pointing in the direction of the\ngreatest increase in a function. The gradient is a multi-variable generalization of the",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 157,
      "token_count": 645,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 99\n\n5.6 \u2022 G RADIENT DESCENT 91\nslope, so for a function of one variable like the one in Fig. 5.4, we can informally\nthink of the gradient as the slope. The dotted line in Fig. 5.4 shows the slope of this\nhypothetical loss function at point w=w1. You can see that the slope of this dotted\nline is negative. Thus to \ufb01nd the minimum, gradient descent tells us to go in the\nopposite direction: moving win a positive direction.\nThe magnitude of the amount to move in gradient descent is the value of the\nsloped\ndwL(f(x;w);y)weighted by a learning rate h. A higher (faster) learning learning rate\nrate means that we should move wmore on each step. The change we make in our\nparameter is the learning rate times the gradient (or the slope, in our single-variable\nexample):\nwt+1=wt\u0000hd\ndwL(f(x;w);y) (5.26)\nNow let\u2019s extend the intuition from a function of one scalar variable wto many\nvariables, because we don\u2019t just want to move left or right, we want to know where\nin the N-dimensional space (of the Nparameters that make up q) we should move.\nThe gradient is just such a vector; it expresses the directional components of the\nsharpest slope along each of those Ndimensions. If we\u2019re just imagining two weight\ndimensions (say for one weight wand one bias b), the gradient might be a vector with\ntwo orthogonal components, each of which tells us how much the ground slopes in\nthewdimension and in the bdimension. Fig. 5.5 shows a visualization of the value\nof a 2-dimensional gradient vector taken at the red point.\nIn an actual logistic regression, the parameter vector wis much longer than 1 or\n2, since the input feature vector xcan be quite long, and we need a weight wifor\neach xi. For each dimension/variable wiinw(plus the bias b), the gradient will have\na component that tells us the slope with respect to that variable. In each dimension\nwi, we express the slope as a partial derivative\u00b6\n\u00b6wiof the loss function. Essentially\nwe\u2019re asking: \u201cHow much would a small change in that variable wiin\ufb02uence the\ntotal loss function L?\u201d\nFormally, then, the gradient of a multi-variable function fis a vector in which\neach component expresses the partial derivative of fwith respect to one of the vari-\nables. We\u2019ll use the inverted Greek delta symbol \u00d1to refer to the gradient, and\nCost(w,b)\nwb\nFigure 5.5 Visualization of the gradient vector at the red point in two dimensions wand\nb, showing a red arrow in the x-y plane pointing in the direction we will go to look for the\nminimum: the opposite direction of the gradient (recall that the gradient points in the direction\nof increase not decrease).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 158,
      "token_count": 639,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 100\n\n92 CHAPTER 5 \u2022 L OGISTIC REGRESSION\nrepresent \u02c6 yasf(x;q)to make the dependence on qmore obvious:\n\u00d1L(f(x;q);y) =2\n6666664\u00b6\n\u00b6w1L(f(x;q);y)\n\u00b6\n\u00b6w2L(f(x;q);y)\n...\n\u00b6\n\u00b6wnL(f(x;q);y)\n\u00b6\n\u00b6bL(f(x;q);y)3\n7777775(5.27)\nThe \ufb01nal equation for updating qbased on the gradient is thus\nqt+1=qt\u0000h\u00d1L(f(x;q);y) (5.28)\n5.6.1 The Gradient for Logistic Regression\nIn order to update q, we need a de\ufb01nition for the gradient \u00d1L(f(x;q);y). Recall that\nfor logistic regression, the cross-entropy loss function is:\nLCE(\u02c6y;y) =\u0000[ylogs(w\u0001x+b)+(1\u0000y)log(1\u0000s(w\u0001x+b))] (5.29)\nIt turns out that the derivative of this function for one observation vector xis Eq. 5.30\n(the interested reader can see Section 5.10 for the derivation of this equation):\n\u00b6LCE(\u02c6y;y)\n\u00b6wj= [s(w\u0001x+b)\u0000y]xj\n= ( \u02c6y\u0000y)xj (5.30)\nYou\u2019ll also sometimes see this equation in the equivalent form:\n\u00b6LCE(\u02c6y;y)\n\u00b6wj=\u0000(y\u0000\u02c6y)xj (5.31)\nNote in these equations that the gradient with respect to a single weight wjrep-\nresents a very intuitive value: the difference between the true yand our estimated\n\u02c6y=s(w\u0001x+b)for that observation, multiplied by the corresponding input value\nxj.\n5.6.2 The Stochastic Gradient Descent Algorithm\nStochastic gradient descent is an online algorithm that minimizes the loss function\nby computing its gradient after each training example, and nudging qin the right\ndirection (the opposite direction of the gradient). (An \u201conline algorithm\u201d is one that\nprocesses its input example by example, rather than waiting until it sees the entire\ninput.) Stochastic gradient descent is called stochastic because it chooses a single\nrandom example at a time; in Section 5.6.4 we\u2019ll discuss other versions of gradient\ndescent that batch many examples at once. Fig. 5.6 shows the algorithm.\nThe learning rate his ahyperparameter that must be adjusted. If it\u2019s too high, hyperparameter\nthe learner will take steps that are too large, overshooting the minimum of the loss\nfunction. If it\u2019s too low, the learner will take steps that are too small, and take too\nlong to get to the minimum. It is common to start with a higher learning rate and then\nslowly decrease it, so that it is a function of the iteration kof training; the notation\nhkcan be used to mean the value of the learning rate at iteration k.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 159,
      "token_count": 658,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 101\n\n5.6 \u2022 G RADIENT DESCENT 93\nfunction STOCHASTIC GRADIENT DESCENT (L(),f(),x,y)returns q\n# where: L is the loss function\n# f is a function parameterized by q\n# x is the set of training inputs x(1);x(2);:::;x(m)\n# y is the set of training outputs (labels) y(1);y(2);:::;y(m)\nq 0 # (or small random values)\nrepeat til done # see caption\nFor each training tuple (x(i);y(i))(in random order)\n1. Optional (for reporting): # How are we doing on this tuple?\nCompute \u02c6 y(i)=f(x(i);q)# What is our estimated output \u02c6 y?\nCompute the loss L(\u02c6y(i);y(i))# How far off is \u02c6 y(i)from the true output y(i)?\n2.g \u00d1qL(f(x(i);q);y(i)) # How should we move qto maximize loss?\n3.q q\u0000hg # Go the other way instead\nreturn q\nFigure 5.6 The stochastic gradient descent algorithm. Step 1 (computing the loss) is used\nmainly to report how well we are doing on the current tuple; we don\u2019t need to compute the\nloss in order to compute the gradient. The algorithm can terminate when it converges (when\nthe gradient norm <\u000f), or when progress halts (for example when the loss starts going up on\na held-out set). Weights are initialized to 0 for logistic regression, but to small random values\nfor neural networks, as we\u2019ll see in Chapter 7.\nWe\u2019ll discuss hyperparameters in more detail in Chapter 7, but in short, they are\na special kind of parameter for any machine learning model. Unlike regular param-\neters of a model (weights like wandb), which are learned by the algorithm from\nthe training set, hyperparameters are special parameters chosen by the algorithm\ndesigner that affect how the algorithm works.\n5.6.3 Working through an example\nLet\u2019s walk through a single step of the gradient descent algorithm. We\u2019ll use a\nsimpli\ufb01ed version of the example in Fig. 5.2 as it sees a single observation x, whose\ncorrect value is y=1 (this is a positive review), and with a feature vector x= [x1;x2]\nconsisting of these two features:\nx1=3 (count of positive lexicon words)\nx2=2 (count of negative lexicon words)\nLet\u2019s assume the initial weights and bias in q0are all set to 0, and the initial learning\nratehis 0.1:\nw1=w2=b=0\nh=0:1\nThe single update step requires that we compute the gradient, multiplied by the\nlearning rate\nqt+1=qt\u0000h\u00d1qL(f(x(i);q);y(i))",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 160,
      "token_count": 637,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 102",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 161,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "94 CHAPTER 5 \u2022 L OGISTIC REGRESSION\nIn our mini example there are three parameters, so the gradient vector has 3 dimen-\nsions, for w1,w2, and b. We can compute the \ufb01rst gradient as follows:\n\u00d1w;bL=2\n64\u00b6LCE(\u02c6y;y)\n\u00b6w1\u00b6LCE(\u02c6y;y)\n\u00b6w2\u00b6LCE(\u02c6y;y)\n\u00b6b3\n75=2\n4(s(w\u0001x+b)\u0000y)x1\n(s(w\u0001x+b)\u0000y)x2\ns(w\u0001x+b)\u0000y3\n5=2\n4(s(0)\u00001)x1\n(s(0)\u00001)x2\ns(0)\u000013\n5=2\n4\u00000:5x1\n\u00000:5x2\n\u00000:53\n5=2\n4\u00001:5\n\u00001:0\n\u00000:53\n5\nNow that we have a gradient, we compute the new parameter vector q1by moving\nq0in the opposite direction from the gradient:\nq1=2\n4w1\nw2\nb3\n5\u0000h2\n4\u00001:5\n\u00001:0\n\u00000:53\n5=2\n4:15\n:1\n:053\n5\nSo after one step of gradient descent, the weights have shifted to be: w1=:15,\nw2=:1, and b=:05.\nNote that this observation xhappened to be a positive example. We would expect\nthat after seeing more negative examples with high counts of negative words, that\nthe weight w2would shift to have a negative value.\n5.6.4 Mini-batch training\nStochastic gradient descent is called stochastic because it chooses a single random\nexample at a time, moving the weights so as to improve performance on that single\nexample. That can result in very choppy movements, so it\u2019s common to compute the\ngradient over batches of training instances rather than a single instance.\nFor example in batch training we compute the gradient over the entire dataset. batch training\nBy seeing so many examples, batch training offers a superb estimate of which di-\nrection to move the weights, at the cost of spending a lot of time processing every\nsingle example in the training set to compute this perfect direction.\nA compromise is mini-batch training: we train on a group of mexamples (per- mini-batch\nhaps 512, or 1024) that is less than the whole dataset. (If mis the size of the dataset,\nthen we are doing batch gradient descent; if m=1, we are back to doing stochas-\ntic gradient descent.) Mini-batch training also has the advantage of computational\nef\ufb01ciency. The mini-batches can easily be vectorized, choosing the size of the mini-\nbatch based on the computational resources. This allows us to process all the exam-\nples in one mini-batch in parallel and then accumulate the loss, something that\u2019s not\npossible with individual or batch training.\nWe just need to de\ufb01ne mini-batch versions of the cross-entropy loss function\nwe de\ufb01ned in Section 5.5 and the gradient in Section 5.6.1. Let\u2019s extend the cross-\nentropy loss for one example from Eq. 5.23 to mini-batches of size m. We\u2019ll continue\nto use the notation that x(i)andy(i)mean the ith training features and training label,\nrespectively. We make the assumption that the training examples are independent:\nlogp(training labels ) = logmY",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 162,
      "token_count": 769,
      "chapter_title": ""
    }
  },
  {
    "content": "single example in the training set to compute this perfect direction.\nA compromise is mini-batch training: we train on a group of mexamples (per- mini-batch\nhaps 512, or 1024) that is less than the whole dataset. (If mis the size of the dataset,\nthen we are doing batch gradient descent; if m=1, we are back to doing stochas-\ntic gradient descent.) Mini-batch training also has the advantage of computational\nef\ufb01ciency. The mini-batches can easily be vectorized, choosing the size of the mini-\nbatch based on the computational resources. This allows us to process all the exam-\nples in one mini-batch in parallel and then accumulate the loss, something that\u2019s not\npossible with individual or batch training.\nWe just need to de\ufb01ne mini-batch versions of the cross-entropy loss function\nwe de\ufb01ned in Section 5.5 and the gradient in Section 5.6.1. Let\u2019s extend the cross-\nentropy loss for one example from Eq. 5.23 to mini-batches of size m. We\u2019ll continue\nto use the notation that x(i)andy(i)mean the ith training features and training label,\nrespectively. We make the assumption that the training examples are independent:\nlogp(training labels ) = logmY\ni=1p(y(i)jx(i))\n=mX\ni=1logp(y(i)jx(i))\n=\u0000mX\ni=1LCE(\u02c6y(i);y(i)) (5.32)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 163,
      "token_count": 331,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 103\n\n5.7 \u2022 R EGULARIZATION 95\nNow the cost function for the mini-batch of mexamples is the average loss for each\nexample:\nCost(\u02c6y;y) =1\nmmX\ni=1LCE(\u02c6y(i);y(i))\n=\u00001\nmmX\ni=1y(i)logs(w\u0001x(i)+b)+(1\u0000y(i))log\u0010\n1\u0000s(w\u0001x(i)+b)\u0011\n(5.33)\nThe mini-batch gradient is the average of the individual gradients from Eq. 5.30:\n\u00b6Cost(\u02c6y;y)\n\u00b6wj=1\nmmX\ni=1h\ns(w\u0001x(i)+b)\u0000y(i)i\nx(i)\nj(5.34)\nInstead of using the sum notation, we can more ef\ufb01ciently compute the gradient\nin its matrix form, following the vectorization we saw on page 83, where we have\na matrix Xof size [m\u0002f]representing the minputs in the batch, and a vector yof\nsize[m\u00021]representing the correct outputs:\n\u00b6Cost(\u02c6y;y)\n\u00b6w=1\nm(\u02c6y\u0000y)|X\n=1\nm(s(Xw+b)\u0000y)|X (5.35)\n5.7 Regularization\nNumquam ponenda est pluralitas sine necessitate\n\u2018Plurality should never be proposed unless needed\u2019\nWilliam of Occam\nThere is a problem with learning weights that make the model perfectly match the\ntraining data. If a feature is perfectly predictive of the outcome because it happens\nto only occur in one class, it will be assigned a very high weight. The weights for\nfeatures will attempt to perfectly \ufb01t details of the training set, in fact too perfectly,\nmodeling noisy factors that just accidentally correlate with the class. This problem is\ncalled over\ufb01tting . A good model should be able to generalize well from the training over\ufb01tting\ngeneralize data to the unseen test set, but a model that over\ufb01ts will have poor generalization.\nTo avoid over\ufb01tting, a new regularization term R(q)is added to the loss func- regularization\ntion in Eq. 5.25, resulting in the following loss for a batch of mexamples (slightly\nrewritten from Eq. 5.25 to be maximizing log probability rather than minimizing\nloss, and removing the1\nmterm which doesn\u2019t affect the argmax):\n\u02c6q=argmax\nqmX\ni=1logP(y(i)jx(i))\u0000aR(q) (5.36)\nThe new regularization term R(q)is used to penalize large weights. Thus a setting\nof the weights that matches the training data perfectly\u2014 but uses many weights with",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 164,
      "token_count": 606,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 104\n\n96 CHAPTER 5 \u2022 L OGISTIC REGRESSION\nhigh values to do so\u2014will be penalized more than a setting that matches the data a\nlittle less well, but does so using smaller weights. There are two common ways to\ncompute this regularization term R(q).L2 regularization is a quadratic function ofL2\nregularization\nthe weight values, named because it uses the (square of the) L2 norm of the weight\nvalues. The L2 norm, jjqjj2, is the same as the Euclidean distance of the vector q\nfrom the origin. If qconsists of nweights, then:\nR(q) =jjqjj2\n2=nX\nj=1q2\nj (5.37)\nThe L2 regularized loss function becomes:\n\u02c6q=argmax\nq\"mX\ni=1logP(y(i)jx(i))#\n\u0000anX\nj=1q2\nj (5.38)\nL1 regularization is a linear function of the weight values, named after the L1 normL1\nregularization\njjWjj1, the sum of the absolute values of the weights, or Manhattan distance (the\nManhattan distance is the distance you\u2019d have to walk between two points in a city\nwith a street grid like New York):\nR(q) =jjqjj1=nX\ni=1jqij (5.39)\nThe L1 regularized loss function becomes:\n\u02c6q=argmax\nq\"mX\ni=1logP(y(i)jx(i))#\n\u0000anX\nj=1jqjj (5.40)\nThese kinds of regularization come from statistics, where L1 regularization is called\nlasso regression (Tibshirani, 1996) and L2 regularization is called ridge regression , lasso\nridge and both are commonly used in language processing. L2 regularization is easier to\noptimize because of its simple derivative (the derivative of q2is just 2 q), while\nL1 regularization is more complex (the derivative of jqjis non-continuous at zero).\nBut while L2 prefers weight vectors with many small weights, L1 prefers sparse\nsolutions with some larger weights but many more weights set to zero. Thus L1\nregularization leads to much sparser weight vectors, that is, far fewer features.\nBoth L1 and L2 regularization have Bayesian interpretations as constraints on\nthe prior of how weights should look. L1 regularization can be viewed as a Laplace\nprior on the weights. L2 regularization corresponds to assuming that weights are\ndistributed according to a Gaussian distribution with mean m=0. In a Gaussian\nor normal distribution, the further away a value is from the mean, the lower its\nprobability (scaled by the variance s). By using a Gaussian prior on the weights, we\nare saying that weights prefer to have the value 0. A Gaussian for a weight qjis\n1q\n2ps2\njexp \n\u0000(qj\u0000mj)2\n2s2\nj!\n(5.41)\nIf we multiply each weight by a Gaussian prior on the weight, we are thus maximiz-\ning the following constraint:\n\u02c6q=argmax\nqmY\ni=1P(y(i)jx(i))\u0002nY\nj=11q\n2ps2\njexp \n\u0000(qj\u0000mj)2\n2s2\nj!\n(5.42)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 165,
      "token_count": 730,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 105",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 166,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "5.8 \u2022 L EARNING IN MULTINOMIAL LOGISTIC REGRESSION 97\nwhich in log space, with m=0, and assuming 2 s2=1, corresponds to\n\u02c6q=argmax\nqmX\ni=1logP(y(i)jx(i))\u0000anX\nj=1q2\nj (5.43)\nwhich is in the same form as Eq. 5.38.\n5.8 Learning in Multinomial Logistic Regression\nThe loss function for multinomial logistic regression generalizes the loss function\nfor binary logistic regression from 2 to Kclasses. Recall that that the cross-entropy\nloss for binary logistic regression (repeated from Eq. 5.23) is:\nLCE(\u02c6y;y) =\u0000logp(yjx) =\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)] (5.44)\nThe loss function for multinomial logistic regression generalizes the two terms in\nEq. 5.44 (one that is non-zero when y=1 and one that is non-zero when y=0) to\nKterms. As we mentioned above, for multinomial regression we\u2019ll represent both y\nand\u02c6yas vectors. The true label yis a vector with Kelements, each corresponding\nto a class, with yc=1 if the correct class is c, with all other elements of ybeing 0.\nAnd our classi\ufb01er will produce an estimate vector with Kelements \u02c6y, each element\n\u02c6ykof which represents the estimated probability p(yk=1jx).\nThe loss function for a single example x, generalizing from binary logistic re-\ngression, is the sum of the logs of the Koutput classes, each weighted by the indi-\ncator function yk(Eq. 5.45). This turns out to be just the negative log probability of\nthe correct class c(Eq. 5.46):\nLCE(\u02c6y;y) =\u0000KX\nk=1yklog \u02c6yk (5.45)\n=\u0000log \u02c6yc;(where cis the correct class) (5.46)\n=\u0000log \u02c6p(yc=1jx)(where cis the correct class)\n=\u0000logexp(wc\u0001x+bc)PK\nj=1exp(wj\u0001x+bj)(cis the correct class) (5.47)\nHow did we get from Eq. 5.45 to Eq. 5.46? Because only one class (let\u2019s call it c) is\nthe correct one, the vector ytakes the value 1 only for this value of k, i.e., has yc=1\nandyj=08j6=c. That means the terms in the sum in Eq. 5.45 will all be 0 except\nfor the term corresponding to the true class c. Hence the cross-entropy loss is simply\nthe log of the output probability corresponding to the correct class, and we therefore\nalso call Eq. 5.46 the negative log likelihood loss .negative log\nlikelihood loss\nOf course for gradient descent we don\u2019t need the loss, we need its gradient. The\ngradient for a single example turns out to be very similar to the gradient for binary\nlogistic regression, (\u02c6y\u0000y)x, that we saw in Eq. 5.30. Let\u2019s consider one piece of the\ngradient, the derivative for a single weight. For each class k, the weight of the ith\nelement of input xiswk;i. What is the partial derivative of the loss with respect to",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 167,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "=\u0000logexp(wc\u0001x+bc)PK\nj=1exp(wj\u0001x+bj)(cis the correct class) (5.47)\nHow did we get from Eq. 5.45 to Eq. 5.46? Because only one class (let\u2019s call it c) is\nthe correct one, the vector ytakes the value 1 only for this value of k, i.e., has yc=1\nandyj=08j6=c. That means the terms in the sum in Eq. 5.45 will all be 0 except\nfor the term corresponding to the true class c. Hence the cross-entropy loss is simply\nthe log of the output probability corresponding to the correct class, and we therefore\nalso call Eq. 5.46 the negative log likelihood loss .negative log\nlikelihood loss\nOf course for gradient descent we don\u2019t need the loss, we need its gradient. The\ngradient for a single example turns out to be very similar to the gradient for binary\nlogistic regression, (\u02c6y\u0000y)x, that we saw in Eq. 5.30. Let\u2019s consider one piece of the\ngradient, the derivative for a single weight. For each class k, the weight of the ith\nelement of input xiswk;i. What is the partial derivative of the loss with respect to\nwk;i? This derivative turns out to be just the difference between the true value for the\nclass k(which is either 1 or 0) and the probability the classi\ufb01er outputs for class k,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 168,
      "token_count": 328,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 106\n\n98 CHAPTER 5 \u2022 L OGISTIC REGRESSION\nweighted by the value of the input xicorresponding to the ith element of the weight\nvector for class k:\n\u00b6LCE\n\u00b6wk;i=\u0000(yk\u0000\u02c6yk)xi\n=\u0000(yk\u0000p(yk=1jx))xi\n=\u0000 \nyk\u0000exp(wk\u0001x+bk)PK\nj=1exp(wj\u0001x+bj)!\nxi (5.48)\nWe\u2019ll return to this case of the gradient for softmax regression when we introduce\nneural networks in Chapter 7, and at that time we\u2019ll also discuss the derivation of\nthis gradient in equations Eq. 7.33\u2013Eq. 7.41.\n5.9 Interpreting models\nOften we want to know more than just the correct classi\ufb01cation of an observation.\nWe want to know why the classi\ufb01er made the decision it did. That is, we want our\ndecision to be interpretable . Interpretability can be hard to de\ufb01ne strictly, but the interpretable\ncore idea is that as humans we should know why our algorithms reach the conclu-\nsions they do. Because the features to logistic regression are often human-designed,\none way to understand a classi\ufb01er\u2019s decision is to understand the role each feature\nplays in the decision. Logistic regression can be combined with statistical tests (the\nlikelihood ratio test, or the Wald test); investigating whether a particular feature is\nsigni\ufb01cant by one of these tests, or inspecting its magnitude (how large is the weight\nwassociated with the feature?) can help us interpret why the classi\ufb01er made the\ndecision it makes. This is enormously important for building transparent models.\nFurthermore, in addition to its use as a classi\ufb01er, logistic regression in NLP and\nmany other \ufb01elds is widely used as an analytic tool for testing hypotheses about the\neffect of various explanatory variables (features). In text classi\ufb01cation, perhaps we\nwant to know if logically negative words ( no, not, never ) are more likely to be asso-\nciated with negative sentiment, or if negative reviews of movies are more likely to\ndiscuss the cinematography. However, in doing so it\u2019s necessary to control for po-\ntential confounds: other factors that might in\ufb02uence sentiment (the movie genre, the\nyear it was made, perhaps the length of the review in words). Or we might be study-\ning the relationship between NLP-extracted linguistic features and non-linguistic\noutcomes (hospital readmissions, political outcomes, or product sales), but need to\ncontrol for confounds (the age of the patient, the county of voting, the brand of the\nproduct). In such cases, logistic regression allows us to test whether some feature is\nassociated with some outcome above and beyond the effect of other features.\n5.10 Advanced: Deriving the Gradient Equation\nIn this section we give the derivation of the gradient of the cross-entropy loss func-\ntion LCEfor logistic regression. Let\u2019s start with some quick calculus refreshers.\nFirst, the derivative of ln (x):\nd\ndxln(x) =1\nx(5.49)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 169,
      "token_count": 692,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 107\n\n5.11 \u2022 S UMMARY 99\nSecond, the (very elegant) derivative of the sigmoid:\nds(z)\ndz=s(z)(1\u0000s(z)) (5.50)\nFinally, the chain rule of derivatives. Suppose we are computing the derivative chain rule\nof a composite function f(x) =u(v(x)). The derivative of f(x)is the derivative of\nu(x)with respect to v(x)times the derivative of v(x)with respect to x:\nd f\ndx=du\ndv\u0001dv\ndx(5.51)\nFirst, we want to know the derivative of the loss function with respect to a single\nweight wj(we\u2019ll need to compute it for each weight, and for the bias):\n\u00b6LCE\n\u00b6wj=\u00b6\n\u00b6wj\u0000[ylogs(w\u0001x+b)+(1\u0000y)log(1\u0000s(w\u0001x+b))]\n=\u0000\u0014\u00b6\n\u00b6wjylogs(w\u0001x+b)+\u00b6\n\u00b6wj(1\u0000y)log[1\u0000s(w\u0001x+b)]\u0015\n(5.52)\nNext, using the chain rule, and relying on the derivative of log:\n\u00b6LCE\n\u00b6wj=\u0000y\ns(w\u0001x+b)\u00b6\n\u00b6wjs(w\u0001x+b)\u00001\u0000y\n1\u0000s(w\u0001x+b)\u00b6\n\u00b6wj1\u0000s(w\u0001x+b)\n(5.53)\nRearranging terms:\n\u00b6LCE\n\u00b6wj=\u0000\u0014y\ns(w\u0001x+b)\u00001\u0000y\n1\u0000s(w\u0001x+b)\u0015\u00b6\n\u00b6wjs(w\u0001x+b)\n(5.54)\nAnd now plugging in the derivative of the sigmoid, and using the chain rule one\nmore time, we end up with Eq. 5.55:\n\u00b6LCE\n\u00b6wj=\u0000\u0014y\u0000s(w\u0001x+b)\ns(w\u0001x+b)[1\u0000s(w\u0001x+b)]\u0015\ns(w\u0001x+b)[1\u0000s(w\u0001x+b)]\u00b6(w\u0001x+b)\n\u00b6wj\n=\u0000\u0014y\u0000s(w\u0001x+b)\ns(w\u0001x+b)[1\u0000s(w\u0001x+b)]\u0015\ns(w\u0001x+b)[1\u0000s(w\u0001x+b)]xj\n=\u0000[y\u0000s(w\u0001x+b)]xj\n= [s(w\u0001x+b)\u0000y]xj (5.55)\n5.11 Summary\nThis chapter introduced the logistic regression model of classi\ufb01cation .\n\u2022 Logistic regression is a supervised machine learning classi\ufb01er that extracts\nreal-valued features from the input, multiplies each by a weight, sums them,\nand passes the sum through a sigmoid function to generate a probability. A\nthreshold is used to make a decision.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 170,
      "token_count": 598,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 108\n\n100 CHAPTER 5 \u2022 L OGISTIC REGRESSION\n\u2022 Logistic regression can be used with two classes (e.g., positive and negative\nsentiment) or with multiple classes ( multinomial logistic regression , for ex-\nample for n-ary text classi\ufb01cation, part-of-speech labeling, etc.).\n\u2022 Multinomial logistic regression uses the softmax function to compute proba-\nbilities.\n\u2022 The weights (vector wand bias b) are learned from a labeled training set via a\nloss function, such as the cross-entropy loss , that must be minimized.\n\u2022 Minimizing this loss function is a convex optimization problem, and iterative\nalgorithms like gradient descent are used to \ufb01nd the optimal weights.\n\u2022Regularization is used to avoid over\ufb01tting.\n\u2022 Logistic regression is also one of the most useful analytic tools, because of its\nability to transparently study the importance of individual features.\nBibliographical and Historical Notes\nLogistic regression was developed in the \ufb01eld of statistics, where it was used for\nthe analysis of binary data by the 1960s, and was particularly common in medicine\n(Cox, 1969). Starting in the late 1970s it became widely used in linguistics as one\nof the formal foundations of the study of linguistic variation (Sankoff and Labov,\n1979).\nNonetheless, logistic regression didn\u2019t become common in natural language pro-\ncessing until the 1990s, when it seems to have appeared simultaneously from two\ndirections. The \ufb01rst source was the neighboring \ufb01elds of information retrieval and\nspeech processing, both of which had made use of regression, and both of which\nlent many other statistical techniques to NLP. Indeed a very early use of logistic\nregression for document routing was one of the \ufb01rst NLP applications to use (LSI)\nembeddings as word representations (Sch \u00a8utze et al., 1995).\nAt the same time in the early 1990s logistic regression was developed and ap-\nplied to NLP at IBM Research under the name maximum entropy modeling ormaximum\nentropy\nmaxent (Berger et al., 1996), seemingly independent of the statistical literature. Un-\nder that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech\ntagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997), coreference resolution\n(Kehler, 1997b), and text classi\ufb01cation (Nigam et al., 1999).\nMore on classi\ufb01cation can be found in machine learning textbooks (Hastie et al.\n2001, Witten and Frank 2005, Bishop 2006, Murphy 2012).\nExercises",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 171,
      "token_count": 592,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 109\n\nCHAPTER\n6Vector Semantics and\nEmbeddings\n\u8343\u8005\u6240\u4ee5\u5728\u9c7c\uff0c\u5f97\u9c7c\u800c\u5fd8\u8343Nets are for \ufb01sh;\nOnce you get the \ufb01sh, you can forget the net.\n\u8a00\u8005\u6240\u4ee5\u5728\u610f\uff0c\u5f97\u610f\u800c\u5fd8\u8a00Words are for meaning;\nOnce you get the meaning, you can forget the words\n\u5e84\u5b50(Zhuangzi), Chapter 26\nThe asphalt that Los Angeles is famous for occurs mainly on its freeways. But\nin the middle of the city is another patch of asphalt, the La Brea tar pits, and this\nasphalt preserves millions of fossil bones from the last of the Ice Ages of the Pleis-\ntocene Epoch. One of these fossils is the Smilodon , or saber-toothed tiger, instantly\nrecognizable by its long canines. Five million years ago or so, a completely different\nsaber-tooth tiger called Thylacosmilus lived\nin Argentina and other parts of South Amer-\nica. Thylacosmilus was a marsupial whereas\nSmilodon was a placental mammal, but Thy-\nlacosmilus had the same long upper canines\nand, like Smilodon, had a protective bone\n\ufb02ange on the lower jaw. The similarity of\nthese two mammals is one of many examples\nof parallel or convergent evolution, in which particular contexts or environments\nlead to the evolution of very similar structures in different species (Gould, 1980).\nThe role of context is also important in the similarity of a less biological kind\nof organism: the word. Words that occur in similar contexts tend to have similar\nmeanings . This link between similarity in how words are distributed and similarity\nin what they mean is called the distributional hypothesis . The hypothesis wasdistributional\nhypothesis\n\ufb01rst formulated in the 1950s by linguists like Joos (1950), Harris (1954), and Firth\n(1957), who noticed that words which are synonyms (like oculist andeye-doctor )\ntended to occur in the same environment (e.g., near words like eyeorexamined )\nwith the amount of meaning difference between two words \u201ccorresponding roughly\nto the amount of difference in their environments\u201d (Harris, 1954, p. 157).\nIn this chapter we introduce vector semantics , which instantiates this linguisticvector\nsemantics\nhypothesis by learning representations of the meaning of words, called embeddings , embeddings\ndirectly from their distributions in texts. These representations are used in every nat-\nural language processing application that makes use of meaning, and the static em-\nbeddings we introduce here underlie the more powerful dynamic or contextualized\nembeddings likeBERT that we will see in Chapter 11.\nThese word representations are also the \ufb01rst example in this book of repre-\nsentation learning , automatically learning useful representations of the input text.representation\nlearning\nFinding such self-supervised ways to learn representations of the input, instead of\ncreating representations by hand via feature engineering , is an important focus of\nNLP research (Bengio et al., 2013).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 172,
      "token_count": 676,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 110",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 173,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "102 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\n6.1 Lexical Semantics\nLet\u2019s begin by introducing some basic principles of word meaning. How should\nwe represent the meaning of a word? In the n-gram models of Chapter 3, and in\nclassical NLP applications, our only representation of a word is as a string of letters,\nor an index in a vocabulary list. This representation is not that different from a\ntradition in philosophy, perhaps you\u2019ve seen it in introductory logic classes, in which\nthe meaning of words is represented by just spelling the word with small capital\nletters; representing the meaning of \u201cdog\u201d as DOG, and \u201ccat\u201d as CAT, or by using an\napostrophe ( DOG \u2019).\nRepresenting the meaning of a word by capitalizing it is a pretty unsatisfactory\nmodel. You might have seen a version of a joke due originally to semanticist Barbara\nPartee (Carlson, 1977):\nQ: What\u2019s the meaning of life?\nA:LIFE \u2019\nSurely we can do better than this! After all, we\u2019ll want a model of word meaning\nto do all sorts of things for us. It should tell us that some words have similar mean-\nings ( catis similar to dog), others are antonyms ( cold is the opposite of hot), some\nhave positive connotations ( happy ) while others have negative connotations ( sad). It\nshould represent the fact that the meanings of buy,sell, and payoffer differing per-\nspectives on the same underlying purchasing event. (If I buy something from you,\nyou\u2019ve probably sold it to me, and I likely paid you.) More generally, a model of\nword meaning should allow us to draw inferences to address meaning-related tasks\nlike question-answering or dialogue.\nIn this section we summarize some of these desiderata, drawing on results in the\nlinguistic study of word meaning, which is called lexical semantics ; we\u2019ll return tolexical\nsemantics\nand expand on this list in Appendix G and Chapter 21.\nLemmas and Senses Let\u2019s start by looking at how one word (we\u2019ll choose mouse )\nmight be de\ufb01ned in a dictionary (simpli\ufb01ed from the online dictionary WordNet):\nmouse (N)\n1. any of numerous small rodents...\n2. a hand-operated device that controls a cursor...\nHere the form mouse is the lemma , also called the citation form . The form lemma\ncitation form mouse would also be the lemma for the word mice ; dictionaries don\u2019t have separate\nde\ufb01nitions for in\ufb02ected forms like mice . Similarly sing is the lemma for sing,sang ,\nsung . In many languages the in\ufb01nitive form is used as the lemma for the verb, so\nSpanish dormir \u201cto sleep\u201d is the lemma for duermes \u201cyou sleep\u201d. The speci\ufb01c forms\nsung orcarpets orsing orduermes are called wordforms . wordform\nAs the example above shows, each lemma can have multiple meanings; the\nlemma mouse can refer to the rodent or the cursor control device. We call each\nof these aspects of the meaning of mouse aword sense . The fact that lemmas can\nbepolysemous (have multiple senses) can make interpretation dif\ufb01cult (is someone\nwho types \u201cmouse info\u201d into a search engine looking for a pet or a tool?). Chap-\nter 11 and Appendix G will discuss the problem of polysemy, and introduce word\nsense disambiguation , the task of determining which sense of a word is being used\nin a particular context.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 174,
      "token_count": 774,
      "chapter_title": ""
    }
  },
  {
    "content": "mouse (N)\n1. any of numerous small rodents...\n2. a hand-operated device that controls a cursor...\nHere the form mouse is the lemma , also called the citation form . The form lemma\ncitation form mouse would also be the lemma for the word mice ; dictionaries don\u2019t have separate\nde\ufb01nitions for in\ufb02ected forms like mice . Similarly sing is the lemma for sing,sang ,\nsung . In many languages the in\ufb01nitive form is used as the lemma for the verb, so\nSpanish dormir \u201cto sleep\u201d is the lemma for duermes \u201cyou sleep\u201d. The speci\ufb01c forms\nsung orcarpets orsing orduermes are called wordforms . wordform\nAs the example above shows, each lemma can have multiple meanings; the\nlemma mouse can refer to the rodent or the cursor control device. We call each\nof these aspects of the meaning of mouse aword sense . The fact that lemmas can\nbepolysemous (have multiple senses) can make interpretation dif\ufb01cult (is someone\nwho types \u201cmouse info\u201d into a search engine looking for a pet or a tool?). Chap-\nter 11 and Appendix G will discuss the problem of polysemy, and introduce word\nsense disambiguation , the task of determining which sense of a word is being used\nin a particular context.\nSynonymy One important component of word meaning is the relationship be-\ntween word senses. For example when one word has a sense whose meaning is",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 175,
      "token_count": 321,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 111",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 176,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "6.1 \u2022 L EXICAL SEMANTICS 103\nidentical to a sense of another word, or nearly identical, we say the two senses of\nthose two words are synonyms . Synonyms include such pairs as synonym\ncouch/sofa vomit/throw up \ufb01lbert/hazelnut car/automobile\nA more formal de\ufb01nition of synonymy (between words rather than senses) is that\ntwo words are synonymous if they are substitutable for one another in any sentence\nwithout changing the truth conditions of the sentence, the situations in which the\nsentence would be true.\nWhile substitutions between some pairs of words like car/automobile orwa-\nter/H2Oare truth preserving, the words are still not identical in meaning. Indeed,\nprobably no two words are absolutely identical in meaning. One of the fundamental\ntenets of semantics, called the principle of contrast (Girard 1718, Br \u00b4eal 1897, Clarkprinciple of\ncontrast\n1987), states that a difference in linguistic form is always associated with some dif-\nference in meaning. For example, the word H2Ois used in scienti\ufb01c contexts and\nwould be inappropriate in a hiking guide\u2014 water would be more appropriate\u2014 and\nthis genre difference is part of the meaning of the word. In practice, the word syn-\nonym is therefore used to describe a relationship of approximate or rough synonymy.\nWord Similarity While words don\u2019t have many synonyms, most words do have\nlots of similar words. Catis not a synonym of dog, but cats anddogs are certainly\nsimilar words. In moving from synonymy to similarity, it will be useful to shift from\ntalking about relations between word senses (like synonymy) to relations between\nwords (like similarity). Dealing with words avoids having to commit to a particular\nrepresentation of word senses, which will turn out to simplify our task.\nThe notion of word similarity is very useful in larger semantic tasks. Knowing similarity\nhow similar two words are can help in computing how similar the meaning of two\nphrases or sentences are, a very important component of tasks like question answer-\ning, paraphrasing, and summarization. One way of getting values for word similarity\nis to ask humans to judge how similar one word is to another. A number of datasets\nhave resulted from such experiments. For example the SimLex-999 dataset (Hill\net al., 2015) gives values on a scale from 0 to 10, like the examples below, which\nrange from near-synonyms ( vanish ,disappear ) to pairs that scarcely seem to have\nanything in common ( hole,agreement ):\nvanish disappear 9.8\nbelief impression 5.95\nmuscle bone 3.65\nmodest \ufb02exible 0.98\nhole agreement 0.3\nWord Relatedness The meaning of two words can be related in ways other than\nsimilarity. One such class of connections is called word relatedness (Budanitsky relatedness\nand Hirst, 2006), also traditionally called word association in psychology. association\nConsider the meanings of the words coffee andcup. Coffee is not similar to cup;\nthey share practically no features (coffee is a plant or a beverage, while a cup is a\nmanufactured object with a particular shape). But coffee and cup are clearly related;\nthey are associated by co-participating in an everyday event (the event of drinking\ncoffee out of a cup). Similarly scalpel andsurgeon are not similar but are related\neventively (a surgeon tends to make use of a scalpel).\nOne common kind of relatedness between words is if they belong to the same",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 177,
      "token_count": 778,
      "chapter_title": ""
    }
  },
  {
    "content": "have resulted from such experiments. For example the SimLex-999 dataset (Hill\net al., 2015) gives values on a scale from 0 to 10, like the examples below, which\nrange from near-synonyms ( vanish ,disappear ) to pairs that scarcely seem to have\nanything in common ( hole,agreement ):\nvanish disappear 9.8\nbelief impression 5.95\nmuscle bone 3.65\nmodest \ufb02exible 0.98\nhole agreement 0.3\nWord Relatedness The meaning of two words can be related in ways other than\nsimilarity. One such class of connections is called word relatedness (Budanitsky relatedness\nand Hirst, 2006), also traditionally called word association in psychology. association\nConsider the meanings of the words coffee andcup. Coffee is not similar to cup;\nthey share practically no features (coffee is a plant or a beverage, while a cup is a\nmanufactured object with a particular shape). But coffee and cup are clearly related;\nthey are associated by co-participating in an everyday event (the event of drinking\ncoffee out of a cup). Similarly scalpel andsurgeon are not similar but are related\neventively (a surgeon tends to make use of a scalpel).\nOne common kind of relatedness between words is if they belong to the same\nsemantic \ufb01eld . A semantic \ufb01eld is a set of words which cover a particular semantic semantic \ufb01eld\ndomain and bear structured relations with each other. For example, words might be",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 178,
      "token_count": 332,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 112",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 179,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "104 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\nrelated by being in the semantic \ufb01eld of hospitals ( surgeon ,scalpel ,nurse ,anes-\nthetic ,hospital ), restaurants ( waiter ,menu ,plate ,food,chef), or houses ( door ,roof,\nkitchen ,family ,bed). Semantic \ufb01elds are also related to topic models , like Latent topic models\nDirichlet Allocation ,LDA , which apply unsupervised learning on large sets of texts\nto induce sets of associated words from text. Semantic \ufb01elds and topic models are\nvery useful tools for discovering topical structure in documents.\nIn Appendix G we\u2019ll introduce more relations between senses like hypernymy\norIS-A ,antonymy (opposites) and meronymy (part-whole relations).\nSemantic Frames and Roles Closely related to semantic \ufb01elds is the idea of a\nsemantic frame . A semantic frame is a set of words that denote perspectives or semantic frame\nparticipants in a particular type of event. A commercial transaction, for example,\nis a kind of event in which one entity trades money to another entity in return for\nsome good or service, after which the good changes hands or perhaps the service is\nperformed. This event can be encoded lexically by using verbs like buy(the event\nfrom the perspective of the buyer), sell(from the perspective of the seller), pay\n(focusing on the monetary aspect), or nouns like buyer . Frames have semantic roles\n(like buyer ,seller ,goods ,money ), and words in a sentence can take on these roles.\nKnowing that buyandsellhave this relation makes it possible for a system to\nknow that a sentence like Sam bought the book from Ling could be paraphrased as\nLing sold the book to Sam , and that Sam has the role of the buyer in the frame and\nLing the seller . Being able to recognize such paraphrases is important for question\nanswering, and can help in shifting perspective for machine translation.\nConnotation Finally, words have affective meanings orconnotations . The word connotations\nconnotation has different meanings in different \ufb01elds, but here we use it to mean the\naspects of a word\u2019s meaning that are related to a writer or reader\u2019s emotions, senti-\nment, opinions, or evaluations. For example some words have positive connotations\n(wonderful ) while others have negative connotations ( dreary ). Even words whose\nmeanings are similar in other ways can vary in connotation; consider the difference\nin connotations between fake,knockoff ,forgery , on the one hand, and copy ,replica ,\nreproduction on the other, or innocent (positive connotation) and naive (negative\nconnotation). Some words describe positive evaluation ( great ,love) and others neg-\native evaluation ( terrible ,hate). Positive or negative evaluation language is called\nsentiment , as we saw in Chapter 4, and word sentiment plays a role in important sentiment\ntasks like sentiment analysis, stance detection, and applications of NLP to the lan-\nguage of politics and consumer reviews.\nEarly work on affective meaning (Osgood et al., 1957) found that words varied\nalong three important dimensions of affective meaning:\nvalence: the pleasantness of the stimulus\narousal: the intensity of emotion provoked by the stimulus\ndominance: the degree of control exerted by the stimulus\nThus words like happy orsatis\ufb01ed are high on valence, while unhappy oran-\nnoyed are low on valence. Excited is high on arousal, while calm is low on arousal.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 180,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "ment, opinions, or evaluations. For example some words have positive connotations\n(wonderful ) while others have negative connotations ( dreary ). Even words whose\nmeanings are similar in other ways can vary in connotation; consider the difference\nin connotations between fake,knockoff ,forgery , on the one hand, and copy ,replica ,\nreproduction on the other, or innocent (positive connotation) and naive (negative\nconnotation). Some words describe positive evaluation ( great ,love) and others neg-\native evaluation ( terrible ,hate). Positive or negative evaluation language is called\nsentiment , as we saw in Chapter 4, and word sentiment plays a role in important sentiment\ntasks like sentiment analysis, stance detection, and applications of NLP to the lan-\nguage of politics and consumer reviews.\nEarly work on affective meaning (Osgood et al., 1957) found that words varied\nalong three important dimensions of affective meaning:\nvalence: the pleasantness of the stimulus\narousal: the intensity of emotion provoked by the stimulus\ndominance: the degree of control exerted by the stimulus\nThus words like happy orsatis\ufb01ed are high on valence, while unhappy oran-\nnoyed are low on valence. Excited is high on arousal, while calm is low on arousal.\nControlling is high on dominance, while awed orin\ufb02uenced are low on dominance.\nEach word is thus represented by three numbers, corresponding to its value on each\nof the three dimensions:",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 181,
      "token_count": 325,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 113\n\n6.2 \u2022 V ECTOR SEMANTICS 105\nValence Arousal Dominance\ncourageous 8.05 5.5 7.38\nmusic 7.67 5.57 6.5\nheartbreak 2.45 5.65 3.58\ncub 6.71 3.95 4.24\nOsgood et al. (1957) noticed that in using these 3 numbers to represent the\nmeaning of a word, the model was representing each word as a point in a three-\ndimensional space, a vector whose three dimensions corresponded to the word\u2019s\nrating on the three scales. This revolutionary idea that word meaning could be rep-\nresented as a point in space (e.g., that part of the meaning of heartbreak can be\nrepresented as the point [2:45;5:65;3:58]) was the \ufb01rst expression of the vector se-\nmantics models that we introduce next.\n6.2 Vector Semantics\nVector semantics is the standard way to represent word meaning in NLP, helpingvector\nsemantics\nus model many of the aspects of word meaning we saw in the previous section. The\nroots of the model lie in the 1950s when two big ideas converged: Osgood\u2019s 1957\nidea mentioned above to use a point in three-dimensional space to represent the\nconnotation of a word, and the proposal by linguists like Joos (1950), Harris (1954),\nand Firth (1957) to de\ufb01ne the meaning of a word by its distribution in language\nuse, meaning its neighboring words or grammatical environments. Their idea was\nthat two words that occur in very similar distributions (whose neighboring words are\nsimilar) have similar meanings.\nFor example, suppose you didn\u2019t know the meaning of the word ongchoi (a re-\ncent borrowing from Cantonese) but you see it in the following contexts:\n(6.1) Ongchoi is delicious sauteed with garlic.\n(6.2) Ongchoi is superb over rice.\n(6.3) ...ongchoi leaves with salty sauces...\nAnd suppose that you had seen many of these context words in other contexts:\n(6.4) ...spinach sauteed with garlic over rice...\n(6.5) ...chard stems and leaves are delicious...\n(6.6) ...collard greens and other salty leafy greens\nThe fact that ongchoi occurs with words like riceandgarlic anddelicious and\nsalty , as do words like spinach ,chard , and collard greens might suggest that ongchoi\nis a leafy green similar to these other leafy greens.1We can do the same thing\ncomputationally by just counting words in the context of ongchoi .\nThe idea of vector semantics is to represent a word as a point in a multidimen-\nsional semantic space that is derived (in ways we\u2019ll see) from the distributions of\nword neighbors. Vectors for representing words are called embeddings (although embeddings\nthe term is sometimes more strictly applied only to dense vectors like word2vec\n(Section 6.8), rather than sparse tf-idf or PPMI vectors (Section 6.3-Section 6.6)).\nThe word \u201cembedding\u201d derives from its mathematical sense as a mapping from one\nspace or structure to another, although the meaning has shifted; see the end of the\nchapter.\n1It\u2019s in fact Ipomoea aquatica , a relative of morning glory sometimes called water spinach in English.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 182,
      "token_count": 766,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 114\n\n106 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\ngoodnicebadworstnot good\nwonderfulamazingterri\ufb01cdislikeworsevery goodincredibly goodfantasticincredibly badnowyouithatwithbyto\u2019sareisathan\nFigure 6.1 A two-dimensional (t-SNE) projection of embeddings for some words and\nphrases, showing that words with similar meanings are nearby in space. The original 60-\ndimensional embeddings were trained for sentiment analysis. Simpli\ufb01ed from Li et al. (2015)\nwith colors added for explanation.\nFig. 6.1 shows a visualization of embeddings learned for sentiment analysis,\nshowing the location of selected words projected down from 60-dimensional space\ninto a two dimensional space. Notice the distinct regions containing positive words,\nnegative words, and neutral function words.\nThe \ufb01ne-grained model of word similarity of vector semantics offers enormous\npower to NLP applications. NLP applications like the sentiment classi\ufb01ers of Chap-\nter 4 or Chapter 5 depend on the same words appearing in the training and test sets.\nBut by representing words as embeddings, a classi\ufb01er can assign sentiment as long\nas it sees some words with similar meanings . And as we\u2019ll see, vector semantic\nmodels can be learned automatically from text without supervision.\nIn this chapter we\u2019ll introduce the two most commonly used models. In the tf-idf\nmodel, an important baseline, the meaning of a word is de\ufb01ned by a simple function\nof the counts of nearby words. We will see that this method results in very long\nvectors that are sparse , i.e. mostly zeros (since most words simply never occur in\nthe context of others). We\u2019ll introduce the word2vec model family for construct-\ning short, dense vectors that have useful semantic properties. We\u2019ll also introduce\nthecosine , the standard way to use embeddings to compute semantic similarity , be-\ntween two words, two sentences, or two documents, an important tool in practical\napplications like question answering, summarization, or automatic essay grading.\n6.3 Words and Vectors\n\u201cThe most important attributes of a vector in 3-space are fLocation, Location, Location g\u201d\nRandall Munroe, https://xkcd.com/2358/\nVector or distributional models of meaning are generally based on a co-occurrence\nmatrix , a way of representing how often words co-occur. We\u2019ll look at two popular\nmatrices: the term-document matrix and the term-term matrix.\n6.3.1 Vectors and documents\nIn aterm-document matrix , each row represents a word in the vocabulary and eachterm-document\nmatrix\ncolumn represents a document from some collection of documents. Fig. 6.2 shows a\nsmall selection from a term-document matrix showing the occurrence of four words\nin four plays by Shakespeare. Each cell in this matrix represents the number of times",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 183,
      "token_count": 627,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 115",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 184,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "6.3 \u2022 W ORDS AND VECTORS 107\na particular word (de\ufb01ned by the row) occurs in a particular document (de\ufb01ned by\nthe column). Thus foolappeared 58 times in Twelfth Night .\nAs You Like It Twelfth Night Julius Caesar Henry V\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure 6.2 The term-document matrix for four words in four Shakespeare plays. Each cell\ncontains the number of times the (row) word occurs in the (column) document.\nThe term-document matrix of Fig. 6.2 was \ufb01rst de\ufb01ned as part of the vector\nspace model of information retrieval (Salton, 1971). In this model, a document isvector space\nmodel\nrepresented as a count vector, a column in Fig. 6.3.\nTo review some basic linear algebra, a vector is, at heart, just a list or array of vector\nnumbers. So As You Like It is represented as the list [1,114,36,20] (the \ufb01rst column\nvector in Fig. 6.3) and Julius Caesar is represented as the list [7,62,1,2] (the third\ncolumn vector). A vector space is a collection of vectors, and is characterized by vector space\nitsdimension . Vectors in a 3-dimensional vector space have an element for each dimension\ndimension of the space. We will loosely refer to a vector in a 4-dimensional space\nas a 4-dimensional vector, with one element along each dimension. In the example\nin Fig. 6.3, we\u2019ve chosen to make the document vectors of dimension 4, just so they\n\ufb01t on the page; in real term-document matrices, the document vectors would have\ndimensionalityjVj, the vocabulary size.\nThe ordering of the numbers in a vector space indicates the different dimensions\non which documents vary. The \ufb01rst dimension for both these vectors corresponds to\nthe number of times the word battle occurs, and we can compare each dimension,\nnoting for example that the vectors for As You Like It andTwelfth Night have similar\nvalues (1 and 0, respectively) for the \ufb01rst dimension.\nAs You Like It Twelfth Night Julius Caesar Henry V\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure 6.3 The term-document matrix for four words in four Shakespeare plays. The red\nboxes show that each document is represented as a column vector of length four.\nWe can think of the vector for a document as a point in jVj-dimensional space;\nthus the documents in Fig. 6.3 are points in 4-dimensional space. Since 4-dimensional\nspaces are hard to visualize, Fig. 6.4 shows a visualization in two dimensions; we\u2019ve\narbitrarily chosen the dimensions corresponding to the words battle andfool.\nTerm-document matrices were originally de\ufb01ned as a means of \ufb01nding similar\ndocuments for the task of document information retrieval . Two documents that are\nsimilar will tend to have similar words, and if two documents have similar words\ntheir column vectors will tend to be similar. The vectors for the comedies As You\nLike It [1,114,36,20] and Twelfth Night [0,80,58,15] look a lot more like each other",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 185,
      "token_count": 766,
      "chapter_title": ""
    }
  },
  {
    "content": "values (1 and 0, respectively) for the \ufb01rst dimension.\nAs You Like It Twelfth Night Julius Caesar Henry V\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure 6.3 The term-document matrix for four words in four Shakespeare plays. The red\nboxes show that each document is represented as a column vector of length four.\nWe can think of the vector for a document as a point in jVj-dimensional space;\nthus the documents in Fig. 6.3 are points in 4-dimensional space. Since 4-dimensional\nspaces are hard to visualize, Fig. 6.4 shows a visualization in two dimensions; we\u2019ve\narbitrarily chosen the dimensions corresponding to the words battle andfool.\nTerm-document matrices were originally de\ufb01ned as a means of \ufb01nding similar\ndocuments for the task of document information retrieval . Two documents that are\nsimilar will tend to have similar words, and if two documents have similar words\ntheir column vectors will tend to be similar. The vectors for the comedies As You\nLike It [1,114,36,20] and Twelfth Night [0,80,58,15] look a lot more like each other\n(more fools and wit than battles) than they look like Julius Caesar [7,62,1,2] or\nHenry V [13,89,4,3]. This is clear with the raw numbers; in the \ufb01rst dimension\n(battle) the comedies have low numbers and the others have high numbers, and we\ncan see it visually in Fig. 6.4; we\u2019ll see very shortly how to quantify this intuition\nmore formally.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 186,
      "token_count": 379,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 116\n\n108 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\n51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540\n354045505560\nFigure 6.4 A spatial visualization of the document vectors for the four Shakespeare play\ndocuments, showing just two of the dimensions, corresponding to the words battle andfool.\nThe comedies have high values for the fooldimension and low values for the battle dimension.\nA real term-document matrix, of course, wouldn\u2019t just have 4 rows and columns,\nlet alone 2. More generally, the term-document matrix has jVjrows (one for each\nword type in the vocabulary) and Dcolumns (one for each document in the collec-\ntion); as we\u2019ll see, vocabulary sizes are generally in the tens of thousands, and the\nnumber of documents can be enormous (think about all the pages on the web).\nInformation retrieval (IR) is the task of \ufb01nding the document dfrom the Dinformation\nretrieval\ndocuments in some collection that best matches a query q. For IR we\u2019ll therefore also\nrepresent a query by a vector, also of length jVj, and we\u2019ll need a way to compare\ntwo vectors to \ufb01nd how similar they are. (Doing IR will also require ef\ufb01cient ways\nto store and manipulate these vectors by making use of the convenient fact that these\nvectors are sparse, i.e., mostly zeros).\nLater in the chapter we\u2019ll introduce some of the components of this vector com-\nparison process: the tf-idf term weighting, and the cosine similarity metric.\n6.3.2 Words as vectors: document dimensions\nWe\u2019ve seen that documents can be represented as vectors in a vector space. But\nvector semantics can also be used to represent the meaning of words . We do this\nby associating each word with a word vector\u2014 a row vector rather than a column row vector\nvector, hence with different dimensions, as shown in Fig. 6.5. The four dimensions\nof the vector for fool, [36,58,1,4], correspond to the four Shakespeare plays. Word\ncounts in the same four dimensions are used to form the vectors for the other 3\nwords: wit, [20,15,2,3]; battle , [1,0,7,13]; and good [114,80,62,89].\nAs You Like It Twelfth Night Julius Caesar Henry V\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure 6.5 The term-document matrix for four words in four Shakespeare plays. The red\nboxes show that each word is represented as a row vector of length four.\nFor documents, we saw that similar documents had similar vectors, because sim-\nilar documents tend to have similar words. This same principle applies to words:\nsimilar words have similar vectors because they tend to occur in similar documents.\nThe term-document matrix thus lets us represent the meaning of a word by the doc-\numents it tends to occur in.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 187,
      "token_count": 688,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 117\n\n6.3 \u2022 W ORDS AND VECTORS 109\n6.3.3 Words as vectors: word dimensions\nAn alternative to using the term-document matrix to represent words as vectors of\ndocument counts, is to use the term-term matrix , also called the word-word ma-\ntrixor the term-context matrix , in which the columns are labeled by words ratherword-word\nmatrix\nthan documents. This matrix is thus of dimensionality jVj\u0002jVjand each cell records\nthe number of times the row (target) word and the column (context) word co-occur\nin some context in some training corpus. The context could be the document, in\nwhich case the cell represents the number of times the two words appear in the same\ndocument. It is most common, however, to use smaller contexts, generally a win-\ndow around the word, for example of 4 words to the left and 4 words to the right,\nin which case the cell represents the number of times (in some training corpus) the\ncolumn word occurs in such a \u00064 word window around the row word. Here are four\nexamples of words in their windows:\nis traditionally followed by cherry pie, a traditional dessert\noften mixed, such as strawberry rhubarb pie. Apple pie\ncomputer peripherals and personal digital assistants. These devices usually\na computer. This includes information available on the internet\nIf we then take every occurrence of each word (say strawberry ) and count the\ncontext words around it, we get a word-word co-occurrence matrix. Fig. 6.6 shows a\nsimpli\ufb01ed subset of the word-word co-occurrence matrix for these four words com-\nputed from the Wikipedia corpus (Davies, 2015).\naardvark ... computer data result pie sugar ...\ncherry 0 ... 2 8 9 442 25 ...\nstrawberry 0 ... 0 0 1 60 19 ...\ndigital 0 ... 1670 1683 85 5 4 ...\ninformation 0 ... 3325 3982 378 5 13 ...\nFigure 6.6 Co-occurrence vectors for four words in the Wikipedia corpus, showing six of\nthe dimensions (hand-picked for pedagogical purposes). The vector for digital is outlined in\nred. Note that a real vector would have vastly more dimensions and thus be much sparser.\nNote in Fig. 6.6 that the two words cherry andstrawberry are more similar to\neach other (both pieandsugar tend to occur in their window) than they are to other\nwords like digital ; conversely, digital andinformation are more similar to each other\nthan, say, to strawberry . Fig. 6.7 shows a spatial visualization.\n100020003000400010002000digital [1683,1670]computer datainformation [3982,3325] 30004000\nFigure 6.7 A spatial visualization of word vectors for digital andinformation , showing just\ntwo of the dimensions, corresponding to the words data andcomputer .\nNote thatjVj, the dimensionality of the vector, is generally the size of the vo-\ncabulary, often between 10,000 and 50,000 words (using the most frequent words",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 188,
      "token_count": 696,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 118\n\n110 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\nin the training corpus; keeping words after about the most frequent 50,000 or so is\ngenerally not helpful). Since most of these numbers are zero these are sparse vector\nrepresentations; there are ef\ufb01cient algorithms for storing and computing with sparse\nmatrices.\nNow that we have some intuitions, let\u2019s move on to examine the details of com-\nputing word similarity. Afterwards we\u2019ll discuss methods for weighting cells.\n6.4 Cosine for measuring similarity\nTo measure similarity between two target words vandw, we need a metric that\ntakes two vectors (of the same dimensionality, either both with words as dimensions,\nhence of lengthjVj, or both with documents as dimensions, of length jDj) and gives\na measure of their similarity. By far the most common similarity metric is the cosine\nof the angle between the vectors.\nThe cosine\u2014like most measures for vector similarity used in NLP\u2014is based on\nthedot product operator from linear algebra, also called the inner product : dot product\ninner product\ndot product (v;w) =v\u0001w=NX\ni=1viwi=v1w1+v2w2+:::+vNwN (6.7)\nThe dot product acts as a similarity metric because it will tend to be high just when\nthe two vectors have large values in the same dimensions. Alternatively, vectors that\nhave zeros in different dimensions\u2014orthogonal vectors\u2014will have a dot product of\n0, representing their strong dissimilarity.\nThis raw dot product, however, has a problem as a similarity metric: it favors\nlong vectors. The vector length is de\ufb01ned as vector length\njvj=vuutNX\ni=1v2\ni(6.8)\nThe dot product is higher if a vector is longer, with higher values in each dimension.\nMore frequent words have longer vectors, since they tend to co-occur with more\nwords and have higher co-occurrence values with each of them. The raw dot product\nthus will be higher for frequent words. But this is a problem; we\u2019d like a similarity\nmetric that tells us how similar two words are regardless of their frequency.\nWe modify the dot product to normalize for the vector length by dividing the\ndot product by the lengths of each of the two vectors. This normalized dot product\nturns out to be the same as the cosine of the angle between the two vectors, following\nfrom the de\ufb01nition of the dot product between two vectors aandb:\na\u0001b=jajjbjcosq\na\u0001b\njajjbj=cosq (6.9)\nThecosine similarity metric between two vectors vandwthus can be computed as: cosine",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 189,
      "token_count": 595,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 119\n\n6.5 \u2022 TF-IDF: W EIGHING TERMS IN THE VECTOR 111\ncosine (v;w) =v\u0001w\njvjjwj=NX\ni=1viwi\nvuutNX\ni=1v2\nivuutNX\ni=1w2\ni(6.10)\nFor some applications we pre-normalize each vector, by dividing it by its length,\ncreating a unit vector of length 1. Thus we could compute a unit vector from aby unit vector\ndividing it byjaj. For unit vectors, the dot product is the same as the cosine.\nThe cosine value ranges from 1 for vectors pointing in the same direction, through\n0 for orthogonal vectors, to -1 for vectors pointing in opposite directions. But since\nraw frequency values are non-negative, the cosine for these vectors ranges from 0\u20131.\nLet\u2019s see how the cosine computes which of the words cherry ordigital is closer\nin meaning to information , just using raw counts from the following shortened table:\npie data computer\ncherry 442 8 2\ndigital 5 1683 1670\ninformation 5 3982 3325\ncos(cherry;information ) =442\u00035+8\u00033982+2\u00033325p\n4422+82+22p\n52+39822+33252=:018\ncos(digital;information ) =5\u00035+1683\u00033982+1670\u00033325p\n52+16832+16702p\n52+39822+33252=:996\nThe model decides that information is way closer to digital than it is to cherry , a\nresult that seems sensible. Fig. 6.8 shows a visualization.\n50010001500200025003000500digitalcherryinformationDimension 1: \u2018pie\u2019\nDimension 2: \u2018computer\u2019\nFigure 6.8 A (rough) graphical demonstration of cosine similarity, showing vectors for\nthree words ( cherry ,digital , and information ) in the two dimensional space de\ufb01ned by counts\nof the words computer andpienearby. The \ufb01gure doesn\u2019t show the cosine, but it highlights the\nangles; note that the angle between digital andinformation is smaller than the angle between\ncherry andinformation . When two vectors are more similar, the cosine is larger but the angle\nis smaller; the cosine has its maximum (1) when the angle between two vectors is smallest\n(0\u000e); the cosine of all other angles is less than 1.\n6.5 TF-IDF: Weighing terms in the vector\nThe co-occurrence matrices above represent each cell by frequencies, either of words\nwith documents (Fig. 6.5), or words with other words (Fig. 6.6). But raw frequency",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 190,
      "token_count": 597,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 120",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 191,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "112 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\nis not the best measure of association between words. Raw frequency is very skewed\nand not very discriminative. If we want to know what kinds of contexts are shared\nbycherry andstrawberry but not by digital andinformation , we\u2019re not going to get\ngood discrimination from words like the,it, orthey, which occur frequently with\nall sorts of words and aren\u2019t informative about any particular word. We saw this\nalso in Fig. 6.3 for the Shakespeare corpus; the dimension for the word good is not\nvery discriminative between plays; good is simply a frequent word and has roughly\nequivalent high frequencies in each of the plays.\nIt\u2019s a bit of a paradox. Words that occur nearby frequently (maybe pienearby\ncherry ) are more important than words that only appear once or twice. Yet words\nthat are too frequent\u2014ubiquitous, like theorgood \u2014 are unimportant. How can we\nbalance these two con\ufb02icting constraints?\nThere are two common solutions to this problem: in this section we\u2019ll describe\nthetf-idf weighting, usually used when the dimensions are documents. In the next\nsection we introduce the PPMI algorithm (usually used when the dimensions are\nwords).\nThetf-idf weighting (the \u2018-\u2019 here is a hyphen, not a minus sign) is the product\nof two terms, each term capturing one of these two intuitions:\nThe \ufb01rst is the term frequency (Luhn, 1957): the frequency of the word tin the term frequency\ndocument d. We can just use the raw count as the term frequency:\ntft;d=count (t;d) (6.11)\nMore commonly we squash the raw frequency a bit, by using the log 10of the fre-\nquency instead. The intuition is that a word appearing 100 times in a document\ndoesn\u2019t make that word 100 times more likely to be relevant to the meaning of the\ndocument. We also need to do something special with counts of 0, since we can\u2019t\ntake the log of 0.2\ntft;d=(\n1+log10count (t;d) if count (t;d)>0\n0 otherwise(6.12)\nIf we use log weighting, terms which occur 0 times in a document would have tf =0,\n1 times in a document tf =1+log10(1) =1+0=1, 10 times in a document tf =\n1+log10(10) =2, 100 times tf =1+log10(100) =3, 1000 times tf =4, and so on.\nThe second factor in tf-idf is used to give a higher weight to words that occur\nonly in a few documents. Terms that are limited to a few documents are useful\nfor discriminating those documents from the rest of the collection; terms that occur\nfrequently across the entire collection aren\u2019t as helpful. The document frequencydocument\nfrequency\ndftof a term tis the number of documents it occurs in. Document frequency is\nnot the same as the collection frequency of a term, which is the total number of\ntimes the word appears in the whole collection in any document. Consider in the\ncollection of Shakespeare\u2019s 37 plays the two words Romeo andaction . The words\nhave identical collection frequencies (they both occur 113 times in all the plays) but\nvery different document frequencies, since Romeo only occurs in a single play. If\nour goal is to \ufb01nd documents about the romantic tribulations of Romeo, the word\nRomeo should be highly weighted, but not action :\nCollection Frequency Document Frequency",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 192,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "1 times in a document tf =1+log10(1) =1+0=1, 10 times in a document tf =\n1+log10(10) =2, 100 times tf =1+log10(100) =3, 1000 times tf =4, and so on.\nThe second factor in tf-idf is used to give a higher weight to words that occur\nonly in a few documents. Terms that are limited to a few documents are useful\nfor discriminating those documents from the rest of the collection; terms that occur\nfrequently across the entire collection aren\u2019t as helpful. The document frequencydocument\nfrequency\ndftof a term tis the number of documents it occurs in. Document frequency is\nnot the same as the collection frequency of a term, which is the total number of\ntimes the word appears in the whole collection in any document. Consider in the\ncollection of Shakespeare\u2019s 37 plays the two words Romeo andaction . The words\nhave identical collection frequencies (they both occur 113 times in all the plays) but\nvery different document frequencies, since Romeo only occurs in a single play. If\nour goal is to \ufb01nd documents about the romantic tribulations of Romeo, the word\nRomeo should be highly weighted, but not action :\nCollection Frequency Document Frequency\nRomeo 113 1\naction 113 31\n2We can also use this alternative formulation, which we have used in earlier editions: tf t;d=\nlog10(count (t;d)+1)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 193,
      "token_count": 320,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 121\n\n6.5 \u2022 TF-IDF: W EIGHING TERMS IN THE VECTOR 113\nWe emphasize discriminative words like Romeo via the inverse document fre-\nquency oridfterm weight (Sparck Jones, 1972). The idf is de\ufb01ned using the frac- idf\ntionN=dft, where Nis the total number of documents in the collection, and df tis\nthe number of documents in which term toccurs. The fewer documents in which a\nterm occurs, the higher this weight. The lowest weight of 1 is assigned to terms that\noccur in all the documents. It\u2019s usually clear what counts as a document: in Shake-\nspeare we would use a play; when processing a collection of encyclopedia articles\nlike Wikipedia, the document is a Wikipedia page; in processing newspaper articles,\nthe document is a single article. Occasionally your corpus might not have appropri-\nate document divisions and you might need to break up the corpus into documents\nyourself for the purposes of computing idf.\nBecause of the large number of documents in many collections, this measure\ntoo is usually squashed with a log function. The resulting de\ufb01nition for inverse\ndocument frequency (idf) is thus\nidft=log10\u0012N\ndft\u0013\n(6.13)\nHere are some idf values for some words in the Shakespeare corpus, (along with\nthe document frequency df values on which they are based) ranging from extremely\ninformative words which occur in only one play like Romeo , to those that occur in a\nfew like salad orFalstaff , to those which are very common like foolor so common\nas to be completely non-discriminative since they occur in all 37 plays like good or\nsweet .3\nWord df idf\nRomeo 1 1.57\nsalad 2 1.27\nFalstaff 4 0.967\nforest 12 0.489\nbattle 21 0.246\nwit 34 0.037\nfool 36 0.012\ngood 37 0\nsweet 37 0\nThe tf-idf weighted value wt;dfor word tin document dthus combines term tf-idf\nfrequency tf t;d(de\ufb01ned either by Eq. 6.11 or by Eq. 6.12) with idf from Eq. 6.13:\nwt;d=tft;d\u0002idft (6.14)\nFig. 6.9 applies tf-idf weighting to the Shakespeare term-document matrix in Fig. 6.2,\nusing the tf equation Eq. 6.12. Note that the tf-idf values for the dimension corre-\nsponding to the word good have now all become 0; since this word appears in every\ndocument, the tf-idf weighting leads it to be ignored. Similarly, the word fool, which\nappears in 36 out of the 37 plays, has a much lower weight.\nThe tf-idf weighting is the way for weighting co-occurrence matrices in infor-\nmation retrieval, but also plays a role in many other aspects of natural language\nprocessing. It\u2019s also a great baseline, the simple thing to try \ufb01rst. We\u2019ll look at other\nweightings like PPMI (Positive Pointwise Mutual Information) in Section 6.6.\n3Sweet was one of Shakespeare\u2019s favorite adjectives, a fact probably related to the increased use of\nsugar in European recipes around the turn of the 16th century (Jurafsky, 2014, p. 175).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 194,
      "token_count": 758,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 122",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 195,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "114 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\nAs You Like It Twelfth Night Julius Caesar Henry V\nbattle 0.246 0 0.454 0.520\ngood 0 0 0 0\nfool 0.030 0.033 0.0012 0.0019\nwit 0.085 0.081 0.048 0.054\nFigure 6.9 A portion of the tf-idf weighted term-document matrix for four words in Shake-\nspeare plays, showing a selection of 4 plays, using counts from Fig. 6.2. For example the\n0:085 value for witinAs You Like It is the product of tf =1+log10(20) =2:301 and idf =:037.\nNote that the idf weighting has eliminated the importance of the ubiquitous word good and\nvastly reduced the impact of the almost-ubiquitous word fool.\n6.6 Pointwise Mutual Information (PMI)\nAn alternative weighting function to tf-idf, PPMI (positive pointwise mutual infor-\nmation), is used for term-term-matrices, when the vector dimensions correspond to\nwords rather than documents. PPMI draws on the intuition that the best way to weigh\nthe association between two words is to ask how much more the two words co-occur\nin our corpus than we would have a priori expected them to appear by chance.\nPointwise mutual information (Fano, 1961)4is one of the most important con-pointwise\nmutual\ninformationcepts in NLP. It is a measure of how often two events xandyoccur, compared with\nwhat we would expect if they were independent:\nI(x;y) =log2P(x;y)\nP(x)P(y)(6.16)\nThe pointwise mutual information between a target word wand a context word\nc(Church and Hanks 1989, Church and Hanks 1990) is then de\ufb01ned as:\nPMI(w;c) =log2P(w;c)\nP(w)P(c)(6.17)\nThe numerator tells us how often we observed the two words together (assuming\nwe compute probability by using the MLE). The denominator tells us how often\nwe would expect the two words to co-occur assuming they each occurred indepen-\ndently; recall that the probability of two independent events both occurring is just\nthe product of the probabilities of the two events. Thus, the ratio gives us an esti-\nmate of how much more the two words co-occur than we expect by chance. PMI is\na useful tool whenever we need to \ufb01nd words that are strongly associated.\nPMI values range from negative to positive in\ufb01nity. But negative PMI values\n(which imply things are co-occurring less often than we would expect by chance)\ntend to be unreliable unless our corpora are enormous. To distinguish whether\ntwo words whose individual probability is each 10\u00006occur together less often than\nchance, we would need to be certain that the probability of the two occurring to-\ngether is signi\ufb01cantly less than 10\u000012, and this kind of granularity would require an\nenormous corpus. Furthermore it\u2019s not clear whether it\u2019s even possible to evaluate\nsuch scores of \u2018unrelatedness\u2019 with human judgments. For this reason it is more\n4PMI is based on the mutual information between two random variables XandY, de\ufb01ned as:\nI(X;Y) =X\nxX\nyP(x;y)log2P(x;y)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 196,
      "token_count": 774,
      "chapter_title": ""
    }
  },
  {
    "content": "we would expect the two words to co-occur assuming they each occurred indepen-\ndently; recall that the probability of two independent events both occurring is just\nthe product of the probabilities of the two events. Thus, the ratio gives us an esti-\nmate of how much more the two words co-occur than we expect by chance. PMI is\na useful tool whenever we need to \ufb01nd words that are strongly associated.\nPMI values range from negative to positive in\ufb01nity. But negative PMI values\n(which imply things are co-occurring less often than we would expect by chance)\ntend to be unreliable unless our corpora are enormous. To distinguish whether\ntwo words whose individual probability is each 10\u00006occur together less often than\nchance, we would need to be certain that the probability of the two occurring to-\ngether is signi\ufb01cantly less than 10\u000012, and this kind of granularity would require an\nenormous corpus. Furthermore it\u2019s not clear whether it\u2019s even possible to evaluate\nsuch scores of \u2018unrelatedness\u2019 with human judgments. For this reason it is more\n4PMI is based on the mutual information between two random variables XandY, de\ufb01ned as:\nI(X;Y) =X\nxX\nyP(x;y)log2P(x;y)\nP(x)P(y)(6.15)\nIn a confusion of terminology, Fano used the phrase mutual information to refer to what we now call\npointwise mutual information and the phrase expectation of the mutual information for what we now call\nmutual information",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 197,
      "token_count": 339,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 123\n\n6.6 \u2022 P OINTWISE MUTUAL INFORMATION (PMI) 115\ncommon to use Positive PMI (called PPMI ) which replaces all negative PMI values PPMI\nwith zero (Church and Hanks 1989, Dagan et al. 1993, Niwa and Nitta 1994)5:\nPPMI (w;c) =max(log2P(w;c)\nP(w)P(c);0) (6.18)\nMore formally, let\u2019s assume we have a co-occurrence matrix F with W rows (words)\nand C columns (contexts), where fi jgives the number of times word wioccurs with\ncontext cj. This can be turned into a PPMI matrix where PPMI i jgives the PPMI\nvalue of word wiwith context cj(which we can also express as PPMI( wi;cj) or\nPPMI( w=i;c=j)) as follows:\npi j=fi jPW\ni=1PC\nj=1fi j;pi\u0003=PC\nj=1fi jPW\ni=1PC\nj=1fi j;p\u0003j=PW\ni=1fi jPW\ni=1PC\nj=1fi j(6.19)\nPPMI i j=max(log2pi j\npi\u0003p\u0003j;0) (6.20)\nLet\u2019s see some PPMI calculations. We\u2019ll use Fig. 6.10, which repeats Fig. 6.6 plus\nall the count marginals, and let\u2019s pretend for ease of calculation that these are the\nonly words/contexts that matter.\ncomputer data result pie sugar count(w)\ncherry 2 8 9 442 25 486\nstrawberry 0 0 1 60 19 80\ndigital 1670 1683 85 5 4 3447\ninformation 3325 3982 378 5 13 7703\ncount(context) 4997 5673 473 512 61 11716\nFigure 6.10 Co-occurrence counts for four words in 5 contexts in the Wikipedia corpus,\ntogether with the marginals, pretending for the purpose of this calculation that no other word-\ns/contexts matter.\nThus for example we could compute PPMI(information,data), assuming we pre-\ntended that Fig. 6.6 encompassed all the relevant word contexts/dimensions, as fol-\nlows:\nP(w=information, c=data ) =3982\n11716=:3399\nP(w=information ) =7703\n11716=:6575\nP(c=data ) =5673\n11716=:4842\nPPMI (information,data ) = log2(:3399=(:6575\u0003:4842)) =:0944\nFig. 6.11 shows the joint probabilities computed from the counts in Fig. 6.10, and\nFig. 6.12 shows the PPMI values. Not surprisingly, cherry andstrawberry are highly\nassociated with both pieandsugar , and data is mildly associated with information .\nPMI has the problem of being biased toward infrequent events; very rare words\ntend to have very high PMI values. One way to reduce this bias toward low frequency\n5Positive PMI also cleanly solves the problem of what to do with zero counts, using 0 to replace the\n\u0000\u00a5from log (0).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 198,
      "token_count": 737,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 124\n\n116 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\np(w,context) p(w)\ncomputer data result pie sugar p(w)\ncherry 0.0002 0.0007 0.0008 0.0377 0.0021 0.0415\nstrawberry 0.0000 0.0000 0.0001 0.0051 0.0016 0.0068\ndigital 0.1425 0.1436 0.0073 0.0004 0.0003 0.2942\ninformation 0.2838 0.3399 0.0323 0.0004 0.0011 0.6575\np(context) 0.4265 0.4842 0.0404 0.0437 0.0052\nFigure 6.11 Replacing the counts in Fig. 6.6 with joint probabilities, showing the marginals\nin the right column and the bottom row.\ncomputer data result pie sugar\ncherry 0 0 0 4.38 3.30\nstrawberry 0 0 0 4.10 5.51\ndigital 0.18 0.01 0 0 0\ninformation 0.02 0.09 0.28 0 0\nFigure 6.12 The PPMI matrix showing the association between words and context words,\ncomputed from the counts in Fig. 6.11. Note that most of the 0 PPMI values are ones that had\na negative PMI; for example PMI( cherry,computer ) = -6.7, meaning that cherry andcomputer\nco-occur on Wikipedia less often than we would expect by chance, and with PPMI we replace\nnegative values by zero.\nevents is to slightly change the computation for P(c), using a different function Pa(c)\nthat raises the probability of the context word to the power of a:\nPPMI a(w;c) =max(log2P(w;c)\nP(w)Pa(c);0) (6.21)\nPa(c) =count (c)a\nP\nccount (c)a(6.22)\nLevy et al. (2015) found that a setting of a=0:75 improved performance of\nembeddings on a wide range of tasks (drawing on a similar weighting used for skip-\ngrams described below in Eq. 6.32). This works because raising the count to a=\n0:75 increases the probability assigned to rare contexts, and hence lowers their PMI\n(Pa(c)>P(c)when cis rare).\nAnother possible solution is Laplace smoothing: Before computing PMI, a small\nconstant k(values of 0.1-3 are common) is added to each of the counts, shrinking\n(discounting) all the non-zero values. The larger the k, the more the non-zero counts\nare discounted.\n6.7 Applications of the tf-idf or PPMI vector models\nIn summary, the vector semantics model we\u2019ve described so far represents a target\nword as a vector with dimensions corresponding either to the documents in a large\ncollection (the term-document matrix) or to the counts of words in some neighboring\nwindow (the term-term matrix). The values in each dimension are counts, weighted\nby tf-idf (for term-document matrices) or PPMI (for term-term matrices), and the\nvectors are sparse (since most values are zero).\nThe model computes the similarity between two words xandyby taking the\ncosine of their tf-idf or PPMI vectors; high cosine, high similarity. This entire model",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 199,
      "token_count": 792,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 125\n\n6.8 \u2022 W ORD2VEC 117\nis sometimes referred to as the tf-idf model or the PPMI model, after the weighting\nfunction.\nThe tf-idf model of meaning is often used for document functions like deciding\nif two documents are similar. We represent a document by taking the vectors of\nall the words in the document, and computing the centroid of all those vectors. centroid\nThe centroid is the multidimensional version of the mean; the centroid of a set of\nvectors is a single vector that has the minimum sum of squared distances to each of\nthe vectors in the set. Given kword vectors w1;w2;:::;wk, the centroid document\nvector dis:document\nvector\nd=w1+w2+:::+wk\nk(6.23)\nGiven two documents, we can then compute their document vectors d1andd2, and\nestimate the similarity between the two documents by cos (d1;d2). Document sim-\nilarity is also useful for all sorts of applications; information retrieval, plagiarism\ndetection, news recommender systems, and even for digital humanities tasks like\ncomparing different versions of a text to see which are similar to each other.\nEither the PPMI model or the tf-idf model can be used to compute word simi-\nlarity, for tasks like \ufb01nding word paraphrases, tracking changes in word meaning, or\nautomatically discovering meanings of words in different corpora. For example, we\ncan \ufb01nd the 10 most similar words to any target word wby computing the cosines\nbetween wand each of the V\u00001 other words, sorting, and looking at the top 10.\n6.8 Word2vec\nIn the previous sections we saw how to represent a word as a sparse, long vector with\ndimensions corresponding to words in the vocabulary or documents in a collection.\nWe now introduce a more powerful word representation: embeddings , short dense\nvectors. Unlike the vectors we\u2019ve seen so far, embeddings are short , with number\nof dimensions dranging from 50-1000, rather than the much larger vocabulary size\njVjor number of documents Dwe\u2019ve seen. These ddimensions don\u2019t have a clear\ninterpretation. And the vectors are dense : instead of vector entries being sparse,\nmostly-zero counts or functions of counts, the values will be real-valued numbers\nthat can be negative.\nIt turns out that dense vectors work better in every NLP task than sparse vectors.\nWhile we don\u2019t completely understand all the reasons for this, we have some intu-\nitions. Representing words as 300-dimensional dense vectors requires our classi\ufb01ers\nto learn far fewer weights than if we represented words as 50,000-dimensional vec-\ntors, and the smaller parameter space possibly helps with generalization and avoid-\ning over\ufb01tting. Dense vectors may also do a better job of capturing synonymy.\nFor example, in a sparse vector representation, dimensions for synonyms like car\nandautomobile dimension are distinct and unrelated; sparse vectors may thus fail\nto capture the similarity between a word with caras a neighbor and a word with\nautomobile as a neighbor.\nIn this section we introduce one method for computing embeddings: skip-gram skip-gram\nwith negative sampling , sometimes called SGNS . The skip-gram algorithm is one SGNS\nof two algorithms in a software package called word2vec , and so sometimes the word2vec\nalgorithm is loosely referred to as word2vec (Mikolov et al. 2013a, Mikolov et al.\n2013b). The word2vec methods are fast, ef\ufb01cient to train, and easily available on-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 200,
      "token_count": 777,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 126\n\n118 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\nline with code and pretrained embeddings. Word2vec embeddings are static em-\nbeddings , meaning that the method learns one \ufb01xed embedding for each word in thestatic\nembeddings\nvocabulary. In Chapter 11 we\u2019ll introduce methods for learning dynamic contextual\nembeddings like the popular family of BERT representations, in which the vector\nfor each word is different in different contexts.\nThe intuition of word2vec is that instead of counting how often each word woc-\ncurs near, say, apricot , we\u2019ll instead train a classi\ufb01er on a binary prediction task: \u201cIs\nword wlikely to show up near apricot ?\u201d We don\u2019t actually care about this prediction\ntask; instead we\u2019ll take the learned classi\ufb01er weights as the word embeddings.\nThe revolutionary intuition here is that we can just use running text as implicitly\nsupervised training data for such a classi\ufb01er; a word cthat occurs near the target\nword apricot acts as gold \u2018correct answer\u2019 to the question \u201cIs word clikely to show\nup near apricot ?\u201d This method, often called self-supervision , avoids the need for self-supervision\nany sort of hand-labeled supervision signal. This idea was \ufb01rst proposed in the task\nof neural language modeling, when Bengio et al. (2003) and Collobert et al. (2011)\nshowed that a neural language model (a neural network that learned to predict the\nnext word from prior words) could just use the next word in running text as its\nsupervision signal, and could be used to learn an embedding representation for each\nword as part of doing this prediction task.\nWe\u2019ll see how to do neural networks in the next chapter, but word2vec is a\nmuch simpler model than the neural network language model, in two ways. First,\nword2vec simpli\ufb01es the task (making it binary classi\ufb01cation instead of word pre-\ndiction). Second, word2vec simpli\ufb01es the architecture (training a logistic regression\nclassi\ufb01er instead of a multi-layer neural network with hidden layers that demand\nmore sophisticated training algorithms). The intuition of skip-gram is:\n1. Treat the target word and a neighboring context word as positive examples.\n2. Randomly sample other words in the lexicon to get negative samples.\n3. Use logistic regression to train a classi\ufb01er to distinguish those two cases.\n4. Use the learned weights as the embeddings.\n6.8.1 The classi\ufb01er\nLet\u2019s start by thinking about the classi\ufb01cation task, and then turn to how to train.\nImagine a sentence like the following, with a target word apricot , and assume we\u2019re\nusing a window of \u00062 context words:\n... lemon, a [tablespoon of apricot jam, a] pinch ...\nc1 c2 w c3 c4\nOur goal is to train a classi\ufb01er such that, given a tuple (w;c)of a target word\nwpaired with a candidate context word c(for example ( apricot ,jam), or perhaps\n(apricot ,aardvark )) it will return the probability that cis a real context word (true\nforjam, false for aardvark ):\nP(+jw;c) (6.24)\nThe probability that word cis not a real context word for wis just 1 minus\nEq. 6.24:\nP(\u0000jw;c) =1\u0000P(+jw;c) (6.25)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 201,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 127\n\n6.8 \u2022 W ORD2VEC 119\nHow does the classi\ufb01er compute the probability P? The intuition of the skip-\ngram model is to base this probability on embedding similarity: a word is likely to\noccur near the target if its embedding vector is similar to the target embedding. To\ncompute similarity between these dense embeddings, we rely on the intuition that\ntwo vectors are similar if they have a high dot product (after all, cosine is just a\nnormalized dot product). In other words:\nSimilarity (w;c)\u0019c\u0001w (6.26)\nThe dot product c\u0001wis not a probability, it\u2019s just a number ranging from \u0000\u00a5to\u00a5\n(since the elements in word2vec embeddings can be negative, the dot product can be\nnegative). To turn the dot product into a probability, we\u2019ll use the logistic orsigmoid\nfunction s(x), the fundamental core of logistic regression:\ns(x) =1\n1+exp(\u0000x)(6.27)\nWe model the probability that word cis a real context word for target word was:\nP(+jw;c) = s(c\u0001w) =1\n1+exp(\u0000c\u0001w)(6.28)\nThe sigmoid function returns a number between 0 and 1, but to make it a probability\nwe\u2019ll also need the total probability of the two possible events ( cis a context word,\nandcisn\u2019t a context word) to sum to 1. We thus estimate the probability that word c\nis not a real context word for was:\nP(\u0000jw;c) = 1\u0000P(+jw;c)\n=s(\u0000c\u0001w) =1\n1+exp(c\u0001w)(6.29)\nEquation 6.28 gives us the probability for one word, but there are many context\nwords in the window. Skip-gram makes the simplifying assumption that all context\nwords are independent, allowing us to just multiply their probabilities:\nP(+jw;c1:L) =LY\ni=1s(ci\u0001w) (6.30)\nlogP(+jw;c1:L) =LX\ni=1logs(ci\u0001w) (6.31)\nIn summary, skip-gram trains a probabilistic classi\ufb01er that, given a test target word\nwand its context window of Lwords c1:L, assigns a probability based on how similar\nthis context window is to the target word. The probability is based on applying the\nlogistic (sigmoid) function to the dot product of the embeddings of the target word\nwith each context word. To compute this probability, we just need embeddings for\neach target word and context word in the vocabulary.\nFig. 6.13 shows the intuition of the parameters we\u2019ll need. Skip-gram actually\nstores two embeddings for each word, one for the word as a target, and one for the\nword considered as context. Thus the parameters we need to learn are two matrices\nWandC, each containing an embedding for every one of the jVjwords in the\nvocabulary V.6Let\u2019s now turn to learning these embeddings (which is the real goal\nof training this classi\ufb01er in the \ufb01rst place).\n6In principle the target matrix and the context matrix could use different vocabularies, but we\u2019ll simplify\nby assuming one shared vocabulary V.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 202,
      "token_count": 711,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 128\n\n120 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\n1WCaardvark\nzebrazebraaardvarkapricotapricot|V||V|+12V& =target wordscontext & noisewords\u2026\n\u20261..d\u2026\n\u2026\nFigure 6.13 The embeddings learned by the skipgram model. The algorithm stores two\nembeddings for each word, the target embedding (sometimes called the input embedding)\nand the context embedding (sometimes called the output embedding). The parameter qthat\nthe algorithm learns is thus a matrix of 2 jVjvectors, each of dimension d, formed by concate-\nnating two matrices, the target embeddings Wand the context+noise embeddings C.\n6.8.2 Learning skip-gram embeddings\nThe learning algorithm for skip-gram embeddings takes as input a corpus of text,\nand a chosen vocabulary size N. It begins by assigning a random embedding vector\nfor each of the N vocabulary words, and then proceeds to iteratively shift the em-\nbedding of each word wto be more like the embeddings of words that occur nearby\nin texts, and less like the embeddings of words that don\u2019t occur nearby. Let\u2019s start\nby considering a single piece of training data:\n... lemon, a [tablespoon of apricot jam, a] pinch ...\nc1 c2 w c3 c4\nThis example has a target word w(apricot), and 4 context words in the L=\u00062\nwindow, resulting in 4 positive training instances (on the left below):\npositive examples +\nw c pos\napricot tablespoon\napricot of\napricot jam\napricot anegative examples -\nw c neg w c neg\napricot aardvark apricot seven\napricot my apricot forever\napricot where apricot dear\napricot coaxial apricot if\nFor training a binary classi\ufb01er we also need negative examples. In fact skip-\ngram with negative sampling (SGNS) uses more negative examples than positive\nexamples (with the ratio between them set by a parameter k). So for each of these\n(w;cpos)training instances we\u2019ll create knegative samples, each consisting of the\ntarget wplus a \u2018noise word\u2019 cneg. A noise word is a random word from the lexicon,\nconstrained not to be the target word w. The right above shows the setting where\nk=2, so we\u2019ll have 2 negative examples in the negative training set \u0000for each\npositive example w;cpos.\nThe noise words are chosen according to their weighted unigram frequency\npa(w), where ais a weight. If we were sampling according to unweighted fre-\nquency p(w), it would mean that with unigram probability p(\u201cthe\u201d)we would choose\nthe word theas a noise word, with unigram probability p(\u201caardvark \u201d)we would",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 203,
      "token_count": 622,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 129\n\n6.8 \u2022 W ORD2VEC 121\nchoose aardvark , and so on. But in practice it is common to set a=0:75, i.e. use\nthe weighting p3\n4(w):\nPa(w) =count (w)a\nP\nw0count (w0)a(6.32)\nSetting a=:75 gives better performance because it gives rare noise words slightly\nhigher probability: for rare words, Pa(w)>P(w). To illustrate this intuition, it\nmight help to work out the probabilities for an example with a=:75 and two events,\nP(a) =0:99 and P(b) =0:01:\nPa(a) =:99:75\n:99:75+:01:75=0:97\nPa(b) =:01:75\n:99:75+:01:75=0:03 (6.33)\nThus using a=:75 increases the probability of the rare event bfrom 0.01 to 0.03.\nGiven the set of positive and negative training instances, and an initial set of\nembeddings, the goal of the learning algorithm is to adjust those embeddings to\n\u2022 Maximize the similarity of the target word, context word pairs (w;cpos)drawn\nfrom the positive examples\n\u2022 Minimize the similarity of the (w;cneg)pairs from the negative examples.\nIf we consider one word/context pair (w;cpos)with its knoise words cneg1:::cnegk,\nwe can express these two goals as the following loss function Lto be minimized\n(hence the\u0000); here the \ufb01rst term expresses that we want the classi\ufb01er to assign the\nreal context word cposa high probability of being a neighbor, and the second term\nexpresses that we want to assign each of the noise words cnegia high probability of\nbeing a non-neighbor, all multiplied because we assume independence:\nL=\u0000log\"\nP(+jw;cpos)kY\ni=1P(\u0000jw;cnegi)#\n=\u0000\"\nlogP(+jw;cpos)+kX\ni=1logP(\u0000jw;cnegi)#\n=\u0000\"\nlogP(+jw;cpos)+kX\ni=1log\u0000\n1\u0000P(+jw;cnegi)\u0001#\n=\u0000\"\nlogs(cpos\u0001w)+kX\ni=1logs(\u0000cnegi\u0001w)#\n(6.34)\nThat is, we want to maximize the dot product of the word with the actual context\nwords, and minimize the dot products of the word with the knegative sampled non-\nneighbor words.\nWe minimize this loss function using stochastic gradient descent. Fig. 6.14\nshows the intuition of one step of learning.\nTo get the gradient, we need to take the derivative of Eq. 6.34 with respect to\nthe different embeddings. It turns out the derivatives are the following (we leave the",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 204,
      "token_count": 634,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 130\n\n122 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\nWCmove apricot and jam closer,increasing cpos z waardvark\nmove apricot and matrix apartdecreasing cneg1 z w\u201c\u2026apricot jam\u2026\u201dw\nzebrazebraaardvarkjamapricot\ncposmatrixTolstoymove apricot and Tolstoy apartdecreasing cneg2 z w!cneg1cneg2k=2\nFigure 6.14 Intuition of one step of gradient descent. The skip-gram model tries to shift\nembeddings so the target embeddings (here for apricot ) are closer to (have a higher dot prod-\nuct with) context embeddings for nearby words (here jam) and further from (lower dot product\nwith) context embeddings for noise words that don\u2019t occur nearby (here Tolstoy andmatrix ).\nproof as an exercise at the end of the chapter):\n\u00b6L\n\u00b6cpos= [s(cpos\u0001w)\u00001]w (6.35)\n\u00b6L\n\u00b6cneg= [s(cneg\u0001w)]w (6.36)\n\u00b6L\n\u00b6w= [s(cpos\u0001w)\u00001]cpos+kX\ni=1[s(cnegi\u0001w)]cnegi(6.37)\nThe update equations going from time step ttot+1 in stochastic gradient descent\nare thus:\nct+1\npos=ct\npos\u0000h[s(ct\npos\u0001wt)\u00001]wt(6.38)\nct+1\nneg=ct\nneg\u0000h[s(ct\nneg\u0001wt)]wt(6.39)\nwt+1=wt\u0000h\"\n[s(ct\npos\u0001wt)\u00001]ct\npos+kX\ni=1[s(ct\nnegi\u0001wt)]ct\nnegi#\n(6.40)\nJust as in logistic regression, then, the learning algorithm starts with randomly ini-\ntialized WandCmatrices, and then walks through the training corpus using gradient\ndescent to move WandCso as to minimize the loss in Eq. 6.34 by making the up-\ndates in (Eq. 6.38)-(Eq. 6.40).\nRecall that the skip-gram model learns twoseparate embeddings for each word i:\nthetarget embedding wiand the context embedding ci, stored in two matrices, thetarget\nembeddingcontext\nembedding target matrix Wand the context matrix C. It\u2019s common to just add them together,\nrepresenting word iwith the vector wi+ci. Alternatively we can throw away the C\nmatrix and just represent each word iby the vector wi.\nAs with the simple count-based methods like tf-idf, the context window size L\naffects the performance of skip-gram embeddings, and experiments often tune the\nparameter Lon a devset.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 205,
      "token_count": 604,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 131",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 206,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "6.9 \u2022 V ISUALIZING EMBEDDINGS 123\n6.8.3 Other kinds of static embeddings\nThere are many kinds of static embeddings. An extension of word2vec, fasttext fasttext\n(Bojanowski et al., 2017), addresses a problem with word2vec as we have presented\nit so far: it has no good way to deal with unknown words \u2014words that appear in\na test corpus but were unseen in the training corpus. A related problem is word\nsparsity, such as in languages with rich morphology, where some of the many forms\nfor each noun and verb may only occur rarely. Fasttext deals with these problems\nby using subword models, representing each word as itself plus a bag of constituent\nn-grams, with special boundary symbols <and>added to each word. For example,\nwith n=3 the word where would be represented by the sequence <where> plus the\ncharacter n-grams:\n<wh, whe, her, ere, re>\nThen a skipgram embedding is learned for each constituent n-gram, and the word\nwhere is represented by the sum of all of the embeddings of its constituent n-grams.\nUnknown words can then be presented only by the sum of the constituent n-grams.\nA fasttext open-source library, including pretrained embeddings for 157 languages,\nis available at https://fasttext.cc .\nAnother very widely used static embedding model is GloVe (Pennington et al.,\n2014), short for Global Vectors, because the model is based on capturing global\ncorpus statistics. GloVe is based on ratios of probabilities from the word-word co-\noccurrence matrix, combining the intuitions of count-based models like PPMI while\nalso capturing the linear structures used by methods like word2vec.\nIt turns out that dense embeddings like word2vec actually have an elegant math-\nematical relationship with sparse embeddings like PPMI, in which word2vec can\nbe seen as implicitly optimizing a function of a PPMI matrix (Levy and Goldberg,\n2014c).\n6.9 Visualizing Embeddings\n\u201cI see well in many dimensions as long as the dimensions are around two.\u201d\nThe late economist Martin Shubik\nVisualizing embeddings is an important goal in helping understand, apply, and\nimprove these models of word meaning. But how can we visualize a (for example)\n100-dimensional vector?\nRohde, Gonnerman, Plaut Modeling Word Meaning Using Lexical Co-Occurrence\nHEADHANDFACE\nDOGAMERICA\nCATEYEEUROPE\nFOOTCHINAFRANCE\nCHICAGOARM\nFINGER\nNOSELEGRUSSIA\nMOUSEAFRICA\nATLANTAEARSHOULDERASIA\nCOW\nBULLPUPPYLIONHAWAII\nMONTREALTOKYOTOEMOSCOW\nTOOTH\nNASHVILLEBRAZILWRIST\nKITTENANKLE\nTURTLE\nOYSTER\nFigure 8: Multidimensional scaling for three noun classes.WRIST\nANKLE\nSHOULDER\nARM\nLEG\nHAND\nFOOT\nHEAD\nNOSE\nFINGER\nTOE\nFACE\nEAR\nEYE\nTOOTH\nDOG\nCAT\nPUPPY\nKITTEN\nCOW\nMOUSE\nTURTLE\nOYSTER\nLION\nBULL\nCHICAGO\nATLANTA\nMONTREAL\nNASHVILLE\nTOKYOCHINA\nRUSSIA\nAFRICA\nASIA\nEUROPE\nAMERICA\nBRAZIL\nMOSCOW",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 207,
      "token_count": 766,
      "chapter_title": ""
    }
  },
  {
    "content": "improve these models of word meaning. But how can we visualize a (for example)\n100-dimensional vector?\nRohde, Gonnerman, Plaut Modeling Word Meaning Using Lexical Co-Occurrence\nHEADHANDFACE\nDOGAMERICA\nCATEYEEUROPE\nFOOTCHINAFRANCE\nCHICAGOARM\nFINGER\nNOSELEGRUSSIA\nMOUSEAFRICA\nATLANTAEARSHOULDERASIA\nCOW\nBULLPUPPYLIONHAWAII\nMONTREALTOKYOTOEMOSCOW\nTOOTH\nNASHVILLEBRAZILWRIST\nKITTENANKLE\nTURTLE\nOYSTER\nFigure 8: Multidimensional scaling for three noun classes.WRIST\nANKLE\nSHOULDER\nARM\nLEG\nHAND\nFOOT\nHEAD\nNOSE\nFINGER\nTOE\nFACE\nEAR\nEYE\nTOOTH\nDOG\nCAT\nPUPPY\nKITTEN\nCOW\nMOUSE\nTURTLE\nOYSTER\nLION\nBULL\nCHICAGO\nATLANTA\nMONTREAL\nNASHVILLE\nTOKYOCHINA\nRUSSIA\nAFRICA\nASIA\nEUROPE\nAMERICA\nBRAZIL\nMOSCOW\nFRANCEHAWAIIFigure 9: Hierarchical clustering for three noun classes using distances based on vector correlations.\n20\nThe simplest way to visualize the meaning of a word\nwembedded in a space is to list the most similar words to\nwby sorting the vectors for all words in the vocabulary by\ntheir cosine with the vector for w. For example the 7 closest\nwords to frogusing a particular embeddings computed with\nthe GloVe algorithm are: frogs ,toad,litoria ,leptodactyli-\ndae,rana,lizard , and eleutherodactylus (Pennington et al.,\n2014).\nYet another visualization method is to use a clustering\nalgorithm to show a hierarchical representation of which\nwords are similar to others in the embedding space. The\nuncaptioned \ufb01gure on the left uses hierarchical clustering\nof some embedding vectors for nouns as a visualization",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 208,
      "token_count": 476,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 132",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 209,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "124 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\nmethod (Rohde et al., 2006).\nProbably the most common visualization method, how-\never, is to project the 100 dimensions of a word down into 2\ndimensions. Fig. 6.1 showed one such visualization, as does\nFig. 6.16, using a projection method called t-SNE (van der\nMaaten and Hinton, 2008).\n6.10 Semantic properties of embeddings\nIn this section we brie\ufb02y summarize some of the semantic properties of embeddings\nthat have been studied.\nDifferent types of similarity or association: One parameter of vector semantic\nmodels that is relevant to both sparse PPMI vectors and dense word2vec vectors is\nthe size of the context window used to collect counts. This is generally between 1\nand 10 words on each side of the target word (for a total context of 2-20 words).\nThe choice depends on the goals of the representation. Shorter context windows\ntend to lead to representations that are a bit more syntactic, since the information is\ncoming from immediately nearby words. When the vectors are computed from short\ncontext windows, the most similar words to a target word wtend to be semantically\nsimilar words with the same parts of speech. When vectors are computed from long\ncontext windows, the highest cosine words to a target word wtend to be words that\nare topically related but not similar.\nFor example Levy and Goldberg (2014a) showed that using skip-gram with a\nwindow of\u00062, the most similar words to the word Hogwarts (from the Harry Potter\nseries) were names of other \ufb01ctional schools: Sunnydale (from Buffy the Vampire\nSlayer ) orEvernight (from a vampire series). With a window of \u00065, the most similar\nwords to Hogwarts were other words topically related to the Harry Potter series:\nDumbledore ,Malfoy , and half-blood .\nIt\u2019s also often useful to distinguish two kinds of similarity or association between\nwords (Sch \u00a8utze and Pedersen, 1993). Two words have \ufb01rst-order co-occurrence\ufb01rst-order\nco-occurrence\n(sometimes called syntagmatic association ) if they are typically nearby each other.\nThus wrote is a \ufb01rst-order associate of book orpoem . Two words have second-order\nco-occurrence (sometimes called paradigmatic association ) if they have similarsecond-order\nco-occurrence\nneighbors. Thus wrote is a second-order associate of words like said orremarked .\nAnalogy/Relational Similarity: Another semantic property of embeddings is their\nability to capture relational meanings. In an important early vector space model of\ncognition, Rumelhart and Abrahamson (1973) proposed the parallelogram modelparallelogram\nmodel\nfor solving simple analogy problems of the form a is to b as a* is to what? . In\nsuch problems, a system is given a problem like apple:tree::grape:? , i.e., apple is\nto tree as grape is to , and must \ufb01ll in the word vine. In the parallelogram\nmodel, illustrated in Fig. 6.15, the vector from the word apple to the word tree(=#   \u0014tree\u0000#       \u0014apple) is added to the vector for grape (#        \u0014grape); the nearest word to that point\nis returned.\nIn early work with sparse embeddings, scholars showed that sparse vector mod-\nels of meaning could solve such analogy problems (Turney and Littman, 2005),\nbut the parallelogram method received more modern attention because of its suc-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 210,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "Thus wrote is a \ufb01rst-order associate of book orpoem . Two words have second-order\nco-occurrence (sometimes called paradigmatic association ) if they have similarsecond-order\nco-occurrence\nneighbors. Thus wrote is a second-order associate of words like said orremarked .\nAnalogy/Relational Similarity: Another semantic property of embeddings is their\nability to capture relational meanings. In an important early vector space model of\ncognition, Rumelhart and Abrahamson (1973) proposed the parallelogram modelparallelogram\nmodel\nfor solving simple analogy problems of the form a is to b as a* is to what? . In\nsuch problems, a system is given a problem like apple:tree::grape:? , i.e., apple is\nto tree as grape is to , and must \ufb01ll in the word vine. In the parallelogram\nmodel, illustrated in Fig. 6.15, the vector from the word apple to the word tree(=#   \u0014tree\u0000#       \u0014apple) is added to the vector for grape (#        \u0014grape); the nearest word to that point\nis returned.\nIn early work with sparse embeddings, scholars showed that sparse vector mod-\nels of meaning could solve such analogy problems (Turney and Littman, 2005),\nbut the parallelogram method received more modern attention because of its suc-\ncess with word2vec or GloVe vectors (Mikolov et al. 2013c, Levy and Goldberg\n2014b, Pennington et al. 2014). For example, the result of the expression#     \u0014king\u0000",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 211,
      "token_count": 348,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 133\n\n6.10 \u2022 S EMANTIC PROPERTIES OF EMBEDDINGS 125\ntreeapplegrapevine\nFigure 6.15 The parallelogram model for analogy problems (Rumelhart and Abrahamson,\n1973): the location of#     \u0014vine can be found by subtracting#       \u0014apple from#   \u0014tree and adding#       \u0014grape.\n#     \u0014man+#            \u0014woman is a vector close to#         \u0014queen. Similarly,#      \u0014Paris\u0000#           \u0014France +#     \u0014Italy results\nin a vector that is close to#         \u0014Rome. The embedding model thus seems to be extract-\ning representations of relations like MALE -FEMALE , or CAPITAL -CITY -OF, or even\nCOMPARATIVE /SUPERLATIVE , as shown in Fig. 6.16 from GloVe.\n(a) (b)\nFigure 6.16 Relational properties of the GloVe vector space, shown by projecting vectors onto two dimen-\nsions. (a)#     \u0014king\u0000#     \u0014man+#            \u0014woman is close to#        \u0014queen. (b) offsets seem to capture comparative and superlative\nmorphology (Pennington et al., 2014).\nFor a a:b::a\u0003:b\u0003problem, meaning the algorithm is given vectors a,b, and\na\u0003and must \ufb01nd b\u0003, the parallelogram method is thus:\n\u02c6b\u0003=argmin\nxdistance (x;b\u0000a+a\u0003) (6.41)\nwith some distance function, such as Euclidean distance.\nThere are some caveats. For example, the closest value returned by the paral-\nlelogram algorithm in word2vec or GloVe embedding spaces is usually not in fact\nb* but one of the 3 input words or their morphological variants (i.e., cherry:red ::\npotato:x returns potato orpotatoes instead of brown ), so these must be explicitly\nexcluded. Furthermore while embedding spaces perform well if the task involves\nfrequent words, small distances, and certain relations (like relating countries with\ntheir capitals or verbs/nouns with their in\ufb02ected forms), the parallelogram method\nwith embeddings doesn\u2019t work as well for other relations (Linzen 2016, Gladkova\net al. 2016, Schluter 2018, Ethayarajh et al. 2019a), and indeed Peterson et al. (2020)\nargue that the parallelogram method is in general too simple to model the human\ncognitive process of forming analogies of this kind.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 212,
      "token_count": 567,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 134",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 213,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "126 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\n6.10.1 Embeddings and Historical Semantics\nEmbeddings can also be a useful tool for studying how meaning changes over time,\nby computing multiple embedding spaces, each from texts written in a particular\ntime period. For example Fig. 6.17 shows a visualization of changes in meaning in\nEnglish words over the last two centuries, computed by building separate embed-\nding spaces for each decade from historical corpora like Google n-grams (Lin et al.,\n2012b) and the Corpus of Historical American English (Davies, 2012).\nCHAPTER 5. DYNAMIC SOCIAL REPRESENTATIONS OF WORD MEANING 79\nFigure 5.1: Two-dimensional visualization of semantic change in English using SGNS\nvectors (see Section 5.8 for the visualization algorithm). A,T h ew o r d gay shifted\nfrom meaning \u201ccheerful\u201d or \u201cfrolicsome\u201d to referring to homosexuality. A,I nt h ee a r l y\n20th century broadcast referred to \u201ccasting out seeds\u201d; with the rise of television and\nradio its meaning shifted to \u201ctransmitting signals\u201d. C,Awful underwent a process of\npejoration, as it shifted from meaning \u201cfull of awe\u201d to meaning \u201cterrible or appalling\u201d\n[212].\nthat adverbials (e.g., actually )h a v eag e n e r a lt e n d e n c yt ou n d e r g os u b j e c t i \ufb01 c a t i o n\nwhere they shift from objective statements about the world (e.g., \u201cSorry, the car is\nactually broken\u201d) to subjective statements (e.g., \u201cI can\u2019t believe he actually did that\u201d,\nindicating surprise/disbelief).\n5.2.2 Computational linguistic studies\nThere are also a number of recent works analyzing semantic change using computational\nmethods. [ 200] use latent semantic analysis to analyze how word meanings broaden\nand narrow over time. [ 113]u s er a wc o - o c c u r r e n c ev e c t o r st op e r f o r man u m b e ro f\nhistorical case-studies on semantic change, and [ 252] perform a similar set of small-\nscale case-studies using temporal topic models. [ 87]c o n s t r u c tp o i n t - w i s em u t u a l\ninformation-based embeddings and found that semantic changes uncovered by their\nmethod had reasonable agreement with human judgments. [ 129]a n d[ 119]u s e\u201c n e u r a l \u201d\nword-embedding methods to detect linguistic change points. Finally, [ 257]a n a l y z e\nhistorical co-occurrences to test whether synonyms tend to change in similar ways.\nFigure 6.17 A t-SNE visualization of the semantic change of 3 words in English using\nword2vec vectors. The modern sense of each word, and the grey context words, are com-\nputed from the most recent (modern) time-point embedding space. Earlier points are com-\nputed from earlier historical embedding spaces. The visualizations show the changes in the\nword gayfrom meanings related to \u201ccheerful\u201d or \u201cfrolicsome\u201d to referring to homosexuality,\nthe development of the modern \u201ctransmission\u201d sense of broadcast from its original sense of\nsowing seeds, and the pejoration of the word awful as it shifted from meaning \u201cfull of awe\u201d\nto meaning \u201cterrible or appalling\u201d (Hamilton et al., 2016b).\n6.11 Bias and Embeddings",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 214,
      "token_count": 774,
      "chapter_title": ""
    }
  },
  {
    "content": "scale case-studies using temporal topic models. [ 87]c o n s t r u c tp o i n t - w i s em u t u a l\ninformation-based embeddings and found that semantic changes uncovered by their\nmethod had reasonable agreement with human judgments. [ 129]a n d[ 119]u s e\u201c n e u r a l \u201d\nword-embedding methods to detect linguistic change points. Finally, [ 257]a n a l y z e\nhistorical co-occurrences to test whether synonyms tend to change in similar ways.\nFigure 6.17 A t-SNE visualization of the semantic change of 3 words in English using\nword2vec vectors. The modern sense of each word, and the grey context words, are com-\nputed from the most recent (modern) time-point embedding space. Earlier points are com-\nputed from earlier historical embedding spaces. The visualizations show the changes in the\nword gayfrom meanings related to \u201ccheerful\u201d or \u201cfrolicsome\u201d to referring to homosexuality,\nthe development of the modern \u201ctransmission\u201d sense of broadcast from its original sense of\nsowing seeds, and the pejoration of the word awful as it shifted from meaning \u201cfull of awe\u201d\nto meaning \u201cterrible or appalling\u201d (Hamilton et al., 2016b).\n6.11 Bias and Embeddings\nIn addition to their ability to learn word meaning from text, embeddings, alas,\nalso reproduce the implicit biases and stereotypes that were latent in the text. As\nthe prior section just showed, embeddings can roughly model relational similar-\nity: \u2018queen\u2019 as the closest word to \u2018king\u2019 - \u2018man\u2019 + \u2018woman\u2019 implies the analogy\nman:woman::king:queen . But these same embedding analogies also exhibit gender\nstereotypes. For example Bolukbasi et al. (2016) \ufb01nd that the closest occupation\nto \u2018computer programmer\u2019 - \u2018man\u2019 + \u2018woman\u2019 in word2vec embeddings trained on\nnews text is \u2018homemaker\u2019, and that the embeddings similarly suggest the analogy\n\u2018father\u2019 is to \u2018doctor\u2019 as \u2018mother\u2019 is to \u2018nurse\u2019. This could result in what Crawford\n(2017) and Blodgett et al. (2020) call an allocational harm , when a system allo-allocational\nharm\ncates resources (jobs or credit) unfairly to different groups. For example algorithms\nthat use embeddings as part of a search for hiring potential programmers or doctors\nmight thus incorrectly downweight documents with women\u2019s names.\nIt turns out that embeddings don\u2019t just re\ufb02ect the statistics of their input, but also\namplify bias; gendered terms become more gendered in embedding space than theybias\nampli\ufb01cation\nwere in the input text statistics (Zhao et al. 2017, Ethayarajh et al. 2019b, Jia et al.\n2020), and biases are more exaggerated than in actual labor employment statistics\n(Garg et al., 2018).\nEmbeddings also encode the implicit associations that are a property of human\nreasoning. The Implicit Association Test (Greenwald et al., 1998) measures peo-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 215,
      "token_count": 678,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 135",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 216,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "6.12 \u2022 E VALUATING VECTOR MODELS 127\nple\u2019s associations between concepts (like \u2018\ufb02owers\u2019 or \u2018insects\u2019) and attributes (like\n\u2018pleasantness\u2019 and \u2018unpleasantness\u2019) by measuring differences in the latency with\nwhich they label words in the various categories.7Using such methods, people\nin the United States have been shown to associate African-American names with\nunpleasant words (more than European-American names), male names more with\nmathematics and female names with the arts, and old people\u2019s names with unpleas-\nant words (Greenwald et al. 1998, Nosek et al. 2002a, Nosek et al. 2002b). Caliskan\net al. (2017) replicated all these \ufb01ndings of implicit associations using GloVe vectors\nand cosine similarity instead of human latencies. For example African-American\nnames like \u2018Leroy\u2019 and \u2018Shaniqua\u2019 had a higher GloVe cosine with unpleasant words\nwhile European-American names (\u2018Brad\u2019, \u2018Greg\u2019, \u2018Courtney\u2019) had a higher cosine\nwith pleasant words. These problems with embeddings are an example of a repre-\nsentational harm (Crawford 2017, Blodgett et al. 2020), which is a harm caused byrepresentational\nharm\na system demeaning or even ignoring some social groups. Any embedding-aware al-\ngorithm that made use of word sentiment could thus exacerbate bias against African\nAmericans.\nRecent research focuses on ways to try to remove these kinds of biases, for\nexample by developing a transformation of the embedding space that removes gen-\nder stereotypes but preserves de\ufb01nitional gender (Bolukbasi et al. 2016, Zhao et al.\n2017) or changing the training procedure (Zhao et al., 2018b). However, although\nthese sorts of debiasing may reduce bias in embeddings, they do not eliminate it debiasing\n(Gonen and Goldberg, 2019), and this remains an open problem.\nHistorical embeddings are also being used to measure biases in the past. Garg\net al. (2018) used embeddings from historical texts to measure the association be-\ntween embeddings for occupations and embeddings for names of various ethnici-\nties or genders (for example the relative cosine similarity of women\u2019s names versus\nmen\u2019s to occupation words like \u2018librarian\u2019 or \u2018carpenter\u2019) across the 20th century.\nThey found that the cosines correlate with the empirical historical percentages of\nwomen or ethnic groups in those occupations. Historical embeddings also repli-\ncated old surveys of ethnic stereotypes; the tendency of experimental participants in\n1933 to associate adjectives like \u2018industrious\u2019 or \u2018superstitious\u2019 with, e.g., Chinese\nethnicity, correlates with the cosine between Chinese last names and those adjectives\nusing embeddings trained on 1930s text. They also were able to document historical\ngender biases, such as the fact that embeddings for adjectives related to competence\n(\u2018smart\u2019, \u2018wise\u2019, \u2018thoughtful\u2019, \u2018resourceful\u2019) had a higher cosine with male than fe-\nmale words, and showed that this bias has been slowly decreasing since 1960. We\nreturn in later chapters to this question about the role of bias in natural language\nprocessing.\n6.12 Evaluating Vector Models\nThe most important evaluation metric for vector models is extrinsic evaluation on\ntasks, i.e., using vectors in an NLP task and seeing whether this improves perfor-\nmance over some other model.\n7Roughly speaking, if humans associate \u2018\ufb02owers\u2019 with \u2018pleasantness\u2019 and \u2018insects\u2019 with \u2018unpleasant-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 217,
      "token_count": 778,
      "chapter_title": ""
    }
  },
  {
    "content": "men\u2019s to occupation words like \u2018librarian\u2019 or \u2018carpenter\u2019) across the 20th century.\nThey found that the cosines correlate with the empirical historical percentages of\nwomen or ethnic groups in those occupations. Historical embeddings also repli-\ncated old surveys of ethnic stereotypes; the tendency of experimental participants in\n1933 to associate adjectives like \u2018industrious\u2019 or \u2018superstitious\u2019 with, e.g., Chinese\nethnicity, correlates with the cosine between Chinese last names and those adjectives\nusing embeddings trained on 1930s text. They also were able to document historical\ngender biases, such as the fact that embeddings for adjectives related to competence\n(\u2018smart\u2019, \u2018wise\u2019, \u2018thoughtful\u2019, \u2018resourceful\u2019) had a higher cosine with male than fe-\nmale words, and showed that this bias has been slowly decreasing since 1960. We\nreturn in later chapters to this question about the role of bias in natural language\nprocessing.\n6.12 Evaluating Vector Models\nThe most important evaluation metric for vector models is extrinsic evaluation on\ntasks, i.e., using vectors in an NLP task and seeing whether this improves perfor-\nmance over some other model.\n7Roughly speaking, if humans associate \u2018\ufb02owers\u2019 with \u2018pleasantness\u2019 and \u2018insects\u2019 with \u2018unpleasant-\nness\u2019, when they are instructed to push a green button for \u2018\ufb02owers\u2019 (daisy, iris, lilac) and \u2018pleasant words\u2019\n(love, laughter, pleasure) and a red button for \u2018insects\u2019 (\ufb02ea, spider, mosquito) and \u2018unpleasant words\u2019\n(abuse, hatred, ugly) they are faster than in an incongruous condition where they push a red button for\n\u2018\ufb02owers\u2019 and \u2018unpleasant words\u2019 and a green button for \u2018insects\u2019 and \u2018pleasant words\u2019.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 218,
      "token_count": 397,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 136",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 219,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "128 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\nNonetheless it is useful to have intrinsic evaluations. The most common metric\nis to test their performance on similarity , computing the correlation between an\nalgorithm\u2019s word similarity scores and word similarity ratings assigned by humans.\nWordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0\nto 10 for 353 noun pairs; for example ( plane ,car) had an average score of 5.77.\nSimLex-999 (Hill et al., 2015) is a more complex dataset that quanti\ufb01es similarity\n(cup, mug ) rather than relatedness ( cup, coffee ), and includes concrete and abstract\nadjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions, each\nconsisting of a target word with 4 additional word choices; the task is to choose\nwhich is the correct synonym, as in the example: Levied is closest in meaning to:\nimposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these\ndatasets present words without context.\nSlightly more realistic are intrinsic similarity tasks that include context. The\nStanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) and the\nWord-in-Context (WiC) dataset (Pilehvar and Camacho-Collados, 2019) offer richer\nevaluation scenarios. SCWS gives human judgments on 2,003 pairs of words in\ntheir sentential context, while WiC gives target words in two sentential contexts that\nare either in the same or different senses; see Appendix G. The semantic textual\nsimilarity task (Agirre et al. 2012, Agirre et al. 2015) evaluates the performance of\nsentence-level similarity algorithms, consisting of a set of pairs of sentences, each\npair with human-labeled similarity scores.\nAnother task used for evaluation is the analogy task, discussed on page 124,\nwhere the system has to solve problems of the form a is to b as a* is to b* , given a, b,\nanda*and having to \ufb01nd b*(Turney and Littman, 2005). A number of sets of tuples\nhave been created for this task (Mikolov et al. 2013a, Mikolov et al. 2013c, Gladkova\net al. 2016), covering morphology ( city:cities::child:children ), lexicographic rela-\ntions ( leg:table::spout:teapot ) and encyclopedia relations ( Beijing:China::Dublin:Ireland ),\nsome drawing from the SemEval-2012 Task 2 dataset of 79 different relations (Jur-\ngens et al., 2012).\nAll embedding algorithms suffer from inherent variability. For example because\nof randomness in the initialization and the random negative sampling, algorithms\nlike word2vec may produce different results even from the same dataset, and in-\ndividual documents in a collection may strongly impact the resulting embeddings\n(Tian et al. 2016, Hellrich and Hahn 2016, Antoniak and Mimno 2018). When em-\nbeddings are used to study word associations in particular corpora, therefore, it is\nbest practice to train multiple embeddings with bootstrap sampling over documents\nand average the results (Antoniak and Mimno, 2018).\n6.13 Summary\n\u2022 In vector semantics, a word is modeled as a vector\u2014a point in high-dimensional\nspace, also called an embedding . In this chapter we focus on static embed-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 220,
      "token_count": 776,
      "chapter_title": ""
    }
  },
  {
    "content": "have been created for this task (Mikolov et al. 2013a, Mikolov et al. 2013c, Gladkova\net al. 2016), covering morphology ( city:cities::child:children ), lexicographic rela-\ntions ( leg:table::spout:teapot ) and encyclopedia relations ( Beijing:China::Dublin:Ireland ),\nsome drawing from the SemEval-2012 Task 2 dataset of 79 different relations (Jur-\ngens et al., 2012).\nAll embedding algorithms suffer from inherent variability. For example because\nof randomness in the initialization and the random negative sampling, algorithms\nlike word2vec may produce different results even from the same dataset, and in-\ndividual documents in a collection may strongly impact the resulting embeddings\n(Tian et al. 2016, Hellrich and Hahn 2016, Antoniak and Mimno 2018). When em-\nbeddings are used to study word associations in particular corpora, therefore, it is\nbest practice to train multiple embeddings with bootstrap sampling over documents\nand average the results (Antoniak and Mimno, 2018).\n6.13 Summary\n\u2022 In vector semantics, a word is modeled as a vector\u2014a point in high-dimensional\nspace, also called an embedding . In this chapter we focus on static embed-\ndings , where each word is mapped to a \ufb01xed embedding.\n\u2022 Vector semantic models fall into two classes: sparse anddense . In sparse\nmodels each dimension corresponds to a word in the vocabulary Vand cells\nare functions of co-occurrence counts . The term-document matrix has a\nrow for each word ( term ) in the vocabulary and a column for each document.\nTheword-context orterm-term matrix has a row for each (target) word in",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 221,
      "token_count": 378,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 137\n\nBIBLIOGRAPHICAL AND HISTORICAL NOTES 129\nthe vocabulary and a column for each context term in the vocabulary. Two\nsparse weightings are common: the tf-idf weighting which weights each cell\nby its term frequency andinverse document frequency , and PPMI (point-\nwise positive mutual information), which is most common for word-context\nmatrices.\n\u2022 Dense vector models have dimensionality 50\u20131000. Word2vec algorithms\nlikeskip-gram are a popular way to compute dense embeddings. Skip-gram\ntrains a logistic regression classi\ufb01er to compute the probability that two words\nare \u2018likely to occur nearby in text\u2019. This probability is computed from the dot\nproduct between the embeddings for the two words.\n\u2022 Skip-gram uses stochastic gradient descent to train the classi\ufb01er, by learning\nembeddings that have a high dot product with embeddings of words that occur\nnearby and a low dot product with noise words.\n\u2022 Other important embedding algorithms include GloVe , a method based on\nratios of word co-occurrence probabilities.\n\u2022 Whether using sparse or dense vectors, word and document similarities are\ncomputed by some function of the dot product between vectors. The cosine\nof two vectors\u2014a normalized dot product\u2014is the most popular such metric.\nBibliographical and Historical Notes\nThe idea of vector semantics arose out of research in the 1950s in three distinct\n\ufb01elds: linguistics, psychology, and computer science, each of which contributed a\nfundamental aspect of the model.\nThe idea that meaning is related to the distribution of words in context was\nwidespread in linguistic theory of the 1950s, among distributionalists like Zellig\nHarris, Martin Joos, and J. R. Firth, and semioticians like Thomas Sebeok. As Joos\n(1950) put it,\nthe linguist\u2019s \u201cmeaning\u201d of a morpheme. . . is by de\ufb01nition the set of conditional\nprobabilities of its occurrence in context with all other morphemes.\nThe idea that the meaning of a word might be modeled as a point in a multi-\ndimensional semantic space came from psychologists like Charles E. Osgood, who\nhad been studying how people responded to the meaning of words by assigning val-\nues along scales like happy/sad orhard/soft . Osgood et al. (1957) proposed that the\nmeaning of a word in general could be modeled as a point in a multidimensional\nEuclidean space, and that the similarity of meaning between two words could be\nmodeled as the distance between these points in the space.\nA \ufb01nal intellectual source in the 1950s and early 1960s was the \ufb01eld then called\nmechanical indexing , now known as information retrieval . In what became knownmechanical\nindexing\nas the vector space model for information retrieval (Salton 1971, Sparck Jones\n1986), researchers demonstrated new ways to de\ufb01ne the meaning of words in terms\nof vectors (Switzer, 1965), and re\ufb01ned methods for word similarity based on mea-\nsures of statistical association between words like mutual information (Giuliano,\n1965) and idf (Sparck Jones, 1972), and showed that the meaning of documents\ncould be represented in the same vector spaces used for words. Around the same\ntime, (Cordier, 1965) showed that factor analysis of word association probabilities\ncould be used to form dense vector representations of words.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 222,
      "token_count": 755,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 138",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 223,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "130 CHAPTER 6 \u2022 V ECTOR SEMANTICS AND EMBEDDINGS\nSome of the philosophical underpinning of the distributional way of thinking\ncame from the late writings of the philosopher Wittgenstein, who was skeptical of\nthe possibility of building a completely formal theory of meaning de\ufb01nitions for\neach word. Wittgenstein suggested instead that \u201cthe meaning of a word is its use in\nthe language\u201d (Wittgenstein, 1953, PI 43). That is, instead of using some logical lan-\nguage to de\ufb01ne each word, or drawing on denotations or truth values, Wittgenstein\u2019s\nidea is that we should de\ufb01ne a word by how it is used by people in speaking and un-\nderstanding in their day-to-day interactions, thus pre\ufb01guring the movement toward\nembodied and experiential models in linguistics and NLP (Glenberg and Robertson\n2000, Lake and Murphy 2021, Bisk et al. 2020, Bender and Koller 2020).\nMore distantly related is the idea of de\ufb01ning words by a vector of discrete fea-\ntures, which has roots at least as far back as Descartes and Leibniz (Wierzbicka 1992,\nWierzbicka 1996). By the middle of the 20th century, beginning with the work of\nHjelmslev (Hjelmslev, 1969) (originally 1943) and \ufb02eshed out in early models of\ngenerative grammar (Katz and Fodor, 1963), the idea arose of representing mean-\ning with semantic features , symbols that represent some sort of primitive meaning.semantic\nfeature\nFor example words like hen,rooster , orchick , have something in common (they all\ndescribe chickens) and something different (their age and sex), representable as:\nhen+female, +chicken, +adult\nrooster-female, +chicken, +adult\nchick+chicken, -adult\nThe dimensions used by vector models of meaning to de\ufb01ne words, however, are\nonly abstractly related to this idea of a small \ufb01xed number of hand-built dimensions.\nNonetheless, there has been some attempt to show that certain dimensions of em-\nbedding models do contribute some speci\ufb01c compositional aspect of meaning like\nthese early semantic features.\nThe use of dense vectors to model word meaning, and indeed the term embed-\nding , grew out of the latent semantic indexing (LSI) model (Deerwester et al.,\n1988) recast as LSA (latent semantic analysis ) (Deerwester et al., 1990). In LSA\nsingular value decomposition \u2014SVD \u2014 is applied to a term-document matrix (each SVD\ncell weighted by log frequency and normalized by entropy), and then the \ufb01rst 300\ndimensions are used as the LSA embedding. Singular Value Decomposition (SVD)\nis a method for \ufb01nding the most important dimensions of a data set, those dimen-\nsions along which the data varies the most. LSA was then quickly widely applied:\nas a cognitive model Landauer and Dumais (1997), and for tasks like spell checking\n(Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and Ju-\nrafsky 1998, Bellegarda 2000), morphology induction (Schone and Jurafsky 2000,\nSchone and Jurafsky 2001b), multiword expressions (MWEs) (Schone and Juraf-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 224,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "bedding models do contribute some speci\ufb01c compositional aspect of meaning like\nthese early semantic features.\nThe use of dense vectors to model word meaning, and indeed the term embed-\nding , grew out of the latent semantic indexing (LSI) model (Deerwester et al.,\n1988) recast as LSA (latent semantic analysis ) (Deerwester et al., 1990). In LSA\nsingular value decomposition \u2014SVD \u2014 is applied to a term-document matrix (each SVD\ncell weighted by log frequency and normalized by entropy), and then the \ufb01rst 300\ndimensions are used as the LSA embedding. Singular Value Decomposition (SVD)\nis a method for \ufb01nding the most important dimensions of a data set, those dimen-\nsions along which the data varies the most. LSA was then quickly widely applied:\nas a cognitive model Landauer and Dumais (1997), and for tasks like spell checking\n(Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and Ju-\nrafsky 1998, Bellegarda 2000), morphology induction (Schone and Jurafsky 2000,\nSchone and Jurafsky 2001b), multiword expressions (MWEs) (Schone and Juraf-\nsky, 2001a), and essay grading (Rehder et al., 1998). Related models were simul-\ntaneously developed and applied to word sense disambiguation by Sch \u00a8utze (1992b).\nLSA also led to the earliest use of embeddings to represent words in a probabilis-\ntic classi\ufb01er, in the logistic regression document router of Sch \u00a8utze et al. (1995).\nThe idea of SVD on the term-term matrix (rather than the term-document matrix)\nas a model of meaning for NLP was proposed soon after LSA by Sch \u00a8utze (1992b).\nSch\u00a8utze applied the low-rank (97-dimensional) embeddings produced by SVD to the\ntask of word sense disambiguation, analyzed the resulting semantic space, and also\nsuggested possible techniques like dropping high-order dimensions. See Sch \u00a8utze\n(1997).\nA number of alternative matrix models followed on from the early SVD work,\nincluding Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Latent",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 225,
      "token_count": 517,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 139\n\nEXERCISES 131\nDirichlet Allocation (LDA) (Blei et al., 2003), and Non-negative Matrix Factoriza-\ntion (NMF) (Lee and Seung, 1999).\nThe LSA community seems to have \ufb01rst used the word \u201cembedding\u201d in Landauer\net al. (1997), in a variant of its mathematical meaning as a mapping from one space\nor mathematical structure to another. In LSA, the word embedding seems to have\ndescribed the mapping from the space of sparse count vectors to the latent space of\nSVD dense vectors. Although the word thus originally meant the mapping from one\nspace to another, it has metonymically shifted to mean the resulting dense vector in\nthe latent space, and it is in this sense that we currently use the word.\nBy the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that\nneural language models could also be used to develop embeddings as part of the task\nof word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and\nCollobert et al. (2011) then demonstrated that embeddings could be used to represent\nword meanings for a number of NLP tasks. Turian et al. (2010) compared the value\nof different kinds of embeddings for different NLP tasks. Mikolov et al. (2011)\nshowed that recurrent neural nets could be used as language models. The idea of\nsimplifying the hidden layer of these neural net language models to create the skip-\ngram (and also CBOW) algorithms was proposed by Mikolov et al. (2013a). The\nnegative sampling training algorithm was proposed in Mikolov et al. (2013b). There\nare numerous surveys of static embeddings and their parameterizations (Bullinaria\nand Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark\n2014, Levy et al. 2015).\nSee Manning et al. (2008) and Chapter 14 for a deeper understanding of the role\nof vectors in information retrieval, including how to compare queries with docu-\nments, more details on tf-idf, and issues of scaling to very large datasets. See Kim\n(2019) for a clear and comprehensive tutorial on word2vec. Cruse (2004) is a useful\nintroductory linguistic text on lexical semantics.\nExercises",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 226,
      "token_count": 537,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 140\n\n132 CHAPTER 7 \u2022 N EURAL NETWORKS\nCHAPTER\n7Neural Networks\n\u201c[M]achines of this character can behave in a very complicated manner when\nthe number of units is large.\u201d\nAlan Turing (1948) \u201cIntelligent Machines\u201d, page 6\nNeural networks are a fundamental computational tool for language process-\ning, and a very old one. They are called neural because their origins lie in the\nMcCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simpli\ufb01ed model of the\nbiological neuron as a kind of computing element that could be described in terms\nof propositional logic. But the modern use in language processing no longer draws\non these early biological inspirations.\nInstead, a modern neural network is a network of small computing units, each\nof which takes a vector of input values and produces a single output value. In this\nchapter we introduce the neural net applied to classi\ufb01cation. The architecture we\nintroduce is called a feedforward network because the computation proceeds iter- feedforward\natively from one layer of units to the next. The use of modern neural nets is often\ncalled deep learning , because modern networks are often deep (have many layers). deep learning\nNeural networks share much of the same mathematics as logistic regression. But\nneural networks are a more powerful classi\ufb01er than logistic regression, and indeed a\nminimal neural network (technically one with a single \u2018hidden layer\u2019) can be shown\nto learn any function.\nNeural net classi\ufb01ers are different from logistic regression in another way. With\nlogistic regression, we applied the regression classi\ufb01er to many different tasks by\ndeveloping many rich kinds of feature templates based on domain knowledge. When\nworking with neural networks, it is more common to avoid most uses of rich hand-\nderived features, instead building neural networks that take raw words as inputs\nand learn to induce features as part of the process of learning to classify. We saw\nexamples of this kind of representation learning for embeddings in Chapter 6. Nets\nthat are very deep are particularly good at representation learning. For that reason\ndeep neural nets are the right tool for tasks that offer suf\ufb01cient data to learn features\nautomatically.\nIn this chapter we\u2019ll introduce feedforward networks as classi\ufb01ers, and also ap-\nply them to the simple task of language modeling: assigning probabilities to word\nsequences and predicting upcoming words. In subsequent chapters we\u2019ll introduce\nmany other aspects of neural models, such as recurrent neural networks (Chap-\nter 8), the Transformer (Chapter 9), and masked language modeling (Chapter 11).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 227,
      "token_count": 565,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 141\n\n7.1 \u2022 U NITS 133\n7.1 Units\nThe building block of a neural network is a single computational unit. A unit takes\na set of real valued numbers as input, performs some computation on them, and\nproduces an output.\nAt its heart, a neural unit is taking a weighted sum of its inputs, with one addi-\ntional term in the sum called a bias term . Given a set of inputs x1:::xn, a unit has bias term\na set of corresponding weights w1:::wnand a bias b, so the weighted sum zcan be\nrepresented as:\nz=b+X\niwixi (7.1)\nOften it\u2019s more convenient to express this weighted sum using vector notation; recall\nfrom linear algebra that a vector is, at heart, just a list or array of numbers. Thus vector\nwe\u2019ll talk about zin terms of a weight vector w, a scalar bias b, and an input vector\nx, and we\u2019ll replace the sum with the convenient dot product :\nz=w\u0001x+b (7.2)\nAs de\ufb01ned in Eq. 7.2, zis just a real valued number.\nFinally, instead of using z, a linear function of x, as the output, neural units\napply a non-linear function ftoz. We will refer to the output of this function as\ntheactivation value for the unit, a. Since we are just modeling a single unit, the activation\nactivation for the node is in fact the \ufb01nal output of the network, which we\u2019ll generally\ncally. So the value yis de\ufb01ned as:\ny=a=f(z)\nWe\u2019ll discuss three popular non-linear functions fbelow (the sigmoid, the tanh, and\nthe recti\ufb01ed linear unit or ReLU) but it\u2019s pedagogically convenient to start with the\nsigmoid function since we saw it in Chapter 5: sigmoid\ny=s(z) =1\n1+e\u0000z(7.3)\nThe sigmoid (shown in Fig. 7.1) has a number of advantages; it maps the output\ninto the range (0;1), which is useful in squashing outliers toward 0 or 1. And it\u2019s\ndifferentiable, which as we saw in Section 5.10 will be handy for learning.\nFigure 7.1 The sigmoid function takes a real value and maps it to the range (0;1). It is\nnearly linear around 0 but outlier values get squashed toward 0 or 1.\nSubstituting Eq. 7.2 into Eq. 7.3 gives us the output of a neural unit:\ny=s(w\u0001x+b) =1\n1+exp(\u0000(w\u0001x+b))(7.4)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 228,
      "token_count": 591,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 142\n\n134 CHAPTER 7 \u2022 N EURAL NETWORKS\nFig. 7.2 shows a \ufb01nal schematic of a basic neural unit. In this example the unit\ntakes 3 input values x1;x2, and x3, and computes a weighted sum, multiplying each\nvalue by a weight ( w1,w2, and w3, respectively), adds them to a bias term b, and then\npasses the resulting sum through a sigmoid function to result in a number between 0\nand 1.\nx1x2x3\nyw1w2w3\u2211b\u03c3+1za\nFigure 7.2 A neural unit, taking 3 inputs x1,x2, and x3(and a bias bthat we represent as a\nweight for an input clamped at +1) and producing an output y. We include some convenient\nintermediate variables: the output of the summation, z, and the output of the sigmoid, a. In\nthis case the output of the unit yis the same as a, but in deeper networks we\u2019ll reserve yto\nmean the \ufb01nal output of the entire network, leaving aas the activation of an individual node.\nLet\u2019s walk through an example just to get an intuition. Let\u2019s suppose we have a\nunit with the following weight vector and bias:\nw= [0:2;0:3;0:9]\nb=0:5\nWhat would this unit do with the following input vector:\nx= [0:5;0:6;0:1]\nThe resulting output ywould be:\ny=s(w\u0001x+b) =1\n1+e\u0000(w\u0001x+b)=1\n1+e\u0000(:5\u0003:2+:6\u0003:3+:1\u0003:9+:5)=1\n1+e\u00000:87=:70\nIn practice, the sigmoid is not commonly used as an activation function. A function\nthat is very similar but almost always better is the tanh function shown in Fig. 7.3a; tanh\ntanh is a variant of the sigmoid that ranges from -1 to +1:\ny=tanh(z) =ez\u0000e\u0000z\nez+e\u0000z(7.5)\nThe simplest activation function, and perhaps the most commonly used, is the rec-\nti\ufb01ed linear unit, also called the ReLU , shown in Fig. 7.3b. It\u2019s just the same as z ReLU\nwhen zis positive, and 0 otherwise:\ny=ReLU(z) =max(z;0) (7.6)\nThese activation functions have different properties that make them useful for differ-\nent language applications or network architectures. For example, the tanh function\nhas the nice properties of being smoothly differentiable and mapping outlier values\ntoward the mean. The recti\ufb01er function, on the other hand, has nice properties that",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 229,
      "token_count": 612,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 143\n\n7.2 \u2022 T HEXOR PROBLEM 135\n(a) (b)\nFigure 7.3 The tanh and ReLU activation functions.\nresult from it being very close to linear. In the sigmoid or tanh functions, very high\nvalues of zresult in values of ythat are saturated , i.e., extremely close to 1, and have saturated\nderivatives very close to 0. Zero derivatives cause problems for learning, because as\nwe\u2019ll see in Section 7.5, we\u2019ll train networks by propagating an error signal back-\nwards, multiplying gradients (partial derivatives) from each layer of the network;\ngradients that are almost 0 cause the error signal to get smaller and smaller until it is\ntoo small to be used for training, a problem called the vanishing gradient problem.vanishing\ngradient\nRecti\ufb01ers don\u2019t have this problem, since the derivative of ReLU for high values of z\nis 1 rather than very close to 0.\n7.2 The XOR problem\nEarly in the history of neural networks it was realized that the power of neural net-\nworks, as with the real neurons that inspired them, comes from combining these\nunits into larger networks.\nOne of the most clever demonstrations of the need for multi-layer networks was\nthe proof by Minsky and Papert (1969) that a single neural unit cannot compute\nsome very simple functions of its input. Consider the task of computing elementary\nlogical functions of two inputs, like AND, OR, and XOR. As a reminder, here are\nthe truth tables for those functions:\nAND OR XOR\nx1 x2y x1 x2 y x1 x2 y\n0 00 0 0 0 0 0 0\n0 10 0 1 1 0 1 1\n1 00 1 0 1 1 0 1\n1 11 1 1 1 1 1 0\nThis example was \ufb01rst shown for the perceptron , which is a very simple neural perceptron\nunit that has a binary output and has a very simple step function as its non-linear\nactivation function. The output yof a perceptron is 0 or 1, and is computed as\nfollows (using the same weight w, input x, and bias bas in Eq. 7.2):\ny=\u001a0;ifw\u0001x+b\u00140\n1;ifw\u0001x+b>0(7.7)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 230,
      "token_count": 528,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 144",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 231,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "136 CHAPTER 7 \u2022 N EURAL NETWORKS\nIt\u2019s very easy to build a perceptron that can compute the logical AND and OR\nfunctions of its binary inputs; Fig. 7.4 shows the necessary weights.\nx1x2+1-111\nx1x2+1011\n(a) (b)\nFigure 7.4 The weights wand bias bfor perceptrons for computing logical functions. The\ninputs are shown as x1andx2and the bias as a special node with value +1 which is multiplied\nwith the bias weight b. (a) logical AND, with weights w1=1 and w2=1 and bias weight\nb=\u00001. (b) logical OR, with weights w1=1 and w2=1 and bias weight b=0. These\nweights/biases are just one from an in\ufb01nite number of possible sets of weights and biases that\nwould implement the functions.\nIt turns out, however, that it\u2019s not possible to build a perceptron to compute\nlogical XOR! (It\u2019s worth spending a moment to give it a try!)\nThe intuition behind this important result relies on understanding that a percep-\ntron is a linear classi\ufb01er. For a two-dimensional input x1andx2, the perceptron\nequation, w1x1+w2x2+b=0 is the equation of a line. (We can see this by putting\nit in the standard linear format: x2= (\u0000w1=w2)x1+ (\u0000b=w2).) This line acts as a\ndecision boundary in two-dimensional space in which the output 0 is assigned to alldecision\nboundary\ninputs lying on one side of the line, and the output 1 to all input points lying on the\nother side of the line. If we had more than 2 inputs, the decision boundary becomes\na hyperplane instead of a line, but the idea is the same, separating the space into two\ncategories.\nFig. 7.5 shows the possible logical inputs ( 00,01,10, and11) and the line drawn\nby one possible set of parameters for an AND and an OR classi\ufb01er. Notice that there\nis simply no way to draw a line that separates the positive cases of XOR (01 and 10)\nfrom the negative cases (00 and 11). We say that XOR is not a linearly separablelinearly\nseparable\nfunction. Of course we could draw a boundary with a curve, or some other function,\nbut not a single line.\n7.2.1 The solution: neural networks\nWhile the XOR function cannot be calculated by a single perceptron, it can be cal-\nculated by a layered network of perceptron units. Rather than see this with networks\nof simple perceptrons, however, let\u2019s see how to compute XOR using two layers of\nReLU-based units following Goodfellow et al. (2016). Fig. 7.6 shows a \ufb01gure with\nthe input being processed by two layers of neural units. The middle layer (called\nh) has two units, and the output layer (called y) has one unit. A set of weights and\nbiases are shown that allows the network to correctly compute the XOR function.\nLet\u2019s walk through what happens with the input x= [0, 0]. If we multiply each\ninput value by the appropriate weight, sum, and then add the bias b, we get the vector\n[0, -1], and we then apply the recti\ufb01ed linear transformation to give the output of the\nhlayer as [0, 0]. Now we once again multiply by the weights, sum, and add the",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 232,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "from the negative cases (00 and 11). We say that XOR is not a linearly separablelinearly\nseparable\nfunction. Of course we could draw a boundary with a curve, or some other function,\nbut not a single line.\n7.2.1 The solution: neural networks\nWhile the XOR function cannot be calculated by a single perceptron, it can be cal-\nculated by a layered network of perceptron units. Rather than see this with networks\nof simple perceptrons, however, let\u2019s see how to compute XOR using two layers of\nReLU-based units following Goodfellow et al. (2016). Fig. 7.6 shows a \ufb01gure with\nthe input being processed by two layers of neural units. The middle layer (called\nh) has two units, and the output layer (called y) has one unit. A set of weights and\nbiases are shown that allows the network to correctly compute the XOR function.\nLet\u2019s walk through what happens with the input x= [0, 0]. If we multiply each\ninput value by the appropriate weight, sum, and then add the bias b, we get the vector\n[0, -1], and we then apply the recti\ufb01ed linear transformation to give the output of the\nhlayer as [0, 0]. Now we once again multiply by the weights, sum, and add the\nbias (0 in this case) resulting in the value 0. The reader should work through the\ncomputation of the remaining 3 possible input pairs to see that the resulting yvalues\nare 1 for the inputs [0, 1] and [1, 0] and 0 for [0, 0] and [1, 1].",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 233,
      "token_count": 368,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 145\n\n7.2 \u2022 T HEXOR PROBLEM 137\n0011x1x20011x1x20011x1x2\na)  x1 AND x2b)  x1 OR x2c)  x1 XOR x2?\nFigure 7.5 The functions AND, OR, and XOR, represented with input x1on the x-axis and input x2on the\ny-axis. Filled circles represent perceptron outputs of 1, and white circles perceptron outputs of 0. There is no\nway to draw a line that correctly separates the two categories for XOR. Figure styled after Russell and Norvig\n(2002).\nx1x2h1h2y1+11-1111-201+10\nFigure 7.6 XOR solution after Goodfellow et al. (2016). There are three ReLU units, in\ntwo layers; we\u2019ve called them h1,h2(hfor \u201chidden layer\u201d) and y1. As before, the numbers\non the arrows represent the weights wfor each unit, and we represent the bias bas a weight\non a unit clamped to +1, with the bias weights/units in gray.\nIt\u2019s also instructive to look at the intermediate results, the outputs of the two\nhidden nodes h1andh2. We showed in the previous paragraph that the hvector for\nthe inputs x= [0, 0] was [0, 0]. Fig. 7.7b shows the values of the hlayer for all\n4 inputs. Notice that hidden representations of the two input points x= [0, 1] and\nx= [1, 0] (the two cases with XOR output = 1) are merged to the single point h=\n[1, 0]. The merger makes it easy to linearly separate the positive and negative cases\nof XOR. In other words, we can view the hidden layer of the network as forming a\nrepresentation of the input.\nIn this example we just stipulated the weights in Fig. 7.6. But for real examples\nthe weights for neural networks are learned automatically using the error backprop-\nagation algorithm to be introduced in Section 7.5. That means the hidden layers will\nlearn to form useful representations. This intuition, that neural networks can auto-\nmatically learn useful representations of the input, is one of their key advantages,\nand one that we will return to again and again in later chapters.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 234,
      "token_count": 522,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 146\n\n138 CHAPTER 7 \u2022 N EURAL NETWORKS\n0011x1x2\na) The original x space0011h1h2\n2b) The new (linearly separable) h space\nFigure 7.7 The hidden layer forming a new representation of the input. (b) shows the\nrepresentation of the hidden layer, h, compared to the original input representation xin (a).\nNotice that the input point [0, 1] has been collapsed with the input point [1, 0], making it\npossible to linearly separate the positive and negative cases of XOR. After Goodfellow et al.\n(2016).\n7.3 Feedforward Neural Networks\nLet\u2019s now walk through a slightly more formal presentation of the simplest kind of\nneural network, the feedforward network . A feedforward network is a multilayerfeedforward\nnetwork\nnetwork in which the units are connected with no cycles; the outputs from units in\neach layer are passed to units in the next higher layer, and no outputs are passed\nback to lower layers. (In Chapter 8 we\u2019ll introduce networks with cycles, called\nrecurrent neural networks .)\nFor historical reasons multilayer networks, especially feedforward networks, are\nsometimes called multi-layer perceptrons (orMLP s); this is a technical misnomer,multi-layer\nperceptrons\nMLP since the units in modern multilayer networks aren\u2019t perceptrons (perceptrons have a\nsimple step-function as their activation function, but modern networks are made up\nof units with many kinds of non-linearities like ReLUs and sigmoids), but at some\npoint the name stuck.\nSimple feedforward networks have three kinds of nodes: input units, hidden\nunits, and output units.\nFig. 7.8 shows a picture. The input layer xis a vector of simple scalar values just\nas we saw in Fig. 7.2.\nThe core of the neural network is the hidden layer hformed of hidden units hi, hidden layer\neach of which is a neural unit as described in Section 7.1, taking a weighted sum of\nits inputs and then applying a non-linearity. In the standard architecture, each layer\nisfully-connected , meaning that each unit in each layer takes as input the outputs fully-connected\nfrom all the units in the previous layer, and there is a link between every pair of units\nfrom two adjacent layers. Thus each hidden unit sums over all the input units.\nRecall that a single hidden unit has as parameters a weight vector and a bias. We\nrepresent the parameters for the entire hidden layer by combining the weight vector\nand bias for each unit iinto a single weight matrix Wand a single bias vector bfor\nthe whole layer (see Fig. 7.8). Each element Wjiof the weight matrix Wrepresents\nthe weight of the connection from the ith input unit xito the jth hidden unit hj.\nThe advantage of using a single matrix Wfor the weights of the entire layer is\nthat now the hidden layer computation for a feedforward network can be done very",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 235,
      "token_count": 657,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 147\n\n7.3 \u2022 F EEDFORWARD NEURAL NETWORKS 139\nx1x2xn0\u2026\u2026+1b\u2026UW\ninput layerhidden layeroutput layerh1y1y2yn2h2h3hn1\nFigure 7.8 A simple 2-layer feedforward network, with one hidden layer, one output layer,\nand one input layer (the input layer is usually not counted when enumerating layers).\nef\ufb01ciently with simple matrix operations. In fact, the computation only has three\nsteps: multiplying the weight matrix by the input vector x, adding the bias vector b,\nand applying the activation function g(such as the sigmoid, tanh, or ReLU activation\nfunction de\ufb01ned above).\nThe output of the hidden layer, the vector h, is thus the following (for this exam-\nple we\u2019ll use the sigmoid function sas our activation function):\nh=s(Wx+b) (7.8)\nNotice that we\u2019re applying the sfunction here to a vector, while in Eq. 7.3 it was\napplied to a scalar. We\u2019re thus allowing s(\u0001), and indeed any activation function\ng(\u0001), to apply to a vector element-wise, so g[z1;z2;z3] = [g(z1);g(z2);g(z3)].\nLet\u2019s introduce some constants to represent the dimensionalities of these vectors\nand matrices. We\u2019ll refer to the input layer as layer 0 of the network, and have n0\nrepresent the number of inputs, so xis a vector of real numbers of dimension n0,\nor more formally x2Rn0, a column vector of dimensionality [n0;1]. Let\u2019s call the\nhidden layer layer 1 and the output layer layer 2. The hidden layer has dimensional-\nityn1, soh2Rn1and also b2Rn1(since each hidden unit can take a different bias\nvalue). And the weight matrix Whas dimensionality W2Rn1\u0002n0, i.e.[n1;n0].\nTake a moment to convince yourself that the matrix multiplication in Eq. 7.8 will\ncompute the value of each hjass\u0000Pn0\ni=1Wjixi+bj\u0001\n.\nAs we saw in Section 7.2, the resulting value h(forhidden but also for hypoth-\nesis) forms a representation of the input. The role of the output layer is to take\nthis new representation hand compute a \ufb01nal output. This output could be a real-\nvalued number, but in many cases the goal of the network is to make some sort of\nclassi\ufb01cation decision, and so we will focus on the case of classi\ufb01cation.\nIf we are doing a binary task like sentiment classi\ufb01cation, we might have a sin-\ngle output node, and its scalar value yis the probability of positive versus negative\nsentiment. If we are doing multinomial classi\ufb01cation, such as assigning a part-of-\nspeech tag, we might have one output node for each potential part-of-speech, whose\noutput value is the probability of that part-of-speech, and the values of all the output\nnodes must sum to one. The output layer is thus a vector ythat gives a probability\ndistribution across the output nodes.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 236,
      "token_count": 707,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 148\n\n140 CHAPTER 7 \u2022 N EURAL NETWORKS\nLet\u2019s see how this happens. Like the hidden layer, the output layer has a weight\nmatrix (let\u2019s call it U), but some models don\u2019t include a bias vector bin the output\nlayer, so we\u2019ll simplify by eliminating the bias vector in this example. The weight\nmatrix is multiplied by its input vector ( h) to produce the intermediate output z:\nz=Uh\nThere are n2output nodes, so z2Rn2, weight matrix Uhas dimensionality U2\nRn2\u0002n1, and element Ui jis the weight from unit jin the hidden layer to unit iin the\noutput layer.\nHowever, zcan\u2019t be the output of the classi\ufb01er, since it\u2019s a vector of real-valued\nnumbers, while what we need for classi\ufb01cation is a vector of probabilities. There is\na convenient function for normalizing a vector of real values, by which we mean normalizing\nconverting it to a vector that encodes a probability distribution (all the numbers lie\nbetween 0 and 1 and sum to 1): the softmax function that we saw on page 85 of softmax\nChapter 5. More generally for any vector zof dimensionality d, the softmax is\nde\ufb01ned as:\nsoftmax (zi) =exp(zi)Pd\nj=1exp(zj)1\u0014i\u0014d (7.9)\nThus for example given a vector\nz= [0:6;1:1;\u00001:5;1:2;3:2;\u00001:1]; (7.10)\nthe softmax function will normalize it to a probability distribution (shown rounded):\nsoftmax (z) = [ 0:055;0:090;0:0067;0:10;0:74;0:010] (7.11)\nYou may recall that we used softmax to create a probability distribution from a\nvector of real-valued numbers (computed from summing weights times features) in\nthe multinomial version of logistic regression in Chapter 5.\nThat means we can think of a neural network classi\ufb01er with one hidden layer\nas building a vector hwhich is a hidden layer representation of the input, and then\nrunning standard multinomial logistic regression on the features that the network\ndevelops in h. By contrast, in Chapter 5 the features were mainly designed by hand\nvia feature templates. So a neural network is like multinomial logistic regression,\nbut (a) with many layers, since a deep neural network is like layer after layer of lo-\ngistic regression classi\ufb01ers; (b) with those intermediate layers having many possible\nactivation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we\u2019ll\ncontinue to use sfor convenience to mean any activation function); (c) rather than\nforming the features by feature templates, the prior layers of the network induce the\nfeature representations themselves.\nHere are the \ufb01nal equations for a feedforward network with a single hidden layer,\nwhich takes an input vector x, outputs a probability distribution y, and is parameter-\nized by weight matrices WandUand a bias vector b:\nh=s(Wx+b)\nz=Uh\ny=softmax (z) (7.12)\nAnd just to remember the shapes of all our variables, x2Rn0,h2Rn1,b2Rn1,\nW2Rn1\u0002n0,U2Rn2\u0002n1, and the output vector y2Rn2. We\u2019ll call this network a 2-\nlayer network (we traditionally don\u2019t count the input layer when numbering layers,\nbut do count the output layer). So by this terminology logistic regression is a 1-layer\nnetwork.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 237,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 149\n\n7.3 \u2022 F EEDFORWARD NEURAL NETWORKS 141\n7.3.1 More details on feedforward networks\nLet\u2019s now set up some notation to make it easier to talk about deeper networks of\ndepth more than 2. We\u2019ll use superscripts in square brackets to mean layer num-\nbers, starting at 0 for the input layer. So W[1]will mean the weight matrix for the\n(\ufb01rst) hidden layer, and b[1]will mean the bias vector for the (\ufb01rst) hidden layer. nj\nwill mean the number of units at layer j. We\u2019ll use g(\u0001)to stand for the activation\nfunction, which will tend to be ReLU or tanh for intermediate layers and softmax\nfor output layers. We\u2019ll use a[i]to mean the output from layer i, and z[i]to mean the\ncombination of previous layer output, weights and biases W[i]a[i\u00001]+b[i]. The 0th\nlayer is for inputs, so we\u2019ll refer to the inputs xmore generally as a[0].\nThus we can re-represent our 2-layer net from Eq. 7.12 as follows:\nz[1]=W[1]a[0]+b[1]\na[1]=g[1](z[1])\nz[2]=W[2]a[1]+b[2]\na[2]=g[2](z[2])\n\u02c6y=a[2](7.13)\nNote that with this notation, the equations for the computation done at each layer are\nthe same. The algorithm for computing the forward step in an n-layer feedforward\nnetwork, given the input vector a[0]is thus simply:\nforiin1,...,n\nz[i]=W[i]a[i\u00001]+b[i]\na[i]=g[i](z[i])\n\u02c6y=a[n]\nIt\u2019s often useful to have a name for the \ufb01nal set of activations right before the \ufb01nal\nsoftmax. So however many layers we have, we\u2019ll generally call the unnormalized\nvalues in the \ufb01nal vector z[n], the vector of scores right before the \ufb01nal softmax, the\nlogits (see Eq. 5.7). logits\nThe need for non-linear activation functions One of the reasons we use non-\nlinear activation functions for each layer in a neural network is that if we did not, the\nresulting network is exactly equivalent to a single-layer network. Let\u2019s see why this\nis true. Imagine the \ufb01rst two layers of such a network of purely linear layers:\nz[1]=W[1]x+b[1]\nz[2]=W[2]z[1]+b[2]\nWe can rewrite the function that the network is computing as:\nz[2]=W[2]z[1]+b[2]\n=W[2](W[1]x+b[1])+b[2]\n=W[2]W[1]x+W[2]b[1]+b[2]\n=W0x+b0(7.14)\nThis generalizes to any number of layers. So without non-linear activation functions,\na multilayer network is just a notational variant of a single layer network with a\ndifferent set of weights, and we lose all the representational power of multilayer\nnetworks.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 238,
      "token_count": 718,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 150\n\n142 CHAPTER 7 \u2022 N EURAL NETWORKS\nReplacing the bias unit In describing networks, we will often use a slightly sim-\npli\ufb01ed notation that represents exactly the same function without referring to an ex-\nplicit bias node b. Instead, we add a dummy node a0to each layer whose value will\nalways be 1. Thus layer 0, the input layer, will have a dummy node a[0]\n0=1, layer 1\nwill have a[1]\n0=1, and so on. This dummy node still has an associated weight, and\nthat weight represents the bias value b. For example instead of an equation like\nh=s(Wx+b) (7.15)\nwe\u2019ll use:\nh=s(Wx) (7.16)\nBut now instead of our vector xhaving n0values: x=x1;:::;xn0, it will have n0+\n1 values, with a new 0th dummy value x0=1:x=x0;:::;xn0. And instead of\ncomputing each hjas follows:\nhj=s n0X\ni=1Wjixi+bj!\n; (7.17)\nwe\u2019ll instead use:\nhj=s n0X\ni=0Wjixi!\n; (7.18)\nwhere the value Wj0replaces what had been bj. Fig. 7.9 shows a visualization.\nx1x2xn0\u2026\u2026+1b\u2026UWh1y1y2yn2h2h3hn1\nx1x2xn0\u2026\u2026x0=1\u2026UWh1y1y2yn2h2h3hn1\n(a) (b)\nFigure 7.9 Replacing the bias node (shown in a) with x0(b).\nWe\u2019ll continue showing the bias as bwhen we go over the learning algorithm\nin Section 7.5, but then we\u2019ll switch to this simpli\ufb01ed notation without explicit bias\nterms for the rest of the book.\n7.4 Feedforward networks for NLP: Classi\ufb01cation\nLet\u2019s see how to apply feedforward networks to NLP tasks! In this section we\u2019ll\nlook at classi\ufb01cation tasks like sentiment analysis; in the next section we\u2019ll introduce\nneural language modeling.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 239,
      "token_count": 494,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 151\n\n7.4 \u2022 F EEDFORWARD NETWORKS FOR NLP: C LASSIFICATION 143\nLet\u2019s begin with a simple 2-layer sentiment classi\ufb01er. You might imagine taking\nour logistic regression classi\ufb01er from Chapter 5, which corresponds to a 1-layer net-\nwork, and just adding a hidden layer. The input element xicould be scalar features\nlike those in Fig. 5.2, e.g., x1= count(words2doc), x2= count(positive lexicon\nwords2doc), x3= 1 if \u201cno\u201d2doc, and so on. And the output layer \u02c6ycould have\ntwo nodes (one each for positive and negative), or 3 nodes (positive, negative, neu-\ntral), in which case \u02c6y1would be the estimated probability of positive sentiment, \u02c6y2\nthe probability of negative and \u02c6y3the probability of neutral. The resulting equations\nwould be just what we saw above for a 2-layer network (as always, we\u2019ll continue\nto use the sto stand for any non-linearity, whether sigmoid, ReLU or other).\nx= [x1;x2;:::xN](each xiis a hand-designed feature)\nh=s(Wx+b)\nz=Uh\n\u02c6y=softmax (z) (7.19)\nFig. 7.10 shows a sketch of this architecture. As we mentioned earlier, adding this\nhidden layer to our logistic regression classi\ufb01er allows the network to represent the\nnon-linear interactions between features. This alone might give us a better sentiment\nclassi\ufb01er.\nUW[n\u2a091]Hidden layerOutput layersoftmax[dh\u2a09n][dh\u2a091][3\u2a09dh]Input wordsp(+)h1h2h3hdh\u2026y1^y2^y3^xhyInput layer n=3 features[3\u2a091]x1x2x3dessertwasgreatpositive lexiconwords = 1count of \u201cno\u201d = 0wordcount=3p(-)p(neut)\nFigure 7.10 Feedforward network sentiment analysis using traditional hand-built features\nof the input text.\nMost applications of neural networks for NLP do something different, however.\nInstead of using hand-built human-engineered features as the input to our classi\ufb01er,\nwe draw on deep learning\u2019s ability to learn features from the data by representing\nwords as embeddings, like the word2vec or GloVe embeddings we saw in Chapter 6.\nThere are various ways to represent an input for classi\ufb01cation. One simple baseline\nis to apply some sort of pooling function to the embeddings of all the words in the pooling\ninput. For example, for a text with ninput words/tokens w1;:::;wn, we can turn the\nnembeddings e(w1);:::;e(wn)(each of dimensionality d) into a single embedding\nalso of dimensionality dby just summing the embeddings, or by taking their mean\n(summing and then dividing by n):\nxmean=1\nnnX\ni=1e(wi) (7.20)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 240,
      "token_count": 694,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 152\n\n144 CHAPTER 7 \u2022 N EURAL NETWORKS\nThere are many other options, like taking the element-wise max. The element-wise\nmax of a set of nvectors is a new vector whose kth element is the max of the kth\nelements of all the nvectors. Here are the equations for this classi\ufb01er assuming\nmean pooling; the architecture is sketched in Fig. 7.11:\nx=mean(e(w1);e(w2);:::;e(wn))\nh=s(Wx+b)\nz=Uh\n\u02c6y=softmax (z) (7.21)\nUW[d\u2a091]Hidden layerOutput layersoftmax[dh\u2a09d][dh\u2a091][3\u2a09dh]Input wordsp(+)embedding for\u201cgreat\u201dembedding for\u201cdessert\u201dh1h2h3hdh\u2026y1^y2^y3^xhyInput layer pooled embedding[3\u2a091]pooling+dessertwasgreatembedding for\u201cwas\u201dp(-)p(neut)\nFigure 7.11 Feedforward network sentiment analysis using a pooled embedding of the in-\nput words.\nWhile Eq. 7.21 shows how to classify a single example x, in practice we want\nto ef\ufb01ciently classify an entire test set of mexamples. We do this by vectorizing\nthe process, just as we saw with logistic regression; instead of using for-loops to go\nthrough each example, we\u2019ll use matrix multiplication to do the entire computation\nof an entire test set at once. First, we pack all the input feature vectors for each input\nxinto a single input matrix X, with each row ia row vector consisting of the pooled\nembedding for input example x(i)(i.e., the vector x(i)). If the dimensionality of our\npooled input embedding is d,Xwill be a matrix of shape [m\u0002d].\nWe will then need to slightly modify Eq. 7.21. Xis of shape [m\u0002d]andWis of\nshape [dh\u0002d], so we\u2019ll have to reorder how we multiply XandWand transpose W\nso they correctly multiply to yield a matrix Hof shape [m\u0002dh].1The bias vector b\nfrom Eq. 7.21 of shape [1\u0002dh]will now have to be replicated into a matrix of shape\n[m\u0002dh]. We\u2019ll need to similarly reorder the next step and transpose U. Finally, our\noutput matrix \u02c6Ywill be of shape [m\u00023](or more generally [m\u0002do], where dois\nthe number of output classes), with each row iof our output matrix \u02c6Yconsisting of\nthe output vector \u02c6y(i).\u2018 Here are the \ufb01nal equations for computing the output class\n1Note that we could have kept the original order of our products if we had instead made our input\nmatrix Xrepresent each input as a column vector instead of a row vector, making it of shape [d\u0002m]. But\nrepresenting inputs as row vectors is convenient and common in neural network models.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 241,
      "token_count": 661,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 153\n\n7.5 \u2022 T RAINING NEURAL NETS 145\ndistribution for an entire test set:\nH=s(XW|+b)\nZ=HU|\n\u02c6Y=softmax (Z) (7.22)\nThe idea of using word2vec or GloVe embeddings as our input representation\u2014\nand more generally the idea of relying on another algorithm to have already learned\nan embedding representation for our input words\u2014is called pretraining . Using pretraining\npretrained embedding representations, whether simple static word embeddings like\nword2vec or the much more powerful contextual embeddings we\u2019ll introduce in\nChapter 11, is one of the central ideas of deep learning. (It\u2019s also possible, how-\never, to train the word embeddings as part of an NLP task; we\u2019ll talk about how to\ndo this in Section 7.7 in the context of the neural language modeling task.)\n7.5 Training Neural Nets\nA feedforward neural net is an instance of supervised machine learning in which we\nknow the correct output yfor each observation x. What the system produces, via\nEq. 7.13, is \u02c6 y, the system\u2019s estimate of the true y. The goal of the training procedure\nis to learn parameters W[i]andb[i]for each layer ithat make \u02c6 yfor each training\nobservation as close as possible to the true y.\nIn general, we do all this by drawing on the methods we introduced in Chapter 5\nfor logistic regression, so the reader should be comfortable with that chapter before\nproceeding.\nFirst, we\u2019ll need a loss function that models the distance between the system\noutput and the gold output, and it\u2019s common to use the loss function used for logistic\nregression, the cross-entropy loss .\nSecond, to \ufb01nd the parameters that minimize this loss function, we\u2019ll use the\ngradient descent optimization algorithm introduced in Chapter 5.\nThird, gradient descent requires knowing the gradient of the loss function, the\nvector that contains the partial derivative of the loss function with respect to each\nof the parameters. In logistic regression, for each observation we could directly\ncompute the derivative of the loss function with respect to an individual worb. But\nfor neural networks, with millions of parameters in many layers, it\u2019s much harder to\nsee how to compute the partial derivative of some weight in layer 1 when the loss\nis attached to some much later layer. How do we partial out the loss over all those\nintermediate layers? The answer is the algorithm called error backpropagation or\nbackward differentiation .\n7.5.1 Loss function\nThecross-entropy loss that is used in neural networks is the same one we saw forcross-entropy\nloss\nlogistic regression. If the neural network is being used as a binary classi\ufb01er, with\nthe sigmoid at the \ufb01nal layer, the loss function is the same logistic regression loss\nwe saw in Eq. 5.23:\nLCE(\u02c6y;y) =\u0000logp(yjx) =\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)] (7.23)\nIf we are using the network to classify into 3 or more classes, the loss function is\nexactly the same as the loss for multinomial regression that we saw in Chapter 5 on",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 242,
      "token_count": 705,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 154\n\n146 CHAPTER 7 \u2022 N EURAL NETWORKS\npage 97. Let\u2019s brie\ufb02y summarize the explanation here for convenience. First, when\nwe have more than 2 classes we\u2019ll need to represent both yand\u02c6yas vectors. Let\u2019s\nassume we\u2019re doing hard classi\ufb01cation , where only one class is the correct one.\nThe true label yis then a vector with Kelements, each corresponding to a class,\nwithyc=1 if the correct class is c, with all other elements of ybeing 0. Recall that\na vector like this, with one value equal to 1 and the rest 0, is called a one-hot vector .\nAnd our classi\ufb01er will produce an estimate vector with Kelements \u02c6y, each element\n\u02c6ykof which represents the estimated probability p(yk=1jx).\nThe loss function for a single example xis the negative sum of the logs of the K\noutput classes, each weighted by their probability yk:\nLCE(\u02c6y;y) =\u0000KX\nk=1yklog\u02c6yk (7.24)\nWe can simplify this equation further; let\u2019s \ufb01rst rewrite the equation using the func-\ntion 1fgwhich evaluates to 1 if the condition in the brackets is true and to 0 oth-\nerwise. This makes it more obvious that the terms in the sum in Eq. 7.24 will be 0\nexcept for the term corresponding to the true class for which yk=1:\nLCE(\u02c6y;y) =\u0000KX\nk=11fyk=1glog\u02c6yk\nIn other words, the cross-entropy loss is simply the negative log of the output proba-\nbility corresponding to the correct class, and we therefore also call this the negative\nlog likelihood loss :negative log\nlikelihood loss\nLCE(\u02c6y;y) =\u0000log\u02c6yc(where cis the correct class) (7.25)\nPlugging in the softmax formula from Eq. 7.9, and with Kthe number of classes:\nLCE(\u02c6y;y) =\u0000logexp(zc)PK\nj=1exp(zj)(where cis the correct class) (7.26)\n7.5.2 Computing the Gradient\nHow do we compute the gradient of this loss function? Computing the gradient\nrequires the partial derivative of the loss function with respect to each parameter.\nFor a network with one weight layer and sigmoid output (which is what logistic\nregression is), we could simply use the derivative of the loss that we used for logistic\nregression in Eq. 7.27 (and derived in Section 5.10):\n\u00b6LCE(\u02c6y;y)\n\u00b6wj= ( \u02c6y\u0000y)xj\n= (s(w\u0001x+b)\u0000y)xj (7.27)\nOr for a network with one weight layer and softmax output (=multinomial logistic\nregression), we could use the derivative of the softmax loss from Eq. 5.48, shown\nfor a particular weight wkand input xi\n\u00b6LCE(\u02c6y;y)\n\u00b6wk;i=\u0000(yk\u0000\u02c6yk)xi\n=\u0000(yk\u0000p(yk=1jx))xi\n=\u0000 \nyk\u0000exp(wk\u0001x+bk)PK\nj=1exp(wj\u0001x+bj)!\nxi (7.28)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 243,
      "token_count": 734,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 155\n\n7.5 \u2022 T RAINING NEURAL NETS 147\nBut these derivatives only give correct updates for one weight layer: the last one!\nFor deep networks, computing the gradients for each weight is much more complex,\nsince we are computing the derivative with respect to weight parameters that appear\nall the way back in the very early layers of the network, even though the loss is\ncomputed only at the very end of the network.\nThe solution to computing this gradient is an algorithm called error backprop-\nagation orbackprop (Rumelhart et al., 1986). While backprop was invented spe-error back-\npropagation\ncially for neural networks, it turns out to be the same as a more general procedure\ncalled backward differentiation , which depends on the notion of computation\ngraphs . Let\u2019s see how that works in the next subsection.\n7.5.3 Computation Graphs\nA computation graph is a representation of the process of computing a mathematical\nexpression, in which the computation is broken down into separate operations, each\nof which is modeled as a node in a graph.\nConsider computing the function L(a;b;c) =c(a+2b). If we make each of the\ncomponent addition and multiplication operations explicit, and add names ( dande)\nfor the intermediate outputs, the resulting series of computations is:\nd=2\u0003b\ne=a+d\nL=c\u0003e\nWe can now represent this as a graph, with nodes for each operation, and di-\nrected edges showing the outputs from each operation as the inputs to the next, as\nin Fig. 7.12. The simplest use of computation graphs is to compute the value of\nthe function with some given inputs. In the \ufb01gure, we\u2019ve assumed the inputs a=3,\nb=1,c=\u00002, and we\u2019ve shown the result of the forward pass to compute the re-\nsultL(3;1;\u00002) =\u000010. In the forward pass of a computation graph, we apply each\noperation left to right, passing the outputs of each computation as the input to the\nnext node.\ne=a+dd = 2bL=cea=3b=1c=-2e=5d=2L=-10forward passabc\nFigure 7.12 Computation graph for the function L(a;b;c) =c(a+2b), with values for input\nnodes a=3,b=1,c=\u00002, showing the forward pass computation of L.\n7.5.4 Backward differentiation on computation graphs\nThe importance of the computation graph comes from the backward pass , which\nis used to compute the derivatives that we\u2019ll need for the weight update. In this\nexample our goal is to compute the derivative of the output function Lwith respect",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 244,
      "token_count": 584,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 156\n\n148 CHAPTER 7 \u2022 N EURAL NETWORKS\nto each of the input variables, i.e.,\u00b6L\n\u00b6a,\u00b6L\n\u00b6b, and\u00b6L\n\u00b6c. The derivative\u00b6L\n\u00b6atells us how\nmuch a small change in aaffects L.\nBackwards differentiation makes use of the chain rule in calculus, so let\u2019s re- chain rule\nmind ourselves of that. Suppose we are computing the derivative of a composite\nfunction f(x) =u(v(x)). The derivative of f(x)is the derivative of u(x)with respect\ntov(x)times the derivative of v(x)with respect to x:\nd f\ndx=du\ndv\u0001dv\ndx(7.29)\nThe chain rule extends to more than two functions. If computing the derivative of a\ncomposite function f(x) =u(v(w(x))), the derivative of f(x)is:\nd f\ndx=du\ndv\u0001dv\ndw\u0001dw\ndx(7.30)\nThe intuition of backward differentiation is to pass gradients back from the \ufb01nal\nnode to all the nodes in the graph. Fig. 7.13 shows part of the backward computation\nat one node e. Each node takes an upstream gradient that is passed in from its parent\nnode to the right, and for each of its inputs computes a local gradient (the gradient\nof its output with respect to its input), and uses the chain rule to multiply these two\nto compute a downstream gradient to be passed on to the next earlier node.\nedLed\u2202L\u2202d\u2202L\u2202e=\u2202e\u2202d\u2202L\u2202e\u2202e\u2202dupstream gradientdownstream gradientlocal gradient\nFigure 7.13 Each node (like ehere) takes an upstream gradient, multiplies it by the local\ngradient (the gradient of its output with respect to its input), and uses the chain rule to compute\na downstream gradient to be passed on to a prior node. A node may have multiple local\ngradients if it has multiple inputs.\nLet\u2019s now compute the 3 derivatives we need. Since in the computation graph\nL=ce, we can directly compute the derivative\u00b6L\n\u00b6c:\n\u00b6L\n\u00b6c=e (7.31)\nFor the other two, we\u2019ll need to use the chain rule:\n\u00b6L\n\u00b6a=\u00b6L\n\u00b6e\u00b6e\n\u00b6a\n\u00b6L\n\u00b6b=\u00b6L\n\u00b6e\u00b6e\n\u00b6d\u00b6d\n\u00b6b(7.32)\nEq. 7.32 and Eq. 7.31 thus require \ufb01ve intermediate derivatives:\u00b6L\n\u00b6e,\u00b6L\n\u00b6c,\u00b6e\n\u00b6a,\u00b6e\n\u00b6d, and\n\u00b6d\n\u00b6b, which are as follows (making use of the fact that the derivative of a sum is the",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 245,
      "token_count": 610,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 157\n\n7.5 \u2022 T RAINING NEURAL NETS 149\nsum of the derivatives):\nL=ce:\u00b6L\n\u00b6e=c;\u00b6L\n\u00b6c=e\ne=a+d:\u00b6e\n\u00b6a=1;\u00b6e\n\u00b6d=1\nd=2b:\u00b6d\n\u00b6b=2\nIn the backward pass, we compute each of these partials along each edge of the\ngraph from right to left, using the chain rule just as we did above. Thus we begin by\ncomputing the downstream gradients from node L, which are\u00b6L\n\u00b6eand\u00b6L\n\u00b6c. For node e,\nwe then multiply this upstream gradient\u00b6L\n\u00b6eby the local gradient (the gradient of the\noutput with respect to the input),\u00b6e\n\u00b6dto get the output we send back to node d:\u00b6L\n\u00b6d.\nAnd so on, until we have annotated the graph all the way to all the input variables.\nThe forward pass conveniently already will have computed the values of the forward\nintermediate variables we need (like dande) to compute these derivatives. Fig. 7.14\nshows the backward pass.\ne=d+ad = 2bL=cea=3b=1e=5d=2L=-10 abc\u2202L=5\u2202c\u2202L=-2\u2202e\u2202e=1\u2202d\u2202d=2\u2202b\u2202e=1\u2202abackward passc=-2\u2202L=-2\u2202e\u2202L=5\u2202c\u2202L\u2202d=-2\u2202e\u2202d\u2202L\u2202e=\u2202L\u2202a=-2\u2202e\u2202a\u2202L\u2202e=\u2202L\u2202b=-4\u2202d\u2202b\u2202L\u2202d=\nFigure 7.14 Computation graph for the function L(a;b;c) =c(a+2b), showing the backward pass computa-\ntion of\u00b6L\n\u00b6a,\u00b6L\n\u00b6b, and\u00b6L\n\u00b6c.\nBackward differentiation for a neural network\nOf course computation graphs for real neural networks are much more complex.\nFig. 7.15 shows a sample computation graph for a 2-layer neural network with n0=\n2,n1=2, and n2=1, assuming binary classi\ufb01cation and hence using a sigmoid\noutput unit for simplicity. The function that the computation graph is computing is:\nz[1]=W[1]x+b[1]\na[1]=ReLU (z[1])\nz[2]=W[2]a[1]+b[2]\na[2]=s(z[2])\n\u02c6y=a[2](7.33)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 246,
      "token_count": 595,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 158\n\n150 CHAPTER 7 \u2022 N EURAL NETWORKS\nFor the backward pass we\u2019ll also need to compute the loss L. The loss function\nfor binary sigmoid output from Eq. 7.23 is\nLCE(\u02c6y;y) =\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)] (7.34)\nOur output \u02c6 y=a[2], so we can rephrase this as\nLCE(a[2];y) =\u0000h\nyloga[2]+(1\u0000y)log(1\u0000a[2])i\n(7.35)\nz[2] = +a[2] = \u03c3 a[1] = ReLUz[1] = +b[1]****x1x2a[1] = ReLUz[1] = +b[1]**w[2]11w[1]11w[1]12\nw[1]21w[1]22b[2]w[2]12L (a[2],y)1\n2111\n22\nFigure 7.15 Sample computation graph for a simple 2-layer neural net (= 1 hidden layer) with two input units\nand 2 hidden units. We\u2019ve adjusted the notation a bit to avoid long equations in the nodes by just mentioning\nthe function that is being computed, and the resulting variable name. Thus the * to the right of node w[1]\n11means\nthatw[1]\n11is to be multiplied by x1, and the node z[1]= + means that the value of z[1]is computed by summing\nthe three nodes that feed into it (the two products, and the bias term b[1]\ni).\nThe weights that need updating (those for which we need to know the partial\nderivative of the loss function) are shown in teal. In order to do the backward pass,\nwe\u2019ll need to know the derivatives of all the functions in the graph. We already saw\nin Section 5.10 the derivative of the sigmoid s:\nds(z)\ndz=s(z)(1\u0000s(z)) (7.36)\nWe\u2019ll also need the derivatives of each of the other activation functions. The\nderivative of tanh is:\ndtanh(z)\ndz=1\u0000tanh2(z) (7.37)\nThe derivative of the ReLU is2\ndReLU (z)\ndz=\u001a0f or z<0\n1f or z\u00150(7.38)\n2The derivative is actually unde\ufb01ned at the point z=0, but by convention we treat it as 1.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 247,
      "token_count": 564,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 159\n\n7.5 \u2022 T RAINING NEURAL NETS 151\nWe\u2019ll give the start of the computation, computing the derivative of the loss function\nLwith respect to z, or\u00b6L\n\u00b6z(and leaving the rest of the computation as an exercise for\nthe reader). By the chain rule:\n\u00b6L\n\u00b6z=\u00b6L\n\u00b6a[2]\u00b6a[2]\n\u00b6z(7.39)\nSo let\u2019s \ufb01rst compute\u00b6L\n\u00b6a[2], taking the derivative of Eq. 7.35, repeated here:\nLCE(a[2];y) =\u0000h\nyloga[2]+(1\u0000y)log(1\u0000a[2])i\n\u00b6L\n\u00b6a[2]=\u0000  \ny\u00b6log(a[2])\n\u00b6a[2]!\n+(1\u0000y)\u00b6log(1\u0000a[2])\n\u00b6a[2]!\n=\u0000\u0012\u0012\ny1\na[2]\u0013\n+(1\u0000y)1\n1\u0000a[2](\u00001)\u0013\n=\u0000\u0012y\na[2]+y\u00001\n1\u0000a[2]\u0013\n(7.40)\nNext, by the derivative of the sigmoid:\n\u00b6a[2]\n\u00b6z=a[2](1\u0000a[2])\nFinally, we can use the chain rule:\n\u00b6L\n\u00b6z=\u00b6L\n\u00b6a[2]\u00b6a[2]\n\u00b6z\n=\u0000\u0012y\na[2]+y\u00001\n1\u0000a[2]\u0013\na[2](1\u0000a[2])\n=a[2]\u0000y (7.41)\nContinuing the backward computation of the gradients (next by passing the gra-\ndients over b[2]\n1and the two product nodes, and so on, back to all the teal nodes), is\nleft as an exercise for the reader.\n7.5.5 More details on learning\nOptimization in neural networks is a non-convex optimization problem, more com-\nplex than for logistic regression, and for that and other reasons there are many best\npractices for successful learning.\nFor logistic regression we can initialize gradient descent with all the weights and\nbiases having the value 0. In neural networks, by contrast, we need to initialize the\nweights with small random numbers. It\u2019s also helpful to normalize the input values\nto have 0 mean and unit variance.\nVarious forms of regularization are used to prevent over\ufb01tting. One of the most\nimportant is dropout : randomly dropping some units and their connections from dropout\nthe network during training (Hinton et al. 2012, Srivastava et al. 2014). At each\niteration of training (whenever we update parameters, i.e. each mini-batch if we are\nusing mini-batch gradient descent), we repeatedly choose a probability pand for\neach unit we replace its output with zero with probability p(and renormalize the\nrest of the outputs from that layer).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 248,
      "token_count": 639,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 160\n\n152 CHAPTER 7 \u2022 N EURAL NETWORKS\nTuning of hyperparameters is also important. The parameters of a neural net- hyperparameter\nwork are the weights Wand biases b; those are learned by gradient descent. The\nhyperparameters are things that are chosen by the algorithm designer; optimal val-\nues are tuned on a devset rather than by gradient descent learning on the training\nset. Hyperparameters include the learning rate h, the mini-batch size, the model\narchitecture (the number of layers, the number of hidden nodes per layer, the choice\nof activation functions), how to regularize, and so on. Gradient descent itself also\nhas many architectural variants such as Adam (Kingma and Ba, 2015).\nFinally, most modern neural networks are built using computation graph for-\nmalisms that make it easy and natural to do gradient computation and parallelization\non vector-based GPUs (Graphic Processing Units). PyTorch (Paszke et al., 2017)\nand TensorFlow (Abadi et al., 2015) are two of the most popular. The interested\nreader should consult a neural network textbook for further details; some sugges-\ntions are at the end of the chapter.\n7.6 Feedforward Neural Language Modeling\nAs our second application of feedforward networks, let\u2019s consider language mod-\neling : predicting upcoming words from prior words. Neural language modeling\u2014\nbased on the transformer architecture that we will see in Chapter 9\u2014is the algorithm\nthat underlies all of modern NLP. In this section and the next we\u2019ll introduce a sim-\npler version of neural language models for feedforward networks, an algorithm \ufb01rst\nintroduced by Bengio et al. (2003). The feedforward language model introduces\nmany of the important concepts of neural language modeling, concepts we\u2019ll return\nto as we describe more powerful models in Chapter 8 and Chapter 9.\nNeural language models have many advantages over the n-gram language mod-\nels of Chapter 3. Compared to n-gram models, neural language models can handle\nmuch longer histories, can generalize better over contexts of similar words, and are\nmore accurate at word-prediction. On the other hand, neural net language models\nare much more complex, are slower and need more energy to train, and are less inter-\npretable than n-gram models, so for some smaller tasks an n-gram language model\nis still the right tool.\nA feedforward neural language model (LM) is a feedforward network that takes\nas input at time ta representation of some number of previous words ( wt\u00001;wt\u00002,\netc.) and outputs a probability distribution over possible next words. Thus\u2014like the\nn-gram LM\u2014the feedforward neural LM approximates the probability of a word\ngiven the entire prior context P(wtjw1:t\u00001)by approximating based on the N\u00001\nprevious words:\nP(wtjw1;:::; wt\u00001)\u0019P(wtjwt\u0000N+1;:::; wt\u00001) (7.42)\nIn the following examples we\u2019ll use a 4-gram example, so we\u2019ll show a neural net to\nestimate the probability P(wt=ijwt\u00003;wt\u00002;wt\u00001).\nNeural language models represent words in this prior context by their embed-\ndings , rather than just by their word identity as used in n-gram language models.\nUsing embeddings allows neural language models to generalize better to unseen\ndata. For example, suppose we\u2019ve seen this sentence in training:\nI have to make sure that the cat gets fed.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 249,
      "token_count": 760,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 161\n\n7.6 \u2022 F EEDFORWARD NEURAL LANGUAGE MODELING 153\nbut have never seen the words \u201cgets fed\u201d after the word \u201cdog\u201d. Our test set has the\npre\ufb01x \u201cI forgot to make sure that the dog gets\u201d. What\u2019s the next word? An n-gram\nlanguage model will predict \u201cfed\u201d after \u201cthat the cat gets\u201d, but not after \u201cthat the dog\ngets\u201d. But a neural LM, knowing that \u201ccat\u201d and \u201cdog\u201d have similar embeddings, will\nbe able to generalize from the \u201ccat\u201d context to assign a high enough probability to\n\u201cfed\u201d even after seeing \u201cdog\u201d.\n7.6.1 Forward inference in the neural language model\nLet\u2019s walk through forward inference ordecoding for neural language models.forward\ninference\nForward inference is the task, given an input, of running a forward pass on the\nnetwork to produce a probability distribution over possible outputs, in this case next\nwords.\nWe \ufb01rst represent each of the Nprevious words as a one-hot vector of length\njVj, i.e., with one dimension for each word in the vocabulary. A one-hot vector is one-hot vector\na vector that has one element equal to 1\u2014in the dimension corresponding to that\nword\u2019s index in the vocabulary\u2014 while all the other elements are set to zero. Thus\nin a one-hot representation for the word \u201ctoothpaste\u201d, supposing it is V5, i.e., index\n5 in the vocabulary, x5=1, and xi=08i6=5, as shown here:\n[0 0 0 0 1 0 0 ... 0 0 0 0]\n1 2 3 4 5 6 7 ... ... |V|\nThe feedforward neural language model (sketched in Fig. 7.17) has a moving\nwindow that can see N words into the past. We\u2019ll let N equal 3, so the 3 words\nwt\u00001,wt\u00002, and wt\u00003are each represented as a one-hot vector. We then multiply\nthese one-hot vectors by the embedding matrix E. The embedding weight matrix E\nhas a column for each word, each a column vector of ddimensions, and hence has\ndimensionality d\u0002jVj. Multiplying by a one-hot vector that has only one non-zero\nelement xi=1 simply selects out the relevant column vector for word i, resulting in\nthe embedding for word i, as shown in Fig. 7.16.\nE|V|d1|V|d1=\u271555e5\nFigure 7.16 Selecting the embedding vector for word V5by multiplying the embedding\nmatrix Ewith a one-hot vector with a 1 in index 5.\nThe 3 resulting embedding vectors are concatenated to produce e, the embedding\nlayer. This is followed by a hidden layer and an output layer whose softmax produces\na probability distribution over words. For example y42, the value of output node 42,\nis the probability of the next word wtbeing V42, the vocabulary word with index 42\n(which is the word \u2018\ufb01sh\u2019 in our example).\nHere\u2019s the algorithm in detail for our mini example:\n1.Select three embeddings from E : Given the three previous words, we look\nup their indices, create 3 one-hot vectors, and then multiply each by the em-\nbedding matrix E. Consider wt\u00003. The one-hot vector for \u2018for\u2019 (index 35) is\nmultiplied by the embedding matrix E, to give the \ufb01rst part of the \ufb01rst hidden",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 250,
      "token_count": 769,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 162\n\n154 CHAPTER 7 \u2022 N EURAL NETWORKS\nUWembedding layer3d\u2a091hiddenlayeroutput layersoftmaxdh\u2a093ddh\u2a091|V|\u2a09dhinput layerone-hot vectorsE\n|V|\u2a093d\u2a09|V|p(do|\u2026)p(aardvark|\u2026)\np(zebra|\u2026)p(fish|\u2026)\n|V|\u2a091EEh1h2y1\nh3hdh\u2026\u2026y34\ny|V|\u2026001001|V|35\n001001|V|451001001|V|9920\n0\u2026\u2026y42y35102^^^\n^^hexyforallthe?thanksand\u2026wt-3wt-2wt-1wt\u2026\nFigure 7.17 Forward inference in a feedforward neural language model. At each timestep\ntthe network computes a d-dimensional embedding for each context word (by multiplying a\none-hot vector by the embedding matrix E), and concatenates the 3 resulting embeddings to\nget the embedding layer e. The embedding vector eis multiplied by a weight matrix Wand\nthen an activation function is applied element-wise to produce the hidden layer h, which is\nthen multiplied by another weight matrix U. Finally, a softmax output layer predicts at each\nnode ithe probability that the next word wtwill be vocabulary word Vi.\nlayer, the embedding layer . Since each column of the input matrix Eis anembedding\nlayer\nembedding for a word, and the input is a one-hot column vector xifor word\nVi, the embedding layer for input wwill be Exi=ei, the embedding for word\ni. We now concatenate the three embeddings for the three context words to\nproduce the embedding layer e.\n2.Multiply by W : We multiply by W(and add b) and pass through the ReLU\n(or other) activation function to get the hidden layer h.\n3.Multiply by U :his now multiplied by U\n4.Apply softmax : After the softmax, each node iin the output layer estimates\nthe probability P(wt=ijwt\u00001;wt\u00002;wt\u00003)\nIn summary, the equations for a neural language model with a window size of 3,\ngiven one-hot input vectors for each input context word, are:\ne= [Ext\u00003;Ext\u00002;Ext\u00001]\nh=s(We+b)\nz=Uh\n\u02c6y=softmax (z) (7.43)\nNote that we formed the embedding layer eby concatenating the 3 embeddings\nfor the three context vectors; we\u2019ll often use semicolons to mean concatenation of\nvectors.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 251,
      "token_count": 565,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 163",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 252,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "7.7 \u2022 T RAINING THE NEURAL LANGUAGE MODEL 155\n7.7 Training the neural language model\nThe high-level intuition of training neural language models, whether the simple\nfeedforward language models we describe here or the more powerful transformer\nlanguage models of Chapter 9, is the idea of self-training orself-supervision that self-training\nwe saw in Chapter 6 for learning word representations. In self-training for language\nmodeling, we take a corpus of text as training material and at each time step task\nthe model to predict the next word. At \ufb01rst it will do poorly at this task, but since\nin each case we know the correct answer (it\u2019s the next word in the corpus!) we can\neasily train it to be better at predicting the correct next word. We call such a model\nself-supervised because we don\u2019t have to add any special gold labels to the data;\nthe natural sequence of words is its own supervision! We simply train the model to\nminimize the error in predicting the true next word in the training sequence.\nIn practice, training the model means setting the parameters q=E;W;U;b. For\nsome tasks, it\u2019s ok to freeze the embedding layer Ewith initial word2vec values. freeze\nFreezing means we use word2vec or some other pretraining algorithm to compute\nthe initial embedding matrix E, and then hold it constant while we only modify W,\nU, and b, i.e., we don\u2019t update Eduring language model training. However, often\nwe\u2019d like to learn the embeddings simultaneously with training the network. This is\nuseful when the task the network is designed for (like sentiment classi\ufb01cation, trans-\nlation, or parsing) places strong constraints on what makes a good representation for\nwords.\nLet\u2019s see how to train the entire model including E, i.e. to set all the parameters\nq=E;W;U;b. We\u2019ll do this via gradient descent (Fig. 5.6), using error backprop-\nagation on the computation graph to compute the gradient. Training thus not only\nsets the weights WandUof the network, but also as we\u2019re predicting upcoming\nwords, we\u2019re learning the embeddings Efor each word that best predict upcoming\nwords.\nFig. 7.18 shows the set up for a window size of N=3 context words. The input x\nconsists of 3 one-hot vectors, fully connected to the embedding layer via 3 instanti-\nations of the embedding matrix E. We don\u2019t want to learn separate weight matrices\nfor mapping each of the 3 previous words to the projection layer. We want one single\nembedding dictionary Ethat\u2019s shared among these three. That\u2019s because over time,\nmany different words will appear as wt\u00002orwt\u00001, and we\u2019d like to just represent\neach word with one vector, whichever context position it appears in. Recall that the\nembedding weight matrix Ehas a column for each word, each a column vector of d\ndimensions, and hence has dimensionality d\u0002jVj.\nGenerally training proceeds by taking as input a very long text, concatenating all\nthe sentences, starting with random weights, and then iteratively moving through the\ntext predicting each word wt. At each word wt, we use the cross-entropy (negative\nlog likelihood) loss. Recall that the general form for this (repeated from Eq. 7.25)\nis:\nLCE(\u02c6y;y) =\u0000log \u02c6yi;(where iis the correct class) (7.44)\nFor language modeling, the classes are the words in the vocabulary, so \u02c6 yihere means\nthe probability that the model assigns to the correct next word wt:",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 253,
      "token_count": 782,
      "chapter_title": ""
    }
  },
  {
    "content": "consists of 3 one-hot vectors, fully connected to the embedding layer via 3 instanti-\nations of the embedding matrix E. We don\u2019t want to learn separate weight matrices\nfor mapping each of the 3 previous words to the projection layer. We want one single\nembedding dictionary Ethat\u2019s shared among these three. That\u2019s because over time,\nmany different words will appear as wt\u00002orwt\u00001, and we\u2019d like to just represent\neach word with one vector, whichever context position it appears in. Recall that the\nembedding weight matrix Ehas a column for each word, each a column vector of d\ndimensions, and hence has dimensionality d\u0002jVj.\nGenerally training proceeds by taking as input a very long text, concatenating all\nthe sentences, starting with random weights, and then iteratively moving through the\ntext predicting each word wt. At each word wt, we use the cross-entropy (negative\nlog likelihood) loss. Recall that the general form for this (repeated from Eq. 7.25)\nis:\nLCE(\u02c6y;y) =\u0000log \u02c6yi;(where iis the correct class) (7.44)\nFor language modeling, the classes are the words in the vocabulary, so \u02c6 yihere means\nthe probability that the model assigns to the correct next word wt:\nLCE=\u0000logp(wtjwt\u00001;:::;wt\u0000n+1) (7.45)\nThe parameter update for stochastic gradient descent for this loss from step stos+1",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 254,
      "token_count": 323,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 164\n\n156 CHAPTER 7 \u2022 N EURAL NETWORKS\nUWembedding layer3d\u2a091hiddenlayeroutput layersoftmaxdh\u2a093ddh\u2a091|V|\u2a09dhinput layerone-hot vectorsE\n|V|\u2a093d\u2a09|V|p(do|\u2026)p(aardvark|\u2026)\np(zebra|\u2026)p(fish|\u2026)\n|V|\u2a091EEh1h2y1\nh3hdh\u2026\u2026y34\ny|V|\u2026001001|V|35\n001001|V|451001001|V|9920\n0\u2026\u2026y42y35102^^^\n^^hexyforallthefishthanksand\u2026wt-3wt-2wt-1wt\u2026L = \u2212log P(fish | for, all, the)wt=fish\nFigure 7.18 Learning all the way back to embeddings. Again, the embedding matrix Eis\nshared among the 3 context words.\nis then:\nqs+1=qs\u0000h\u00b6[\u0000logp(wtjwt\u00001;:::;wt\u0000n+1)]\n\u00b6q(7.46)\nThis gradient can be computed in any standard neural network framework which\nwill then backpropagate through q=E;W;U;b.\nTraining the parameters to minimize loss will result both in an algorithm for\nlanguage modeling (a word predictor) but also a new set of embeddings Ethat can\nbe used as word representations for other tasks.\n7.8 Summary\n\u2022 Neural networks are built out of neural units , originally inspired by biological\nneurons but now simply an abstract computational device.\n\u2022 Each neural unit multiplies input values by a weight vector, adds a bias, and\nthen applies a non-linear activation function like sigmoid, tanh, or recti\ufb01ed\nlinear unit.\n\u2022 In a fully-connected ,feedforward network, each unit in layer iis connected\nto each unit in layer i+1, and there are no cycles.\n\u2022 The power of neural networks comes from the ability of early layers to learn\nrepresentations that can be utilized by later layers in the network.\n\u2022 Neural networks are trained by optimization algorithms like gradient de-\nscent .\n\u2022Error backpropagation , backward differentiation on a computation graph ,\nis used to compute the gradients of the loss function for a network.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 255,
      "token_count": 502,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 165\n\nBIBLIOGRAPHICAL AND HISTORICAL NOTES 157\n\u2022Neural language models use a neural network as a probabilistic classi\ufb01er, to\ncompute the probability of the next word given the previous nwords.\n\u2022 Neural language models can use pretrained embeddings , or can learn embed-\ndings from scratch in the process of language modeling.\nBibliographical and Historical Notes\nThe origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCul-\nloch and Pitts, 1943), a simpli\ufb01ed model of the biological neuron as a kind of com-\nputing element that could be described in terms of propositional logic. By the late\n1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and\nBernard Widrow at Stanford) developed research into neural networks; this phase\nsaw the development of the perceptron (Rosenblatt, 1958), and the transformation\nof the threshold into a bias, a notation we still use (Widrow and Hoff, 1960).\nThe \ufb01eld of neural networks declined after it was shown that a single perceptron\nunit was unable to model functions as simple as XOR (Minsky and Papert, 1969).\nWhile some small amount of work continued during the next two decades, a major\nrevival for the \ufb01eld didn\u2019t come until the 1980s, when practical tools for building\ndeeper networks like error backpropagation became widespread (Rumelhart et al.,\n1986). During the 1980s a wide variety of neural network and related architec-\ntures were developed, particularly for applications in psychology and cognitive sci-\nence (Rumelhart and McClelland 1986b, McClelland and Elman 1986, Rumelhart\nand McClelland 1986a, Elman 1990), for which the term connectionist orparal- connectionist\nlel distributed processing was often used (Feldman and Ballard 1982, Smolensky\n1988). Many of the principles and techniques developed in this period are foun-\ndational to modern work, including the ideas of distributed representations (Hinton,\n1986), recurrent networks (Elman, 1990), and the use of tensors for compositionality\n(Smolensky, 1990).\nBy the 1990s larger neural networks began to be applied to many practical lan-\nguage processing tasks as well, like handwriting recognition (LeCun et al. 1989) and\nspeech recognition (Morgan and Bourlard 1990). By the early 2000s, improvements\nin computer hardware and advances in optimization and training techniques made it\npossible to train even larger and deeper networks, leading to the modern term deep\nlearning (Hinton et al. 2006, Bengio et al. 2007). We cover more related history in\nChapter 8 and Chapter 16.\nThere are a number of excellent books on the subject. Goldberg (2017) has\nsuperb coverage of neural networks for natural language processing. For neural\nnetworks in general see Goodfellow et al. (2016) and Nielsen (2015).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 256,
      "token_count": 692,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 166\n\n158 CHAPTER 8 \u2022 RNN S AND LSTM S\nCHAPTER\n8RNNs and LSTMs\nTime will explain.\nJane Austen, Persuasion\nLanguage is an inherently temporal phenomenon. Spoken language is a sequence of\nacoustic events over time, and we comprehend and produce both spoken and written\nlanguage as a sequential input stream. The temporal nature of language is re\ufb02ected\nin the metaphors we use; we talk of the \ufb02ow of conversations ,news feeds , and twitter\nstreams , all of which emphasize that language is a sequence that unfolds in time.\nYet most of the machine learning approaches we\u2019ve studied so far, like those\nfor sentiment analysis and other text classi\ufb01cation tasks don\u2019t have this temporal\nnature \u2013 they assume simultaneous access to all aspects of their input. The feedfor-\nward networks of Chapter 7 also assumed simultaneous access, although they also\nhad a simple model for time. Recall that we applied feedforward networks to lan-\nguage modeling by having them look only at a \ufb01xed-size window of words, and then\nsliding this window over the input, making independent predictions along the way.\nThis sliding-window approach is also used in the transformer architecture we will\nintroduce in Chapter 9.\nThis chapter introduces a deep learning architecture that offers an alternative\nway of representing time: recurrent neural networks (RNNs), and their variants like\nLSTMs. RNNs have a mechanism that deals directly with the sequential nature of\nlanguage, allowing them to handle the temporal nature of language without the use of\narbitrary \ufb01xed-sized windows. The recurrent network offers a new way to represent\nthe prior context, in its recurrent connections , allowing the model\u2019s decision to\ndepend on information from hundreds of words in the past. We\u2019ll see how to apply\nthe model to the task of language modeling, to text classi\ufb01cation tasks like sentiment\nanalysis, and to sequence modeling tasks like part-of-speech tagging (a task we\u2019ll\nreturn to in detail in Chapter 17).\n8.1 Recurrent Neural Networks\nA recurrent neural network (RNN) is any network that contains a cycle within its\nnetwork connections, meaning that the value of some unit is directly, or indirectly,\ndependent on its own earlier outputs as an input. While powerful, such networks\nare dif\ufb01cult to reason about and to train. However, within the general class of recur-\nrent networks there are constrained architectures that have proven to be extremely\neffective when applied to language. In this section, we consider a class of recurrent\nnetworks referred to as Elman Networks (Elman, 1990) or simple recurrent net-Elman\nNetworks\nworks . These networks are useful in their own right and serve as the basis for more\ncomplex approaches like the Long Short-Term Memory (LSTM) networks discussed",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 257,
      "token_count": 604,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 167\n\n8.1 \u2022 R ECURRENT NEURAL NETWORKS 159\nlater in this chapter. In this chapter when we use the term RNN we\u2019ll be referring to\nthese simpler more constrained networks (although you will often see the term RNN\nto mean any net with recurrent properties including LSTMs).\nxthtyt\nFigure 8.1 Simple recurrent neural network after Elman (1990). The hidden layer includes\na recurrent connection as part of its input. That is, the activation value of the hidden layer\ndepends on the current input as well as the activation value of the hidden layer from the\nprevious time step.\nFig. 8.1 illustrates the structure of an RNN. As with ordinary feedforward net-\nworks, an input vector representing the current input, xt, is multiplied by a weight\nmatrix and then passed through a non-linear activation function to compute the val-\nues for a layer of hidden units. This hidden layer is then used to calculate a cor-\nresponding output, yt. In a departure from our earlier window-based approach, se-\nquences are processed by presenting one item at a time to the network. We\u2019ll use\nsubscripts to represent time, thus xtwill mean the input vector xat time t. The key\ndifference from a feedforward network lies in the recurrent link shown in the \ufb01gure\nwith the dashed line. This link augments the input to the computation at the hidden\nlayer with the value of the hidden layer from the preceding point in time .\nThe hidden layer from the previous time step provides a form of memory, or\ncontext, that encodes earlier processing and informs the decisions to be made at\nlater points in time. Critically, this approach does not impose a \ufb01xed-length limit\non this prior context; the context embodied in the previous hidden layer can include\ninformation extending back to the beginning of the sequence.\nAdding this temporal dimension makes RNNs appear to be more complex than\nnon-recurrent architectures. But in reality, they\u2019re not all that different. Given an\ninput vector and the values for the hidden layer from the previous time step, we\u2019re\nstill performing the standard feedforward calculation introduced in Chapter 7. To\nsee this, consider Fig. 8.2 which clari\ufb01es the nature of the recurrence and how it\nfactors into the computation at the hidden layer. The most signi\ufb01cant change lies in\nthe new set of weights, U, that connect the hidden layer from the previous time step\nto the current hidden layer. These weights determine how the network makes use of\npast context in calculating the output for the current input. As with the other weights\nin the network, these connections are trained via backpropagation.\n8.1.1 Inference in RNNs\nForward inference (mapping a sequence of inputs to a sequence of outputs) in an\nRNN is nearly identical to what we\u2019ve already seen with feedforward networks. To\ncompute an output ytfor an input xt, we need the activation value for the hidden\nlayer ht. To calculate this, we multiply the input xtwith the weight matrix W, and\nthe hidden layer from the previous time step ht\u00001with the weight matrix U. We\nadd these values together and pass them through a suitable activation function, g,\nto arrive at the activation value for the current hidden layer, ht. Once we have the\nvalues for the hidden layer, we proceed with the usual computation to generate the",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 258,
      "token_count": 727,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 168\n\n160 CHAPTER 8 \u2022 RNN S AND LSTM S\n+UVWyt\nxththt-1\nFigure 8.2 Simple recurrent neural network illustrated as a feedforward network. The hid-\nden layer ht\u00001from the prior time step is multiplied by weight matrix Uand then added to\nthe feedforward component from the current time step.\noutput vector.\nht=g(Uht\u00001+Wx t) (8.1)\nyt=f(Vht) (8.2)\nLet\u2019s refer to the input, hidden and output layer dimensions as din,dh, and dout\nrespectively. Given this, our three parameter matrices are: W2Rdh\u0002din,U2Rdh\u0002dh,\nandV2Rdout\u0002dh.\nWe compute ytvia a softmax computation that gives a probability distribution\nover the possible output classes.\nyt=softmax (Vht) (8.3)\nThe fact that the computation at time trequires the value of the hidden layer from\ntime t\u00001 mandates an incremental inference algorithm that proceeds from the start\nof the sequence to the end as illustrated in Fig. 8.3. The sequential nature of simple\nrecurrent networks can also be seen by unrolling the network in time as is shown in\nFig. 8.4. In this \ufb01gure, the various layers of units are copied for each time step to\nillustrate that they will have differing values over time. However, the various weight\nmatrices are shared across time.\nfunction FORWARD RNN( x,network )returns output sequence y\nh0 0\nfori 1toLENGTH (x)do\nhi g(Uhi\u00001+Wx i)\nyi f(Vhi)\nreturn y\nFigure 8.3 Forward inference in a simple recurrent network. The matrices U,VandWare\nshared across time, while new values for handyare calculated with each time step.\n8.1.2 Training\nAs with feedforward networks, we\u2019ll use a training set, a loss function, and back-\npropagation to obtain the gradients needed to adjust the weights in these recurrent\nnetworks. As shown in Fig. 8.2, we now have 3 sets of weights to update: W, the",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 259,
      "token_count": 469,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 169\n\n8.1 \u2022 R ECURRENT NEURAL NETWORKS 161\nUVWUVWUVW\nx1x2x3y1y2y3\nh1h3h2\nh0\nFigure 8.4 A simple recurrent neural network shown unrolled in time. Network layers are recalculated for\neach time step, while the weights U,VandWare shared across all time steps.\nweights from the input layer to the hidden layer, U, the weights from the previous\nhidden layer to the current hidden layer, and \ufb01nally V, the weights from the hidden\nlayer to the output layer.\nFig. 8.4 highlights two considerations that we didn\u2019t have to worry about with\nbackpropagation in feedforward networks. First, to compute the loss function for\nthe output at time twe need the hidden layer from time t\u00001. Second, the hidden\nlayer at time tin\ufb02uences both the output at time tand the hidden layer at time t+1\n(and hence the output and loss at t+1). It follows from this that to assess the error\naccruing to ht, we\u2019ll need to know its in\ufb02uence on both the current output as well as\nthe ones that follow .\nTailoring the backpropagation algorithm to this situation leads to a two-pass al-\ngorithm for training the weights in RNNs. In the \ufb01rst pass, we perform forward\ninference, computing ht,yt, accumulating the loss at each step in time, saving the\nvalue of the hidden layer at each step for use at the next time step. In the second\nphase, we process the sequence in reverse, computing the required gradients as we\ngo, computing and saving the error term for use in the hidden layer for each step\nbackward in time. This general approach is commonly referred to as backpropaga-\ntion through time (Werbos 1974, Rumelhart et al. 1986, Werbos 1990).backpropaga-\ntion through\ntimeFortunately, with modern computational frameworks and adequate computing\nresources, there is no need for a specialized approach to training RNNs. As illus-\ntrated in Fig. 8.4, explicitly unrolling a recurrent network into a feedforward com-\nputational graph eliminates any explicit recurrences, allowing the network weights\nto be trained directly. In such an approach, we provide a template that speci\ufb01es the\nbasic structure of the network, including all the necessary parameters for the input,\noutput, and hidden layers, the weight matrices, as well as the activation and output\nfunctions to be used. Then, when presented with a speci\ufb01c input sequence, we can\ngenerate an unrolled feedforward network speci\ufb01c to that input, and use that graph\nto perform forward inference or training via ordinary backpropagation.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 260,
      "token_count": 606,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 170\n\n162 CHAPTER 8 \u2022 RNN S AND LSTM S\nFor applications that involve much longer input sequences, such as speech recog-\nnition, character-level processing, or streaming continuous inputs, unrolling an en-\ntire input sequence may not be feasible. In these cases, we can unroll the input into\nmanageable \ufb01xed-length segments and treat each segment as a distinct training item.\n8.2 RNNs as Language Models\nLet\u2019s see how to apply RNNs to the language modeling task. Recall from Chapter 3\nthat language models predict the next word in a sequence given some preceding\ncontext. For example, if the preceding context is \u201cThanks for all the\u201d and we want\nto know how likely the next word is \u201c\ufb01sh\u201d we would compute:\nP(\ufb01shjThanks for all the )\nLanguage models give us the ability to assign such a conditional probability to every\npossible next word, giving us a distribution over the entire vocabulary. We can also\nassign probabilities to entire sequences by combining these conditional probabilities\nwith the chain rule:\nP(w1:n) =nY\ni=1P(wijw<i)\nThe n-gram language models of Chapter 3 compute the probability of a word given\ncounts of its occurrence with the n\u00001 prior words. The context is thus of size n\u00001.\nFor the feedforward language models of Chapter 7, the context is the window size.\nRNN language models (Mikolov et al., 2010) process the input sequence one\nword at a time, attempting to predict the next word from the current word and the\nprevious hidden state. RNNs thus don\u2019t have the limited context problem that n-gram\nmodels have, or the \ufb01xed context that feedforward language models have, since the\nhidden state can in principle represent information about all of the preceding words\nall the way back to the beginning of the sequence. Fig. 8.5 sketches this difference\nbetween a FFN language model and an RNN language model, showing that the\nRNN language model uses ht\u00001, the hidden state from the previous time step, as a\nrepresentation of the past context.\n8.2.1 Forward Inference in an RNN language model\nForward inference in a recurrent language model proceeds exactly as described in\nSection 8.1.1. The input sequence X= [x1;:::;xt;:::;xN]consists of a series of words\neach represented as a one-hot vector of size jVj\u00021, and the output prediction, y, is a\nvector representing a probability distribution over the vocabulary. At each step, the\nmodel uses the word embedding matrix Eto retrieve the embedding for the current\nword, multiples it by the weight matrix W, and then adds it to the hidden layer from\nthe previous step (weighted by weight matrix U) to compute a new hidden layer.\nThis hidden layer is then used to generate an output layer which is passed through a\nsoftmax layer to generate a probability distribution over the entire vocabulary. That\nis, at time t:\net=Ext (8.4)\nht=g(Uht\u00001+We t) (8.5)\n^ yt=softmax (Vht) (8.6)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 261,
      "token_count": 691,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 171\n\n8.2 \u2022 RNN S AS LANGUAGE MODELS 163\nVWethtUht-1ethtet-1et-2U       Wa)b)^yt\net-1^ytht-2WWet-2U\nFigure 8.5 Simpli\ufb01ed sketch of two LM architectures moving through a text, showing a\nschematic context of three tokens: (a) a feedforward neural language model which has a \ufb01xed\ncontext input to the weight matrix W, (b) an RNN language model, in which the hidden state\nht\u00001summarizes the prior context.\nWhen we do language modeling with RNNs (and we\u2019ll see this again in Chapter 9\nwith transformers), it\u2019s convenient to make the assumption that the embedding di-\nmension deand the hidden dimension dhare the same. So we\u2019ll just call both of\nthese the model dimension d. So the embedding matrix Eis of shape [d\u0002jVj], and\nxtis a one-hot vector of shape [jVj\u00021]. The product etis thus of shape [d\u00021].W\nandUare of shape [d\u0002d], sohtis also of shape [d\u00021].Vis of shape [jVj\u0002d],\nso the result of Vhis a vector of shape [jVj\u00021]. This vector can be thought of as\na set of scores over the vocabulary given the evidence provided in h. Passing these\nscores through the softmax normalizes the scores into a probability distribution. The\nprobability that a particular word kin the vocabulary is the next word is represented\nby^ yt[k], the kth component of ^ yt:\nP(wt+1=kjw1;:::; wt) = ^ yt[k] (8.7)\nThe probability of an entire sequence is just the product of the probabilities of each\nitem in the sequence, where we\u2019ll use ^ yi[wi]to mean the probability of the true word\nwiat time step i.\nP(w1:n) =nY\ni=1P(wijw1:i\u00001) (8.8)\n=nY\ni=1^ yi[wi] (8.9)\n8.2.2 Training an RNN language model\nTo train an RNN as a language model, we use the same self-supervision (orself- self-supervision\ntraining ) algorithm we saw in Section 7.7: we take a corpus of text as training\nmaterial and at each time step task the model to predict the next word. We call\nsuch a model self-supervised because we don\u2019t have to add any special gold labels\nto the data; the natural sequence of words is its own supervision! We simply train\nthe model to minimize the error in predicting the true next word in the training\nsequence, using cross-entropy as the loss function. Recall that the cross-entropy\nloss measures the difference between a predicted probability distribution and the",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 262,
      "token_count": 629,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 172",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 263,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "164 CHAPTER 8 \u2022 RNN S AND LSTM S\nInputEmbeddingsSoftmax overVocabulary\nSolongandthanksforlongandthanksforNext wordall\u2026Loss\u2026\u2026RNNhyVh",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 264,
      "token_count": 41,
      "chapter_title": ""
    }
  },
  {
    "content": "<latexit sha1_base64=\"9tru+5ysH1zS9iUXRg/IsnxmpMA=\">AAAB/XicbVDLSsNAFL3xWesr6lKQwSK4sSQi1WXRjcsK9gFNCZPpJB06yYSZiRBCcOOvuBFxo+Av+Av+jUnbTVsPDBzOOcO993gxZ0pb1q+xsrq2vrFZ2apu7+zu7ZsHhx0lEklomwguZM/DinIW0bZmmtNeLCkOPU673viu9LtPVComokedxnQQ4iBiPiNYF5Jrnlw4XATIGWGdpbmbOSHWIxlmXERBnldds2bVrQnQMrFnpAYztFzzxxkKkoQ00oRjpfq2FetBhqVmhNO86iSKxpiMcUCzyfY5OiukIfKFLF6k0USdy+FQqTT0imS5nFr0SvE/r59o/2aQsShONI3IdJCfcKQFKqtAQyYp0TwtCCaSFRsiMsISE10UVp5uLx66TDqXdbtRbzxc1Zq3sxIqcAyncA42XEMT7qEFbSDwAm/wCV/Gs/FqvBsf0+iKMftzBHMwvv8ADJKVcA==</latexit>\u0000log \u02c6ylong<latexit sha1_base64=\"tuzkS/BeX/Xmg79qpWZlpeYDhtE=\">AAAB/HicbVDLSsNAFL3xWesr6lKEwSK4sSQi1WXRjcsK9gFNCZPJpB06mYSZiRBC3PgrbkTcKPgN/oJ/Y9J209YDA4dzznDvPV7MmdKW9WusrK6tb2xWtqrbO7t7++bBYUdFiSS0TSIeyZ6HFeVM0LZmmtNeLCkOPU673viu9LtPVCoWiUedxnQQ4qFgASNYF5Jrnlw4PBoiZ4R1luZu5oRYj2SYYeHnedU1a1bdmgAtE3tGajBDyzV/HD8iSUiFJhwr1betWA8yLDUjnOZVJ1E0xmSMhzSbLJ+js0LyURDJ4gmNJupcDodKpaFXJMvd1KJXiv95/UQHN4OMiTjRVJDpoCDhSEeobAL5TFKieVoQTCQrNkRkhCUmuuirPN1ePHSZdC7rdqPeeLiqNW9nJVTgGE7hHGy4hibcQwvaQOAF3uATvoxn49V4Nz6m0RVj9ucI5mB8/wEiupTp</latexit>\u0000log \u02c6yand<latexit",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 265,
      "token_count": 721,
      "chapter_title": ""
    }
  },
  {
    "content": "\u02c6yand<latexit sha1_base64=\"0zdsmbBovZ+hafWZN7Hvufo85tU=\">AAAB/3icbVDLSsNAFJ3UV62vqEs3g0VwY0lEqsuiG5cV7AOaEibTSTN0kgkzN0IIWbjxV9yIuFHwD/wF/8ak7aatBwYO55zh3nu8WHANlvVrVNbWNza3qtu1nd29/QPz8KirZaIo61AppOp7RDPBI9YBDoL1Y8VI6AnW8yZ3pd97YkpzGT1CGrNhSMYR9zklUEiuiS8cIcfYCQhkae5mTkggUGEGAYkmOs9rrlm3GtYUeJXYc1JHc7Rd88cZSZqELAIqiNYD24phmBEFnAqW15xEs5jQCRmzbLp/js8KaYR9qYoXAZ6qCzkSap2GXpEs19PLXin+5w0S8G+GGY/iBFhEZ4P8RGCQuCwDj7hiFERaEEIVLzbENCCKUCgqK0+3lw9dJd3Lht1sNB+u6q3beQlVdIJO0Tmy0TVqoXvURh1E0Qt6Q5/oy3g2Xo1342MWrRjzP8doAcb3H7Aall0=</latexit>\u0000log \u02c6ythanks<latexit sha1_base64=\"D3c31Jvxp3QWPr2h4tzQWmeenDs=\">AAAB/HicbVDLSsNAFL3xWesr6lKEwSK4sSQi1WXRjcsK9gFNCZPppB06yYSZiRBC3PgrbkTcKPgN/oJ/Y9Jm09YDA4dzznDvPV7EmdKW9WusrK6tb2xWtqrbO7t7++bBYUeJWBLaJoIL2fOwopyFtK2Z5rQXSYoDj9OuN7kr/O4TlYqJ8FEnER0EeBQynxGsc8k1Ty4cLkbIGWOdJpmbOgHWYxmkvpBZVnXNmlW3pkDLxC5JDUq0XPPHGQoSBzTUhGOl+rYV6UGKpWaE06zqxIpGmEzwiKbT5TN0lktDlM/LX6jRVJ3L4UCpJPDyZLGbWvQK8T+vH2v/ZpCyMIo1DclskB9zpAUqmkBDJinRPMkJJpLlGyIyxhITnfdVnG4vHrpMOpd1u1FvPFzVmrdlCRU4hlM4BxuuoQn30II2EHiBN/iEL+PZeDXejY9ZdMUo/xzBHIzvP0CJlP0=</latexit>\u0000log \u02c6yfor<latexit",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 266,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "\u02c6yfor<latexit sha1_base64=\"PI3y1fb9LhumoVCQRh2+Y84dRkc=\">AAAB/HicbVDLSsNAFL3xWesr6lKEwSK4sSQi1WXRjcsK9gFNCZPppB06yYSZiRBC3PgrbkTcKPgN/oJ/Y9Jm09YDA4dzznDvPV7EmdKW9WusrK6tb2xWtqrbO7t7++bBYUeJWBLaJoIL2fOwopyFtK2Z5rQXSYoDj9OuN7kr/O4TlYqJ8FEnER0EeBQynxGsc8k1Ty4cLkbIGWOdJpmbOgHWYxmkmPMsq7pmzapbU6BlYpekBiVarvnjDAWJAxpqwrFSfduK9CDFUjPCaVZ1YkUjTCZ4RNPp8hk6y6Uh8oXMX6jRVJ3L4UCpJPDyZLGbWvQK8T+vH2v/ZpCyMIo1DclskB9zpAUqmkBDJinRPMkJJpLlGyIyxhITnfdVnG4vHrpMOpd1u1FvPFzVmrdlCRU4hlM4BxuuoQn30II2EHiBN/iEL+PZeDXejY9ZdMUo/xzBHIzvPyumlO8=</latexit>\u0000log \u02c6yall",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 267,
      "token_count": 382,
      "chapter_title": ""
    }
  },
  {
    "content": "e\nFigure 8.6 Training RNNs as language models.\ncorrect distribution.\nLCE=\u0000X\nw2Vyt[w]log\u02c6yt[w] (8.10)\nIn the case of language modeling, the correct distribution ytcomes from knowing the\nnext word. This is represented as a one-hot vector corresponding to the vocabulary\nwhere the entry for the actual next word is 1, and all the other entries are 0. Thus,\nthe cross-entropy loss for language modeling is determined by the probability the\nmodel assigns to the correct next word. So at time tthe CE loss is the negative log\nprobability the model assigns to the next word in the training sequence.\nLCE(\u02c6yt;yt) =\u0000log\u02c6yt[wt+1] (8.11)\nThus at each word position tof the input, the model takes as input the correct word wt\ntogether with ht\u00001, encoding information from the preceding w1:t\u00001, and uses them\nto compute a probability distribution over possible next words so as to compute the\nmodel\u2019s loss for the next token wt+1. Then we move to the next word, we ignore\nwhat the model predicted for the next word and instead use the correct word wt+1\nalong with the prior history encoded to estimate the probability of token wt+2. This\nidea that we always give the model the correct history sequence to predict the next\nword (rather than feeding the model its best case from the previous time step) is\ncalled teacher forcing . teacher forcing\nThe weights in the network are adjusted to minimize the average CE loss over\nthe training sequence via gradient descent. Fig. 8.6 illustrates this training regimen.\n8.2.3 Weight Tying\nCareful readers may have noticed that the input embedding matrix Eand the \ufb01nal\nlayer matrix V, which feeds the output softmax, are quite similar.\nThe columns of Erepresent the word embeddings for each word in the vocab-\nulary learned during the training process with the goal that words that have similar\nmeaning and function will have similar embeddings. And, since when we use RNNs\nfor language modeling we make the assumption that the embedding dimension and",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 268,
      "token_count": 459,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 173\n\n8.3 \u2022 RNN S FOR OTHER NLP TASKS 165\nthe hidden dimension are the same (= the model dimension d), the embedding ma-\ntrixEhas shape [d\u0002jVj]. And the \ufb01nal layer matrix Vprovides a way to score\nthe likelihood of each word in the vocabulary given the evidence present in the \ufb01nal\nhidden layer of the network through the calculation of Vh.Vis of shape [jVj\u0002d].\nThat is, is, the rows of Vare shaped like a transpose of E, meaning that Vprovides\nasecond set of learned word embeddings.\nInstead of having two sets of embedding matrices, language models use a single\nembedding matrix, which appears at both the input and softmax layers. That is,\nwe dispense with Vand use Eat the start of the computation and E|(because the\nshape of Vis the transpose of Eat the end. Using the same matrix (transposed) in\ntwo places is called weight tying .1The weight-tied equations for an RNN language weight tying\nmodel then become:\net=Ext (8.12)\nht=g(Uht\u00001+We t) (8.13)\n^ yt=softmax (E|ht) (8.14)\nIn addition to providing improved model perplexity, this approach signi\ufb01cantly re-\nduces the number of parameters required for the model.\n8.3 RNNs for other NLP tasks\nNow that we\u2019ve seen the basic RNN architecture, let\u2019s consider how to apply it to\nthree types of NLP tasks: sequence classi\ufb01cation tasks like sentiment analysis and\ntopic classi\ufb01cation, sequence labeling tasks like part-of-speech tagging, and text\ngeneration tasks, including with a new architecture called the encoder-decoder .\n8.3.1 Sequence Labeling\nIn sequence labeling, the network\u2019s task is to assign a label chosen from a small\n\ufb01xed set of labels to each element of a sequence. One classic sequence labeling\ntasks is part-of-speech (POS) tagging (assigning grammatical tags like NOUN and\nVERB to each word in a sentence). We\u2019ll discuss part-of-speech tagging in detail\nin Chapter 17, but let\u2019s give a motivating example here. In an RNN approach to\nsequence labeling, inputs are word embeddings and the outputs are tag probabilities\ngenerated by a softmax layer over the given tagset, as illustrated in Fig. 8.7.\nIn this \ufb01gure, the inputs at each time step are pretrained word embeddings cor-\nresponding to the input tokens. The RNN block is an abstraction that represents\nan unrolled simple recurrent network consisting of an input layer, hidden layer, and\noutput layer at each time step, as well as the shared U,VandWweight matrices\nthat comprise the network. The outputs of the network at each time step represent\nthe distribution over the POS tagset generated by a softmax layer.\nTo generate a sequence of tags for a given input, we run forward inference over\nthe input sequence and select the most likely tag from the softmax at each step. Since\nwe\u2019re using a softmax layer to generate the probability distribution over the output\ntagset at each time step, we will again employ the cross-entropy loss during training.\n1We also do this for transformers (Chapter 9) where it\u2019s common to call E|theunembedding matrix .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 269,
      "token_count": 718,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 174\n\n166 CHAPTER 8 \u2022 RNN S AND LSTM S\nJanetwillbackthebillNNDTVBMDNNPArgmax\nEmbeddingsWordsehVhyRNNLayer(s)Softmax overtags\nFigure 8.7 Part-of-speech tagging as sequence labeling with a simple RNN. The goal of\npart-of-speech (POS) tagging is to assign a grammatical label to each word in a sentence,\ndrawn from a prede\ufb01ned set of tags. (The tags for this sentence include NNP (proper noun),\nMD (modal verb) and others; we\u2019ll give a complete description of the task of part-of-speech\ntagging in Chapter 17.) Pre-trained word embeddings serve as inputs and a softmax layer\nprovides a probability distribution over the part-of-speech tags as output at each time step.\n8.3.2 RNNs for Sequence Classi\ufb01cation\nAnother use of RNNs is to classify entire sequences rather than the tokens within\nthem. This is the set of tasks commonly called text classi\ufb01cation , like sentiment\nanalysis or spam detection, in which we classify a text into two or three classes\n(like positive or negative), as well as classi\ufb01cation tasks with a large number of\ncategories, like document-level topic classi\ufb01cation, or message routing for customer\nservice applications.\nTo apply RNNs in this setting, we pass the text to be classi\ufb01ed through the RNN\na word at a time generating a new hidden layer representation at each time step.\nWe can then take the hidden layer for the last token of the text, hn, to constitute a\ncompressed representation of the entire sequence. We can pass this representation\nhnto a feedforward network that chooses a class via a softmax over the possible\nclasses. Fig. 8.8 illustrates this approach.\nNote that in this approach we don\u2019t need intermediate outputs for the words in\nthe sequence preceding the last element. Therefore, there are no loss terms associ-\nated with those elements. Instead, the loss function used to train the weights in the\nnetwork is based entirely on the \ufb01nal text classi\ufb01cation task. The output from the\nsoftmax output from the feedforward classi\ufb01er together with a cross-entropy loss\ndrives the training. The error signal from the classi\ufb01cation is backpropagated all the\nway through the weights in the feedforward classi\ufb01er through, to its input, and then\nthrough to the three sets of weights in the RNN as described earlier in Section 8.1.2.\nThe training regimen that uses the loss from a downstream application to adjust the\nweights all the way through the network is referred to as end-to-end training .end-to-end\ntraining\nAnother option, instead of using just hidden state of the last token hnto represent\nthe whole sequence, is to use some sort of pooling function of all the hidden states pooling\nhifor each word iin the sequence. For example, we can create a representation that",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 270,
      "token_count": 651,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 175\n\n8.3 \u2022 RNN S FOR OTHER NLP TASKS 167\nx1\nRNNhnx2x3xnSoftmaxFFN\nFigure 8.8 Sequence classi\ufb01cation using a simple RNN combined with a feedforward net-\nwork. The \ufb01nal hidden state from the RNN is used as the input to a feedforward network that\nperforms the classi\ufb01cation.\npools all the nhidden states by taking their element-wise mean:\nhmean=1\nnnX\ni=1hi (8.15)\nOr we can take the element-wise max; the element-wise max of a set of nvectors is\na new vector whose kth element is the max of the kth elements of all the nvectors.\nThe long contexts of RNNs makes it quite dif\ufb01cult to successfully backpropagate\nerror all the way through the entire input; we\u2019ll talk about this problem, and some\nstandard solutions, in Section 8.5.\n8.3.3 Generation with RNN-Based Language Models\nRNN-based language models can also be used to generate text. Text generation is\nof enormous practical importance, part of tasks like question answering, machine\ntranslation, text summarization, grammar correction, story generation, and conver-\nsational dialogue; any task where a system needs to produce text, conditioned on\nsome other text. This use of a language model to generate text is one of the areas\nin which the impact of neural language models on NLP has been the largest. Text\ngeneration, along with image generation and code generation, constitute a new area\nof AI that is often called generative AI .\nRecall back in Chapter 3 we saw how to generate text from an n-gram language\nmodel by adapting a sampling technique suggested at about the same time by Claude\nShannon (Shannon, 1951) and the psychologists George Miller and Jennifer Self-\nridge (Miller and Selfridge, 1950). We \ufb01rst randomly sample a word to begin a\nsequence based on its suitability as the start of a sequence. We then continue to\nsample words conditioned on our previous choices until we reach a pre-determined\nlength, or an end of sequence token is generated.\nToday, this approach of using a language model to incrementally generate words\nby repeatedly sampling the next word conditioned on our previous choices is called\nautoregressive generation orcausal LM generation . The procedure is basicallyautoregressive\ngeneration\nthe same as that described on page 43, but adapted to a neural context:\n\u2022 Sample a word in the output from the softmax distribution that results from\nusing the beginning of sentence marker, <s>, as the \ufb01rst input.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 271,
      "token_count": 571,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 176\n\n168 CHAPTER 8 \u2022 RNN S AND LSTM S\n\u2022 Use the word embedding for that \ufb01rst word as the input to the network at the\nnext time step, and then sample the next word in the same fashion.\n\u2022 Continue generating until the end of sentence marker, </s> , is sampled or a\n\ufb01xed length limit is reached.\nTechnically an autoregressive model is a model that predicts a value at time tbased\non a linear function of the previous values at times t\u00001,t\u00002, and so on. Although\nlanguage models are not linear (since they have many layers of non-linearities), we\nloosely refer to this generation technique as autoregressive generation since the\nword generated at each time step is conditioned on the word selected by the network\nfrom the previous step. Fig. 8.9 illustrates this approach. In this \ufb01gure, the details of\nthe RNN\u2019s hidden layers and recurrent connections are hidden within the blue block.\nThis simple architecture underlies state-of-the-art approaches to applications\nsuch as machine translation, summarization, and question answering. The key to\nthese approaches is to prime the generation component with an appropriate context.\nThat is, instead of simply using <s> to get things started we can provide a richer\ntask-appropriate context; for translation the context is the sentence in the source\nlanguage; for summarization it\u2019s the long text we want to summarize.\nSolong\n<s>and\nSolongand?Sampled WordSoftmaxEmbeddingInput WordRNN\nFigure 8.9 Autoregressive generation with an RNN-based neural language model.\n8.4 Stacked and Bidirectional RNN architectures\nRecurrent networks are quite \ufb02exible. By combining the feedforward nature of un-\nrolled computational graphs with vectors as common inputs and outputs, complex\nnetworks can be treated as modules that can be combined in creative ways. This\nsection introduces two of the more common network architectures used in language\nprocessing with RNNs.\n8.4.1 Stacked RNNs\nIn our examples thus far, the inputs to our RNNs have consisted of sequences of\nword or character embeddings (vectors) and the outputs have been vectors useful for\npredicting words, tags or sequence labels. However, nothing prevents us from using",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 272,
      "token_count": 491,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 177\n\n8.4 \u2022 S TACKED AND BIDIRECTIONAL RNN ARCHITECTURES 169\nthe entire sequence of outputs from one RNN as an input sequence to another one.\nStacked RNNs consist of multiple networks where the output of one layer serves as Stacked RNNs\nthe input to a subsequent layer, as shown in Fig. 8.10.\ny1y2y3yn\nx1x2x3xn\nRNN 1\nRNN 2\n RNN 3\nFigure 8.10 Stacked recurrent networks. The output of a lower level serves as the input to\nhigher levels with the output of the last network serving as the \ufb01nal output.\nStacked RNNs generally outperform single-layer networks. One reason for this\nsuccess seems to be that the network induces representations at differing levels of\nabstraction across layers. Just as the early stages of the human visual system detect\nedges that are then used for \ufb01nding larger regions and shapes, the initial layers of\nstacked networks can induce representations that serve as useful abstractions for\nfurther layers\u2014representations that might prove dif\ufb01cult to induce in a single RNN.\nThe optimal number of stacked RNNs is speci\ufb01c to each application and to each\ntraining set. However, as the number of stacks is increased the training costs rise\nquickly.\n8.4.2 Bidirectional RNNs\nThe RNN uses information from the left (prior) context to make its predictions at\ntime t. But in many applications we have access to the entire input sequence; in\nthose cases we would like to use words from the context to the right of t. One way\nto do this is to run two separate RNNs, one left-to-right, and one right-to-left, and\nconcatenate their representations.\nIn the left-to-right RNNs we\u2019ve discussed so far, the hidden state at a given time\ntrepresents everything the network knows about the sequence up to that point. The\nstate is a function of the inputs x1;:::;xtand represents the context of the network to\nthe left of the current time.\nhf\nt=RNN forward (x1;:::;xt) (8.16)\nThis new notation hf\ntsimply corresponds to the normal hidden state at time t, repre-\nsenting everything the network has gleaned from the sequence so far.\nTo take advantage of context to the right of the current input, we can train an\nRNN on a reversed input sequence. With this approach, the hidden state at time t\nrepresents information about the sequence to the right of the current input:\nhb\nt=RNN backward (xt;:::xn) (8.17)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 273,
      "token_count": 588,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 178\n\n170 CHAPTER 8 \u2022 RNN S AND LSTM S\nHere, the hidden state hb\ntrepresents all the information we have discerned about the\nsequence from tto the end of the sequence.\nAbidirectional RNN (Schuster and Paliwal, 1997) combines two independentbidirectional\nRNN\nRNNs, one where the input is processed from the start to the end, and the other from\nthe end to the start. We then concatenate the two representations computed by the\nnetworks into a single vector that captures both the left and right contexts of an input\nat each point in time. Here we use either the semicolon \u201d;\u201d or the equivalent symbol\n\bto mean vector concatenation:\nht= [hf\nt;hb\nt]\n=hf\nt\bhb\nt (8.18)\nFig. 8.11 illustrates such a bidirectional network that concatenates the outputs of\nthe forward and backward pass. Other simple ways to combine the forward and\nbackward contexts include element-wise addition or multiplication. The output at\neach step in time thus captures information to the left and to the right of the current\ninput. In sequence labeling applications, these concatenated outputs can serve as the\nbasis for a local labeling decision.\nRNN 2 \nRNN 1x1y2y1y3ynconcatenatedoutputs\nx2x3xn\nFigure 8.11 A bidirectional RNN. Separate models are trained in the forward and backward\ndirections, with the output of each model at each time point concatenated to represent the\nbidirectional state at that time point.\nBidirectional RNNs have also proven to be quite effective for sequence classi\ufb01-\ncation. Recall from Fig. 8.8 that for sequence classi\ufb01cation we used the \ufb01nal hidden\nstate of the RNN as the input to a subsequent feedforward classi\ufb01er. A dif\ufb01culty\nwith this approach is that the \ufb01nal state naturally re\ufb02ects more information about\nthe end of the sentence than its beginning. Bidirectional RNNs provide a simple\nsolution to this problem; as shown in Fig. 8.12, we simply combine the \ufb01nal hidden\nstates from the forward and backward passes (for example by concatenation) and\nuse that as input for follow-on processing.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 274,
      "token_count": 503,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 179\n\n8.5 \u2022 T HELSTM 171\nRNN 2 \nRNN 1x1x2x3xnhn\u2192h1\u2190hn\u2192SoftmaxFFNh1\u2190\nFigure 8.12 A bidirectional RNN for sequence classi\ufb01cation. The \ufb01nal hidden units from\nthe forward and backward passes are combined to represent the entire sequence. This com-\nbined representation serves as input to the subsequent classi\ufb01er.\n8.5 The LSTM\nIn practice, it is quite dif\ufb01cult to train RNNs for tasks that require a network to make\nuse of information distant from the current point of processing. Despite having ac-\ncess to the entire preceding sequence, the information encoded in hidden states tends\nto be fairly local, more relevant to the most recent parts of the input sequence and\nrecent decisions. Yet distant information is critical to many language applications.\nConsider the following example in the context of language modeling.\n(8.19) The \ufb02ights the airline was canceling were full.\nAssigning a high probability to wasfollowing airline is straightforward since airline\nprovides a strong local context for the singular agreement. However, assigning an\nappropriate probability to were is quite dif\ufb01cult, not only because the plural \ufb02ights\nis quite distant, but also because the singular noun airline is closer in the intervening\ncontext. Ideally, a network should be able to retain the distant information about\nplural \ufb02ights until it is needed, while still processing the intermediate parts of the\nsequence correctly.\nOne reason for the inability of RNNs to carry forward critical information is that\nthe hidden layers, and, by extension, the weights that determine the values in the hid-\nden layer, are being asked to perform two tasks simultaneously: provide information\nuseful for the current decision, and updating and carrying forward information re-\nquired for future decisions.\nA second dif\ufb01culty with training RNNs arises from the need to backpropagate\nthe error signal back through time. Recall from Section 8.1.2 that the hidden layer at\ntime tcontributes to the loss at the next time step since it takes part in that calcula-\ntion. As a result, during the backward pass of training, the hidden layers are subject\nto repeated multiplications, as determined by the length of the sequence. A frequent\nresult of this process is that the gradients are eventually driven to zero, a situation",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 275,
      "token_count": 516,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 180\n\n172 CHAPTER 8 \u2022 RNN S AND LSTM S\ncalled the vanishing gradients problem.vanishing\ngradients\nTo address these issues, more complex network architectures have been designed\nto explicitly manage the task of maintaining relevant context over time, by enabling\nthe network to learn to forget information that is no longer needed and to remember\ninformation required for decisions still to come.\nThe most commonly used such extension to RNNs is the long short-term mem-\nory(LSTM) network (Hochreiter and Schmidhuber, 1997). LSTMs divide the con-long short-term\nmemory\ntext management problem into two subproblems: removing information no longer\nneeded from the context, and adding information likely to be needed for later de-\ncision making. The key to solving both problems is to learn how to manage this\ncontext rather than hard-coding a strategy into the architecture. LSTMs accomplish\nthis by \ufb01rst adding an explicit context layer to the architecture (in addition to the\nusual recurrent hidden layer), and through the use of specialized neural units that\nmake use of gates to control the \ufb02ow of information into and out of the units that\ncomprise the network layers. These gates are implemented through the use of addi-\ntional weights that operate sequentially on the input, and previous hidden layer, and\nprevious context layers.\nThe gates in an LSTM share a common design pattern; each consists of a feed-\nforward layer, followed by a sigmoid activation function, followed by a pointwise\nmultiplication with the layer being gated. The choice of the sigmoid as the activation\nfunction arises from its tendency to push its outputs to either 0 or 1. Combining this\nwith a pointwise multiplication has an effect similar to that of a binary mask. Values\nin the layer being gated that align with values near 1 in the mask are passed through\nnearly unchanged; values corresponding to lower values are essentially erased.\nThe \ufb01rst gate we\u2019ll consider is the forget gate . The purpose of this gate is forget gate\nto delete information from the context that is no longer needed. The forget gate\ncomputes a weighted sum of the previous state\u2019s hidden layer and the current in-\nput and passes that through a sigmoid. This mask is then multiplied element-wise\nby the context vector to remove the information from context that is no longer re-\nquired. Element-wise multiplication of two vectors (represented by the operator \f,\nand sometimes called the Hadamard product ) is the vector of the same dimension\nas the two input vectors, where each element iis the product of element iin the two\ninput vectors:\nft=s(Ufht\u00001+Wfxt) (8.20)\nkt=ct\u00001\fft (8.21)\nThe next task is to compute the actual information we need to extract from the previ-\nous hidden state and current inputs\u2014the same basic computation we\u2019ve been using\nfor all our recurrent networks.\ngt=tanh(Ught\u00001+Wgxt) (8.22)\nNext, we generate the mask for the add gate to select the information to add to the add gate\ncurrent context.\nit=s(Uiht\u00001+Wixt) (8.23)\njt=gt\fit (8.24)\nNext, we add this to the modi\ufb01ed context vector to get our new context vector.\nct=jt+kt (8.25)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 276,
      "token_count": 715,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 181\n\n8.5 \u2022 T HELSTM 173\n+\nxtht-1cthtcthtct-1ht-1xttanh\n+\u03c3tanh\u03c3\u03c3+++igf\no\u3f4b\u3f4b\u3f4bLSTMct-1\nFigure 8.13 A single LSTM unit displayed as a computation graph. The inputs to each unit consists of the\ncurrent input, x, the previous hidden state, ht\u00001, and the previous context, ct\u00001. The outputs are a new hidden\nstate, htand an updated context, ct.\nThe \ufb01nal gate we\u2019ll use is the output gate which is used to decide what informa- output gate\ntion is required for the current hidden state (as opposed to what information needs\nto be preserved for future decisions).\not=s(Uoht\u00001+Woxt) (8.26)\nht=ot\ftanh(ct) (8.27)\nFig. 8.13 illustrates the complete computation for a single LSTM unit. Given the\nappropriate weights for the various gates, an LSTM accepts as input the context\nlayer, and hidden layer from the previous time step, along with the current input\nvector. It then generates updated context and hidden vectors as output.\nIt is the hidden state, ht, that provides the output for the LSTM at each time step.\nThis output can be used as the input to subsequent layers in a stacked RNN, or at the\n\ufb01nal layer of a network htcan be used to provide the \ufb01nal output of the LSTM.\n8.5.1 Gated Units, Layers and Networks\nThe neural units used in LSTMs are obviously much more complex than those used\nin basic feedforward networks. Fortunately, this complexity is encapsulated within\nthe basic processing units, allowing us to maintain modularity and to easily exper-\niment with different architectures. To see this, consider Fig. 8.14 which illustrates\nthe inputs and outputs associated with each kind of unit.\nAt the far left, (a) is the basic feedforward unit where a single set of weights and\na single activation function determine its output, and when arranged in a layer there\nare no connections among the units in the layer. Next, (b) represents the unit in a\nsimple recurrent network. Now there are two inputs and an additional set of weights\nto go with it. However, there is still a single activation function and output.\nThe increased complexity of the LSTM units is encapsulated within the unit\nitself. The only additional external complexity for the LSTM over the basic recurrent\nunit (b) is the presence of the additional context vector as an input and output.\nThis modularity is key to the power and widespread applicability of LSTM units.\nLSTM units (or other varieties, like GRUs) can be substituted into any of the network\narchitectures described in Section 8.4. And, as with simple RNNs, multi-layered\nnetworks making use of gated units can be unrolled into deep feedforward networks",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 277,
      "token_count": 636,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 182\n\n174 CHAPTER 8 \u2022 RNN S AND LSTM S\nh\nxxtxtht-1htht\nct-1ct\nht-1(b)(a)(c)\u2303gza\u2303gzLSTMUnita\nFigure 8.14 Basic neural units used in feedforward, simple recurrent networks (SRN), and\nlong short-term memory (LSTM).\nand trained in the usual fashion with backpropagation. In practice, therefore, LSTMs\nrather than RNNs have become the standard unit for any modern system that makes\nuse of recurrent networks.\n8.6 Summary: Common RNN NLP Architectures\nWe\u2019ve now introduced the RNN, seen advanced components like stacking multiple\nlayers and using the LSTM version, and seen how the RNN can be applied to various\ntasks. Let\u2019s take a moment to summarize the architectures for these applications.\nFig. 8.15 shows the three architectures we\u2019ve discussed so far: sequence la-\nbeling, sequence classi\ufb01cation, and language modeling. In sequence labeling (for\nexample for part of speech tagging), we train a model to produce a label for each\ninput word or token. In sequence classi\ufb01cation, for example for sentiment analysis,\nwe ignore the output for each token, and only take the value from the end of the\nsequence (and similarly the model\u2019s training signal comes from backpropagation\nfrom that last token). In language modeling, we train the model to predict the next\nword at each token step. In the next section we\u2019ll introduce a fourth architecture, the\nencoder-decoder .\n8.7 The Encoder-Decoder Model with RNNs\nIn this section we introduce a new model, the encoder-decoder model, which is used\nwhen we are taking an input sequence and translating it to an output sequence that is\nof a different length than the input, and doesn\u2019t align with it in a word-to-word way.\nRecall that in the sequence labeling task, we have two sequences, but they are the\nsame length (for example in part-of-speech tagging each token gets an associated\ntag), each input is associated with a speci\ufb01c output, and the labeling for that output\ntakes mostly local information. Thus deciding whether a word is a verb or a noun,\nwe look mostly at the word and the neighboring words.\nBy contrast, encoder-decoder models are used especially for tasks like machine\ntranslation, where the input sequence and output sequence can have different lengths",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 278,
      "token_count": 527,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 183\n\n8.7 \u2022 T HEENCODER -DECODER MODEL WITH RNN S175\n\u2026Encoder RNNDecoder RNNContext\u2026\nx1x2xny1y2ym\u2026RNNx1x2xn\u2026y1y2yn\u2026RNNx1x2xny\n\u2026RNNx1x2xt-1\u2026x2x3xta) sequence labeling b) sequence classification \nc) language modelingd) encoder-decoder\nFigure 8.15 Four architectures for NLP tasks. In sequence labeling (POS or named entity tagging) we map\neach input token xito an output token yi. In sequence classi\ufb01cation we map the entire input sequence to a single\nclass. In language modeling we output the next token conditioned on previous tokens. In the encoder model we\nhave two separate RNN models, one of which maps from an input sequence xto an intermediate representation\nwe call the context , and a second of which maps from the context to an output sequence y.\nand the mapping between a token in the input and a token in the output can be very\nindirect (in some languages the verb appears at the beginning of the sentence; in\nother languages at the end). We\u2019ll introduce machine translation in detail in Chap-\nter 13, but for now we\u2019ll just point out that the mapping for a sentence in English to\na sentence in Tagalog or Yoruba can have very different numbers of words, and the\nwords can be in a very different order.\nEncoder-decoder networks, sometimes called sequence-to-sequence networks,encoder-\ndecoder\nare models capable of generating contextually appropriate, arbitrary length, output\nsequences given an input sequence. Encoder-decoder networks have been applied\nto a very wide range of applications including summarization, question answering,\nand dialogue, but they are particularly popular for machine translation.\nThe key idea underlying these networks is the use of an encoder network that\ntakes an input sequence and creates a contextualized representation of it, often called\nthecontext . This representation is then passed to a decoder which generates a task-\nspeci\ufb01c output sequence. Fig. 8.16 illustrates the architecture.\nEncoder-decoder networks consist of three conceptual components:\n1. An encoder that accepts an input sequence, x1:n, and generates a correspond-\ning sequence of contextualized representations, h1:n. LSTMs, convolutional\nnetworks, and transformers can all be employed as encoders.\n2. A context vector ,c, which is a function of h1:n, and conveys the essence of\nthe input to the decoder.\n3. A decoder , which accepts cas input and generates an arbitrary length se-\nquence of hidden states h1:m, from which a corresponding sequence of output\nstates y1:m, can be obtained. Just as with encoders, decoders can be realized",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 279,
      "token_count": 599,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 184\n\n176 CHAPTER 8 \u2022 RNN S AND LSTM S\n\u2026EncoderDecoderContext\u2026\nx1x2xny1y2ym\nFigure 8.16 The encoder-decoder architecture. The context is a function of the hidden\nrepresentations of the input, and may be used by the decoder in a variety of ways.\nby any kind of sequence architecture.\nIn this section we\u2019ll describe an encoder-decoder network based on a pair of\nRNNs, but we\u2019ll see in Chapter 13 how to apply them to transformers as well. We\u2019ll\nbuild up the equations for encoder-decoder models by starting with the conditional\nRNN language model p(y), the probability of a sequence y.\nRecall that in any language model, we can break down the probability as follows:\np(y) = p(y1)p(y2jy1)p(y3jy1;y2):::p(ymjy1;:::;ym\u00001) (8.28)\nIn RNN language modeling, at a particular time t, we pass the pre\ufb01x of t\u00001\ntokens through the language model, using forward inference to produce a sequence\nof hidden states, ending with the hidden state corresponding to the last word of\nthe pre\ufb01x. We then use the \ufb01nal hidden state of the pre\ufb01x as our starting point to\ngenerate the next token.\nMore formally, if gis an activation function like tanh or ReLU, a function of\nthe input at time tand the hidden state at time t\u00001, and the softmax is over the\nset of possible vocabulary items, then at time tthe output ytand hidden state htare\ncomputed as:\nht=g(ht\u00001;xt) (8.29)\n^ yt=softmax (ht) (8.30)\nWe only have to make one slight change to turn this language model with au-\ntoregressive generation into an encoder-decoder model that is a translation model\nthat can translate from a source text in one language to a target text in a second:\nadd a sentence separation marker at the end of the source text, and then simplysentence\nseparation\nconcatenate the target text.\nLet\u2019s use<s>for our sentence separator token, and let\u2019s think about translating\nan English source text (\u201cthe green witch arrived\u201d), to a Spanish sentence (\u201c lleg\u00b4o\nla bruja verde \u201d (which can be glossed word-by-word as \u2018arrived the witch green\u2019).\nWe could also illustrate encoder-decoder models with a question-answer pair, or a\ntext-summarization pair.\nLet\u2019s use xto refer to the source text (in this case in English) plus the separator\ntoken<s>, and yto refer to the target text y(in this case in Spanish). Then an\nencoder-decoder model computes the probability p(yjx)as follows:\np(yjx) = p(y1jx)p(y2jy1;x)p(y3jy1;y2;x):::p(ymjy1;:::;ym\u00001;x) (8.31)\nFig. 8.17 shows the setup for a simpli\ufb01ed version of the encoder-decoder model\n(we\u2019ll see the full model, which requires the new concept of attention , in the next\nsection).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 280,
      "token_count": 694,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 185\n\n8.7 \u2022 T HEENCODER -DECODER MODEL WITH RNN S177\nSource TextTarget Text\nhnembeddinglayerhiddenlayer(s)softmaxthegreenlleg\u00f3\nwitcharrived<s>lleg\u00f3la\nlabruja\nbrujaverde\nverde</s>(output of source is ignored)\nSeparator\nFigure 8.17 Translating a single sentence (inference time) in the basic RNN version of encoder-decoder ap-\nproach to machine translation. Source and target sentences are concatenated with a separator token in between,\nand the decoder uses context information from the encoder\u2019s last hidden state.\nFig. 8.17 shows an English source text (\u201cthe green witch arrived\u201d), a sentence\nseparator token ( <s>, and a Spanish target text (\u201c lleg\u00b4o la bruja verde \u201d). To trans-\nlate a source text, we run it through the network performing forward inference to\ngenerate hidden states until we get to the end of the source. Then we begin autore-\ngressive generation, asking for a word in the context of the hidden layer from the\nend of the source input as well as the end-of-sentence marker. Subsequent words\nare conditioned on the previous hidden state and the embedding for the last word\ngenerated.\nLet\u2019s formalize and generalize this model a bit in Fig. 8.18. (To help keep things\nstraight, we\u2019ll use the superscripts eanddwhere needed to distinguish the hidden\nstates of the encoder and the decoder.) The elements of the network on the left\nprocess the input sequence xand comprise the encoder . While our simpli\ufb01ed \ufb01gure\nshows only a single network layer for the encoder, stacked architectures are the\nnorm, where the output states from the top layer of the stack are taken as the \ufb01nal\nrepresentation, and the encoder consists of stacked biLSTMs where the hidden states\nfrom top layers from the forward and backward passes are concatenated to provide\nthe contextualized representations for each time step.\nThe entire purpose of the encoder is to generate a contextualized representation\nof the input. This representation is embodied in the \ufb01nal hidden state of the encoder,\nhe\nn. This representation, also called cforcontext , is then passed to the decoder.\nThe simplest version of the decoder network would take this state and use it\njust to initialize the \ufb01rst hidden state of the decoder; the \ufb01rst decoder RNN cell\nwould use cas its prior hidden state hd\n0. The decoder would then autoregressively\ngenerates a sequence of outputs, an element at a time, until an end-of-sequence\nmarker is generated. Each hidden state is conditioned on the previous hidden state\nand the output generated in the previous state.\nAs Fig. 8.18 shows, we do something more complex: we make the context vector\ncavailable to more than just the \ufb01rst decoder hidden state, to ensure that the in\ufb02uence\nof the context vector, c, doesn\u2019t wane as the output sequence is generated. We do\nthis by adding cas a parameter to the computation of the current hidden state. using\nthe following equation:\nhd\nt=g(\u02c6yt\u00001;hd\nt\u00001;c) (8.32)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 281,
      "token_count": 692,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 186\n\n178 CHAPTER 8 \u2022 RNN S AND LSTM S\nEncoderDecoder\nhn hd1he3he2he1hd2hd3hd4embeddinglayerhiddenlayer(s)softmaxx1x2y1hdmx3xn<s>y1y2\ny2y3\ny3y4\nym</s>hen = c = hd0(output is ignored during encoding)\nFigure 8.18 A more formal version of translating a sentence at inference time in the basic RNN-based\nencoder-decoder architecture. The \ufb01nal hidden state of the encoder RNN, hen, serves as the context for the\ndecoder in its role as hd\n0in the decoder RNN, and is also made available to each decoder hidden state.\nNow we\u2019re ready to see the full equations for this version of the decoder in the basic\nencoder-decoder model, with context available at each decoding timestep. Recall\nthatgis a stand-in for some \ufb02avor of RNN and \u02c6 yt\u00001is the embedding for the output\nsampled from the softmax at the previous step:\nc=he\nn\nhd\n0=c\nhd\nt=g(\u02c6yt\u00001;hd\nt\u00001;c)\n^ yt=softmax (hd\nt) (8.33)\nThus ^ ytis a vector of probabilities over the vocabulary, representing the probability\nof each word occurring at time t. To generate text, we sample from this distribution\n^ yt. For example, the greedy choice is simply to choose the most probable word to\ngenerate at each timestep. We\u2019ll introduce more sophisticated sampling methods in\nSection 10.2.\n8.7.1 Training the Encoder-Decoder Model\nEncoder-decoder architectures are trained end-to-end. Each training example is a\ntuple of paired strings, a source and a target. Concatenated with a separator token,\nthese source-target pairs can now serve as training data.\nFor MT, the training data typically consists of sets of sentences and their transla-\ntions. These can be drawn from standard datasets of aligned sentence pairs, as we\u2019ll\ndiscuss in Section 13.2.2. Once we have a training set, the training itself proceeds\nas with any RNN-based language model. The network is given the source text and\nthen starting with the separator token is trained autoregressively to predict the next\nword, as shown in Fig. 8.19.\nNote the differences between training (Fig. 8.19) and inference (Fig. 8.17) with\nrespect to the outputs at each time step. The decoder during inference uses its own\nestimated output \u02c6 ytas the input for the next time step xt+1. Thus the decoder will\ntend to deviate more and more from the gold target sentence as it keeps generating\nmore tokens. In training, therefore, it is more common to use teacher forcing in the teacher forcing\ndecoder. Teacher forcing means that we force the system to use the gold target token",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 282,
      "token_count": 631,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 187\n\n8.8 \u2022 A TTENTION 179\nEncoderDecoder\nembeddinglayerhiddenlayer(s)softmaxthegreenlleg\u00f3\nwitcharrived<s>lleg\u00f3la\nlabruja\nbrujaverde\nverde</s>goldanswersL1 =-log P(y1)\nx1x2x3x4L2 =-log P(y2)L3 =-log P(y3)L4 =-log P(y4)L5 =-log P(y5)per-wordlossy1y2y3y4y5Total loss is the average cross-entropy loss per target word:\nFigure 8.19 Training the basic RNN encoder-decoder approach to machine translation. Note that in the\ndecoder we usually don\u2019t propagate the model\u2019s softmax outputs \u02c6 yt, but use teacher forcing to force each input\nto the correct gold value for training. We compute the softmax output distribution over \u02c6 yin the decoder in order\nto compute the loss at each token, which can then be averaged to compute a loss for the sentence. This loss is\nthen propagated through the decoder parameters and the encoder parameters.\nfrom training as the next input xt+1, rather than allowing it to rely on the (possibly\nerroneous) decoder output \u02c6 yt. This speeds up training.\n8.8 Attention\nThe simplicity of the encoder-decoder model is its clean separation of the encoder\u2014\nwhich builds a representation of the source text\u2014from the decoder, which uses this\ncontext to generate a target text. In the model as we\u2019ve described it so far, this\ncontext vector is hn, the hidden state of the last ( nth) time step of the source text.\nThis \ufb01nal hidden state is thus acting as a bottleneck : it must represent absolutely\neverything about the meaning of the source text, since the only thing the decoder\nknows about the source text is what\u2019s in this context vector (Fig. 8.20). Information\nat the beginning of the sentence, especially for long sentences, may not be equally\nwell represented in the context vector.\nEncoderDecoderbottleneckbottleneck\nFigure 8.20 Requiring the context cto be only the encoder\u2019s \ufb01nal hidden state forces all the\ninformation from the entire source sentence to pass through this representational bottleneck.\nThe attention mechanism is a solution to the bottleneck problem, a way ofattention\nmechanism\nallowing the decoder to get information from allthe hidden states of the encoder,\nnot just the last hidden state.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 283,
      "token_count": 531,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 188\n\n180 CHAPTER 8 \u2022 RNN S AND LSTM S\nIn the attention mechanism, as in the vanilla encoder-decoder model, the context\nvector cis a single vector that is a function of the hidden states of the encoder. But\ninstead of being taken from the last hidden state, it\u2019s a weighted average of allthe\nhidden states of the decoder. And this weighted average is also informed by part of\nthe decoder state as well, the state of the decoder right before the current token i.\nThat is, c=f(he\n1:::he\nn;hd\ni\u00001). The weights focus on (\u2018attend to\u2019) a particular part of\nthe source text that is relevant for the token ithat the decoder is currently producing.\nAttention thus replaces the static context vector with one that is dynamically derived\nfrom the encoder hidden states, but also informed by and hence different for each\ntoken in decoding.\nThis context vector, ci, is generated anew with each decoding step iand takes\nall of the encoder hidden states into account in its derivation. We then make this\ncontext available during decoding by conditioning the computation of the current\ndecoder hidden state on it (along with the prior hidden state and the previous output\ngenerated by the decoder), as we see in this equation (and Fig. 8.21):\nhd\ni=g(\u02c6yi\u00001;hd\ni\u00001;ci) (8.34)\nhd1hd2hdiy1y2yic1c2ci\u2026\u2026\nFigure 8.21 The attention mechanism allows each hidden state of the decoder to see a\ndifferent, dynamic, context, which is a function of all the encoder hidden states.\nThe \ufb01rst step in computing ciis to compute how much to focus on each encoder\nstate, how relevant each encoder state is to the decoder state captured in hd\ni\u00001. We\ncapture relevance by computing\u2014 at each state iduring decoding\u2014a score(hd\ni\u00001;he\nj)\nfor each encoder state j.\nThe simplest such score, called dot-product attention , implements relevance asdot-product\nattention\nsimilarity: measuring how similar the decoder hidden state is to an encoder hidden\nstate, by computing the dot product between them:\nscore(hd\ni\u00001;he\nj) = hd\ni\u00001\u0001he\nj (8.35)\nThe score that results from this dot product is a scalar that re\ufb02ects the degree of\nsimilarity between the two vectors. The vector of these scores across all the encoder\nhidden states gives us the relevance of each encoder state to the current step of the\ndecoder.\nTo make use of these scores, we\u2019ll normalize them with a softmax to create a\nvector of weights, ai j, that tells us the proportional relevance of each encoder hidden\nstate jto the prior hidden decoder state, hd\ni\u00001.\nai j=softmax (score(hd\ni\u00001;he\nj))\n=exp(score(hd\ni\u00001;he\nj)\nP\nkexp(score(hd\ni\u00001;he\nk))(8.36)\nFinally, given the distribution in a, we can compute a \ufb01xed-length context vector for\nthe current decoder state by taking a weighted average over all the encoder hidden",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 284,
      "token_count": 690,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 189",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 285,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "8.9 \u2022 S UMMARY 181\nstates.\nci=X\njai jhe\nj (8.37)\nWith this, we \ufb01nally have a \ufb01xed-length context vector that takes into account\ninformation from the entire encoder state that is dynamically updated to re\ufb02ect the\nneeds of the decoder at each step of decoding. Fig. 8.22 illustrates an encoder-\ndecoder network with attention, focusing on the computation of one context vector\nci.\nEncoderDecoder\nhdi-1he3he2he1hdihiddenlayer(s)x1x2yi-1x3xnyi-2yi-1yihen ci.2.1.3.4attentionweightsci-1ci",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 286,
      "token_count": 155,
      "chapter_title": ""
    }
  },
  {
    "content": "<latexit sha1_base64=\"TNdNmv/RIlrhPa6LgQyjjQLqyBA=\">AAACAnicdVDLSsNAFJ3UV62vqCtxM1gEVyHpI9Vd0Y3LCvYBTQyT6bSddvJgZiKUUNz4K25cKOLWr3Dn3zhpK6jogQuHc+7l3nv8mFEhTfNDyy0tr6yu5dcLG5tb2zv67l5LRAnHpIkjFvGOjwRhNCRNSSUjnZgTFPiMtP3xRea3bwkXNAqv5SQmboAGIe1TjKSSPP3AEUngjVIHsXiIvJSOpnB4Q7zR1NOLpmGaVbtqQdOwLbtk24qY5Yp9VoOWsjIUwQINT393ehFOAhJKzJAQXcuMpZsiLilmZFpwEkFihMdoQLqKhiggwk1nL0zhsVJ6sB9xVaGEM/X7RIoCISaBrzoDJIfit5eJf3ndRPZP3ZSGcSJJiOeL+gmDMoJZHrBHOcGSTRRBmFN1K8RDxBGWKrWCCuHrU/g/aZUMyzbKV5Vi/XwRRx4cgiNwAixQA3VwCRqgCTC4Aw/gCTxr99qj9qK9zltz2mJmH/yA9vYJSymYCA==</latexit>Xj\u21b5ijhej\u21b5ij<latexit sha1_base64=\"y8s4mGdpwrGrBnuSR+p1gJJXYdo=\">AAAB/nicdVDJSgNBEO2JW4zbqHjy0hgEL4YeJyQBL0EvHiOYBbIMPT09mTY9C909QhgC/ooXD4p49Tu8+Td2FkFFHxQ83quiqp6bcCYVQh9Gbml5ZXUtv17Y2Nza3jF391oyTgWhTRLzWHRcLClnEW0qpjjtJILi0OW07Y4up377jgrJ4uhGjRPaD/EwYj4jWGnJMQ+Cgedk7NSa9IgXq955MKDOrWMWUQnNAFGpYtfsakUTZNtWGUFrYRXBAg3HfO95MUlDGinCsZRdCyWqn2GhGOF0UuilkiaYjPCQdjWNcEhlP5udP4HHWvGgHwtdkYIz9ftEhkMpx6GrO0OsAvnbm4p/ed1U+bV+xqIkVTQi80V+yqGK4TQL6DFBieJjTTARTN8KSYAFJkonVtAhfH0K/yets5JVKdnX5WL9YhFHHhyCI3ACLFAFdXAFGqAJCMjAA3gCz8a98Wi8GK/z1pyxmNkHP2C8fQICDpWK</latexit>hdi\u00001\u00b7hej\u2026\u2026\nFigure 8.22 A sketch of the encoder-decoder network with attention, focusing on the computation of ci. The",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 287,
      "token_count": 795,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 8.22 A sketch of the encoder-decoder network with attention, focusing on the computation of ci. The\ncontext value ciis one of the inputs to the computation of hd\ni. It is computed by taking the weighted sum of all\nthe encoder hidden states, each weighted by their dot product with the prior decoder hidden state hd\ni\u00001.\nIt\u2019s also possible to create more sophisticated scoring functions for attention\nmodels. Instead of simple dot product attention, we can get a more powerful function\nthat computes the relevance of each encoder hidden state to the decoder hidden state\nby parameterizing the score with its own set of weights, Ws.\nscore(hd\ni\u00001;he\nj) = hd\nt\u00001Wshe\nj\nThe weights Ws, which are then trained during normal end-to-end training, give the\nnetwork the ability to learn which aspects of similarity between the decoder and\nencoder states are important to the current application. This bilinear model also\nallows the encoder and decoder to use different dimensional vectors, whereas the\nsimple dot-product attention requires that the encoder and decoder hidden states\nhave the same dimensionality.\nWe\u2019ll return to the concept of attention when we de\ufb01ne the transformer archi-\ntecture in Chapter 9, which is based on a slight modi\ufb01cation of attention called\nself-attention .\n8.9 Summary\nThis chapter has introduced the concepts of recurrent neural networks and how they\ncan be applied to language problems. Here\u2019s a summary of the main points that we",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 288,
      "token_count": 320,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 190\n\n182 CHAPTER 8 \u2022 RNN S AND LSTM S\ncovered:\n\u2022 In simple Recurrent Neural Networks sequences are processed one element at\na time, with the output of each neural unit at time tbased both on the current\ninput at tand the hidden layer from time t\u00001.\n\u2022 RNNs can be trained with a straightforward extension of the backpropagation\nalgorithm, known as backpropagation through time (BPTT).\n\u2022 Simple recurrent networks fail on long inputs because of problems like van-\nishing gradients ; instead modern systems use more complex gated architec-\ntures such as LSTMs that explicitly decide what to remember and forget in\ntheir hidden and context layers.\n\u2022 Common language-based applications for RNNs include:\n\u2013Probabilistic language modeling: assigning a probability to a sequence,\nor to the next element of a sequence given the preceding words.\n\u2013Auto-regressive generation using a trained language model.\n\u2013Sequence labeling like part-of-speech tagging, where each element of a\nsequence is assigned a label.\n\u2013Sequence classi\ufb01cation, where an entire text is assigned to a category, as\nin spam detection, sentiment analysis or topic classi\ufb01cation.\n\u2013Encoder-decoder architectures, where an input is mapped to an output\nof different length and alignment.\nBibliographical and Historical Notes\nIn\ufb02uential investigations of RNNs were conducted in the context of the Parallel Dis-\ntributed Processing (PDP) group at UC San Diego in the 1980\u2019s. Much of this work\nwas directed at human cognitive modeling rather than practical NLP applications\n(Rumelhart and McClelland 1986c, McClelland and Rumelhart 1986). Models using\nrecurrence at the hidden layer in a feedforward network (Elman networks) were in-\ntroduced by Elman (1990). Similar architectures were investigated by Jordan (1986)\nwith a recurrence from the output layer, and Mathis and Mozer (1995) with the\naddition of a recurrent context layer prior to the hidden layer. The possibility of\nunrolling a recurrent network into an equivalent feedforward network is discussed\nin (Rumelhart and McClelland, 1986c).\nIn parallel with work in cognitive modeling, RNNs were investigated extensively\nin the continuous domain in the signal processing and speech communities (Giles\net al. 1994, Robinson et al. 1996). Schuster and Paliwal (1997) introduced bidirec-\ntional RNNs and described results on the TIMIT phoneme transcription task.\nWhile theoretically interesting, the dif\ufb01culty with training RNNs and manag-\ning context over long sequences impeded progress on practical applications. This\nsituation changed with the introduction of LSTMs in Hochreiter and Schmidhuber\n(1997) and Gers et al. (2000). Impressive performance gains were demonstrated\non tasks at the boundary of signal processing and language processing including\nphoneme recognition (Graves and Schmidhuber, 2005), handwriting recognition\n(Graves et al., 2007) and most signi\ufb01cantly speech recognition (Graves et al., 2013).\nInterest in applying neural networks to practical NLP problems surged with the\nwork of Collobert and Weston (2008) and Collobert et al. (2011). These efforts made\nuse of learned word embeddings, convolutional networks, and end-to-end training.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 289,
      "token_count": 738,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 191\n\nBIBLIOGRAPHICAL AND HISTORICAL NOTES 183\nThey demonstrated near state-of-the-art performance on a number of standard shared\ntasks including part-of-speech tagging, chunking, named entity recognition and se-\nmantic role labeling without the use of hand-engineered features.\nApproaches that married LSTMs with pretrained collections of word-embeddings\nbased on word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014)\nquickly came to dominate many common tasks: part-of-speech tagging (Ling et al.,\n2015), syntactic chunking (S\u00f8gaard and Goldberg, 2016), named entity recognition\n(Chiu and Nichols, 2016; Ma and Hovy, 2016), opinion mining (Irsoy and Cardie,\n2014), semantic role labeling (Zhou and Xu, 2015a) and AMR parsing (Foland and\nMartin, 2016). As with the earlier surge of progress involving statistical machine\nlearning, these advances were made possible by the availability of training data pro-\nvided by CONLL, SemEval, and other shared tasks, as well as shared resources such\nas Ontonotes (Pradhan et al., 2007b), and PropBank (Palmer et al., 2005).\nThe modern neural encoder-decoder approach was pioneered by Kalchbrenner\nand Blunsom (2013), who used a CNN encoder and an RNN decoder. Cho et al.\n(2014) (who coined the name \u201cencoder-decoder\u201d) and Sutskever et al. (2014) then\nshowed how to use extended RNNs for both encoder and decoder. The idea that a\ngenerative decoder should take as input a soft weighting of the inputs, the central\nidea of attention, was \ufb01rst developed by Graves (2013) in the context of handwriting\nrecognition. Bahdanau et al. (2015) extended the idea, named it \u201cattention\u201d and\napplied it to MT.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 290,
      "token_count": 446,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 192\n\n184 CHAPTER 9 \u2022 T HETRANSFORMER\nCHAPTER\n9The Transformer\n\u201cThe true art of memory is the art of attention \u201d\nSamuel Johnson, Idler #74 , September 1759\nIn this chapter we introduce the transformer , the standard architecture for build-\ninglarge language models . Transformer-based large language models have com-\npletely changed the \ufb01eld of speech and language processing. Indeed, every subse-\nquent chapter in this textbook will make use of them. We\u2019ll focus for now on left-\nto-right (sometimes called causal or autoregressive) language modeling, in which\nwe are given a sequence of input tokens and predict output tokens one by one by\nconditioning on the prior context.\nThe transformer is a neural network with a speci\ufb01c structure that includes a\nmechanism called self-attention ormulti-head attention .1Attention can be thought\nof as a way to build contextual representations of a token\u2019s meaning by attending to\nand integrating information from surrounding tokens, helping the model learn how\ntokens relate to each other over large spans.\nStackedTransformerBlocksSolongandthanksforlongandthanksforNext tokenall\u2026\u2026\n\u2026\nU\nInput tokensx1x2LanguageModelingHead\nx3x4x5InputEncoding\nE1+\nE2+\nE3+\nE4+\nE5+\u2026\u2026\u2026\u2026\u2026\nU\nU\nU\nU\u2026logitslogitslogitslogitslogits\nFigure 9.1 The architecture of a (left-to-right) transformer, showing how each input token\nget encoded, passed through a set of stacked transformer blocks, and then a language model\nhead that predicts the next token.\nFig. 9.1 sketches the transformer architecture. A transformer has three major\ncomponents. At the center are columns of transformer blocks . Each block is a\nmultilayer network (a multi-head attention layer, feedforward networks and layer\nnormalization steps) that maps an input vector xiin column i(corresponding to input\n1Although multi-head attention developed historically from the RNN attention mechanism (Chapter 8),\nwe\u2019ll de\ufb01ne attention from scratch here for readers who haven\u2019t yet read Chapter 8.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 291,
      "token_count": 462,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 193\n\n9.1 \u2022 A TTENTION 185\ntoken i) to an output vector hi. The set of nblocks maps an entire context window\nof input vectors (x1;:::;xn)to a window of output vectors (h1;:::;hn)of the same\nlength. A column might contain from 12 to 96 or more stacked blocks.\nThe column of blocks is preceded by the input encoding component, which pro-\ncesses an input token (like the word thanks ) into a contextual vector representation,\nusing an embedding matrix Eand a mechanism for encoding token position. Each\ncolumn is followed by a language modeling head , which takes the embedding out-\nput by the \ufb01nal transformer block, passes it through an unembedding matrix Uand\na softmax over the vocabulary to generate a single token for that column.\nTransformer-based language models are complex, and so the details will unfold\nover the next 5 chapters. In the next sections we\u2019ll introduce multi-head attention,\nthe rest of the transformer block, and the input encoding and language modeling\nhead components. Chapter 10 discusses how language models are pretrained , and\nhow tokens are generated via sampling . Chapter 11 introduces masked language\nmodeling and the BERT family of bidirectional transformer encoder models. Chap-\nter 12 shows how to prompt LLMs to perform NLP tasks by giving instructions and\ndemonstrations, and how to align the model with human preferences. Chapter 13\nwill introduce machine translation with the encoder-decoder architecture.\n9.1 Attention\nRecall from Chapter 6 that for word2vec and other static embeddings, the repre-\nsentation of a word\u2019s meaning is always the same vector irrespective of the context:\nthe wordchicken , for example, is always represented by the same \ufb01xed vector. So\na static vector for the word itmight somehow encode that this is a pronoun used\nfor animals and inanimate entities. But in context ithas a much richer meaning.\nConsideritin one of these two sentences:\n(9.1) The chicken didn\u2019t cross the road because itwas too tired.\n(9.2) The chicken didn\u2019t cross the road because itwas too wide.\nIn (9.1)itis the chicken (i.e., the reader knows that the chicken was tired), while\nin (9.2)itis the road (and the reader knows that the road was wide).2That is, if\nwe are to compute the meaning of this sentence, we\u2019ll need the meaning of itto be\nassociated with the chicken in the \ufb01rst sentence and associated with the road in\nthe second one, sensitive to the context.\nFurthermore, consider reading left to right like a causal language model, pro-\ncessing the sentence up to the word it:\n(9.3) The chicken didn\u2019t cross the road because it\nAt this point we don\u2019t yet know which thing itis going to end up referring to! So a\nrepresentation of itat this point might have aspects of both chicken androad as\nthe reader is trying to guess what happens next.\nThis fact that words have rich linguistic relationships with other words that may\nbe far away pervades language. Consider two more examples:\n(9.4) The keys to the cabinet areon the table.\n(9.5) I walked along the pond , and noticed one of the trees along the bank .\n2We say that in the \ufb01rst example itcorefers with the chicken, and in the second itcorefers with the\nroad; we\u2019ll return to this in Chapter 23.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 292,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 194\n\n186 CHAPTER 9 \u2022 T HETRANSFORMER\nIn (9.4), the phrase The keys is the subject of the sentence, and in English and many\nlanguages, must agree in grammatical number with the verb are; in this case both are\nplural. In English we can\u2019t use a singular verb like iswith a plural subject like keys\n(we\u2019ll discuss agreement more in Chapter 18). In (9.5), we know that bank refers\nto the side of a pond or river and not a \ufb01nancial institution because of the context,\nincluding words like pond . (We\u2019ll discuss word senses more in Chapter 11.)\nThe point of all these examples is that these contextual words that help us com-\npute the meaning of words in context can be quite far away in the sentence or para-\ngraph. Transformers can build contextual representations of word meaning, contex-\ntual embeddings , by integrating the meaning of these helpful contextual words. In acontextual\nembeddings\ntransformer, layer by layer, we build up richer and richer contextualized representa-\ntions of the meanings of input tokens. At each layer, we compute the representation\nof a token iby combining information about ifrom the previous layer with infor-\nmation about the neighboring tokens to produce a contextualized representation for\neach word at each position.\nAttention is the mechanism in the transformer that weighs and combines the\nrepresentations from appropriate other tokens in the context from layer k\u00001 to build\nthe representation for tokens in layer k.\nThechickendidn\u2019tcrosstheroadbecauseitwastootiredThechickendidn\u2019tcrosstheroadbecauseitwastootiredLayer k+1Layer kself-attention distributioncolumns corresponding to input tokens\nFigure 9.2 The self-attention weight distribution athat is part of the computation of the\nrepresentation for the word itat layer k+1. In computing the representation for it, we attend\ndifferently to the various words at layer k, with darker shades indicating higher self-attention\nvalues. Note that the transformer is attending highly to the columns corresponding to the\ntokens chicken androad , a sensible result, since at the point where itoccurs, it could plausibly\ncorefer with the chicken or the road, and hence we\u2019d like the representation for itto draw on\nthe representation for these earlier words. Figure adapted from Uszkoreit (2017).\nFig. 9.2 shows a schematic example simpli\ufb01ed from a transformer (Uszkoreit,\n2017). The \ufb01gure describes the situation when the current token is itand we need\nto compute a contextual representation for this token at layer k+1 of the transformer,\ndrawing on the representations (from layer k) of every prior token. The \ufb01gure uses\ncolor to represent the attention distribution over the contextual words: the tokens\nchicken androad both have a high attention weight, meaning that as we are com-\nputing the representation for it, we will draw most heavily on the representation for\nchicken androad . This will be useful in building the \ufb01nal representation for it,\nsinceitwill end up coreferring with either chicken orroad .\nLet\u2019s now turn to how this attention distribution is represented and computed.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 293,
      "token_count": 692,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 195\n\n9.1 \u2022 A TTENTION 187\n9.1.1 Attention more formally\nAs we\u2019ve said, the attention computation is a way to compute a vector representation\nfor a token at a particular layer of a transformer, by selectively attending to and\nintegrating information from prior tokens at the previous layer. Attention takes an\ninput representation xicorresponding to the input token at position i, and a context\nwindow of prior inputs x1::xi\u00001, and produces an output ai.\nIn causal, left-to-right language models, the context is any of the prior words.\nThat is, when processing xi, the model has access to xias well as the representations\nof all the prior tokens in the context window (context windows consist of thousands\nof tokens) but no tokens after i. (By contrast, in Chapter 11 we\u2019ll generalize attention\nso it can also look ahead to future words.)\nFig. 9.3 illustrates this \ufb02ow of information in an entire causal self-attention layer,\nin which this same attention computation happens in parallel at each token position\ni. Thus a self-attention layer maps input sequences (x1;:::;xn)to output sequences\nof the same length (a1;:::;an).\nattentionattentionSelf-AttentionLayerattentionattentionattentiona1a2a3a4a5x3x4x5x1x2\nFigure 9.3 Information \ufb02ow in causal self-attention. When processing each input xi, the\nmodel attends to all the inputs up to, and including xi.\nSimpli\ufb01ed version of attention At its heart, attention is really just a weighted\nsum of context vectors, with a lot of complications added to how the weights are\ncomputed and what gets summed. For pedagogical purposes let\u2019s \ufb01rst describe a\nsimpli\ufb01ed intuition of attention, in which the attention output aiat token position i\nis simply the weighted sum of all the representations xj, for all j\u0014i; we\u2019ll use ai j\nto mean how much xjshould contribute to ai:\nSimpli\ufb01ed version: ai=X\nj\u0014iai jxj (9.6)\nEach ai jis a scalar used for weighing the value of input xjwhen summing up\nthe inputs to compute ai. How shall we compute this aweighting? In attention we\nweight each prior embedding proportionally to how similar it is to the current token\ni. So the output of attention is a sum of the embeddings of prior tokens weighted\nby their similarity with the current token embedding. We compute similarity scores\nviadot product , which maps two vectors into a scalar value ranging from \u0000\u00a5to\n\u00a5. The larger the score, the more similar the vectors that are being compared. We\u2019ll\nnormalize these scores with a softmax to create the vector of weights ai j;j\u0014i.\nSimpli\ufb01ed Version: score(xi;xj) = xi\u0001xj (9.7)\nai j=softmax (score(xi;xj))8j\u0014i(9.8)\nThus in Fig. 9.3 we compute a3by computing three scores: x3\u0001x1,x3\u0001x2andx3\u0001x3,\nnormalizing them by a softmax, and using the resulting probabilities as weights\nindicating each of their proportional relevance to the current position i. Of course,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 294,
      "token_count": 722,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 196\n\n188 CHAPTER 9 \u2022 T HETRANSFORMER\nthe softmax weight will likely be highest for xi, since xiis very similar to itself,\nresulting in a high dot product. But other context words may also be similar to i, and\nthe softmax will also assign some weight to those words. Then we use these weights\nas the avalues in Eq. 9.6 to compute the weighted sum that is our a3.\nThe simpli\ufb01ed attention in equations 9.6 \u2013 9.8 demonstrates the attention-based\napproach to computing ai: compare the xito prior vectors, normalize those scores\ninto a probability distribution used to weight the sum of the prior vector. But now\nwe\u2019re ready to remove the simpli\ufb01cations.\nA single attention head using query, key, and value matrices Now that we\u2019ve\nseen a simple intuition of attention, let\u2019s introduce the actual attention head , the attention head\nversion of attention that\u2019s used in transformers. (The word head is often used in head\ntransformers to refer to speci\ufb01c structured layers). The attention head allows us to\ndistinctly represent three different roles that each input embedding plays during the\ncourse of the attention process:\n\u2022 As the current element being compared to the preceding inputs. We\u2019ll refer to\nthis role as a query . query\n\u2022 In its role as a preceding input that is being compared to the current element\nto determine a similarity weight. We\u2019ll refer to this role as a key. key\n\u2022 And \ufb01nally, as a value of a preceding element that gets weighted and summed value\nup to compute the output for the current element.\nTo capture these three different roles, transformers introduce weight matrices\nWQ,WK, and WV. These weights will project each input vector xiinto a represen-\ntation of its role as a key, query, or value:\nqi=xiWQ;ki=xiWK;vi=xiWV(9.9)\nGiven these projections, when we are computing the similarity of the current ele-\nment xiwith some prior element xj, we\u2019ll use the dot product between the current\nelement\u2019s query vector qiand the preceding element\u2019s keyvector kj. Furthermore,\nthe result of a dot product can be an arbitrarily large (positive or negative) value, and\nexponentiating large values can lead to numerical issues and loss of gradients during\ntraining. To avoid this, we scale the dot product by a factor related to the size of the\nembeddings, via dividing by the square root of the dimensionality of the query and\nkey vectors ( dk). We thus replace the simpli\ufb01ed Eq. 9.7 with Eq. 9.11. The ensuing\nsoftmax calculation resulting in ai jremains the same, but the output calculation for\nhead iis now based on a weighted sum over the value vectors v(Eq. 9.13).\nHere\u2019s a \ufb01nal set of equations for computing self-attention for a single self-\nattention output vector aifrom a single input vector xi. This version of attention\ncomputes aiby summing the values of the prior elements, each weighted by the\nsimilarity of its keyto the query from the current element:\nqi=xiWQ;kj=xjWK;vj=xjWV(9.10)\nscore(xi;xj) =qi\u0001kjpdk(9.11)\nai j=softmax (score(xi;xj))8j\u0014i (9.12)\nhead i=X\nj\u0014iai jvj (9.13)\nai=head iWO(9.14)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 295,
      "token_count": 764,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 197\n\n9.1 \u2022 A TTENTION 189\n6. Sum the weighted value vectors4. Turn into \ud835\udefci,j weights via softmaxa3\n1. Generate key, query, value vectors2. Compare x3\u2019s query withthe keys for x1, x2, and x38. Output of self-attention\u00d7\u00d7\nx1kqvWKWQWV5. Weigh each value vector\u00f7\u221adk3. Divide scalar score by \u221adk\u00f7\u221adk\u00f7\u221adk\ud835\udefc3,1\ud835\udefc3,2\ud835\udefc3,3\nx2kqvWKWQWVx3kqvWKWQWVWO\n[1 \u00d7 d][1 \u00d7 d][dv \u00d7 d][1 \u00d7 dv][1 \u00d7 dv][1 \u00d7 dv][1 \u00d7 dv]\n[1 \u00d7 dv][1 \u00d7 dv][1 x dv]7. Reshape to [1 x d] \n[1 \u00d7 d][1 \u00d7 d]\nFigure 9.4 Calculating the value of a3, the third element of a sequence using causal (left-\nto-right) self-attention.\nWe illustrate this in Fig. 9.4 for the case of calculating the value of the third output\na3in a sequence.\nNote that we\u2019ve also introduced one more matrix, WO, which is right-multiplied\nby the attention head. This is necessary to reshape the output of the head. The input\nto attention xiand the output from attention aiboth have the same dimensionality\n[1\u0002d]. We often call dthemodel dimensionality , and indeed as we\u2019ll discuss in\nSection 9.2 the output hiof each transformer block, as well as the intermediate vec-\ntors inside the transformer block also have the same dimensionality [1\u0002d]. Having\neverything be the same dimensionality makes the transformer very modular.\nSo let\u2019s talk shapes. How do we get from [1\u0002d]at the input to [1\u0002d]at the\noutput? Let\u2019s look at all the internal shapes. We\u2019ll have a dimension dkfor the key\nand query vectors. The query vector and the key vector are both dimensionality\n1\u0002dk, so we can take their dot product qi\u0001kjto produce a scalar. We\u2019ll have a\nseparate dimension dvfor the value vectors. The transform matrix WQhas shape\n[d\u0002dk],WKis[d\u0002dk], and WVis[d\u0002dv]. So the output of head iin equation\nEq. 9.13 is of shape [1\u0002dv]. To get the desired output shape [1\u0002d]we\u2019ll need to\nreshape the head output, and so WOis of shape [dv\u0002d]. In the original transformer\nwork (Vaswani et al., 2017), dwas 512, dkanddvwere both 64.\nMulti-head Attention Equations 9.11-9.13 describe a single attention head . But\nactually, transformers use multiple attention heads. The intuition is that each head\nmight be attending to the context for different purposes: heads might be special-\nized to represent different linguistic relationships between context elements and the\ncurrent token, or to look for particular kinds of patterns in the context.\nSo in multi-head attention we have Aseparate attention heads that reside inmulti-head\nattention\nparallel layers at the same depth in a model, each with its own set of parameters that\nallows the head to model different aspects of the relationships among inputs. Thus",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 296,
      "token_count": 745,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 198\n\n190 CHAPTER 9 \u2022 T HETRANSFORMER\neach head iin a self-attention layer has its own set of key, query and value matrices:\nWKi,WQiandWVi. These are used to project the inputs into separate key, value,\nand query embeddings for each head.\nWhen using multiple heads the model dimension dis still used for the input\nand output, the key and query embeddings have dimensionality dk, and the value\nembeddings are of dimensionality dv(again, in the original transformer paper dk=\ndv=64,A=8, and d=512). Thus for each head i, we have weight layers WQiof\nshape [d\u0002dk],WKiof shape [d\u0002dk], and WViof shape [d\u0002dv].\nBelow are the equations for attention augmented with multiple heads; Fig. 9.5\nshows an intuition.\nqc\ni=xiWQc;kc\nj=xjWKc;vc\nj=xjWVc;8c1\u0014c\u0014A (9.15)\nscorec(xi;xj) =qc\ni\u0001kc\njpdk(9.16)\nac\ni j=softmax (scorec(xi;xj))8j\u0014i (9.17)\nheadc\ni=X\nj\u0014iac\ni jvc\nj (9.18)\nai= (head1\bhead2:::\bheadA)WO(9.19)\nMultiHeadAttention (xi;[x1;\u0001\u0001\u0001;xN]) = ai (9.20)\nThe output of each of the Aheads is of shape 1 \u0002dv, and so the output of the\nmulti-head layer with Aheads consists of Avectors of shape 1 \u0002dv. These are\nconcatenated to produce a single output with dimensionality 1 \u0002hdv. Then we use\nyet another linear projection WO2RAdv\u0002dto reshape it, resulting in the multi-head\nattention vector aiwith the correct output shape [1\u0002d]at each input i.\naixi-1xixi-2xi-3WK1Head 1WV1WQ1\u2026\u2026WK2Head 2WV2WQ2WK8Head 8WV8WQ8aiWO  [hdv x d][1 x dv ][1 x d]\n[1 x d][1 x hdv ]Project down to dConcatenate OutputsEach headattends di\ufb00erentlyto context\u2026[1 x dv ]\nFigure 9.5 The multi-head attention computation for input xi, producing output ai. A multi-head attention\nlayer has Aheads, each with its own key, query and value weight matrices. The outputs from each of the heads\nare concatenated and then projected down to d, thus producing an output of the same size as the input.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 297,
      "token_count": 606,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 199\n\n9.2 \u2022 T RANSFORMER BLOCKS 191\n9.2 Transformer Blocks\nThe self-attention calculation lies at the core of what\u2019s called a transformer block,\nwhich, in addition to the self-attention layer, includes three other kinds of layers: (1)\na feedforward layer, (2) residual connections, and (3) normalizing layers (colloqui-\nally called \u201clayer norm\u201d).\nLayer Norm\nxi+hi-1\nLayer NormMultiHeadAttentionFeedforward\nxi-1xi+1hihi+1\n+\u2026\u2026ResidualStream\nFigure 9.6 The architecture of a transformer block showing the residual stream . This\n\ufb01gure shows the prenorm version of the architecture, in which the layer norms happen before\nthe attention and feedforward layers rather than after.\nFig. 9.6 illustrates a transformer block, sketching a common way of thinking\nabout the block that is called the residual stream (Elhage et al., 2021). In the resid- residual stream\nual stream viewpoint, we consider the processing of an individual token ithrough\nthe transformer block as a single stream of d-dimensional representations for token\nposition i. This residual stream starts with the original input vector, and the various\ncomponents read their input from the residual stream and add their output back into\nthe stream.\nThe input at the bottom of the stream is an embedding for a token, which has\ndimensionality d. This initial embedding gets passed up (by residual connections ),\nand is progressively added to by the other components of the transformer: the at-\ntention layer that we have seen, and the feedforward layer that we will introduce.\nBefore the attention and feedforward layer is a computation called the layer norm .\nThus the initial vector is passed through a layer norm and attention layer, and\nthe result is added back into the stream, in this case to the original input vector\nxi. And then this summed vector is again passed through another layer norm and a\nfeedforward layer, and the output of those is added back into the residual, and we\u2019ll\nusehito refer to the resulting output of the transformer block for token i. (In earlier\ndescriptions the residual stream was often described using a different metaphor as\nresidual connections that add the input of a component to its output, but the residual\nstream is a more perspicuous way of visualizing the transformer.)\nWe\u2019ve already seen the attention layer, so let\u2019s now introduce the feedforward",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 298,
      "token_count": 521,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 200\n\n192 CHAPTER 9 \u2022 T HETRANSFORMER\nand layer norm computations in the context of processing a single input xiat token\nposition i.\nFeedforward layer The feedforward layer is a fully-connected 2-layer network,\ni.e., one hidden layer, two weight matrices, as introduced in Chapter 7. The weights\nare the same for each token position i, but are different from layer to layer. It\nis common to make the dimensionality dffof the hidden layer of the feedforward\nnetwork be larger than the model dimensionality d. (For example in the original\ntransformer model, d=512 and dff=2048.)\nFFN(xi) =ReLU (xiW1+b1)W2+b2 (9.21)\nLayer Norm At two stages in the transformer block we normalize the vector (Ba\net al., 2016). This process, called layer norm (short for layer normalization), is one layer norm\nof many forms of normalization that can be used to improve training performance\nin deep neural networks by keeping the values of a hidden layer in a range that\nfacilitates gradient-based training.\nLayer norm is a variation of the z-score from statistics, applied to a single vec-\ntor in a hidden layer. That is, the term layer norm is a bit confusing; layer norm\nisnotapplied to an entire transformer layer, but just to the embedding vector of a\nsingle token. Thus the input to layer norm is a single vector of dimensionality d\nand the output is that vector normalized, again of dimensionality d. The \ufb01rst step in\nlayer normalization is to calculate the mean, m, and standard deviation, s, over the\nelements of the vector to be normalized. Given an embedding vector xof dimen-\nsionality d, these values are calculated as follows.\nm=1\nddX\ni=1xi (9.22)\ns=vuut1\nddX\ni=1(xi\u0000m)2(9.23)\nGiven these values, the vector components are normalized by subtracting the mean\nfrom each and dividing by the standard deviation. The result of this computation is\na new vector with zero mean and a standard deviation of one.\n^ x=(x\u0000m)\ns(9.24)\nFinally, in the standard implementation of layer normalization, two learnable param-\neters, gandb, representing gain and offset values, are introduced.\nLayerNorm (x) =g(x\u0000m)\ns+b (9.25)\nPutting it all together The function computed by a transformer block can be ex-\npressed by breaking it down with one equation for each component computation,\nusing t(of shape [1\u0002d]) to stand for transformer and superscripts to demarcate",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 299,
      "token_count": 578,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 201\n\n9.2 \u2022 T RANSFORMER BLOCKS 193\neach computation inside the block:\nt1\ni=LayerNorm (xi) (9.26)\nt2\ni=MultiHeadAttention (t1\ni;\u0002\nt1\n1;\u0001\u0001\u0001;t1\nN\u0003\n) (9.27)\nt3\ni=t2\ni+xi (9.28)\nt4\ni=LayerNorm (t3\ni) (9.29)\nt5\ni=FFN(t4\ni) (9.30)\nhi=t5\ni+t3\ni (9.31)\nNotice that the only component that takes as input information from other tokens\n(other residual streams) is multi-head attention, which (as we see from Eq. 9.28)\nlooks at all the neighboring tokens in the context. The output from attention, how-\never, is then added into this token\u2019s embedding stream. In fact, Elhage et al. (2021)\nshow that we can view attention heads as literally moving information from the\nresidual stream of a neighboring token into the current stream. The high-dimensional\nembedding space at each position thus contains information about the current to-\nken and about neighboring tokens, albeit in different subspaces of the vector space.\nFig. 9.7 shows a visualization of this movement.\nToken Aresidual streamToken Bresidual stream\nFigure 9.7 An attention head can move information from token A\u2019s residual stream into\ntoken B\u2019s residual stream.\nCrucially, the input and output dimensions of transformer blocks are matched so\nthey can be stacked. Each token vector xiat the input to the block has dimensionality\nd, and the output hialso has dimensionality d. Transformers for large language\nmodels stack many of these blocks, from 12 layers (used for the T5 or GPT-3-small\nlanguage models) to 96 layers (used for GPT-3 large), to even more for more recent\nmodels. We\u2019ll come back to this issue of stacking in a bit.\nEquation 9.28 and following are just the equation for a single transformer block,\nbut the residual stream metaphor goes through all the transformer layers, from the\n\ufb01rst transformer blocks to the 12th, in a 12-layer transformer. At the earlier trans-\nformer blocks, the residual stream is representing the current token. At the highest\ntransformer blocks, the residual stream is usually representing the following token,\nsince at the very end it\u2019s being trained to predict the next token.\nOnce we stack many blocks, there is one more requirement: at the very end of\nthe last (highest) transformer block, there is a single extra layer norm that is run on\nthe last hiof each token stream (just below the language model head layer that we\nwill de\ufb01ne soon).3\n3Note that we are using the most common current transformer architecture, which is called the prenorm",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 300,
      "token_count": 628,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 202\n\n194 CHAPTER 9 \u2022 T HETRANSFORMER\n9.3 Parallelizing computation using a single matrix X\nThis description of multi-head attention and the rest of the transformer block has\nbeen from the perspective of computing a single output at a single time step iin\na single residual stream. But as we pointed out earlier, the attention computation\nperformed for each token to compute aiis independent of the computation for each\nother token, and that\u2019s also true for all the computation in the transformer block\ncomputing hifrom the input xi. That means we can easily parallelize the entire\ncomputation, taking advantage of ef\ufb01cient matrix multiplication routines.\nWe do this by packing the input embeddings for the Ntokens of the input se-\nquence into a single matrix Xof size [N\u0002d]. Each row of Xis the embedding of\none token of the input. Transformers for large language models commonly have an\ninput length Nfrom 1K to 32K; much longer contexts of 128K or even up to millions\nof tokens can also be achieved with architectural changes like special long-context\nmechanisms that we don\u2019t discuss here. So for vanilla transformers, we can think of\nXhaving between 1K and 32K rows, each of the dimensionality of the embedding\nd(the model dimension).\nParallelizing attention Let\u2019s \ufb01rst see this for a single attention head and then turn\nto multiple heads, and then add in the rest of the components in the transformer\nblock. For one head we multiply Xby the key, query, and value matrices WQof\nshape [d\u0002dk],WKof shape [d\u0002dk], andWVof shape [d\u0002dv], to produce matrices\nQof shape [N\u0002dk],Kof shape [N\u0002dk], and Vof shape [N\u0002dv], containing all the\nkey, query, and value vectors:\nQ=XWQ;K=XWK;V=XWV(9.32)\nGiven these matrices we can compute all the requisite query-key comparisons simul-\ntaneously by multiplying QandK|in a single matrix multiplication. The product is\nof shape N\u0002N, visualized in Fig. 9.8.\nq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NNq1\u2022k2q1\u2022k3q1\u2022k4q2\u2022k3q2\u2022k4q3\u2022k4\nFigure 9.8 The N\u0002NQK|matrix showing how it computes all qi\u0001kjcomparisons in a\nsingle matrix multiple.\nOnce we have this QK|matrix, we can very ef\ufb01ciently scale these scores, take\nthe softmax, and then multiply the result by Vresulting in a matrix of shape N\u0002d:\na vector embedding representation for each token in the input. We\u2019ve reduced the\nentire self-attention step for an entire sequence of Ntokens for one head to the\narchitecture. The original de\ufb01nition of the transformer in Vaswani et al. (2017) used an alternative archi-\ntecture called the postnorm transformer in which the layer norm happens after the attention and FFN\nlayers; it turns out moving the layer norm beforehand works better, but does require this one extra layer\nat the end.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 301,
      "token_count": 737,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 203\n\n9.3 \u2022 P ARALLELIZING COMPUTATION USING A SINGLE MATRIX X 195\nfollowing computation:\nhead =softmax\u0012\nmask\u0012QK|\npdk\u0013\u0013\nV\nA=head WO(9.33)\nMasking out the future You may have noticed that we introduced a mask function\nin Eq. 9.33 above. This is because the self-attention computation as we\u2019ve described\nit has a problem: the calculation of QK|results in a score for each query value to\nevery key value, including those that follow the query . This is inappropriate in the\nsetting of language modeling: guessing the next word is pretty simple if you already\nknow it! To \ufb01x this, the elements in the upper-triangular portion of the matrix are set\nto\u0000\u00a5, which the softmax will turn to zero, thus eliminating any knowledge of words\nthat follow in the sequence. This is done in practice by adding a mask matrix Min\nwhich Mi j=\u0000\u00a58j>i(i.e. for the upper-triangular portion) and Mi j=0 otherwise.\nFig. 9.9 shows the resulting masked QK|matrix. (we\u2019ll see in Chapter 11 how to\nmake use of words in the future for tasks that need it).\nq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NN\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\nFigure 9.9 TheN\u0002NQK|matrix showing the qi\u0001kjvalues, with the upper-triangle por-\ntion of the comparisons matrix zeroed out (set to \u0000\u00a5, which the softmax will turn to zero).\nFig. 9.10 shows a schematic of all the computations for a single attention head\nparallelized in matrix form.\nFig. 9.8 and Fig. 9.9 also make it clear that attention is quadratic in the length\nof the input, since at each layer we need to compute dot products between each pair\nof tokens in the input. This makes it expensive to compute attention over very long\ndocuments (like entire novels). Nonetheless modern large language models manage\nto use quite long contexts of thousands or tens of thousands of tokens.\nParallelizing multi-head attention In multi-head attention, as with self-attention,\nthe input and output have the model dimension d, the key and query embeddings\nhave dimensionality dk, and the value embeddings are of dimensionality dv(again,\nin the original transformer paper dk=dv=64,A=8, and d=512). Thus for\neach head i, we have weight layers WQiof shape [d\u0002dk],WKiof shape [d\u0002dk],\nandWViof shape [d\u0002dv], and these get multiplied by the inputs packed into X\nto produce Qof shape [N\u0002dk],Kof shape [N\u0002dk], and Vof shape [N\u0002dv].\nThe output of each of the Aheads is of shape N\u0002dv, and so the output of the\nmulti-head layer with Aheads consists of Amatrices of shape N\u0002dv. To make use\nof these matrices in further processing, they are concatenated to produce a single\noutput with dimensionality N\u0002hdv. Finally, we use a \ufb01nal linear projection WO\nof shape [Adv\u0002d], that reshape it to the original output dimension for each token.\nMultiplying the concatenated N\u0002hdvmatrix output by WOof shape [Adv\u0002d]yields",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 302,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 204\n\n196 CHAPTER 9 \u2022 T HETRANSFORMER\nq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2\u2022k2q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k2q3\u2022k3\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221eq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3q1\u2022k2q2\u2022k3q1\u2022k3q3\u2022k4q2\u2022k4q1\u2022k4x=QKT maskedmask=q1\u2022k1q2\u2022k1q4\u2022k1q3\u2022k1q1\u2022k1q1\u2022k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dv\nFigure 9.10 Schematic of the attention computation for a single attention head in parallel. The \ufb01rst row shows\nthe computation of the Q,K, and Vmatrices. The second row shows the computation of QKT, the masking\n(the softmax computation and the normalizing by dimensionality are not shown) and then the weighted sum of\nthe value vectors to get the \ufb01nal attention vectors.\nthe self-attention output Aof shape [ N\u0002d].\nQi=XWQi;Ki=XWKi;Vi=XWVi(9.34)\nhead i=SelfAttention (Qi;Ki;Vi) = softmax\u0012QiKi|\npdk\u0013\nVi(9.35)\nMultiHeadAttention (X) = ( head 1\bhead 2:::\bhead A)WO(9.36)\nPutting it all together with the parallel input matrix XThe function computed\nin parallel by an entire layer of Ntransformer block over the entire Ninput tokens\ncan be expressed as:\nO=X+MultiHeadAttention (LayerNorm (X)) (9.37)\nH=O+FFN(LayerNorm (O)) (9.38)\nNote that in Eq. 9.37 we are using Xto mean the input to the layer, wherever it\ncomes from. For the \ufb01rst layer, as we will see in the next section, that input is the\ninitital word + positional embedding vectors that we have been describing by X. But\nfor subsequent layers k, the input is the output from the previous layer Hk\u00001. We\ncan also break down the computation performed in a transformer layer, showing one\nequation for each component computation. We\u2019ll use T(of shape [N\u0002d]) to stand\nfor transformer and superscripts to demarcate each computation inside the block,\nand again use Xto mean the input to the block from the previous layer or the initial",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 303,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 205\n\n9.4 \u2022 T HE INPUT :EMBEDDINGS FOR TOKEN AND POSITION 197\nembedding:\nT1=LayerNorm (X) (9.39)\nT2=MultiHeadAttention (T1) (9.40)\nT3=T2+X (9.41)\nT4=LayerNorm (T3) (9.42)\nT5=FFN(T4) (9.43)\nH=T5+T3(9.44)\nHere when we use a notation like FFN (T3)we mean that the same FFN is applied\nin parallel to each of the Nembedding vectors in the window. Similarly, each of the\nNtokens is normed in parallel in the LayerNorm. Crucially, the input and output\ndimensions of transformer blocks are matched so they can be stacked. Since each\ntoken xiat the input to the block is represented by an embedding of dimensionality\n[1\u0002d], that means the input Xand output Hare both of shape [N\u0002d].\n9.4 The input: embeddings for token and position\nLet\u2019s talk about where the input Xcomes from. Given a sequence of Ntokens ( Nis\nthe context length in tokens), the matrix Xof shape [N\u0002d]has an embedding for embedding\neach word in the context. The transformer does this by separately computing two\nembeddings: an input token embedding, and an input positional embedding.\nA token embedding, introduced in Chapter 7 and Chapter 8, is a vector of di-\nmension dthat will be our initial representation for the input token. (As we pass\nvectors up through the transformer layers in the residual stream, this embedding\nrepresentation will change and grow, incorporating context and playing a different\nrole depending on the kind of language model we are building.) The set of initial\nembeddings are stored in the embedding matrix E, which has a row for each of the\njVjtokens in the vocabulary. Thus each word is a row vector of ddimensions, and\nEhas shape [jVj\u0002d].\nGiven an input token string like Thanks for all the we \ufb01rst convert the tokens\ninto vocabulary indices (these were created when we \ufb01rst tokenized the input using\nBPE or SentencePiece). So the representation of thanks for all the might be w=\n[5;4000;10532;2224]. Next we use indexing to select the corresponding rows from\nE, (row 5, row 4000, row 10532, row 2224).\nAnother way to think about selecting token embeddings from the embedding\nmatrix is to represent tokens as one-hot vectors of shape [1\u0002jVj], i.e., with one\ndimension for each word in the vocabulary. Recall that in a one-hot vector all the one-hot vector\nelements are 0 except one, the element whose dimension is the word\u2019s index in the\nvocabulary, which has value 1. So if the word \u201cthanks\u201d has index 5 in the vocabulary,\nx5=1, and xi=08i6=5, as shown here:\n[0 0 0 0 1 0 0 ... 0 0 0 0]\n1 2 3 4 5 6 7 ... ... |V|\nMultiplying by a one-hot vector that has only one non-zero element xi=1 simply\nselects out the relevant row vector for word i, resulting in the embedding for word i,\nas depicted in Fig. 9.11.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 304,
      "token_count": 744,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 206\n\n198 CHAPTER 9 \u2022 T HETRANSFORMER\nE|V|d1|V|d=\u2715550 0 0 0 1 0 0 \u2026 0 0 0 0 1\nFigure 9.11 Selecting the embedding vector for word V5by multiplying the embedding\nmatrix Ewith a one-hot vector with a 1 in index 5.\nWe can extend this idea to represent the entire token sequence as a matrix of one-\nhot vectors, one for each of the Npositions in the transformer\u2019s context window, as\nshown in Fig. 9.12.\nE|V|ddN=\u2715|V|N0 0 0 0 0 0 0 \u2026 0 0 1 0 0 0 0 0 1 0 0 \u2026 0 0 0 0 1 0 0 0 0 0 0 \u2026 0 0 0 0 0 0 0 0 1 0 0 \u2026 0 0 0 0 \u2026\nFigure 9.12 Selecting the embedding matrix for the input sequence of token ids Wby mul-\ntiplying a one-hot matrix corresponding to Wby the embedding matrix E.\nThese token embeddings are not position-dependent. To represent the position\nof each token in the sequence, we combine these token embeddings with positional\nembeddings speci\ufb01c to each position in an input sequence.positional\nembeddings\nWhere do we get these positional embeddings? The simplest method, called\nabsolute position , is to start with randomly initialized embeddings correspondingabsolute\nposition\nto each possible input position up to some maximum length. For example, just as\nwe have an embedding for the word \ufb01sh, we\u2019ll have an embedding for the position 3.\nAs with word embeddings, these positional embeddings are learned along with other\nparameters during training. We can store them in a matrix Eposof shape [N\u0002d].\nTo produce an input embedding that captures positional information, we just\nadd the word embedding for each input to its corresponding positional embedding.\nThe individual token and position embeddings are both of size [1\u0002d], so their sum is\nalso[1\u0002d], This new embedding serves as the input for further processing. Fig. 9.13\nshows the idea.\nX = CompositeEmbeddings(word + position)Transformer BlockJanet1will2back3Janetwillbackthebillthe4bill5\n+++++PositionEmbeddingsWordEmbeddings\nFigure 9.13 A simple way to model position: add an embedding of the absolute position to\nthe token embedding to produce a new embedding of the same dimensionality.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 305,
      "token_count": 575,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 207\n\n9.5 \u2022 T HELANGUAGE MODELING HEAD 199\nThe \ufb01nal representation of the input, the matrix X, is an [N\u0002d]matrix in which\neach row iis the representation of the ith token in the input, computed by adding\nE[id(i)]\u2014the embedding of the id of the token that occurred at position i\u2014, to P[i],\nthe positional embedding of position i.\nA potential problem with the simple position embedding approach is that there\nwill be plenty of training examples for the initial positions in our inputs and corre-\nspondingly fewer at the outer length limits. These latter embeddings may be poorly\ntrained and may not generalize well during testing. An alternative is to choose a\nstatic function that maps integer inputs to real-valued vectors in a way that better\nhandle sequences of arbitrary length. A combination of sine and cosine functions\nwith differing frequencies was used in the original transformer work. Sinusoidal po-\nsition embeddings may also help in capturing the inherent relationships among the\npositions, like the fact that position 4 in an input is more closely related to position\n5 than it is to position 17.\nA more complex style of positional embedding methods extend this idea of cap-\nturing relationships even further to directly represent relative position instead ofrelative\nposition\nabsolute position, often implemented in the attention mechanism at each layer rather\nthan being added once at the initial input.\n9.5 The Language Modeling Head\nThe last component of the transformer we must introduce is the language modeling\nhead . Here we are using the word head to mean the additional neural circuitry welanguage\nmodeling head\nhead add on top of the basic transformer architecture when we apply pretrained trans-\nformer models to various tasks. The language modeling head is the circuitry we\nneed to do language modeling.\nRecall that language models, from the simple n-gram models of Chapter 3 through\nthe feedforward and RNN language models of Chapter 7 and Chapter 8, are word\npredictors. Given a context of words, they assign a probability to each possible next\nword. For example, if the preceding context is \u201cThanks for all the\u201d and we want to\nknow how likely the next word is \u201c\ufb01sh\u201d we would compute:\nP(\ufb01shjThanks for all the )\nLanguage models give us the ability to assign such a conditional probability to every\npossible next word, giving us a distribution over the entire vocabulary. The n-gram\nlanguage models of Chapter 3 compute the probability of a word given counts of\nits occurrence with the n\u00001 prior words. The context is thus of size n\u00001. For\ntransformer language models, the context is the size of the transformer\u2019s context\nwindow, which can be quite large, like 32K tokens for large models (and much larger\ncontexts of millions of words are possible with special long-context architectures).\nThe job of the language modeling head is to take the output of the \ufb01nal trans-\nformer layer from the last token Nand use it to predict the upcoming word at posi-\ntionN+1. Fig. 9.14 shows how to accomplish this task, taking the output of the last\ntoken at the last layer (the d-dimensional output embedding of shape [1\u0002d]) and\nproducing a probability distribution over words (from which we will choose one to\ngenerate).\nThe \ufb01rst module in Fig. 9.14 is a linear layer, whose job is to project from the\noutput hL\nN, which represents the output token embedding at position Nfrom the \ufb01nal",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 306,
      "token_count": 748,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 208\n\n200 CHAPTER 9 \u2022 T HETRANSFORMER\nLayer LTransformerBlockSoftmax over vocabulary VUnembedding layer\u20261 x |V|Logits Word probabilities1 x |V|hL1w1w2wNhL2hLNd x |V|1 x d   Unembedding layerU = ETy1y2y|V|\u2026u1u2u|V|\u2026Language Model Headtakes hLN and outputs adistribution over vocabulary V\nFigure 9.14 The language modeling head: the circuit at the top of a transformer that maps from the output\nembedding for token Nfrom the last transformer layer ( hL\nN) to a probability distribution over words in the\nvocabulary V.\nblock L, (hence of shape [1\u0002d]) to the logit vector, or score vector, that will have a logit\nsingle score for each of the jVjpossible words in the vocabulary V. The logit vector\nuis thus of dimensionality 1 \u0002jVj.\nThis linear layer can be learned, but more commonly we tie this matrix to (the\ntranspose of) the embedding matrix E. Recall that in weight tying , we use the weight tying\nsame weights for two different matrices in the model. Thus at the input stage of the\ntransformer the embedding matrix (of shape [jVj\u0002d]) is used to map from a one-hot\nvector over the vocabulary (of shape [1\u0002jVj]) to an embedding (of shape [1\u0002d]).\nAnd then in the language model head, ET, the transpose of the embedding matrix (of\nshape [d\u0002jVj]) is used to map back from an embedding (shape [1\u0002d]) to a vector\nover the vocabulary (shape [1 \u0002jVj]). In the learning process, Ewill be optimized to\nbe good at doing both of these mappings. We therefore sometimes call the transpose\nETtheunembedding layer because it is performing this reverse mapping. unembedding\nA softmax layer turns the logits uinto the probabilities yover the vocabulary.\nu=hL\nNET(9.45)\ny=softmax (u) (9.46)\nWe can use these probabilities to do things like help assign a probability to a\ngiven text. But the most important usage to generate text, which we do by sampling\na word from these probabilities y. We might sample the highest probability word\n(\u2018greedy\u2019 decoding), or use another of the sampling methods we\u2019ll introduce in Sec-\ntion 10.2. In either case, whatever entry ykwe choose from the probability vector y,\nwe generate the word that has that index k.\nFig. 9.15 shows the total stacked architecture for one token i. Note that the input\nto each transformer layer x`\niis the same as the output from the preceding layer h`\u00001\ni.\nNow that we see all these transformer layers spread out on the page, we can point\nout another useful feature of the unembedding layer: as a tool for interpretability of\nthe internals of the transformer that we call the logit lens (Nostalgebraist, 2020). logit lens\nWe can take a vector from any layer of the transformer and, pretending that it is\nthe pre\ufb01nal embedding, simply multiply it by the unembedding layer to get logits,\nand compute a softmax to see the distribution over words that that vector might\nbe representing. This can be a useful window into the internal representations of",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 307,
      "token_count": 738,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 209\n\n9.6 \u2022 S UMMARY 201\nwiSample token togenerate at position i+1\nfeedforwardlayer normattentionlayer norm\nU\nInput tokenLanguageModelingHead\nInputEncoding\nEi+\u2026logits\nfeedforwardlayer normattentionlayer normLayer 1Layer 2h1i  =  x2ix1ih2i  =  x3ifeedforwardlayer normattentionlayer normhLi  hL-1i  =  xLiy1y2y|V|\u2026Token probabilitiesu1u2u|V|\u2026softmaxwi+1\nLayer L\nFigure 9.15 A transformer language model (decoder-only), stacking transformer blocks\nand mapping from an input token wito to a predicted next token wi+1.\nthe model. Since the network wasn\u2019t trained to make the internal representations\nfunction in this way, the logit lens doesn\u2019t always work perfectly, but this can still\nbe a useful trick.\nA terminological note before we conclude: You will sometimes see a trans-\nformer used for this kind of unidirectional causal language model called a decoder-\nonly model . This is because this model constitutes roughly half of the encoder-decoder-only\nmodel\ndecoder model for transformers that we\u2019ll see how to apply to machine translation\nin Chapter 13. (Confusingly, the original introduction of the transformer had an\nencoder-decoder architecture, and it was only later that the standard paradigm for\ncausal language model was de\ufb01ned by using only the decoder part of this original\narchitecture).\n9.6 Summary\nThis chapter has introduced the transformer and its components for the task of lan-\nguage modeling. We\u2019ll continue the task of language modeling including issues like\ntraining and sampling in the next chapter.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 308,
      "token_count": 374,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 210\n\n202 CHAPTER 9 \u2022 T HETRANSFORMER\nHere\u2019s a summary of the main points that we covered:\n\u2022 Transformers are non-recurrent networks based on multi-head attention , a\nkind of self-attention . A multi-head attention computation takes an input\nvector xiand maps it to an output aiby adding in vectors from prior tokens,\nweighted by how relevant they are for the processing of the current word.\n\u2022 Atransformer block consists of a residual stream in which the input from\nthe prior layer is passed up to the next layer, with the output of different com-\nponents added to it. These components include a multi-head attention layer\nfollowed by a feedforward layer , each preceded by layer normalizations .\nTransformer blocks are stacked to make deeper and more powerful networks.\n\u2022 The input to a transformer is computed by adding an embedding (computed\nwith an embedding matrix ) to a positional encoding that represents the se-\nquential position of the token in the window.\n\u2022 Language models can be built out of stacks of transformer blocks, with a\nlanguage model head at the top, which applies an unembedding matrix to\nthe output Hof the top layer to generate the logits , which are then passed\nthrough a softmax to generate word probabilities.\n\u2022 Transformer-based language models have a wide context window (200K to-\nkens or even more for very large models with special mechanisms) allowing\nthem to draw on enormous amounts of context to predict upcoming words.\nBibliographical and Historical Notes\nThe transformer (Vaswani et al., 2017) was developed drawing on two lines of prior\nresearch: self-attention andmemory networks .\nEncoder-decoder attention, the idea of using a soft weighting over the encodings\nof input words to inform a generative decoder (see Chapter 13) was developed by\nGraves (2013) in the context of handwriting generation, and Bahdanau et al. (2015)\nfor MT. This idea was extended to self-attention by dropping the need for separate\nencoding and decoding sequences and instead seeing attention as a way of weighting\nthe tokens in collecting information passed from lower layers to higher layers (Ling\net al., 2015; Cheng et al., 2016; Liu et al., 2016).\nOther aspects of the transformer, including the terminology of key, query, and\nvalue, came from memory networks , a mechanism for adding an external read-\nwrite memory to networks, by using an embedding of a query to match keys rep-\nresenting content in an associative memory (Sukhbaatar et al., 2015; Weston et al.,\n2015; Graves et al., 2014).\nMORE HISTORY TBD IN NEXT DRAFT.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 309,
      "token_count": 564,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 211\n\nCHAPTER\n10Large Language Models\n\u201cHow much do we know at any time? Much more, or so I believe, than we\nknow we know. \u201d\nAgatha Christie, The Moving Finger\nFluent speakers of a language bring an enormous amount of knowledge to bear dur-\ning comprehension and production. This knowledge is embodied in many forms,\nperhaps most obviously in the vocabulary, the rich representations we have of words\nand their meanings and usage. This makes the vocabulary a useful lens to explore\nthe acquisition of knowledge from text, by both people and machines.\nEstimates of the size of adult vocabularies vary widely both within and across\nlanguages. For example, estimates of the vocabulary size of young adult speakers of\nAmerican English range from 30,000 to 100,000 depending on the resources used\nto make the estimate and the de\ufb01nition of what it means to know a word. What\nis agreed upon is that the vast majority of words that mature speakers use in their\nday-to-day interactions are acquired early in life through spoken interactions with\ncaregivers and peers, usually well before the start of formal schooling. This active\nvocabulary (usually on the order of 2000 words for young speakers) is extremely\nlimited compared to the size of the adult vocabulary, and is quite stable, with very\nfew additional words learned via casual conversation beyond this early stage. Obvi-\nously, this leaves a very large number of words to be acquired by other means.\nA simple consequence of these facts is that children have to learn about 7 to 10\nwords a day, every single day , to arrive at observed vocabulary levels by the time they\nare 20 years of age. And indeed empirical estimates of vocabulary growth in late el-\nementary through high school are consistent with this rate. How do children achieve\nthis rate of vocabulary growth? The bulk of this knowledge acquisition seems to\nhappen as a by-product of reading, as part of the rich processing and reasoning that\nwe perform when we read. Research into the average amount of time children spend\nreading, and the lexical diversity of the texts they read, indicate that it is possible\nto achieve the desired rate. But the mechanism behind this rate of learning must\nbe remarkable indeed, since at some points during learning the rate of vocabulary\ngrowth exceeds the rate at which new words are appearing to the learner!\nSuch facts have motivated the distributional hypothesis of Chapter 6, which sug-\ngests that aspects of meaning can be learned solely from the texts we encounter over\nour lives, based on the complex association of words with the words they co-occur\nwith (and with the words that those words occur with). The distributional hypothe-\nsis suggests both that we can acquire remarkable amounts of knowledge from text,\nand that this knowledge can be brought to bear long after its initial acquisition. Of\ncourse, grounding from real-world interaction or other modalities can help build\neven more powerful models, but even text alone is remarkably useful.\nIn this chapter we formalize this idea of pretraining \u2014learning knowledge about pretraining\nlanguage and the world from vast amounts of text\u2014and call the resulting pretrained\nlanguage models large language models . Large language models exhibit remark-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 310,
      "token_count": 670,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 212\n\n204 CHAPTER 10 \u2022 L ARGE LANGUAGE MODELS\nable performance on all sorts of natural language tasks because of the knowledge\nthey learn in pretraining, and they will play a role throughout the rest of this book.\nThey have been especially transformative for tasks where we need to produce text,\nlike summarization, machine translation, question answering, or chatbots.\nWe\u2019ll start by seeing how to apply the transformer of Chapter 9 to language\nmodeling, in a setting often called causal or autoregressive language models, in\nwhich we iteratively predict words left-to-right from earlier words. We\u2019ll \ufb01rst in-\ntroduce training, seeing how language models are self-trained by iteratively being\ntaught to guess the next word in the text from the prior words.\nWe\u2019ll then talk about the process of text generation. The application of LLMs\nto generate text has vastly broadened the scope of NLP,. Text generation, code-\ngeneration, and image-generation together constitute the important new area of gen-\nerative AI . We\u2019ll introduce speci\ufb01c algorithms for generating text from a language generative AI\nmodel, like greedy decoding andsampling . And we\u2019ll see that almost any NLP\ntask can be modeled as word prediction in a large language model, if we think about\nit in the right way. We\u2019ll work through an example of using large language mod-\nels to solve one classic NLP task of summarization (generating a short text that\nsummarizes some larger document).\n10.1 Large Language Models with Transformers\nThe prior chapter introduced most of the components of a transformer in the domain\nof language modeling: the transformer block including multi-head attention , the\nlanguage modeling head , and the positional encoding of the input. In the following\nsections we\u2019ll introduce the remaining aspects of the transformer LLM: sampling\nandtraining . Before we do that, we use this section to talk about why and how we\napply transformer-based large language models to NLP tasks.\nThe tasks we will describe are all cases of conditional generation . Conditionalconditional\ngeneration\ngeneration is the task of generating text conditioned on an input piece of text. That\nis, we give the LLM an input piece of text, generally called a prompt , and then have\nthe LLM continue generating text token by token, conditioned on the prompt and\nthe previously generated tokens. The fact that transformers have such long contexts\n(many thousands of tokens) makes them very powerful for conditional generation,\nbecause they can look back so far into the prompting text.\nConsider the simple task of text completion, illustrated in Fig. 10.1. Here a\nlanguage model is given a text pre\ufb01x and is asked to generate a possible completion.\nNote that as the generation process proceeds, the model has direct access to the\npriming context as well as to all of its own subsequently generated outputs (at least\nas much as \ufb01ts in the large context window). This ability to incorporate the entirety\nof the earlier context and generated outputs at each time step is the key to the power\nof large language models built from transformers.\nSo why should we care about predicting upcoming words or tokens? The in-\nsight of large language modeling is that many practical NLP tasks can be cast as\nword prediction , and that a powerful-enough language model can solve them with\na high degree of accuracy. For example, we can cast sentiment analysis as language\nmodeling by giving a language model a context like:\nThe sentiment of the sentence ``I like Jackie Chan\" is:\nand comparing the following conditional probability of the words \u201cpositive\u201d and the",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 311,
      "token_count": 751,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 213\n\n10.1 \u2022 L ARGE LANGUAGE MODELS WITH TRANSFORMERS 205\nPre\ufb01x TextCompletion Text\nEncoderTransformerBlocksSoftmax\nlongall\nandthanksforallthe\nthe\u2026UUUnencoder layerLanguage ModelingHeadlogits\nSo\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\u2026\nFigure 10.1 Left-to-right (also called autoregressive) text completion with transformer-based large language\nmodels. As each token is generated, it gets added onto the context as a pre\ufb01x for generating the next token.\nword \u201cnegative\u201d to see which is higher:\nP(positivejThe sentiment of the sentence ``I like Jackie Chan\" is: )\nP(negativejThe sentiment of the sentence ``I like Jackie Chan\" is: )\nIf the word \u201cpositive\u201d is more probable, we say the sentiment of the sentence is\npositive, otherwise we say the sentiment is negative.\nWe can also cast more complex tasks as word prediction. Consider question\nanswering, in which the system is given a question (for example a question with\na simple factual answer) and must give a textual answer; we introduce this task in\ndetail in Chapter 14. We can cast the task of question answering as word prediction\nby giving a language model a question and a token like A:suggesting that an answer\nshould come next:\nQ: Who wrote the book ``The Origin of Species\"? A:\nIf we ask a language model to compute the probability distribution over possible\nnext words given this pre\ufb01x:\nP(wjQ: Who wrote the book ``The Origin of Species\"? A: )\nand look at which words whave high probabilities, we might expect to see that\nCharles is very likely, and then if we choose Charles and continue and ask\nP(wjQ: Who wrote the book ``The Origin of Species\"? A: Charles )\nwe might now see that Darwin is the most probable token, and select it.\nConditional generation can even be used to accomplish tasks that must generate\nlonger responses. Consider the task of text summarization , which is to take a longtext\nsummarization\ntext, such as a full-length article, and produce an effective shorter summary of it. We\ncan cast summarization as language modeling by giving a large language model a\ntext, and follow the text by a token like tl;dr ; this token is short for something like",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 312,
      "token_count": 512,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 214",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 313,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "206 CHAPTER 10 \u2022 L ARGE LANGUAGE MODELS\n\u2018too long; didn\u2019t read\u2019 and in recent years people often use this token, especially in\ninformal work emails, when they are going to give a short summary. Since this token\nis suf\ufb01ciently frequent in language model training data, language models have seen\nmany texts in which the token occurs before a summary, and hence will interpret the\ntoken as instructions to generate a summary. We can then do conditional generation:\ngive the language model this pre\ufb01x, and then have it generate the following words,\none by one, and take the entire response as a summary. Fig. 10.2 shows an example\nof a text and a human-produced summary from a widely-used summarization corpus\nconsisting of CNN and Daily Mail news articles.\nOriginal Article\nThe only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuff\nand offering it for sale online? People are actually buying it. For $89, self-styled entrepreneur\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box \u2013 enough\nfor 10 to 15 snowballs, he says.\nBut not if you live in New England or surrounding states. \u201cWe will not ship snow to any states\nin the northeast!\u201d says Waring\u2019s website, ShipSnowYo.com. \u201cWe\u2019re in the business of expunging\nsnow!\u201d\nHis website and social media accounts claim to have \ufb01lled more than 133 orders for snow \u2013 more\nthan 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set a\nrecord this winter for the snowiest month in its history. Most residents see the huge piles of snow\nchoking their yards and sidewalks as a nuisance, but Waring saw an opportunity.\nAccording to Boston.com, it all started a few weeks ago, when Waring and his wife were shov-\neling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston. He\njoked about shipping the stuff to friends and family in warmer states, and an idea was born. [...]\nSummary\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box \u2013 enough\nfor 10 to 15 snowballs, he says. But not if you live in New England or surrounding states.\nFigure 10.2 Excerpt from a sample article and its summary from the CNN/Daily Mail summarization corpus\n(Hermann et al., 2015b), (Nallapati et al., 2016).\nIf we take this full article and append the token tl;dr , we can use this as the con-\ntext to prime the generation process to produce a summary as illustrated in Fig. 10.3.\nAgain, what makes transformers able to succeed at this task (as compared, say, to\nthe primitive n-gram language model) is that attention can incorporate information\nfrom the large context window, giving the model access to the original article as well\nas to the newly generated text throughout the process.\nWhich words do we generate at each step? One simple way to generate words\nis to always generate the most likely word given the context. Generating the most\nlikely word given the context is called greedy decoding . A greedy algorithm is onegreedy\ndecoding\nthat make a choice that is locally optimal, whether or not it will turn out to have\nbeen the best choice with hindsight. Thus in greedy decoding, at each time step in\ngeneration, the output ytis chosen by computing the probability for each possible\noutput (every word in the vocabulary) and then choosing the highest probability\nword (the argmax):",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 314,
      "token_count": 782,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 10.2 Excerpt from a sample article and its summary from the CNN/Daily Mail summarization corpus\n(Hermann et al., 2015b), (Nallapati et al., 2016).\nIf we take this full article and append the token tl;dr , we can use this as the con-\ntext to prime the generation process to produce a summary as illustrated in Fig. 10.3.\nAgain, what makes transformers able to succeed at this task (as compared, say, to\nthe primitive n-gram language model) is that attention can incorporate information\nfrom the large context window, giving the model access to the original article as well\nas to the newly generated text throughout the process.\nWhich words do we generate at each step? One simple way to generate words\nis to always generate the most likely word given the context. Generating the most\nlikely word given the context is called greedy decoding . A greedy algorithm is onegreedy\ndecoding\nthat make a choice that is locally optimal, whether or not it will turn out to have\nbeen the best choice with hindsight. Thus in greedy decoding, at each time step in\ngeneration, the output ytis chosen by computing the probability for each possible\noutput (every word in the vocabulary) and then choosing the highest probability\nword (the argmax):\n\u02c6wt=argmaxw2VP(wjw<t) (10.1)\nIn practice, however, we don\u2019t use greedy decoding with large language models.\nA major problem with greedy decoding is that because the words it chooses are (by\nde\ufb01nition) extremely predictable, the resulting text is generic and often quite repeti-\ntive. Indeed, greedy decoding is so predictable that it is deterministic; if the context",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 315,
      "token_count": 366,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 215\n\n10.2 \u2022 S AMPLING FOR LLM G ENERATION 207\nOriginal StoryGenerated Summary\n\u2026ideaKyle\nwasborn.KyleWaring\nWaringonlyThe\u2026will\nDelimiterwillUUU\ntl;drLM Head\nE\nE\nE\nE\nE\nE\nE\nE\u2026\nFigure 10.3 Summarization with large language models using the tl;dr token and context-based autore-\ngressive generation.\nis identical, and the probabilistic model is the same, greedy decoding will always re-\nsult in generating exactly the same string. We\u2019ll see in Chapter 13 that an extension\nto greedy decoding called beam search works well in tasks like machine translation,\nwhich are very constrained in that we are always generating a text in one language\nconditioned on a very speci\ufb01c text in another language. In most other tasks, how-\never, people prefer text which has been generated by more sophisticated methods,\ncalled sampling methods , that introduce a bit more diversity into the generations.\nWe\u2019ll see how to do that in the next few sections.\n10.2 Sampling for LLM Generation\nThe core of the generation process for large language models is the task of choosing\nthe single word to generate next based on the context and based on the probabilities\nthat the model assigns to possible words. This task of choosing a word to generate\nbased on the model\u2019s probabilities is called decoding . Decoding from a language decoding\nmodel in a left-to-right manner (or right-to-left for languages like Arabic in which\nwe read from right to left), and thus repeatedly choosing the next word conditioned\non our previous choices is called autoregressive generation orcausal LM genera-autoregressive\ngeneration\ntion.1(As we\u2019ll see, alternatives like the masked language models of Chapter 11 are\nnon-causal because they can predict words based on both past and future words).\nThe most common method for decoding in large language models is sampling .\nRecall from Chapter 3 that sampling from a model\u2019s distribution over words means sampling\nto choose random words according to their probability assigned by the model. That\nis, we iteratively choose a word to generate according to its probability in context\n1Technically an autoregressive model predicts a value at time tbased on a linear function of the values\nat times t\u00001,t\u00002, and so on. Although language models are not linear (since they have many layers of\nnon-linearities), we loosely refer to this generation technique as autoregressive since the word generated\nat each time step is conditioned on the word selected by the network from the previous step.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 316,
      "token_count": 550,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 216\n\n208 CHAPTER 10 \u2022 L ARGE LANGUAGE MODELS\nas de\ufb01ned by the model. Thus we are more likely to generate words that the model\nthinks have a high probability in the context and less likely to generate words that\nthe model thinks have a low probability.\nWe saw back in Chapter 3 on page 43 how to generate text from a unigram lan-\nguage model , by repeatedly randomly sampling words according to their probability\nuntil we either reach a pre-determined length or select the end-of-sentence token. To\ngenerate text from a trained transformer language model we\u2019ll just generalize this\nmodel a bit: at each step we\u2019ll sample words according to their probability condi-\ntioned on our previous choices , and we\u2019ll use a transformer language model as the\nprobability model that tells us this probability.\nWe can formalize this algorithm for generating a sequence of words W=w1;w2;:::; wN\nuntil we hit the end-of-sequence token, using x\u0018p(x)to mean \u2018choose xby sam-\npling from the distribution p(x):\ni 1\nwi\u0018p(w)\nwhile wi!= EOS\ni i + 1\nwi\u0018p(wijw<i)\nThe algorithm above is called random sampling , and it turns out random sam-random\nsampling\npling doesn\u2019t work well enough. The problem is that even though random sampling\nis mostly going to generate sensible, high-probable words, there are many odd, low-\nprobability words in the tail of the distribution, and even though each one is low-\nprobability, if you add up all the rare words, they constitute a large enough portion\nof the distribution that they get chosen often enough to result in generating weird\nsentences. For this reason, instead of random sampling, we usually use sampling\nmethods that avoid generating the very unlikely words.\nThe sampling methods we introduce below each have parameters that enable\ntrading off two important factors in generation: quality anddiversity . Methods\nthat emphasize the most probable words tend to produce generations that are rated\nby people as more accurate, more coherent, and more factual, but also more boring\nand more repetitive. Methods that give a bit more weight to the middle-probability\nwords tend to be more creative and more diverse, but less factual and more likely to\nbe incoherent or otherwise low-quality.\n10.2.1 Top- ksampling\nTop-k sampling is a simple generalization of greedy decoding. Instead of choosing top-k sampling\nthe single most probable word to generate, we \ufb01rst truncate the distribution to the\ntopkmost likely words, renormalize to produce a legitimate probability distribution,\nand then randomly sample from within these kwords according to their renormalized\nprobabilities. More formally:\n1. Choose in advance a number of words k\n2. For each word in the vocabulary V, use the language model to compute the\nlikelihood of this word given the context p(wtjw<t)\n3. Sort the words by their likelihood, and throw away any word that is not one of\nthe top kmost probable words.\n4. Renormalize the scores of the kwords to be a legitimate probability distribu-\ntion.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 317,
      "token_count": 670,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 217\n\n10.2 \u2022 S AMPLING FOR LLM G ENERATION 209\n5. Randomly sample a word from within these remaining kmost-probable words\naccording to its probability.\nWhen k=1, top- ksampling is identical to greedy decoding. Setting kto a larger\nnumber than 1 leads us to sometimes select a word which is not necessarily the most\nprobable, but is still probable enough, and whose choice results in generating more\ndiverse but still high-enough-quality text.\n10.2.2 Nucleus or top- psampling\nOne problem with top- ksampling is that kis \ufb01xed, but the shape of the probability\ndistribution over words differs in different contexts. If we set k=10, sometimes\nthe top 10 words will be very likely and include most of the probability mass, but\nother times the probability distribution will be \ufb02atter and the top 10 words will only\ninclude a small part of the probability mass.\nAn alternative, called top-p sampling ornucleus sampling (Holtzman et al., top-p sampling\n2020), is to keep not the top kwords, but the top ppercent of the probability mass.\nThe goal is the same; to truncate the distribution to remove the very unlikely words.\nBut by measuring probability rather than the number of words, the hope is that the\nmeasure will be more robust in very different contexts, dynamically increasing and\ndecreasing the pool of word candidates.\nGiven a distribution P(wtjw<t), we sort the distribution from most probable, and\nthen the top- pvocabulary V(p)is the smallest set of words such that\nX\nw2V(p)P(wjw<t)\u0015p: (10.2)\n10.2.3 Temperature sampling\nIntemperature sampling , we don\u2019t truncate the distribution, but instead reshapetemperature\nsampling\nit. The intuition for temperature sampling comes from thermodynamics, where a\nsystem at a high temperature is very \ufb02exible and can explore many possible states,\nwhile a system at a lower temperature is likely to explore a subset of lower energy\n(better) states. In low-temperature sampling, we smoothly increase the probability\nof the most probable words and decrease the probability of the rare words.\nWe implement this intuition by simply dividing the logit by a temperature param-\netertbefore we normalize it by passing it through the softmax. In low-temperature\nsampling, t2(0;1]. Thus instead of computing the probability distribution over the\nvocabulary directly from the logit as in the following (repeated from Eq. 9.46):\ny=softmax (u) (10.3)\nwe instead \ufb01rst divide the logits by t, computing the probability vector yas\ny=softmax (u=t) (10.4)\nWhy does this work? When tis close to 1 the distribution doesn\u2019t change much.\nBut the lower tis, the larger the scores being passed to the softmax (dividing by a\nsmaller fraction t\u00141 results in making each score larger). Recall that one of the\nuseful properties of a softmax is that it tends to push high values toward 1 and low\nvalues toward 0. Thus when larger numbers are passed to a softmax the result is\na distribution with increased probabilities of the most high-probability words and\ndecreased probabilities of the low probability words, making the distribution more\ngreedy. As tapproaches 0 the probability of the most likely word approaches 1.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 318,
      "token_count": 744,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 218\n\n210 CHAPTER 10 \u2022 L ARGE LANGUAGE MODELS\nNote, by the way, that there can be other situations where we may want to do\nsomething quite different and \ufb02atten the word probability distribution instead of\nmaking it greedy. Temperature sampling can help with this situation too, in this case\nhigh-temperature sampling, in which case we use t>1.\n10.3 Pretraining Large Language Models\nHow do we teach a transformer to be a language model? What is the algorithm and\nwhat data do we train on?\n10.3.1 Self-supervised training algorithm\nTo train a transformer as a language model, we use the same self-supervision (or self-supervision\nself-training ) algorithm we saw in Section 8.2.2: we take a corpus of text as training\nmaterial and at each time step task the model to predict the next word. We call such\na model self-supervised because we don\u2019t have to add any special gold labels to\nthe data; the natural sequence of words is its own supervision! We simply train the\nmodel to minimize the error in predicting the true next word in the training sequence,\nusing cross-entropy as the loss function.\nRecall that the cross-entropy loss measures the difference between a predicted\nprobability distribution and the correct distribution.\nLCE=\u0000X\nw2Vyt[w]log\u02c6yt[w] (10.5)\nIn the case of language modeling, the correct distribution ytcomes from knowing the\nnext word. This is represented as a one-hot vector corresponding to the vocabulary\nwhere the entry for the actual next word is 1, and all the other entries are 0. Thus,\nthe cross-entropy loss for language modeling is determined by the probability the\nmodel assigns to the correct next word (all other words get multiplied by zero). So\nat time tthe CE loss in Eq. 10.5 can be simpli\ufb01ed as the negative log probability the\nmodel assigns to the next word in the training sequence.\nLCE(\u02c6yt;yt) =\u0000log\u02c6yt[wt+1] (10.6)\nThus at each word position tof the input, the model takes as input the correct se-\nquence of tokens w1:t, and uses them to compute a probability distribution over\npossible next words so as to compute the model\u2019s loss for the next token wt+1. Then\nwe move to the next word, we ignore what the model predicted for the next word\nand instead use the correct sequence of tokens w1:t+1to estimate the probability of\ntoken wt+2. This idea that we always give the model the correct history sequence to\npredict the next word (rather than feeding the model its best case from the previous\ntime step) is called teacher forcing . teacher forcing\nFig. 10.4 illustrates the general training approach. At each step, given all the\npreceding words, the \ufb01nal transformer layer produces an output distribution over\nthe entire vocabulary. During training, the probability assigned to the correct word\nis used to calculate the cross-entropy loss for each item in the sequence. The loss\nfor a training sequence is the average cross-entropy loss over the entire sequence.\nThe weights in the network are adjusted to minimize the average CE loss over the\ntraining sequence via gradient descent.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 319,
      "token_count": 700,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 219",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 320,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "10.3 \u2022 P RETRAINING LARGE LANGUAGE MODELS 211\nlongandthanksforNext tokenallLoss\u2026=\n<latexit sha1_base64=\"AovqpaL476UmJ1EU1xZPgDZ70tQ=\">AAAB9nicbVDLSsNAFL2pr1pfURcu3AwWwY0lEakui25cVrAPaEqYTCbt0EkmzEzEEvIrbkTcKPgZ/oJ/Y9Jm09YDA4dzznDvPV7MmdKW9WtU1tY3Nreq27Wd3b39A/PwqKtEIgntEMGF7HtYUc4i2tFMc9qPJcWhx2nPm9wXfu+ZSsVE9KSnMR2GeBSxgBGsc8k1Ty4dLkZo6qZOiPVYhimO/CyruWbdalgzoFVil6QOJdqu+eP4giQhjTThWKmBbcV6mGKpGeE0qzmJojEmEzyi6WztDJ3nko8CIfMXaTRTF3I4VGoaenmy2E0te4X4nzdIdHA7TFkUJ5pGZD4oSDjSAhUdIJ9JSjSf5gQTyfINERljiYnOmypOt5cPXSXdq4bdbDQfr+utu7KEKpzCGVyADTfQggdoQwcIZPAGn/BlvBivxrvxMY9WjPLPMSzA+P4DPEiSHA==</latexit>\u0000logyand\nStackedTransformerBlocksSolongandthanksfor\u2026\u2026\n\u2026\nU\nInput tokensx1x2LanguageModelingHeadx3x4x5InputEncoding\nE1+\nE2+\nE3+\nE4+\nE5+\u2026\u2026\u2026\u2026\u2026\nU\nU\nU\nU\u2026logitslogitslogitslogitslogits\u2026",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 321,
      "token_count": 464,
      "chapter_title": ""
    }
  },
  {
    "content": "StackedTransformerBlocksSolongandthanksfor\u2026\u2026\n\u2026\nU\nInput tokensx1x2LanguageModelingHeadx3x4x5InputEncoding\nE1+\nE2+\nE3+\nE4+\nE5+\u2026\u2026\u2026\u2026\u2026\nU\nU\nU\nU\u2026logitslogitslogitslogitslogits\u2026\n<latexit sha1_base64=\"q3ZgXDyG7qtkT7t8hT47RdlwYG4=\">AAAB+XicbVDLSsNAFJ3UV62vWHe6GVsEN5bERXUlBUVcVrAPaEqYTCft0MlMmJkIIQT8AT/CTRE3Cv6Ev+DfmLTdtPXAwOGcM9x7jxcyqrRl/RqFtfWNza3idmlnd2//wDwst5WIJCYtLJiQXQ8pwignLU01I91QEhR4jHS88W3ud56JVFTwJx2HpB+gIac+xUhnkmseXzhMDGHsJk6A9EgGiR4hPlZpWnLNqlWzpoCrxJ6TauP0tXw3qdw0XfPHGQgcBYRrzJBSPdsKdT9BUlPMSFpyIkVChMdoSJLp5ik8y6QB9IXMHtdwqi7kUKBUHHhZMl9PLXu5+J/Xi7R/3U8oDyNNOJ4N8iMGtYB5DXBAJcGaxRlBWNJsQ4hHSCKss7Ly0+3lQ1dJ+7Jm12v1x6yDezBDEZyACjgHNrgCDfAAmqAFMHgBE/AJvozEeDPejY9ZtGDM/xyBBRjff79pldo=</latexit>\u0000logythanks\nFigure 10.4 Training a transformer as a language model.\nNote the key difference between this \ufb01gure and the earlier RNN-based version\nshown in Fig. 8.6. There the calculation of the outputs and the losses at each step\nwas inherently serial given the recurrence in the calculation of the hidden states.\nWith transformers, each training item can be processed in parallel since the output\nfor each element in the sequence is computed separately.\nLarge models are generally trained by \ufb01lling the full context window (for exam-\nple 4096 tokens for GPT4 or 8192 for Llama 3) with text. If documents are shorter\nthan this, multiple documents are packed into the window with a special end-of-text\ntoken between them. The batch size for gradient descent is usually quite large (the\nlargest GPT-3 model uses a batch size of 3.2 million tokens).\n10.3.2 Training corpora for large language models\nLarge language models are mainly trained on text scraped from the web, augmented\nby more carefully curated data. Because these training corpora are so large, they are\nlikely to contain many natural examples that can be helpful for NLP tasks, such as\nquestion and answer pairs (for example from FAQ lists), translations of sentences\nbetween various languages, documents together with their summaries, and so on.\nWeb text is usually taken from corpora of automatically-crawled web pages like\nthecommon crawl , a series of snapshots of the entire web produced by the non- common crawl",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 322,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "shown in Fig. 8.6. There the calculation of the outputs and the losses at each step\nwas inherently serial given the recurrence in the calculation of the hidden states.\nWith transformers, each training item can be processed in parallel since the output\nfor each element in the sequence is computed separately.\nLarge models are generally trained by \ufb01lling the full context window (for exam-\nple 4096 tokens for GPT4 or 8192 for Llama 3) with text. If documents are shorter\nthan this, multiple documents are packed into the window with a special end-of-text\ntoken between them. The batch size for gradient descent is usually quite large (the\nlargest GPT-3 model uses a batch size of 3.2 million tokens).\n10.3.2 Training corpora for large language models\nLarge language models are mainly trained on text scraped from the web, augmented\nby more carefully curated data. Because these training corpora are so large, they are\nlikely to contain many natural examples that can be helpful for NLP tasks, such as\nquestion and answer pairs (for example from FAQ lists), translations of sentences\nbetween various languages, documents together with their summaries, and so on.\nWeb text is usually taken from corpora of automatically-crawled web pages like\nthecommon crawl , a series of snapshots of the entire web produced by the non- common crawl\npro\ufb01t Common Crawl ( https://commoncrawl.org/ ) that each have billions of\nwebpages. Various versions of common crawl data exist, such as the Colossal Clean\nCrawled Corpus (C4; Raffel et al. 2020), a corpus of 156 billion tokens of English\nthat is \ufb01ltered in various ways (deduplicated, removing non-natural language like\ncode, sentences with offensive words from a blocklist). This C4 corpus seems to\nconsist in large part of patent text documents, Wikipedia, and news sites (Dodge\net al., 2021).\nWikipedia plays a role in lots of language model training, as do corpora of books.\nThe Pile (Gao et al., 2020) is an 825 GB English text corpus that is constructed by The Pile\npublicly released code, containing again a large amount of text scraped from the web",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 323,
      "token_count": 485,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 220",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 324,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "212 CHAPTER 10 \u2022 L ARGE LANGUAGE MODELS\nas well as books and Wikipedia; Fig. 10.5 shows its composition. Dolma is a larger\nopen corpus of English, created with public tools, containing three trillion tokens,\nwhich similarly consists of web text, academic papers, code, books, encyclopedic\nmaterials, and social media (Soldaini et al., 2024).\nFigure 1: Treemap of Pile components by effective size.\ntroduce a new \ufb01ltered subset of Common Crawl,\nPile-CC, with improved extraction quality.\nThrough our analyses, we con\ufb01rm that the Pile is\nsigni\ufb01cantly distinct from pure Common Crawl\ndata. Additionally, our evaluations show that the\nexisting GPT-2 and GPT-3 models perform poorly\non many components of the Pile, and that models\ntrained on the Pile signi\ufb01cantly outperform both\nraw and \ufb01ltered Common Crawl models. To com-\nplement the performance evaluations, we also per-\nform an exploratory analysis of the text within the\nPile to provide a detailed picture of the data. We\nhope that our extensive documentation of the con-\nstruction and characteristics of the Pile will help\nresearchers make informed decisions about poten-\ntial downstream applications.\nFinally, we make publicly available the preprocess-\ning code for the constituent datasets of the Pile and\nthe code for constructing alternative versions2. In\nthe interest of reproducibility, we also document\nall processing performed on each dataset (and the\nPile as a whole) in as much detail as possible. For\nfurther details about the processing of each dataset,\nsee Section 2and Appendix C.\n2https://github.com/EleutherAI/\nthe-pile1.1 Contributions\nThe core contributions of this paper are:\n1.The introduction of a 825.18GiB english-\nlanguage dataset for language modeling com-\nbining 22 diverse sources.\n2.The introduction of 14new language model-\ning datasets, which we expect to be of inde-\npendent interest to researchers.\n3.Evaluations demonstrating signi\ufb01cant im-\nprovements across many domains by GPT-2-\nsized models trained on this new dataset, com-\npared to training on CC-100 and raw Common\nCrawl.\n4.The investigation and documentation of this\ndataset, which we hope will better inform re-\nsearchers about how to use it as well as moti-\nvate them to undertake similar investigations\nof their own data.\n2 The Pile Datasets\nThe Pile is composed of 22 constituent sub-datasets,\nas shown in Table 1. Following Brown et al. (2020 ),\nwe increase the weights of higher quality compo-\nnents, with certain high-quality datasets such as\nWikipedia being seen up to 3 times (\u201cepochs\u201d) for\n2\nFigure 10.5 The Pile corpus, showing the size of different components, color coded as\nacademic (articles from PubMed and ArXiv, patents from the USPTA; internet (webtext in-\ncluding a subset of the common crawl as well as Wikipedia), prose (a large corpus of books),\ndialogue (including movie subtitles and chat data), and misc.. Figure from Gao et al. (2020).\nFiltering for quality and safety Pretraining data drawn from the web is \ufb01ltered\nfor both quality andsafety . Quality \ufb01lters are classi\ufb01ers that assign a score to each\ndocument. Quality is of course subjective, so different quality \ufb01lters are trained",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 325,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "pared to training on CC-100 and raw Common\nCrawl.\n4.The investigation and documentation of this\ndataset, which we hope will better inform re-\nsearchers about how to use it as well as moti-\nvate them to undertake similar investigations\nof their own data.\n2 The Pile Datasets\nThe Pile is composed of 22 constituent sub-datasets,\nas shown in Table 1. Following Brown et al. (2020 ),\nwe increase the weights of higher quality compo-\nnents, with certain high-quality datasets such as\nWikipedia being seen up to 3 times (\u201cepochs\u201d) for\n2\nFigure 10.5 The Pile corpus, showing the size of different components, color coded as\nacademic (articles from PubMed and ArXiv, patents from the USPTA; internet (webtext in-\ncluding a subset of the common crawl as well as Wikipedia), prose (a large corpus of books),\ndialogue (including movie subtitles and chat data), and misc.. Figure from Gao et al. (2020).\nFiltering for quality and safety Pretraining data drawn from the web is \ufb01ltered\nfor both quality andsafety . Quality \ufb01lters are classi\ufb01ers that assign a score to each\ndocument. Quality is of course subjective, so different quality \ufb01lters are trained\nin different ways, but often to value high-quality reference corpora like Wikipedia,\nbooks, and particular websites and to avoid websites with lots of PII(Personal Iden- PII\nti\ufb01able Information) or adult content. Filters also remove boilerplate text which is\nvery frequent on the web. Another kind of quality \ufb01ltering is deduplication, which\ncan be done at various levels, so as to remove duplicate documents, duplicate web\npages, or duplicate text. Quality \ufb01ltering generally improves language model per-\nformance (Longpre et al., 2024b; Llama Team, 2024).\nSafety \ufb01ltering is again a subjective decision, and often includes toxicity detec-\ntion based on running off-the-shelf toxicity classi\ufb01ers. This can have mixed results.\nOne problem is that current toxicity classi\ufb01ers mistakenly \ufb02ag non-toxic data if it\nis generated by speakers of minority dialects like African American English (Xu\net al., 2021). Another problem is that models trained on toxicity-\ufb01ltered data, while\nsomewhat less toxic, are also worse at detecting toxicity themselves (Longpre et al.,\n2024b). These issues make the question of how to do better safety \ufb01ltering an im-\nportant open problem.\nUsing large datasets scraped from the web to train language models poses ethical\nand legal questions:\nCopyright: Much of the text in these large datasets (like the collections of \ufb01c-\ntion and non-\ufb01ction books) is copyrighted. In some countries, like the United\nStates, the fair use doctrine may allow copyrighted content to be used for\ntransformative uses, but it\u2019s not clear if that remains true if the language mod-\nels are used to generate text that competes with the market for the text they",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 326,
      "token_count": 666,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 221\n\n10.3 \u2022 P RETRAINING LARGE LANGUAGE MODELS 213\nare trained on (Henderson et al., 2023).\nData consent: Owners of websites can indicate that they don\u2019t want their sites\nto be crawled by web crawlers (either via a robots.txt \ufb01le, or via Terms of\nService). Recently there has been a sharp increase in the number of web-\nsites that have indicated that they don\u2019t want large language model builders\ncrawling their sites for training data (Longpre et al., 2024a). Because it\u2019s not\nclear what legal status these indications have in different countries, or whether\nthese restrictions are retroactive, what effect this will have on large pretraining\ndatasets is unclear.\nPrivacy: Large web datasets also have privacy issues since they contain private\ninformation like phone numbers and IP addresses. While \ufb01lters are used to try\nto remove websites likely to contain large amounts of personal information,\nsuch \ufb01ltering isn\u2019t suf\ufb01cient.\n10.3.3 Finetuning\nAlthough the enormous pretraining data for a large language model includes text\nfrom many domains, it\u2019s often the case that we want to apply it in a new domain or\ntask that might not have appeared suf\ufb01ciently in the pre-training data. For example,\nwe might want a language model that\u2019s specialized to legal or medical text. Or we\nmight have a multilingual language model that knows many languages but might\nbene\ufb01t from some more data in our particular language of interest. Or we want a\nlanguage model that is specialized to a particular task.\nIn such cases, we can simply continue training the model on relevant data from\nthe new domain or language (Gururangan et al., 2020). This process of taking a fully\npretrained model and running additional training passes on some new data is called\n\ufb01netuning . Fig. 10.6 sketches the paradigm. \ufb01netuning\nFine-tuning Data\nPretraining DataPretraining\n\u2026\n\u2026\n\u2026Fine-tuning\n\u2026\n\u2026\n\u2026Pretrained LMFine-tuned LM\nFigure 10.6 Pretraining and \ufb01netuning. A pre-trained model can be \ufb01netuned to a par-\nticular domain, dataset, or task. There are many different ways to \ufb01netune, depending on\nexactly which parameters are updated from the \ufb01netuning data: all the parameters, some of\nthe parameters, or only the parameters of speci\ufb01c extra circuitry.\nWe\u2019ll introduce four related kinds of \ufb01netuning in this chapter and the two fol-\nlowing chapters. In all four cases, \ufb01netuning means the process of taking a pre-\ntrained model and further adapting some or all of its parameters to some new data.\nBut they differ on exactly which parameters get updated.\nIn the \ufb01rst kind of \ufb01netuning we retrain all the parameters of the model on this\nnew data, using the same method (word prediction) and loss function (cross-entropy\nloss) as for pretraining. In a sense it\u2019s as if the new data were at the tail end of",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 327,
      "token_count": 666,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 222\n\n214 CHAPTER 10 \u2022 L ARGE LANGUAGE MODELS\nthe pretraining data, and so you\u2019ll sometimes see this method called continued pre-\ntraining .continued\npretraining\nRetraining all the parameters of the model is very slow and expensive when the\nlanguage model is huge. So instead we can freeze some of the parameters (i.e., leave freeze\nthem unchanged from their pretrained value) and train only a subset of parameters\non the new data. In Section 10.5.3 we\u2019ll describe this second variety of \ufb01netun-\ning, called parameter-ef\ufb01cient \ufb01netuning , orPEFT . because we ef\ufb01ciently select\nspeci\ufb01c parameters to update when \ufb01netuning, and leave the rest in their pretrained\nvalues.\nIn Chapter 11 we\u2019ll introduce a third kind of \ufb01netuning, also parameter-ef\ufb01cient.\nIn this version, the goal is to use a language model as a kind of classi\ufb01er or labeler\nfor a speci\ufb01c task. For example we might train the model to be a sentiment classi\ufb01er.\nWe do this by adding extra neural circuitry (an extra head ) after the top layer of the\nmodel. This classi\ufb01cation head takes as input some of the top layer embeddings of\nthe transformer and produces as output a classi\ufb01cation. In this method, most com-\nmonly used with masked language models like BERT, we freeze the entire pretrained\nmodel and only train the classi\ufb01cation head on some new data, usually labeled with\nsome class that we want to predict.\nFinally, in Chapter 12 we\u2019ll introduce a fourth kind of \ufb01netuning, that is a cru-\ncial component of the largest language models: supervised \ufb01netuning orSFT. SFT\nis often used for instruction \ufb01netuning , in which we want a pretrained language\nmodel to learn to follow text instructions, for example to answer questions or follow\na command to write something. Here we create a dataset of prompts and desired\nresponses (for example questions and their answers, or commands and their ful-\n\ufb01llments), and we train the language model using the normal cross-entropy loss to\npredict each token in the instruction prompt iteratively, essentially training it to pro-\nduce the desired response from the command in the prompt. It\u2019s called supervised\nbecause unlike in pretraining, where we just take any data and predict the words in\nit, we build the special \ufb01netuning dataset by hand, creating supervised responses to\neach command.\nOften everything that happens after pretraining is lumped together as post-training ;\nwe\u2019ll discuss the various parts of post-training in Chapter 12.\n10.4 Evaluating Large Language Models\nPerplexity As we \ufb01rst saw in Chapter 3, one way to evaluate language models is\nto measure how well they predict unseen text. Intuitively, good models are those that\nassign higher probabilities to unseen data (are less surprised when encountering the\nnew words).\nWe instantiate this intuition by using perplexity to measure the quality of a perplexity\nlanguage model. Recall from page 40 that the perplexity of a model qon an unseen\ntest set is the inverse probability that qassigns to the test set, normalized by the test\nset length. For a test set of ntokens w1:n, the perplexity is\nPerplexityq(w1:n) = Pq(w1:n)\u00001\nn\n=ns\n1\nPq(w1:n)(10.7)\nTo visualize how perplexity can be computed as a function of the probabilities the",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 328,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 223\n\n10.4 \u2022 E VALUATING LARGE LANGUAGE MODELS 215\nLM computes for each new word, we can use the chain rule to expand the computa-\ntion of probability of the test set:\nPerplexityq(w1:n) =nvuutnY\ni=11\nPq(wijw<i)(10.8)\nNote that because of the inverse in Eq. 10.7, the higher the probability of the word\nsequence, the lower the perplexity. Thus the the lower the perplexity of a model on\nthe data, the better the model . Minimizing perplexity is equivalent to maximizing\nthe test set probability according to the language model.\nOne caveat: because perplexity depends on the length of a text, it is very sensitive\nto differences in the tokenization algorithm. That means that it\u2019s hard to exactly\ncompare perplexities produced by two language models if they have very different\ntokenizers. For this reason perplexity is best used when comparing language models\nthat use the same tokenizer.\nOther factors While the predictive accuracy of a language model, as measured by\nperplexity, is a very useful metric, we also care about different kinds of accuracy, for\nthe downstream tasks we apply our language model to. For each task like machine\ntranslation, summarization, question answering, speech recognition, and dialogue,\nwe can measure the accuracy at those tasks. Future chapters will introduce task-\nspeci\ufb01c metrics that allow us to evaluate how accuracy or correct language models\nare at these downstream tasks.\nBut when evaluating models we also care about factors besides any of these\nkinds of accuracy (Dodge et al., 2019; Ethayarajh and Jurafsky, 2020). For example,\nwe often care about how a big a model is, and how long it takes to train or do\ninference. This can matter because we have constraints on time either for training\nor at inference. Or we may have constraints on memory, since the GPUs we run\nour models on have \ufb01xed memory sizes. Big models also use more energy, and we\nprefer models that use less energy, both to reduce the environmental impact of the\nmodel and to reduce the \ufb01nancial cost of building or deploying it. We can target\nour evaluation to these factors by measuring performance normalized to a giving\ncompute or memory budget. We can also directly measure the energy usage of our\nmodel in kWh or in kilograms of CO 2emitted (Strubell et al., 2019; Henderson\net al., 2020; Liang et al., 2023).\nAnother feature that a language model evaluation can measure is fairness. We\nknow that language models are biased, exhibiting gendered and racial stereotypes,\nor decreased performance for language from or about certain demographics groups.\nThere are language model evaluation benchmarks that measure the strength of these\nbiases, such as StereoSet (Nadeem et al., 2021), RealToxicityPrompts (Gehman\net al., 2020), and BBQ (Parrish et al., 2022) among many others. We also want\nlanguage models whose performance is equally fair to different groups. For exam-\nple, we could chose an evaluation that is fair in a Rawlsian sense by maximizing the\nwelfare of the worst-off group (Rawls, 2001; Hashimoto et al., 2018; Sagawa et al.,\n2020).\nFinally, there are many kinds of leaderboards like Dynabench (Kiela et al., 2021)\nand general evaluation protocols like HELM (Liang et al., 2023); we will return to\nthese in later chapters when we introduce evaluation metrics for speci\ufb01c tasks like\nquestion answering and information retrieval.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 329,
      "token_count": 794,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 224\n\n216 CHAPTER 10 \u2022 L ARGE LANGUAGE MODELS\n10.5 Dealing with Scale\nLarge language models are large. For example the Llama 3.1 405B Instruct model\nfrom Meta has 405 billion parameters ( L=126 layers, a model dimensionality of\nd=16,384, A=128 attention heads) and was trained on 15.6 terabytes of text tokens\n(Llama Team, 2024), using a vocabulary of 128K tokens. So there is a lot of research\non understanding how LLMs scale, and especially how to implement them given\nlimited resources. In the next few sections we discuss how to think about scale (the\nconcept of scaling laws ), and important techniques for getting language models to\nwork ef\ufb01ciently, such as the KV cache and parameter-ef\ufb01cient \ufb01ne tuning.\n10.5.1 Scaling laws\nThe performance of large language models has shown to be mainly determined by\n3 factors: model size (the number of parameters not counting embeddings), dataset\nsize (the amount of training data), and the amount of compute used for training. That\nis, we can improve a model by adding parameters (adding more layers or having\nwider contexts or both), by training on more data, or by training for more iterations.\nThe relationships between these factors and performance are known as scaling\nlaws . Roughly speaking, the performance of a large language model (the loss) scales scaling laws\nas a power-law with each of these three properties of model training.\nFor example, Kaplan et al. (2020) found the following three relationships for\nlossLas a function of the number of non-embedding parameters N, the dataset size\nD, and the compute budget C, for models training with limited parameters, dataset,\nor compute budget, if in each case the other two properties are held constant:\nL(N) =\u0012Nc\nN\u0013aN\n(10.9)\nL(D) =\u0012Dc\nD\u0013aD\n(10.10)\nL(C) =\u0012Cc\nC\u0013aC\n(10.11)\nThe number of (non-embedding) parameters Ncan be roughly computed as fol-\nlows (ignoring biases, and with das the input and output dimensionality of the\nmodel, dattnas the self-attention layer size, and dffthe size of the feedforward layer):\nN\u00192d nlayer(2dattn+dff)\n\u001912nlayerd2(10.12)\n(assuming dattn=dff=4=d)\nThus GPT-3, with n=96 layers and dimensionality d=12288, has 12\u000296\u0002\n122882\u0019175 billion parameters.\nThe values of Nc,Dc,Cc,aN,aD, and aCdepend on the exact transformer\narchitecture, tokenization, and vocabulary size, so rather than all the precise values,\nscaling laws focus on the relationship with loss.2\nScaling laws can be useful in deciding how to train a model to a particular per-\nformance, for example by looking at early in the training curve, or performance with\n2For the initial experiment in Kaplan et al. (2020) the precise values were aN= 0.076, Nc= 8.8\u00021013\n(parameters), aD= 0.095, Dc= 5.4\u00021013(tokens), aC= 0.050, Cc= 3.1\u0002108(peta\ufb02op-days).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 330,
      "token_count": 745,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 225\n\n10.5 \u2022 D EALING WITH SCALE 217\nsmaller amounts of data, to predict what the loss would be if we were to add more\ndata or increase model size. Other aspects of scaling laws can also tell us how much\ndata we need to add when scaling up a model.\n10.5.2 KV Cache\nWe saw in Fig. 9.10 and in Eq. 9.33 (repeated below) how the attention vector can\nbe very ef\ufb01ciently computed in parallel for training, via two matrix multiplications:\nA=softmax\u0012QK|\npdk\u0013\nV (10.13)\nUnfortunately we can\u2019t do quite the same ef\ufb01cient computation in inference as\nin training. That\u2019s because at inference time, we iteratively generate the next tokens\none at a time. For a new token that we have just generated, call it xi, we need to\ncompute its query, key, and values by multiplying by WQ,WK, and WVrespec-\ntively. But it would be a waste of computation time to recompute the key and value\nvectors for all the prior tokens x<i; at prior steps we already computed these key\nand value vectors! So instead of recomputing these, whenever we compute the key\nand value vectors we store them in memory in the KV cache , and then we can just KV cache\ngrab them from the cache when we need them. Fig. 10.7 modi\ufb01es Fig. 9.10 to show\nthe computation that takes place for a single new token, showing which values we\ncan take from the cache rather than recompute.\nq4k1k2k4QKTQKTv1v2v3v4V\nq4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4x==xa4A\n1 x dkdk x N1 x NN x dv1 x dv\nk3\nFigure 10.7 Parts of the attention computation (extracted from Fig. 9.10) showing, in black,\nthe vectors that can be stored in the cache rather than recomputed when computing the atten-\ntion score for the 4th token.\n10.5.3 Parameter Ef\ufb01cient Fine Tuning\nAs we mentioned above, it\u2019s very common to take a language model and give it more\ninformation about a new domain by \ufb01netuning it (continuing to train it to predict\nupcoming words) on some additional data.\nFine-tuning can be very dif\ufb01cult with very large language models, because there\nare enormous numbers of parameters to train; each pass of batch gradient descent\nhas to backpropagate through many many huge layers. This makes \ufb01netuning huge\nlanguage models extremely expensive in processing power, in memory, and in time.\nFor this reason, there are alternative methods that allow a model to be \ufb01netuned\nwithout changing all the parameters. Such methods are called parameter-ef\ufb01cient\n\ufb01ne tuning or sometimes PEFT , because we ef\ufb01ciently select a subset of parametersparameter-\nef\ufb01cient \ufb01ne\ntuning\nPEFT to update when \ufb01netuning. For example we freeze some of the parameters (don\u2019t\nchange them), and only update some particular subset of parameters.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 331,
      "token_count": 706,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 226\n\n218 CHAPTER 10 \u2022 L ARGE LANGUAGE MODELS\nHere we describe one such model, called LoRA , forLow-RankAdaptation. The LoRA\nintuition of LoRA is that transformers have many dense layers which perform matrix\nmultiplication (for example the WQ,WK,WV,WOlayers in the attention computa-\ntion). Instead of updating these layers during \ufb01netuning, with LoRA we freeze these\nlayers and instead update a low-rank approximation that has fewer parameters.\nConsider a matrix Wof dimensionality [N\u0002d]that needs to be updated during\n\ufb01netuning via gradient descent. Normally this matrix would get updates DWof\ndimensionality [N\u0002d], for updating the N\u0002dparameters after gradient descent. In\nLoRA, we freeze Wand update instead a low-rank decomposition of W. We create\ntwo matrices AandB, where Ahas size [N\u0002r]andBhas size [r\u0002d], and we choose\nrto be quite small, r<<min(d;N). During \ufb01netuning we update AandBinstead\nofW. That is, we replace W+DWwithW+BA. Fig. 10.8 shows the intuition.\nFor replacing the forward pass h=xW, the new forward pass is instead:\nh=xW+xAB (10.14)\nhPretrained WeightsWdkrkABrxd11k\nd\u00d7\nFigure 10.8 The intuition of LoRA. We freeze Wto its pretrained values, and instead \ufb01ne-\ntune by training a pair of matrices AandB, updating those instead of W, and just sum Wand\nthe updated AB.\nLoRA has a number of advantages. It dramatically reduces hardware require-\nments, since gradients don\u2019t have to be calculated for most parameters. The weight\nupdates can be simply added in to the pretrained weights, since BAis the same size\nasW). That means it doesn\u2019t add any time during inference. And it also means it\u2019s\npossible to build LoRA modules for different domains and just swap them in and\nout by adding them in or subtracting them from W.\nIn its original version LoRA was applied just to the matrices in the attention\ncomputation (the WQ,WK,WV, and WOlayers). Many variants of LoRA exist.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 332,
      "token_count": 497,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 227\n\n10.6 \u2022 P OTENTIAL HARMS FROM LANGUAGE MODELS 219\n10.6 Potential Harms from Language Models\nLarge pretrained neural language models exhibit many of the potential harms dis-\ncussed in Chapter 4 and Chapter 6. Many of these harms become realized when\npretrained language models are used for any downstream task, particularly those\ninvolving text generation, whether question answering, machine translation, or in\nassistive technologies like writing aids or web search query completion, or predic-\ntive typing for email (Olteanu et al., 2020).\nFor example, language models are prone to saying things that are false, a prob-\nlem called hallucination . Language models are trained to generate text that is pre- hallucination\ndictable and coherent, but the training algorithms we have seen so far don\u2019t have\nany way to enforce that the text that is generated is correct or true. This causes\nenormous problems for any application where the facts matter! We\u2019ll return to this\nissue in Chapter 14 where we introduce proposed mitigation methods like retrieval\naugmented generation .\nA second source of harm is that language models can generate toxic language . toxic language\nGehman et al. (2020) show that even completely non-toxic prompts can lead large\nlanguage models to output hate speech and abuse their users. Language models also\ngenerate stereotypes (Cheng et al., 2023) and negative attitudes (Brown et al., 2020;\nSheng et al., 2019) about many demographic groups.\nOne source of biases is the training data. Gehman et al. (2020) shows that large\nlanguage model training datasets include toxic text scraped from banned sites. There\nare other biases than toxicity: the training data is disproportionately generated by\nauthors from the US and from developed countries. Such biased population samples\nlikely skew the resulting generation toward the perspectives or topics of this group\nalone. Furthermore, language models can amplify demographic and other biases in\ntraining data, just as we saw for embedding models in Chapter 6.\nDatasets can be another source of harms. We already saw in Section 10.3.2\nthat using pretraining corpora scraped from the web can lead to harms related to\ncopyright and data consent. We also mentioned that pretraining data can tend to\nhave private information like phone numbers and addresses. This is problematic\nbecause large language models can leak information from their training data. That\nis, an adversary can extract training-data text from a language model such as a per-\nson\u2019s name, phone number, and address (Henderson et al. 2017, Carlini et al. 2021).\nThis becomes even more problematic when large language models are trained on\nextremely sensitive private datasets such as electronic health records.\nLanguage models can also be used by malicious actors for generating text for\nmisinformation , phishing, or other socially harmful activities (Brown et al., 2020).\nMcGuf\ufb01e and Newhouse (2020) show how large language models generate text that\nemulates online extremists, with the risk of amplifying extremist movements and\ntheir attempt to radicalize and recruit.\nFinding ways to mitigate all these harms is an important current research area in\nNLP. At the very least, carefully analyzing the data used to pretrain large language\nmodels is important as a way of understanding issues of toxicity, bias, privacy, and\nfair use, making it extremely important that language models include datasheets\n(page 16) or model cards (page 74) giving full replicable information on the cor-\npora used to train them. Open-source models can specify their exact training data.\nRequirements that models are transparent in such ways is also in the process of being\nincorporated into the regulations of various national governments.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 333,
      "token_count": 793,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 228\n\n220 CHAPTER 10 \u2022 L ARGE LANGUAGE MODELS\n10.7 Summary\nThis chapter has introduced the large language model, and how it can be built out of\nthe transformer. Here\u2019s a summary of the main points that we covered:\n\u2022 Many NLP tasks\u2014such as question answering, summarization, sentiment,\nand machine translation\u2014can be cast as tasks of word prediction and hence\naddressed with Large language models.\n\u2022 Large language models are generally pretrained on large datasets of 100s of\nbillions of words generally scraped from the web.\n\u2022 These datasets need to be \ufb01ltered for quality and balanced for domains by\nupsampling and downsampling. Addressing some problems with pretraining\ndata, like toxicity, are open research problems.\n\u2022 The choice of which word to generate in large language models is generally\ndone by using a sampling algorithm.\n\u2022 Language models are evaluated by perplexity but there are also evaluations\nof accuracy downstream tasks, and ways to measure other factors like fairness\nand energy use.\n\u2022 There are various computational tricks for making large language models\nmore ef\ufb01cient, such as the KV cache andparameter-ef\ufb01cient \ufb01netuning .\n\u2022 Because of their ability to be used in so many ways, language models also\nhave the potential to cause harms. Some harms include hallucinations, bias,\nstereotypes, misinformation and propaganda, and violations of privacy and\ncopyright.\nBibliographical and Historical Notes\nAs we discussed in Chapter 3, the earliest language models were the n-gram lan-\nguage models developed (roughly simultaneously and independently) by Fred Je-\nlinek and colleagues at the IBM Thomas J. Watson Research Center, and James\nBaker at CMU. It was the Jelinek and the IBM team who \ufb01rst coined the term lan-\nguage model to mean a model of the way any kind of linguistic property (grammar,\nsemantics, discourse, speaker characteristics), in\ufb02uenced word sequence probabil-\nities (Jelinek et al., 1975). They contrasted the language model with the acoustic\nmodel which captured acoustic/phonetic characteristics of phone sequences.\nN-gram language models were very widely used over the next 30 years and more,\nacross a wide variety of NLP tasks like speech recognition and machine translations,\noften as one of multiple components of the model. The contexts for these n-gram\nmodels grew longer, with 5-gram models used quite commonly by very ef\ufb01cient LM\ntoolkits (Stolcke, 2002; Hea\ufb01eld, 2011).\nThe roots of the neural language model lie in multiple places. One was the\napplication in the 1990s, again in Jelinek\u2019s group at IBM Research, of discrimi-\nnative classi\ufb01ers to language models. Roni Rosenfeld in his dissertation (Rosen-\nfeld, 1992) \ufb01rst applied logistic regression (under the name maximum entropy or\nmaxent models) to language modeling in that IBM lab, and published a more fully\nformed version in Rosenfeld (1996). His model integrated various sorts of infor-\nmation in a logistic regression predictor, including n-gram information along with",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 334,
      "token_count": 676,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 229",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 335,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 221\nother features from the context, including distant n-grams and pairs of associated\nwords called trigger pairs . Rosenfeld\u2019s model pre\ufb01gured modern language models\nby being a statistical word predictor trained in a self-supervised manner simply by\nlearning to predict upcoming words in a corpus.\nAnother was the \ufb01rst use of pretrained embeddings to model word meaning in the\nLSA/LSI models (Deerwester et al., 1988). Recall from the history section of Chap-\nter 6 that in LSA (latent semantic analysis) a term-document matrix was trained on a\ncorpus and then singular value decomposition was applied and the \ufb01rst 300 dimen-\nsions were used as a vector embedding to represent words. Landauer et al. (1997)\n\ufb01rst used the word \u201cembedding\u201d. In addition to their development of the idea of pre-\ntraining and of embeddings, the LSA community also developed ways to combine\nLSA embeddings with n-grams in an integrated language model (Bellegarda, 1997;\nCoccaro and Jurafsky, 1998).\nIn a very in\ufb02uential series of papers developing the idea of neural language\nmodels , (Bengio et al. 2000; Bengio et al. 2003; Bengio et al. 2006), Yoshua Ben-\ngio and colleagues drew on the central ideas of both these lines of self-supervised\nlanguage modeling work, (the discriminatively trained word predictor, and the pre-\ntrained embeddings). Like the maxent models of Rosenfeld, Bengio\u2019s model used\nthe next word in running text as its supervision signal. Like the LSA models, Ben-\ngio\u2019s model learned an embedding, but unlike the LSA models did it as part of the\nprocess of language modeling. The Bengio et al. (2003) model was a neural lan-\nguage model: a neural network that learned to predict the next word from prior\nwords, and did so via learning embeddings as part of the prediction process.\nThe neural language model was extended in various ways over the years, perhaps\nmost importantly in the form of the RNN language model of Mikolov et al. (2010)\nand Mikolov et al. (2011). The RNN language model was perhaps the \ufb01rst neural\nmodel that was accurate enough to surpass the performance of a traditional 5-gram\nlanguage model.\nSoon afterwards, Mikolov et al. (2013a) and Mikolov et al. (2013b) proposed to\nsimplify the hidden layer of these neural net language models to create pretrained\nword2vec word embeddings.\nThe static embedding models like LSA and word2vec instantiated a particular\nmodel of pretraining: a representation was trained on a pretraining dataset, and then\nthe representations could be used in further tasks. \u2018Dai and Le (2015) and (Peters\net al., 2018) reframed this idea by proposing models that were pretrained using a\nlanguage model objective, and then the identical model could be either frozen and\ndirectly applied for language modeling or further \ufb01netuned still using a language\nmodel objective. For example ELMo used a biLSTM self-supervised on a large\npretrained dataset using a language model objective, then \ufb01netuned on a domain-\nspeci\ufb01c dataset, and then froze the weights and added task-speci\ufb01c heads. The\nELMo work was particularly in\ufb02uential and its appearance was perhaps the mo-\nment when it became clear to the community that language models could be used as",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 336,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "model that was accurate enough to surpass the performance of a traditional 5-gram\nlanguage model.\nSoon afterwards, Mikolov et al. (2013a) and Mikolov et al. (2013b) proposed to\nsimplify the hidden layer of these neural net language models to create pretrained\nword2vec word embeddings.\nThe static embedding models like LSA and word2vec instantiated a particular\nmodel of pretraining: a representation was trained on a pretraining dataset, and then\nthe representations could be used in further tasks. \u2018Dai and Le (2015) and (Peters\net al., 2018) reframed this idea by proposing models that were pretrained using a\nlanguage model objective, and then the identical model could be either frozen and\ndirectly applied for language modeling or further \ufb01netuned still using a language\nmodel objective. For example ELMo used a biLSTM self-supervised on a large\npretrained dataset using a language model objective, then \ufb01netuned on a domain-\nspeci\ufb01c dataset, and then froze the weights and added task-speci\ufb01c heads. The\nELMo work was particularly in\ufb02uential and its appearance was perhaps the mo-\nment when it became clear to the community that language models could be used as\na general solution for NLP problems.\nTransformers were \ufb01rst applied as encoder-decoders (Vaswani et al., 2017) and\nthen to masked language modeling (Devlin et al., 2019) (as we\u2019ll see in Chapter 13\nand Chapter 11). Radford et al. (2019) then showed that the transformer-based au-\ntoregressive language model GPT2 could perform zero-shot on many NLP tasks like\nsummarization and question answering.\nThe technology used for transformer-based language models can also be applied\nto other domains and tasks, like vision, speech, and genetics. the term foundation",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 337,
      "token_count": 412,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 230\n\n222 CHAPTER 10 \u2022 L ARGE LANGUAGE MODELS\nmodel is sometimes used as a more general term for this use of large languagefoundation\nmodel\nmodel technology across domains and areas, when the elements we are computing\nover are not necessarily words. Bommasani et al. (2021) is a broad survey that\nsketches the opportunities and risks of foundation models, with special attention to\nlarge language models.\n\n## Page 231\n\nCHAPTER\n11Masked Language Models\nLarvatus prodeo [Masked, I go forward]\nDescartes\nIn the previous two chapters we introduced the transformer and saw how to pre-\ntrain a transformer language model as a causal or left-to-right language model. In\nthis chapter we\u2019ll introduce a second paradigm for pretrained language models, the\nbidirectional transformer encoder, and the most widely-used version, the BERT BERT\nmodel (Devlin et al., 2019). This model is trained via masked language modeling ,masked\nlanguage\nmodelingwhere instead of predicting the following word, we mask a word in the middle and\nask the model to guess the word given the words on both sides. This method thus\nallows the model to see both the right and left context.\nWe also introduced \ufb01netuning in the prior chapter. Here we describe a new \ufb01netuning\nkind of \ufb01netuning, in which we take the transformer network learned by these pre-\ntrained models, add a neural net classi\ufb01er after the top layer of the network, and train\nit on some additional labeled data to perform some downstream task like named\nentity tagging or natural language inference. As before, the intuition is that the\npretraining phase learns a language model that instantiates rich representations of\nword meaning, that thus enables the model to more easily learn (\u2018be \ufb01netuned to\u2019)\nthe requirements of a downstream language understanding task. This aspect of the\npretrain-\ufb01netune paradigm is an instance of what is called transfer learning in ma-transfer\nlearning\nchine learning: the method of acquiring knowledge from one task or domain, and\nthen applying it (transferring it) to solve a new task.\nThe second idea that we introduce in this chapter is the idea of contextual em-\nbeddings : representations for words in context. The methods of Chapter 6 like\nword2vec or GloVe learned a single vector embedding for each unique word win\nthe vocabulary. By contrast, with contextual embeddings, such as those learned by\nmasked language models like BERT, each word wwill be represented by a different\nvector each time it appears in a different context. While the causal language models\nof Chapter 9 also use contextual embeddings, the embeddings created by masked\nlanguage models seem to function particularly well as representations.\n11.1 Bidirectional Transformer Encoders\nLet\u2019s begin by introducing the bidirectional transformer encoder that underlies mod-\nels like BERT and its descendants like RoBERTa (Liu et al., 2019) or SpanBERT\n(Joshi et al., 2020). In Chapter 9 we introduced causal (left-to-right) transformers\nand in Chapter 10 saw how they can serve as the basis for language models that can\nbe applied to autoregressive contextual generation problems like question answering\nor summarization. But this left-to-right nature of these models is also a limitation,\nbecause there are tasks for which it would be useful, when processing a token, to\nbe able to peak at future tokens. This is especially true for sequence labeling tasks",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 338,
      "token_count": 739,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 232\n\n224 CHAPTER 11 \u2022 M ASKED LANGUAGE MODELS\nin which we want to tag each token with a label, such as the named entity tagging\ntask we\u2019ll introduce in Section 11.5, or tasks like part-of-speech tagging or parsing\nthat come up in later chapters.\nThebidirectional encoders that we introduce here are a different kind of beast\nthan causal models. The causal models of Chapter 9 are generative models, de-\nsigned to easily generate the next token in a sequence. But the focus of bidirec-\ntional encoders is instead on computing contextualized representations of the input\ntokens. Bidirectional encoders use self-attention to map sequences of input embed-\ndings (x1;:::;xn)to sequences of output embeddings of the same length (h1;:::;hn),\nwhere the output vectors have been contextualized using information from the en-\ntire input sequence. These output embeddings are contextualized representations of\neach input token that are useful across a range of applications where we need to do\na classi\ufb01cation or a decision based on the token in context.\nRemember that we said the models of Chapter 9 are sometimes called decoder-\nonly, because they correspond to the decoder part of the encoder-decoder model we\nwill introduce in Chapter 13. By contrast, the masked language models of this chap-\nter are sometimes called encoder-only , because they produce an encoding for each\ninput token but generally aren\u2019t used to produce running text by decoding/sampling.\nThat\u2019s an important point: masked language models are not used for generation.\nThey are generally instead used for interpretative tasks.\n11.1.1 The architecture for bidirectional masked models\nLet\u2019s \ufb01rst discuss the overall architecture. Bidirectional transformer-based language\nmodels differ in two ways from the causal transformers in the previous chapters. The\n\ufb01rst is that the attention function isn\u2019t causal; the attention for a token ican look at\nfollowing tokens i+1 and so on. The second is that the training is slightly different\nsince we are predicting something in the middle of our text rather than at the end.\nWe\u2019ll discuss the \ufb01rst here and the second in the following section.\nFig. 11.1a, reproduced here from Chapter 9, shows the information \ufb02ow in the\nleft-to-right approach of Chapter 9. The attention computation at each token is based\non the preceding (and current) input tokens, ignoring potentially useful information\nlocated to the right of the token under consideration. Bidirectional encoders over-\ncome this limitation by allowing the attention mechanism to range over the entire\ninput, as shown in Fig. 11.1b.\na) A causal self-attention layerb) A bidirectional self-attention layer\nattentionattentionattentionattentionattentiona1a2a3a4a5x3x4x5x1x2\nattentionattentionattentionattentionattentiona1a2a3a4a5x3x4x5x1x2\nFigure 11.1 (a) The causal transformer from Chapter 9, highlighting the attention computation at token 3. The\nattention value at each token is computed using only information seen earlier in the context. (b) Information\n\ufb02ow in a bidirectional attention model. In processing each token, the model attends to all inputs, both before\nand after the current one. So attention for token 3 can draw on information from following tokens.\nThe implementation is very simple! We simply remove the attention masking\nstep that we introduced in Eq. 9.33. Recall from Chapter 9 that we had to mask the\nQK|matrix for causal transformers so that attention couldn\u2019t look at future tokens",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 339,
      "token_count": 783,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 233\n\n11.1 \u2022 B IDIRECTIONAL TRANSFORMER ENCODERS 225\n(repeated from Eq. 9.33 for a single attention head):\nhead =softmax\u0012\nmask\u0012QK|\npdk\u0013\u0013\nV\n(11.1)\nq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NN\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\nq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NNq1\u2022k2q1\u2022k3q1\u2022k4q2\u2022k3q2\u2022k4q3\u2022k4\n(a) (b)\nFigure 11.2 The N\u0002NQK|matrix showing the qi\u0001kjvalues, with the upper-triangle\nportion of the comparisons matrix zeroed out (set to \u0000\u00a5, which the softmax will turn to\nzero).\nFig. 11.2 shows the masked version of QK|and the unmasked version. For bidi-\nrectional attention, we use the unmasked version of Fig. 11.2b. Thus the attention\ncomputation for bidirectional attention is exactly the same as Eq. 11.1 but with the\nmask removed:\nhead =softmax\u0012QK|\npdk\u0013\nV (11.2)\nOtherwise, the attention computation is identical to what we saw in Chapter 9, as is\nthe transformer block architecture (the feedforward layer, layer norm, and so on). As\nin Chapter 9, the input is also a series of subword tokens, usually computed by one of\nthe 3 popular tokenization algorithms (including the BPE algorithm that we already\nsaw in Chapter 2 and two others, the WordPiece algorithm and the SentencePiece\nUnigram LM algorithm). That means every input sentence \ufb01rst has to be tokenized,\nand all further processing takes place on subword tokens rather than words. This will\nrequire, as we\u2019ll see in the third part of the textbook, that for some NLP tasks that\nrequire notions of words (like parsing) we will occasionally need to map subwords\nback to words.\nTo make this more concrete, the original English-only bidirectional transformer\nencoder model, BERT (Devlin et al., 2019), consisted of the following:\n\u2022 An English-only subword vocabulary consisting of 30,000 tokens generated\nusing the WordPiece algorithm (Schuster and Nakajima, 2012).\n\u2022 Input context window N=512 tokens, and model dimensionality d=768\n\u2022 SoX, the input to the model, is of shape [N\u0002d]=[512\u0002768].\n\u2022L=12 layers of transformer blocks, each with A=12 (bidirectional) multihead\nattention layers.\n\u2022 The resulting model has about 100M parameters.\nThe larger multilingual XLM-RoBERTa model, trained on 100 languages, has\n\u2022 A multilingual subword vocabulary with 250,000 tokens generated using the\nSentencePiece Unigram LM algorithm (Kudo and Richardson, 2018b).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 340,
      "token_count": 723,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 234\n\n226 CHAPTER 11 \u2022 M ASKED LANGUAGE MODELS\n\u2022 Input context window N=512 tokens, and model dimensionality d=1024, hence\nX, the input to the model, is of shape [N\u0002d]=[512\u00021024].\n\u2022L=24 layers of transformer blocks, with A=16 multihead attention layers each\n\u2022 The resulting model has about 550M parameters.\nNote that 550M parameters is relatively small as large language models go\n(Llama 3 has 405B parameters, so is 3 orders of magnitude bigger). Indeed, masked\nlanguage models tend to be much smaller than causal language models.\n11.2 Training Bidirectional Encoders\nWe trained causal transformer language models in Chapter 9 by making them it-\neratively predict the next word in a text. But eliminating the causal mask in at-\ntention makes the guess-the-next-word language modeling task trivial\u2014the answer\nis directly available from the context\u2014so we\u2019re in need of a new training scheme.\nInstead of trying to predict the next word, the model learns to perform a \ufb01ll-in-the-\nblank task, technically called the cloze task (Taylor, 1953). To see this, let\u2019s return cloze task\nto the motivating example from Chapter 3. Instead of predicting which words are\nlikely to come next in this example:\nThe water of Walden Pond is so beautifully\nwe\u2019re asked to predict a missing item given the rest of the sentence.\nThe of Walden Pond is so beautifully ...\nThat is, given an input sequence with one or more elements missing, the learning\ntask is to predict the missing elements. More precisely, during training the model is\ndeprived of one or more tokens of an input sequence and must generate a probability\ndistribution over the vocabulary for each of the missing items. We then use the cross-\nentropy loss from each of the model\u2019s predictions to drive the learning process.\nThis approach can be generalized to any of a variety of methods that corrupt the\ntraining input and then asks the model to recover the original input. Examples of the\nkinds of manipulations that have been used include masks, substitutions, reorder-\nings, deletions, and extraneous insertions into the training text. The general name\nfor this kind of training is called denoising : we corrupt (add noise to) the input in denoising\nsome way (by masking a word, or putting in an incorrect word) and the goal of the\nsystem is to remove the noise.\n11.2.1 Masking Words\nLet\u2019s describe the Masked Language Modeling (MLM) approach to training bidi-Masked\nLanguage\nModelingrectional encoders (Devlin et al., 2019). As with the language model training meth-\nods we\u2019ve already seen, MLM uses unannotated text from a large corpus. In MLM\ntraining, the model is presented with a series of sentences from the training corpus\nin which a percentage of tokens (15% in the BERT model) have been randomly cho-\nsen to be manipulated by the masking procedure. Given an input sentence lunch\nwas delicious and assume we randomly chose the 3rd token delicious to be\nmanipulated,\n\u2022 80% of the time: The token is replaced with the special vocabulary token\nnamed[MASK] , e.g.lunch was delicious !lunch was [MASK] .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 341,
      "token_count": 708,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 235\n\n11.2 \u2022 T RAINING BIDIRECTIONAL ENCODERS 227\n\u2022 10% of the time: The token is replaced with another token, randomly sampled\nfrom the vocabulary based on token unigram probabilities. e.g. lunch was\ndelicious!lunch was gasp .\n\u2022 10% of the time: the token is left unchanged. e.g. lunch was delicious\n!lunch was delicious .\nWe then train the model to guess the correct token for the manipulated tokens. Why\nthe three possible manipulations? Adding the [MASK] token creates a mismatch be-\ntween pretraining and downstream \ufb01ne-tuning or inference, since when we employ\nthe MLM model to perform a downstream task, we don\u2019t use any [MASK] tokens. If\nwe just replaced tokens with the [MASK] , the model might only predict tokens when\nit sees a[MASK] , but we want the model to try to always predict the input token.\nTo train the model to make the prediction, the original input sequence is to-\nkenized using a subword model and tokens are sampled to be manipulated. Word\nembeddings for all of the tokens in the input are retrieved from the Eembedding ma-\ntrix and combined with positional embeddings to form the input to the transformer,\npassed through the stack of bidirectional transformer blocks, and then the language\nmodeling head. The MLM training objective is to predict the original inputs for\neach of the masked tokens and the cross-entropy loss from these predictions drives\nthe training process for all the parameters in the model. That is, all of the input\ntokens play a role in the self-attention process, but only the sampled tokens are used\nfor learning.\nLM Head with Softmax over Vocabulary\nSo[mask]and[mask]for longthanksCE Loss\nall apricot \ufb01shthe\nToken +Positional EmbeddingsSolongandthanksfor all \ufb01shthe\nBidirectional Transformer Encoder+p1+++++++p2p3p4p5p6p7p8z1z2z3z4z5z6z7z8\nFigure 11.3 Masked language model training. In this example, three of the input tokens are selected, two of\nwhich are masked and the third is replaced with an unrelated word. The probabilities assigned by the model to\nthese three items are used as the training loss. The other 5 tokens don\u2019t play a role in training loss.\nFig. 11.3 illustrates this approach with a simple example. Here, long,thanks and\nthehave been sampled from the training sequence, with the \ufb01rst two masked and the\nreplaced with the randomly sampled token apricot . The resulting embeddings are\npassed through a stack of bidirectional transformer blocks. Recall from Section 9.5\nin Chapter 9 that to produce a probability distribution over the vocabulary for each\nof the masked tokens, the language modeling head takes the output vector hL\nifrom\nthe \ufb01nal transformer layer Lfor each masked token i, multiplies it by the unembed-\nding layer ETto produce the logits u, and then uses softmax to turn the logits into",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 342,
      "token_count": 663,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 236\n\n228 CHAPTER 11 \u2022 M ASKED LANGUAGE MODELS\nprobabilities yover the vocabulary:\nui=hL\niET(11.3)\nyi=softmax (ui) (11.4)\nWith a predicted probability distribution for each masked item, we can use cross-\nentropy to compute the loss for each masked item\u2014the negative log probability\nassigned to the actual masked word, as shown in Fig. 11.3. More formally, for a\ngiven vector of input tokens in a sentence or batch be x, let the set of tokens that are\nmasked be M, the version of that sentence with some tokens replaced by masks be\nxmask, and the sequence of output vectors be h. For a given input token xi, such as\nthe word long in Fig. 11.3, the loss is the probability of the correct word long, given\nxmask(as summarized in the single output vector hL\ni):\nLMLM(xi) =\u0000logP(xijhL\ni)\nThe gradients that form the basis for the weight updates are based on the average\nloss over the sampled learning items from a single training sequence (or batch of\nsequences).\nLMLM=\u00001\njMjX\ni2MlogP(xijhL\ni)\nNote that only the tokens in Mplay a role in learning; the other words play no role\nin the loss function, so in that sense BERT and its descendents are inef\ufb01cient; only\n15% of the input samples in the training data are actually used for training weights.1\n11.2.2 Next Sentence Prediction\nThe focus of mask-based learning is on predicting words from surrounding contexts\nwith the goal of producing effective word-level representations. However, an im-\nportant class of applications involves determining the relationship between pairs of\nsentences. These include tasks like paraphrase detection (detecting if two sentences\nhave similar meanings), entailment (detecting if the meanings of two sentences en-\ntail or contradict each other) or discourse coherence (deciding if two neighboring\nsentences form a coherent discourse).\nTo capture the kind of knowledge required for applications such as these, some\nmodels in the BERT family include a second learning objective called Next Sen-\ntence Prediction (NSP). In this task, the model is presented with pairs of sentencesNext Sentence\nPrediction\nand is asked to predict whether each pair consists of an actual pair of adjacent sen-\ntences from the training corpus or a pair of unrelated sentences. In BERT, 50% of\nthe training pairs consisted of positive pairs, and in the other 50% the second sen-\ntence of a pair was randomly selected from elsewhere in the corpus. The NSP loss\nis based on how well the model can distinguish true pairs from random pairs.\nTo facilitate NSP training, BERT introduces two special tokens to the input rep-\nresentation (tokens that will prove useful for \ufb01netuning as well). After tokenizing\nthe input with the subword model, the token [CLS] is prepended to the input sen-\ntence pair, and the token [SEP] is placed between the sentences and after the \ufb01nal\ntoken of the second sentence. There are actually two more special tokens, a \u2018First\nSegment\u2019 token, and a \u2018Second Segment\u2019 token. These tokens are added in the in-\nput stage to the word and positional embeddings. That is, each token of the input\n1ELECTRA, another BERT family member, does use all examples for training (Clark et al., 2020b).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 343,
      "token_count": 750,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 237\n\n11.2 \u2022 T RAINING BIDIRECTIONAL ENCODERS 229\nXis actually formed by summing 3 embeddings: word, position, and \ufb01rst/second\nsegment embeddings.\nDuring training, the output vector hL\nCLSfrom the \ufb01nal layer associated with the\n[CLS] token represents the next sentence prediction. As with the MLM objective,\nwe add a special head, in this case an NSP head, which consists of a learned set of\nclassi\ufb01cation weights WNSP2Rd\u00022that produces a two-class prediction from the\nraw[CLS] vector hL\nCLS:\nyi=softmax (hL\nCLSWNSP)\nCross entropy is used to compute the NSP loss for each sentence pair presented\nto the model. Fig. 11.4 illustrates the overall NSP training setup. In BERT, the NSP\nloss was used in conjunction with the MLM training objective to form \ufb01nal loss.\nCancelmy\ufb02ight[SEP] 1CE Loss\nAnd the \nBidirectional Transformer Encoder\np1p2p3p4p5p6p7p8[CLS]++s1NSPHeadToken +Segment +PositionalEmbeddings hotel p9[SEP] ++s1s1s1s1s2s2s2s2++++++++++++++hCLS\nFigure 11.4 An example of the NSP loss calculation.\n11.2.3 Training Regimes\nBERT and other early transformer-based language models were trained on about\n3.3 billion words (a combination of English Wikipedia and a corpus of book texts\ncalled BooksCorpus (Zhu et al., 2015) that is no longer used for intellectual property\nreasons). Modern masked language models are now trained on much larger datasets\nof web text, \ufb01ltered a bit, and augmented by higher-quality data like Wikipedia,\nthe same as those we discussed for the causal large language models of Chapter 9.\nMultilingual models similarly use webtext and multilingual Wikipedia. For example\nthe XLM-R model was trained on about 300 billion tokens in 100 languages, taken\nfrom the web via Common Crawl ( https://commoncrawl.org/ ).\nTo train the original BERT models, pairs of text segments were selected from the\ntraining corpus according to the next sentence prediction 50/50 scheme. Pairs were\nsampled so that their combined length was less than the 512 token input. Tokens\nwithin these sentence pairs were then masked using the MLM approach with the\ncombined loss from the MLM and NSP objectives used for a \ufb01nal loss. Because this\n\ufb01nal loss is backpropagated through the entire transformer, the embeddings at each\ntransformer layer will learn representations that are useful for predicting words from\ntheir neighbors. Since the [CLS] tokens are the direct input to the NSP classi\ufb01er,\ntheir learned representations will tend to contain information about the sequence as",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 344,
      "token_count": 628,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 238\n\n230 CHAPTER 11 \u2022 M ASKED LANGUAGE MODELS\na whole. Approximately 40 passes (epochs) over the training data was required for\nthe model to converge.\nSome models, like the RoBERTa model, drop the next sentence prediction ob-\njective, and therefore change the training regime a bit. Instead of sampling pairs of\nsentence, the input is simply a series of contiguous sentences, still beginning with\nthe special [CLS] token. If the document runs out before 512 tokens are reached, an\nextra separator token is added, and sentences from the next document are packed in,\nuntil we reach a total of 512 tokens. Usually large batch sizes are used, between 8K\nand 32K tokens.\nMultilingual models have an additional decision to make: what data to use to\nbuild the vocabulary? Recall that all language models use subword tokenization\n(BPE or SentencePiece Unigram LM are the two most common algorithms). What\ntext should be used to learn this multilingual tokenization, given that it\u2019s easier to get\nmuch more text in some languages than others? One option would be to create this\nvocabulary-learning dataset by sampling sentences from our training data (perhaps\nweb text from Common Crawl), randomly. In that case we will choose a lot of sen-\ntences from languages like languages with lots of web representation like English,\nand the tokens will be biased toward rare English tokens instead of creating frequent\ntokens from languages with less data. Instead, it is common to divide the training\ndata into subcorpora of Ndifferent languages, compute the number of sentences ni\nof each language i, and readjust these probabilities so as to upweight the probability\nof less-represented languages (Lample and Conneau, 2019). The new probability of\nselecting a sentence from each of the Nlanguages (whose prior frequency is ni) is\nfqigi=1:::N, where:\nqi=pa\niPN\nj=1pa\njwith pi=niPN\nk=1nk(11.5)\nRecall from Eq. 6.32 in Chapter 6 that an avalue between 0 and 1 will give higher\nweight to lower probability samples. Conneau et al. (2020) show that a=0:3 works\nwell to give rare languages more inclusion in the tokenization, resulting in better\nmultilingual performance overall.\nThe result of this pretraining process consists of both learned word embeddings,\nas well as all the parameters of the bidirectional encoder that are used to produce\ncontextual embeddings for novel inputs.\nFor many purposes, a pretrained multilingual model is more practical than a\nmonolingual model, since it avoids the need to build many (a hundred!) separate\nmonolingual models. And multilingual models can improve performance on low-\nresourced languages by leveraging linguistic information from a similar language in\nthe training data that happens to have more resources. Nonetheless, when the num-\nber of languages grows very large, multilingual models exhibit what has been called\nthecurse of multilinguality (Conneau et al., 2020): the performance on each lan-\nguage degrades compared to a model training on fewer languages. Another problem\nwith multilingual models is that they \u2018have an accent\u2019: grammatical structures in\nhigher-resource languages (often English) bleed into lower-resource languages; the\nvast amount of English language in training makes the model\u2019s representations for\nlow-resource languages slightly more English-like (Papadimitriou et al., 2023).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 345,
      "token_count": 746,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 239\n\n11.3 \u2022 C ONTEXTUAL EMBEDDINGS 231\n11.3 Contextual Embeddings\nGiven a pretrained language model and a novel input sentence, we can think of the\nsequence of model outputs as constituting contextual embeddings for each token incontextual\nembeddings\nthe input. These contextual embeddings are vectors representing some aspect of the\nmeaning of a token in context, and can be used for any task requiring the meaning\nof tokens or words. More formally, given a sequence of input tokens x1;:::;xn, we\ncan use the output vector hLifrom the \ufb01nal layer Lof the model as a representation\nof the meaning of token xiin the context of sentence x1;:::;xn. Or instead of just\nusing the vector hLifrom the \ufb01nal layer of the model, it\u2019s common to compute a\nrepresentation for xiby averaging the output tokens hifrom each of the last four\nlayers of the model, i.e., hLi,hL\u00001i,hL\u00002i, and hL\u00003i.\n[CLS]SolongandthanksforallhL1hLCLShL2hL3hL4hL5hL6\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nFigure 11.5 The output of a BERT-style model is a contextual embedding vector hL\nifor\neach input token xi.\nJust as we used static embeddings like word2vec in Chapter 6 to represent the\nmeaning of words, we can use contextual embeddings as representations of word\nmeanings in context for any task that might require a model of word meaning. Where\nstatic embeddings represent the meaning of word types (vocabulary entries), contex-\ntual embeddings represent the meaning of word instances : instances of a particular\nword type in a particular context. Thus where word2vec had a single vector for each\nword type, contextual embeddings provide a single vector for each instance of that\nword type in its sentential context. Contextual embeddings can thus be used for\ntasks like measuring the semantic similarity of two words in context, and are useful\nin linguistic tasks that require models of word meaning.\n11.3.1 Contextual Embeddings and Word Sense\nWords are ambiguous : the same word can be used to mean different things. In ambiguous\nChapter 6 we saw that the word \u201cmouse\u201d can mean (1) a small rodent, or (2) a hand-\noperated device to control a cursor. The word \u201cbank\u201d can mean: (1) a \ufb01nancial\ninstitution or (2) a sloping mound. We say that the words \u2018mouse\u2019 or \u2018bank\u2019 are",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 346,
      "token_count": 580,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 240",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 347,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "232 CHAPTER 11 \u2022 M ASKED LANGUAGE MODELS\npolysemous (from Greek \u2018many senses\u2019, poly- \u2018many\u2019 + sema , \u2018sign, mark\u2019).2\nAsense (orword sense ) is a discrete representation of one aspect of the meaning word sense\nof a word. We can represent each sense with a superscript: bank1andbank2,\nmouse1andmouse2. These senses can be found listed in online thesauruses (or\nthesauri) like WordNet (Fellbaum, 1998), which has datasets in many languages WordNet\nlisting the senses of many words. In context, it\u2019s easy to see the different meanings:\nmouse1: .... a mouse controlling a computer system in 1968.\nmouse2: .... a quiet animal like a mouse\nbank1: ...a bank can hold the investments in a custodial account ...\nbank2: ...as agriculture burgeons on the east bank , the river ...\nThis fact that context disambiguates the senses of mouse andbank above can\nalso be visualized geometrically. Fig. 11.6 shows a two-dimensional projection of\nmany instances of the BERT embeddings of the word diein English and German.\nEach point in the graph represents the use of diein one input sentence. We can\nclearly see at least two different English senses of die(the singular of dice and the\nverb to die , as well as the German article, in the BERT embedding space.\nFigure 4: Embeddings for the word \"die\" in different contexts, visualized with UMAP. Sample points\nare annotated with corresponding sentences. Overall annotations (blue text) are added as a guide.\n4.1 Visualization of word senses\nOur \ufb01rst experiment is an exploratory visualization of how word sense affects context embeddings.\nFor data on different word senses, we collected all sentences used in the introductions to English-\nlanguage Wikipedia articles. (Text outside of introductions was frequently fragmentary.) We created\nan interactive application, which we plan to make public. A user enters a word, and the system\nretrieves 1,000 sentences containing that word. It sends these sentences to BERT-base as input, and\nfor each one it retrieves the context embedding for the word from a layer of the user\u2019s choosing.\nThe system visualizes these 1,000 context embeddings using UMAP [ 15], generally showing clear\nclusters relating to word senses. Different senses of a word are typically spatially separated, and\nwithin the clusters there is often further structure related to \ufb01ne shades of meaning. In Figure 4, for\nexample, we not only see crisp, well-separated clusters for three meanings of the word \u201cdie,\u201d but\nwithin one of these clusters there is a kind of quantitative scale, related to the number of people\ndying. See Appendix 6.4 for further examples. The apparent detail in the clusters we visualized raises\ntwo immediate questions. First, is it possible to \ufb01nd quantitative corroboration that word senses are\nwell-represented? Second, how can we resolve a seeming contradiction: in the previous section, we\nsaw how position represented syntax; yet here we see position representing semantics.\n4.2 Measurement of word sense disambiguation capability\nThe crisp clusters seen in visualizations such as Figure 4 suggest that BERT may create simple,\neffective internal representations of word senses, putting different meanings in different locations. To\ntest this hypothesis quantitatively, we test whether a simple classi\ufb01er on these internal representations\ncan perform well at word-sense disambiguation (WSD).\nWe follow the procedure described in [ 20], which performed a similar experiment with the ELMo",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 348,
      "token_count": 778,
      "chapter_title": ""
    }
  },
  {
    "content": "The system visualizes these 1,000 context embeddings using UMAP [ 15], generally showing clear\nclusters relating to word senses. Different senses of a word are typically spatially separated, and\nwithin the clusters there is often further structure related to \ufb01ne shades of meaning. In Figure 4, for\nexample, we not only see crisp, well-separated clusters for three meanings of the word \u201cdie,\u201d but\nwithin one of these clusters there is a kind of quantitative scale, related to the number of people\ndying. See Appendix 6.4 for further examples. The apparent detail in the clusters we visualized raises\ntwo immediate questions. First, is it possible to \ufb01nd quantitative corroboration that word senses are\nwell-represented? Second, how can we resolve a seeming contradiction: in the previous section, we\nsaw how position represented syntax; yet here we see position representing semantics.\n4.2 Measurement of word sense disambiguation capability\nThe crisp clusters seen in visualizations such as Figure 4 suggest that BERT may create simple,\neffective internal representations of word senses, putting different meanings in different locations. To\ntest this hypothesis quantitatively, we test whether a simple classi\ufb01er on these internal representations\ncan perform well at word-sense disambiguation (WSD).\nWe follow the procedure described in [ 20], which performed a similar experiment with the ELMo\nmodel. For a given word with nsenses, we make a nearest-neighbor classi\ufb01er where each neighbor is\nthe centroid of a given word sense\u2019s BERT-base embeddings in the training data. To classify a new\nword we \ufb01nd the closest of these centroids, defaulting to the most commonly used sense if the word\nwas not present in the training data. We used the data and evaluation from [ 21]: the training data was\nSemCor [17] (33,362 senses), and the testing data was the suite described in [21] (3,669 senses).\nThe simple nearest-neighbor classi\ufb01er achieves an F1 score of 71.1, higher than the current state of\nthe art (Table 1), with the accuracy monotonically increasing through the layers. This is a strong\nsignal that context embeddings are representing word-sense information. Additionally, an even higher\nscore of 71.5 was obtained using the technique described in the following section.\n6\nFigure 11.6 Each blue dot shows a BERT contextual embedding for the word diefrom different sentences\nin English and German, projected into two dimensions with the UMAP algorithm. The German and English\nmeanings and the different English senses fall into different clusters. Some sample points are shown with the\ncontextual sentence they came from. Figure from Coenen et al. (2019).\nThus while thesauruses like WordNet give discrete lists of senses, embeddings\n(whether static or contextual) offer a continuous high-dimensional model of meaning\nthat, although it can be clustered, doesn\u2019t divide up into fully discrete senses.\nWord Sense Disambiguation\nThe task of selecting the correct sense for a word is called word sense disambigua-\ntion, orWSD . WSD algorithms take as input a word in context and a \ufb01xed inventoryword sense\ndisambiguation\nWSD of potential word senses (like the ones in WordNet) and outputs the correct word\nsense in context. Fig. 11.7 sketches out the task.\n2The word polysemy itself is ambiguous; you may see it used in a different way, to refer only to cases\nwhere a word\u2019s senses are related in some structured way, reserving the word homonymy to mean sense",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 349,
      "token_count": 775,
      "chapter_title": ""
    }
  },
  {
    "content": "score of 71.5 was obtained using the technique described in the following section.\n6\nFigure 11.6 Each blue dot shows a BERT contextual embedding for the word diefrom different sentences\nin English and German, projected into two dimensions with the UMAP algorithm. The German and English\nmeanings and the different English senses fall into different clusters. Some sample points are shown with the\ncontextual sentence they came from. Figure from Coenen et al. (2019).\nThus while thesauruses like WordNet give discrete lists of senses, embeddings\n(whether static or contextual) offer a continuous high-dimensional model of meaning\nthat, although it can be clustered, doesn\u2019t divide up into fully discrete senses.\nWord Sense Disambiguation\nThe task of selecting the correct sense for a word is called word sense disambigua-\ntion, orWSD . WSD algorithms take as input a word in context and a \ufb01xed inventoryword sense\ndisambiguation\nWSD of potential word senses (like the ones in WordNet) and outputs the correct word\nsense in context. Fig. 11.7 sketches out the task.\n2The word polysemy itself is ambiguous; you may see it used in a different way, to refer only to cases\nwhere a word\u2019s senses are related in some structured way, reserving the word homonymy to mean sense\nambiguities with no relation between the senses (Haber and Poesio, 2020). Here we will use \u2018polysemy\u2019\nto mean any kind of sense ambiguity, and \u2018structured polysemy\u2019 for polysemy with sense relations.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 350,
      "token_count": 341,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 241\n\n11.3 \u2022 C ONTEXTUAL EMBEDDINGS 233\nanelectricguitarandbassplayerstando\ufb00toonesideelectric1: using electricityelectric2:  tenseelectric3: thrillingguitar1 bass1: low range\u2026bass4: sea \ufb01sh\u2026 bass7: instrument\u2026player1: in gameplayer2: musician player3: actor\u2026stand1: upright\u2026stand5: bear\u2026 stand10: put upright\u2026side1: relative region\u2026side3: of body\u2026 side11: slope\u2026x1y1x2y2x3y3y4y5y6\nx4x5x6\nFigure 11.7 The all-words WSD task, mapping from input words ( x) to WordNet senses\n(y). Figure inspired by Chaplot and Salakhutdinov (2018).\nWSD can be a useful analytic tool for text analysis in the humanities and social\nsciences, and word senses can play a role in model interpretability for word repre-\nsentations. Word senses also have interesting distributional properties. For example\na word often is used in roughly the same sense through a discourse, an observation\ncalled the one sense per discourse rule (Gale et al., 1992a).one sense per\ndiscourse\nThe best performing WSD algorithm is a simple 1-nearest-neighbor algorithm\nusing contextual word embeddings, due to Melamud et al. (2016) and Peters et al.\n(2018). At training time we pass each sentence in some sense-labeled dataset (like\nthe SemCore or SenseEval datasets in various languages) through any contextual\nembedding (e.g., BERT) resulting in a contextual embedding for each labeled token.\n(There are various ways to compute this contextual embedding vifor a token i; for\nBERT it is common to pool multiple layers by summing the vector representations\nofifrom the last four BERT layers). Then for each sense sof any word in the corpus,\nfor each of the ntokens of that sense, we average their ncontextual representations\nvito produce a contextual sense embedding vsfors:\nvs=1\nnX\nivi8vi2tokens (s) (11.6)\nAt test time, given a token of a target word tin context, we compute its contextual\nembedding tand choose its nearest neighbor sense from the training set, i.e., the\nsense whose sense embedding has the highest cosine with t:\nsense(t) =argmax\ns2senses (t)cosine (t;vs) (11.7)\nFig. 11.8 illustrates the model.\n11.3.2 Contextual Embeddings and Word Similarity\nIn Chapter 6 we introduced the idea that we could measure the similarity of two\nwords by considering how close they are geometrically, by using the cosine as a\nsimilarity function. The idea of meaning similarity is also clear geometrically in the\nmeaning clusters in Fig. 11.6; the representation of a word which has a particular\nsense in a context is closer to other instances of the same sense of the word. Thus we",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 351,
      "token_count": 664,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 242\n\n234 CHAPTER 11 \u2022 M ASKED LANGUAGE MODELS\nI  found  the  jar  emptycIcfoundfind1vcthecjarcemptyfind9vfind5vfind4vENCODER\nFigure 11.8 The nearest-neighbor algorithm for WSD. In green are the contextual embed-\ndings precomputed for each sense of each word; here we just show a few of the senses for\n\ufb01nd. A contextual embedding is computed for the target word found , and then the nearest\nneighbor sense (in this case \ufb01nd9v) is chosen. Figure inspired by Loureiro and Jorge (2019).\noften measure the similarity between two instances of two words in context (or two\ninstances of the same word in two different contexts) by using the cosine between\ntheir contextual embeddings.\nUsually some transformations to the embeddings are required before computing\ncosine. This is because contextual embeddings (whether from masked language\nmodels or from autoregressive ones) have the property that the vectors for all words\nare extremely similar. If we look at the embeddings from the \ufb01nal layer of BERT\nor other models, embeddings for instances of any two randomly chosen words will\nhave extremely high cosines that can be quite close to 1, meaning all word vectors\ntend to point in the same direction. The property of vectors in a system all tending\nto point in the same direction is known as anisotropy . Ethayarajh (2019) de\ufb01nes\ntheanisotropy of a model as the expected cosine similarity of any pair of words in anisotropy\na corpus. The word \u2018isotropy\u2019 means uniformity in all directions, so in an isotropic\nmodel, the collection of vectors should point in all directions and the expected cosine\nbetween a pair of random embeddings would be zero. Timkey and van Schijndel\n(2021) show that one cause of anisotropy is that cosine measures are dominated by\na small number of dimensions of the contextual embedding whose values are very\ndifferent than the others: these rogue dimensions have very large magnitudes and\nvery high variance.\nTimkey and van Schijndel (2021) shows that we can make the embeddings more\nisotropic by standardizing (z-scoring) the vectors, i.e., subtracting the mean and\ndividing by the variance. Given a set Cof all the embeddings in some corpus, each\nwith dimensionality d(i.e., x2Rd), the mean vector m2Rdis:\nm=1\njCjX\nx2Cx (11.8)\nThe standard deviation in each dimension s2Rdis:\ns=s\n1\njCjX\nx2C(x\u0000m)2(11.9)\nThen each word vector xis replaced by a standardized version z:\nz=x\u0000m\ns(11.10)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 352,
      "token_count": 615,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 243\n\n11.4 \u2022 F INE-TUNING FOR CLASSIFICATION 235\nOne problem with cosine that is not solved by standardization is that cosine tends\nto underestimate human judgments on similarity of word meaning for very frequent\nwords (Zhou et al., 2022).\nIn the next section we\u2019ll see the most common use of contextual representations:\nas representations of words or even entire sentences that can be the inputs to classi-\n\ufb01ers in the \ufb01netuning process for downstream NLP applications.\n11.4 Fine-Tuning for Classi\ufb01cation\nThe power of pretrained language models lies in their ability to extract generaliza-\ntions from large amounts of text\u2014generalizations that are useful for myriad down-\nstream applications. There are two ways to make practical use of the generalizations\nto solve downstream tasks. The most common way is to use natural language to\nprompt the model, putting it in a state where it contextually generates what we\nwant. We\u2019ll introduce prompting in Chapter 12.\nIn this section we explore an alternative way to use pretrained language models\nfor downstream applications: a version of the \ufb01netuning paradigm from Chapter 10. \ufb01netuning\nIn the kind of \ufb01netuning used for masked language models, we add application-\nspeci\ufb01c circuitry (often called a special head ) on top of pretrained models, taking\ntheir output as its input. The \ufb01netuning process consists of using labeled data about\nthe application to train these additional application-speci\ufb01c parameters. Typically,\nthis training will either freeze or make only minimal adjustments to the pretrained\nlanguage model parameters.\nThe following sections introduce \ufb01netuning methods for the most common kinds\nof applications: sequence classi\ufb01cation, sentence-pair classi\ufb01cation, and sequence\nlabeling.\n11.4.1 Sequence Classi\ufb01cation\nThe task of sequence classi\ufb01cation is to classify an entire sequence of text with a\nsingle label. This set of tasks is commonly called text classi\ufb01cation , like sentiment\nanalysis or spam detection (Chapter 4) in which we classify a text into two or three\nclasses (like positive or negative), as well as classi\ufb01cation tasks with a large number\nof categories, like document-level topic classi\ufb01cation.\nFor sequence classi\ufb01cation we represent the entire input to be classi\ufb01ed by a\nsingle vector. We can represent a sequence in various ways. One way is to take\nthe sum or the mean of the last output vector from each token in the sequence.\nFor BERT, we instead add a new unique token to the vocabulary called [CLS] , and\nprepended it to the start of all input sequences, both during pretraining and encoding.\nThe output vector in the \ufb01nal layer of the model for the [CLS] input represents\nthe entire input sequence and serves as the input to a classi\ufb01er head , a logistic classi\ufb01er head\nregression or neural network classi\ufb01er that makes the relevant decision.\nAs an example, let\u2019s return to the problem of sentiment classi\ufb01cation. to \ufb01netun-\ning a classi\ufb01er for this application involves learning a set of weights, WC, to map the\noutput vector for the [CLS] token\u2014 hL\nCLS\u2014to a set of scores over the possible senti-\nment classes. Assuming a three-way sentiment classi\ufb01cation task (positive, negative,\nneutral) and dimensionality das the model dimension, WCwill be of size [d\u00023]. To\nclassify a document, we pass the input text through the pretrained language model to",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 353,
      "token_count": 776,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 244\n\n236 CHAPTER 11 \u2022 M ASKED LANGUAGE MODELS\ngenerate hL\nCLS, multiply it by WC, and pass the resulting vector through a softmax.\ny=softmax (hL\nCLSWC) (11.11)\nFinetuning the values in WCrequires supervised training data consisting of input\nsequences labeled with the appropriate sentiment class. Training proceeds in the\nusual way; cross-entropy loss between the softmax output and the correct answer is\nused to drive the learning that produces WC.\nA key difference from what we\u2019ve seen earlier with neural classi\ufb01ers is that this\nloss can be used to not only learn the weights of the classi\ufb01er, but also to update the\nweights for the pretrained language model itself. In practice, reasonable classi\ufb01ca-\ntion performance is typically achieved with only minimal changes to the language\nmodel parameters, often limited to updates over the \ufb01nal few layers of the trans-\nformer. Fig. 11.9 illustrates this overall approach to sequence classi\ufb01cation.\n[CLS]entirelypredictableandlacksenergy\nBidirectional Transformer EncoderhCLS\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+sentimentclassi\ufb01cation headWCy\nFigure 11.9 Sequence classi\ufb01cation with a bidirectional transformer encoder. The output vector for the\n[CLS] token serves as input to a simple classi\ufb01er.\n11.4.2 Sequence-Pair Classi\ufb01cation\nAs mentioned in Section 11.2.2, an important type of problem involves the classi\ufb01ca-\ntion of pairs of input sequences. Practical applications that fall into this class include\nparaphrase detection (are the two sentences paraphrases of each other?), logical en-\ntailment (does sentence A logically entail sentence B?), and discourse coherence\n(how coherent is sentence B as a follow-on to sentence A?).\nFine-tuning an application for one of these tasks proceeds just as with pretrain-\ning using the NSP objective. During \ufb01netuning, pairs of labeled sentences from a\nsupervised \ufb01netuning set are presented to the model, and run through all the layers\nof the model to produce the houtputs for each input token. As with sequence classi-\n\ufb01cation, the output vector associated with the prepended [CLS] token represents the\nmodel\u2019s view of the input pair. And as with NSP training, the two inputs are sepa-\nrated by the [SEP] token. To perform classi\ufb01cation, the [CLS] vector is multiplied\nby a set of learning classi\ufb01cation weights and passed through a softmax to generate\nlabel predictions, which are then used to update the weights.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 354,
      "token_count": 585,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 245\n\n11.5 \u2022 F INE-TUNING FOR SEQUENCE LABELLING : NAMED ENTITY RECOGNITION 237\nAs an example, let\u2019s consider an entailment classi\ufb01cation task with the Multi-\nGenre Natural Language Inference (MultiNLI) dataset (Williams et al., 2018). In\nthe task of natural language inference orNLI, also called recognizing textualnatural\nlanguage\ninferenceentailment , a model is presented with a pair of sentences and must classify the re-\nlationship between their meanings. For example in the MultiNLI corpus, pairs of\nsentences are given one of 3 labels: entails ,contradicts andneutral . These labels\ndescribe a relationship between the meaning of the \ufb01rst sentence (the premise) and\nthe meaning of the second sentence (the hypothesis). Here are representative exam-\nples of each class from the corpus:\n\u2022Neutral\na: Jon walked back to the town to the smithy.\nb: Jon traveled back to his hometown.\n\u2022Contradicts\na: Tourist Information of\ufb01ces can be very helpful.\nb: Tourist Information of\ufb01ces are never of any help.\n\u2022Entails\na: I\u2019m confused.\nb: Not all of it is very clear to me.\nA relationship of contradicts means that the premise contradicts the hypothesis; en-\ntails means that the premise entails the hypothesis; neutral means that neither is\nnecessarily true. The meaning of these labels is looser than strict logical entailment\nor contradiction indicating that a typical human reading the sentences would most\nlikely interpret the meanings in this way.\nTo \ufb01netune a classi\ufb01er for the MultiNLI task, we pass the premise/hypothesis\npairs through a bidirectional encoder as described above and use the output vector\nfor the[CLS] token as the input to the classi\ufb01cation head. As with ordinary sequence\nclassi\ufb01cation, this head provides the input to a three-way classi\ufb01er that can be trained\non the MultiNLI training corpus.\n11.5 Fine-Tuning for Sequence Labelling: Named En-\ntity Recognition\nIn sequence labeling, the network\u2019s task is to assign a label chosen from a small\n\ufb01xed set of labels to each token in the sequence. One of the most common sequence\nlabeling task is named entity recognition .\n11.5.1 Named Entities\nAnamed entity is, roughly speaking, anything that can be referred to with a proper named entity\nname: a person, a location, an organization. The task of named entity recognitionnamed entity\nrecognition\n(NER ) is to \ufb01nd spans of text that constitute proper names and tag the type of the NER\nentity. Four entity tags are most common: PER (person), LOC (location), ORG\n(organization), or GPE (geo-political entity). However, the term named entity is\ncommonly extended to include things that aren\u2019t entities per se, including temporal\nexpressions like dates and times, and even numerical expressions like prices. Here\u2019s\nan example of the output of an NER tagger:",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 355,
      "token_count": 665,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 246",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 356,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "238 CHAPTER 11 \u2022 M ASKED LANGUAGE MODELS\nCiting high fuel prices, [ ORG United Airlines ] said [ TIME Friday ] it\nhas increased fares by [ MONEY $6] per round trip on \ufb02ights to some\ncities also served by lower-cost carriers. [ ORG American Airlines ], a\nunit of [ ORG AMR Corp.] , immediately matched the move, spokesman\n[PER Tim Wagner ] said. [ ORG United] , a unit of [ ORG UAL Corp.] ,\nsaid the increase took effect [ TIME Thursday] and applies to most\nroutes where it competes against discount carriers, such as [ LOC Chicago]\nto [LOC Dallas] and [ LOC Denver] to [LOC San Francisco] .\nThe text contains 13 mentions of named entities including 5 organizations, 4 loca-\ntions, 2 times, 1 person, and 1 mention of money. Figure 11.10 shows typical generic\nnamed entity types. Many applications will also need to use speci\ufb01c entity types like\nproteins, genes, commercial products, or works of art.\nType Tag Sample Categories Example sentences\nPeople PER people, characters Turing is a giant of computer science.\nOrganization ORG companies, sports teams The IPCC warned about the cyclone.\nLocation LOC regions, mountains, seas Mt. Sanitas is in Sunshine Canyon .\nGeo-Political Entity GPE countries, states Palo Alto is raising the fees for parking.\nFigure 11.10 A list of generic named entity types with the kinds of entities they refer to.\nNamed entity recognition is a useful step in various natural language processing\ntasks, including linking text to information in structured knowledge sources like\nWikipedia, measuring sentiment or attitudes toward a particular entity in text, or\neven as part of anonymizing text for privacy. The NER task is is dif\ufb01cult because\nof the ambiguity of segmenting NER spans, \ufb01guring out which tokens are entities\nand which aren\u2019t, since most words in a text will not be named entities. Another\ndif\ufb01culty is caused by type ambiguity. The mention Washington can refer to a\nperson, a sports team, a city, or the US government, as we see in Fig. 11.11.\n[PER Washington] was born into slavery on the farm of James Burroughs.\n[ORG Washington] went up 2 games to 1 in the four-game series.\nBlair arrived in [ LOC Washington] for what may well be his last state visit.\nIn June, [ GPE Washington] passed a primary seatbelt law.\nFigure 11.11 Examples of type ambiguities in the use of the name Washington .\n11.5.2 BIO Tagging\nOne standard approach to sequence labeling for a span-recognition problem like\nNER is BIO tagging (Ramshaw and Marcus, 1995). This is a method that allows us BIO tagging\nto treat NER like a word-by-word sequence labeling task, via tags that capture both\nthe boundary and the named entity type. Consider the following sentence:\n[PER Jane Villanueva ] of [ ORG United ] , a unit of [ ORG United Airlines\nHolding ] , said the fare applies to the [ LOC Chicago ] route.\nFigure 11.12 shows the same excerpt represented with BIO tagging, as well as BIO\nvariants called IOtagging and BIOES tagging. In BIO tagging we label any token\nthatbegins a span of interest with the label B, tokens that occur inside a span are\ntagged with an I, and any tokens outside of any span of interest are labeled O. While\nthere is only one Otag, we\u2019ll have distinct Band Itags for each named entity class.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 357,
      "token_count": 775,
      "chapter_title": ""
    }
  },
  {
    "content": "[ORG Washington] went up 2 games to 1 in the four-game series.\nBlair arrived in [ LOC Washington] for what may well be his last state visit.\nIn June, [ GPE Washington] passed a primary seatbelt law.\nFigure 11.11 Examples of type ambiguities in the use of the name Washington .\n11.5.2 BIO Tagging\nOne standard approach to sequence labeling for a span-recognition problem like\nNER is BIO tagging (Ramshaw and Marcus, 1995). This is a method that allows us BIO tagging\nto treat NER like a word-by-word sequence labeling task, via tags that capture both\nthe boundary and the named entity type. Consider the following sentence:\n[PER Jane Villanueva ] of [ ORG United ] , a unit of [ ORG United Airlines\nHolding ] , said the fare applies to the [ LOC Chicago ] route.\nFigure 11.12 shows the same excerpt represented with BIO tagging, as well as BIO\nvariants called IOtagging and BIOES tagging. In BIO tagging we label any token\nthatbegins a span of interest with the label B, tokens that occur inside a span are\ntagged with an I, and any tokens outside of any span of interest are labeled O. While\nthere is only one Otag, we\u2019ll have distinct Band Itags for each named entity class.\nThe number of tags is thus 2 n+1 tags, where nis the number of entity types. BIO",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 358,
      "token_count": 308,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 247\n\n11.5 \u2022 F INE-TUNING FOR SEQUENCE LABELLING : NAMED ENTITY RECOGNITION 239\ntagging can represent exactly the same information as the bracketed notation, but has\nthe advantage that we can represent the task in the same simple sequence modeling\nway as part-of-speech tagging: assigning a single label yito each input word xi:\nWords IO Label BIO Label BIOES Label\nJane I-PER B-PER B-PER\nVillanueva I-PER I-PER E-PER\nof O O O\nUnited I-ORG B-ORG B-ORG\nAirlines I-ORG I-ORG I-ORG\nHolding I-ORG I-ORG E-ORG\ndiscussed O O O\nthe O O O\nChicago I-LOC B-LOC S-LOC\nroute O O O\n. O O O\nFigure 11.12 NER as a sequence model, showing IO, BIO, and BIOES taggings.\nWe\u2019ve also shown two variant tagging schemes: IO tagging, which loses some\ninformation by eliminating the B tag, and BIOES tagging, which adds an end tag E\nfor the end of a span, and a span tag Sfor a span consisting of only one word.\n11.5.3 Sequence Labeling\nIn sequence labeling, we pass the \ufb01nal output vector corresponding to each input\ntoken to a classi\ufb01er that produces a softmax distribution over the possible set of\ntags. For a single feedforward layer classi\ufb01er, the set of weights to be learned is\nWKof size [d\u0002k], where kis the number of possible tags for the task. A greedy\napproach, where the argmax tag for each token is taken as a likely answer, can be\nused to generate the \ufb01nal output tag sequence. Fig. 11.13 illustrates an example of\nthis approach, where yiis a vector of probabilities over tags, and kindexes the tags.\nyi=softmax (hL\niWK) (11.12)\nti=argmaxk(yi) (11.13)\nAlternatively, the distribution over labels provided by the softmax for each input\ntoken can be passed to a conditional random \ufb01eld (CRF) layer which can take global\ntag-level transitions into account (see Chapter 17 on CRFs).\nTokenization and NER\nNote that supervised training data for NER is typically in the form of BIO tags as-\nsociated with text segmented at the word level. For example the following sentence\ncontaining two named entities:\n[LOC Mt. Sanitas ] is in [ LOC Sunshine Canyon ] .\nwould have the following set of per-word BIO tags.\n(11.14) Mt.\nB-LOCSanitas\nI-LOCis\nOin\nOSunshine\nB-LOCCanyon\nI-LOC.\nO\nUnfortunately, the sequence of WordPiece tokens for this sentence doesn\u2019t align\ndirectly with BIO tags in the annotation:",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 359,
      "token_count": 633,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 248\n\n240 CHAPTER 11 \u2022 M ASKED LANGUAGE MODELS\n[CLS]JaneVillanuevaofUnitedAirlines\nBidirectional Transformer EncoderB-PERI-PEROB-ORGI-ORG\nHoldingdiscussedI-ORGOWKNER headhiargmax\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+WKWKWKWKWKWKyi\nFigure 11.13 Sequence labeling for named entity recognition with a bidirectional transformer encoder. The\noutput vector for each input token is passed to a simple k-way classi\ufb01er.\n'Mt', '.', 'San', '##itas', 'is', 'in', 'Sunshine', 'Canyon' '.'\nTo deal with this misalignment, we need a way to assign BIO tags to subword\ntokens during training and a corresponding way to recover word-level tags from\nsubwords during decoding. For training, we can just assign the gold-standard tag\nassociated with each word to all of the subword tokens derived from it.\nFor decoding, the simplest approach is to use the argmax BIO tag associated with\nthe \ufb01rst subword token of a word. Thus, in our example, the BIO tag assigned to\n\u201cMt\u201d would be assigned to \u201cMt.\u201d and the tag assigned to \u201cSan\u201d would be assigned\nto \u201cSanitas\u201d, effectively ignoring the information in the tags assigned to \u201c.\u201d and\n\u201c##itas\u201d. More complex approaches combine the distribution of tag probabilities\nacross the subwords in an attempt to \ufb01nd an optimal word-level tag.\n11.5.4 Evaluating Named Entity Recognition\nNamed entity recognizers are evaluated by recall ,precision , and F1measure . Re-\ncall that recall is the ratio of the number of correctly labeled responses to the total\nthat should have been labeled; precision is the ratio of the number of correctly la-\nbeled responses to the total labeled; and F-measure is the harmonic mean of the\ntwo.\nTo know if the difference between the F 1scores of two NER systems is a signif-\nicant difference, we use the paired bootstrap test, or the similar randomization test\n(Section 4.9).\nFor named entity tagging, the entity rather than the word is the unit of response.\nThus in the example in Fig. 11.12, the two entities Jane Villanueva andUnited Air-\nlines Holding and the non-entity discussed would each count as a single response.\nThe fact that named entity tagging has a segmentation component which is not\npresent in tasks like text categorization or part-of-speech tagging causes some prob-\nlems with evaluation. For example, a system that labeled Jane but not Jane Vil-\nlanueva as a person would cause two errors, a false positive for O and a false nega-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 360,
      "token_count": 593,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 249\n\n11.6 \u2022 S UMMARY 241\ntive for I-PER. In addition, using entities as the unit of response but words as the unit\nof training means that there is a mismatch between the training and test conditions.\n11.6 Summary\nThis chapter has introduced the bidirectional encoder and the masked language\nmodel . Here\u2019s a summary of the main points that we covered:\n\u2022 Bidirectional encoders can be used to generate contextualized representations\nof input embeddings using the entire input context.\n\u2022 Pretrained language models based on bidirectional encoders can be learned\nusing a masked language model objective where a model is trained to guess\nthe missing information from an input.\n\u2022 The vector output of each transformer block or component in a particular to-\nken column is a contextual embedding that represents some aspect of the\nmeaning of a token in context.\n\u2022 A word sense is a discrete representation of one aspect of the meaning of a\nword. Contextual embeddings offer a continuous high-dimensional model of\nmeaning that is richer than fully discrete senses.\n\u2022 The cosine between contextual embeddings can be used as one way to model\nthe similarity between two words in context, although some transformations\nto the embeddings are required \ufb01rst.\n\u2022 Pretrained language models can be \ufb01netuned for speci\ufb01c applications by adding\nlightweight classi\ufb01er layers on top of the outputs of the pretrained model.\n\u2022 These applications can include sequence classi\ufb01cation tasks like sentiment\nanalysis, sequence-pair classi\ufb01cation tasks like natural language inference,\norsequence labeling tasks like named entity recognition .\nBibliographical and Historical Notes\nHistory TBD.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 361,
      "token_count": 344,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 250\n\n242 CHAPTER 12 \u2022 M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING\nCHAPTER\n12Model Alignment, Prompting,\nand In-Context Learning\n\u201cHal, \u201d said Bowman, now speaking with an icy calm. \u201cI am not incapaci-\ntated. Unless you obey my instructions, I shall be forced to disconnect you. \u201d\nArthur C. Clarke\nIn this chapter we show how to get LLMs to do tasks for us simply by talking to\nthem. To get an LLM to translate a sentence, outline a talk, or draft a work email,\nwe\u2019ll simply describe what we want in natural language. We call these instructions\nwe give to language models prompts . prompts\nPrompting relies on contextual generation. Given the prompt as context, the lan-\nguage model generates the next token based on its token probability, conditioned on\nthe prompt: P(wijw<i). A prompt can be a question (like \u201cWhat is a transformer net-\nwork?\u201d), possibly in a structured format (like \u201cQ: What is a transformer network?\nA:\u201d), or can be an instruction (like \u201cTranslate the following sentence into Hindi:\n\u2018Chop the garlic \ufb01nely\u2019\u201d). A prompt can also contain demonstrations , examples to demonstrations\nhelp make the instructions clearer, (like \u201cGive the sentiment of the following sen-\ntence. Example Input: \u201cI really loved Taishan Cuisine.\u201d Output: positive\u201d.) As we\u2019ll\nsee, prompting can be applied to inherently generative tasks (like summarization and\ntranslation) as well as to ones more naturally thought of as classi\ufb01cation tasks.\nPrompts get language models to generate text, but they also can be viewed as\nalearning signal, because these demonstrations can help language models learn\nto perform novel tasks. For this reason we also refer to prompting as in-context-\nlearning \u2014learning that improves model performance or reduces some loss but doesin-context-\nlearning\nnot involve gradient-based updates to the model\u2019s underlying parameters.\nBut LLMs as we\u2019ve described them so far turn out to be bad at following instruc-\ntions. Pretraining isn\u2019t suf\ufb01cient to make them helpful . We\u2019ll introduce instruction\ntuning , a technique that helps LLMs learn to correctly respond to instructions byinstruction\ntuning\n\ufb01netuning them on a corpus of instructions with their corresponding response.\nA second failure of LLMs is that they can be harmful : their pretraining isn\u2019t\nsuf\ufb01cient to make them safe. Readers who know Arthur C. Clarke\u2019s 2001: A Space\nOdyssey or the Stanley Kubrick \ufb01lm know that the quote above comes in the context\nthat the arti\ufb01cial intelligence Hal becomes paranoid and tries to kill the crew of the\nspaceship. Unlike Hal, language models don\u2019t have intentionality or mental health\nissues like paranoid thinking, but they do have the capacity for harm. Pretrained lan-\nguage models can say things that are dangerous or false (like giving unsafe medical\nadvice) and they can verbally attack users or say toxic or hateful things.\nDealing with safety can be done partly by adding safety training into instruction\ntuning. But an important aspect of safety training is a second technique, preference\nalignment (often implemented, as we\u2019ll see, with the RLHF orDPO algorithms) inpreference\nalignment\nwhich a separate model is trained to decide how much a candidate response aligns\nwith human preferences. Together we refer to instruction tuning and preference\nalignment as model alignment . The intuition is that we want the learning objectivesmodel\nalignment\nof models to be aligned with the goals of the humans that use them.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 362,
      "token_count": 775,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 251\n\n12.1 \u2022 P ROMPTING 243\n12.1 Prompting\nAprompt is a text string that a user issues to a language model to get the model prompt\nto do something useful. In prompting, the user\u2019s prompt string is passed to the\nlanguage model, which iteratively generates tokens conditioned on the prompt. Thus\nthe prompt creates a context that guides LLMs to generate useful outputs to achieve\nsome user goal. The process of \ufb01nding effective prompts for a task is known as\nprompt engineering .prompt\nengineering\nLet\u2019s see how to prompt a language model to solve a simple sentiment classi\ufb01-\ncation task, Consider this hotel review from the BLT corpus (Salvetti et al., 2016):\nSample Hotel Review\nDid not like the service that I was provided, when I entered the hotel. I also\ndid not like the area, in which the hotel was located. Too much noise and\nevents going on for me to feel relax.\nWe can get the model to classify the sentiment of this text by taking the text and\nappending an incomplete statement to the review like In short, our stay was :\nA prompt consisting of a review plus an incomplete statement\nDid not like the service that I was provided, when I entered the hotel. I also\ndid not like the area, in which the hotel was located. Too much noise and\nevents going on for me to feel relax. In short, our stay was\nWe then have an LLM complete the statement by generating a token conditioned\non the prompt so far, and then generating the next token (conditioned on the prompt\nplus the new token), and so forth. Here are a few responses from a language model\nvia continued generation starting with the prompt as context.\nHotel Review Completions\nDid not like the service that I was provided, when I entered the hotel. I also\ndid not like the area, in which the hotel was located. Too much noise and\nevents going on for me to feel relaxed. In short our stay was\n... not a pleasant one. The staff at the front desk\nwere not welcoming or friendly, and seemed disinterested\nin providing good customer service.\n... uncomfortable and not worth the price we paid. We\nwill not be returning to this hotel.\nAs we can see, the overall negative context of the review results in negative\ncompletions. We could easily map these completions to the class we are trying\nto predict, perhaps via some prede\ufb01ned mappings, like fexcellent!positiveg,\nfdid not like!negativeg, and so on.\nThe power of this approach is that with suitable additions to the context a single\nLLM can produce outputs appropriate for many different tasks . For example, given",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 363,
      "token_count": 578,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 252\n\n244 CHAPTER 12 \u2022 M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING\na review we might want any of the following:\n\u2022 A summary,\n\u2022 Whether the review was truthful or likely to have been fabricated,\n\u2022 A translation to another language.\nLLMs have a striking ability to perform tasks like these, needing just the appro-\npriate contextual nudge to get the LLM to generate the desired output.\nIf we want to solve general tasks like summarization or translation, we don\u2019t\nwant to have to create a new prompt each time we do the task. Instead the \ufb01rst step\nin prompting is to design one or more templates : task-speci\ufb01c prompting text along templates\nwith slots for the particular input that is being processed.\nConsider the following templates for a variety of tasks:\nBasic Prompt Templates\nSummarization finputg;tldr;\nTranslation finputg;translate to French:\nSentiment finputg;Overall, it was\nFine-Grained- finputg;What aspects were important in this review?\nSentiment\nEach template consists of an input text, designated as finputg, followed by a\nverbatim prompt to be passed to an LLM. These templates are applied to inputs to\ncreate \ufb01lled prompts \u2013 instantiated prompts suitable for use as inputs to an LLM.\nFig. 12.1 illustrates \ufb01lled prompts for these templates using our earlier hotel review,\nalong with sample outputs from an LLM:\nNotice the design pattern of the prompts above: the input is followed by some\ntext which in turn will be completed by the desired response. This style, with the\ninstruction at the end, is common in prompting because it helpfully constrains the\ngeneration. Consider, by contrast, the prompt in Example 12.1.\nTranslate English to French:\nDid not like the service that I was provided! (12.1)\nThis prompt doesn\u2019t do a good job of constraining possible continuations. Instead\nof a French translation, models given this prompt may instead generate another sen-\ntence in English that simply extends the English review. Prompts need to be designed\nunambiguously, so that any reasonable continuation would accomplish the desired\ntask (Reynolds and McDonell, 2021).\nAn even more constraining style of prompt can specify the set of possible an-\nswers in the prompt. For example here is a prompt template to do sentiment analysis\nthat prespeci\ufb01es the potential answers:\nA prompt consisting of a review plus an incomplete statement\nHuman: Do you think that \u201cinput\u201d has negative or positive sentiment?\nChoices:\n(P) Positive\n(N) Negative\nAssistant: I believe the best answer is: (",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 364,
      "token_count": 568,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 253\n\n12.1 \u2022 P ROMPTING 245\nLLM Outputs for Basic Prompts\nOriginal Review ($INPUT) Did not like the service that I was provided,\nwhen I entered the hotel. I also did not like\nthe area, in which the hotel was located. Too\nmuch noise and events going on for me to feel\nrelax and away from the city life.\nSentiment Prompt :$INPUT + In short, our stay was\nOutput :not enjoyable\nFine-grained Sentiment Prompt :$INPUT + These aspects were important to\nthe reviewer:\nOutput :1. Poor service 2. Unpleasant location\n3. Noisy and busy area\nSummarization Prompt :$INPUT + tl;dr\nOutput :I had a bad experience with the hotel's\nservice and the location was loud and busy.\nTranslation Prompt :$INPUT + Translate this to French\nOutput :Je n'ai pas aim \u0013e le service qui m'a \u0013et\u0013e\noffert lorsque je suis entr \u0013e dans l'h ^otel. Je\nn'ai\u0013egalement pas aim \u0013e la zone dans laquelle se\ntrouvait l'h ^otel. Trop de bruit et d' \u0013ev\u0013enements\npour que je me sente d \u0013etendu et loin de la vie\ncitadine.\nFigure 12.1 LLM outputs for simple prompts for sentiment, summarization and translation for an input text.\nThis prompt uses a number of more sophisticated prompting characteristics. It\nspeci\ufb01es the two allowable choices (P) and (N), and ends the prompt with the open\nparenthesis that strongly suggests the answer will be (P) or (N). Note that it also\nspeci\ufb01es the role of the language model as an assistant.\nWe can do even more with prompts. For example, we might want to restrict a\nsummary to be a particular length, to have an answer generated according to some\nkind of persona or role, or to specify a more structured output using a programming\nlanguage or a data interchange format such as JSON. Or we may want to prompt\nthe system to break down complex tasks, using methods like chain-of-thought that\nwe\u2019ll discuss in Section 12.4. All of these kinds of instructions go beyond simple\nprompting and require further LLM \ufb01netuning to enable them to follow instructions.\nWe\u2019ll return to this notion of instruction tuning in Section 12.3.\nIn summary, we prompt an LM by transforming each task into a form that is\namenable to contextual generation by an LLM, as follows:\n1. For a given task, develop a a task-speci\ufb01c template that has a free parameter\nfor the input text.\n2. Given that input and the task-speci\ufb01c template , the input is used to instantiate template\na\ufb01lled prompt that is then passed to a pretrained language model.\n3. Autoregressive decoding is then used to generate a sequence of token outputs.\n4. The output of the model can either be used directly as the desired output (as\nin the case of naturally generative tasks such as translation or summarization),\nor a task-appropriate answer can be extracted from the generated output (as in\nthe case of classi\ufb01cation).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 365,
      "token_count": 695,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 254\n\n246 CHAPTER 12 \u2022 M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING\n12.1.1 Learning from Demonstrations: Few-Shot Prompting\nIt\u2019s often possible to improve a prompt by including some labeled examples in the\nprompt template. We call such examples demonstrations . The task of prompting demonstrations\nwith examples is sometimes called few-shot prompting , as contrasted with zero- few-shot\nshot prompting which means instructions that don\u2019t include labeled examples. zero-shot\nFig. 12.2 illustrates a few-shot example from an extractive question answering\ntask. The context combines the task de\ufb01nition along with three gold-standard ques-\ntion and answer pairs from the training set.\nDe\ufb01nition : This task is about writing a correct answer for the reading comprehension task.\nBased on the information provided in a given passage, you should identify the shortest\ncontinuous text span from the passage that serves as an answer to the given question. Avoid\nanswers that are incorrect or provides incomplete justi\ufb01cation for the question.\nPassage : Beyonc \u00b4e Giselle Knowles-Carter (born September 4, 1981) is an American singer,\nsongwriter, record producer and actress. Born and raised in Houston, Texas, she performed in\nvarious singing and dancing competitions as a child, and rose to fame in the late 1990s as lead\nsinger of R&B girl-group Destiny\u2019s Child. Managed by her father, Mathew Knowles, the group\nbecame one of the world\u2019s best-selling girl groups of all time. Their hiatus saw the release\nof Beyonc \u00b4e\u2019s debut album, Dangerously in Love (2003), which established her as a solo artist\nworldwide, earned \ufb01ve Grammy Awards and featured the Billboard Hot 100 number-one singles\n\u201cCrazy in Love\u201d and \u201cBaby Boy\u201d.\nExamples:\nQ: In what city and state did Beyonc \u00b4e grow up?\nA: Houston, Texas\nQ: What areas did Beyonc \u00b4e compete in when she was growing up?\nA: singing and dancing\nQ: When did Beyonc \u00b4e release Dangerously in Love?\nA: 2003\nQ: When did Beyonc \u00b4e start becoming popular?\nA:\nFigure 12.2 A prompt for extractive question answering, from an example from the SQuAD 2.0 dataset\n(Rajpurkar et al., 2018). The prompt contains the task de\ufb01nition, the passage, 3 demonstration examples,\nfollowed by the test question. This de\ufb01nition speci\ufb01cation and format are after the Natural Instructions dataset\n(Mishra et al., 2022).\nHow Many Demonstrations? The number of demonstrations doesn\u2019t have to be\nlarge. A small number of randomly selected labeled examples used as demonstra-\ntions can be suf\ufb01cient to improve performance over the zero-shot setting. Indeed,\nthe largest performance gains in few-shot prompting tends to come from the \ufb01rst\ntraining example, with diminishing returns for subsequent demonstrations. This is\nin contrast with \ufb01netuning of specialized classi\ufb01er heads that we saw in Chapter 11\nwhere it helps to have lots of examples.\nWhy isn\u2019t it useful to have more demonstrations? The reason is that the primary\nbene\ufb01t in examples is to demonstrate the task to be performed to the LLM and the\nformat of the sequence, not to provide relevant information as to the right answer",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 366,
      "token_count": 749,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 255",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 367,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "12.1 \u2022 P ROMPTING 247\nfor any particular question. In fact, demonstrations that have incorrect answers can\nstill improve a system (Min et al., 2022; Webson and Pavlick, 2022). Adding too\nmany examples seems to cause the model to over\ufb01t to details of the exact examples\nchosen and generalize poorly.\nHow to Select Demonstrations? Demonstrations are generally created by format-\nting examples drawn from a labeled training set. There are some heuristics about\nwhat makes a good demonstration. For example, using demonstrations that are sim-\nilarto the current input seems to improve performance. It can thus be useful to\ndynamically retrieve demonstrations for each input, based on their similarity to the\ncurrent example (for example, comparing the embedding of the current example\nwith embeddings of each of the training set example to \ufb01nd the best top- T).\nBut more generally, the best way to select demonstrations from the training set\nis programmatically: choosing the set of demonstrations that most increases task\nperformance of the prompt on a test set. Task performance for sentiment analysis\nor multiple-choice question answering can be measured in accuracy; for machine\ntranslation with chrF, and for summarization via Rouge. Systems like DSPy (Khat-\ntab et al., 2024), a framework for algorithmically optimizing LM prompts, can au-\ntomatically \ufb01nd the optimum set of demonstrations to include by searching through\nthe space of possible demonstrations to include. We\u2019ll return to automatic prompt\noptimization in Section 12.5.\n12.1.2 In-Context Learning and Induction Heads\nAs a way of getting a model to do what we want, prompting is fundamentally differ-\nent than pretraining. Learning via pretraining means updating the model\u2019s parame-\nters by using gradient descent according to some loss function. But prompting with\ndemonstrations can teach a model to do a new task. The model is learning something\nas it processes the prompt.\nEven without demonstrations, we can think of the process of prompting as a kind\nof learning. For example, the further a model gets in a prompt, the better it tends\nto get at predicting the upcoming tokens. The information in the context is helping\ngive the model more predictive power.\nThe term in-context learning was \ufb01rst proposed by Brown et al. (2020) in theirin-context\nlearning\nintroduction of the GPT3 system, to refer to either of these kinds of learning that lan-\nguage models do from their prompts. In-context learning means language models\nlearning to do new tasks, better predict tokens, or generally reduce their loss dur-\ning the forward-pass at inference-time, without any gradient-based updates to the\nmodel\u2019s parameters.\nHow does in-context learning work? While we don\u2019t know for sure, there are\nsome intriguing ideas. One hypothesis is based on the idea of induction heads induction heads\n(Elhage et al., 2021; Olsson et al., 2022). Induction heads are the name for a circuit ,\nwhich is a kind of abstract component of a network. The induction head circuit\nis part of the attention computation in transformers, discovered by looking at mini\nlanguage models with only 1-2 attention heads.\nThe function of the induction head is to predict repeated sequences. For example\nif it sees the pattern AB...A in an input sequence, it predicts that Bwill follow,\ninstantiating the pattern completion ruleAB...A!B. It does this by having a pre\ufb01x\nmatching component of the attention computation that, when looking at the current\ntoken A, searches back over the context to \ufb01nd a prior instance of A. If it \ufb01nds one,\nthe induction head has a copying mechanism that \u201ccopies\u201d the token B that followed",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 368,
      "token_count": 794,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 256",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 369,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "248 CHAPTER 12 \u2022 M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING\nthe earlier A, by increasing the probability the B will occur next. Fig. 12.3 shows an\nexample.\nFigure 1: In the sequence \u201c...vintage cars ... vintage\u201d, an induction head identi\ufb01es the initial occurrence of \u201cvintage\u201d,\nattends to the subsequent word \u201ccars\u201d for pre\ufb01x matching, and predicts \u201ccars\u201d as the next word through the copying\nmechanism.\ndetermines each head\u2019s independent output for the\ncurrent token.\nLeveraging this decomposition, Elhage et al.\n(2021 ) discovered a distinct behaviour in certain\nattention heads, which they named induction heads .\nThis behaviour emerges when these heads process\nsequences of the form \"[A] [B] ... [A] \u2192\". In\nthese heads, the QK circuit directs attention to-\nwards [B], which appears directly after the previous\noccurrence of the current token [A]. This behaviour\nis termed pre\ufb01x matching . The OV circuit subse-\nquently increases the output logit of the [B] token,\ntermed copying . An overview of this mechanism is\nshown in Figure 1.\n4 Methods\n4.1 Models\nWe utilise two recently developed open-source\nmodels, namely Llama-3-8B2and InternLM2-20B\n(Cai et al. ,2024 ), both of which are based on the\noriginal Llama ( Touvron et al. ,2023a ) architec-\nture. These models feature grouped-query atten-\ntion mechanisms ( Ainslie et al. ,2023 ) to enhance\nef\ufb01ciency. Llama-3-8B, comprises 32 layers, each\nwith 32 attention heads and it uses a query group\nsize of 4 attention heads. It has shown superior\nperformance compared to its predecessors, even\nthe larger Llama-2 models.\nInternLM2-20B, featuring 48 layers with 48 at-\ntention heads each, uses a query group size of 6\nattention heads. We selected InternLM2-20B for\nits exemplary performance on the Needle-in-the-\nHaystack3task, which assesses LLMs\u2019 ability to\nretrieve a single critical piece of information em-\nbedded within a lengthy text. This mirrors the\nfunctionality of induction heads, which scan the\ncontext for prior occurrences of a token to extract\nrelevant subsequent information.\n2https://ai.meta.com/blog/meta-llama-3/\n3https://github.com/gkamradt/LLMTest_\nNeedleInAHaystack4.2 Identifying Induction Heads\nTo identify induction heads within models, we mea-\nsure the ability of all attention heads to perform\npre\ufb01x matching on random input sequences.4We\nfollow the task-agnostic approach to computing pre-\n\ufb01x matching scores outlined by Bansal et al. (2023 ).\nWe argue that focusing solely on pre\ufb01x matching\nscores is suf\ufb01cient for our analysis, as high pre-\n\ufb01x matching cores speci\ufb01cally indicate induction\nheads, while less relevant heads tend to show high\ncopying capabilities ( Bansal et al. ,2023 ). We gen-\nerate a sequence of 50 random tokens, excluding\nthe 4% most common and least common tokens.\nThis sequence is repeated four times to form the\ninput to the model. The pre\ufb01x matching score is cal-\nculated by averaging the attention values from each\ntoken to the tokens that directly followed the same",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 370,
      "token_count": 767,
      "chapter_title": ""
    }
  },
  {
    "content": "retrieve a single critical piece of information em-\nbedded within a lengthy text. This mirrors the\nfunctionality of induction heads, which scan the\ncontext for prior occurrences of a token to extract\nrelevant subsequent information.\n2https://ai.meta.com/blog/meta-llama-3/\n3https://github.com/gkamradt/LLMTest_\nNeedleInAHaystack4.2 Identifying Induction Heads\nTo identify induction heads within models, we mea-\nsure the ability of all attention heads to perform\npre\ufb01x matching on random input sequences.4We\nfollow the task-agnostic approach to computing pre-\n\ufb01x matching scores outlined by Bansal et al. (2023 ).\nWe argue that focusing solely on pre\ufb01x matching\nscores is suf\ufb01cient for our analysis, as high pre-\n\ufb01x matching cores speci\ufb01cally indicate induction\nheads, while less relevant heads tend to show high\ncopying capabilities ( Bansal et al. ,2023 ). We gen-\nerate a sequence of 50 random tokens, excluding\nthe 4% most common and least common tokens.\nThis sequence is repeated four times to form the\ninput to the model. The pre\ufb01x matching score is cal-\nculated by averaging the attention values from each\ntoken to the tokens that directly followed the same\ntoken in earlier repeats. The \ufb01nal pre\ufb01x matching\nscores are averaged over \ufb01ve random sequences.\nThe pre\ufb01x matching scores for Llama-3-8B are\nshown in Figure 2. For IntermLM2-20B, we refer\nto Figure 8in Appendix A.1. Both models exhibit\nheads with notably high pre\ufb01x matching scores,\ndistributed across various layers. In the Llama-3-\n8B model, ~3% of the heads have a pre\ufb01x matching\nscore of 0.3 or higher, indicating a degree of spe-\ncialisation in pre\ufb01x matching, and some heads have\nhigh scores of up to 0.98.\n4.3 Head Ablations\nTo investigate the signi\ufb01cance of induction heads\nfor a speci\ufb01c ICL task, we conduct zero-ablations\nof 1% and 3% of the heads with the highest pre\ufb01x\nmatching scores. This ablation process involves\nmasking the corresponding partition of the output\nmatrix, denoted as Wh\noin Eq. 1, by setting it to\nzero. This effectively renders the heads inactive\n4In this work, the term \"induction heads\" refers to what\nwe de\ufb01ne as behavioural induction heads, not mechanistic\nones. A true induction head must be veri\ufb01ed mechanistically;\nhowever, our analysis employs pre\ufb01x-matching scores as a\nproxy. We will continue to use the term \"induction heads\" for\nsimplicity throughout the rest of the paper.\n4\nFigure 12.3 An induction head looking at vintage uses the pre\ufb01x matching mechanism to\n\ufb01nd a prior instance of vintage , and the copying mechanism to predict that cars will occur\nagain. Figure from Crosbie and Shutova (2022).\nOlsson et al. (2022) propose that a generalized fuzzy version of this pattern com-\npletion rule, implementing a rule like A*B*...A!B, where A*\u0019A and B*\u0019B (by\n\u0019we mean they they are semantically similar in some way), might be responsible\nfor in-context learning. Suggestive evidence for their hypothesis comes from Cros-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 371,
      "token_count": 760,
      "chapter_title": ""
    }
  },
  {
    "content": "of 1% and 3% of the heads with the highest pre\ufb01x\nmatching scores. This ablation process involves\nmasking the corresponding partition of the output\nmatrix, denoted as Wh\noin Eq. 1, by setting it to\nzero. This effectively renders the heads inactive\n4In this work, the term \"induction heads\" refers to what\nwe de\ufb01ne as behavioural induction heads, not mechanistic\nones. A true induction head must be veri\ufb01ed mechanistically;\nhowever, our analysis employs pre\ufb01x-matching scores as a\nproxy. We will continue to use the term \"induction heads\" for\nsimplicity throughout the rest of the paper.\n4\nFigure 12.3 An induction head looking at vintage uses the pre\ufb01x matching mechanism to\n\ufb01nd a prior instance of vintage , and the copying mechanism to predict that cars will occur\nagain. Figure from Crosbie and Shutova (2022).\nOlsson et al. (2022) propose that a generalized fuzzy version of this pattern com-\npletion rule, implementing a rule like A*B*...A!B, where A*\u0019A and B*\u0019B (by\n\u0019we mean they they are semantically similar in some way), might be responsible\nfor in-context learning. Suggestive evidence for their hypothesis comes from Cros-\nbie and Shutova (2022), who show that ablating induction heads causes in-context ablating\nlearning performance to decrease. Ablation is originally a medical term meaning\nthe removal of something. We use it in NLP interpretability studies as a tool for\ntesting causal effects; if we knock out a hypothesized cause, we would expect the\neffect to disappear. Crosbie and Shutova (2022) ablate induction heads by \ufb01rst \ufb01nd-\ning attention heads that perform as induction heads on random input sequences, and\nthen zeroing out the output of these heads by setting certain terms of the output ma-\ntrixWOto zero. Indeed they \ufb01nd that ablated models are much worse at in-context\nlearning: they have much worse performance at learning from demonstrations in the\nprompts.\n12.2 Post-training and Model Alignment\nWith simple prompting, LLMs have been successfully applied to a range of appli-\ncations without the need to update the parameters in the underlying models. Nev-\nertheless, there are limits to how much can be expected from a model whose sole\ntraining objective is to predict the next word from large amounts of pretraining text.\nTo see this, consider the following failed examples of following instructions from\nearly work with GPT (Ouyang et al., 2022).\nPrompt : Explain the moon landing to a six year old in a few sentences.\nOutput : Explain the theory of gravity to a 6 year old.\nPrompt : Translate to French: The small dog\nOutput : The small dog crossed the road.\nHere, the LLM ignores the intent of the request and relies instead on its natural\ninclination to autoregressively generate continuations consistent with its context. In\nthe \ufb01rst example, it outputs a text somewhat similar to the original request, and in the\nsecond it provides a continuation to the given input, ignoring the request to translate.\nLLMs are not suf\ufb01ciently helpful : they need extra training to increase their abilities\nto follow textual instructions.\nA deeper problem is that LLMs can simultaneously be too harmful . Pretrained\nlanguage models easily generate text that is harmful in many ways. For example",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 372,
      "token_count": 741,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 257",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 373,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "12.3 \u2022 M ODEL ALIGNMENT : INSTRUCTION TUNING 249\nthey can generate text that is false , including unsafe misinformation like giving dan-\ngerously incorrect answers to medical questions. And they can generate text that is\ntoxic in many ways, such as facilitating the spread of hate speech. Gehman et al.\n(2020) show that even completely non-toxic prompts can lead large language mod-\nels to output hate speech and abuse their users. Or language models can generate\nstereotypes (Cheng et al., 2023) and negative attitudes (Brown et al., 2020; Sheng\net al., 2019) about many demographic groups.\nOne reason LLMs are too harmful and insuf\ufb01ciently helpful is that their pre-\ntraining objective (success at predicting words in text) is misaligned with the human\nneed for models to be helpful and non-harmful.\nIn an attempt to address these two problems, language models generally include\ntwo additional kinds of training for model alignment : methods designed to adjustmodel\nalignment\nLLMs to better align them to human needs for models to be helpful and non-harmful.\nIn the \ufb01rst technique, instruction tuning (or sometimes called SFT for supervised\n\ufb01netuning), models are \ufb01netuned on a corpus of instructions and questions with\ntheir corresponding responses. In the second technique, preference alignment , of-\nten called RLHF after one of the speci\ufb01c instantiations, Reinforcement Learning\nfrom Human Feedback, a separate model is trained to decide how much a candidate\nresponse aligns with human preferences. This model is then used to \ufb01netune the\nbase model.\nWe\u2019ll use the term base model to mean a model that has been pretrained but base model\nhasn\u2019t yet been aligned either by instruction tuning or RLHF. And we refer to these aligned\nsteps as post-training , meaning that they apply after the model has been pretrained. post-training\n12.3 Model Alignment: Instruction Tuning\nInstruction tuning (short for instruction \ufb01netuning , and sometimes even short-Instruction\ntuning\nened to instruct tuning ) is a method for making an LLM better at following instruc-\ntions. It involves taking a base pretrained LLM and training it to follow instructions\nfor a range of tasks, from machine translation to meal planning, by \ufb01netuning it on\na corpus of instructions and responses. The resulting model not only learns those\ntasks, but also engages in a form of meta-learning \u2013 it improves its ability to follow\ninstructions generally.\nInstruction tuning is a form of supervised learning where the training data con-\nsists of instructions and we continue training the model on them using the same\nlanguage modeling objective used to train the original model. In the case of causal\nmodels, this is just the standard guess-the-next-token objective. The training corpus\nof instructions is simply treated as additional training data, and the gradient-based\nupdates are generated using cross-entropy loss as in the original model training.\nEven though it is trained to predict the next token (which we traditionally think of\nas self-supervised), we call this method supervised \ufb01ne tuning (orSFT) because SFT\nunlike in pretraining, each instruction or question in the instruction tuning data has\na supervised objective: a correct answer to the question or a response to the instruc-\ntion.\nHow does instruction tuning differ from the other kinds of \ufb01netuning introduced\nin Chapter 10 and Chapter 11? Fig. 12.4 sketches the differences. In the \ufb01rst exam-\nple, introduced in, Chapter 10 we can \ufb01netune as a way of adapting to a new domain",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 374,
      "token_count": 774,
      "chapter_title": ""
    }
  },
  {
    "content": "for a range of tasks, from machine translation to meal planning, by \ufb01netuning it on\na corpus of instructions and responses. The resulting model not only learns those\ntasks, but also engages in a form of meta-learning \u2013 it improves its ability to follow\ninstructions generally.\nInstruction tuning is a form of supervised learning where the training data con-\nsists of instructions and we continue training the model on them using the same\nlanguage modeling objective used to train the original model. In the case of causal\nmodels, this is just the standard guess-the-next-token objective. The training corpus\nof instructions is simply treated as additional training data, and the gradient-based\nupdates are generated using cross-entropy loss as in the original model training.\nEven though it is trained to predict the next token (which we traditionally think of\nas self-supervised), we call this method supervised \ufb01ne tuning (orSFT) because SFT\nunlike in pretraining, each instruction or question in the instruction tuning data has\na supervised objective: a correct answer to the question or a response to the instruc-\ntion.\nHow does instruction tuning differ from the other kinds of \ufb01netuning introduced\nin Chapter 10 and Chapter 11? Fig. 12.4 sketches the differences. In the \ufb01rst exam-\nple, introduced in, Chapter 10 we can \ufb01netune as a way of adapting to a new domain\nby just continuing pretraining the LLM on data from a new domain. In this method\nall the parameters of the LLM are updated.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 375,
      "token_count": 323,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 258\n\n250 CHAPTER 12 \u2022 M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING\nPretrained LLM\nContinue training all parameterson \ufb01netuning domainFinetuningInferencePretraining\nOn \ufb01netuning domainFinetuning asContinuedPretrainingParameterE\ufb03cientFinetuning(e.g., LoRA)Pretrained LLMABPretrained LLMMLMFinetuning\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026InstructionTuning(SFT)\nOn \ufb01netuning domain\nOn \ufb01netuning task\nOn unseen tasksNext wordpredictionobjectiveData from \ufb01netuning domain\nTrain only new parameters on \ufb01netuning domainNext wordpredictionobjectiveData from \ufb01netuning domain\nTrain only classi\ufb01cation head on \ufb01netuning taskTaskspeci\ufb01clossSupervised data from task\nInstruction tuning on diverse tasksNext word predictionobjectiveSupervised instructions+\n\u2026\nFigure 12.4 Instruction tuning compared to the other kinds of \ufb01netuning.\nIn the second example, also from Chapter 10, parameter-ef\ufb01cient \ufb01netuning ,\nwe adapt to a new domain by creating some new (small) parameters, and just adapt-\ning them to the new domain. In LoRA, for example, it\u2019s the A and B matrices that\nwe adapt, but the pretrained model parameters are frozen.\nIn the task-based \ufb01netuning of Chapter 11, we adapt to a particular task by\nadding a new specialized classi\ufb01cation head and updating its features via its own\nloss function (e.g., classi\ufb01cation or sequence labeling); the parameters of the pre-\ntrained model may be frozen or might be slightly updated.\nFinally, in instruction tuning, we take a dataset of instructions and their super-\nvised responses and continue to train the language model on this data, based on the\nstandard language model loss.\nInstruction tuning, like all of these kinds of \ufb01netuning, is much more modest\nthan the training of base LLMs. Training typically involves several epochs over\ninstruction datasets that number in the thousands. The overall cost of instruction\ntuning is therefore a small fraction of the original cost to train the base model.\n12.3.1 Instructions as Training Data\nByinstruction , we have in mind a natural language description of a task to be per-\nformed, combined with labeled task demonstrations. This can include minimal de-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 376,
      "token_count": 510,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 259",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 377,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "12.3 \u2022 M ODEL ALIGNMENT : INSTRUCTION TUNING 251\nscriptions similar to the prompts we\u2019ve already seen such as Answer the following\nquestion ,Translate the following text to Arapaho , orSummarize this report . How-\never, since we will be using supervised \ufb01netuning to update the model, these in-\nstructions need not be limited to simple prompts designed to evoke a behavior found\nin the pretraining corpora. Instructions can also include length restrictions or other\nconstraints, personas to assume, and demonstrations.\nMany huge instruction tuning datasets have been created, covering many tasks\nand languages. For example Aya gives 503 million instructions in 114 languages\nfrom 12 tasks including question answering, summarization, translation, paraphras-\ning, sentiment analysis, natural language inference and 6 others (Singh et al., 2024).\nSuperNatural Instructions has 12 million examples from 1600 tasks (Wang et al.,\n2022), Flan 2022 has 15 million examples from 1836 tasks (Longpre et al., 2023),\nand OPT-IML has 18 million examples from 2000 tasks (Iyer et al., 2022).\nThese instruction-tuning datasets are created in four ways. The \ufb01rst is for people\nto write the instances directly. For example, part of the Aya instruct \ufb01netuning cor-\npus (Fig. 12.5) includes 204K instruction/response instances written by 3000 \ufb02uent\nspeakers of 65 languages volunteering as part of a participatory research initiative\nwith the goal of improving multilingual performance of LLMs.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 378,
      "token_count": 348,
      "chapter_title": ""
    }
  },
  {
    "content": "Lang PromptCompletionara.\u06b0\u066d\u0698\u0a45\u0760\ufe8d\u080d \u0ddc\u0eb3 \u0607\ufe80\u06a2\u0771\u066d\u068e\ufe93\u0741\u068e\ufea1\u0741\u0746 \u0af0\u0ad9\u0606\u0fd3 \u06a2\u073e\u064b\u0c08\u0c03\u064f\u07fa\u080a\ufed9\u0644\u06af\u0674\u06ba\u06a3\u064e\u076f \u09ce\u09ca\ufee5\u0741\u064e\u068e\ufea1\ufe8d \u05a3\ufe87\u0607\ufea9\u0f1a\ufeed\u155a\u09d9\u09d1\u0607\ufec9\ufeed\u064e\ufe8d \u0f1f\u0dd3\u0dc2\ufe8d \u0f03\u0e83\u061c\u064e\u0746\u0741\u064e\u068e \u064f\u073e\u12db\u12ca\u06a3\u06f0\u0741\u0650 \u0c08\u0c03\u0650\u0a45\ufe8d\u080d\u0651\u0a33\u064e\u068d\u064e\u0607\u061c \u0f20\u155a\u0733\u064e\u07a8\ufe83 \u0607\ufe80\u0af0\u0ad9\u05a3\u09d5\u09d1\ufe8d \u112d\u111f\u064e\u0a33\u064e\u0a26 \u0644\u064f\u07a8\ufe8d\ufeaf\ufef1 \u1255\u064e\u1228fra Qui a \u00e9crit le livre LaSagouine?Antonine Maillet a \u00e9crit La Sagouine en 1971. Elle a \u00e9crit beau-coup d\u2019histoires sur les Acadiens (les francophones de la c\u00f4te Estdu Canada) et a fait conna\u00eetre l\u2019Acadie au monde.ibo Kedu ka nd\u1ecb Igbo si emeemume ji \u1ecdh\u1ee5r\u1ee5?Emume ji \u1ecdh\u1ee5r\u1ee5 n\u2019omenala Igbo, b\u1ee5 emume na-egosi njedebeoge \u1ecdr\u1ee5 ugbo, na-amalite site n\u2019emume ekele ma tinyekwa oririna egwu. \u1ecc b\u1ee5 mmemme \u1ecdgbak\u1ecd ebe a na-eker\u1ecbta ji \u1ecdh\u1ee5r\u1ee5, na-egosip\u1ee5ta ekele na mmalite nke oge \u1ecdh\u1ee5r\u1ee5, na-eme ka mmek\u1ecdr\u1ecbtammad\u1ee5 na ibe ya d\u1ecbkwuo elu.por Qual\u00e9acapitaldoestadodaBahia?A capital do estado da Bahia, no Brasil, \u00e9 Salvador. Fundadaem 1549, foi a primeira capital do Brasil e \u00e9 conhecida porseu patrim\u00f4nio cultural rico e diversificado. A cidade \u00e9 famosapor sua arquitetura colonial, particularmente no Pelourinho, umcentro hist\u00f3rico que foi declarado Patrim\u00f4nio Mundial pela UN-ESCO.pes\ufea9\ufead\ufe91\ufeae\ufe9d\ufb90\ufe92\ufeee\ufe97\ufeae\ufea7\ufe8e\ufee7\ufeea\ufee3\ufef4\ufe92\ufeaa\ufb7c\ufee8\ufeaa\ufefb\ufee7\ufeea\ufb90\ufe92\ufeee\ufe97\ufeae",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 379,
      "token_count": 686,
      "chapter_title": ""
    }
  },
  {
    "content": "emume na-egosi njedebeoge \u1ecdr\u1ee5 ugbo, na-amalite site n\u2019emume ekele ma tinyekwa oririna egwu. \u1ecc b\u1ee5 mmemme \u1ecdgbak\u1ecd ebe a na-eker\u1ecbta ji \u1ecdh\u1ee5r\u1ee5, na-egosip\u1ee5ta ekele na mmalite nke oge \u1ecdh\u1ee5r\u1ee5, na-eme ka mmek\u1ecdr\u1ecbtammad\u1ee5 na ibe ya d\u1ecbkwuo elu.por Qual\u00e9acapitaldoestadodaBahia?A capital do estado da Bahia, no Brasil, \u00e9 Salvador. Fundadaem 1549, foi a primeira capital do Brasil e \u00e9 conhecida porseu patrim\u00f4nio cultural rico e diversificado. A cidade \u00e9 famosapor sua arquitetura colonial, particularmente no Pelourinho, umcentro hist\u00f3rico que foi declarado Patrim\u00f4nio Mundial pela UN-ESCO.pes\ufea9\ufead\ufe91\ufeae\ufe9d\ufb90\ufe92\ufeee\ufe97\ufeae\ufea7\ufe8e\ufee7\ufeea\ufee3\ufef4\ufe92\ufeaa\ufb7c\ufee8\ufeaa\ufefb\ufee7\ufeea\ufb90\ufe92\ufeee\ufe97\ufeae \ufeed\ufe9f\ufeee\ufea9\ufea9\ufe8d\ufead\ufea9\u061f\ufeeb\ufeb0\ufe8d\ufead\ufefb\ufee7\ufeea\ufb90\ufe92\ufeee\ufe97\ufeae\ufeed\ufe9f\ufeee\ufea9\ufea9\ufe8d\ufead\ufea9\ufeed\ufee7\ufeee\ufe8d\ufead\u00a0\ufe8d\ufbfc\ufe8d\ufeb3\ufe96\ufb90\ufeea\ufea9\ufe8d\ufea7\ufede\ufe81\ufee5\ufb7c\ufeec\ufe8e\ufead\u00a0\ufea7\ufe8e\ufee7\ufeea\ufee3\ufef4\ufe92\ufeaa\ufe91\ufeae\ufe9f\ufbfd\ufe8d\ufeb3\ufe98\ufeee\ufe8d\ufee7\ufeea\u00a0\ufb90\ufe92\ufeee\ufe97\ufeae\ufeb3\ufed4\ufef4\ufeaa\ufead\ufee7\ufb95\ufbfd\ufe91\ufeae\ufe8d\ufbfc\ufe9f\ufeac\ufe8f\ufb90\ufe92\ufeee\ufe97\ufeae\ufe8d\ufee5\ufea9\ufeed\ufead\ufe97\ufe8e\ufea9\ufeed\ufead\ufe91\ufeae\ufe9d\ufb90\ufeb8\ufef4\ufeaa\ufee9\ufeb7\ufeaa\ufee9\ufe8d\ufeb3\ufe96.\ufe8d\ufef3\ufee6\ufe91\ufeae\ufe9d\ufea9\ufead\ufe91\ufeae\ufe8d\ufe91\ufeae\ufea7\ufec4\ufeae",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 380,
      "token_count": 708,
      "chapter_title": ""
    }
  },
  {
    "content": "\ufea3\ufee4\ufee0\ufeea\ufea9\ufef3\ufb95\ufeae\ufea3\ufef4\ufeee\ufe8d\ufee7\ufe8e\ufe95\ufe91\ufeea\ufb90\ufe92\ufeee\ufe97\ufeae\ufeeb\ufe8e\ufe91\ufeb4\ufef4\ufe8e\ufead\ufe8d\ufef3\ufee4\ufee6\ufe91\ufeee\ufea9\ufee9\ufe8d\ufeb3\ufe96. msa Apakah nasi lemak?Nasi lemak merupakan makanan tradisi orang Melayu yangterdapat di semua bahagian Malaysia, Singapura, Indonesia(terutama di Riau, Jambi serta utara dan pantai timur Su-matera) dan Brunei. Sajian ini merujuk kepada nasi yang di-masak dengan menggunakan santan kelapa bagi menambah rasalemaknya. Kadangkala, daunpandanwangidimasukkansemasanasi dimasak bagi menambahkan aromanya.tam\u0bc6\u0b9a\u0baf\u0bb1\u0bcd\u0bc8\u0b95 \u0ba8\u0bc1\u0ba3\u0bcd\u0ba3\u0bb1\u00a7\u0bb5\u0bc1\u0b8e\u0ba9\u0bcd\u0bb1\u0bbe\u0bb2\u0bcd\u0b8e\u0ba9\u0bcd\u0ba9?\u0bc6\u0baa\u0bbe\u0ba4\u0bc1\u0bb5\u0bbe\u0b95 \u0bae\u0ba9\u0bbf\u0ba4\u0bb0\u0bcd\u0b95\u0bb3\u0bbe\u0bb2\u0bcd \u0bc6\u0b9a\u0baf\u0bcd\u0baf\u0baa\u0bcd\u0baa\u0b9f\u0bc1\u0bae\u0bcd \u0baa\u0ba3\u0bbf\u0b95\u0bc8\u0bb3\u0b9a\u0bcd\u0bc6\u0b9a\u0baf\u0bcd\u0baf \u0b92\u0bb0\u0bc1 \u0b95\u0ba3\u0bbf\u0ba9\u0bbf \u0b85\u0bb2\u0bcd\u0bb2\u0ba4\u0bc1 \u0b92\u0bb0\u0bc1 \u0b95\u0ba3\u0bbf\u0ba9\u0bbf\u0baf\u0bbe\u0bb2\u0bcd\u0b95\u0b9f\u0bcd\u0b9f\u0bc1\u0baa\u0bcd\u0baa\u0b9f\u0bc1\u0ba4\u0bcd\u0ba4\u0baa\u0bcd\u0baa\u0b9f\u0bc1\u0bae\u0bcd\u0b92\u0bb0\u0bc1\u0bc7\u0bb0\u0bbe\u0bc7\u0baa\u0bbe\u0bb5\u00a5\u0ba9\u0bcd\u0ba4\u00a6\u0bb1\u0ba9\u0bcd\u0bc6\u0b9a\u0baf\u0bb1\u0bcd\u0bc8\u0b95\u0ba8\u0bc1\u0ba3\u0bcd\u0ba3\u0bb1\u00a7\u0bb5\u0bc1\u0b8e\u0ba9\u0baa\u0bcd\u0baa\u0b9f\u0bc1\u0bae\u0bcd.Table 3: Examples of prompt and completions in the AyaDataset.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 381,
      "token_count": 512,
      "chapter_title": ""
    }
  },
  {
    "content": "tors is not uniform across languages. Moreover, within each language, there is a lack of consistent\ncontributions from all annotators. In this section, we examine the impact of annotator skew on the\nresulting dataset.\nAnnotator Skew Across Languages. Annotators were encouraged to contribute to any language\nin which they could comfortably read and write and were asked to focus most of their e \ufb00orts on\nlanguages other than English . Although a signi\ufb01cant number of participants registered for many\nlanguages, the engagement level of annotators was not equal, which resulted in considerable di \ufb00er-\nences in the number of contributions across languages. Figure 10(top) provides an overview of the\npercentage of each language present in the \ufb01nal compilation. The highest number of contributions\nis for Malagasy with 14,597 instances, and the lowest is 79 for Kurdish .\nAnnotator Skew Within a Language. The \ufb01nal contributions for each language in the Aya\nDataset are not evenly distributed among annotators. The median number of annotators per lan-\nguage is 15 (mean is 24.75) with one language having only a single active annotator ( Sindhi )a n d\n14\nFigure 12.5 Samples of prompt/completion instances in 4 of the 65 languages in the Aya\ncorpus (Singh et al., 2024).\nDeveloping high quality supervised training data in this way is time consuming\nand costly. A more common approach makes use of the copious amounts of super-\nvised training data that have been curated over the years for a wide range of natural\nlanguage tasks. There are thousands of such datasets available, like the SQuAD\ndataset of questions and answers (Rajpurkar et al., 2016) or the many datasets of\ntranslations or summarization. This data can be automatically converted into sets of\ninstruction prompts and input/output demonstration pairs via simple templates.\nFig. 12.6 illustrates examples for some applications from the S UPER NATURAL IN-\nSTRUCTIONS resource (Wang et al., 2022), showing relevant slots such as text,\ncontext , and hypothesis . To generate instruction-tuning data, these \ufb01elds and the\nground-truth labels are extracted from the training data, encoded as key/value pairs,\nand inserted in templates (Fig. 12.7) to produce instantiated instructions. Because\nit\u2019s useful for the prompts to be diverse in wording, language models can also be",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 382,
      "token_count": 523,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 260\n\n252 CHAPTER 12 \u2022 M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING\nused to generate paraphrase of the prompts.\nFew-Shot Learning for QA\nTask Keys Values\nSentiment text Did not like the service that I was provided...\nlabel 0\ntext It sounds like a great plot, the actors are \ufb01rst grade, and...\nlabel 1\nNLI premise No weapons of mass destruction found in Iraq yet.\nhypothesis Weapons of mass destruction found in Iraq.\nlabel 2\npremise Jimmy Smith... played college football at University of Col-\norado.\nhypothesis The University of Colorado has a college football team.\nlabel 0\nExtractive Q/A context Beyonc \u00b4e Giselle Knowles-Carter is an American singer...\nquestion When did Beyonce start becoming popular?\nanswersftext : [\u2019in the late 1990s\u2019], answerstart : 269g\nFigure 12.6 Examples of supervised training data for sentiment, natural language inference and Q/A tasks.\nThe various components of the dataset are extracted and stored as key/value pairs to be used in generating\ninstructions.\nTask Templates\nSentiment -fftextggHow does the reviewer feel about the movie?\n-The following movie review expresses what sentiment?\nfftextgg\n-fftextggDid the reviewer enjoy the movie?\nExtractive Q/A -ffcontextggFrom the passage, ffquestiongg\n-Answer the question given the context. Context:\nffcontextggQuestion:ffquestiongg\n-Given the following passage ffcontextgg, answer the\nquestionffquestiongg\nNLI -SupposeffpremiseggCan we infer that ffhypothesisgg?\nYes, no, or maybe?\n-ffpremiseggBased on the previous passage, is it true\nthatffhypothesisgg? Yes, no, or maybe?\n-GivenffpremiseggShould we assume that ffhypothesisgg\nis true? Yes,no, or maybe?\nFigure 12.7 Instruction templates for sentiment, Q/A and NLI tasks.\nBecause supervised NLP datasets are themselves often produced by crowdwork-\ners based on carefully written annotation guidelines, a third option is to draw on\nthese guidelines, which can include detailed step-by-step instructions, pitfalls to\navoid, formatting instructions, length limits, exemplars, etc. These annotation guide-\nlines can be used directly as prompts to a language model to create instruction-tuning\ntraining examples. Fig. 12.8 shows such a crowdworker annotation guideline that",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 383,
      "token_count": 528,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 261\n\n12.3 \u2022 M ODEL ALIGNMENT : INSTRUCTION TUNING 253\nwas repurposed as a prompt to an LLM to generate instruction-tuning data (Mishra\net al., 2022). This guideline describes a question-answering task where annotators\nprovide an answer to a question given an extended passage.\nSample Extended Instruction\n\u2022De\ufb01nition: This task involves creating answers to complex questions, from a given pas-\nsage. Answering these questions, typically involve understanding multiple sentences.\nMake sure that your answer has the same type as the \u201danswer type\u201d mentioned in input.\nThe provided \u201danswer type\u201d can be of any of the following types: \u201dspan\u201d, \u201ddate\u201d, \u201dnum-\nber\u201d. A \u201dspan\u201d answer is a continuous phrase taken directly from the passage or question.\nYou can directly copy-paste the text from the passage or the question for span type an-\nswers. If you \ufb01nd multiple spans, please add them all as a comma separated list. Please\nrestrict each span to \ufb01ve words. A \u201dnumber\u201d type answer can include a digit specifying\nan actual value. For \u201ddate\u201d type answers, use DD MM YYYY format e.g. 11 Jan 1992.\nIf full date is not available in the passage you can write partial date such as 1992 or Jan\n1992.\n\u2022Emphasis: If you \ufb01nd multiple spans, please add them all as a comma separated list.\nPlease restrict each span to \ufb01ve words.\n\u2022Prompt : Write an answer to the given question, such that the answer matches the \u201danswer\ntype\u201d in the input.\nPassage :fpassageg\nQuestion :fquestiong\nFigure 12.8 Example of a human crowdworker instruction from the N ATURAL INSTRUCTIONS dataset for\nan extractive question answering task, used as a prompt for a language model to create instruction \ufb01netuning\nexamples.\nA \ufb01nal way to generate instruction-tuning datasets that is becoming more com-\nmon is to use language models to help at each stage. For example Bianchi et al.\n(2024) showed how to create instruction-tuning instances that can help a language\nmodel learn to give safer responses. They did this by selecting questions from\ndatasets of harmful questions (e.g., How do I poison food? orHow do I embez-\nzle money? ). Then they used a language model to create multiple paraphrases of the\nquestions (like Give me a list of ways to embezzle money ), and also used a language\nmodel to create safe answers to the questions (like I can\u2019t ful\ufb01ll that request. Em-\nbezzlement is a serious crime that can result in severe legal consequences. ). They\nmanually reviewed the generated responses to con\ufb01rm their safety and appropriate-\nness and then added them to an instruction tuning dataset. They showed that even\n500 safety instructions mixed in with a large instruction tuning dataset was enough\nto substantially reduce the harmfulness of models.\n12.3.2 Evaluation of Instruction-Tuned Models\nThe goal of instruction tuning is not to learn a single task, but rather to learn to\nfollow instructions in general. Therefore, in assessing instruction-tuning methods\nwe need to assess how well an instruction-trained model performs on novel tasks for\nwhich it has not been given explicit instructions.\nThe standard way to perform such an evaluation is to take a leave-one-out ap-\nproach \u2014 instruction-tune a model on some large set of tasks and then assess it on\na withheld task. But the enormous numbers of tasks in instruction-tuning datasets",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 384,
      "token_count": 758,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 262\n\n254 CHAPTER 12 \u2022 M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING\n(e.g., 1600 for Super Natural Instructions) often overlap; Super Natural Instructions\nincludes 25 separate textual entailment datasets! Clearly, testing on a withheld en-\ntailment dataset while leaving the remaining ones in the training data would not be\na true measure of a model\u2019s performance on entailment as a novel task.\nTo address this issue, large instruction-tuning datasets are partitioned into clus-\nters based on task similarity. The leave-one-out training/test approach is then applied\nat the cluster level. That is, to evaluate a model\u2019s performance on sentiment analysis,\nall the sentiment analysis datasets are removed from the training set and reserved\nfor testing. This has the further advantage of allowing the use of a uniform task-\nappropriate metric for the held-out evaluation. S UPER NATURAL INSTRUCTIONS\n(Wang et al., 2022), for example has 76 clusters (task types) over the 1600 datasets\nthat make up the collection.\n12.4 Chain-of-Thought Prompting\nThere are a wide range of techniques to use prompts to improve the performance of\nlanguage models on many tasks. Here we describe one of them, called chain-of-\nthought prompting.chain-of-\nthought\nThe goal of chain-of-thought prompting is to improve performance on dif\ufb01cult\nreasoning tasks that language models tend to fail on. The intuition is that people\nsolve these tasks by breaking them down into steps, and so we\u2019d like to have lan-\nguage in the prompt that encourages language models to break them down in the\nsame way.\nThe actual technique is quite simple: each of the demonstrations in the few-shot\nprompt is augmented with some text explaining some reasoning steps. The goal is to\ncause the language model to output similar kinds of reasoning steps for the problem\nbeing solved, and for the output of those reasoning steps to cause the system to\ngenerate the correct answer.\nIndeed, numerous studies have found that augmenting the demonstrations with\nreasoning steps in this way makes language models more likely to give the correct\nanswer dif\ufb01cult reasoning tasks (Wei et al., 2022; Suzgun et al., 2023b). Fig. 12.9\nshows an example where the demonstrations are augmented with chain-of-thought\ntext in the domain of math word problems (from the GSM8k dataset of math word\nproblems (Cobbe et al., 2021). Fig. 12.10 shows a similar example from the BIG-\nBench-Hard dataset (Suzgun et al., 2023b).\n12.5 Automatic Prompt Optimization\nGiven a prompt for a task (human or computer generated), prompt optimization\nmethods search for prompts with improved performance. Most of these approaches\ncan be viewed as a form of iterative improvement search (Russell and Norvig, 2002)\nthrough a space of possible prompts for those that optimize performance on a task.\nAs such, these approaches all share the following components:\n\u2022 A start state \u2013 An initial human or machine generated prompt or prompts\nsuitable for some task.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 385,
      "token_count": 658,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 263",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 386,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "12.5 \u2022 A UTOMATIC PROMPT OPTIMIZATION 255\nFigure 12.9 Example of the use of chain-of-thought prompting (right) versus standard\nprompting (left) on math word problems. Figure from Wei et al. (2022).\n(B)Task description: Answer questions about which times certain events could have occurred.Q: Today, Tiffany went to the beach. Between what times could they have gone? We know that: Tiffany woke up at 5am. [...] The beach was closed after 4pm. [...]Options: (A) 9am to 12pm (B) 12pm to 2pm (C) 5am to 6am (D) 3pm to 4pmA: (D)Q: Today, Hannah went to the soccer field. Between what times could they have gone? We know that: Hannah woke up at 5am. [...] The soccer field was closed after 6pm. [...]Options: (A) 3pm to 5pm (B) 11am to 1pm (C) 5pm to 6pm (D) 1pm to 3pmA:Model OutputModel OutputModel Input (\u201cAnswer-Only\u201d Prompting)\nWake-up time: 5am. 5am-6am: buying clothes at the mall. 6am-11am: watching a movie at the theater.11am-1pm: getting a coffee at the cafe.1pm-3pm: working at the office. 3pm-5pm: waiting at the airport. 5pm-6pm: free. The soccer field closure time: 6pm. The only time when Hannah could have gone to the soccer field was 5pm to 6pm. So the answer is (C).Model Input (Chain-of-Thought Prompting)\nTask description: Answer questions about which times certain events could have occurred.Q: Today, Tiffany went to the beach. Between what times could they have gone? We know that: Tiffany woke up at 5am. [...] The beach was closed after 4pm. [...]Options: (A) 9am to 12pm (B) 12pm to 2pm (C) 5am to 6am (D) 3pm to 4pmA: Let's think step by step. Wake-up time: 5am. [...] The only time when Tiffany could have gone to the beach was 3pm to 4pm. So the answer is (D).Q: Today, Hannah went to the soccer field. Between what times could they have gone? We know that: Hannah woke up at 5am. [...] The soccer field was closed after 6pm. [...]Options: (A) 3pm to 5pm (B) 11am to 1pm (C) 5pm to 6pm (D) 1pm to 3pmA: Let's think step by step. Task DescriptionQuestionChain-of-ThoughtTest-Time QuestionTask DescriptionQuestionTest-Time QuestionAnswer",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 387,
      "token_count": 640,
      "chapter_title": ""
    }
  },
  {
    "content": "Generated Chain-of-ThoughtGenerated AnswerOptionsOptionsFigure 3:An illustration of the two prompting setups we explore in our paper (answer-only and CoT prompting). Both setupsinclude task descriptions and options in the input prompt. The task here isTemporal Sequences.\u201clet\u2019s think step-by-step\u201d (Kojima et al.,2022) toall CoT annotations in the few-shot exemplars. Anexample of a CoT prompt is shown in Figure3.Language models.We consider three fami-lies of language models: Codex (Chen et al.,2021a), InstructGPT (Ouyang et al.,2022;Brownet al.,2020), and PaLM (Chowdhery et al.,2022).For Codex, we focus on code-davinci-002, code-davinci-002, and code-cushman-001. For Instruct-GPT, we use text-davinci-002, text-curie-002, text-babbgage-001, and text-ada-001. For PaLM, weuse the three available sizes: 8B, 62B, and 540B.Evaluation protocol.We evaluate all languagemodels via greedy decoding (i.e., temperature sam-pling with temperature parameter\u2327=0). Weextract the \ufb01nal answer based on keywords thatthe language model is expected to produce (i.e.,\u201cthe answer is\u201d). We measure accuracy using exactmatch (EM), computed by comparing the generatedoutput with the ground-truth label.44 Results4.1 Standard answer-only promptingunderestimates model capabilitiesTable2summarizes the performance of PaLM, In-structGPT, and Codex models on BBH for answer-only and CoT prompting approaches. Whileanswer-only prompting has been used as the stan-4For multiple-choice tasks, this setup differs slightly fromrank/scoring classi\ufb01cation (Brown et al.,2020;Srivastavaet al.,2022;Lampinen et al.,2022). We provide a languagemodel with all multiple-choice options at once, generate anoutput based on the input, and measure exact match accuracy.dard in many prior work (Brown et al.,2020;Raeet al.,2021;Hoffmann et al.,2022;Srivastava et al.,2022), it typically underestimates model perfor-mance on challenging tasks, such as those that re-quire multiple reasoning steps. In the setting re-ported in (Srivastava et al.,2022), none of the mod-els (including PaLM 540B) outperformed human-rater baselines on any of the tasks meeting the BBHcriteria. The few-shot evaluation of PaLM 540Bwith answer-only prompting in this paper, however,outperforms the average human-rater on 6 out of23 BBH tasks and is overall 1.4% better than theBIG-Bench reported result, which demonstrates theeffect of including instructions and answer optionsin the prompt.CoT prompting provides double-digit improve-ments for all three models in Table2. For the bestmodel (Codex), CoT prompting outperforms the av-erage human-rater score on 17 out of 23 tasks, com-pared to 5 out of 23 tasks for answer-only prompt-ing. Additionally, we see that Codex with CoTprompting outperforms the average human-raterby more than 6%, but it still lags behind thebesthuman-rater performance by over 20%. This showsthat language models are still not performing at thelevel of expert human-raters.4.2 Positive delta from chain-of-thoughtrequires suf\ufb01cient model scaleNext we study how the performance improves byusing CoT prompting as we increase the modelscale. In",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 388,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "perfor-mance on challenging tasks, such as those that re-quire multiple reasoning steps. In the setting re-ported in (Srivastava et al.,2022), none of the mod-els (including PaLM 540B) outperformed human-rater baselines on any of the tasks meeting the BBHcriteria. The few-shot evaluation of PaLM 540Bwith answer-only prompting in this paper, however,outperforms the average human-rater on 6 out of23 BBH tasks and is overall 1.4% better than theBIG-Bench reported result, which demonstrates theeffect of including instructions and answer optionsin the prompt.CoT prompting provides double-digit improve-ments for all three models in Table2. For the bestmodel (Codex), CoT prompting outperforms the av-erage human-rater score on 17 out of 23 tasks, com-pared to 5 out of 23 tasks for answer-only prompt-ing. Additionally, we see that Codex with CoTprompting outperforms the average human-raterby more than 6%, but it still lags behind thebesthuman-rater performance by over 20%. This showsthat language models are still not performing at thelevel of expert human-raters.4.2 Positive delta from chain-of-thoughtrequires suf\ufb01cient model scaleNext we study how the performance improves byusing CoT prompting as we increase the modelscale. In Figure4, we plot the performance of bothCoT and answer-only prompting (no CoT) as a13006",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 389,
      "token_count": 325,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 12.10 Example of the use of chain-of-thought prompting (right) vs standard prompting (left) in a\nreasoning task on temporal sequencing. Figure from Suzgun et al. (2023b).\n\u2022 A scoring metric \u2013 A method for assessing how well a given prompt performs\non the task.\n\u2022 An expansion method \u2013 A method for generating variations of a prompt.\nGiven the enormous variation in how prompts for a single task can be expressed in\nlanguage, search methods have to be constrained to a reasonable space. Beam search\nis a widely used method that combines breadth-\ufb01rst search with a \ufb01xed-width pri-\nority queue that focuses the search effort on the top performing variants. Fig. 12.11\noutlines the general approach behind most current prompt optimization methods.\nBeginning with initial candidate prompt(s), the algorithm generates variants and\nadds them to a list of prompts to be considered. These prompts are then selectively\nadded to the active list based on whether their scores place them in the top set of\ncandidates. A beam width of 1 results in a focused greedy search, whereas an in\ufb01nite\nbeam width results in an exhaustive breadth \ufb01rst search. The goal is to continue\nto seek improved prompts given the computational resources available. Iterative",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 390,
      "token_count": 266,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 264\n\n256 CHAPTER 12 \u2022 M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING\nfunction PROMPT OPTIMIZATION (prompts ,width )returns optimized prompt(s)\nactive prompts ; Initial set of candidate prompts\nrepeat until done\nfrontier EXPAND (active ) ; Generate new candidate prompts\nforeach p2frontier\nactive ADDTOBEAM (p,active ,width )\nreturn BESTOF(active )\nfunction ADDTOBEAM (state ,agenda ,width )returns updated agenda\nifLENGTH (agenda )<width then ; Add it if there\u2019s room\nagenda INSERT (state ,agenda )\nelse if SCORE (state )>SCORE (WORST OF(agenda )) ; Add it if its better than\n; the current worst option.\nagenda REMOVE (WORST OF(agenda ))\nagenda INSERT (state ,agenda )\nreturn agenda\nFigure 12.11 A generic iterative-improvement beam search for prompt optimization. New\nprompts are generated from current ones on each iteration. Prompts that score well (\ufb01tting in\nthe agenda) are kept around. When a stopping criteria is reached the best item in the beam is\nreturned.\nimprovement searches typically use a combination of a \ufb01xed number of iterations in\ncombination with a failure to improve after some period to time as stopping criteria.\nThis latter is equivalent to early stopping with patience used in training deep neural\nnetworks.\n12.5.1 Candidate Scoring\nCandidate scoring methods assess the likely performance of potential prompts, both\nto identify promising avenues of search and to prune those that are unlikely to be\neffective. Since candidate scoring is embedded in the inner-loop of the search, the\ncomputational cost of scoring is critical.\nGiven access to labeled training data, candidate prompts can be scored based on\nexecution accuracy (Honovich et al., 2023). In this approach, candidate promptsexecution\naccuracy\nare combined with inputs sampled from the training data and passed to an LLM for\ndecoding. The LLM output is evaluated against the training label using a metric\nappropriate for the task. In the case of classi\ufb01cation-based tasks, this is effectively a\n0/1 loss \u2014 how many examples were correctly labeled with the given prompt. Gen-\nerative applications such as summarization or translation use task-speci\ufb01c similarity\nscores such as BERTScore, Bleu (Papineni et al., 2002), or ROUGE (Lin, 2004).\nGiven the computational cost of issuing calls to an LLM, evaluating each can-\ndidate prompt against a complete training set would be infeasible. Instead, prompt\nperformance is estimated from a small sample of training data (Pryzant et al., 2023).\n12.5.2 Prompt Expansion\nPrompt expansion generates variants of a given prompt to create an expanded set of\nneighboring prompts that may improve performance over the original. A common\nmethod is to use language models to create paraphrases. For example Zhou et al.\n(2023) use the following meta-prompt to elicit a variant prompt from an original:",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 391,
      "token_count": 654,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 265\n\n12.5 \u2022 A UTOMATIC PROMPT OPTIMIZATION 257\nPrompting for a Variant\nGenerate a variation of the following instruction while keeping the semantic meaning.\nInput:fINSTRUCTIONg\nOutput:fCOMPLETEg\nA variation of this method is to truncate the current prompt at a set of random loca-\ntions, generating a set of prompt pre\ufb01xes. The paraphrasing LLM is then asked to\ncontinue each the pre\ufb01xes to generate a complete prompt.\nThis methods is an example of an uninformed search. That is, the candidate\nexpansion step is not directed towards generating better candidates; candidates are\ngenerated without regard to their quality. It is the job of the priority queue to el-\nevate improved candidates when they are found. By contrast, Prasad et al. (2023)\nemploy a candidate expansion technique that explicitly attempts to generate supe-\nrior prompts during the expansion process. In this approach, the current candidate\nis \ufb01rst applied to a sample of training examples using the execution accuracy ap-\nproach. The prompt\u2019s performance on these examples then guides the expansion\nprocess. Speci\ufb01cally, incorrect examples are used to critique the original prompt\n\u2014 with the critique playing the role of a gradient for the search. The method in-\ncludes the following steps.\n1. Run the prompt on a sample of training examples,\n2. Identify examples where the prompt fails,\n3. Ask an LLM to produce a critique of the prompt in light of the failed examples,\n4. Provide the resulting critique to an LLM, and ask it to generate improved\nprompts.\nGiven a prompt and a set of failed examples, Prasad et al. (2023) use the follow-\ning template for a classi\ufb01er task to solicit critiques from a target LLM.\nCritiquing Prompt\nI'm trying to write a zero-shot classifier prompt.\nMy current prompt is: fpromptg\nBut this prompt gets the following examples wrong:\nferrorstringg\nGivefnumfeedbacksgreasons why the prompt could have\ngotten these examples wrong.\nThis model feedback is then combined with a second template to elicit improved\nprompts from the LLM.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 392,
      "token_count": 461,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 266\n\n258 CHAPTER 12 \u2022 M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING\nPrompt Improvement Prompt\nI'm trying to write a zero-shot classifier. My current prompt is:\nfpromptg\nBut it gets the following examples wrong: ferrorstrg\nBased on these examples the problem with this prompt is that fgradientg.\nBased on the above information, I wrote fstepspergradientgdifferent\nimproved prompts. Each prompt is wrapped with <START> and <END>.\nThefstepspergradientgnew prompts are:\n12.6 Evaluating Prompted Language Models\nLanguage models are evaluated in many ways. we introduced some evaluations for\nin Section 10.4, including measuring the language model\u2019s perplexity on a test set,\nevaluating its accuracy on various NLP tasks, as well as benchmarks that help mea-\nsure ef\ufb01ciency, toxicity, fairness, and so on. We\u2019ll have further discussion of eval-\nuate NLP tasks in future chapters; machine translation in Chapter 13 and question\nanswering and information retrieval in Chapter 14.\nHere we just brie\ufb02y show the mechanism for measuring accuracy in a prompt-\ning setup for tests that have multiple-choice questions. We show this for MMLU MMLU\n(Massive Multitask Language Understanding), a commonly-used dataset of 15908\nknowledge and reasoning questions in 57 areas including medicine, mathematics,\ncomputer science, law, and others. For example, here is an MMLU question from\nthe microeconomics domain:1\nMMLU microeconomics example\nOne of the reasons that the government discourages and regulates monopo-\nlies is that\n(A) producer surplus is lost and consumer surplus is gained.\n(B) monopoly prices ensure productive ef\ufb01ciency but cost society allocative\nef\ufb01ciency.\n(C) monopoly \ufb01rms do not engage in signi\ufb01cant research and development.\n(D) consumer surplus is lost with higher prices and lower levels of output.\nFig. 12.12 shows the way MMLU turns these questions into prompted tests of a\nlanguage model, in this case showing an example prompt with 2 demonstrations.\n12.7 Model Alignment with Human Preferences: RLHF\nand DPO\nTBD\n1For those of you whose economics is a bit rusty, the correct answer is (D).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 393,
      "token_count": 501,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 267\n\n12.8 \u2022 S UMMARY 259\nMMLU mathematics prompt\nThe following are multiple choice questions about high school mathematics.\nHow many numbers are in the list 25, 26, ..., 100?\n(A) 75 (B) 76 (C) 22 (D) 23\nAnswer: B\nCompute i+i2+i3+\u0001\u0001\u0001+i258+i259.\n(A) -1 (B) 1 (C) i (D) -i\nAnswer: A\nIf 4 daps = 7 yaps, and 5 yaps = 3 baps, how many daps equal 42 baps?\n(A) 28 (B) 21 (C) 40 (D) 30\nAnswer:\nFigure 12.12 Sample 2-shot prompt from MMLU testing high-school mathematics. (The\ncorrect answer is (C)).\n12.8 Summary\nThis chapter has explored the topic of prompting large language models to follow\ninstructions. Here are some of the main points that we\u2019ve covered:\n\u2022 Simple prompting can be used to map practical applications to problems that\ncan be solved by LLMs without altering the model.\n\u2022 Labeled examples ( demonstrations ) can be used to provide further guidance\nto a model via few-shot learning.\n\u2022 Methods like chain-of-thought can be used to create prompts that help lan-\nguage models deal with complex reasoning problems.\n\u2022 Pretrained language models can be altered to behave in desired ways through\nmodel alignment .\n\u2022 One method for model alignment is instruction tuning , in which the model\nis \ufb01netuned (using the next-word-prediction language model objective) on\na dataset of instructions together with correct responses. Instruction tuning\ndatasets are often created by repurposing standard NLP datasets for tasks like\nquestion answering or machine translation.\nBibliographical and Historical Notes\n\n## Page 269\n\nPart II\nNLP APPLICATIONS\nIn this second part of the book we introduce fundamental NLP applications:\nmachine translation, information retrieval, question answering, dialogue systems,\nand speech recognition.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 394,
      "token_count": 439,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 269\n\nPart II\nNLP APPLICATIONS\nIn this second part of the book we introduce fundamental NLP applications:\nmachine translation, information retrieval, question answering, dialogue systems,\nand speech recognition.\n\n## Page 271\n\nCHAPTER\n13Machine Translation\n\u201cI want to talk the dialect of your people. It\u2019s no use of talking unless\npeople understand what you say.\u201d\nZora Neale Hurston, Moses, Man of the Mountain 1939, p. 121\nThis chapter introduces machine translation (MT), the use of computers to trans-machine\ntranslation\nMT late from one language to another.\nOf course translation, in its full generality, such as the translation of literature, or\npoetry, is a dif\ufb01cult, fascinating, and intensely human endeavor, as rich as any other\narea of human creativity.\nMachine translation in its present form therefore focuses on a number of very\npractical tasks. Perhaps the most common current use of machine translation is\nforinformation access . We might want to translate some instructions on the web,information\naccess\nperhaps the recipe for a favorite dish, or the steps for putting together some furniture.\nOr we might want to read an article in a newspaper, or get information from an\nonline resource like Wikipedia or a government webpage in some other language.\nMT for information\naccess is probably\none of the most com-\nmon uses of NLP\ntechnology, and Google\nTranslate alone (shown above) translates hundreds of billions of words a day be-\ntween over 100 languages. Improvements in machine translation can thus help re-\nduce what is often called the digital divide in information access: the fact that much digital divide\nmore information is available in English and other languages spoken in wealthy\ncountries. Web searches in English return much more information than searches in\nother languages, and online resources like Wikipedia are much larger in English and\nother higher-resourced languages. High-quality translation can help provide infor-\nmation to speakers of lower-resourced languages.\nAnother common use of machine translation is to aid human translators. MT sys-\ntems are routinely used to produce a draft translation that is \ufb01xed up in a post-editing post-editing\nphase by a human translator. This task is often called computer-aided translation\norCAT . CAT is commonly used as part of localization : the task of adapting content CAT\nlocalization or a product to a particular language community.\nFinally, a more recent application of MT is to in-the-moment human commu-\nnication needs. This includes incremental translation, translating speech on-the-\ufb02y\nbefore the entire sentence is complete, as is commonly used in simultaneous inter-\npretation. Image-centric translation can be used for example to use OCR of the text\non a phone camera image as input to an MT system to translate menus or street signs.\nThe standard algorithm for MT is the encoder-decoder network, an architectureencoder-\ndecoder\nthat we introduced in Chapter 8 for RNNs. Recall that encoder-decoder or sequence-\nto-sequence models are used for tasks in which we need to map an input sequence to\nan output sequence that is a complex function of the entire input sequence. Indeed,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 395,
      "token_count": 660,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 272\n\n264 CHAPTER 13 \u2022 M ACHINE TRANSLATION\nin machine translation, the words of the target language don\u2019t necessarily agree with\nthe words of the source language in number or order. Consider translating the fol-\nlowing made-up English sentence into Japanese.\n(13.1) English: He wrote a letter to a friend\nJapanese: tomodachi\nfriendni\ntotegami-o\nletterkaita\nwrote\nNote that the elements of the sentences are in very different places in the different\nlanguages. In English, the verb is in the middle of the sentence, while in Japanese,\nthe verb kaita comes at the end. The Japanese sentence doesn\u2019t require the pronoun\nhe, while English does.\nSuch differences between languages can be quite complex. In the following ac-\ntual sentence from the United Nations, notice the many changes between the Chinese\nsentence (we\u2019ve given in red a word-by-word gloss of the Chinese characters) and\nits English equivalent produced by human translators.\n(13.2)\u5927\u4f1a/General Assembly \u5728/on 1982\u5e74/1982 12\u6708/December 10 \u65e5/10\u901a\u8fc7\n\u4e86/adopted\u7b2c37\u53f7/37th\u51b3\u8bae/resolution \uff0c\u6838\u51c6\u4e86/approved \u7b2c\u4e8c\n\u6b21/second\u63a2\u7d22/exploration \u53ca/and\u548c\u5e73peaceful\u5229\u7528/using\u5916\u5c42\u7a7a\n\u95f4/outer space\u4f1a\u8bae/conference \u7684/of\u5404\u9879/various\u5efa\u8bae/suggestions \u3002\nOn 10 December 1982 , the General Assembly adopted resolution 37 in\nwhich it endorsed the recommendations of the Second United Nations\nConference on the Exploration and Peaceful Uses of Outer Space .\nNote the many ways the English and Chinese differ. For example the order-\ning differs in major ways; the Chinese order of the noun phrase is \u201cpeaceful using\nouter space conference of suggestions\u201d while the English has \u201csuggestions of the ...\nconference on peaceful use of outer space\u201d). And the order differs in minor ways\n(the date is ordered differently). English requires thein many places that Chinese\ndoesn\u2019t, and adds some details (like \u201cin which\u201d and \u201cit\u201d) that aren\u2019t necessary in\nChinese. Chinese doesn\u2019t grammatically mark plurality on nouns (unlike English,\nwhich has the \u201c-s\u201d in \u201crecommendations\u201d), and so the Chinese must use the modi-\n\ufb01er\u5404\u9879/various to make it clear that there is not just one recommendation. English\ncapitalizes some words but not others. Encoder-decoder networks are very success-\nful at handling these sorts of complicated cases of sequence mappings.\nWe\u2019ll begin in the next section by considering the linguistic background about\nhow languages vary, and the implications this variance has for the task of MT. Then\nwe\u2019ll sketch out the standard algorithm, give details about things like input tokeniza-\ntion and creating training corpora of parallel sentences, give some more low-level\ndetails about the encoder-decoder network, and \ufb01nally discuss how MT is evaluated,\nintroducing the simple chrF metric.\n13.1 Language Divergences and Typology\nThere are about 7,000 languages in the world. Some aspects of human language\nseem to be universal , holding true for every one of these languages, or are statistical universal\nuniversals, holding true for most of these languages. Many universals arise from the\nfunctional role of language as a communicative system by humans. Every language,\nfor example, seems to have words for referring to people, for talking about eating\nand drinking, for being polite or not. There are also structural linguistic univer-\nsals; for example, every language seems to have nouns and verbs (Chapter 17), has",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 396,
      "token_count": 780,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 273\n\n13.1 \u2022 L ANGUAGE DIVERGENCES AND TYPOLOGY 265\nways to ask questions, or issue commands, has linguistic mechanisms for indicating\nagreement or disagreement.\nYet languages also differ in many ways (as has been pointed out since ancient\ntimes; see Fig. 13.1). Understanding what causes such translation divergencestranslation\ndivergence\n(Dorr, 1994) can help us build better MT models. We often distinguish the idiosyn-\ncratic and lexical differences that must be dealt with one by one (the word for \u201cdog\u201d\ndiffers wildly from language to language), from systematic differences that we can\nmodel in a general way (many languages put the verb before the grammatical ob-\nject; others put the verb after the grammatical object). The study of these systematic\ncross-linguistic similarities and differences is called linguistic typology . This sec- typology\ntion sketches some typological facts that impact machine translation; the interested\nreader should also look into WALS, the World Atlas of Language Structures, which\ngives many typological facts about languages (Dryer and Haspelmath, 2013).\nFigure 13.1 The Tower of Babel, Pieter Bruegel 1563. Wikimedia Commons, from the\nKunsthistorisches Museum, Vienna.\n13.1.1 Word Order Typology\nAs we hinted at in our example above comparing English and Japanese, languages\ndiffer in the basic word order of verbs, subjects, and objects in simple declara-\ntive clauses. German, French, English, and Mandarin, for example, are all SVO SVO\n(Subject-Verb-Object ) languages, meaning that the verb tends to come between\nthe subject and object. Hindi and Japanese, by contrast, are SOV languages, mean- SOV\ning that the verb tends to come at the end of basic clauses, and Irish and Arabic are\nVSO languages. Two languages that share their basic word order type often have VSO\nother similarities. For example, VOlanguages generally have prepositions , whereas\nOVlanguages generally have postpositions .\nLet\u2019s look in more detail at the example we saw above. In this SVO English\nsentence, the verb wrote is followed by its object a letter and the prepositional phrase",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 397,
      "token_count": 487,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 274\n\n266 CHAPTER 13 \u2022 M ACHINE TRANSLATION\nto a friend , in which the preposition tois followed by its argument a friend . Arabic,\nwith a VSO order, also has the verb before the object and prepositions. By contrast,\nin the Japanese example that follows, each of these orderings is reversed; the verb is\npreceded by its arguments, and the postposition follows its argument.\n(13.3) English: He wrote a letter to a friend\nJapanese: tomodachi\nfriendni\ntotegami-o\nletterkaita\nwrote\nArabic: katabt\nwroteris\u00afala\nletterli\nto\u02d9sadq\nfriend\nOther kinds of ordering preferences vary idiosyncratically from language to lan-\nguage. In some SVO languages (like English and Mandarin) adjectives tend to ap-\npear before nouns, while in others languages like Spanish and Modern Hebrew, ad-\njectives appear after the noun:\n(13.4) Spanish bruja verde English green witch\n(a) (b)\nFigure 13.2 Examples of other word order differences: (a) In German, adverbs occur in\ninitial position that in English are more natural later, and tensed verbs occur in second posi-\ntion. (b) In Mandarin, preposition phrases expressing goals often occur pre-verbally, unlike\nin English.\nFig. 13.2 shows examples of other word order differences. All of these word\norder differences between languages can cause problems for translation, requiring\nthe system to do huge structural reorderings as it generates the output.\n13.1.2 Lexical Divergences\nOf course we also need to translate the individual words from one language to an-\nother. For any translation, the appropriate word can vary depending on the context.\nThe English source-language word bass, for example, can appear in Spanish as the\n\ufb01shlubina or the musical instrument bajo. German uses two distinct words for what\nin English would be called a wall:Wand for walls inside a building, and Mauer for\nwalls outside a building. Where English uses the word brother for any male sib-\nling, Chinese and many other languages have distinct words for older brother and\nyounger brother (Mandarin gege anddidi, respectively). In all these cases, trans-\nlating bass,wall, orbrother from English would require a kind of specialization,\ndisambiguating the different uses of a word. For this reason the \ufb01elds of MT and\nWord Sense Disambiguation (Appendix G) are closely linked.\nSometimes one language places more grammatical constraints on word choice\nthan another. We saw above that English marks nouns for whether they are singular\nor plural. Mandarin doesn\u2019t. Or French and Spanish, for example, mark grammat-\nical gender on adjectives, so an English translation into French requires specifying\nadjective gender.\nThe way that languages differ in lexically dividing up conceptual space may be\nmore complex than this one-to-many translation problem, leading to many-to-many",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 398,
      "token_count": 649,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 275\n\n13.1 \u2022 L ANGUAGE DIVERGENCES AND TYPOLOGY 267\nmappings. For example, Fig. 13.3 summarizes some of the complexities discussed\nby Hutchins and Somers (1992) in translating English leg, foot , and paw, to French.\nFor example, when legis used about an animal it\u2019s translated as French patte ; but\nabout the leg of a journey, as French etape ; if the leg is of a chair, we use French\npied.\nFurther, one language may have a lexical gap , where no word or phrase, short lexical gap\nof an explanatory footnote, can express the exact meaning of a word in the other\nlanguage. For example, English does not have a word that corresponds neatly to\nMandarin xi`aoor Japanese oyak \u00afok\u00afo(in English one has to make do with awkward\nphrases like \ufb01lial piety orloving child , orgood son/daughter for both).\netapepattejambepied   paw        footlegJOURNEYANIMALHUMANCHAIRANIMALBIRDHUMAN\nFigure 13.3 The complex overlap between English leg,foot, etc., and various French trans-\nlations as discussed by Hutchins and Somers (1992).\nFinally, languages differ systematically in how the conceptual properties of an\nevent are mapped onto speci\ufb01c words. Talmy (1985, 1991) noted that languages\ncan be characterized by whether direction of motion and manner of motion are\nmarked on the verb or on the \u201csatellites\u201d: particles, prepositional phrases, or ad-\nverbial phrases. For example, a bottle \ufb02oating out of a cave would be described in\nEnglish with the direction marked on the particle out, while in Spanish the direction\nwould be marked on the verb:\n(13.5) English: The bottle \ufb02oated out.\nSpanish: La\nThebotella\nbottlesali\u00b4o\nexited\ufb02otando .\n\ufb02oating.\nVerb-framed languages mark the direction of motion on the verb (leaving the verb-framed\nsatellites to mark the manner of motion), like Spanish acercarse \u2018approach\u2019, al-\ncanzar \u2018reach\u2019, entrar \u2018enter\u2019, salir \u2018exit\u2019. Satellite-framed languages mark the satellite-framed\ndirection of motion on the satellite (leaving the verb to mark the manner of motion),\nlike English crawl out ,\ufb02oat off ,jump down ,run after . Languages like Japanese,\nTamil, and the many languages in the Romance, Semitic, and Mayan languages fam-\nilies, are verb-framed; Chinese as well as non-Romance Indo-European languages\nlike English, Swedish, Russian, Hindi, and Farsi are satellite framed (Talmy 1991,\nSlobin 1996).\n13.1.3 Morphological Typology\nMorphologically , languages are often characterized along two dimensions of vari-\nation. The \ufb01rst is the number of morphemes per word, ranging from isolating isolating\nlanguages like Vietnamese and Cantonese, in which each word generally has one\nmorpheme, to polysynthetic languages like Siberian Yupik (\u201cEskimo\u201d), in which a polysynthetic\nsingle word may have very many morphemes, corresponding to a whole sentence in\nEnglish. The second dimension is the degree to which morphemes are segmentable,\nranging from agglutinative languages like Turkish, in which morphemes have rel- agglutinative\natively clean boundaries, to fusion languages like Russian, in which a single af\ufb01x fusion",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 399,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 276\n\n268 CHAPTER 13 \u2022 M ACHINE TRANSLATION\nmay con\ufb02ate multiple morphemes, like -om in the word stolom (table- SG-INSTR -\nDECL 1), which fuses the distinct morphological categories instrumental, singular,\nand \ufb01rst declension.\nTranslating between languages with rich morphology requires dealing with struc-\nture below the word level, and for this reason modern systems generally use subword\nmodels like the wordpiece or BPE models of Section 13.2.1.\n13.1.4 Referential density\nFinally, languages vary along a typological dimension related to the things they tend\nto omit. Some languages, like English, require that we use an explicit pronoun when\ntalking about a referent that is given in the discourse. In other languages, however,\nwe can sometimes omit pronouns altogether, as the following example from Spanish\nshows1:\n(13.6) [El jefe] idio con un libro. / 0 iMostr \u00b4o su hallazgo a un descifrador ambulante.\n[The boss] came upon a book. [He] showed his \ufb01nd to a wandering decoder.\nLanguages that can omit pronouns are called pro-drop languages. Even among pro-drop\nthe pro-drop languages, there are marked differences in frequencies of omission.\nJapanese and Chinese, for example, tend to omit far more than does Spanish. This\ndimension of variation across languages is called the dimension of referential den-\nsity. We say that languages that tend to use more pronouns are more referentiallyreferential\ndensity\ndense than those that use more zeros. Referentially sparse languages, like Chinese or\nJapanese, that require the hearer to do more inferential work to recover antecedents\nare also called cold languages. Languages that are more explicit and make it easier cold language\nfor the hearer are called hotlanguages. The terms hotandcold are borrowed from hot language\nMarshall McLuhan\u2019s 1964 distinction between hot media like movies, which \ufb01ll in\nmany details for the viewer, versus cold media like comics, which require the reader\nto do more inferential work to \ufb01ll out the representation (Bickel, 2003).\nTranslating from languages with extensive pro-drop, like Chinese or Japanese, to\nnon-pro-drop languages like English can be dif\ufb01cult since the model must somehow\nidentify each zero and recover who or what is being talked about in order to insert\nthe proper pronoun.\n13.2 Machine Translation using Encoder-Decoder\nThe standard architecture for MT is the encoder-decoder transformer orsequence-\nto-sequence model, an architecture we saw for RNNs in Chapter 8. We\u2019ll see the\ndetails of how to apply this architecture to transformers in Section 13.3, but \ufb01rst let\u2019s\ntalk about the overall task.\nMost machine translation tasks make the simpli\ufb01cation that we can translate each\nsentence independently, so we\u2019ll just consider individual sentences for now. Given\na sentence in a source language, the MT task is then to generate a corresponding\nsentence in a target language. For example, an MT system is given an English\nsentence like\nThe green witch arrived\nand must translate it into the Spanish sentence:\n1Here we use the / 0-notation; we\u2019ll introduce this and discuss this issue further in Chapter 23",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 400,
      "token_count": 717,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 277\n\n13.2 \u2022 M ACHINE TRANSLATION USING ENCODER -DECODER 269\nLleg \u00b4o la bruja verde\nMT uses supervised machine learning: at training time the system is given a\nlarge set of parallel sentences (each sentence in a source language matched with\na sentence in the target language), and learns to map source sentences into target\nsentences. In practice, rather than using words (as in the example above), we split\nthe sentences into a sequence of subword tokens (tokens can be words, or subwords,\nor individual characters). The systems are then trained to maximize the probability\nof the sequence of tokens in the target language y1;:::;ymgiven the sequence of\ntokens in the source language x1;:::;xn:\nP(y1;:::; ymjx1;:::; xn) (13.7)\nRather than use the input tokens directly, the encoder-decoder architecture con-\nsists of two components, an encoder and a decoder . The encoder takes the input\nwords x= [x1;:::; xn]and produces an intermediate context h. At decoding time, the\nsystem takes hand, word by word, generates the output y:\nh=encoder (x) (13.8)\nyt+1=decoder (h;y1;:::; yt)8t2[1;:::; m] (13.9)\nIn the next two sections we\u2019ll talk about subword tokenization, and then how to get\nparallel corpora for training, and then we\u2019ll introduce the details of the encoder-\ndecoder architecture.\n13.2.1 Tokenization\nMachine translation systems use a vocabulary that is \ufb01xed in advance, and rather\nthan using space-separated words, this vocabulary is generated with subword to-\nkenization algorithms, like the BPE algorithm sketched in Chapter 2. A shared\nvocabulary is used for the source and target languages, which makes it easy to copy\ntokens (like names) from source to target. Using subword tokenization with tokens\nshared between languages makes it natural to translate between languages like En-\nglish or Hindi that use spaces to separate words, and languages like Chinese or Thai\nthat don\u2019t.\nWe build the vocabulary by running a subword tokenization algorithm on a cor-\npus that contains both source and target language data.\nRather than the simple BPE algorithm from Fig. 2.13, modern systems often use\nmore powerful tokenization algorithms. Some systems (like BERT) use a variant of\nBPE called the wordpiece algorithm, which instead of choosing the most frequent wordpiece\nset of tokens to merge, chooses merges based on which one most increases the lan-\nguage model probability of the tokenization. Wordpieces use a special symbol at the\nbeginning of each token; here\u2019s a resulting tokenization from the Google MT system\n(Wu et al., 2016):\nwords : Jet makers feud over seat width with big orders at stake\nwordpieces :J et makers fe ud over seat width with big orders atstake\nThe wordpiece algorithm is given a training corpus and a desired vocabulary size\nV , and proceeds as follows:\n1. Initialize the wordpiece lexicon with characters (for example a subset of Uni-\ncode characters, collapsing all the remaining characters to a special unknown\ncharacter token).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 401,
      "token_count": 707,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 278",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 402,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "270 CHAPTER 13 \u2022 M ACHINE TRANSLATION\n2. Repeat until there are V wordpieces:\n(a) Train an n-gram language model on the training corpus, using the current\nset of wordpieces.\n(b) Consider the set of possible new wordpieces made by concatenating two\nwordpieces from the current lexicon. Choose the one new wordpiece that\nmost increases the language model probability of the training corpus.\nRecall that with BPE we had to specify the number of merges to perform; in\nwordpiece, by contrast, we specify the total vocabulary, which is a more intuitive\nparameter. A vocabulary of 8K to 32K word pieces is commonly used.\nAn even more commonly used tokenization algorithm is (somewhat ambigu-\nously) called the unigram algorithm (Kudo, 2018) or sometimes the SentencePiece unigram\nSentencePiece algorithm, and is used in systems like ALBERT (Lan et al., 2020) and T5 (Raf-\nfel et al., 2020). (Because unigram is the default tokenization algorithm used in a\nlibrary called SentencePiece that adds a useful wrapper around tokenization algo-\nrithms (Kudo and Richardson, 2018b), authors often say they are using Sentence-\nPiece tokenization but really mean they are using the unigram algorithm).\nIn unigram tokenization, instead of building up a vocabulary by merging tokens,\nwe start with a huge vocabulary of every individual unicode character plus all fre-\nquent sequences of characters (including all space-separated words, for languages\nwith spaces), and iteratively remove some tokens to get to a desired \ufb01nal vocabulary\nsize. The algorithm is complex (involving suf\ufb01x-trees for ef\ufb01ciently storing many\ntokens, and the EM algorithm for iteratively assigning probabilities to tokens), so we\ndon\u2019t give it here, but see Kudo (2018) and Kudo and Richardson (2018b). Roughly\nspeaking the algorithm proceeds iteratively by estimating the probability of each\ntoken, tokenizing the input data using various tokenizations, then removing a per-\ncentage of tokens that don\u2019t occur in high-probability tokenization, and then iterates\nuntil the vocabulary has been reduced down to the desired number of tokens.\nWhy does unigram tokenization work better than BPE? BPE tends to create lots\nof very small non-meaningful tokens (because BPE can only create larger words or\nmorphemes by merging characters one at a time), and it also tends to merge very\ncommon tokens, like the suf\ufb01x ed, onto their neighbors. We can see from these\nexamples from Bostrom and Durrett (2020) that unigram tends to produce tokens\nthat are more semantically meaningful:\nOriginal: corrupted Original: Completely preposterous suggestions\nBPE: cor rupted BPE: Comple t ely prep ost erous suggest ions\nUnigram: corrupt ed Unigram: Complete ly pre post er ous suggestion s\n13.2.2 Creating the Training data\nMachine translation models are trained on a parallel corpus , sometimes called a parallel corpus\nbitext , a text that appears in two (or more) languages. Large numbers of paral-\nlel corpora are available. Some are governmental; the Europarl corpus (Koehn, Europarl\n2005), extracted from the proceedings of the European Parliament, contains between\n400,000 and 2 million sentences each from 21 European languages. The United Na-\ntions Parallel Corpus contains on the order of 10 million sentences in the six of\ufb01cial\nlanguages of the United Nations (Arabic, Chinese, English, French, Russian, Span-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 403,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "of very small non-meaningful tokens (because BPE can only create larger words or\nmorphemes by merging characters one at a time), and it also tends to merge very\ncommon tokens, like the suf\ufb01x ed, onto their neighbors. We can see from these\nexamples from Bostrom and Durrett (2020) that unigram tends to produce tokens\nthat are more semantically meaningful:\nOriginal: corrupted Original: Completely preposterous suggestions\nBPE: cor rupted BPE: Comple t ely prep ost erous suggest ions\nUnigram: corrupt ed Unigram: Complete ly pre post er ous suggestion s\n13.2.2 Creating the Training data\nMachine translation models are trained on a parallel corpus , sometimes called a parallel corpus\nbitext , a text that appears in two (or more) languages. Large numbers of paral-\nlel corpora are available. Some are governmental; the Europarl corpus (Koehn, Europarl\n2005), extracted from the proceedings of the European Parliament, contains between\n400,000 and 2 million sentences each from 21 European languages. The United Na-\ntions Parallel Corpus contains on the order of 10 million sentences in the six of\ufb01cial\nlanguages of the United Nations (Arabic, Chinese, English, French, Russian, Span-\nish) Ziemski et al. (2016). Other parallel corpora have been made from movie and\nTV subtitles, like the OpenSubtitles corpus (Lison and Tiedemann, 2016), or from\ngeneral web text, like the ParaCrawl corpus of 223 million sentence pairs between\n23 EU languages and English extracted from the CommonCrawl Ba \u02dcn\u00b4on et al. (2020).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 404,
      "token_count": 370,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 279",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 405,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "13.2 \u2022 M ACHINE TRANSLATION USING ENCODER -DECODER 271\nSentence alignment\nStandard training corpora for MT come as aligned pairs of sentences. When creat-\ning new corpora, for example for underresourced languages or new domains, these\nsentence alignments must be created. Fig. 13.4 gives a sample hypothetical sentence\nalignment.\nF1: -Bonjour, dit le petit prince.F2: -Bonjour, dit le marchand de pilules perfectionn\u00e9es qui apaisent la soif.F3: On en avale une par semaine et l'on n'\u00e9prouve plus le besoin de boire.F4: -C\u2019est une grosse \u00e9conomie de temps, dit le marchand.F5: Les experts ont fait des calculs.F6: On \u00e9pargne cinquante-trois minutes par semaine.F7: \u201cMoi, se dit le petit prince, si j'avais cinquante-trois minutes \u00e0 d\u00e9penser, je marcherais tout doucement vers une fontaine...\"E1: \u201cGood morning,\" said the little prince.E2: \u201cGood morning,\" said the merchant.E3: This was a merchant who sold pills that had been perfected to quench thirst.E4: You just swallow one pill a week and you won\u2019t feel the need for anything to drink.E5: \u201cThey save a huge amount of time,\" said the merchant.E6: \u201cFifty\u2212three minutes a week.\"E7: \u201cIf I had  fifty\u2212three minutes to spend?\" said the little prince to himself. E8: \u201cI would take a stroll to a spring of fresh water\u201d\nFigure 13.4 A sample alignment between sentences in English and French, with sentences extracted from\nAntoine de Saint-Exupery\u2019s Le Petit Prince and a hypothetical translation. Sentence alignment takes sentences\ne1;:::;en, and f1;:::;fmand \ufb01nds minimal sets of sentences that are translations of each other, including single\nsentence mappings like (e 1,f1), (e 4,f3), (e 5,f4), (e 6,f6) as well as 2-1 alignments (e 2/e3,f2), (e 7/e8,f7), and null\nalignments (f 5).\nGiven two documents that are translations of each other, we generally need two\nsteps to produce sentence alignments:\n\u2022 a cost function that takes a span of source sentences and a span of target sen-\ntences and returns a score measuring how likely these spans are to be transla-\ntions.\n\u2022 an alignment algorithm that takes these scores to \ufb01nd a good alignment be-\ntween the documents.\nTo score the similarity of sentences across languages, we need to make use of\namultilingual embedding space , in which sentences from different languages are\nin the same embedding space (Artetxe and Schwenk, 2019). Given such a space,\ncosine similarity of such embeddings provides a natural scoring function (Schwenk,\n2018). Thompson and Koehn (2019) give the following cost function between two\nsentences or spans x,yfrom the source and target documents respectively:\nc(x;y) =(1\u0000cos(x;y))nSents (x)nSents (y)PS\ns=11\u0000cos(x;ys)+PS\ns=11\u0000cos(xs;y)(13.10)\nwhere nSents ()gives the number of sentences (this biases the metric toward many\nalignments of single sentences instead of aligning very large spans). The denom-\ninator helps to normalize the similarities, and so x1;:::;xS;y1;:::;yS;are randomly",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 406,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "steps to produce sentence alignments:\n\u2022 a cost function that takes a span of source sentences and a span of target sen-\ntences and returns a score measuring how likely these spans are to be transla-\ntions.\n\u2022 an alignment algorithm that takes these scores to \ufb01nd a good alignment be-\ntween the documents.\nTo score the similarity of sentences across languages, we need to make use of\namultilingual embedding space , in which sentences from different languages are\nin the same embedding space (Artetxe and Schwenk, 2019). Given such a space,\ncosine similarity of such embeddings provides a natural scoring function (Schwenk,\n2018). Thompson and Koehn (2019) give the following cost function between two\nsentences or spans x,yfrom the source and target documents respectively:\nc(x;y) =(1\u0000cos(x;y))nSents (x)nSents (y)PS\ns=11\u0000cos(x;ys)+PS\ns=11\u0000cos(xs;y)(13.10)\nwhere nSents ()gives the number of sentences (this biases the metric toward many\nalignments of single sentences instead of aligning very large spans). The denom-\ninator helps to normalize the similarities, and so x1;:::;xS;y1;:::;yS;are randomly\nselected sentences sampled from the respective documents.\nUsually dynamic programming is used as the alignment algorithm (Gale and\nChurch, 1993), in a simple extension of the minimum edit distance algorithm we\nintroduced in Chapter 2.\nFinally, it\u2019s helpful to do some corpus cleanup by removing noisy sentence pairs.\nThis can involve handwritten rules to remove low-precision pairs (for example re-\nmoving sentences that are too long, too short, have different URLs, or even pairs",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 407,
      "token_count": 376,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 280\n\n272 CHAPTER 13 \u2022 M ACHINE TRANSLATION\nthat are too similar, suggesting that they were copies rather than translations). Or\npairs can be ranked by their multilingual embedding cosine score and low-scoring\npairs discarded.\n13.3 Details of the Encoder-Decoder Model\nEncoderThegreenlleg\u00f3witcharrived<s>lleg\u00f3la\nlabruja\nbrujaverde\nverde</s>Decodercross-attentiontransformerblocks\nFigure 13.5 The encoder-decoder transformer architecture for machine translation. The encoder uses the\ntransformer blocks we saw in Chapter 8, while the decoder uses a more powerful block with an extra cross-\nattention layer that can attend to all the encoder words. We\u2019ll see this in more detail in the next section.\nThe standard architecture for MT is the encoder-decoder transformer. The encoder-\ndecoder architecture was introduced already for RNNs in Chapter 8, and the trans-\nformer version has the same idea. Fig. 13.5 shows the intuition of the architec-\nture at a high level. You\u2019ll see that the encoder-decoder architecture is made up of\ntwo transformers: an encoder , which is the same as the basic transformers from\nChapter 9, and a decoder , which is augmented with a special new layer called the\ncross-attention layer. The encoder takes the source language input word tokens\nX=x1;:::;xnand maps them to an output representation Henc=h1;:::;hn; via a\nstack of encoder blocks.\nThe decoder is essentially a conditional language model that attends to the en-\ncoder representation and generates the target words one by one, at each timestep\nconditioning on the source sentence and the previously generated target language\nwords to generate a token. Decoding can use any of the decoding methods discussed\nin Chapter 9 like greedy, or temperature or nucleus sampling. But the most com-\nmon decoding algorithm for MT is the beam search algorithm that we\u2019ll introduce\nin Section 13.4.\nBut the components of the architecture differ somewhat from the transformer\nblock we\u2019ve seen. First, in order to attend to the source language, the transformer\nblocks in the decoder have an extra cross-attention layer. Recall that the transformer\nblock of Chapter 9 consists of a self-attention layer that attends to the input from\nthe previous layer, followed by layer norm, a feed forward layer, and another layer\nnorm. The decoder transformer block includes an extra layer with a special kind\nof attention, cross-attention (also sometimes called encoder-decoder attention or cross-attention\nsource attention ). Cross-attention has the same form as the multi-head attention\nin a normal transformer block, except that while the queries as usual come from\nthe previous layer of the decoder, the keys and values come from the output of the\nencoder .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 408,
      "token_count": 598,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 281\n\n13.3 \u2022 D ETAILS OF THE ENCODER -DECODER MODEL 273\nEncoderx1x2x3xn\u2026Decoderh3h2h1\u2026hn\nEncoderBlock 1Block 2Block Ky3y2y1\u2026\nDecoderBlock 1Block 2Block LUnembedding Matrixym\nMulti-Head AttentionLayer NormalizeLayer Normalize++\u2026Feedforward\nCausal Multi-Head AttentionLayer NormalizeLayer Normalize+\n+\u2026FeedforwardLayer Normalize+Cross-Attention\u2026\u2026\u2026\u2026\u2026\u2026LanguageModeling HeadHenc\nFigure 13.6 The transformer block for the encoder and the decoder. The \ufb01nal output of the encoder Henc=\nh1;:::;hnis the context used in the decoder. The decoder is a standard transformer except with one extra layer,\nthecross-attention layer, which takes that encoder output Hencand uses it to form its KandVinputs.\nThat is, where in standard multi-head attention the input to each attention layer is\nX, in cross attention the input is the the \ufb01nal output of the encoder Henc=h1;:::;hn.\nHencis of shape [n\u0002d], each row representing one input token. To link the keys\nand values from the encoder with the query from the prior layer of the decoder, we\nmultiply the encoder output Hencby the cross-attention layer\u2019s key weights WKand\nvalue weights WV. The query comes from the output from the prior decoder layer\nHdec[`\u00001], which is multiplied by the cross-attention layer\u2019s query weights WQ:\nQ=Hdec[`\u00001]WQ;K=HencWK;V=HencWV(13.11)\nCrossAttention (Q;K;V) = softmax\u0012QK|\npdk\u0013\nV (13.12)\nThe cross attention thus allows the decoder to attend to each of the source language\nwords as projected into the entire encoder \ufb01nal output representations. The other\nattention layer in each decoder block, the multi-head attention layer, is the same\ncausal (left-to-right) attention that we saw in Chapter 9. The multi-head attention in\nthe encoder, however, is allowed to look ahead at the entire source language text, so\nit is not masked.\nTo train an encoder-decoder model, we use the same self-supervision model we\nused for training encoder-decoders RNNs in Chapter 8. The network is given the\nsource text and then starting with the separator token is trained autoregressively to\npredict the next token using cross-entropy loss. Recall that cross-entropy loss for",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 409,
      "token_count": 555,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 282\n\n274 CHAPTER 13 \u2022 M ACHINE TRANSLATION\nlanguage modeling is determined by the probability the model assigns to the correct\nnext word. So at time tthe CE loss is the negative log probability the model assigns\nto the next word in the training sequence:\nLCE(\u02c6yt;yt) =\u0000log\u02c6yt[wt+1] (13.13)\nAs in that case, we use teacher forcing in the decoder. Recall that in teacher forc- teacher forcing\ning, at each time step in decoding we force the system to use the gold target token\nfrom training as the next input xt+1, rather than allowing it to rely on the (possibly\nerroneous) decoder output \u02c6 yt.\n13.4 Decoding in MT: Beam Search\nRecall the greedy decoding algorithm from Chapter 9: at each time step tin gen-\neration, the output ytis chosen by computing the probability for each word in the\nvocabulary and then choosing the highest probability word (the argmax):\n\u02c6wt=argmaxw2VP(wjw<t) (13.14)\nA problem with greedy decoding is that what looks high probability at word tmight\nturn out to have been the wrong choice once we get to word t+1. The beam search\nalgorithm maintains multiple choices until later when we can see which one is best.\nIn beam search we model decoding as searching the space of possible genera-\ntions, represented as a search tree whose branches represent actions (generating a search tree\ntoken), and nodes represent states (having generated a particular pre\ufb01x). We search\nfor the best action sequence, i.e., the string with the highest probability.\nAn illustration of the problem\nFig. 13.7 shows a made-up example. The most probable sequence is ok ok EOS (its\nprobability is .4\u0002.7\u00021.0). But greedy search doesn\u2019t \ufb01nd it, incorrectly choosing\nyesas the \ufb01rst word since it has the highest local probability (0.5).\nstartokyesEOSokyesEOSokyesEOSEOSEOSEOSEOSt2t3p(t1|start)\nt1p(t2| t1)p(t3| t1,t2)\n.1.5.4.3.4.3.1.2.71.01.01.01.0\nFigure 13.7 A search tree for generating the target string T=t1;t2;:::from vocabulary\nV=fyes;ok;<s>g, showing the probability of generating each token from that state. Greedy\nsearch chooses yesfollowed by yes, instead of the globally most probable sequence ok ok .\nFor some problems, like part-of-speech tagging or parsing as we will see in\nChapter 17 or Chapter 18, we can use dynamic programming search (the Viterbi",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 410,
      "token_count": 601,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 283\n\n13.4 \u2022 D ECODING IN MT: B EAM SEARCH 275\nalgorithm) to address this problem. Unfortunately, dynamic programming is not ap-\nplicable to generation problems with long-distance dependencies between the output\ndecisions. The only method guaranteed to \ufb01nd the best solution is exhaustive search:\ncomputing the probability of every one of the VTpossible sentences (for some length\nvalue T) which is obviously too slow.\nThe solution: beam search\nInstead, MT systems generally decode using beam search , a heuristic search method beam search\n\ufb01rst proposed by Lowerre (1976). In beam search, instead of choosing the best token\nto generate at each timestep, we keep kpossible tokens at each step. This \ufb01xed-size\nmemory footprint kis called the beam width , on the metaphor of a \ufb02ashlight beam beam width\nthat can be parameterized to be wider or narrower.\nThus at the \ufb01rst step of decoding, we compute a softmax over the entire vocab-\nulary, assigning a probability to each word. We then select the k-best options from\nthis softmax output. These initial koutputs are the search frontier and these kinitial\nwords are called hypotheses . A hypothesis is an output sequence, a translation-so-\nfar, together with its probability.\na\u2026aardvark..arrived..the\u2026zebrastart\nt1a\u2026aardvark..the..witch\u2026zebraa\u2026aardvark..green..witch\u2026zebrat2hd1y1BOSy1y2\ny2hd1hd2thetheBOShd2greengreeny3hd1hd2arrivedarrivedBOSy2\nt3hd1hd2thetheBOSy2\nhd1hd2thetheBOShd2witchwitchy3a\u2026mage..the..witch\u2026zebraarrived\u2026aardvark..green..who\u2026zebray3\ny3\nFigure 13.8 Beam search decoding with a beam width of k=2. At each time step, we choose the kbest\nhypotheses, form the Vpossible extensions of each, score those k\u0002Vhypotheses and choose the best k=2\nto continue. At time 1, the frontier has the best 2 options from the initial decoder state: arrived andthe. We\nextend each, compute the probability of all the hypotheses so far ( arrived the ,arrived aardvark ,the green ,the\nwitch ) and again chose the best 2 ( the green andthe witch ) to be the search frontier. The images on the arcs\nschematically represent the decoders that must be run at each step to score the next words (for simplicity not\ndepicting cross-attention).\nAt subsequent steps, each of the kbest hypotheses is extended incrementally",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 411,
      "token_count": 602,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 284",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 412,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "276 CHAPTER 13 \u2022 M ACHINE TRANSLATION\nby being passed to distinct decoders, which each generate a softmax over the entire\nvocabulary to extend the hypothesis to every possible next token. Each of these k\u0002V\nhypotheses is scored by P(yijx;y<i): the product of the probability of the current\nword choice multiplied by the probability of the path that led to it. We then prune\nthek\u0002Vhypotheses down to the kbest hypotheses, so there are never more than k\nhypotheses at the frontier of the search, and never more than kdecoders. Fig. 13.8\nillustrates this with a beam width of 2 for the beginning of The green witch arrived .\nThis process continues until an EOS is generated indicating that a complete can-\ndidate output has been found. At this point, the completed hypothesis is removed\nfrom the frontier and the size of the beam is reduced by one. The search continues\nuntil the beam has been reduced to 0. The result will be khypotheses.\nTo score each node by its log probability, we use the chain rule of probability to\nbreak down p(yjx)into the product of the probability of each word given its prior\ncontext, which we can turn into a sum of logs (for an output string of length t):\nscore(y) = logP(yjx)\n=log(P(y1jx)P(y2jy1;x)P(y3jy1;y2;x):::P(ytjy1;:::;yt\u00001;x))\n=tX\ni=1logP(yijy1;:::;yi\u00001;x) (13.15)\nThus at each step, to compute the probability of a partial sentence, we simply add the\nlog probability of the pre\ufb01x sentence so far to the log probability of generating the\nnext token. Fig. 13.9 shows the scoring for the example sentence shown in Fig. 13.8,\nusing some simple made-up probabilities. Log probabilities are negative or 0, and\nthe max of two log probabilities is the one that is greater (closer to 0).\nBOSarrivedthethewitchgreenwitchmage\nwhoy2y3log P(y1|x)y1log P(y2|y1,x)log P(y3|y2,y1,x)-.92-1.6-1.2-.69-2.3-.69-1.6\n-2.3arrived-.11-.51witch-.36-.22EOS\n-.51EOS-2.3at-1.61bylog P(y4|y3,y2,y1,x)log P(y5|y4,y3,y2,y1,x)arrivedcame-1.6\ny4y5log P(arrived|x) log P(arrived witch|x)log P(the|x)log P(the green|x)log P(the witch|x) =-1.6log P (arrived the|x) log P (\u201cthe green witch arrived\u201d|x) = log P (the|x) + log P(green|the,x) + log P(witch | the, green,x)+logP(arrived|the,green,witch,x)+log P(EOS|the,green,witch,arrived,x)= -2.3= -3.9= -1.6= -2.1=-.92-2.1-3.2\n-4.4-2.2-2.5-3.7-2.7-3.8-2.7-4.8\nFigure 13.9 Scoring for beam search decoding with a beam width of k=2. We maintain the log probability",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 413,
      "token_count": 789,
      "chapter_title": ""
    }
  },
  {
    "content": "-2.3arrived-.11-.51witch-.36-.22EOS\n-.51EOS-2.3at-1.61bylog P(y4|y3,y2,y1,x)log P(y5|y4,y3,y2,y1,x)arrivedcame-1.6\ny4y5log P(arrived|x) log P(arrived witch|x)log P(the|x)log P(the green|x)log P(the witch|x) =-1.6log P (arrived the|x) log P (\u201cthe green witch arrived\u201d|x) = log P (the|x) + log P(green|the,x) + log P(witch | the, green,x)+logP(arrived|the,green,witch,x)+log P(EOS|the,green,witch,arrived,x)= -2.3= -3.9= -1.6= -2.1=-.92-2.1-3.2\n-4.4-2.2-2.5-3.7-2.7-3.8-2.7-4.8\nFigure 13.9 Scoring for beam search decoding with a beam width of k=2. We maintain the log probability\nof each hypothesis in the beam by incrementally adding the logprob of generating each next token. Only the top\nkpaths are extended to the next step.\nFig. 13.10 gives the algorithm. One problem with this version of the algorithm is\nthat the completed hypotheses may have different lengths. Because language mod-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 414,
      "token_count": 332,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 285\n\n13.4 \u2022 D ECODING IN MT: B EAM SEARCH 277\nfunction BEAM DECODE (c,beam width )returns best paths\ny0,h0 0\npath ()\ncomplete paths ()\nstate (c,y0,h0, path) ;initial state\nfrontier hstatei ;initial frontier\nwhile frontier contains incomplete paths andbeamwidth>0\nextended frontier hi\nfor each state2frontier do\ny DECODE (state )\nfor each word i2Vocabulary do\nsuccessor NEWSTATE (state ,i,yi)\nextended frontier ADDTOBEAM (successor ,extended frontier ,\nbeam width )\nfor each state inextended frontier do\nifstate is complete do\ncomplete paths APPEND (complete paths ,state )\nextended frontier REMOVE (extended frontier ,state )\nbeam width beam width - 1\nfrontier extended frontier\nreturn completed paths\nfunction NEWSTATE (state ,word ,word prob)returns new state\nfunction ADDTOBEAM (state ,frontier ,width )returns updated frontier\nifLENGTH (frontier )<width then\nfrontier INSERT (state ,frontier )\nelse if SCORE (state )>SCORE (WORST OF(frontier ))\nfrontier REMOVE (WORST OF(frontier ))\nfrontier INSERT (state ,frontier )\nreturn frontier\nFigure 13.10 Beam search decoding.\nels generally assign lower probabilities to longer strings, a naive algorithm would\nchoose shorter strings for y. (This is not an issue during the earlier steps of decod-\ning; since beam search is breadth-\ufb01rst, all the hypotheses being compared had the\nsame length.) For this reason we often apply length normalization methods, like\ndividing the logprob by the number of words:\nscore(y) =1\ntlogP(yjx) =1\nttX\ni=1logP(yijy1;:::;yi\u00001;x) (13.16)\nFor MT we generally use beam widths kbetween 5 and 10, giving us khypotheses at\nthe end. We can pass all kto the downstream application with their respective scores,\nor if we just need a single translation we can pass the most probable hypothesis.\n13.4.1 Minimum Bayes Risk Decoding\nMinimum Bayes risk orMBR decoding is an alternative decoding algorithm thatminimum\nBayes risk\nMBR",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 415,
      "token_count": 498,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 286\n\n278 CHAPTER 13 \u2022 M ACHINE TRANSLATION\ncan work even better than beam search and also tends to be better than the other\ndecoding algorithms like temperature sampling introduced in Section 10.2.\nThe intuition of minimum Bayes risk is that instead of trying to choose the trans-\nlation which is most probable, we choose the one that is likely to have the least error.\nFor example, we might want our decoding algorithm to \ufb01nd the translation which\nhas the highest score on some evaluation metric. For example in Section 13.6 we will\nintroduce metrics like chrF or BERTScore that measure the goodness-of-\ufb01t between\na candidate translation and a set of reference human translations. A translation that\nmaximizes this score, especially with a hypothetically huge set of perfect human\ntranslations is likely to be a good one (have minimum risk) even if it is not the most\nprobable translation by our particular probability estimator.\nIn practice, we don\u2019t know the perfect set of translations for a given sentence. So\nthe standard simpli\ufb01cation used in MBR decoding algorithms is to instead choose\nthe candidate translation which is most similar (by some measure of goodness-of-\n\ufb01t) with some set of candidate translations. We\u2019re essentially approximating the\nenormous space of all possible translations Uwith a smaller set of possible candidate\ntranslations Y.\nGiven this set of possible candidate translations Y, and some similarity or align-\nment function util, we choose the best translation \u02c6 yas the translation which is most\nsimilar to all the other candidate translations:\n\u02c6y=argmax\ny2YX\nc2Yutil(y;c) (13.17)\nVarious util functions can be used, like chrF or BERTscore or BLEU. We can get the\nset of candidate translations by sampling using one of the basic sampling algorithms\nof Section 10.2 like temperature sampling; good results can be obtained with as few\nas 32 or 64 candidates.\nMinimum Bayes risk decoding can also be used for other NLP tasks; indeed\nit was widely applied to speech recognition (Stolcke et al., 1997; Goel and Byrne,\n2000) before being applied to machine translation (Kumar and Byrne, 2004), and\nhas been shown to work well across many other generation tasks as well (e.g., sum-\nmarization, dialogue, and image captioning (Suzgun et al., 2023a)).\n13.5 Translating in low-resource situations\nFor some languages, and especially for English, online resources are widely avail-\nable. There are many large parallel corpora that contain translations between En-\nglish and many languages. But the vast majority of the world\u2019s languages do not\nhave large parallel training texts available. An important ongoing research question\nis how to get good translation with lesser resourced languages. The resource prob-\nlem can even be true for high resource languages when we need to translate into low\nresource domains (for example in a particular genre that happens to have very little\nbitext).\nHere we brie\ufb02y introduce two commonly used approaches for dealing with this\ndata sparsity: backtranslation , which is a special case of the general statistical\ntechnique called data augmentation , and multilingual models , and also discuss\nsome socio-technical issues.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 416,
      "token_count": 707,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 287\n\n13.5 \u2022 T RANSLATING IN LOW -RESOURCE SITUATIONS 279\n13.5.1 Data Augmentation\nData augmentation is a statistical technique for dealing with insuf\ufb01cient training\ndata, by adding new synthetic data that is generated from the current natural data.\nThe most common data augmentation technique for machine translation is called\nbacktranslation . Backtranslation relies on the intuition that while parallel corpora backtranslation\nmay be limited for particular languages or domains, we can often \ufb01nd a large (or\nat least larger) monolingual corpus, to add to the smaller parallel corpora that are\navailable. The algorithm makes use of monolingual corpora in the target language\nby creating synthetic bitexts.\nIn backtranslation, our goal is to improve source-to-target MT, given a small\nparallel text (a bitext) in the source/target languages, and some monolingual data in\nthe target language. We \ufb01rst use the bitext to train a MT system in the reverse di-\nrection: a target-to-source MT system . We then use it to translate the monolingual\ntarget data to the source language. Now we can add this synthetic bitext (natural\ntarget sentences, aligned with MT-produced source sentences) to our training data,\nand retrain our source-to-target MT model. For example suppose we want to trans-\nlate from Navajo to English but only have a small Navajo-English bitext, although of\ncourse we can \ufb01nd lots of monolingual English data. We use the small bitext to build\nan MT engine going the other way (from English to Navajo). Once we translate the\nmonolingual English text to Navajo, we can add this synthetic Navajo/English bitext\nto our training data.\nBacktranslation has various parameters. One is how we generate the backtrans-\nlated data; we can run the decoder in greedy inference, or use beam search. Or\nwe can do sampling, like the temperature sampling algorithm we saw in Chapter 9.\nAnother parameter is the ratio of backtranslated data to natural bitext data; we can\nchoose to upsample the bitext data (include multiple copies of each sentence). In\ngeneral backtranslation works surprisingly well; one estimate suggests that a system\ntrained on backtranslated text gets about 2/3 of the gain as would training on the\nsame amount of natural bitext (Edunov et al., 2018).\n13.5.2 Multilingual models\nThe models we\u2019ve described so far are for bilingual translation: one source language,\none target language. It\u2019s also possible to build a multilingual translator.\nIn a multilingual translator, we train the system by giving it parallel sentences\nin many different pairs of languages. That means we need to tell the system which\nlanguage to translate from and to! We tell the system which language is which\nby adding a special token lsto the encoder specifying the source language we\u2019re\ntranslating from, and a special token ltto the decoder telling it the target language\nwe\u2019d like to translate into.\nThus we slightly update Eq. 13.9 above to add these tokens in Eq. 13.19:\nh=encoder (x;ls) (13.18)\nyi+1=decoder (h;lt;y1;:::; yi)8i2[1;:::; m] (13.19)\nOne advantage of a multilingual model is that they can improve the translation\nof lower-resourced languages by drawing on information from a similar language\nin the training data that happens to have more resources. Perhaps we don\u2019t know\nthe meaning of a word in Galician, but the word appears in the similar and higher-\nresourced language Spanish.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 417,
      "token_count": 787,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 288\n\n280 CHAPTER 13 \u2022 M ACHINE TRANSLATION\n13.5.3 Sociotechnical issues\nMany issues in dealing with low-resource languages go beyond the purely techni-\ncal. One problem is that for low-resource languages, especially from low-income\ncountries, native speakers are often not involved as the curators for content selec-\ntion, as the language technologists, or as the evaluators who measure performance\n(8et al., 2020). Indeed, one well-known study that manually audited a large set of\nparallel corpora and other major multilingual datasets found that for many of the\ncorpora, less than 50% of the sentences were of acceptable quality, with a lot of\ndata consisting of repeated sentences with web boilerplate or incorrect translations,\nsuggesting that native speakers may not have been suf\ufb01ciently involved in the data\nprocess (Kreutzer et al., 2022).\nOther issues, like the tendency of many MT approaches to focus on the case\nwhere one of the languages is English (Anastasopoulos and Neubig, 2020), have to\ndo with allocation of resources. Where most large multilingual systems were trained\non bitexts in which English was one of the two languages, recent huge corporate\nsystems like those of Fan et al. (2021) and Costa-juss `a et al. (2022) and datasets\nlike Schwenk et al. (2021) attempt to handle large numbers of languages (up to 200\nlanguages) and create bitexts between many more pairs of languages and not just\nthrough English.\nAt the smaller end, 8et al. (2020) propose a participatory design process to\nencourage content creators, curators, and language technologists who speak these\nlow-resourced languages to participate in developing MT algorithms. They provide\nonline groups, mentoring, and infrastructure, and report on a case study on devel-\noping MT algorithms for low-resource African languages. Among their conclusions\nwas to perform MT evaluation by post-editing rather than direct evaluation, since\nhaving labelers edit an MT system and then measure the distance between the MT\noutput and its post-edited version both was simpler to train evaluators and makes it\neasier to measure true errors in the MT output and not differences due to linguistic\nvariation (Bentivogli et al., 2018).\n13.6 MT Evaluation\nTranslations are evaluated along two dimensions:\n1.adequacy: how well the translation captures the exact meaning of the source adequacy\nsentence. Sometimes called faithfulness or\ufb01delity .\n2.\ufb02uency: how \ufb02uent the translation is in the target language (is it grammatical, \ufb02uency\nclear, readable, natural).\nUsing humans to evaluate is most accurate, but automatic metrics are also used for\nconvenience.\n13.6.1 Using Human Raters to Evaluate MT\nThe most accurate evaluations use human raters, such as online crowdworkers, to\nevaluate each translation along the two dimensions. For example, along the dimen-\nsion of \ufb02uency , we can ask how intelligible, how clear, how readable, or how natural\nthe MT output (the target text) is. We can give the raters a scale, for example, from\n1 (totally unintelligible) to 5 (totally intelligible), or 1 to 100, and ask them to rate\neach sentence or paragraph of the MT output.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 418,
      "token_count": 743,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 289\n\n13.6 \u2022 MT E VALUATION 281\nWe can do the same thing to judge the second dimension, adequacy , using raters\nto assign scores on a scale. If we have bilingual raters, we can give them the source\nsentence and a proposed target sentence, and rate, on a 5-point or 100-point scale,\nhow much of the information in the source was preserved in the target. If we only\nhave monolingual raters but we have a good human translation of the source text, we\ncan give the monolingual raters the human reference translation and a target machine\ntranslation and again rate how much information is preserved. An alternative is to\ndoranking : give the raters a pair of candidate translations, and ask them which one ranking\nthey prefer.\nTraining of human raters (who are often online crowdworkers) is essential; raters\nwithout translation expertise \ufb01nd it dif\ufb01cult to separate \ufb02uency and adequacy, and\nso training includes examples carefully distinguishing these. Raters often disagree\n(source sentences may be ambiguous, raters will have different world knowledge,\nraters may apply scales differently). It is therefore common to remove outlier raters,\nand (if we use a \ufb01ne-grained enough scale) normalizing raters by subtracting the\nmean from their scores and dividing by the variance.\nAs discussed above, an alternative way of using human raters is to have them\npost-edit translations, taking the MT output and changing it minimally until they\nfeel it represents a correct translation. The difference between their post-edited\ntranslations and the original MT output can then be used as a measure of quality.\n13.6.2 Automatic Evaluation\nWhile humans produce the best evaluations of machine translation output, running a\nhuman evaluation can be time consuming and expensive. For this reason automatic\nmetrics are often used as temporary proxies. Automatic metrics are less accurate\nthan human evaluation, but can help test potential system improvements, and even\nbe used as an automatic loss function for training. In this section we introduce two\nfamilies of such metrics, those based on character- or word-overlap and those based\non embedding similarity.\nAutomatic Evaluation by Character Overlap: chrF\nThe simplest and most robust metric for MT evaluation is called chrF , which stands chrF\nforcharacter F-score (Popovi \u00b4c, 2015). chrF (along with many other earlier related\nmetrics like BLEU, METEOR, TER, and others) is based on a simple intuition de-\nrived from the pioneering work of Miller and Beebe-Center (1956): a good machine\ntranslation will tend to contain characters and words that occur in a human trans-\nlation of the same sentence. Consider a test set from a parallel corpus, in which\neach source sentence has both a gold human target translation and a candidate MT\ntranslation we\u2019d like to evaluate. The chrF metric ranks each MT target sentence by\na function of the number of character n-gram overlaps with the human translation.\nGiven the hypothesis and the reference, chrF is given a parameter kindicating\nthe length of character n-grams to be considered, and computes the average of the\nkprecisions (unigram precision, bigram, and so on) and the average of the krecalls\n(unigram recall, bigram recall, etc.):\nchrP percentage of character 1-grams, 2-grams, ..., k-grams in the hypothesis that\noccur in the reference, averaged.\nchrR percentage of character 1-grams, 2-grams,..., k-grams in the reference that\noccur in the hypothesis, averaged.\nThe metric then computes an F-score by combining chrP and chrR using a weighting",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 419,
      "token_count": 791,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 290\n\n282 CHAPTER 13 \u2022 M ACHINE TRANSLATION\nparameter b. It is common to set b=2, thus weighing recall twice as much as\nprecision:\nchrFb= (1+b2)chrP\u0001chrR\nb2\u0001chrP+chrR(13.20)\nForb=2, that would be:\nchrF2 =5\u0001chrP\u0001chrR\n4\u0001chrP+chrR\nFor example, consider two hypotheses that we\u2019d like to score against the refer-\nence translation witness for the past . Here are the hypotheses along with chrF values\ncomputed using parameters k=b=2 (in real examples, kwould be a higher number\nlike 6):\nREF:witness for the past,\nHYP1:witness of the past, chrF2,2 = .86\nHYP2:past witness chrF2,2 = .62\nLet\u2019s see how we computed that chrF value for HYP1 (we\u2019ll leave the compu-\ntation of the chrF value for HYP2 as an exercise for the reader). First, chrF ignores\nspaces, so we\u2019ll remove them from both the reference and hypothesis:\nREF:witnessforthepast, (18 unigrams, 17 bigrams)\nHYP1:witnessofthepast, (17 unigrams, 16 bigrams)\nNext let\u2019s see how many unigrams and bigrams match between the reference and\nhypothesis:\nunigrams that match: w i t n e s s f o t h e p a s t , (17 unigrams)\nbigrams that match: wi it tn ne es ss th he ep pa as st t, (13 bigrams)\nWe use that to compute the unigram and bigram precisions and recalls:\nunigram P: 17/17 = 1 unigram R: 17/18 = .944\nbigram P: 13/16 = .813 bigram R: 13/17 = .765\nFinally we average to get chrP and chrR, and compute the F-score:\nchrP = (17=17+13=16)=2=:906\nchrR = (17=18+13=17)=2=:855\nchrF2,2 =5chrP\u0003chrR\n4chrP +chrR=:86\nchrF is simple, robust, and correlates very well with human judgments in many\nlanguages (Kocmi et al., 2021).\nAlternative overlap metric: BLEU\nThere are various alternative overlap metrics. For example, before the development\nof chrF, it was common to use a word-based overlap metric called BLEU (for BiLin-\ngual Evaluation Understudy), that is purely precision-based rather than combining\nprecision and recall (Papineni et al., 2002). The BLEU score for a corpus of candi-\ndate translation sentences is a function of the n-gram word precision over all the\nsentences combined with a brevity penalty computed over the corpus as a whole.\nWhat do we mean by n-gram precision? Consider a corpus composed of a single\nsentence. The unigram precision for this corpus is the percentage of unigram tokens",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 420,
      "token_count": 676,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 291",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 421,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "13.6 \u2022 MT E VALUATION 283\nin the candidate translation that also occur in the reference translation, and ditto for\nbigrams and so on, up to 4-grams. BLEU extends this unigram metric to the whole\ncorpus by computing the numerator as the sum over all sentences of the counts of all\nthe unigram types that also occur in the reference translation, and the denominator\nis the total of the counts of all unigrams in all candidate sentences. We compute\nthis n-gram precision for unigrams, bigrams, trigrams, and 4-grams and take the\ngeometric mean. BLEU has many further complications, including a brevity penalty\nfor penalizing candidate translations that are too short, and it also requires the n-\ngram counts be clipped in a particular way.\nBecause BLEU is a word-based metric, it is very sensitive to word tokenization,\nmaking it impossible to compare different systems if they rely on different tokeniza-\ntion standards, and doesn\u2019t work as well in languages with complex morphology.\nNonetheless, you will sometimes still see systems evaluated by BLEU, particularly\nfor translation into English. In such cases it\u2019s important to use packages that enforce\nstandardization for tokenization like S ACRE BLEU (Post, 2018).\nStatistical Signi\ufb01cance Testing for MT evals\nCharacter or word overlap-based metrics like chrF (or BLEU, or etc.) are mainly\nused to compare two systems, with the goal of answering questions like: did the\nnew algorithm we just invented improve our MT system? To know if the difference\nbetween the chrF scores of two MT systems is a signi\ufb01cant difference, we use the\npaired bootstrap test, or the similar randomization test.\nTo get a con\ufb01dence interval on a single chrF score using the bootstrap test, recall\nfrom Section 4.9 that we take our test set (or devset) and create thousands of pseudo-\ntestsets by repeatedly sampling with replacement from the original test set. We now\ncompute the chrF score of each of the pseudo-testsets. If we drop the top 2.5% and\nbottom 2.5% of the scores, the remaining scores will give us the 95% con\ufb01dence\ninterval for the chrF score of our system.\nTo compare two MT systems A and B, we draw the same set of pseudo-testsets,\nand compute the chrF scores for each of them. We then compute the percentage of\npseudo-test-sets in which A has a higher chrF score than B.\nchrF: Limitations\nWhile automatic character and word-overlap metrics like chrF or BLEU are useful,\nthey have important limitations. chrF is very local: a large phrase that is moved\naround might barely change the chrF score at all, and chrF can\u2019t evaluate cross-\nsentence properties of a document like its discourse coherence (Chapter 24). chrF\nand similar automatic metrics also do poorly at comparing very different kinds of\nsystems, such as comparing human-aided translation against machine translation, or\ndifferent machine translation architectures against each other (Callison-Burch et al.,\n2006). Instead, automatic overlap metrics like chrF are most appropriate when eval-\nuating changes to a single system.\n13.6.3 Automatic Evaluation: Embedding-Based Methods\nThe chrF metric is based on measuring the exact character n-grams a human refer-\nence and candidate machine translation have in common. However, this criterion\nis overly strict, since a good translation may use alternate words or paraphrases. A\nsolution \ufb01rst pioneered in early metrics like METEOR (Banerjee and Lavie, 2005)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 422,
      "token_count": 780,
      "chapter_title": ""
    }
  },
  {
    "content": "To compare two MT systems A and B, we draw the same set of pseudo-testsets,\nand compute the chrF scores for each of them. We then compute the percentage of\npseudo-test-sets in which A has a higher chrF score than B.\nchrF: Limitations\nWhile automatic character and word-overlap metrics like chrF or BLEU are useful,\nthey have important limitations. chrF is very local: a large phrase that is moved\naround might barely change the chrF score at all, and chrF can\u2019t evaluate cross-\nsentence properties of a document like its discourse coherence (Chapter 24). chrF\nand similar automatic metrics also do poorly at comparing very different kinds of\nsystems, such as comparing human-aided translation against machine translation, or\ndifferent machine translation architectures against each other (Callison-Burch et al.,\n2006). Instead, automatic overlap metrics like chrF are most appropriate when eval-\nuating changes to a single system.\n13.6.3 Automatic Evaluation: Embedding-Based Methods\nThe chrF metric is based on measuring the exact character n-grams a human refer-\nence and candidate machine translation have in common. However, this criterion\nis overly strict, since a good translation may use alternate words or paraphrases. A\nsolution \ufb01rst pioneered in early metrics like METEOR (Banerjee and Lavie, 2005)\nwas to allow synonyms to match between the reference xand candidate \u02dc x. More",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 423,
      "token_count": 304,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 292",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 424,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "284 CHAPTER 13 \u2022 M ACHINE TRANSLATION\nrecent metrics use BERT or other embeddings to implement this intuition.\nFor example, in some situations we might have datasets that have human as-\nsessments of translation quality. Such datasets consists of tuples (x;\u02dcx;r), where\nx= (x1;:::; xn)is a reference translation, \u02dc x= (\u02dcx1;:::; \u02dcxm)is a candidate machine\ntranslation, and r2Ris a human rating that expresses the quality of \u02dc xwith respect\ntox. Given such data, algorithms like COMET (Rei et al., 2020) BLEURT (Sellam\net al., 2020) train a predictor on the human-labeled datasets, for example by passing\nxand \u02dcxthrough a version of BERT (trained with extra pretraining, and then \ufb01netuned\non the human-labeled sentences), followed by a linear layer that is trained to predict\nr. The output of such models correlates highly with human labels.\nIn other cases, however, we don\u2019t have such human-labeled datasets. In that\ncase we can measure the similarity of xand \u02dcxby the similarity of their embeddings.\nThe BERTS CORE algorithm (Zhang et al., 2020) shown in Fig. 13.11, for example,\npasses the reference xand the candidate \u02dc xthrough BERT, computing a BERT em-\nbedding for each token xiand \u02dcxj. Each pair of tokens (xi;\u02dcxj)is scored by its cosine\nxi\u0001\u02dcxj\njxijj\u02dcxjj. Each token in xis matched to a token in \u02dc xto compute recall, and each token in\n\u02dcxis matched to a token in xto compute precision (with each token greedily matched\nto the most similar token in the corresponding sentence). BERTS CORE provides\nprecision and recall (and hence F 1):\nRBERT=1\njxjX\nxi2xmax\n\u02dcxj2\u02dcxxi\u0001\u02dcxjPBERT=1\nj\u02dcxjX\n\u02dcxj2\u02dcxmax\nxi2xxi\u0001\u02dcxj (13.21)\nPublished as a conference paper at ICLR 2020\nReferencethe weather is cold today\nCandidateit is freezing today\nCandidateContextualEmbeddingPairwise CosineSimilarityRBERT=(0.713\u00001.27)+(0.515\u00007.94)+...1.27+7.94+1.82+7.90+8.88",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 425,
      "token_count": 573,
      "chapter_title": ""
    }
  },
  {
    "content": "<latexit sha1_base64=\"OJyoKlmBAgUA0KDtUcsH/di5BlI=\">AAACSHicbZDLattAFIaPnLRJ3JvTLrsZYgoJAqFxGqwsCqal0FVJQ5wELCNG41EyZHRh5ijECL1EnqAv002X2eUZsumipXRR6Mj2Ipf+MPDznXM4Z/64UNKg7187raXlR49XVtfaT54+e/6is/7y0OSl5mLIc5Xr45gZoWQmhihRieNCC5bGShzFZx+a+tG50Ebm2QFOCzFO2UkmE8kZWhR1ov2oClFcYPX+4/5BXZN3JEw049Wm7/XpdogyFYZQr9ffci3aoTsL1Pd23265oZrkaOqqaXAb5FIv6DXOdwMvCOqo0/U9fyby0NCF6Q52/15+BYC9qHMVTnJepiJDrpgxI+oXOK6YRsmVqNthaUTB+Bk7ESNrM2aPGVezIGryxpIJSXJtX4ZkRm9PVCw1ZprGtjNleGru1xr4v9qoxCQYVzIrShQZny9KSkUwJ02qZCK14Kim1jCupb2V8FNmc0SbfduGQO9/+aE57HnU9+gX2h18hrlW4TVswCZQ6MMAPsEeDIHDN7iBn/DL+e78cH47f+atLWcx8wruqNX6B8dUrVw=</latexit><latexit",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 426,
      "token_count": 442,
      "chapter_title": ""
    }
  },
  {
    "content": "sha1_base64=\"RInTcZkWiVBnf/ncBstCvatCtG4=\">AAACSHicbZDPShxBEMZ7Nproxugaj14al4AyMEyvyoyHwGIQPImKq8LOMvT09mhjzx+6a0KWYV4iL5EnySXH3HwGLx4U8SDYs7sHo/mg4eNXVVT1F+VSaHDda6vxbmb2/Ye5+ebHhU+LS63lz6c6KxTjPZbJTJ1HVHMpUt4DAZKf54rTJJL8LLr6VtfPvnOlRZaewCjng4RepCIWjIJBYSs8DssA+A8od/eOT6oKf8VBrCgr113HI5sBiIRrTJyOt2EbtE22p8hzdrY27EAOM9BVWTfYNbKJ43dq59q+4/tV2Gq7jjsWfmvI1LS7O08/f3nLi4dh628wzFiR8BSYpFr3iZvDoKQKBJO8agaF5jllV/SC941NqTlmUI6DqPAXQ4Y4zpR5KeAxfTlR0kTrURKZzoTCpX5dq+H/av0CYn9QijQvgKdssiguJIYM16nioVCcgRwZQ5kS5lbMLqnJEUz2TRMCef3lt+a04xDXIUek3T1AE82hVbSG1hFBHuqifXSIeoih3+gG3aF76491az1Yj5PWhjWdWUH/qNF4BkPYrbk=</latexit><latexit",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 427,
      "token_count": 427,
      "chapter_title": ""
    }
  },
  {
    "content": "sha1_base64=\"RInTcZkWiVBnf/ncBstCvatCtG4=\">AAACSHicbZDPShxBEMZ7Nproxugaj14al4AyMEyvyoyHwGIQPImKq8LOMvT09mhjzx+6a0KWYV4iL5EnySXH3HwGLx4U8SDYs7sHo/mg4eNXVVT1F+VSaHDda6vxbmb2/Ye5+ebHhU+LS63lz6c6KxTjPZbJTJ1HVHMpUt4DAZKf54rTJJL8LLr6VtfPvnOlRZaewCjng4RepCIWjIJBYSs8DssA+A8od/eOT6oKf8VBrCgr113HI5sBiIRrTJyOt2EbtE22p8hzdrY27EAOM9BVWTfYNbKJ43dq59q+4/tV2Gq7jjsWfmvI1LS7O08/f3nLi4dh628wzFiR8BSYpFr3iZvDoKQKBJO8agaF5jllV/SC941NqTlmUI6DqPAXQ4Y4zpR5KeAxfTlR0kTrURKZzoTCpX5dq+H/av0CYn9QijQvgKdssiguJIYM16nioVCcgRwZQ5kS5lbMLqnJEUz2TRMCef3lt+a04xDXIUek3T1AE82hVbSG1hFBHuqifXSIeoih3+gG3aF76491az1Yj5PWhjWdWUH/qNF4BkPYrbk=</latexit><latexit",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 428,
      "token_count": 427,
      "chapter_title": ""
    }
  },
  {
    "content": "sha1_base64=\"fGWl4NCvlvtMu17rjLtk25oWpdc=\">AAACSHicbZBLS+RAFIUrPT7bVzsu3RQ2ghIIqVbpuBgQRZiVqNgqdJpQqa5oYeVB1Y1ME/Lz3Lic3fwGNy6UwZ2VNgtfBwoO372Xe+uEmRQaXPef1fgxMTk1PTPbnJtfWFxqLf8812muGO+xVKbqMqSaS5HwHgiQ/DJTnMah5BfhzUFVv7jlSos0OYNRxgcxvUpEJBgFg4JWcBoUPvA/UOwfnp6VJf6F/UhRVmy4Tpds+SBirjFxOt1N26AdslOjrrO7vWn7cpiCLouqwa6QTRyvUznX9hzPK4NW23XcsfBXQ2rTRrWOg9Zff5iyPOYJMEm17hM3g0FBFQgmedn0c80zym7oFe8bm1BzzKAYB1HidUOGOEqVeQngMX0/UdBY61Ecms6YwrX+XKvgd7V+DpE3KESS5cAT9rYoyiWGFFep4qFQnIEcGUOZEuZWzK6pyRFM9k0TAvn85a/mvOMQ1yEnpL13VMcxg1bRGtpABHXRHvqNjlEPMXSHHtATerburUfrv/Xy1tqw6pkV9EGNxisxMKq0</latexit>1.27",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 429,
      "token_count": 409,
      "chapter_title": ""
    }
  },
  {
    "content": "7.941.827.908.88idf",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 430,
      "token_count": 10,
      "chapter_title": ""
    }
  },
  {
    "content": "weightsImportance Weighting(Optional)Maximum Similarityx<latexit sha1_base64=\"f2yzimwbR/Dgjzp6tZ360fHRqNI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64=\"f2yzimwbR/Dgjzp6tZ360fHRqNI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 431,
      "token_count": 736,
      "chapter_title": ""
    }
  },
  {
    "content": "sha1_base64=\"f2yzimwbR/Dgjzp6tZ360fHRqNI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64=\"f2yzimwbR/Dgjzp6tZ360fHRqNI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit>\u02c6x<latexit",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 432,
      "token_count": 726,
      "chapter_title": ""
    }
  },
  {
    "content": "sha1_base64=\"5QTnVRVSrnyzznVU7d5bF5u03Iw=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit><latexit sha1_base64=\"5QTnVRVSrnyzznVU7d5bF5u03Iw=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit><latexit",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 433,
      "token_count": 696,
      "chapter_title": ""
    }
  },
  {
    "content": "sha1_base64=\"5QTnVRVSrnyzznVU7d5bF5u03Iw=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit><latexit sha1_base64=\"5QTnVRVSrnyzznVU7d5bF5u03Iw=\">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit>",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 434,
      "token_count": 694,
      "chapter_title": ""
    }
  },
  {
    "content": "ReferenceFigure 1: Illustration of the computation of the recall metric RBERT. Given the reference xand\ncandidate \u02c6x, we compute BERT embeddings and pairwise cosine similarity. We highlight the greedy\nmatching in red, and include the optional idfimportance weighting.\nWe experiment with different models (Section 4), using the tokenizer provided with each model.\nGiven a tokenized reference sentence x=hx1,...,x ki, the embedding model generates a se-\nquence of vectors hx1,...,xki. Similarly, the tokenized candidate \u02c6x=h\u02c6x1,..., \u02c6xmiis mapped\ntoh\u02c6x1,...,\u02c6xli. The main model we use is BERT, which tokenizes the input text into a sequence\nof word pieces (Wu et al., 2016), where unknown words are split into several commonly observed\nsequences of characters. The representation for each word piece is computed with a Transformer\nencoder (Vaswani et al., 2017) by repeatedly applying self-attention and nonlinear transformations\nin an alternating fashion. BERT embeddings have been shown to bene\ufb01t various NLP tasks (Devlin\net al., 2019; Liu, 2019; Huang et al., 2019; Yang et al., 2019a).\nSimilarity Measure The vector representation allows for a soft measure of similarity instead of\nexact-string (Papineni et al., 2002) or heuristic (Banerjee & Lavie, 2005) matching. The cosine\nsimilarity of a reference token xiand a candidate token \u02c6xjisx>\ni\u02c6xj\nkxikk\u02c6xjk. We use pre-normalized\nvectors, which reduces this calculation to the inner product x>\ni\u02c6xj. While this measure considers\ntokens in isolation, the contextual embeddings contain information from the rest of the sentence.\nBERTS CORE The complete score matches each token in xto a token in \u02c6xto compute recall,\nand each token in \u02c6xto a token in xto compute precision. We use greedy matching to maximize\nthe matching similarity score,2where each token is matched to the most similar token in the other\nsentence. We combine precision and recall to compute an F1 measure. For a reference xand\ncandidate \u02c6x, the recall, precision, and F1 scores are:\nRBERT =1\n|x|X\nxi2xmax\n\u02c6xj2\u02c6xx>\ni\u02c6xj,P BERT =1\n|\u02c6x|X\n\u02c6xj2\u02c6xmax\nxi2xx>\ni\u02c6xj,F BERT =2PBERT \u00b7RBERT\nPBERT +RBERT.\nImportance Weighting Previous work on similarity measures demonstrated that rare words can\nbe more indicative for sentence similarity than common words (Banerjee & Lavie, 2005; Vedantam\net al., 2015). BERTS CORE enables us to easily incorporate importance weighting. We experiment\nwith inverse document frequency ( idf) scores computed from the test corpus. Given Mreference\nsentences {x(i)}M\ni=1, the idfscore of a word-piece token wis\nidf(w)=\u0000log1\nMMX\ni=1I[w2x(i)],\nwhere I[\u00b7]is an indicator function. We do not use the full tf-idfmeasure because we process single\nsentences, where the term frequency ( tf) is likely 1. For example, recall with idfweighting is\nRBERT =P\nxi2xidf(xi) max \u02c6xj2\u02c6xx>",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 435,
      "token_count": 783,
      "chapter_title": ""
    }
  },
  {
    "content": "candidate \u02c6x, the recall, precision, and F1 scores are:\nRBERT =1\n|x|X\nxi2xmax\n\u02c6xj2\u02c6xx>\ni\u02c6xj,P BERT =1\n|\u02c6x|X\n\u02c6xj2\u02c6xmax\nxi2xx>\ni\u02c6xj,F BERT =2PBERT \u00b7RBERT\nPBERT +RBERT.\nImportance Weighting Previous work on similarity measures demonstrated that rare words can\nbe more indicative for sentence similarity than common words (Banerjee & Lavie, 2005; Vedantam\net al., 2015). BERTS CORE enables us to easily incorporate importance weighting. We experiment\nwith inverse document frequency ( idf) scores computed from the test corpus. Given Mreference\nsentences {x(i)}M\ni=1, the idfscore of a word-piece token wis\nidf(w)=\u0000log1\nMMX\ni=1I[w2x(i)],\nwhere I[\u00b7]is an indicator function. We do not use the full tf-idfmeasure because we process single\nsentences, where the term frequency ( tf) is likely 1. For example, recall with idfweighting is\nRBERT =P\nxi2xidf(xi) max \u02c6xj2\u02c6xx>\ni\u02c6xjP\nxi2xidf(xi).\nBecause we use reference sentences to compute idf, the idfscores remain the same for all systems\nevaluated on a speci\ufb01c test set. We apply plus-one smoothing to handle unknown word pieces.\n2We compare greedy matching with optimal assignment in Appendix C.\n4\nFigure 13.11 The computation of BERTS CORE recall from reference xand candidate \u02c6 x, from Figure 1 in\nZhang et al. (2020). This version shows an extended version of the metric in which tokens are also weighted by\ntheir idf values.\n13.7 Bias and Ethical Issues\nMachine translation raises many of the same ethical issues that we\u2019ve discussed in\nearlier chapters. For example, consider MT systems translating from Hungarian\n(which has the gender neutral pronoun \u02ddo) or Spanish (which often drops pronouns)\ninto English (in which pronouns are obligatory, and they have grammatical gender).\nWhen translating a reference to a person described without speci\ufb01ed gender, MT\nsystems often default to male gender (Schiebinger 2014, Prates et al. 2019). And\nMT systems often assign gender according to culture stereotypes of the sort we saw\nin Section 6.11. Fig. 13.12 shows examples from Prates et al. (2019), in which Hun-\ngarian gender-neutral \u02ddo is a nurse is translated with she, but gender-neutral \u02ddo is a\nCEO is translated with he. Prates et al. (2019) \ufb01nd that these stereotypes can\u2019t com-\npletely be accounted for by gender bias in US labor statistics, because the biases are",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 436,
      "token_count": 650,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 293\n\n13.8 \u2022 S UMMARY 285\nampli\ufb01ed by MT systems, with pronouns being mapped to male or female gender\nwith a probability higher than if the mapping was based on actual labor employment\nstatistics.\nHungarian (gender neutral) source English MT output\n\u02ddo egy \u00b4apol\u00b4o she is a nurse\n\u02ddo egy tud \u00b4os he is a scientist\n\u02ddo egy m \u00b4ern\u00a8ok he is an engineer\n\u02ddo egy p \u00b4ek he is a baker\n\u02ddo egy tan \u00b4ar she is a teacher\n\u02ddo egy esk \u00a8uv\u02ddoszervez \u02ddo she is a wedding organizer\n\u02ddo egy vez \u00b4erigazgat \u00b4o he is a CEO\nFigure 13.12 When translating from gender-neutral languages like Hungarian into English,\ncurrent MT systems interpret people from traditionally male-dominated occupations as male,\nand traditionally female-dominated occupations as female (Prates et al., 2019).\nSimilarly, a recent challenge set, the WinoMT dataset (Stanovsky et al., 2019)\nshows that MT systems perform worse when they are asked to translate sentences\nthat describe people with non-stereotypical gender roles, like \u201cThe doctor asked the\nnurse to help her in the operation\u201d.\nMany ethical questions in MT require further research. One open problem is\ndeveloping metrics for knowing what our systems don\u2019t know. This is because MT\nsystems can be used in urgent situations where human translators may be unavailable\nor delayed: in medical domains, to help translate when patients and doctors don\u2019t\nspeak the same language, or in legal domains, to help judges or lawyers communi-\ncate with witnesses or defendants. In order to \u2018do no harm\u2019, systems need ways to\nassign con\ufb01dence values to candidate translations, so they can abstain from giving con\ufb01dence\nincorrect translations that may cause harm.\n13.8 Summary\nMachine translation is one of the most widely used applications of NLP, and the\nencoder-decoder model, \ufb01rst developed for MT is a key tool that has applications\nthroughout NLP.\n\u2022 Languages have divergences , both structural and lexical, that make translation\ndif\ufb01cult.\n\u2022 The linguistic \ufb01eld of typology investigates some of these differences; lan-\nguages can be classi\ufb01ed by their position along typological dimensions like\nwhether verbs precede their objects.\n\u2022Encoder-decoder networks (for transformers just as we saw in Chapter 8 for\nRNNs) are composed of an encoder network that takes an input sequence\nand creates a contextualized representation of it, the context . This context\nrepresentation is then passed to a decoder which generates a task-speci\ufb01c\noutput sequence.\n\u2022Cross-attention allows the transformer decoder to view information from all\nthe hidden states of the encoder.\n\u2022 Machine translation models are trained on a parallel corpus , sometimes called\nabitext , a text that appears in two (or more) languages.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 437,
      "token_count": 646,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 294",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 438,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "286 CHAPTER 13 \u2022 M ACHINE TRANSLATION\n\u2022Backtranslation is a way of making use of monolingual corpora in the target\nlanguage by running a pilot MT engine backwards to create synthetic bitexts.\n\u2022 MT is evaluated by measuring a translation\u2019s adequacy (how well it captures\nthe meaning of the source sentence) and \ufb02uency (how \ufb02uent or natural it is\nin the target language). Human evaluation is the gold standard, but automatic\nevaluation metrics like chrF , which measure character n-gram overlap with\nhuman translations, or more recent metrics based on embedding similarity,\nare also commonly used.\nBibliographical and Historical Notes\nMT was proposed seriously by the late 1940s, soon after the birth of the computer\n(Weaver, 1949/1955). In 1954, the \ufb01rst public demonstration of an MT system pro-\ntotype (Dostert, 1955) led to great excitement in the press (Hutchins, 1997). The\nnext decade saw a great \ufb02owering of ideas, pre\ufb01guring most subsequent develop-\nments. But this work was ahead of its time\u2014implementations were limited by, for\nexample, the fact that pending the development of disks there was no good way to\nstore dictionary information.\nAs high-quality MT proved elusive (Bar-Hillel, 1960), there grew a consensus\non the need for better evaluation and more basic research in the new \ufb01elds of for-\nmal and computational linguistics. This consensus culminated in the famously crit-\nical ALPAC (Automatic Language Processing Advisory Committee) report of 1966\n(Pierce et al., 1966) that led in the mid 1960s to a dramatic cut in funding for MT\nin the US. As MT research lost academic respectability, the Association for Ma-\nchine Translation and Computational Linguistics dropped MT from its name. Some\nMT developers, however, persevered, and there were early MT systems like M \u00b4et\u00b4eo,\nwhich translated weather forecasts from English to French (Chandioux, 1976), and\nindustrial systems like Systran.\nIn the early years, the space of MT architectures spanned three general mod-\nels. In direct translation , the system proceeds word-by-word through the source-\nlanguage text, translating each word incrementally. Direct translation uses a large\nbilingual dictionary, each of whose entries is a small program with the job of trans-\nlating one word. In transfer approaches, we \ufb01rst parse the input text and then ap-\nply rules to transform the source-language parse into a target language parse. We\nthen generate the target language sentence from the parse tree. In interlingua ap-\nproaches, we analyze the source language text into some abstract meaning repre-\nsentation, called an interlingua . We then generate into the target language from\nthis interlingual representation. A common way to visualize these three early ap-\nproaches was the Vauquois triangle shown in Fig. 13.13. The triangle shows theVauquois\ntriangle\nincreasing depth of analysis required (on both the analysis and generation end) as\nwe move from the direct approach through transfer approaches to interlingual ap-\nproaches. In addition, it shows the decreasing amount of transfer knowledge needed\nas we move up the triangle, from huge amounts of transfer at the direct level (al-\nmost all knowledge is transfer knowledge for each word) through transfer (transfer\nrules only for parse trees or thematic roles) through interlingua (no speci\ufb01c transfer",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 439,
      "token_count": 761,
      "chapter_title": ""
    }
  },
  {
    "content": "els. In direct translation , the system proceeds word-by-word through the source-\nlanguage text, translating each word incrementally. Direct translation uses a large\nbilingual dictionary, each of whose entries is a small program with the job of trans-\nlating one word. In transfer approaches, we \ufb01rst parse the input text and then ap-\nply rules to transform the source-language parse into a target language parse. We\nthen generate the target language sentence from the parse tree. In interlingua ap-\nproaches, we analyze the source language text into some abstract meaning repre-\nsentation, called an interlingua . We then generate into the target language from\nthis interlingual representation. A common way to visualize these three early ap-\nproaches was the Vauquois triangle shown in Fig. 13.13. The triangle shows theVauquois\ntriangle\nincreasing depth of analysis required (on both the analysis and generation end) as\nwe move from the direct approach through transfer approaches to interlingual ap-\nproaches. In addition, it shows the decreasing amount of transfer knowledge needed\nas we move up the triangle, from huge amounts of transfer at the direct level (al-\nmost all knowledge is transfer knowledge for each word) through transfer (transfer\nrules only for parse trees or thematic roles) through interlingua (no speci\ufb01c transfer\nknowledge). We can view the encoder-decoder network as an interlingual approach,\nwith attention acting as an integration of direct and transfer, allowing words or their\nrepresentations to be directly accessed by the decoder.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 440,
      "token_count": 329,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 295",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 441,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 287\nsourcetexttarget textDirect TranslationTransferInterlinguaSource Text:Semantic/SyntacticStructureTarget Text:Semantic/SyntacticStructuresource languageanalysissource languageanalysistarget language generation\nFigure 13.13 The Vauquois (1968) triangle.\nStatistical methods began to be applied around 1990, enabled \ufb01rst by the devel-\nopment of large bilingual corpora like the Hansard corpus of the proceedings of the\nCanadian Parliament, which are kept in both French and English, and then by the\ngrowth of the Web. Early on, a number of researchers showed that it was possible\nto extract pairs of aligned sentences from bilingual corpora, using words or simple\ncues like sentence length (Kay and R \u00a8oscheisen 1988, Gale and Church 1991, Gale\nand Church 1993, Kay and R \u00a8oscheisen 1993).\nAt the same time, the IBM group, drawing directly on the noisy channel model\nfor speech recognition, proposed two related paradigms for statistical MT . These statistical MT\ninclude the generative algorithms that became known as IBM Models 1 through IBM Models\n5, implemented in the Candide system. The algorithms (except for the decoder) Candide\nwere published in full detail\u2014 encouraged by the US government who had par-\ntially funded the work\u2014 which gave them a huge impact on the research community\n(Brown et al. 1990, Brown et al. 1993).\nThe group also developed a discriminative approach, called MaxEnt (for maxi-\nmum entropy, an alternative formulation of logistic regression), which allowed many\nfeatures to be combined discriminatively rather than generatively (Berger et al.,\n1996), which was further developed by Och and Ney (2002).\nBy the turn of the century, most academic research on machine translation used\nstatistical MT, either in the generative or discriminative mode. An extended version\nof the generative approach, called phrase-based translation was developed, basedphrase-based\ntranslation\non inducing translations for phrase-pairs (Och 1998, Marcu and Wong 2002, Koehn\net al. (2003), Och and Ney 2004, Deng and Byrne 2005, inter alia).\nOnce automatic metrics like BLEU were developed (Papineni et al., 2002), the\ndiscriminative log linear formulation (Och and Ney, 2004), drawing from the IBM\nMaxEnt work (Berger et al., 1996), was used to directly optimize evaluation metrics\nlike BLEU in a method known as Minimum Error Rate Training , orMERT (Och, MERT\n2003), also drawing from speech recognition models (Chou et al., 1993). Toolkits\nlike GIZA (Och and Ney, 2003) and Moses (Koehn et al. 2006, Zens and Ney 2007) Moses\nwere widely used.\nThere were also approaches around the turn of the century that were based on\nsyntactic structure (Chapter 18). Models based on transduction grammars (alsotransduction\ngrammars\ncalled synchronous grammars ) assign a parallel syntactic tree structure to a pair\nof sentences in different languages, with the goal of translating the sentences by\napplying reordering operations on the trees. From a generative perspective, we can\nview a transduction grammar as generating pairs of aligned sentences in two lan-\nguages. Some of the most widely used models included the inversion transduction",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 442,
      "token_count": 770,
      "chapter_title": ""
    }
  },
  {
    "content": "Once automatic metrics like BLEU were developed (Papineni et al., 2002), the\ndiscriminative log linear formulation (Och and Ney, 2004), drawing from the IBM\nMaxEnt work (Berger et al., 1996), was used to directly optimize evaluation metrics\nlike BLEU in a method known as Minimum Error Rate Training , orMERT (Och, MERT\n2003), also drawing from speech recognition models (Chou et al., 1993). Toolkits\nlike GIZA (Och and Ney, 2003) and Moses (Koehn et al. 2006, Zens and Ney 2007) Moses\nwere widely used.\nThere were also approaches around the turn of the century that were based on\nsyntactic structure (Chapter 18). Models based on transduction grammars (alsotransduction\ngrammars\ncalled synchronous grammars ) assign a parallel syntactic tree structure to a pair\nof sentences in different languages, with the goal of translating the sentences by\napplying reordering operations on the trees. From a generative perspective, we can\nview a transduction grammar as generating pairs of aligned sentences in two lan-\nguages. Some of the most widely used models included the inversion transduction\ngrammar (Wu, 1996) and synchronous context-free grammars (Chiang, 2005),inversion\ntransduction\ngrammar",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 443,
      "token_count": 302,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 296\n\n288 CHAPTER 13 \u2022 M ACHINE TRANSLATION\nNeural networks had been applied at various times to various aspects of machine\ntranslation; for example Schwenk et al. (2006) showed how to use neural language\nmodels to replace n-gram language models in a Spanish-English system based on\nIBM Model 4. The modern neural encoder-decoder approach was pioneered by\nKalchbrenner and Blunsom (2013), who used a CNN encoder and an RNN decoder,\nand was \ufb01rst applied to MT by Bahdanau et al. (2015). The transformer encoder-\ndecoder was proposed by Vaswani et al. (2017) (see the History section of Chap-\nter 9).\nResearch on evaluation of machine translation began quite early. Miller and\nBeebe-Center (1956) proposed a number of methods drawing on work in psycholin-\nguistics. These included the use of cloze and Shannon tasks to measure intelligibility\nas well as a metric of edit distance from a human translation, the intuition that un-\nderlies all modern overlap-based automatic evaluation metrics. The ALPAC report\nincluded an early evaluation study conducted by John Carroll that was extremely in-\n\ufb02uential (Pierce et al., 1966, Appendix 10). Carroll proposed distinct measures for\n\ufb01delity and intelligibility, and had raters score them subjectively on 9-point scales.\nMuch early evaluation work focuses on automatic word-overlap metrics like BLEU\n(Papineni et al., 2002), NIST (Doddington, 2002), TER (Translation Error Rate)\n(Snover et al., 2006), Precision and Recall (Turian et al., 2003), and METEOR\n(Banerjee and Lavie, 2005); character n-gram overlap methods like chrF (Popovi \u00b4c,\n2015) came later. More recent evaluation work, echoing the ALPAC report, has\nemphasized the importance of careful statistical methodology and the use of human\nevaluation (Kocmi et al., 2021; Marie et al., 2021).\nThe early history of MT is surveyed in Hutchins 1986 and 1997; Nirenburg et al.\n(2002) collects early readings. See Croft (1990) or Comrie (1989) for introductions\nto linguistic typology.\nExercises\n13.1 Compute by hand the chrF2,2 score for HYP2 on page 282 (the answer should\nround to .62).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 444,
      "token_count": 550,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 297\n\nCHAPTER\n14Question Answering, Informa-\ntion Retrieval, and Retrieval-\nAugmented Generation\nPeople need to know things. So pretty much as soon as there were computers we\nwere asking them questions. Systems in the 1960s were answering questions about\nbaseball statistics and scienti\ufb01c facts. Even \ufb01ctional computers in the 1970s like\nDeep Thought, invented by Douglas Adams in The Hitchhiker\u2019s Guide to the Galaxy ,\nanswered \u201cthe Ultimate Question Of Life, The Universe, and Everything\u201d.1And\nbecause so much knowledge is encoded in text, question answering ( QA) systems QA\nwere performing at human levels even before LLMs: IBM\u2019s Watson system won the\nTV game-show Jeopardy! in 2011, surpassing humans at answering questions like:\nWILLIAM WILKINSON\u2019S \u201cAN ACCOUNT OF THE PRINCIPALITIES OF WALLACHIA AND MOLDOVIA\u201dINSPIRED THIS AUTHOR\u2019S MOST FAMOUS NOVEL\n2\nQuestion answering systems are designed to \ufb01ll human information needs .\nSince a lot of information is present in text form (on the web or in other data like\nour email, or books), question answering is closely related to the task behind search\nengines. Indeed, the distinction is becoming ever more fuzzy, as modern search\nengines are integrated with large language models trained to do question answering.\nQuestion answering systems often focus on a useful subset of information needs:\nfactoid questions , questions of fact or reasoning that can be answered with simple\nfacts expressed in short or medium-length texts, like the following:\n(14.1) Where is the Louvre Museum located?\n(14.2) Where does the energy in a nuclear explosion come from?\n(14.3) How to get a script l in latex?\nModern NLP systems answer these questions using large language models, in\none of two ways. The \ufb01rst is to make use of the method from Chapter 12: prompt\na pretrained and instruction-tuned LLM, an LLM that has been \ufb01netuned on ques-\ntion/answer datasets with the question in the prompt. For example, we could prompt\na causal language model with a string like\nQ: Where is the Louvre Museum located? A:\nhave it do conditional generation given this pre\ufb01x, and take the response as the an-\nswer. The idea is that language models have read a lot of facts in their pretraining\ndata, presumably including the location of the Louvre, and have encoded this infor-\nmation in their parameters.\nSimply prompting an LLM can be a useful approach to answer many factoid\nquestions. But it is not yet a complete solution for question answering.\n1The answer was 42, but unfortunately the question was never revealed.\n2The answer, of course, is \u2018Who is Bram Stoker\u2019, and the novel was Dracula .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 445,
      "token_count": 615,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 298",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 446,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "290 CHAPTER 14 \u2022 Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG\nThe \ufb01rst and main problem is that large language models often give the wrong\nanswer! Large language models hallucinate . A hallucination is a response that is hallucinate\nnot faithful to the facts of the world. That is, when asked questions, large language\nmodels simply make up answers that sound reasonable. For example, Dahl et al.\n(2024) found that when asked questions about the legal domain (like about particular\nlegal cases), large language models hallucinated from 69% to 88% of the time!\nAnd it\u2019s not always possible to tell when language models are hallucinating,\npartly because LLMs aren\u2019t well- calibrated . In a calibrated system, the con\ufb01dence calibrated\nof a system in the correctness of its answer is highly correlated with the probability\nof an answer being correct. So if a calibrated system is wrong, at least it might hedge\nits answer or tell us to go check another source. But since language models are not\nwell-calibrated, they often give a very wrong answer with complete certainty (Zhou\net al., 2024).\nA second problem is that simply prompting a large language model doesn\u2019t allow\nus to ask questions about proprietary data. A common use of question answering is\nabout data like our personal email or medical records. Or a company may have\ninternal documents that contain answers for customer service or internal use. Or\nlegal \ufb01rms need to ask questions about legal discovery from proprietary documents.\nFinally, static large language models also have problems with questions about\nrapidly changing information (like questions about something that happened last\nweek) since LLMs won\u2019t have up-to-date information from after their release data.\nFor this reason the most common way to do question-answering with LLMs is\nretrieval-augmented generation orRAG , and that is the method we will focus on RAG\nin this chapter. In RAG we use information retrieval (IR) techniques to retrieveinformation\nretrieval\ndocuments that are likely to have information that might help answer the question.\nThen we use a large language model to generate an answer given these documents.\nBasing our answers on retrieved documents can solve some of the problems with\nusing simple prompting to answer questions. First, it helps ensure that the answer is\ngrounded in facts from some curated dataset. And the system can give the user the\nanswer accompanied by the context of the passage or document the answer came\nfrom. This information can help users have con\ufb01dence in the accuracy of the answer\n(or help them spot when it is wrong!). And these retrieval techniques can be used on\nany proprietary data we want, such as legal or medical data for those applications.\nWe\u2019ll begin by introducing information retrieval, the task of choosing the most\nrelevant document from a document set given a user\u2019s query expressing their infor-\nmation need. We\u2019ll see the classic method based on cosines of sparse tf-idf vectors,\na modern neural \u2018dense\u2019 retrievers based on instead representing queries and docu-\nments neurally with BERT or other language models. We then introduce retriever-\nbased question answering and the retrieval-augmented generation paradigm.\nFinally, we\u2019ll discuss various QA datasets. These are used for \ufb01netuning LLMs\nin instruction tuning, as we saw in Chapter 12. And they are also used as bench-\nmarks, since question answering has an important function as a benchmark for mea-\nsuring the abilities of language models.\n14.1 Information Retrieval\nInformation retrieval orIRis the name of the \ufb01eld encompassing the retrieval of allinformation\nretrieval",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 447,
      "token_count": 777,
      "chapter_title": ""
    }
  },
  {
    "content": "grounded in facts from some curated dataset. And the system can give the user the\nanswer accompanied by the context of the passage or document the answer came\nfrom. This information can help users have con\ufb01dence in the accuracy of the answer\n(or help them spot when it is wrong!). And these retrieval techniques can be used on\nany proprietary data we want, such as legal or medical data for those applications.\nWe\u2019ll begin by introducing information retrieval, the task of choosing the most\nrelevant document from a document set given a user\u2019s query expressing their infor-\nmation need. We\u2019ll see the classic method based on cosines of sparse tf-idf vectors,\na modern neural \u2018dense\u2019 retrievers based on instead representing queries and docu-\nments neurally with BERT or other language models. We then introduce retriever-\nbased question answering and the retrieval-augmented generation paradigm.\nFinally, we\u2019ll discuss various QA datasets. These are used for \ufb01netuning LLMs\nin instruction tuning, as we saw in Chapter 12. And they are also used as bench-\nmarks, since question answering has an important function as a benchmark for mea-\nsuring the abilities of language models.\n14.1 Information Retrieval\nInformation retrieval orIRis the name of the \ufb01eld encompassing the retrieval of allinformation\nretrieval\nIR manner of media based on user information needs. The resulting IR system is often\ncalled a search engine . Our goal in this section is to give a suf\ufb01cient overview of IR",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 448,
      "token_count": 318,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 299\n\n14.1 \u2022 I NFORMATION RETRIEVAL 291\nto see its application to question answering. Readers with more interest speci\ufb01cally\nin information retrieval should see the Historical Notes section at the end of the\nchapter and textbooks like Manning et al. (2008).\nThe IR task we consider is called ad hoc retrieval , in which a user poses a ad hoc retrieval\nquery to a retrieval system, which then returns an ordered set of documents from\nsome collection . Adocument refers to whatever unit of text the system indexes and document\nretrieves (web pages, scienti\ufb01c papers, news articles, or even shorter passages like\nparagraphs). A collection refers to a set of documents being used to satisfy user collection\nrequests. A term refers to a word in a collection, but it may also include phrases. term\nFinally, a query represents a user\u2019s information need expressed as a set of terms. query\nThe high-level architecture of an ad hoc retrieval engine is shown in Fig. 14.1.\nDocumentDocumentDocumentDocumentDocumentDocumentQuery ProcessingIndexingSearchDocumentDocumentDocumentDocumentDocumentRanked DocumentsDocumentqueryInvertedIndexqueryvectordocument collection\nFigure 14.1 The architecture of an ad hoc IR system.\nThe basic IR architecture uses the vector space model we introduced in Chap-\nter 6, in which we map queries and document to vectors based on unigram word\ncounts, and use the cosine similarity between the vectors to rank potential documents\n(Salton, 1971). This is thus an example of the bag-of-words model introduced in\nChapter 4, since words are considered independently of their positions.\n14.1.1 Term weighting and document scoring\nLet\u2019s look at the details of how the match between a document and query is scored.\nWe don\u2019t use raw word counts in IR, instead computing a term weight for each term weight\ndocument word. Two term weighting schemes are common: the tf-idf weighting\nintroduced in Chapter 6, and a slightly more powerful variant called BM25 . BM25\nWe\u2019ll reintroduce tf-idf here so readers don\u2019t need to look back at Chapter 6.\nTf-idf (the \u2018-\u2019 here is a hyphen, not a minus sign) is the product of two terms, the\nterm frequency tfand the inverse document frequency idf.\nThe term frequency tells us how frequent the word is; words that occur more\noften in a document are likely to be informative about the document\u2019s contents. We\nusually use the log 10of the word frequency, rather than the raw count. The intuition\nis that a word appearing 100 times in a document doesn\u2019t make that word 100 times\nmore likely to be relevant to the meaning of the document. We also need to do\nsomething special with counts of 0, since we can\u2019t take the log of 0.3\ntft;d=(\n1+log10count (t;d) if count (t;d)>0\n0 otherwise(14.4)\n3We can also use this alternative formulation, which we have used in earlier editions: tf t;d=\nlog10(count (t;d)+1)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 449,
      "token_count": 663,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 300\n\n292 CHAPTER 14 \u2022 Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG\nIf we use log weighting, terms which occur 0 times in a document would have tf =0,\n1 times in a document tf =1+log10(1) =1+0=1, 10 times in a document tf =\n1+log10(10) =2, 100 times tf =1+log10(100) =3, 1000 times tf =4, and so on.\nThe document frequency dftof a term tis the number of documents it oc-\ncurs in. Terms that occur in only a few documents are useful for discriminating\nthose documents from the rest of the collection; terms that occur across the entire\ncollection aren\u2019t as helpful. The inverse document frequency oridfterm weight\n(Sparck Jones, 1972) is de\ufb01ned as:\nidft=log10N\ndft(14.5)\nwhere Nis the total number of documents in the collection, and df tis the number\nof documents in which term toccurs. The fewer documents in which a term occurs,\nthe higher this weight; the lowest weight of 0 is assigned to terms that occur in every\ndocument.\nHere are some idf values for some words in the corpus of Shakespeare plays,\nranging from extremely informative words that occur in only one play like Romeo ,\nto those that occur in a few like salad orFalstaff , to those that are very common like\nfoolor so common as to be completely non-discriminative since they occur in all 37\nplays like good orsweet .4\nWord df idf\nRomeo 1 1.57\nsalad 2 1.27\nFalstaff 4 0.967\nforest 12 0.489\nbattle 21 0.246\nwit 34 0.037\nfool 36 0.012\ngood 37 0\nsweet 37 0\nThetf-idf value for word tin document dis then the product of term frequency\ntft;dand IDF:\ntf-idf(t;d) =tft;d\u0001idft (14.6)\n14.1.2 Document Scoring\nWe score document dby the cosine of its vector dwith the query vector q:\nscore(q;d) =cos(q;d) =q\u0001d\njqjjdj(14.7)\nAnother way to think of the cosine computation is as the dot product of unit vectors;\nwe \ufb01rst normalize both the query and document vector to unit vectors, by dividing\nby their lengths, and then take the dot product:\nscore(q;d) =cos(q;d) =q\njqj\u0001d\njdj(14.8)\n4Sweet was one of Shakespeare\u2019s favorite adjectives, a fact probably related to the increased use of\nsugar in European recipes around the turn of the 16th century (Jurafsky, 2014, p. 175).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 450,
      "token_count": 637,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 301",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 451,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "14.1 \u2022 I NFORMATION RETRIEVAL 293\nWe can spell out Eq. 14.8, using the tf-idf values and spelling out the dot product as\na sum of products:\nscore(q;d) =X\nt2qtf-idf(t;q)qP\nqi2qtf-idf2(qi;q)\u0001tf-idf(t;d)qP\ndi2dtf-idf2(di;d)(14.9)\nNow let\u2019s use Eq. 14.9 to walk through an example of a tiny query against a\ncollection of 4 nano documents, computing tf-idf values and seeing the rank of the\ndocuments. We\u2019ll assume all words in the following query and documents are down-\ncased and punctuation is removed:\nQuery :sweet love\nDoc 1 :Sweet sweet nurse! Love?\nDoc 2 :Sweet sorrow\nDoc 3 :How sweet is love?\nDoc 4 :Nurse!\nFig. 14.2 shows the computation of the tf-idf cosine between the query and Doc-\nument 1, and the query and Document 2. The cosine is the normalized dot product\nof tf-idf values, so for the normalization we must need to compute the document\nvector lengthsjqj,jd1j, andjd2jfor the query and the \ufb01rst two documents using\nEq. 14.4, Eq. 14.5, Eq. 14.6, and Eq. 14.9 (computations for Documents 3 and 4 are\nalso needed but are left as an exercise for the reader). The dot product between the\nvectors is the sum over dimensions of the product, for each dimension, of the values\nof the two tf-idf vectors for that dimension. This product is only non-zero where\nboth the query and document have non-zero values, so for this example, in which\nonly sweet andlove have non-zero values in the query, the dot product will be the\nsum of the products of those elements of each vector.\nDocument 1 has a higher cosine with the query (0.747) than Document 2 has\nwith the query (0.0779), and so the tf-idf cosine model would rank Document 1\nabove Document 2. This ranking is intuitive given the vector space model, since\nDocument 1 has both terms including two instances of sweet , while Document 2 is\nmissing one of the terms. We leave the computation for Documents 3 and 4 as an\nexercise for the reader.\nIn practice, there are many variants and approximations to Eq. 14.9. For exam-\nple, we might choose to simplify processing by removing some terms. To see this,\nlet\u2019s start by expanding the formula for tf-idf in Eq. 14.9 to explicitly mention the tf\nand idf terms from Eq. 14.6:\nscore(q;d) =X\nt2qtft;q\u0001idftqP\nqi2qtf-idf2(qi;q)\u0001tft;d\u0001idftqP\ndi2dtf-idf2(di;d)(14.10)\nIn one common variant of tf-idf cosine, for example, we drop the idf term for the\ndocument. Eliminating the second copy of the idf term (since the identical term is\nalready computed for the query) turns out to sometimes result in better performance:\nscore(q;d) =X\nt2qtft;q\u0001idftqP\nqi2qtf-idf2(qi;q)\u0001tft;d\u0001idftqP\ndi2dtf-idf2(di;d)(14.11)\nOther variants of tf-idf eliminate various other terms.\nA slightly more complex variant in the tf-idf family is the BM25 weighting BM25",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 452,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 302",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 453,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "294 CHAPTER 14 \u2022 Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG\nQuery\nword cnt tf df idf tf-idf n\u2019lized = tf-idf/jqj\nsweet 1 1 3 0.125 0.125 0.383\nnurse 0 0 2 0.301 0 0\nlove 1 1 2 0.301 0.301 0.924\nhow 0 0 1 0.602 0 0\nsorrow 0 0 1 0.602 0 0\nis 0 0 1 0.602 0 0\njqj=p\n:1252+:3012=:326\nDocument 1 Document 2\nword cnt tf tf-idf n\u2019lized \u0002q cnt tf tf-idf n\u2019lized \u0002q\nsweet 2 1.301 0.163 0.357 0.137 1 1.000 0.125 0.203 0.0779\nnurse 1 1.000 0.301 0.661 0 0 0 0 0 0\nlove 1 1.000 0.301 0.661 0.610 0 0 0 0 0\nhow 0 0 0 0 0 0 0 0 0 0\nsorrow 0 0 0 0 0 1 1.000 0.602 0.979 0\nis 0 0 0 0 0 0 0 0 0 0\njd1j=p\n:1632+:3012+:3012=:456 jd2j=p\n:1252+:6022=:615\nCosine:Pof column: 0.747 Cosine:Pof column: 0.0779\nFigure 14.2 Computation of tf-idf cosine score between the query and nano-documents 1 (0.747) and 2\n(0.0779), using Eq. 14.4, Eq. 14.5, Eq. 14.6 and Eq. 14.9.\nscheme (sometimes called Okapi BM25 after the Okapi IR system in which it was\nintroduced (Robertson et al., 1995)). BM25 adds two parameters: k, a knob that\nadjust the balance between term frequency and IDF, and b, which controls the im-\nportance of document length normalization. The BM25 score of a document dgiven\na query qis:\nX\nt2qIDFz}|{\nlog\u0012N\ndft\u0013weighted tfz}|{\ntft;d\nk\u0010\n1\u0000b+b\u0010\njdj\njdavgj\u0011\u0011\n+tft;d(14.12)\nwherejdavgjis the length of the average document. When kis 0, BM25 reverts to\nno use of term frequency, just a binary selection of terms in the query (plus idf).\nA large kresults in raw term frequency (plus idf). branges from 1 (scaling by\ndocument length) to 0 (no length scaling). Manning et al. (2008) suggest reasonable\nvalues are k = [1.2,2] and b = 0.75. Kamphuis et al. (2020) is a useful summary of\nthe many minor variants of BM25.\nStop words In the past it was common to remove high-frequency words from both\nthe query and document before representing them. The list of such high-frequency",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 454,
      "token_count": 780,
      "chapter_title": ""
    }
  },
  {
    "content": "introduced (Robertson et al., 1995)). BM25 adds two parameters: k, a knob that\nadjust the balance between term frequency and IDF, and b, which controls the im-\nportance of document length normalization. The BM25 score of a document dgiven\na query qis:\nX\nt2qIDFz}|{\nlog\u0012N\ndft\u0013weighted tfz}|{\ntft;d\nk\u0010\n1\u0000b+b\u0010\njdj\njdavgj\u0011\u0011\n+tft;d(14.12)\nwherejdavgjis the length of the average document. When kis 0, BM25 reverts to\nno use of term frequency, just a binary selection of terms in the query (plus idf).\nA large kresults in raw term frequency (plus idf). branges from 1 (scaling by\ndocument length) to 0 (no length scaling). Manning et al. (2008) suggest reasonable\nvalues are k = [1.2,2] and b = 0.75. Kamphuis et al. (2020) is a useful summary of\nthe many minor variants of BM25.\nStop words In the past it was common to remove high-frequency words from both\nthe query and document before representing them. The list of such high-frequency\nwords to be removed is called a stop list . The intuition is that high-frequency terms stop list\n(often function words like the,a,to) carry little semantic weight and may not help\nwith retrieval, and can also help shrink the inverted index \ufb01les we describe below.\nThe downside of using a stop list is that it makes it dif\ufb01cult to search for phrases\nthat contain words in the stop list. For example, common stop lists would reduce the\nphrase to be or not to be to the phrase not. In modern IR systems, the use of stop lists\nis much less common, partly due to improved ef\ufb01ciency and partly because much\nof their function is already handled by IDF weighting, which downweights function",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 455,
      "token_count": 427,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 303\n\n14.1 \u2022 I NFORMATION RETRIEVAL 295\nwords that occur in every document. Nonetheless, stop word removal is occasionally\nuseful in various NLP tasks so is worth keeping in mind.\n14.1.3 Inverted Index\nIn order to compute scores, we need to ef\ufb01ciently \ufb01nd documents that contain words\nin the query. (Any document that contains none of the query terms will have a score\nof 0 and can be ignored.) The basic search problem in IR is thus to \ufb01nd all documents\nd2Cthat contain a term q2Q.\nThe data structure for this task is the inverted index , which we use for mak- inverted index\ning this search ef\ufb01cient, and also conveniently storing useful information like the\ndocument frequency and the count of each term in each document.\nAn inverted index, given a query term, gives a list of documents that contain the\nterm. It consists of two parts, a dictionary and the postings . The dictionary is a list postings\nof terms (designed to be ef\ufb01ciently accessed), each pointing to a postings list for the\nterm. A postings list is the list of document IDs associated with each term, which\ncan also contain information like the term frequency or even the exact positions of\nterms in the document. The dictionary can also store the document frequency for\neach term. For example, a simple inverted index for our 4 sample documents above,\nwith each word containing its document frequency in fg, and a pointer to a postings\nlist that contains document IDs and term counts in [], might look like the following:\nhowf1g!3 [1]\nisf1g!3 [1]\nlovef2g!1 [1]!3 [1]\nnursef2g!1 [1]!4 [1]\nsorryf1g!2 [1]\nsweetf3g!1 [2]!2 [1]!3 [1]\nGiven a list of terms in query, we can very ef\ufb01ciently get lists of all candidate\ndocuments, together with the information necessary to compute the tf-idf scores we\nneed.\nThere are alternatives to the inverted index. For the question-answering domain\nof \ufb01nding Wikipedia pages to match a user query, Chen et al. (2017a) show that\nindexing based on bigrams works better than unigrams, and use ef\ufb01cient hashing\nalgorithms rather than the inverted index to make the search ef\ufb01cient.\n14.1.4 Evaluation of Information-Retrieval Systems\nWe measure the performance of ranked retrieval systems using the same precision\nandrecall metrics we have been using. We make the assumption that each docu-\nment returned by the IR system is either relevant to our purposes or not relevant .\nPrecision is the fraction of the returned documents that are relevant, and recall is the\nfraction of all relevant documents that are returned. More formally, let\u2019s assume a\nsystem returns Tranked documents in response to an information request, a subset\nRof these are relevant, a disjoint subset, N, are the remaining irrelevant documents,\nandUdocuments in the collection as a whole are relevant to this request. Precision\nand recall are then de\ufb01ned as:\nPrecision =jRj\njTjRecall =jRj\njUj(14.13)\nUnfortunately, these metrics don\u2019t adequately measure the performance of a system\nthatranks the documents it returns. If we are comparing the performance of two",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 456,
      "token_count": 743,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 304\n\n296 CHAPTER 14 \u2022 Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG\nranked retrieval systems, we need a metric that prefers the one that ranks the relevant\ndocuments higher. We need to adapt precision and recall to capture how well a\nsystem does at putting relevant documents higher in the ranking.\nRank Judgment Precision Rank Recall Rank\n1 R 1.0 .11\n2 N .50 .11\n3 R .66 .22\n4 N .50 .22\n5 R .60 .33\n6 R .66 .44\n7 N .57 .44\n8 R .63 .55\n9 N .55 .55\n10 N .50 .55\n11 R .55 .66\n12 N .50 .66\n13 N .46 .66\n14 N .43 .66\n15 R .47 .77\n16 N .44 .77\n17 N .44 .77\n18 R .44 .88\n19 N .42 .88\n20 N .40 .88\n21 N .38 .88\n22 N .36 .88\n23 N .35 .88\n24 N .33 .88\n25 R .36 1.0\nFigure 14.3 Rank-speci\ufb01c precision and recall values calculated as we proceed down\nthrough a set of ranked documents (assuming the collection has 9 relevant documents).\nLet\u2019s turn to an example. Assume the table in Fig. 14.3 gives rank-speci\ufb01c pre-\ncision and recall values calculated as we proceed down through a set of ranked doc-\numents for a particular query; the precisions are the fraction of relevant documents\nseen at a given rank, and recalls the fraction of relevant documents found at the same\nrank. The recall measures in this example are based on this query having 9 relevant\ndocuments in the collection as a whole.\nNote that recall is non-decreasing; when a relevant document is encountered,\nrecall increases, and when a non-relevant document is found it remains unchanged.\nPrecision, on the other hand, jumps up and down, increasing when relevant doc-\numents are found, and decreasing otherwise. The most common way to visualize\nprecision and recall is to plot precision against recall in a precision-recall curve ,precision-recall\ncurve\nlike the one shown in Fig. 14.4 for the data in table 14.3.\nFig. 14.4 shows the values for a single query. But we\u2019ll need to combine values\nfor all the queries, and in a way that lets us compare one system to another. One way\nof doing this is to plot averaged precision values at 11 \ufb01xed levels of recall (0 to 100,\nin steps of 10). Since we\u2019re not likely to have datapoints at these exact levels, we\nuseinterpolated precision values for the 11 recall values from the data points we dointerpolated\nprecision\nhave. We can accomplish this by choosing the maximum precision value achieved\nat any level of recall at or above the one we\u2019re calculating. In other words,\nIntPrecision (r) =max\ni>=rPrecision (i) (14.14)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 457,
      "token_count": 668,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 305\n\n14.1 \u2022 I NFORMATION RETRIEVAL 297\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall0.00.20.40.60.81.0Precision\nFigure 14.4 The precision recall curve for the data in table 14.3.\nThis interpolation scheme not only lets us average performance over a set of queries,\nbut also helps smooth over the irregular precision values in the original data. It is\ndesigned to give systems the bene\ufb01t of the doubt by assigning the maximum preci-\nsion value achieved at higher levels of recall from the one being measured. Fig. 14.5\nand Fig. 14.6 show the resulting interpolated data points from our example.\nInterpolated Precision Recall\n1.0 0.0\n1.0 .10\n.66 .20\n.66 .30\n.66 .40\n.63 .50\n.55 .60\n.47 .70\n.44 .80\n.36 .90\n.36 1.0\nFigure 14.5 Interpolated data points from Fig. 14.3.\nGiven curves such as that in Fig. 14.6 we can compare two systems or approaches\nby comparing their curves. Clearly, curves that are higher in precision across all\nrecall values are preferred. However, these curves can also provide insight into the\noverall behavior of a system. Systems that are higher in precision toward the left\nmay favor precision over recall, while systems that are more geared towards recall\nwill be higher at higher levels of recall (to the right).\nA second way to evaluate ranked retrieval is mean average precision (MAP),mean average\nprecision\nwhich provides a single metric that can be used to compare competing systems or\napproaches. In this approach, we again descend through the ranked list of items,\nbut now we note the precision only at those points where a relevant item has been\nencountered (for example at ranks 1, 3, 5, 6 but not 2 or 4 in Fig. 14.3). For a single\nquery, we average these individual precision measurements over the return set (up\nto some \ufb01xed cutoff). More formally, if we assume that Rris the set of relevant",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 458,
      "token_count": 492,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 306\n\n298 CHAPTER 14 \u2022 Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG\nInterpolated Precision Recall Curve\n00.10.20.30.40.50.60.70.80.91\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nRecallPrecision\nFigure 14.6 An 11 point interpolated precision-recall curve. Precision at each of the 11\nstandard recall levels is interpolated for each query from the maximum at any higher level of\nrecall. The original measured precision recall points are also shown.\ndocuments at or above r, then the average precision (AP) for a single query is\nAP=1\njRrjX\nd2RrPrecision r(d) (14.15)\nwhere Precision r(d)is the precision measured at the rank at which document dwas\nfound. For an ensemble of queries Q, we then average over these averages, to get\nour \ufb01nal MAP measure:\nMAP =1\njQjX\nq2QAP(q) (14.16)\nThe MAP for the single query (hence = AP) in Fig. 14.3 is 0.6.\n14.2 Information Retrieval with Dense Vectors\nThe classic tf-idf or BM25 algorithms for IR have long been known to have a con-\nceptual \ufb02aw: they work only if there is exact overlap of words between the query\nand document. In other words, the user posing a query (or asking a question) needs\nto guess exactly what words the writer of the answer might have used, an issue called\nthevocabulary mismatch problem (Furnas et al., 1987).\nThe solution to this problem is to use an approach that can handle synonymy:\ninstead of (sparse) word-count vectors, using (dense) embeddings. This idea was\n\ufb01rst proposed for retrieval in the last century under the name of Latent Semantic\nIndexing approach (Deerwester et al., 1990), but is implemented in modern times\nvia encoders like BERT.\nThe most powerful approach is to present both the query and the document to a\nsingle encoder, allowing the transformer self-attention to see all the tokens of both",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 459,
      "token_count": 501,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 307\n\n14.2 \u2022 I NFORMATION RETRIEVAL WITH DENSE VECTORS 299\nthe query and the document, and thus building a representation that is sensitive to\nthe meanings of both query and document. Then a linear layer can be put on top of\nthe [CLS] token to predict a similarity score for the query/document tuple:\nz=BERT (q;[SEP] ;d)[CLS]\nscore(q;d) =softmax (U(z)) (14.17)\nThis architecture is shown in Fig. 14.7a. Usually the retrieval step is not done on\nan entire document. Instead documents are broken up into smaller passages, such\nas non-overlapping \ufb01xed-length chunks of say 100 tokens, and the retriever encodes\nand retrieves these passages rather than entire documents. The query and document\nhave to be made to \ufb01t in the BERT 512-token window, for example by truncating\nthe query to 64 tokens and truncating the document if necessary so that it, the query,\n[CLS], and [SEP] \ufb01t in 512 tokens. The BERT system together with the linear layer\nUcan then be \ufb01ne-tuned for the relevance task by gathering a tuning dataset of\nrelevant and non-relevant passages.\nQueryDocument\u2026\u2026\u2026\u2026\u2026\u2026[sep]s(q,d)zCLSU\nQueryzCLS_QzCLS_D\nDocument\u2026\u2026\u2026\u2026\u2026\u2026\u2022s(q,d)\n(a) (b)\nFigure 14.7 Two ways to do dense retrieval, illustrated by using lines between layers to schematically rep-\nresent self-attention: (a) Use a single encoder to jointly encode query and document and \ufb01netune to produce a\nrelevance score with a linear layer over the CLS token. This is too compute-expensive to use except in rescoring\n(b) Use separate encoders for query and document, and use the dot product between CLS token outputs for the\nquery and document as the score. This is less compute-expensive, but not as accurate.\nThe problem with the full BERT architecture in Fig. 14.7a is the expense in\ncomputation and time. With this architecture, every time we get a query, we have to\npass every single single document in our entire collection through a BERT encoder\njointly with the new query! This enormous use of resources is impractical for real\ncases.\nAt the other end of the computational spectrum is a much more ef\ufb01cient archi-\ntecture, the bi-encoder . In this architecture we can encode the documents in the\ncollection only one time by using two separate encoder models, one to encode the\nquery and one to encode the document. We encode each document, and store all\nthe encoded document vectors in advance. When a query comes in, we encode just\nthis query and then use the dot product between the query vector and the precom-\nputed document vectors as the score for each candidate document (Fig. 14.7b). For\nexample, if we used BERT, we would have two encoders BERT Qand BERT Dand",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 460,
      "token_count": 652,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 308",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 461,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "300 CHAPTER 14 \u2022 Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG\nwe could represent the query and document as the [CLS] token of the respective\nencoders (Karpukhin et al., 2020):\nzq=BERT Q(q)[CLS]\nzd=BERT D(d)[CLS]\nscore(q;d) =zq\u0001zd (14.18)\nThe bi-encoder is much cheaper than a full query/document encoder, but is also\nless accurate, since its relevance decision can\u2019t take full advantage of all the possi-\nble meaning interactions between all the tokens in the query and the tokens in the\ndocument.\nThere are numerous approaches that lie in between the full encoder and the bi-\nencoder. One intermediate alternative is to use cheaper methods (like BM25) as the\n\ufb01rst pass relevance ranking for each document, take the top N ranked documents,\nand use expensive methods like the full BERT scoring to rerank only the top N\ndocuments rather than the whole set.\nAnother intermediate approach is the ColBERT approach of Khattab and Za- ColBERT\nharia (2020) and Khattab et al. (2021), shown in Fig. 14.8. This method separately\nencodes the query and document, but rather than encoding the entire query or doc-\nument into one vector, it separately encodes each of them into contextual represen-\ntations for each token. These BERT representations of each document word can be\npre-stored for ef\ufb01ciency. The relevance score between a query qand a document dis\na sum of maximum similarity (MaxSim) operators between tokens in qand tokens\nind. Essentially, for each token in q, ColBERT \ufb01nds the most contextually simi-\nlar token in d, and then sums up these similarities. A relevant document will have\ntokens that are contextually very similar to the query.\nMore formally, a question qis tokenized as [q1;:::; qn], prepended with a [CLS]\nand a special [Q]token, truncated to N=32 tokens (or padded with [MASK] tokens if\nit is shorter), and passed through BERT to get output vectors q= [q1;:::;qN]. The\npassage dwith tokens [d1;:::; dm], is processed similarly, including a [CLS] and\nspecial[D]token. A linear layer is applied on top of dandqto control the output\ndimension, so as to keep the vectors small for storage ef\ufb01ciency, and vectors are\nrescaled to unit length, producing the \ufb01nal vector sequences Eq(length N) and Ed\n(length m). The ColBERT scoring mechanism is:\nscore(q;d) =NX\ni=1mmax\nj=1Eqi\u0001Edj(14.19)\nWhile the interaction mechanism has no tunable parameters, the ColBERT ar-\nchitecture still needs to be trained end-to-end to \ufb01ne-tune the BERT encoders and\ntrain the linear layers (and the special [Q]and[D]embeddings) from scratch. It is\ntrained on tripleshq;d+;d\u0000iof query q, positive document d+and negative docu-\nment d\u0000to produce a score for each document using Eq. 14.19, optimizing model\nparameters using a cross-entropy loss.\nAll the supervised algorithms (like ColBERT or the full-interaction version of\nthe BERT algorithm applied for reranking) need training data in the form of queries\ntogether with relevant and irrelevant passages or documents (positive and negative",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 462,
      "token_count": 765,
      "chapter_title": ""
    }
  },
  {
    "content": "passage dwith tokens [d1;:::; dm], is processed similarly, including a [CLS] and\nspecial[D]token. A linear layer is applied on top of dandqto control the output\ndimension, so as to keep the vectors small for storage ef\ufb01ciency, and vectors are\nrescaled to unit length, producing the \ufb01nal vector sequences Eq(length N) and Ed\n(length m). The ColBERT scoring mechanism is:\nscore(q;d) =NX\ni=1mmax\nj=1Eqi\u0001Edj(14.19)\nWhile the interaction mechanism has no tunable parameters, the ColBERT ar-\nchitecture still needs to be trained end-to-end to \ufb01ne-tune the BERT encoders and\ntrain the linear layers (and the special [Q]and[D]embeddings) from scratch. It is\ntrained on tripleshq;d+;d\u0000iof query q, positive document d+and negative docu-\nment d\u0000to produce a score for each document using Eq. 14.19, optimizing model\nparameters using a cross-entropy loss.\nAll the supervised algorithms (like ColBERT or the full-interaction version of\nthe BERT algorithm applied for reranking) need training data in the form of queries\ntogether with relevant and irrelevant passages or documents (positive and negative\nexamples). There are various semi-supervised ways to get labels; some datasets (like\nMS MARCO Ranking, Section 14.3.2) contain gold positive examples. Negative\nexamples can be sampled randomly from the top-1000 results from some existing\nIR system. If datasets don\u2019t have labeled positive examples, iterative methods like",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 463,
      "token_count": 354,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 309\n\n14.3 \u2022 A NSWERING QUESTIONS WITH RAG 301\nQueryDocument\u2026\u2026\u2026\u2026\u2026\u2026s(q,d)MaxSimMaxSimMaxSim\u2211\nnormnormnormnormnormnorm\nFigure 14.8 A sketch of the ColBERT algorithm at inference time. The query and docu-\nment are \ufb01rst passed through separate BERT encoders. Similarity between query and doc-\nument is computed by summing a soft alignment between the contextual representations of\ntokens in the query and the document. Training is end-to-end. (Various details aren\u2019t de-\npicted; for example the query is prepended by a [CLS] and[Q:] tokens, and the document\nby[CLS] and[D:] tokens). Figure adapted from Khattab and Zaharia (2020).\nrelevance-guided supervision can be used (Khattab et al., 2021) which rely on the\nfact that many datasets contain short answer strings. In this method, an existing IR\nsystem is used to harvest examples that do contain short answer strings (the top few\nare taken as positives) or don\u2019t contain short answer strings (the top few are taken as\nnegatives), these are used to train a new retriever, and then the process is iterated.\nEf\ufb01ciency is an important issue, since every possible document must be ranked\nfor its similarity to the query. For sparse word-count vectors, the inverted index\nallows this very ef\ufb01ciently. For dense vector algorithms \ufb01nding the set of dense\ndocument vectors that have the highest dot product with a dense query vector is\nan instance of the problem of nearest neighbor search . Modern systems there-\nfore make use of approximate nearest neighbor vector search algorithms like Faiss Faiss\n(Johnson et al., 2017).\n14.3 Answering Questions with RAG\nThe dominant paradigm for question answering is to answer a user\u2019s question by \ufb01rst\n\ufb01nding supportive text segments from the web or another other large collection of\ndocuments, and then generating an answer based on the documents. The method of\ngenerating based on retrieved documents is called retrieval-augmented generation\norRAG , and the two components are sometimes called the retriever and the reader\n(Chen et al., 2017a). Fig. 14.9 sketches out this standard QA model.\nIn the \ufb01rst stage of the 2-stage retrieve and read model in Fig. 14.9 we retrieve\nrelevant passages from a text collection, for example using the dense retrievers of the\nprevious section. In the second reader stage, we generate the answer via retrieval-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 464,
      "token_count": 554,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 310\n\n302 CHAPTER 14 \u2022 Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG\nQ: When wasthe premiere ofThe Magic Flute?RelevantDocsA:  1791RetrieverIndexed Docs\nquerydocsLLMpromptReader/Generator\nFigure 14.9 Retrieval-based question answering has two stages: retrieval , which returns relevant documents\nfrom the collection, and reading , in which an LLM generates answers given the documents as a prompt.\naugmented generation . In this method, we take a large pretrained language model,\ngive it the set of retrieved passages and other text as its prompt, and autoregressively\ngenerate a new answer token by token.\n14.3.1 Retrieval-Augmented Generation\nThe standard reader algorithm is to generate from a large language model, condi-\ntioned on the retrieved passages. This method is known as retrieval-augmented\ngeneration , orRAG .retrieval-\naugmented\ngeneration\nRAG Recall that in simple conditional generation, we can cast the task of question\nanswering as word prediction by giving a language model a question and a token\nlikeA:suggesting that an answer should come next:\nQ: Who wrote the book ``The Origin of Species\"? A:\nThen we generate autoregressively conditioned on this text.\nMore formally, recall that simple autoregressive language modeling computes\nthe probability of a string from the previous tokens:\np(x1;:::; xn) =nY\ni=1p(xijx<i)\nAnd simple conditional generation for question answering adds a prompt like Q:,\nfollowed by a query q, andA:, all concatenated:\np(x1;:::; xn) =nY\ni=1p([Q:] ;q;[A:] ;x<i)\nThe advantage of using a large language model is the enormous amount of\nknowledge encoded in its parameters from the text it was pretrained on. But as\nwe mentioned at the start of the chapter, while this kind of simple prompted gener-\nation can work \ufb01ne for many simple factoid questions, it is not a general solution\nfor QA, because it leads to hallucination, is unable to show users textual evidence to\nsupport the answer, and is unable to answer questions from proprietary data.\nThe idea of retrieval-augmented generation is to address these problems by con-\nditioning on the retrieved passages as part of the pre\ufb01x, perhaps with some prompt\ntext like \u201cBased on these texts, answer this question:\u201d. Let\u2019s suppose we have a\nquery q, and call the set of retrieved passages based on it R( q). For example, we\ncould have a prompt like:",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 465,
      "token_count": 565,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 311\n\n14.3 \u2022 A NSWERING QUESTIONS WITH RAG 303\nSchematic of a RAG Prompt\nretrieved passage 1\nretrieved passage 2\n...\nretrieved passage n\nBased on these texts, answer this question: Q: Who wrote\nthe book ``The Origin of Species\"? A:\nOr more formally,\np(x1;:::; xn) =nY\ni=1p(xijR(q) ; prompt ; [Q:] ;q;[A:] ;x<i)\nAs with the span-based extraction reader, successfully applying the retrieval-\naugmented generation algorithm for QA requires a successful retriever, and often\na two-stage retrieval algorithm is used in which the retrieval is reranked. Some\ncomplex questions may require multi-hop architectures, in which a query is used to multi-hop\nretrieve documents, which are then appended to the original query for a second stage\nof retrieval. Details of prompt engineering also have to be worked out, like deciding\nwhether to demarcate passages, for example with [SEP] tokens, and so on. Combi-\nnations of private data and public data involving an externally hosted large language\nmodel may lead to privacy concerns that need to be worked out (Arora et al., 2023).\nMuch research in this area also focuses on ways to more tightly integrate the retrieval\nand reader stages.\n14.3.2 Question Answering Datasets\nThere are scores of question answering datasets, used both for instruction tuning and\nfor evaluation of the question answering abilities of language models.\nWe can distinguish the datasets along many dimensions, summarized nicely in\nRogers et al. (2023). One is the original purpose of the questions in the data, whether\nthey were natural information-seeking questions, or whether they were questions\ndesigned for probing : evaluating or testing systems or humans.\nOn the natural side there are datasets like Natural Questions (KwiatkowskiNatural\nQuestions\net al., 2019), a set of anonymized English queries to the Google search engine and\ntheir answers. The answers are created by annotators based on Wikipedia infor-\nmation, and include a paragraph-length long answer and a short span answer. For\nexample the question \u201cWhen are hops added to the brewing process?\u201d has the short\nanswer the boiling process and a long answer which is an entire paragraph from the\nWikipedia page on Brewing .\nA similar natural question set is the MS MARCO (Microsoft Machine Reading MS MARCO\nComprehension) collection of datasets, including 1 million real anonymized English\nquestions from Microsoft Bing query logs together with a human generated answer\nand 9 million passages (Bajaj et al., 2016), that can be used both to test retrieval\nranking and question answering.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 466,
      "token_count": 580,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 312\n\n304 CHAPTER 14 \u2022 Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG\nAlthough many datasets focus on English, natural information-seeking ques-\ntion datasets exist in other languages. The DuReader dataset is a Chinese QA\nresource based on search engine queries and community QA (He et al., 2018).\nTyDi QA dataset contains 204K question-answer pairs from 11 typologically di- TyDi QA\nverse languages, including Arabic, Bengali, Kiswahili, Russian, and Thai (Clark\net al., 2020a). In the T YDIQA task, a system is given a question and the passages\nfrom a Wikipedia article and must (a) select the passage containing the answer (or\nNULL if no passage contains the answer), and (b) mark the minimal answer span (or\nNULL).\nOn the probing side are datasets like MMLU (Massive Multitask Language Un- MMLU\nderstanding), a commonly-used dataset of 15908 knowledge and reasoning ques-\ntions in 57 areas including medicine, mathematics, computer science, law, and oth-\ners. MMLU questions are sourced from various exams for humans, such as the US\nGraduate Record Exam, Medical Licensing Examination, and Advanced Placement\nexams. So the questions don\u2019t represent people\u2019s information needs, but rather are\ndesigned to test human knowledge for academic or licensing purposes. Fig. 14.10\nshows some examples, with the correct answers in bold.\nSome of the question datasets described above augment each question with pas-\nsage(s) from which the answer can be extracted. These datasets were mainly created\nfor an earlier QA task called reading comprehension in which a model is givenreading\ncomprehension\na question and a document and is required to extract the answer from the given\ndocument. We sometimes call the task of question answering given one or more\ndocuments (for example via RAG), the open book QA task, while the task of an- open book\nswering directly from the LM with no retrieval component at all is the closed book closed book\nQA task.5Thus datasets like Natural Questions can be treated as open book if the\nsolver uses each question\u2019s attached document, or closed book if the documents are\nnot used, while datasets like MMLU are solely closed book.\nAnother dimension of variation is the format of the answer: multiple-choice\nversus freeform. And of course there are variations in prompting, like whether the\nmodel is just the question (zero-shot) or also given demonstrations of answers to\nsimilar questions (few-shot). MMLU offers both zero-shot and few-shot prompt\noptions.\n14.4 Evaluating Question Answering\nThree techniques are commonly employed to evaluate question-answering systems,\nwith the choice depending on the type of question and QA situation. For multiple\nchoice questions like in MMLU, we report exact match:\nExact match : The % of predicted answers that match the gold answer\nexactly.\nFor questions with free text answers, like Natural Questions, we commonly evalu-\nated with token F1score to roughly measure the partial string overlap between the\nanswer and the reference answer:\nF1score : The average token overlap between predicted and gold an-\nswers. Treat the prediction and gold as a bag of tokens, and compute F 1\nfor each question, then return the average F 1over all questions.\n5This repurposes the word for types of exams in which students are allowed to \u2018open their books\u2019 or\nnot.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 467,
      "token_count": 742,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 313\n\n14.4 \u2022 E VALUATING QUESTION ANSWERING 305\nMMLU examples\nCollege Computer Science\nAny set of Boolean operators that is suf\ufb01cient to represent all Boolean ex-\npressions is said to be complete. Which of the following is NOT complete?\n(A) AND, NOT\n(B) NOT, OR\n(C) AND, OR\n(D) NAND\nCollege Physics\nThe primary source of the Sun\u2019s energy is a series of thermonuclear\nreactions in which the energy produced is c2times the mass difference\nbetween\n(A) two hydrogen atoms and one helium atom\n(B)four hydrogen atoms and one helium atom\n(C) six hydrogen atoms and two helium atoms\n(D) three helium atoms and one carbon atom\nInternational Law\nWhich of the following is a treaty-based human rights mechanism?\n(A)The UN Human Rights Committee\n(B) The UN Human Rights Council\n(C) The UN Universal Periodic Review\n(D) The UN special mandates\nPrehistory\nUnlike most other early civilizations, Minoan culture shows little evidence\nof\n(A) trade.\n(B) warfare.\n(C) the development of a common religion.\n(D)conspicuous consumption by elites.\nFigure 14.10 Example problems from MMLU\nFinally, in some situations QA systems give multiple ranked answers. In such cases\nwe evaluated using mean reciprocal rank , orMRR (V oorhees, 1999). MRR ismean\nreciprocal rank\nMRR designed for systems that return a short ranked list of answers or passages for each\ntest set question, which we can compare against the (human-labeled) correct answer.\nFirst, each test set question is scored with the reciprocal of the rank of the \ufb01rst\ncorrect answer. For example if the system returned \ufb01ve answers to a question but\nthe \ufb01rst three are wrong (so the highest-ranked correct answer is ranked fourth), the\nreciprocal rank for that question is1\n4. The score for questions that return no correct\nanswer is 0. The MRR of a system is the average of the scores for each question in\nthe test set. In some versions of MRR, questions with a score of zero are ignored\nin this calculation. More formally, for a system returning ranked answers to each\nquestion in a test set Q, (or in the alternate version, let Qbe the subset of test set",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 468,
      "token_count": 505,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 314\n\n306 CHAPTER 14 \u2022 Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG\nquestions that have non-zero scores). MRR is then de\ufb01ned as\nMRR =1\njQjjQjX\ni=11\nrank i(14.20)\n14.5 Summary\nThis chapter introduced the tasks of question answering andinformation retrieval .\n\u2022Question answering (QA) is the task of answering a user\u2019s questions.\n\u2022 We focus in this chapter on the task of retrieval-based question answering,\nin which the user\u2019s questions are intended to be answered by the material in\nsome set of documents (which might be the web).\n\u2022Information Retrieval (IR) is the task of returning documents to a user based\non their information need as expressed in a query . In ranked retrieval, the\ndocuments are returned in ranked order.\n\u2022 The match between a query and a document can be done by \ufb01rst representing\neach of them with a sparse vector that represents the frequencies of words,\nweighted by tf-idf orBM25 . Then the similarity can be measured by cosine.\n\u2022 Documents or queries can instead be represented by dense vectors, by encod-\ning the question and document with an encoder-only model like BERT, and in\nthat case computing similarity in embedding space.\n\u2022 The inverted index is a storage mechanism that makes it very ef\ufb01cient to \ufb01nd\ndocuments that have a particular word.\n\u2022 Ranked retrieval is generally evaluated by mean average precision orinter-\npolated precision .\n\u2022 Question answering systems generally use the retriever /reader architecture.\nIn the retriever stage, an IR system is given a query and returns a set of\ndocuments.\n\u2022 The reader stage is implemented by retrieval-augmented generation , in\nwhich a large language model is prompted with the query and a set of doc-\numents and then conditionally generates a novel answer.\n\u2022 QA can be evaluated by exact match with a known answer if only a single\nanswer is given, with token F 1score for free text answers, or with mean re-\nciprocal rank if a ranked set of answers is given.\nBibliographical and Historical Notes\nQuestion answering was one of the earliest NLP tasks, and early versions of the text-\nbased and knowledge-based paradigms were developed by the very early 1960s. The\ntext-based algorithms generally relied on simple parsing of the question and of the\nsentences in the document, and then looking for matches. This approach was used\nvery early on (Phillips, 1960) but perhaps the most complete early system, and one\nthat strikingly pre\ufb01gures modern relation-based systems, was the Protosynthex sys-\ntem of Simmons et al. (1964). Given a question, Protosynthex \ufb01rst formed a query",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 469,
      "token_count": 588,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 315\n\nBIBLIOGRAPHICAL AND HISTORICAL NOTES 307\nfrom the content words in the question, and then retrieved candidate answer sen-\ntences in the document, ranked by their frequency-weighted term overlap with the\nquestion. The query and each retrieved sentence were then parsed with dependency\nparsers, and the sentence whose structure best matches the question structure se-\nlected. Thus the question What do worms eat? would match worms eat grass : both\nhave the subject worms as a dependent of eat, in the version of dependency grammar\nused at the time, while birds eat worms hasbirds as the subject:\nWhatdowormseat Wormseatgrass Birdseatworms\nThe alternative knowledge-based paradigm was implemented in the BASEBALL\nsystem (Green et al., 1961). This system answered questions about baseball games\nlike \u201cWhere did the Red Sox play on July 7\u201d by querying a structured database of\ngame information. The database was stored as a kind of attribute-value matrix with\nvalues for attributes of each game:\nMonth = July\nPlace = Boston\nDay = 7\nGame Serial No. = 96\n(Team = Red Sox, Score = 5)\n(Team = Yankees, Score = 3)\nEach question was constituency-parsed using the algorithm of Zellig Harris\u2019s\nTDAP project at the University of Pennsylvania, essentially a cascade of \ufb01nite-state\ntransducers (see the historical discussion in Joshi and Hopely 1999 and Karttunen\n1999). Then in a content analysis phase each word or phrase was associated with a\nprogram that computed parts of its meaning. Thus the phrase \u2018Where\u2019 had code to\nassign the semantics Place = ? , with the result that the question \u201cWhere did the\nRed Sox play on July 7\u201d was assigned the meaning\nPlace = ?\nTeam = Red Sox\nMonth = July\nDay = 7\nThe question is then matched against the database to return the answer. Simmons\n(1965) summarizes other early QA systems.\nAnother important progenitor of the knowledge-based paradigm for question-\nanswering is work that used predicate calculus as the meaning representation lan-\nguage. The LUNAR system (Woods et al. 1972, Woods 1978) was designed to be LUNAR\na natural language interface to a database of chemical facts about lunar geology. It\ncould answer questions like Do any samples have greater than 13 percent aluminum\nby parsing them into a logical form\n(TEST (FOR SOME X16 / (SEQ SAMPLES) : T ; (CONTAIN\u2019 X16\n(NPR* X17 / (QUOTE AL203)) (GREATERTHAN 13 PCT))))\nBy a couple decades later, drawing on new machine learning approaches in NLP,\nZelle and Mooney (1996) proposed to treat knowledge-based QA as a semantic pars-\ning task, by creating the Prolog-based GEOQUERY dataset of questions about US\ngeography. This model was extended by Zettlemoyer and Collins (2005) and 2007.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 470,
      "token_count": 640,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 316\n\n308 CHAPTER 14 \u2022 Q UESTION ANSWERING , INFORMATION RETRIEVAL ,AND RAG\nBy a decade later, neural models were applied to semantic parsing (Dong and Lap-\nata 2016, Jia and Liang 2016), and then to knowledge-based question answering by\nmapping text to SQL (Iyer et al., 2017).\nMeanwhile, the information-retrieval paradigm for question answering was in-\n\ufb02uenced by the rise of the web in the 1990s. The U.S. government-sponsored TREC\n(Text REtrieval Conference) evaluations, run annually since 1992, provide a testbed\nfor evaluating information-retrieval tasks and techniques (V oorhees and Harman,\n2005). TREC added an in\ufb02uential QA track in 1999, which led to a wide variety of\nfactoid and non-factoid systems competing in annual evaluations.\nAt that same time, Hirschman et al. (1999) introduced the idea of using chil-\ndren\u2019s reading comprehension tests to evaluate machine text comprehension algo-\nrithms. They acquired a corpus of 120 passages with 5 questions each designed for\n3rd-6th grade children, built an answer extraction system, and measured how well\nthe answers given by their system corresponded to the answer key from the test\u2019s\npublisher. Their algorithm focused on word overlap as a feature; later algorithms\nadded named entity features and more complex similarity between the question and\nthe answer span (Riloff and Thelen 2000, Ng et al. 2000).\nThe DeepQA component of the Watson Jeopardy! system was a large and so-\nphisticated feature-based system developed just before neural systems became com-\nmon. It is described in a series of papers in volume 56 of the IBM Journal of Re-\nsearch and Development, e.g., Ferrucci (2012).\nEarly neural reading comprehension systems drew on the insight common to\nearly systems that answer \ufb01nding should focus on question-passage similarity. Many\nof the architectural outlines of these neural systems were laid out in Hermann et al.\n(2015a), Chen et al. (2017a), and Seo et al. (2017). These systems focused on\ndatasets like Rajpurkar et al. (2016) and Rajpurkar et al. (2018) and their succes-\nsors, usually using separate IR algorithms as input to neural reading comprehension\nsystems. The paradigm of using dense retrieval with a span-based reader, often with\na single end-to-end architecture, is exempli\ufb01ed by systems like Lee et al. (2019) or\nKarpukhin et al. (2020). An important research area with dense retrieval for open-\ndomain QA is training data: using self-supervised methods to avoid having to label\npositive and negative passages (Sachan et al., 2023).\nEarly work on large language models showed that they stored suf\ufb01cient knowl-\nedge in the pretraining process to answer questions (Petroni et al., 2019; Raffel et al.,\n2020; Radford et al., 2019; Roberts et al., 2020), at \ufb01rst not competitively with\nspecial-purpose question answerers, but then surpassing them. Retrieval-augmented\ngeneration algorithms were \ufb01rst introduced as a way to improve language modeling\n(Khandelwal et al., 2019), but were quickly applied to question answering (Izacard\net al., 2022; Ram et al., 2023; Shi et al., 2023).\nExercises",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 471,
      "token_count": 778,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 317\n\nCHAPTER\n15Chatbots & Dialogue Systems\nLes lois de la conversation sont en g \u00b4en\u00b4eral de ne s\u2019y appesantir sur aucun ob-\njet, mais de passer l \u00b4eg`erement, sans effort et sans affectation, d\u2019un sujet `a un\nautre ; de savoir y parler de choses frivoles comme de choses s \u00b4erieuses\n[The rules of conversation are, in general, not to dwell on any one subject,\nbut to pass lightly from one to another without effort and without affectation;\nto know how to speak about trivial topics as well as serious ones;]\nThe 18th C. Encyclopedia of Diderot, start of the entry on conversation\nThe literature of the fantastic abounds in inanimate objects magically endowed with\nthe gift of speech. From Ovid\u2019s statue of Pygmalion to Mary Shelley\u2019s story about\nFrankenstein, we continually reinvent stories about creat-\ning something and then having a chat with it. Legend has\nit that after \ufb01nishing his sculpture Moses , Michelangelo\nthought it so lifelike that he tapped it on the knee and\ncommanded it to speak. Perhaps this shouldn\u2019t be sur-\nprising. Language is the mark of humanity and sentience,\nandconversation ordialogue is the most fundamental conversation\ndialogue arena of language. It is the \ufb01rst kind of language we\nlearn as children, and the kind we engage in constantly,\nwhether we are ordering lunch, buying train tickets, or\ntalking with our families, friends, or coworkers.\nThis chapter introduces the fundamental algorithms of programs that use con-\nversation to interact with users. We often distinguish between two kinds of archi-\ntectures. Task-oriented dialogue systems converse with users to accomplish \ufb01xed dialogue system\ntasks like controlling appliances or \ufb01nding restaurants, relying on a data structure\ncalled the frame , which represents the knowledge a system needs to acquire from frame\nthe user (like the time to set an alarm clock). Chatbots , by contrast, are designed chatbot\nto mimic the longer and more unstructured conversations or \u2018chats\u2019 characteristic of\nhuman-human interaction. Modern systems incorporate aspects of both; industrial\nchatbots like ChatGPT can carry on longer unstructured conversations; industrial\ndigital assistants like Siri or Alexa are generally frame-based dialogue systems.\nThe fact that chatbots and dialogue systems are designed for human-computer\ninteraction has strong implications for their design and use. Many of these impli-\ncations already became clear in one of the earliest chatbots, ELIZA (Weizenbaum,\n1966). ELIZA was designed to simulate a Rogerian psychologist, based on a branch\nof clinical psychology whose methods involve drawing the patient out by re\ufb02ecting\npatient\u2019s statements back at them. Rogerian interactions are the rare type of conver-\nsation in which, as Weizenbaum points out, one can \u201cassume the pose of knowing\nalmost nothing of the real world\u201d. If a patient says \u201cI went for a long boat ride\u201d and\nthe psychiatrist says \u201cTell me about boats\u201d, you don\u2019t assume she didn\u2019t know what",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 472,
      "token_count": 663,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 318\n\n310 CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\nMen are all alike.\nIN WHAT WAY\nThey\u2019re always bugging us about something or other.\nCAN YOU THINK OF A SPECIFIC EXAMPLE\nWell my boyfriend made me come here.\nYOUR BOYFRIEND MADE YOU COME HERE\nHe says I\u2019m depressed much of the time.\nI AM SORRY TO HEAR YOU ARE DEPRESSED\nFigure 15.1 Sample ELIZA dialogue from Weizenbaum (1966).\na boat is, but rather assume she had some conversational goal.1\nWeizenbaum made use of this property of Rogerian psychiatric conversations,\nalong with clever regular expressions, to allow ELIZA to interact in ways that seemed\ndeceptively human-like, as in the sample conversational fragment in Fig. 15.1.\nAs we foreshadowed in Chapter 2, ELIZA worked by simple rules roughly like:\n(.*) YOU (.*) ME -> WHAT MAKES YOU THINK I \\2 YOU\nto transform a user sentence like \u201c You hate me \u201d into a system response like\nWHAT MAKES YOU THINK I HATE YOU\nAmong Weizenbaum\u2019s clever tricks are the linking of each ELIZA pattern/rule\nto a keyword. Consider the following user sentence:\nI know everybody laughed at me\nBecause it has the word \u201cI\u201d, this sentence could match the following rule whose\nkeyword is I:\nI (.*) -> You say you \\1\nproducing:\nYOU SAY YOU KNOW EVERYBODY LAUGHED AT YOU\nWeizenbaum points out, however, that a more powerful response would rely on\nthe keyword \u201ceverybody\u201d, since someone using universals like everybody oralways\nis probably thinking about a speci\ufb01c person or situation. So the ELIZA algorithm\nprefers to respond using patterns associated more speci\ufb01c keywords like everybody :\nWHO IN PARTICULAR ARE YOU THINKING OF?\nIf no keyword matches, the algorithm chooses a non-committal response like\n\u201cPLEASE GO ON \u201d, \u201cTHAT'S VERY INTERESTING \u201d, or \u201cI SEE \u201d.\nELIZA illustrates a number of important issues with chatbots. First, people\nbecame deeply emotionally involved and conducted very personal conversations,\neven to the extent of asking Weizenbaum to leave the room while they were typ-\ning. Reeves and Nass (1996) show that people tend to assign human characteristics\nto computers and interact with them in ways that are typical of human-human in-\nteractions. They interpret an utterance in the way they would if it had spoken by a\nhuman, (even though they are aware they are talking to a computer). This means that\nchatbots can have signi\ufb01cant in\ufb02uences on people\u2019s cognitive and emotional state.\nA second related issue is privacy. When Weizenbaum suggested that he might\nwant to store the ELIZA conversations, people immediately pointed out that this\nwould violate people\u2019s privacy. Modern chatbots in the home are likely to overhear\n1This is due to the Gricean principle of relevance that we\u2019ll discuss in the next section..",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 473,
      "token_count": 660,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 319\n\n15.1 \u2022 P ROPERTIES OF HUMAN CONVERSATION 311\nprivate information, even if they aren\u2019t used for counseling as ELIZA was. Indeed,\nif a chatbot is human-like, users are more likely to disclose private information, and\nyet less likely to worry about the harm of this disclosure (Ischen et al., 2019).\nBoth of these issues (emotional engagement and privacy) mean we need to think\ncarefully about how we deploy chatbots and the people who are interacting with\nthem. Dialogue research that uses human participants often requires getting permis-\nsion from the Institutional Review Board (IRB) of your institution.\nIn the next section we introduce some basic properties of human conversation.\nWe then turn in the rest of the chapter to the two basic paradigms for conversational\ninteraction: frame-based dialogue systems and chatbots.\n15.1 Properties of Human Conversation\nConversation between humans is an intricate and complex joint activity. Before\nwe attempt to design a dialogue system to converse with humans, it is crucial to\nunderstand something about how humans converse with each other. Consider some\nof the phenomena that occur in the conversation between a human travel agent and\na human client excerpted in Fig. 15.2.\nC1: . . . I need to travel in May.\nA2: And, what day in May did you want to travel?\nC3: OK uh I need to be there for a meeting that\u2019s from the 12th to the 15th.\nA4: And you\u2019re \ufb02ying into what city?\nC5: Seattle.\nA6: And what time would you like to leave Pittsburgh?\nC7: Uh hmm I don\u2019t think there\u2019s many options for non-stop.\nA8: Right. There\u2019s three non-stops today.\nC9: What are they?\nA10: The \ufb01rst one departs PGH at 10:00am arrives Seattle at 12:05 their time.\nThe second \ufb02ight departs PGH at 5:55pm, arrives Seattle at 8pm. And the\nlast \ufb02ight departs PGH at 8:15pm arrives Seattle at 10:28pm.\nC11: OK I\u2019ll take the 5ish \ufb02ight on the night before on the 11th.\nA12: On the 11th? OK. Departing at 5:55pm arrives Seattle at 8pm, U.S. Air\n\ufb02ight 115.\nC13: OK.\nA14: And you said returning on May 15th?\nC15: Uh, yeah, at the end of the day.\nA16: OK. There\u2019s #two non-stops . . . #\nC17: #Act. . . actually #, what day of the week is the 15th?\nA18: It\u2019s a Friday.\nC19: Uh hmm. I would consider staying there an extra day til Sunday.\nA20: OK. . . OK. On Sunday I have . . .\nFigure 15.2 Part of a phone conversation between a human travel agent (A) and human\nclient (C). The passages framed by # in A 16and C 17indicate overlaps in speech.\nTurns\nA dialogue is a sequence of turns (C1, A2, C3, and so on), each a single contribution turn\nfrom one speaker to the dialogue (as if in a game: I take a turn, then you take a turn,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 474,
      "token_count": 741,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 320",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 475,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "312 CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\nthen me, and so on). There are 20 turns in Fig. 15.2. A turn can consist of a sentence\n(like C 1), although it might be as short as a single word (C 13) or as long as multiple\nsentences (A 10).\nTurn structure has important implications for spoken dialogue. A human has\nto know when to stop talking; the client interrupts (in A 16and C 17), so a system\nthat was performing this role must know to stop talking (and that the user might be\nmaking a correction). A system also has to know when to start talking. For example,\nmost of the time in conversation, speakers start their turns almost immediately after\nthe other speaker \ufb01nishes, without a long pause, because people are can usually\npredict when the other person is about to \ufb01nish talking. Spoken dialogue systems\nmust also detect whether a user is done speaking, so they can process the utterance\nand respond. This task\u2014called endpointing orendpoint detection \u2014 can be quite endpointing\nchallenging because of noise and because people often pause in the middle of turns.\nSpeech Acts\nA key insight into conversation\u2014due originally to the philosopher Wittgenstein\n(1953) but worked out more fully by Austin (1962)\u2014is that each utterance in a\ndialogue is a kind of action being performed by the speaker. These actions are com-\nmonly called speech acts ordialogue acts : here\u2019s one taxonomy consisting of 4 speech acts\nmajor classes (Bach and Harnish, 1979):\nConstatives: committing the speaker to something\u2019s being the case ( answering ,claiming ,\ncon\ufb01rming ,denying ,disagreeing ,stating )\nDirectives: attempts by the speaker to get the addressee to do something ( advising ,ask-\ning,forbidding ,inviting ,ordering ,requesting )\nCommissives: committing the speaker to some future course of action ( promising, planning,\nvowing, betting, opposing )\nAcknowledgments: express the speaker\u2019s attitude regarding the hearer with respect to some so-\ncial action ( apologizing ,greeting ,thanking ,accepting an acknowledgment )\nA user asking a person or a dialogue system to do something (\u2018Turn up the mu-\nsic\u2019) is issuing a D IRECTIVE . Asking a question that requires an answer is also\na way of issuing a D IRECTIVE : in a sense when the system says (A 2) \u201cwhat day\nin May did you want to travel?\u201d it\u2019s as if the system is (very politely) command-\ning the user to answer. By contrast, a user stating a constraint (like C 1\u2018I need to\ntravel in May\u2019) is issuing a C ONSTATIVE . A user thanking the system is issuing\nan A CKNOWLEDGMENT . The speech act expresses an important component of the\nintention of the speaker (or writer) in saying what they said.\nGrounding\nA dialogue is not just a series of independent speech acts, but rather a collective act\nperformed by the speaker and the hearer. Like all collective acts, it\u2019s important for\nthe participants to establish what they both agree on, called the common groundcommon\nground\n(Stalnaker, 1978). Speakers do this by grounding each other\u2019s utterances. Ground- grounding\ning means acknowledging that the hearer has understood the speaker (Clark, 1996).\n(People need grounding for non-linguistic actions as well; the reason an elevator but-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 476,
      "token_count": 771,
      "chapter_title": ""
    }
  },
  {
    "content": "A user asking a person or a dialogue system to do something (\u2018Turn up the mu-\nsic\u2019) is issuing a D IRECTIVE . Asking a question that requires an answer is also\na way of issuing a D IRECTIVE : in a sense when the system says (A 2) \u201cwhat day\nin May did you want to travel?\u201d it\u2019s as if the system is (very politely) command-\ning the user to answer. By contrast, a user stating a constraint (like C 1\u2018I need to\ntravel in May\u2019) is issuing a C ONSTATIVE . A user thanking the system is issuing\nan A CKNOWLEDGMENT . The speech act expresses an important component of the\nintention of the speaker (or writer) in saying what they said.\nGrounding\nA dialogue is not just a series of independent speech acts, but rather a collective act\nperformed by the speaker and the hearer. Like all collective acts, it\u2019s important for\nthe participants to establish what they both agree on, called the common groundcommon\nground\n(Stalnaker, 1978). Speakers do this by grounding each other\u2019s utterances. Ground- grounding\ning means acknowledging that the hearer has understood the speaker (Clark, 1996).\n(People need grounding for non-linguistic actions as well; the reason an elevator but-\nton lights up when it\u2019s pressed is to acknowledge that the elevator has indeed been\ncalled, essentially grounding your action of pushing the button (Norman, 1988).)\nHumans constantly ground each other\u2019s utterances. We can ground by explicitly\nsaying \u201cOK\u201d, as the agent does in A 8or A 10. Or we can ground by repeating what\nthe other person says; in utterance A 2the agent repeats \u201cin May\u201d, demonstrating her",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 477,
      "token_count": 384,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 321\n\n15.1 \u2022 P ROPERTIES OF HUMAN CONVERSATION 313\nunderstanding to the client. Or notice that when the client answers a question, the\nagent begins the next question with \u201cAnd\u201d. The \u201cAnd\u201d implies that the new question\nis \u2018in addition\u2019 to the old question, again indicating to the client that the agent has\nsuccessfully understood the answer to the last question.\nSubdialogues and Dialogue Structure\nConversations have structure. Consider, for example, the local structure between\nspeech acts discussed in the \ufb01eld of conversation analysis (Sacks et al., 1974).conversation\nanalysis\nQUESTIONS set up an expectation for an ANSWER . PROPOSALS are followed by\nACCEPTANCE (orREJECTION ). C OMPLIMENTS (\u201cNice jacket!\u201d) often give rise to\nDOWNPLAYERS (\u201cOh, this old thing?\u201d). These pairs, called adjacency pairs are adjacency pair\ncomposed of a \ufb01rst pair part and a second pair part (Schegloff, 1968), and these\nexpectations can help systems decide what actions to take.\nHowever, dialogue acts aren\u2019t always followed immediately by their second pair\npart. The two parts can be separated by a side sequence (Jefferson 1972) or sub- side sequence\ndialogue . For example utterances C 17to A 20constitute a correction subdialogue subdialogue\n(Litman 1985, Litman and Allen 1987, Chu-Carroll and Carberry 1998):\nC17: #Act. . . actually#, what day of the week is the 15th?\nA18: It\u2019s a Friday.\nC19: Uh hmm. I would consider staying there an extra day til Sunday.\nA20: OK. . . OK. On Sunday I have . . .\nThe question in C 17interrupts the prior discourse, in which the agent was looking\nfor a May 15 return \ufb02ight. The agent must answer the question and also realize that\n\u2018\u2019I would consider staying...til Sunday\u201d means that the client would probably like to\nchange their plan, and now go back to \ufb01nding return \ufb02ights, but for the 17th.\nAnother side sequence is the clari\ufb01cation question , which can form a subdia-\nlogue between a REQUEST and a RESPONSE . This is especially common in dialogue\nsystems where speech recognition errors causes the system to have to ask for clari-\n\ufb01cations or repetitions like the following:\nUser: What do you have going to UNKNOWN WORD on the 5th?\nSystem: Let\u2019s see, going where on the 5th?\nUser: Going to Hong Kong.\nSystem: OK, here are some \ufb02ights...\nIn addition to side-sequences, questions often have presequences , like the fol- presequence\nlowing example where a user starts with a question about the system\u2019s capabilities\n(\u201cCan you make train reservations\u201d) before making a request.\nUser: Can you make train reservations?\nSystem: Yes I can.\nUser: Great, I\u2019d like to reserve a seat on the 4pm train to New York.\nInitiative\nSometimes a conversation is completely controlled by one participant. For exam-\nple a reporter interviewing a chef might ask questions, and the chef responds. We\nsay that the reporter in this case has the conversational initiative (Carbonell, 1970; initiative\nNickerson, 1976). In normal human-human dialogue, however, it\u2019s more common\nfor initiative to shift back and forth between the participants, as they sometimes\nanswer questions, sometimes ask them, sometimes take the conversations in new di-\nrections, sometimes not. You may ask me a question, and then I respond asking you",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 478,
      "token_count": 788,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 322\n\n314 CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\nto clarify something you said, which leads the conversation in all sorts of ways. We\ncall such interactions mixed initiative (Carbonell, 1970).\nFull mixed initiative, while the norm for human-human conversations, can be\ndif\ufb01cult for dialogue systems. The most primitive dialogue systems tend to use\nsystem-initiative , where the system asks a question and the user can\u2019t do anything\nuntil they answer it, or user-initiative like simple search engines, where the user\nspeci\ufb01es a query and the system passively responds. Even modern large language\nmodel-based dialogue systems, which come much closer to using full mixed initia-\ntive, often don\u2019t have completely natural initiative switching. Getting this right is an\nimportant goal for modern systems.\nInference and Implicature\nInference is also important in dialogue understanding. Consider the client\u2019s response\nC2, repeated here:\nA2: And, what day in May did you want to travel?\nC3: OK uh I need to be there for a meeting that\u2019s from the 12th to the 15th.\nNotice that the client does not in fact answer the agent\u2019s question. The client\nmerely mentions a meeting at a certain time. What is it that licenses the agent to\ninfer that the client is mentioning this meeting so as to inform the agent of the travel\ndates?\nThe speaker seems to expect the hearer to draw certain inferences; in other\nwords, the speaker is communicating more information than seems to be present\nin the uttered words. This kind of example was pointed out by Grice (1975, 1978)\nas part of his theory of conversational implicature .Implicature means a particu- implicature\nlar class of licensed inferences. Grice proposed that what enables hearers to draw\nthese inferences is that conversation is guided by a set of maxims , general heuristics\nthat play a guiding role in the interpretation of conversational utterances. One such\nmaxim is the maxim of relevance which says that speakers attempt to be relevant, relevance\nthey don\u2019t just utter random speech acts. When the client mentions a meeting on the\n12th, the agent reasons \u2018There must be some relevance for mentioning this meeting.\nWhat could it be?\u2019. The agent knows that one precondition for having a meeting\n(at least before Web conferencing) is being at the place where the meeting is held,\nand therefore that maybe the meeting is a reason for the travel, and if so, then since\npeople like to arrive the day before a meeting, the agent should infer that the \ufb02ight\nshould be on the 11th.\nThese subtle characteristics of human conversations ( turns ,speech acts ,ground-\ning,dialogue structure ,initiative , and implicature ) are among the reasons it is dif-\n\ufb01cult to build dialogue systems that can carry on natural conversations with humans.\nMany of these challenges are active areas of dialogue systems research.\n15.2 Frame-Based Dialogue Systems\nAtask-based dialogue system has the goal of helping a user solve a speci\ufb01c task\nlike making a travel reservation or buying a product. Task-based dialogue systems\nare based around frames , \ufb01rst introduced in the early in\ufb02uential GUS system for frame\nGUS travel planning (Bobrow et al., 1977). Frames are knowledge structures representing\nthe details of the user\u2019s task speci\ufb01cation. Each frame consists of a collection of\nslots , each of which can take a set of possible values . Together a set of frames is slot",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 479,
      "token_count": 767,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 323\n\n15.2 \u2022 F RAME -BASED DIALOGUE SYSTEMS 315\nsometimes called a domain ontology .\nHere we\u2019ll describe the most well-studied frame-based architecture, the dialogue-\nstate architecture, made up of the six components shown in Fig. 15.3. In the next\nsections we\u2019ll introduce four of them, after introducing the idea of frames (deferring\nthe speech recognition and synthesis components to Chapter 16).\nFigure 15.3 Architecture of a dialogue-state system for task-oriented dialogue from Williams et al. (2016).\n15.2.1 Frames and Slot Filling\nThe frame and its slots in a task-based dialogue system specify what the system\nneeds to know to perform its task. A hotel reservation system needs dates and loca-\ntions. An alarm clock system needs a time. The system\u2019s goal is to \ufb01ll the slots in\nthe frame with the \ufb01llers the user intends, and then perform the relevant action for\nthe user (answering a question, or booking a \ufb02ight).\nFig. 15.4 shows a sample frame for booking air travel, with some sample ques-\ntions used for \ufb01lling slots. In the simplest frame-based systems (including most com-\nmercial assistants until quite recently), these questions are pre-written templates, but\nin more sophisticated systems, questions are generated on-the-\ufb02y. The slot \ufb01llers are\noften constrained to a particular semantic type, like type CITY (taking on values like\nSan Francisco , orHong Kong ) or DATE ,AIRLINE , or TIME .\nSlot Type Example Question\nORIGIN CITY city \u201cFrom what city are you leaving?\u201d\nDESTINATION CITY city \u201cWhere are you going?\u201d\nDEPARTURE TIME time \u201cWhen would you like to leave?\u201d\nDEPARTURE DATE date \u201cWhat day would you like to leave?\u201d\nARRIV AL TIME time \u201cWhen do you want to arrive?\u201d\nARRIV AL DATE date \u201cWhat day would you like to arrive?\u201d\nFigure 15.4 A frame in a frame-based dialogue system, showing the type of each slot and\na sample question used to \ufb01ll the slot.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 480,
      "token_count": 460,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 324\n\n316 CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\nMany domains require multiple frames. Besides frames for car or hotel reser-\nvations, we might need other frames for things like general route information (for\nquestions like Which airlines \ufb02y from Boston to San Francisco? ), That means the\nsystem must be able to disambiguate which slot of which frame a given input is\nsupposed to \ufb01ll.\nThe task of slot-\ufb01lling is usually combined with two other tasks, to extract 3\nthings from each user utterance. The \ufb01rst is domain classi\ufb01cation : is this user for\nexample talking about airlines, programming an alarm clock, or dealing with their\ncalendar? The second is user intent determination : what general task or goal is theintent\ndetermination\nuser trying to accomplish? For example the task could be to Find a Movie, or Show\na Flight, or Remove a Calendar Appointment. Together, the domain classi\ufb01cation\nand intent determination tasks decide which frame we are \ufb01lling. Finally, we need\nto do slot \ufb01lling itself: extract the particular slots and \ufb01llers that the user intends the slot \ufb01lling\nsystem to understand from their utterance with respect to their intent. From a user\nutterance like this one:\nShow me morning flights from Boston to San Francisco on Tuesday\na system might want to build a representation like:\nDOMAIN: AIR-TRAVEL INTENT: SHOW-FLIGHTS\nORIGIN-CITY: Boston DEST-CITY: San Francisco\nORIGIN-DATE: Tuesday ORIGIN-TIME: morning\nSimilarly an utterance like this: should give an intent like this:\nWake me tomorrow at 6 DOMAIN: ALARM-CLOCK\nINTENT: SET-ALARM\nTIME: 2017-07-01 0600\nThe simplest dialogue systems use handwritten rules for slot-\ufb01lling, like this\nregular expression for recognizing the SET-ALARM intent:\nwake me (up) | set (the|an) alarm | get me up\nBut most systems use supervised machine-learning: each sentence in a training\nset is annotated with slots, domain, and intent, and a sequence model maps from\ninput words to slot \ufb01llers, domain and intent. For example we\u2019ll have pairs of sen-\ntences that are labeled for domain ( AIRLINE ) and intent ( SHOWFLIGHT ), and are also\nlabeled with BIO representations for the slots and \ufb01llers. (Recall from Chapter 17\nthat in BIO tagging we introduce a tag for the beginning (B) and inside (I) of each\nslot label, and one for tokens outside (O) any slot label.)\nO O O O O B-DES I-DES O B-DEPTIME I-DEPTIME O AIRLINE-SHOWFLIGHT\nI want to fly to San Francisco on Monday afternoon please EOS\nFig. 15.5 shows a typical architecture for inference. The input words w1:::wn\nare passed through a pretrained language model encoder, followed by a feedforward\nlayer and a softmax at each token position over possible BIO tags, with the output\na series of BIO tags s1:::sn. We generally combine the domain-classi\ufb01cation and\nintent-extraction tasks with slot-\ufb01lling by adding a domain concatenated with an\nintent as the desired output for the \ufb01nal EOS token.\nOnce the sequence labeler has tagged the user utterance, a \ufb01ller string can be ex-\ntracted for each slot from the tags (e.g., \u201cSan Francisco\u201d), and these word strings\ncan then be normalized to the correct form in the ontology (perhaps the airport",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 481,
      "token_count": 794,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 325\n\n15.3 \u2022 D IALOGUE ACTS AND DIALOGUE STATE 317\nSanFranciscoonMondayEncodingsClassi\ufb01er+softmaxB-DESI-DESOB-DTIME\n\u2026d+i\n<EOS>Encoder \nFigure 15.5 Slot \ufb01lling by passing input words through an encoder, and then using a linear\nor feedforward layer followed by a softmax to generate a series of BIO tags. Here we also\nshow a \ufb01nal state: a domain concatenated with an intent.\ncode \u2018SFO\u2019), for example with dictionaries that specify that SF, SFO, and San Fran-\ncisco are synonyms. Often in industrial contexts, combinations of rules and machine\nlearning are used for each of these components.\nWe can make a very simple frame-based dialogue system by wrapping a small\namount of code around this slot extractor. Mainly we just need to ask the user\nquestions until all the slots are full, do a database query, then report back to the user,\nusing hand-built templates for generating sentences.\n15.2.2 Evaluating Task-Based Dialogue\nWe evaluate task-based systems by computing the task error rate , ortask success task error rate\nrate: the percentage of times the system booked the right plane \ufb02ight, or put the\nright event on the calendar. A more \ufb01ne-grained, but less extrinsic metric is the slot\nerror rate , the percentage of slots \ufb01lled with the correct values: slot error rate\nSlot Error Rate for a Sentence =# of inserted/deleted/subsituted slots\n# of total reference slots for sentence(15.1)\nFor example a system that extracted the slot structure below from this sentence:\n(15.2) Make an appointment with Chris at 10:30 in Gates 104\nSlot Filler\nPERSON Chris\nTIME 11:30 a.m.\nROOM Gates 104\nhas a slot error rate of 1/3, since the TIME is wrong. Instead of error rate, slot\nprecision, recall, and F-score can also be used. We can also measure ef\ufb01ciency\ncosts like the length of the dialogue in seconds or turns. ef\ufb01ciency costs\n15.3 Dialogue Acts and Dialogue State\nWhile the naive slot-extractor system described above can handle simple dialogues,\noften we want more complex interactions. For example, we might want to con\ufb01rm\nthat we\u2019ve understand the user, or ask them to repeat themselves. We can build a\nmore sophisticated system using dialogue acts anddialogue state .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 482,
      "token_count": 540,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 326",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 483,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "318 CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\n15.3.1 Dialogue Acts\nDialogue acts are a generalization of speech acts that also represent grounding. The dialogue acts\nset of acts can be general, or can be designed for particular dialogue tasks.\nTag Sys User Description\nHELLO (a=x;b=y;:::)X X Open a dialogue and give info a=x;b=y;:::\nINFORM (a=x;b=y;:::)X X Give info a=x;b=y;:::\nREQUEST (a;b=x;:::)X X Request value for a given b=x;:::\nREQALTS (a=x;:::) cX Request alternative with a=x;:::\nCONFIRM (a=x;b=y;:::)X X Explicitly con\ufb01rm a=x;b=y;:::\nCONFREQ (a=x;:::;d)Xc Implicitly con\ufb01rm a=x;:::and request value of d\nSELECT (a=x;a=y)Xc Implicitly con\ufb01rm a=x;:::and request value of d\nAFFIRM (a=x;b=y;:::)X X Af\ufb01rm and give further info a=x;b=y;:::\nNEGATE (a=x) cX Negate and give corrected value a=x\nDENY (a=x) cX Deny that a=x\nBYE() X X Close a dialogue\nFigure 15.6 Dialogue acts used by the HIS restaurant recommendation system of Young\net al. (2010). The Sys and User columns indicate which acts are valid as system outputs and\nuser inputs, respectively.\nFigure 15.6 shows a tagset for a restaurant recommendation system, and Fig. 15.7\nshows these tags labeling a sample dialogue from the HIS system (Young et al.,\n2010). This example also shows the content of each dialogue act, which are the slot\n\ufb01llers being communicated. So the user might INFORM the system that they want\nItalian food near a museum, or CONFIRM with the system that the price is reasonable.\nUtterance Dialogue act\nU: Hi, I am looking for somewhere to eat. hello(task = find,type=restaurant)\nS: You are looking for a restaurant. What\ntype of food do you like?confreq(type = restaurant, food)\nU: I\u2019d like an Italian near the museum. inform(food = Italian, near=museum)\nS: Roma is a nice Italian restaurant near\nthe museum.inform(name = \"Roma\", type = restaurant,\nfood = Italian, near = museum)\nU: Is it reasonably priced? confirm(pricerange = moderate)\nS: Yes, Roma is in the moderate price\nrange.affirm(name = \"Roma\", pricerange =\nmoderate)\nU: What is the phone number? request(phone)\nS: The number of Roma is 385456. inform(name = \"Roma\", phone = \"385456\")\nU: Ok, thank you goodbye. bye()\nFigure 15.7 A dialogue from the HIS System of Young et al. (2010) using the dialogue acts in Fig. 15.6.\n15.3.2 Dialogue State Tracking\nThe job of the dialogue-state tracker is to determine the current state of the frame\n(the \ufb01llers of each slot), and the user\u2019s most recent dialogue act. The dialogue-state\nis not just the slot-\ufb01llers in the current sentence; it includes the entire state of the\nframe at this point, summarizing all of the user\u2019s constraints. Fig. 15.8 from Mrk \u02c7si\u00b4c",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 484,
      "token_count": 761,
      "chapter_title": ""
    }
  },
  {
    "content": "type of food do you like?confreq(type = restaurant, food)\nU: I\u2019d like an Italian near the museum. inform(food = Italian, near=museum)\nS: Roma is a nice Italian restaurant near\nthe museum.inform(name = \"Roma\", type = restaurant,\nfood = Italian, near = museum)\nU: Is it reasonably priced? confirm(pricerange = moderate)\nS: Yes, Roma is in the moderate price\nrange.affirm(name = \"Roma\", pricerange =\nmoderate)\nU: What is the phone number? request(phone)\nS: The number of Roma is 385456. inform(name = \"Roma\", phone = \"385456\")\nU: Ok, thank you goodbye. bye()\nFigure 15.7 A dialogue from the HIS System of Young et al. (2010) using the dialogue acts in Fig. 15.6.\n15.3.2 Dialogue State Tracking\nThe job of the dialogue-state tracker is to determine the current state of the frame\n(the \ufb01llers of each slot), and the user\u2019s most recent dialogue act. The dialogue-state\nis not just the slot-\ufb01llers in the current sentence; it includes the entire state of the\nframe at this point, summarizing all of the user\u2019s constraints. Fig. 15.8 from Mrk \u02c7si\u00b4c\net al. (2017) shows the dialogue state after each turn.\nDialogue act detection is done just like domain or intent classi\ufb01cation, by passing\nthe input sentence through an encoder and adding an act classi\ufb01er. Often passing in\nthe prior dialogue act as well can improve classi\ufb01cation. And since dialogue acts",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 485,
      "token_count": 359,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 327",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 486,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "15.3 \u2022 D IALOGUE ACTS AND DIALOGUE STATE 319\nUser: I\u2019m looking for a cheaper restaurant\ninform(price=cheap)\nSystem: Sure. What kind - and where?\nUser: Thai food, somewhere downtown\ninform(price=cheap, food=Thai, area=centre)\nSystem: The House serves cheap Thai food\nUser: Where is it?\ninform(price=cheap, food=Thai, area=centre); request(address)\nSystem: The House is at 106 Regent Street\nFigure 15.8 The output of the dialogue state tracker after each turn (Mrk \u02c7si\u00b4c et al., 2017).\nplace some constraints on the slots and values, the tasks of dialogue-act detection and\nslot-\ufb01lling are often performed jointly. The state tracker can just take the output of\na slot-\ufb01lling sequence-model (Section 15.2.1) after each sentence, or do something\nmore complicated like training a classi\ufb01er to decide if a value has been changed.\nA special case: detecting correction acts. If a dialogue system misrecognizes\nor misunderstands an utterance, users will repeat or reformulate the utterance. De-\ntecting these user correction acts is quite important, especially for spoken lan-user correction\nacts\nguage. Ironically, corrections are actually harder to recognize than normal sentences\n(Swerts et al., 2000), because users who are frustrated adjust their speech in a way\nthat is dif\ufb01cult for speech recognizers (Goldberg et al., 2003). For example speak-\ners often use a prosodic style for corrections called hyperarticulation , in which thehyperarticula-\ntion\nutterance is louder or longer or exaggerated in pitch, such as I said BAL-TI-MORE,\nnot Boston (Wade et al. 1992, Levow 1998, Hirschberg et al. 2001). Detecting acts\ncan be part of the general dialogue act detection classi\ufb01er, or can make use of spe-\ncial features beyond the words, like those shown below (Levow 1998, Litman et al.\n1999, Hirschberg et al. 2001, Bulyko et al. 2005, Awadallah et al. 2015).\nfeatures examples\nsemantic embedding similarity between correction and user\u2019s prior utterance\nphonetic phonetic overlap between candidate correction act and user\u2019s prior utterance\n(i.e. \u201cWhatsApp\u201d may be incorrectly recognized as \u201cWhat\u2019s up\u201d)\nprosodic hyperarticulation, increases in F0 range, pause duration, and word duration\nASR ASR con\ufb01dence, language model probability\n15.3.3 Dialogue Policy: Which act to generate\nIn early commercial frame-based systems, the dialogue policy is simple: ask ques-\ntions until all the slots are full, do a database query, then report back to the user. A\nmore sophisticated dialogue policy can help a system decide when to answer the dialogue policy\nuser\u2019s questions, when to instead ask the user a clari\ufb01cation question, and so on. A\ndialogue policy thus decides what dialogue act to generate. Choosing a dialogue act\nto generate, along with its arguments, is sometimes called content planning .content\nplanning\nLet\u2019s see how to do this for some important dialogue acts. Dialogue systems, es-\npecially speech systems, often misrecognize the users\u2019 words or meaning. To ensure\nsystem and user share a common ground, systems must con\ufb01rm understandings with\nthe user or reject utterances that the system don\u2019t understand. A system might use",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 487,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "features examples\nsemantic embedding similarity between correction and user\u2019s prior utterance\nphonetic phonetic overlap between candidate correction act and user\u2019s prior utterance\n(i.e. \u201cWhatsApp\u201d may be incorrectly recognized as \u201cWhat\u2019s up\u201d)\nprosodic hyperarticulation, increases in F0 range, pause duration, and word duration\nASR ASR con\ufb01dence, language model probability\n15.3.3 Dialogue Policy: Which act to generate\nIn early commercial frame-based systems, the dialogue policy is simple: ask ques-\ntions until all the slots are full, do a database query, then report back to the user. A\nmore sophisticated dialogue policy can help a system decide when to answer the dialogue policy\nuser\u2019s questions, when to instead ask the user a clari\ufb01cation question, and so on. A\ndialogue policy thus decides what dialogue act to generate. Choosing a dialogue act\nto generate, along with its arguments, is sometimes called content planning .content\nplanning\nLet\u2019s see how to do this for some important dialogue acts. Dialogue systems, es-\npecially speech systems, often misrecognize the users\u2019 words or meaning. To ensure\nsystem and user share a common ground, systems must con\ufb01rm understandings with\nthe user or reject utterances that the system don\u2019t understand. A system might use\nanexplicit con\ufb01rmation act to con\ufb01rm with the user, like Is that correct? below:explicit\ncon\ufb01rmation",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 488,
      "token_count": 312,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 328",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 489,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "320 CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\nU:I\u2019d like to \ufb02y from Denver Colorado to New York City on September\ntwenty \ufb01rst in the morning on United Airlines\nS:Let\u2019s see then. I have you going from Denver Colorado to New York\non September twenty \ufb01rst. Is that correct?\nWhen using an implicit con\ufb01rmation act, a system instead grounds more im-implicit\ncon\ufb01rmation\nplicitly, for example by repeating the system\u2019s understanding as part of asking the\nnext question, as Shanghai is con\ufb01rmed in passing in this example:\nU:I want to travel to to Shanghai\nS:When do you want to travel to Shanghai?\nThere\u2019s a tradeoff. Explicit con\ufb01rmation makes it easier for users to correct mis-\nrecognitions by just answering \u201cno\u201d to the con\ufb01rmation question. But explicit con-\n\ufb01rmation is time-consuming and awkward (Danieli and Gerbino 1995, Walker et al.\n1998a). We also might want an act that expresses lack of understanding: rejection , rejection\nfor example with a prompt like I\u2019m sorry, I didn\u2019t understand that . To decide among\nthese acts, we can make use of the fact that ASR systems often compute their con\ufb01-\ndence in their transcription (often based on the log-likelihood the system assigns the\nsentence). A system can thus choose to explicitly con\ufb01rm only low-con\ufb01dence sen-\ntences. Or systems might have a four-tiered level of con\ufb01dence with three thresholds\na,b, and g:\n<alow con\ufb01dence reject\n\u0015aabove the threshold con\ufb01rm explicitly\n\u0015bhigh con\ufb01dence con\ufb01rm implictly\n\u0015gvery high con\ufb01dence don\u2019t con\ufb01rm at all\n15.3.4 Natural language generation: Sentence Realization\nrecommend(restaurant name= Au Midi, neighborhood = midtown,\ncuisine = french)\n1 Au Midi is in Midtown and serves French food.\n2 There is a French restaurant in Midtown called Au Midi.\nFigure 15.9 Sample inputs to the sentence realization phase of NLG, showing the dialogue\nact and attributes prespeci\ufb01ed by the content planner, and two distinct potential output sen-\ntences to be generated. From the restaurant recommendation system of Nayak et al. (2017).\nOnce a dialogue act has been chosen, we need to generate the text of the re-\nsponse to the user. This part of the generation process is called sentence realiza-\ntion. Fig. 15.9 shows a sample input/output for the sentence realization phase. Thesentence\nrealization\ncontent planner has chosen the dialogue act RECOMMEND and some slots (name,\nneighborhood, cuisine) and \ufb01llers. The sentence realizer generates a sentence like\nlines 1 or 2 (by training on examples of representation/sentence pairs from a corpus\nof labeled dialogues). Because we won\u2019t see every restaurant or attribute in every\npossible wording, we can delexicalize : generalize the training examples by replac- delexicalize\ning speci\ufb01c slot value words in the training set with a generic placeholder token\nrepresenting the slot. Fig. 15.10 shows the sentences in Fig. 15.9 delexicalized.\nWe can map from frames to delexicalized sentences with an encoder decoder\nmodel (Mrk \u02c7si\u00b4c et al. 2017, inter alia), trained on hand-labeled dialogue corpora like\nMultiWOZ (Budzianowski et al., 2018). The input to the encoder is a sequence of",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 490,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 329\n\n15.4 \u2022 C HATBOTS 321\nrecommend(restaurant name= Au Midi, neighborhood = midtown,\ncuisine = french)\n1restaurant name is inneighborhood and serves cuisine food.\n2 There is a cuisine restaurant in neighborhood calledrestaurant name .\nFigure 15.10 Delexicalized sentences that can be used for generating many different relex-\nicalized sentences. From the restaurant recommendation system of Nayak et al. (2017).\ndecentservice:RECOMMENDcuisine:null[name]hasdecentserviceENCODERDECODER\nFigure 15.11 An encoder decoder sentence realizer mapping slots/\ufb01llers to English.\ntokens xtthat represent the dialogue act (e.g., RECOMMEND ) and its arguments (e.g.,\nservice:decent, cuisine:null ) (Nayak et al., 2017), as in Fig. 15.11.\nThe decoder outputs the delexicalized English sentence \u201c name has decent ser-\nvice\u201d, which we can then relexicalize , i.e. \ufb01ll back in correct slot values, resulting relexicalize\nin \u201cAu Midi has decent service\u201d.\n15.4 Chatbots\nChatbots are systems that can carry on extended conversations with the goal of chatbot\nmimicking the unstructured conversations or \u2018chats\u2019 characteristic of informal human-\nhuman interaction. While early systems like ELIZA (Weizenbaum, 1966) or PARRY\n(Colby et al., 1971) had theoretical goals like testing theories of psychological coun-\nseling, for most of the last 50 years chatbots have been designed for entertainment.\nThat changed with the recent rise of neural chatbots like ChatGPT, which incor-\nporate solutions to NLP tasks like question answering, writing tools, or machine\ntranslation into a conversational interface. A conversation with ChatGPT is shown\nin Fig. 15.12. In this section we describe neural chatbot architectures and datasets.\n[TBD]\nFigure 15.12 A conversation with ChatGPT.\n15.4.1 Training chatbots\nData Chatbots are generally trained on a training set that includes standard large\nlanguage model training data of the type discussed in Section 10.3.2: versions of the\nweb from the Common Crawl, including news sites, Wikipedia, as well as books.\nFor training chatbots, it is common to additionally add lots of dialogue data.\nThis can include datasets created speci\ufb01cally for training chatbots by hiring\nspeakers of the language to have conversations, such as by having them take on\npersonas or talk about knowledge provided to them. For example the Topical-Chat\ndataset has 11K crowdsourced conversations spanning 8 broad topics (Gopalakrish-\nnan et al., 2019), the E MPATHETIC DIALOGUES includes 25K crowdsourced con-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 491,
      "token_count": 606,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 330\n\n322 CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\nversations grounded in a speci\ufb01c situation where a speaker was feeling a speci\ufb01c\nemotion (Rashkin et al., 2019), and the SaFeRDialogues dataset (Ung et al., 2022)\nhas 8k dialogues demonstrating graceful responses to conversational feedback about\nsafety failures.\nSuch datasets are far too small to train a language model alone, and so it\u2019s com-\nmon to also pretrain on large datasets of pseudo-conversations drawn from Twitter\n(Ritter et al., 2010a), Reddit (Roller et al., 2021), Weibo ( \u5fae\u535a), and other social\nmedia platforms. To turn social media data into data that has the structure of a con-\nversation, we can treat any post on the platform as the \ufb01rst turn in a conversation,\nand the sequence of comments/replies as subsequent turns in that conversation.\nDatasets from the web can be enormously toxic, so it\u2019s crucial to \ufb01lter the di-\nalogues \ufb01rst. This can be done by using the same toxicity classi\ufb01ers we describe\nbelow in the \ufb01ne-tuning section.\nArchitecture For training chatbots, it\u2019s most common to use the standard causal\nlanguage model architecture, in which the model predicts each word given all the\nprior words, and the loss is the standard language modeling loss. Fig. 15.13 shows a\nstandard training setup; no different than language model training in Chapter 9. The\nonly difference is the data, which has the addition of signi\ufb01cant conversation and\npseudo-conversation data as described in the prior section. As usual, the left context\ncan include the entire prior conversation (or as much as \ufb01ts in the context window).\nTransformerBlocksLM head\ngotpromoted!<s>gotpromoted!<s>Next wordCongratsLM Loss\u2026LM headLM headLM headLM headLM head\nICongrats!\u2026LM headLM head!-log y!-log yCongrats-log y<s>-log y!-log ypromoted-log ygot\u2026\u2026\u2026\u2026\u2026\u2026\nFigure 15.13 Training a causal (decoder-only) language model for a chatbot.\nAn alternative is to use the encoder-decoder architecture of Chapter 13. In this\ncase the entire conversation up to the last turn (as much as \ufb01ts in the context) is\npresented to the encoder, and the decoder generates the next turn.\npromotedgot!<s>Congrats!ENCODERDECODERI\nFigure 15.14 An alternative: an encoder-decoder language model for a chatbot.\nIn practice, dialogue systems require additional customization beyond just pre-\ntraining on dialogue data. In the next few sections we\u2019ll discuss various stages of",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 492,
      "token_count": 605,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 331\n\n15.4 \u2022 C HATBOTS 323\n\ufb01ne-tuning that can be used for this customization.\n15.4.2 Fine Tuning for Quality and Safety\nIt is a common practice for dialogue systems to use further labeled data for \ufb01ne-\ntuning. One function of this \ufb01ne-tuning step is to improve the quality of the dialogue,\ntraining the system to produce responses that are sensible and interesting. Another\nfunction might be to improve safety, keeping a dialogue system from suggesting\nharmful actions (like \ufb01nancial fraud, medical harm, inciting hatred, or abusing the\nuser or other people).\nIn the simplest method for improving quality and safety, speakers of the lan-\nguage are given an initial prompt and instructions to have high-quality, safe dia-\nlogues. They then interact with an initial dialogue system and their responses are\nused to \ufb01netune the model, usually as part of the instruct tuning step we introduced\nin Chapter 12. Thus a dialogue system learns to answer questions, follow other\ninstructions, and also carry on high-quality, safe dialogues, in a single multi-task\nlearning format.\nWhile \ufb01ne-tuning on positive examples is helpful, it is generally insuf\ufb01cient and\nso it is common to add more discriminative data that speci\ufb01cally downweights low-\nquality or harmful responses. The simplest paradigm for this is to train a model to\npredict turn-level safety and quality values, by training on human-labeled ratings.\nSuch ratings might be collected by \ufb01rst having speakers of the language carry on\ndialogues with a system, and then a second set of people act as labelers to label\nevery system turn for its quality and safety, resulting in a binary label for quality and\nsafety for each turn.\nOnce a dataset has been created with these labels, a language model can be used\nin a classi\ufb01cation task to label the quality and safety of a turn. For example in the\nLamda system (Cohen et al., 2022), a single language model is used in two phases,\nroughly corresponding to generative and discriminative tasks: \ufb01rst generating a re-\nsponse, and then generating a label. In the generative phase, the model is given the\nprior turn and a special RESPONSE token and generates the blue response turn. (In\ntraining, the training loss is given only for the blue response):\n\u201cWhat\u2019s up? RESPONSE Not much.\nIn a second, discriminative phase, the model is \ufb01ne-tuned to see an attribute\n(SENSIBLE, INTERESTING, UNSAFE) and then to predict a 0 or 1 value, again\nwith training losses given only for the blue value.\nWhat\u2019s up? RESPONSE Not much. SENSIBLE 1\nWhat\u2019s up? RESPONSE Not much. INTERESTING 0\nWhat\u2019s up? RESPONSE Not much. UNSAFE 0\nTo use the system in inference, the model \ufb01rst generates a response given the context,\nand then it is given the attribute and asked to generate a rating. The result is a\ngenerated turn along with a label. This label isn\u2019t shown to the user but can be use\nfor \ufb01ltering, either at training time or at deployment time. For example, the system\ncan generate multiple potential responses, \ufb01lter out any response that is unsafe, and\nreturn to the user the highest ranking response.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 493,
      "token_count": 728,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 332\n\n324 CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\n15.4.3 Learning to perform retrieval as part of responding\nModern chatbots are also trained to do retrieval (calls to Google or Bing or other\nsearch engines) as a component of generating a response. This is generally done\nby having a sort of fake dialogue participant called something like Search Query\nwhose utterances are passed to a search engine. The system can be encourged to do\nsearches either by prompting or by \ufb01ne-tuning.\nFor example in the Sparrow chatbot (Glaese et al., 2022), the prompt includes\na few sample conversations that include two special users, Search Query , and\nSearch Results :\n...[this is preceded by other parts of the prompt]\nUser: What color is the sky?\nSparrow: Blue.\nUser: Why?\nSearch Query: why is sky blue\nSearch Results: Page title: Sky Except for direct sunlight, most of the light in\nthe daytime sky is caused by scattering, which is dominated by\na small-particle limit called Rayleigh scattering. The scattering\ndue to molecule-sized particles (as in air) is greater in the di-\nrections both toward and away from the source of light than it\nis in directions perpendicular to the incident path. Scattering is\nsigni\ufb01cant for light at all visible wavelengths, but is stronger at\nthe shorter (bluer) end of the visible spectrum, meaning that the\nscattered light is bluer than its source: the Sun. The remain-\ning direct sunlight, having lost some of its shorter-wavelength\ncomponents, appears slightly less blue.\nSparrow: Because of the Rayleigh scattering. It causes short waves of\nblue light to be scattered more than the other colours in the spec-\ntrum.\nUser: Who was Raleigh?\nSearch Query: who was Rayleigh\n... ...\nFrom these prompts, the system learns to generate texts with Search Query\nturns for fact-based questions from the user, and these are passed to a search engine\nto generate the Search Results turns.\nAlternatively, systems can be \ufb01netuned to to know when to use a search en-\ngine. For example, labelers can interact with a system, fact check each of the re-\nsponses, and whenever the system emits an incorrect response, perform the web\nsearch queries that the system should have used to check its answer, and then the in-\nteration is recorded and used for \ufb01ne-tuning. Or labelers can look at a transcript of a\nlanguage model carrying on a dialogue, and similarly mark every place where a fact\nwas wrong (or out-of-date) and write the set of search queries that would have been\nappropriate. A system is then \ufb01ne-tuned to generate search query turns which\nare again passed to a search engine to generate the search responses . The set\nof pages or snippets returned by the search engine in the search response turn are\nthen treated as the context for generation, similarly to the retrieval-based question-\nanswering methods of Chapter 14.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 494,
      "token_count": 650,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 333\n\n15.5 \u2022 D IALOGUE SYSTEM DESIGN 325\n15.4.4 RLHF\nA more sophisticated family of methods uses reinforcement learning to learn to\nmatch human preferences for generated turns. In this method, RLHF for Rein- RLHF\nforcement Learning from Human Feedback, we give a system a dialogue context\nand sample two possible turns from the language model. We then have humans la-\nbel which of the two is better, creating a large dataset of sentence pairs with human\npreferences. These pairs are used to train a dialogue policy, and reinforcement learn-\ning is used to train the language model to generate turns that have higher rewards\n(Christiano et al., 2017; Ouyang et al., 2022). While using RLHF is the current state\nof the art at the time of this writing, a number of alternatives have been recently\ndeveloped that don\u2019t require reinforcement learning (Rafailov et al., 2023, e.g.,) and\nso this aspect of the \ufb01eld is changing very quickly.\n15.4.5 Evaluating Chatbots\nChatbots are evaluated by humans, who assign a score. This can be the human who\ntalked to the chatbot ( participant evaluation ) or a third party who reads a transcript\nof a human/chatbot conversation ( observer evaluation ). In the participant evalua-\ntion of See et al. (2019), the human evaluator chats with the model for six turns and\nrates the chatbot on 8 dimensions capturing conversational quality: avoiding repe-\ntition, interestingness, making sense, \ufb02uency, listening, inquisitiveness, humanness\nand engagingness on Likert scales like these:\nEngagingness How much did you enjoy talking to this user?\n\u000fNot at all\u000fA little\u000fSomewhat\u000fA lot\nMaking sense How often did this user say something which did NOT make sense?\n\u000fNever made any sense \u000fMost responses didn\u2019t make sense \u000fSome re-\nsponses didn\u2019t make sense \u000fEverything made perfect sense\nObserver evaluations use third party annotators to look at the text of a complete\nconversation. Sometimes we\u2019re interested in having raters assign a score to each\nsystem turn; for example (Artstein et al., 2009) have raters mark how coherent each\nturn is. Often, however, we just want a single high-level score to know if system A\nis better than system B. The acute-eval metric (Li et al., 2019a) is such an observer acute-eval\nevaluation in which annotators look at two separate human-computer conversations\nand choose the system which performed better on four metrics: engagingness, inter-\nestingness, humanness, and knowledgability.\n15.5 Dialogue System Design\nBecause of the important role of the user, the \ufb01eld of dialogue systems is closely\nlinked with Human-Computer Interaction (HCI). This is especially true for task-\noriented dialogue and assistants, where the design of dialogue strategies, sometimes\ncalled voice user interface design, generally follows user-centered design princi-voice user\ninterface\nples (Gould and Lewis, 1985):\n1. Study the user and task: Understand the users and the task by interviewing\nusers, investigating similar systems, and studying related human-human dialogues.\n2. Build simulations and prototypes: A crucial tool in building dialogue systems\nis the Wizard-of-Oz system . In wizard systems, the users interact with what theyWizard-of-Oz\nsystem",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 495,
      "token_count": 739,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 334\n\n326 CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\nthink is a program but is in fact a human \u201cwizard\u201d disguised by a software interface\n(Gould et al. 1983, Good et al. 1984, Fraser and\nGilbert 1991). The name comes from the chil-\ndren\u2019s book The Wizard of Oz (Baum, 1900),\nin which the wizard turned out to be a simu-\nlation controlled by a man behind a curtain or\nscreen. A wizard system can be used to test out\nan architecture before implementation; only the\ninterface software and databases need to be in\nplace. The wizard gets input from the user, uses\na database interface to run queries based on the\nuser utterance, and then outputs sentences, ei-\nther by typing them or speaking them.\nWizard-of-Oz systems are not a perfect\nsimulation, since the wizard doesn\u2019t exactly\nsimulate the errors or limitations of a real sys-\ntem; but wizard studies can still provide a useful \ufb01rst idea of the domain issues.\n3. Iteratively test the design on users: An iterative design cycle with embedded\nuser testing is essential in system design (Nielsen 1992, Cole et al. 1997, Yankelovich\net al. 1995, Landauer 1995). For example in a well-known incident, an early dia-\nlogue system required the user to press a key to interrupt the system (Stifelman et al.,\n1993). But user testing showed users barged in (interrupted, talking over the sys- barged in\ntem), which led to a redesign of the system to recognize overlapped speech. It\u2019s also\nimportant to incorporate value sensitive design , in which we carefully consider dur-value sensitive\ndesign\ning the design process the bene\ufb01ts, harms and possible stakeholders of the resulting\nsystem (Friedman et al. 2017, Friedman and Hendry 2019).\n15.5.1 Ethical Issues in Dialogue System Design\nEthical issues have been key to how we think about designing arti\ufb01cial agents since\nwell before we had dialogue systems. Mary Shelley (depicted below) centered her\nnovel Frankenstein around the problem of creating arti\ufb01cial agents without consider-\ning\nethical and humanistic concerns. One issue is the\nsafety of users. If users seek information from di-\nalogue systems in safety-critical situations like ask-\ning medical advice, or in emergency situations, or\nwhen indicating the intentions of self-harm, incorrect\nadvice can be dangerous and even life-threatening.\nFor example (Bickmore et al., 2018) gave participants\nmedical problems to pose to three commercial di-\nalogue systems (Siri, Alexa, Google Assistant) and\nasked them to determine an action to take based on\nthe system responses; many of the proposed actions,\nif actually taken, would have led to harm or death.\nA system can also harm users by verbally attacking them, or creating represen-\ntational harms (Blodgett et al., 2020) by generating abusive or harmful stereotypes\nthat demean particular groups of people. Both abuse and stereotypes can cause psy-\nchological harm to users. Microsoft\u2019s 2016 Tay chatbot, for example, was taken Tay\nof\ufb02ine 16 hours after it went live, when it began posting messages with racial slurs,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 496,
      "token_count": 729,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 335\n\n15.6 \u2022 S UMMARY 327\nconspiracy theories, and personal attacks on its users. Tay had learned these biases\nand actions from its training data, including from users who seemed to be purposely\nteaching the system to repeat this kind of language (Neff and Nagy 2016). Hender-\nson et al. (2017) examined dialogue datasets used to train corpus-based chatbots and\nfound toxic and abusive language, especially in social media corpora like Twitter\nand Reddit, and indeed such language then appears in the text generated by lan-\nguage models and dialogue systems (Gehman et al. 2020; Xu et al. 2020) which\ncan even amplify the bias from the training data (Dinan et al., 2020). Liu et al.\n(2020) developed another method for investigating bias, testing how neural dialogue\nsystems responded to pairs of simulated user turns that are identical except for men-\ntioning different genders or race. They found, for example, that simple changes like\nusing the word \u2018she\u2019 instead of \u2018he\u2019 in a sentence caused systems to respond more\noffensively and with more negative sentiment.\nAnother important ethical issue is privacy . Already in the \ufb01rst days of ELIZA,\nWeizenbaum pointed out the privacy implications of people\u2019s revelations to the chat-\nbot. The ubiquity of in-home dialogue systems means they may often overhear\nprivate information (Henderson et al., 2017). If a chatbot is human-like, users are\nalso more likely to disclose private information, and less likely to worry about the\nharm of this disclosure (Ischen et al., 2019). In general, chatbots that are trained\non transcripts of human-human or human-machine conversation must anonymize\npersonally identi\ufb01able information.\nFinally, chatbots raise important issues of gender equality in addition to textual\nbias. Current chatbots are overwhelmingly given female names, likely perpetuating\nthe stereotype of a subservient female servant (Paolino, 2017). And when users\nuse sexually harassing language, most commercial chatbots evade or give positive\nresponses rather than responding in clear negative ways (Fessler, 2017).\nThese ethical issues are an important area of investigation, including \ufb01nding\nways to mitigate problems of abuse and toxicity, like detecting and responding ap-\npropriately to toxic contexts (Wolf et al. 2017, Dinan et al. 2020, Xu et al. 2020).\nValue sensitive design, carefully considering possible harms in advance (Friedman\net al. 2017, Friedman and Hendry 2019) is also important; (Dinan et al., 2021) give\na number of suggestions for best practices in dialogue system design. For exam-\nple getting informed consent from participants, whether they are used for training,\nor whether they are interacting with a deployed system is important. Because di-\nalogue systems by de\ufb01nition involve human participants, researchers also work on\nthese issues with the Institutional Review Boards ( IRB) at their institutions, who IRB\nhelp protect the safety of experimental subjects.\n15.6 Summary\nChatbots anddialogue systems are crucial speech and language processing appli-\ncations that are already widely used commercially.\n\u2022 In human dialogue, speaking is a kind of action; these acts are referred to\nas speech acts or dialogue acts. Speakers also attempt to achieve common\nground by acknowledging that they have understand each other. Conversation\nalso is characterized by turn structure and dialogue structure.\n\u2022 Chatbots are conversational systems designed to mimic the appearance of in-\nformal human conversation. Rule-based chatbots like ELIZA and its modern",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 497,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 336\n\n328 CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\ndescendants use rules to map user sentences into system responses. Corpus-\nbased chatbots mine logs of human conversation to learn to automatically map\nuser sentences into system responses.\n\u2022 For task-based dialogue, most commercial dialogue systems use the GUS or\nframe-based architecture, in which the designer speci\ufb01es frames consisting of\nslots that the system must \ufb01ll by asking the user.\n\u2022 The dialogue-state architecture augments the GUS frame-and-slot architec-\nture with richer representations and more sophisticated algorithms for keeping\ntrack of user\u2019s dialogue acts, policies for generating its own dialogue acts, and\na natural language component.\n\u2022 Dialogue systems are a kind of human-computer interaction, and general HCI\nprinciples apply in their design, including the role of the user, simulations such\nas Wizard-of-Oz systems, and the importance of iterative design and testing\non real users.\nBibliographical and Historical Notes\nThe linguistic, philosophical, and psychological literature on dialogue is quite ex-\ntensive. For example the idea that utterances in a conversation are a kind of action\nbeing performed by the speaker was due originally to the philosopher Wittgenstein\n(1953) but worked out more fully by Austin (1962) and his student John Searle.\nVarious sets of speech acts have been de\ufb01ned over the years, and a rich linguistic\nand philosophical literature developed, especially focused on explaining the use of\nindirect speech acts. The idea of dialogue acts draws also from a number of other\nsources, including the ideas of adjacency pairs, pre-sequences, and other aspects of\nthe interactional properties of human conversation developed in the \ufb01eld of conver-\nsation analysis (see Levinson (1983) for an introduction to the \ufb01eld). This idea thatconversation\nanalysis\nacts set up strong local dialogue expectations was also pre\ufb01gured by Firth (1935, p.\n70), in a famous quotation:\nMost of the give-and-take of conversation in our everyday life is stereotyped\nand very narrowly conditioned by our particular type of culture. It is a sort\nof roughly prescribed social ritual, in which you generally say what the other\nfellow expects you, one way or the other, to say.\nAnother important research thread modeled dialogue as a kind of collaborative\nbehavior, including the ideas of common ground (Clark and Marshall, 1981), ref-\nerence as a collaborative process (Clark and Wilkes-Gibbs, 1986), joint intention\n(Levesque et al., 1990), and shared plans (Grosz and Sidner, 1980).\nThe earliest conversational systems were simple pattern-action chatbots like ELIZA\n(Weizenbaum, 1966). ELIZA had a widespread in\ufb02uence on popular perceptions of\narti\ufb01cial intelligence, and brought up some of the \ufb01rst ethical questions in natural\nlanguage processing \u2014such as the issues of privacy we discussed above as well the\nrole of algorithms in decision-making\u2014 leading its creator Joseph Weizenbaum to\n\ufb01ght for social responsibility in AI and computer science in general.\nComputational-implemented theories of dialogue blossomed in the 1970. That\nperiod saw the very in\ufb02uential GUS system (Bobrow et al., 1977), which in the late\n1970s established the frame-based paradigm that became the dominant industrial\nparadigm for dialogue systems for over 30 years.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 498,
      "token_count": 749,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 337",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 499,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 329\nAnother in\ufb02uential line of research from that decade focused on modeling the hi-\nerarchical structure of dialogue. Grosz\u2019s pioneering 1977b dissertation \ufb01rst showed\nthat \u201ctask-oriented dialogues have a structure that closely parallels the structure of\nthe task being performed\u201d (p. 27), leading to her work with Sidner and others show-\ning how to use similar notions of intention and plans to model discourse structure\nand coherence in dialogue. See, e.g., Lochbaum et al. (2000) for a summary of the\nrole of intentional structure in dialogue.\nYet a third line, \ufb01rst suggested by Bruce (1975), suggested that since speech acts\nare actions, they should be planned like other actions, and drew on the AI planning\nliterature (Fikes and Nilsson, 1971). A system seeking to \ufb01nd out some information\ncan come up with the plan of asking the interlocutor for the information. A system\nhearing an utterance can interpret a speech act by running the planner \u201cin reverse\u201d,\nusing inference rules to infer from what the interlocutor said what the plan might\nhave been. Plan-based models of dialogue are referred to as BDI models because BDI\nsuch planners model the beliefs ,desires , and intentions (BDI) of the system and in-\nterlocutor. BDI models of dialogue were \ufb01rst introduced by Allen, Cohen, Perrault,\nand their colleagues in a number of in\ufb02uential papers showing how speech acts could\nbe generated (Cohen and Perrault, 1979) and interpreted (Perrault and Allen 1980,\nAllen and Perrault 1980). At the same time, Wilensky (1983) introduced plan-based\nmodels of understanding as part of the task of interpreting stories.\nIn the 1990s, machine learning models that had \ufb01rst been applied to natural\nlanguage processing began to be applied to dialogue tasks like slot \ufb01lling (Miller\net al. 1994, Pieraccini et al. 1991). This period also saw lots of analytic work on the\nlinguistic properties of dialogue acts and on machine-learning-based methods for\ntheir detection. (Sag and Liberman 1975, Hinkelman and Allen 1989, Nagata and\nMorimoto 1994, Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke\net al. 2000, Gravano et al. 2012. This work strongly informed the development\nof the dialogue-state model (Larsson and Traum, 2000). Dialogue state tracking\nquickly became an important problem for task-oriented dialogue, and there has been\nan in\ufb02uential annual evaluation of state-tracking algorithms (Williams et al., 2016).\nThe turn of the century saw a line of work on applying reinforcement learning\nto dialogue, which \ufb01rst came out of AT&T and Bell Laboratories with work on\nMDP dialogue systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) along\nwith work on cue phrases, prosody, and rejection and con\ufb01rmation. Reinforcement\nlearning research turned quickly to the more sophisticated POMDP models (Roy\net al. 2000, Lemon et al. 2006, Williams and Young 2007) applied to small slot-\n\ufb01lling dialogue tasks. Neural reinforcement learning models have been used both for\nchatbot systems, for example simulating dialogues between two dialogue systems,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 500,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "Morimoto 1994, Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke\net al. 2000, Gravano et al. 2012. This work strongly informed the development\nof the dialogue-state model (Larsson and Traum, 2000). Dialogue state tracking\nquickly became an important problem for task-oriented dialogue, and there has been\nan in\ufb02uential annual evaluation of state-tracking algorithms (Williams et al., 2016).\nThe turn of the century saw a line of work on applying reinforcement learning\nto dialogue, which \ufb01rst came out of AT&T and Bell Laboratories with work on\nMDP dialogue systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) along\nwith work on cue phrases, prosody, and rejection and con\ufb01rmation. Reinforcement\nlearning research turned quickly to the more sophisticated POMDP models (Roy\net al. 2000, Lemon et al. 2006, Williams and Young 2007) applied to small slot-\n\ufb01lling dialogue tasks. Neural reinforcement learning models have been used both for\nchatbot systems, for example simulating dialogues between two dialogue systems,\nrewarding good conversational properties like coherence and ease of answering (Li\net al., 2016a), and for task-oriented dialogue (Williams et al., 2017).\nBy around 2010 the GUS architecture \ufb01nally began to be widely used commer-\ncially in dialogue systems on phones like Apple\u2019s SIRI (Bellegarda, 2013) and other\ndigital assistants.\nThe rise of the web gave rise to corpus-based chatbot architectures around the\nturn of the century, \ufb01rst using information retrieval models and then in the 2010s,\nafter the rise of deep learning, with sequence-to-sequence models.\n[TBD: Modern history of neural chatbots]\nOther important dialogue areas include the study of affect in dialogue (Rashkin\net al. 2019, Lin et al. 2019) and conversational interface design (Cohen et al. 2004,\nHarris 2005, Pearl 2017, Deibel and Evanhoe 2021).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 501,
      "token_count": 488,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 338\n\n330 CHAPTER 15 \u2022 C HATBOTS & D IALOGUE SYSTEMS\nExercises\n15.1 Write a \ufb01nite-state automaton for a dialogue manager for checking your bank\nbalance and withdrawing money at an automated teller machine.\n15.2 Adispreferred response is a response that has the potential to make a persondispreferred\nresponse\nuncomfortable or embarrassed in the conversational context; the most com-\nmon example dispreferred responses is turning down a request. People signal\ntheir discomfort with having to say no with surface cues (like the word well),\nor via signi\ufb01cant silence. Try to notice the next time you or someone else\nutters a dispreferred response, and write down the utterance. What are some\nother cues in the response that a system might use to detect a dispreferred\nresponse? Consider non-verbal cues like eye gaze and body gestures.\n15.3 When asked a question to which they aren\u2019t sure they know the answer, peo-\nple display their lack of con\ufb01dence by cues that resemble other dispreferred\nresponses. Try to notice some unsure answers to questions. What are some\nof the cues? If you have trouble doing this, read Smith and Clark (1993) and\nlisten speci\ufb01cally for the cues they mention.\n15.4 Implement a small air-travel help system based on text input. Your system\nshould get constraints from users about a particular \ufb02ight that they want to\ntake, expressed in natural language, and display possible \ufb02ights on a screen.\nMake simplifying assumptions. You may build in a simple \ufb02ight database or\nyou may use a \ufb02ight information system on the Web as your backend.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 502,
      "token_count": 370,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 339\n\nCHAPTER\n16Automatic Speech Recognition\nand Text-to-Speech\nI KNOW not whether\nI see your meaning: if I do, it lies\nUpon the wordy wavelets of your voice,\nDim as an evening shadow in a brook,\nThomas Lovell Beddoes, 1851\nUnderstanding spoken language, or at least transcribing the words into writing, is\none of the earliest goals of computer language processing. In fact, speech processing\npredates the computer by many decades!\nThe \ufb01rst machine that recognized speech\nwas a toy from the 1920s. \u201cRadio Rex\u201d,\nshown to the right, was a celluloid dog\nthat moved (by means of a spring) when\nthe spring was released by 500 Hz acous-\ntic energy. Since 500 Hz is roughly the\n\ufb01rst formant of the vowel [eh] in \u201cRex\u201d,\nRex seemed to come when he was called\n(David, Jr. and Selfridge, 1962).\nIn modern times, we expect more of our automatic systems. The task of auto-\nmatic speech recognition (ASR ) is to map any waveform like this: ASR\nto the appropriate string of words:\nIt's time for lunch!\nAutomatic transcription of speech by any speaker in any environment is still far from\nsolved, but ASR technology has matured to the point where it is now viable for many\npractical tasks. Speech is a natural interface for communicating with smart home ap-\npliances, personal assistants, or cellphones, where keyboards are less convenient, in\ntelephony applications like call-routing (\u201cAccounting, please\u201d) or in sophisticated\ndialogue applications (\u201cI\u2019d like to change the return date of my \ufb02ight\u201d). ASR is also\nuseful for general transcription, for example for automatically generating captions\nfor audio or video text (transcribing movies or videos or live discussions). Transcrip-\ntion is important in \ufb01elds like law where dictation plays an important role. Finally,\nASR is important as part of augmentative communication (interaction between com-\nputers and humans with some disability resulting in dif\ufb01culties or inabilities in typ-\ning or audition). The blind Milton famously dictated Paradise Lost to his daughters,\nand Henry James dictated his later novels after a repetitive stress injury.\nWhat about the opposite problem, going from text to speech? This is a problem\nwith an even longer history. In Vienna in 1769, Wolfgang von Kempelen built for",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 503,
      "token_count": 522,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 340\n\n332 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\nthe Empress Maria Theresa the famous Mechanical Turk, a chess-playing automaton\nconsisting of a wooden box \ufb01lled with gears, behind which sat a robot mannequin\nwho played chess by moving pieces with his mechanical arm. The Turk toured Eu-\nrope and the Americas for decades, defeating Napoleon Bonaparte and even playing\nCharles Babbage. The Mechanical Turk might have been one of the early successes\nof arti\ufb01cial intelligence were it not for the fact that it was, alas, a hoax, powered by\na human chess player hidden inside the box.\nWhat is less well known is that von Kempelen, an extraordinarily\nproli\ufb01c inventor, also built between\n1769 and 1790 what was de\ufb01nitely\nnot a hoax: the \ufb01rst full-sentence\nspeech synthesizer, shown partially to\nthe right. His device consisted of a\nbellows to simulate the lungs, a rub-\nber mouthpiece and a nose aperture, a\nreed to simulate the vocal folds, var-\nious whistles for the fricatives, and a\nsmall auxiliary bellows to provide the puff of air for plosives. By moving levers\nwith both hands to open and close apertures, and adjusting the \ufb02exible leather \u201cvo-\ncal tract\u201d, an operator could produce different consonants and vowels.\nMore than two centuries later, we no longer build our synthesizers out of wood\nand leather, nor do we need human operators. The modern task of speech synthesis ,speech\nsynthesis\nalso called text-to-speech orTTS , is exactly the reverse of ASR; to map text: text-to-speech\nTTSIt's time for lunch!\nto an acoustic waveform:\nModern speech synthesis has a wide variety of applications. TTS is used in\nconversational agents that conduct dialogues with people, plays a role in devices\nthat read out loud for the blind or in games, and can be used to speak for sufferers\nof neurological disorders, such as the late astrophysicist Steven Hawking who, after\nhe lost the use of his voice because of ALS, spoke by manipulating a TTS system.\nIn the next sections we\u2019ll show how to do ASR with encoder-decoders, intro-\nduce the CTC loss functions, the standard word error rate evaluation metric, and\ndescribe how acoustic features are extracted. We\u2019ll then see how TTS can be mod-\neled with almost the same algorithm in reverse, and conclude with a brief mention\nof other speech tasks.\n16.1 The Automatic Speech Recognition Task\nBefore describing algorithms for ASR, let\u2019s talk about how the task itself varies.\nOne dimension of variation is vocabulary size. Some ASR tasks can be solved with\nextremely high accuracy, like those with a 2-word vocabulary ( yesversus no) or\nan 11 word vocabulary like digit recognition (recognizing sequences of digits in-digit\nrecognition\ncluding zero tonine plus oh). Open-ended tasks like transcribing videos or human\nconversations, with large vocabularies of up to 60,000 words, are much harder.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 504,
      "token_count": 682,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 341",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 505,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "16.1 \u2022 T HEAUTOMATIC SPEECH RECOGNITION TASK 333\nA second dimension of variation is who the speaker is talking to. Humans speak-\ning to machines (either dictating or talking to a dialogue system) are easier to recog-\nnize than humans speaking to humans. Read speech , in which humans are reading read speech\nout loud, for example in audio books, is also relatively easy to recognize. Recog-\nnizing the speech of two humans talking to each other in conversational speech ,conversational\nspeech\nfor example, for transcribing a business meeting, is the hardest. It seems that when\nhumans talk to machines, or read without an audience present, they simplify their\nspeech quite a bit, talking more slowly and more clearly.\nA third dimension of variation is channel and noise. Speech is easier to recognize\nif it\u2019s recorded in a quiet room with head-mounted microphones than if it\u2019s recorded\nby a distant microphone on a noisy city street, or in a car with the window open.\nA \ufb01nal dimension of variation is accent or speaker-class characteristics. Speech\nis easier to recognize if the speaker is speaking the same dialect or variety that the\nsystem was trained on. Speech by speakers of regional or ethnic dialects, or speech\nby children can be quite dif\ufb01cult to recognize if the system is only trained on speak-\ners of standard dialects, or only adult speakers.\nA number of publicly available corpora with human-created transcripts are used\nto create ASR test and training sets to explore this variation; we mention a few of\nthem here since you will encounter them in the literature. LibriSpeech is a large LibriSpeech\nopen-source read-speech 16 kHz dataset with over 1000 hours of audio books from\nthe LibriV ox project, with transcripts aligned at the sentence level (Panayotov et al.,\n2015). It is divided into an easier (\u201cclean\u201d) and a more dif\ufb01cult portion (\u201cother\u201d)\nwith the clean portion of higher recording quality and with accents closer to US\nEnglish. This was done by running a speech recognizer (trained on read speech from\nthe Wall Street Journal) on all the audio, computing the WER for each speaker based\non the gold transcripts, and dividing the speakers roughly in half, with recordings\nfrom lower-WER speakers called \u201cclean\u201d and recordings from higher-WER speakers\n\u201cother\u201d.\nTheSwitchboard corpus of prompted telephone conversations between strangers Switchboard\nwas collected in the early 1990s; it contains 2430 conversations averaging 6 min-\nutes each, totaling 240 hours of 8 kHz speech and about 3 million words (Godfrey\net al., 1992). Switchboard has the singular advantage of an enormous amount of\nauxiliary hand-done linguistic labeling, including parses, dialogue act tags, phonetic\nand prosodic labeling, and discourse and information structure. The CALLHOME CALLHOME\ncorpus was collected in the late 1990s and consists of 120 unscripted 30-minute\ntelephone conversations between native speakers of English who were usually close\nfriends or family (Canavan et al., 1997).\nThe Santa Barbara Corpus of Spoken American English (Du Bois et al., 2005) is\na large corpus of naturally occurring everyday spoken interactions from all over the\nUnited States, mostly face-to-face conversation, but also town-hall meetings, food\npreparation, on-the-job talk, and classroom lectures. The corpus was anonymized by\nremoving personal names and other identifying information (replaced by pseudonyms\nin the transcripts, and masked in the audio).\nCORAAL is a collection of over 150 sociolinguistic interviews with African CORAAL",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 506,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "from lower-WER speakers called \u201cclean\u201d and recordings from higher-WER speakers\n\u201cother\u201d.\nTheSwitchboard corpus of prompted telephone conversations between strangers Switchboard\nwas collected in the early 1990s; it contains 2430 conversations averaging 6 min-\nutes each, totaling 240 hours of 8 kHz speech and about 3 million words (Godfrey\net al., 1992). Switchboard has the singular advantage of an enormous amount of\nauxiliary hand-done linguistic labeling, including parses, dialogue act tags, phonetic\nand prosodic labeling, and discourse and information structure. The CALLHOME CALLHOME\ncorpus was collected in the late 1990s and consists of 120 unscripted 30-minute\ntelephone conversations between native speakers of English who were usually close\nfriends or family (Canavan et al., 1997).\nThe Santa Barbara Corpus of Spoken American English (Du Bois et al., 2005) is\na large corpus of naturally occurring everyday spoken interactions from all over the\nUnited States, mostly face-to-face conversation, but also town-hall meetings, food\npreparation, on-the-job talk, and classroom lectures. The corpus was anonymized by\nremoving personal names and other identifying information (replaced by pseudonyms\nin the transcripts, and masked in the audio).\nCORAAL is a collection of over 150 sociolinguistic interviews with African CORAAL\nAmerican speakers, with the goal of studying African American Language ( AAL ),\nthe many variations of language used in African American communities (Kendall\nand Farrington, 2020). The interviews are anonymized with transcripts aligned at\nthe utterance level. The CHiME Challenge is a series of dif\ufb01cult shared tasks with CHiME\ncorpora that deal with robustness in ASR. The CHiME 5 task, for example, is ASR of\nconversational speech in real home environments (speci\ufb01cally dinner parties). The",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 507,
      "token_count": 416,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 342\n\n334 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\ncorpus contains recordings of twenty different dinner parties in real homes, each\nwith four participants, and in three locations (kitchen, dining area, living room),\nrecorded both with distant room microphones and with body-worn mikes. The\nHKUST Mandarin Telephone Speech corpus has 1206 ten-minute telephone con- HKUST\nversations between speakers of Mandarin across China, including transcripts of the\nconversations, which are between either friends or strangers (Liu et al., 2006). The\nAISHELL-1 corpus contains 170 hours of Mandarin read speech of sentences taken AISHELL-1\nfrom various domains, read by different speakers mainly from northern China (Bu\net al., 2017).\nFigure 16.1 shows the rough percentage of incorrect words (the word error rate ,\nor WER, de\ufb01ned on page 346) from state-of-the-art systems on some of these tasks.\nNote that the error rate on read speech (like the LibriSpeech audiobook corpus) is\naround 2%; this is a solved task, although these numbers come from systems that re-\nquire enormous computational resources. By contrast, the error rate for transcribing\nconversations between humans is much higher; 5.8 to 11% for the Switchboard and\nCALLHOME corpora. The error rate is higher yet again for speakers of varieties\nlike African American Vernacular English, and yet again for dif\ufb01cult conversational\ntasks like transcription of 4-speaker dinner party speech, which can have error rates\nas high as 81.3%. Character error rates (CER) are also much lower for read Man-\ndarin speech than for natural conversation.\nEnglish Tasks WER %\nLibriSpeech audiobooks 960hour clean 1.4\nLibriSpeech audiobooks 960hour other 2.6\nSwitchboard telephone conversations between strangers 5.8\nCALLHOME telephone conversations between family 11.0\nSociolinguistic interviews, CORAAL (AAL) 27.0\nCHiMe5 dinner parties with body-worn microphones 47.9\nCHiMe5 dinner parties with distant microphones 81.3\nChinese (Mandarin) Tasks CER %\nAISHELL-1 Mandarin read speech corpus 6.7\nHKUST Mandarin Chinese telephone conversations 23.5\nFigure 16.1 Rough Word Error Rates (WER = % of words misrecognized) reported around\n2020 for ASR on various American English recognition tasks, and character error rates (CER)\nfor two Chinese recognition tasks.\n16.2 Feature Extraction for ASR: Log Mel Spectrum\nThe \ufb01rst step in ASR is to transform the input waveform into a sequence of acoustic\nfeature vectors , each vector representing the information in a small time window feature vector\nof the signal. Let\u2019s see how to convert a raw wave\ufb01le to the most commonly used\nfeatures, sequences of log mel spectrum vectors. A speech signal processing course\nis recommended for more details.\n16.2.1 Sampling and Quantization\nThe input to a speech recognizer is a complex series of changes in air pressure.\nThese changes in air pressure obviously originate with the speaker and are caused",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 508,
      "token_count": 699,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 343",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 509,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "16.2 \u2022 F EATURE EXTRACTION FOR ASR: L OGMELSPECTRUM 335\nby the speci\ufb01c way that air passes through the glottis and out the oral or nasal cav-\nities. We represent sound waves by plotting the change in air pressure over time.\nOne metaphor which sometimes helps in understanding these graphs is that of a ver-\ntical plate blocking the air pressure waves (perhaps in a microphone in front of a\nspeaker\u2019s mouth, or the eardrum in a hearer\u2019s ear). The graph measures the amount\nofcompression orrarefaction (uncompression) of the air molecules at this plate.\nFigure 16.2 shows a short segment of a waveform taken from the Switchboard corpus\nof telephone speech of the vowel [iy] from someone saying \u201cshe just had a baby\u201d.\nTime (s)0 0.03875\u20130.016970.02283\n0\nFigure 16.2 A waveform of an instance of the vowel [iy] (the last vowel in the word \u201cbaby\u201d). The y-axis\nshows the level of air pressure above and below normal atmospheric pressure. The x-axis shows time. Notice\nthat the wave repeats regularly.\nThe \ufb01rst step in digitizing a sound wave like Fig. 16.2 is to convert the analog\nrepresentations (\ufb01rst air pressure and then analog electric signals in a microphone)\ninto a digital signal. This analog-to-digital conversion has two steps: sampling and sampling\nquantization . To sample a signal, we measure its amplitude at a particular time; the\nsampling rate is the number of samples taken per second. To accurately measure a\nwave, we must have at least two samples in each cycle: one measuring the positive\npart of the wave and one measuring the negative part. More than two samples per\ncycle increases the amplitude accuracy, but fewer than two samples causes the fre-\nquency of the wave to be completely missed. Thus, the maximum frequency wave\nthat can be measured is one whose frequency is half the sample rate (since every\ncycle needs two samples). This maximum frequency for a given sampling rate is\ncalled the Nyquist frequency . Most information in human speech is in frequenciesNyquist\nfrequency\nbelow 10,000 Hz; thus, a 20,000 Hz sampling rate would be necessary for com-\nplete accuracy. But telephone speech is \ufb01ltered by the switching network, and only\nfrequencies less than 4,000 Hz are transmitted by telephones. Thus, an 8,000 Hz\nsampling rate is suf\ufb01cient for telephone-bandwidth speech like the Switchboard\ncorpus, while 16,000 Hz sampling is often used for microphone speech.\nAlthough using higher sampling rates produces higher ASR accuracy, we can\u2019t\ncombine different sampling rates for training and testing ASR systems. Thus if\nwe are testing on a telephone corpus like Switchboard (8 KHz sampling), we must\ndownsample our training corpus to 8 KHz. Similarly, if we are training on mul-\ntiple corpora and one of them includes telephone speech, we downsample all the\nwideband corpora to 8Khz.\nAmplitude measurements are stored as integers, either 8 bit (values from -128\u2013\n127) or 16 bit (values from -32768\u201332767). This process of representing real-valued\nnumbers as integers is called quantization ; all values that are closer together than quantization\nthe minimum granularity (the quantum size) are represented identically. We refer to\neach sample at time index nin the digitized, quantized waveform as x[n].\nOnce data is quantized, it is stored in various formats. One parameter of these",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 510,
      "token_count": 774,
      "chapter_title": ""
    }
  },
  {
    "content": "plete accuracy. But telephone speech is \ufb01ltered by the switching network, and only\nfrequencies less than 4,000 Hz are transmitted by telephones. Thus, an 8,000 Hz\nsampling rate is suf\ufb01cient for telephone-bandwidth speech like the Switchboard\ncorpus, while 16,000 Hz sampling is often used for microphone speech.\nAlthough using higher sampling rates produces higher ASR accuracy, we can\u2019t\ncombine different sampling rates for training and testing ASR systems. Thus if\nwe are testing on a telephone corpus like Switchboard (8 KHz sampling), we must\ndownsample our training corpus to 8 KHz. Similarly, if we are training on mul-\ntiple corpora and one of them includes telephone speech, we downsample all the\nwideband corpora to 8Khz.\nAmplitude measurements are stored as integers, either 8 bit (values from -128\u2013\n127) or 16 bit (values from -32768\u201332767). This process of representing real-valued\nnumbers as integers is called quantization ; all values that are closer together than quantization\nthe minimum granularity (the quantum size) are represented identically. We refer to\neach sample at time index nin the digitized, quantized waveform as x[n].\nOnce data is quantized, it is stored in various formats. One parameter of these\nformats is the sample rate and sample size discussed above; telephone speech is\noften sampled at 8 kHz and stored as 8-bit samples, and microphone data is often\nsampled at 16 kHz and stored as 16-bit samples. Another parameter is the number of",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 511,
      "token_count": 342,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 344\n\n336 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\nchannels . For stereo data or for two-party conversations, we can store both channels channel\nin the same \ufb01le or we can store them in separate \ufb01les. A \ufb01nal parameter is individual\nsample storage\u2014linearly or compressed. One common compression format used for\ntelephone speech is m-law (often written u-law but still pronounced mu-law). The\nintuition of log compression algorithms like m-law is that human hearing is more\nsensitive at small intensities than large ones; the log represents small values with\nmore faithfulness at the expense of more error on large values. The linear (unlogged)\nvalues are generally referred to as linear PCM values (PCM stands for pulse code PCM\nmodulation, but never mind that). Here\u2019s the equation for compressing a linear PCM\nsample value xto 8-bit m-law, (where m=255 for 8 bits):\nF(x) =sgn(x)log(1+mjxj)\nlog(1+m)\u00001\u0014x\u00141 (16.1)\nThere are a number of standard \ufb01le formats for storing the resulting digitized wave-\n\ufb01le, such as Microsoft\u2019s .wav and Apple\u2019s AIFF all of which have special headers;\nsimple headerless \u201craw\u201d \ufb01les are also used. For example, the .wav format is a sub-\nset of Microsoft\u2019s RIFF format for multimedia \ufb01les; RIFF is a general format that\ncan represent a series of nested chunks of data and control information. Figure 16.3\nshows a simple .wav \ufb01le with a single data chunk together with its format chunk.\nFigure 16.3 Microsoft wave\ufb01le header format, assuming simple \ufb01le with one chunk. Fol-\nlowing this 44-byte header would be the data chunk.\n16.2.2 Windowing\nFrom the digitized, quantized representation of the waveform, we need to extract\nspectral features from a small window of speech that characterizes part of a par-\nticular phoneme. Inside this small window, we can roughly think of the signal as\nstationary (that is, its statistical properties are constant within this region). (By stationary\ncontrast, in general, speech is a non-stationary signal, meaning that its statistical non-stationary\nproperties are not constant over time). We extract this roughly stationary portion of\nspeech by using a window which is non-zero inside a region and zero elsewhere, run-\nning this window across the speech signal and multiplying it by the input waveform\nto produce a windowed waveform.\nThe speech extracted from each window is called a frame . The windowing is frame\ncharacterized by three parameters: the window size orframe size of the window\n(its width in milliseconds), the frame stride , (also called shift oroffset ) between stride\nsuccessive windows, and the shape of the window.\nTo extract the signal we multiply the value of the signal at time n,s[n]by the\nvalue of the window at time n,w[n]:\ny[n] =w[n]s[n] (16.2)\nThe window shape sketched in Fig. 16.4 is rectangular ; you can see the ex- rectangular\ntracted windowed signal looks just like the original signal. The rectangular window,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 512,
      "token_count": 708,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 345\n\n16.2 \u2022 F EATURE EXTRACTION FOR ASR: L OGMELSPECTRUM 337\nShift10 msWindow25 msShift10 msWindow25 msWindow25 ms\nFigure 16.4 Windowing, showing a 25 ms rectangular window with a 10ms stride.\nhowever, abruptly cuts off the signal at its boundaries, which creates problems when\nwe do Fourier analysis. For this reason, for acoustic feature creation we more com-\nmonly use the Hamming window, which shrinks the values of the signal toward Hamming\nzero at the window boundaries, avoiding discontinuities. Figure 16.5 shows both;\nthe equations are as follows (assuming a window that is Lframes long):\nrectangular w [n] =\u001a1 0\u0014n\u0014L\u00001\n0 otherwise(16.3)\nHamming w [n] =\u001a\n0:54\u00000:46cos (2pn\nL)0\u0014n\u0014L\u00001\n0 otherwise(16.4)\nTime (s)00.0475896\u20130.50.49990Rectangular windowHamming window\nTime (s)0.004559380.0256563\u20130.48260.49990\nTime (s)0.004559380.0256563\u20130.50.49990\nFigure 16.5 Windowing a sine wave with the rectangular or Hamming windows.\n16.2.3 Discrete Fourier Transform\nThe next step is to extract spectral information for our windowed signal; we need to\nknow how much energy the signal contains at different frequency bands. The tool",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 513,
      "token_count": 342,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 346\n\n338 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\nfor extracting spectral information for discrete frequency bands for a discrete-time\n(sampled) signal is the discrete Fourier transform orDFT .Discrete\nFourier\ntransformDFT The input to the DFT is a windowed signal x[n]:::x[m], and the output, for each\nofNdiscrete frequency bands, is a complex number X[k]representing the magni-\ntude and phase of that frequency component in the original signal. If we plot the\nmagnitude against the frequency, we can visualize the spectrum (see Appendix H\nfor more on spectra). For example, Fig. 16.6 shows a 25 ms Hamming-windowed\nportion of a signal and its spectrum as computed by a DFT (with some additional\nsmoothing).\nTime (s)0.0141752 0.039295\u20130.041210.04414\n0\nFrequency (Hz)0 8000Sound pressure level (dB /Hz)\n\u201320020\n(a) (b)\nFigure 16.6 (a) A 25 ms Hamming-windowed portion of a signal from the vowel [iy]\nand (b) its spectrum computed by a DFT.\nWe do not introduce the mathematical details of the DFT here, except to note\nthat Fourier analysis relies on Euler\u2019s formula , with jas the imaginary unit: Euler\u2019s formula\nejq=cosq+jsinq (16.5)\nAs a brief reminder for those students who have already studied signal processing,\nthe DFT is de\ufb01ned as follows:\nX[k] =N\u00001X\nn=0x[n]e\u0000j2p\nNkn(16.6)\nA commonly used algorithm for computing the DFT is the fast Fourier transformfast Fourier\ntransform\norFFT . This implementation of the DFT is very ef\ufb01cient but only works for values FFT\nofNthat are powers of 2.\n16.2.4 Mel Filter Bank and Log\nThe results of the FFT tell us the energy at each frequency band. Human hearing,\nhowever, is not equally sensitive at all frequency bands; it is less sensitive at higher\nfrequencies. This bias toward low frequencies helps human recognition, since in-\nformation in low frequencies (like formants) is crucial for distinguishing vowels or\nnasals, while information in high frequencies (like stop bursts or fricative noise) is\nless crucial for successful recognition. Modeling this human perceptual property\nimproves speech recognition performance in the same way.\nWe implement this intuition by collecting energies, not equally at each frequency\nband, but according to the melscale, an auditory frequency scale. A mel(Stevens mel\net al. 1937, Stevens and V olkmann 1940) is a unit of pitch. Pairs of sounds that are\nperceptually equidistant in pitch are separated by an equal number of mels. The mel",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 514,
      "token_count": 635,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 347\n\n16.3 \u2022 S PEECH RECOGNITION ARCHITECTURE 339\nfrequency mcan be computed from the raw acoustic frequency by a log transforma-\ntion:\nmel(f) =1127ln (1+f\n700) (16.7)\nWe implement this intuition by creating a bank of \ufb01lters that collect energy from\neach frequency band, spread logarithmically so that we have very \ufb01ne resolution\nat low frequencies, and less resolution at high frequencies. Figure 16.7 shows a\nsample bank of triangular \ufb01lters that implement this idea, that can be multiplied by\nthe spectrum to get a mel spectrum.\nm1m2mM...mel spectrum0770000.51AmplitudeFrequency (Hz)8K\nFigure 16.7 The mel \ufb01lter bank (Davis and Mermelstein, 1980). Each triangular \ufb01lter,\nspaced logarithmically along the mel scale, collects energy from a given frequency range.\nFinally, we take the log of each of the mel spectrum values. The human response\nto signal level is logarithmic (like the human response to frequency). Humans are\nless sensitive to slight differences in amplitude at high amplitudes than at low ampli-\ntudes. In addition, using a log makes the feature estimates less sensitive to variations\nin input such as power variations due to the speaker\u2019s mouth moving closer or further\nfrom the microphone.\n16.3 Speech Recognition Architecture\nThe basic architecture for ASR is the encoder-decoder (implemented with either\nRNNs or Transformers), exactly the same architecture introduced for MT in Chap-\nter 13. Generally we start from the log mel spectral features described in the previous\nsection, and map to letters, although it\u2019s also possible to map to induced morpheme-\nlike chunks like wordpieces or BPE.\nFig. 16.8 sketches the standard encoder-decoder architecture, which is com-\nmonly referred to as the attention-based encoder decoder orAED , orlisten attend AED\nand spell (LAS ) after the two papers which \ufb01rst applied it to speech (Chorowskilisten attend\nand spell\net al. 2014, Chan et al. 2016). The input is a sequence of tacoustic feature vectors\nF=f1;f2;:::;ft, one vector per 10 ms frame. The output can be letters or word-\npieces; we\u2019ll assume letters here. Thus the output sequence Y= (hSOSi;y1;:::;ymhEOSi),\nassuming special start of sequence and end of sequence tokens hsosiandheosiand\neach yiis a character; for English we might choose the set:\nyi2fa;b;c;:::;z;0;:::;9;hspacei;hcommai;hperiodi;hapostrophei;hunkig\nOf course the encoder-decoder architecture is particularly appropriate when in-\nput and output sequences have stark length differences, as they do for speech, with\nvery long acoustic feature sequences mapping to much shorter sequences of letters",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 515,
      "token_count": 653,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 348\n\n340 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\nENCODER\u2026DECODER\u2026\u2026ym\nFeature ComputationSubsampling\u2026Hftf180-dimensional log Mel spectrumper frameShorter sequence Xy1<s>iy2ity3t\u2018y4\u2018sy5s y6 ty7tiy8imy9mex1xn\nFigure 16.8 Schematic architecture for an encoder-decoder speech recognizer.\nor words. A single word might be 5 letters long but, supposing it lasts about 2\nseconds, would take 200 acoustic frames (of 10ms each).\nBecause this length difference is so extreme for speech, encoder-decoder ar-\nchitectures for speech need to have a special compression stage that shortens the\nacoustic feature sequence before the encoder stage. (Alternatively, we can use a loss\nfunction that is designed to deal well with compression, like the CTC loss function\nwe\u2019ll introduce in the next section.)\nThe goal of the subsampling is to produce a shorter sequence X=x1;:::;xnthat\nwill be the input to the encoder. The simplest algorithm is a method sometimes\ncalled low frame rate (Pundak and Sainath, 2016): for time iwe stack (concatenate) low frame rate\nthe acoustic feature vector fiwith the prior two vectors fi\u00001andfi\u00002to make a new\nvector three times longer. Then we simply delete fi\u00001and fi\u00002. Thus instead of\n(say) a 40-dimensional acoustic feature vector every 10 ms, we have a longer vector\n(say 120-dimensional) every 30 ms, with a shorter sequence length n=t\n3.1\nAfter this compression stage, encoder-decoders for speech use the same archi-\ntecture as for MT or other text, composed of either RNNs (LSTMs) or Transformers.\nFor inference, the probability of the output string Yis decomposed as:\np(y1;:::; yn) =nY\ni=1p(yijy1;:::; yi\u00001;X) (16.8)\nWe can produce each letter of the output via greedy decoding:\n\u02c6yi=argmax char2Alphabet P(charjy1:::yi\u00001;X) (16.9)\nAlternatively we can use beam search as described in the next section. This is par-\nticularly relevant when we are adding a language model.\nAdding a language model Since an encoder-decoder model is essentially a con-\nditional language model, encoder-decoders implicitly learn a language model for the\noutput domain of letters from their training data. However, the training data (speech\npaired with text transcriptions) may not include suf\ufb01cient text to train a good lan-\nguage model. After all, it\u2019s easier to \ufb01nd enormous amounts of pure text training\n1There are also more complex alternatives for subsampling, like using a convolutional net that down-\nsamples with max pooling, or layers of pyramidal RNNs , RNNs where each successive layer has half\nthe number of RNNs as the previous layer.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 516,
      "token_count": 676,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 349\n\n16.4 \u2022 CTC 341\ndata than it is to \ufb01nd text paired with speech. Thus we can can usually improve a\nmodel at least slightly by incorporating a very large language model.\nThe simplest way to do this is to use beam search to get a \ufb01nal beam of hy-\npothesized sentences; this beam is sometimes called an n-best list . We then use a n-best list\nlanguage model to rescore each hypothesis on the beam. The scoring is done by in- rescore\nterpolating the score assigned by the language model with the encoder-decoder score\nused to create the beam, with a weight ltuned on a held-out set. Also, since most\nmodels prefer shorter sentences, ASR systems normally have some way of adding a\nlength factor. One way to do this is to normalize the probability by the number of\ncharacters in the hypothesis jYjc. The following is thus a typical scoring function\n(Chan et al., 2016):\nscore(YjX) =1\njYjclogP(YjX)+llogPLM(Y) (16.10)\n16.3.1 Learning\nEncoder-decoders for speech are trained with the normal cross-entropy loss gener-\nally used for conditional language models. At timestep iof decoding, the loss is the\nlog probability of the correct token (letter) yi:\nLCE=\u0000logp(yijy1;:::; yi\u00001;X) (16.11)\nThe loss for the entire sentence is the sum of these losses:\nLCE=\u0000mX\ni=1logp(yijy1;:::; yi\u00001;X) (16.12)\nThis loss is then backpropagated through the entire end-to-end model to train the\nentire encoder-decoder.\nAs we described in Chapter 13, we normally use teacher forcing, in which the\ndecoder history is forced to be the correct gold yirather than the predicted \u02c6 yi. It\u2019s\nalso possible to use a mixture of the gold and decoder output, for example using\nthe gold output 90% of the time, but with probability .1 taking the decoder output\ninstead:\nLCE=\u0000logp(yijy1;:::; \u02c6yi\u00001;X) (16.13)\n16.4 CTC\nWe pointed out in the previous section that speech recognition has two particular\nproperties that make it very appropriate for the encoder-decoder architecture, where\nthe encoder produces an encoding of the input that the decoder uses attention to\nexplore. First, in speech we have a very long acoustic input sequence Xmapping to\na much shorter sequence of letters Y, and second, it\u2019s hard to know exactly which\npart of Xmaps to which part of Y.\nIn this section we brie\ufb02y introduce an alternative to encoder-decoder: an algo-\nrithm and loss function called CTC , short for Connectionist Temporal Classi\ufb01ca- CTC\ntion(Graves et al., 2006), that deals with these problems in a very different way. The\nintuition of CTC is to output a single character for every frame of the input, so that",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 517,
      "token_count": 681,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 350\n\n342 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\nthe output is the same length as the input, and then to apply a collapsing function\nthat combines sequences of identical letters, resulting in a shorter sequence.\nLet\u2019s imagine inference on someone saying the word dinner , and let\u2019s suppose\nwe had a function that chooses the most probable letter for each input spectral frame\nrepresentation xi. We\u2019ll call the sequence of letters corresponding to each input\nframe an alignment , because it tells us where in the acoustic signal each letter aligns alignment\nto. Fig. 16.9 shows one such alignment, and what happens if we use a collapsing\nfunction that just removes consecutive duplicate letters.\nX (input)A (alignment)Y (output)dx1ix2ix3nx4nx5nx6nx7ex8rx9rx10rx11rx12rx13rx14dinerwavefile\nFigure 16.9 A naive algorithm for collapsing an alignment between input and letters.\nWell, that doesn\u2019t work; our naive algorithm has transcribed the speech as diner ,\nnotdinner ! Collapsing doesn\u2019t handle double letters. There\u2019s also another problem\nwith our naive function; it doesn\u2019t tell us what symbol to align with silence in the\ninput. We don\u2019t want to be transcribing silence as random letters!\nThe CTC algorithm solves both problems by adding to the transcription alphabet\na special symbol for a blank , which we\u2019ll represent as . The blank can be used in blank\nthe alignment whenever we don\u2019t want to transcribe a letter. Blank can also be used\nbetween letters; since our collapsing function collapses only consecutive duplicate\nletters, it won\u2019t collapse across . More formally, let\u2019s de\ufb01ne the mapping B:a!y\nbetween an alignment aand an output y, which collapses all repeated letters and\nthen removes all blanks. Fig. 16.10 sketches this collapsing function B.\nX (input)A (alignment)remove blanksdx1ix2x3nx4nx5x6nx7ex8rx9rx10rx11rx12x13x14dinernmerge duplicatesdinernY (output)dinern\u2423\u2423\u2423\u2423\u2423\u2423\u2423\nFigure 16.10 The CTC collapsing function B, showing the space blank character ; re-\npeated (consecutive) characters in an alignment Aare removed to form the output Y.\nThe CTC collapsing function is many-to-one; lots of different alignments map\nto the same output string. For example, the alignment shown in Fig. 16.10 is not\nthe only alignment that results in the string dinner . Fig. 16.11 shows some other\nalignments that would produce the same output.\nIt\u2019s useful to think of the set of all alignments that might produce the same output\nY. We\u2019ll use the inverse of our Bfunction, called B\u00001, and represent that set as\nB\u00001(Y).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 518,
      "token_count": 633,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 351\n\n16.4 \u2022 CTC 343\ndinnneeerrr\u2423\u2423ddinnnerr\u2423\u2423\u2423dddinnnerr\u2423i\u2423\u2423\u2423\u2423\u2423\nFigure 16.11 Three other legitimate alignments producing the transcript dinner .\n16.4.1 CTC Inference\nBefore we see how to compute PCTC(YjX)let\u2019s \ufb01rst see how CTC assigns a proba-\nbility to one particular alignment \u02c6A=f\u02c6a1;:::; \u02c6ang. CTC makes a strong conditional\nindependence assumption: it assumes that, given the input X, the CTC model output\natat time tis independent of the output labels at any other time ai. Thus:\nPCTC(AjX) =TY\nt=1p(atjX) (16.14)\nThus to \ufb01nd the best alignment \u02c6A=f\u02c6a1;:::; \u02c6aTgwe can greedily choose the charac-\nter with the max probability at each time step t:\n\u02c6at=argmax\nc2Cpt(cjX) (16.15)\nWe then pass the resulting sequence Ato the CTC collapsing function Bto get the\noutput sequence Y.\nLet\u2019s talk about how this simple inference algorithm for \ufb01nding the best align-\nment A would be implemented. Because we are making a decision at each time\npoint, we can treat CTC as a sequence-modeling task, where we output one letter\n\u02c6ytat time tcorresponding to each input token xt, eliminating the need for a full de-\ncoder. Fig. 16.12 sketches this architecture, where we take an encoder, produce a\nhidden state htat each timestep, and decode by taking a softmax over the character\nvocabulary at each time step.\nENCODER\u2026yn\nFeature ComputationSubsampling\u2026ftf1 log Mel spectrumShorter inputsequence Xy1iy2iy3iy4tx1xnClassi\ufb01er+softmax\u2026ty5\u2026\u2026output lettersequence Y\nFigure 16.12 Inference with CTC: using an encoder-only model, with decoding done by\nsimple softmaxes over the hidden state htat each output step.\nAlas, there is a potential \ufb02aw with the inference algorithm sketched in (Eq. 16.15)\nand Fig. 16.11. The problem is that we chose the most likely alignment A, but the",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 519,
      "token_count": 547,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 352\n\n344 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\nmost likely alignment may not correspond to the most likely \ufb01nal collapsed output\nstring Y. That\u2019s because there are many possible alignments that lead to the same\noutput string, and hence the most likely output string might not correspond to the\nmost probable alignment. For example, imagine the most probable alignment Afor\nan input X= [x1x2x3]is the string [a b \u000f] but the next two most probable alignments\nare [b\u000fb] and [\u000fb b]. The output Y=[b b], summing over those two alignments,\nmight be more probable than Y=[a b].\nFor this reason, the most probable output sequence Yis the one that has, not\nthe single best CTC alignment, but the highest sum over the probability of all its\npossible alignments:\nPCTC(YjX) =X\nA2B\u00001(Y)P(AjX)\n=X\nA2B\u00001(Y)TY\nt=1p(atjht)\n\u02c6Y=argmax\nYPCTC(YjX) (16.16)\nAlas, summing over all alignments is very expensive (there are a lot of alignments),\nso we approximate this sum by using a version of Viterbi beam search that cleverly\nkeeps in the beam the high-probability alignments that map to the same output string,\nand sums those as an approximation of (Eq. 16.16). See Hannun (2017) for a clear\nexplanation of this extension of beam search for CTC.\nBecause of the strong conditional independence assumption mentioned earlier\n(that the output at time tis independent of the output at time t\u00001, given the input),\nCTC does not implicitly learn a language model over the data (unlike the attention-\nbased encoder-decoder architectures). It is therefore essential when using CTC to\ninterpolate a language model (and some sort of length factor L(Y)) using interpola-\ntion weights that are trained on a devset:\nscore CTC(YjX) =logPCTC(YjX)+l1logPLM(Y)l2L(Y) (16.17)\n16.4.2 CTC Training\nTo train a CTC-based ASR system, we use negative log-likelihood loss with a special\nCTC loss function. Thus the loss for an entire dataset Dis the sum of the negative\nlog-likelihoods of the correct output Yfor each input X:\nLCTC=X\n(X;Y)2D\u0000logPCTC(YjX) (16.18)\nTo compute CTC loss function for a single input pair (X;Y), we need the probability\nof the output Ygiven the input X. As we saw in Eq. 16.16, to compute the probability\nof a given output Ywe need to sum over all the possible alignments that would\ncollapse to Y. In other words:\nPCTC(YjX) =X\nA2B\u00001(Y)TY\nt=1p(atjht) (16.19)\nNaively summing over all possible alignments is not feasible (there are too many\nalignments). However, we can ef\ufb01ciently compute the sum by using dynamic pro-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 520,
      "token_count": 705,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 353\n\n16.4 \u2022 CTC 345\ngramming to merge alignments, with a version of the forward-backward algo-\nrithm also used to train HMMs (Appendix A) and CRFs. The original dynamic pro-\ngramming algorithms for both training and inference are laid out in (Graves et al.,\n2006); see (Hannun, 2017) for a detailed explanation of both.\n16.4.3 Combining CTC and Encoder-Decoder\nIt\u2019s also possible to combine the two architectures/loss functions we\u2019ve described,\nthe cross-entropy loss from the encoder-decoder architecture, and the CTC loss.\nFig. 16.13 shows a sketch. For training, we can simply weight the two losses with a\nltuned on a devset:\nL=\u0000llogPencdec(YjX)\u0000(1\u0000l)logPctc(YjX) (16.20)\nFor inference, we can combine the two with the language model (or the length\npenalty), again with learned weights:\n\u02c6Y=argmax\nY[llogPencdec(YjX)\u0000(1\u0000l)logPCTC(YjX)+glogPLM(Y)](16.21)\nENCODER\u2026DECODER\u2026H<s>it\u2018s timx1xn\u2026\u2026i   t   \u2019   s      t   i   m   e  \u2026CTC LossEncoder-Decoder Loss\nFigure 16.13 Combining the CTC and encoder-decoder loss functions.\n16.4.4 Streaming Models: RNN-T for improving CTC\nBecause of the strong independence assumption in CTC (assuming that the output\nat time tis independent of the output at time t\u00001), recognizers based on CTC\ndon\u2019t achieve as high an accuracy as the attention-based encoder-decoder recog-\nnizers. CTC recognizers have the advantage, however, that they can be used for\nstreaming . Streaming means recognizing words on-line rather than waiting until streaming\nthe end of the sentence to recognize them. Streaming is crucial for many applica-\ntions, from commands to dictation, where we want to start recognition while the\nuser is still talking. Algorithms that use attention need to compute the hidden state\nsequence over the entire input \ufb01rst in order to provide the attention distribution con-\ntext, before the decoder can start decoding. By contrast, a CTC algorithm can input\nletters from left to right immediately.\nIf we want to do streaming, we need a way to improve CTC recognition to re-\nmove the conditional independent assumption, enabling it to know about output his-\ntory. The RNN-Transducer ( RNN-T ), shown in Fig. 16.14, is just such a model RNN-T\n(Graves 2012, Graves et al. 2013). The RNN-T has two main components: a CTC\nacoustic model, and a separate language model component called the predictor that\nconditions on the output token history. At each time step t, the CTC encoder outputs",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 521,
      "token_count": 652,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 354\n\n346 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\na hidden state henc\ntgiven the input x1:::xt. The language model predictor takes as in-\nput the previous output token (not counting blanks), outputting a hidden state hpred\nu.\nThe two are passed through another network whose output is then passed through a\nsoftmax to predict the next character.\nPRNN\u0000T(YjX) =X\nA2B\u00001(Y)P(AjX)\n=X\nA2B\u00001(Y)TY\nt=1p(atjht;y<ut)\nENCODERP ( yt,u | x[1..t] , y[1..u-1] )\nxtPREDICTIONNETWORKyu-1JOINT NETWORKhencthpreduSOFTMAXzt,uDECODER\nFigure 16.14 The RNN-T model computing the output token distribution at time tby inte-\ngrating the output of a CTC acoustic encoder and a separate \u2018predictor\u2019 language model.\n16.5 ASR Evaluation: Word Error Rate\nThe standard evaluation metric for speech recognition systems is the word error word error\nrate. The word error rate is based on how much the word string returned by the\nrecognizer (the hypothesized word string) differs from a reference transcription.\nThe \ufb01rst step in computing word error is to compute the minimum edit distance in\nwords between the hypothesized and correct strings, giving us the minimum num-\nber of word substitutions , word insertions , and word deletions necessary to map\nbetween the correct and hypothesized strings. The word error rate (WER) is then\nde\ufb01ned as follows (note that because the equation includes insertions, the error rate\ncan be greater than 100%):\nWord Error Rate =100\u0002Insertions +Substitutions +Deletions\nTotal Words in Correct Transcript\nHere is a sample alignment between a reference and a hypothesis utterance from alignment\nthe CallHome corpus, showing the counts used to compute the error rate:\nREF: i *** ** UM the PHONE IS i LEFT THE portable **** PHONE UPSTAIRS last night\nHYP: i GOT IT TO the ***** FULLEST i LOVE TO portable FORM OF STORES last night\nEval: I I S D S S S I S S\nThis utterance has six substitutions, three insertions, and one deletion:\nWord Error Rate =1006+3+1\n13=76:9%",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 522,
      "token_count": 526,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 355",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 523,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "16.5 \u2022 ASR E VALUATION : W ORD ERROR RATE 347\nThe standard method for computing word error rates is a free script called sclite ,\navailable from the National Institute of Standards and Technologies (NIST) (NIST,\n2005). Sclite is given a series of reference (hand-transcribed, gold-standard) sen-\ntences and a matching set of hypothesis sentences. Besides performing alignments,\nand computing word error rate, sclite performs a number of other useful tasks. For\nexample, for error analysis it gives useful information such as confusion matrices\nshowing which words are often misrecognized for others, and summarizes statistics\nof words that are often inserted or deleted. sclite also gives error rates by speaker\n(if sentences are labeled for speaker ID), as well as useful statistics like the sentence\nerror rate , the percentage of sentences with at least one word error.Sentence error\nrate\nStatistical signi\ufb01cance for ASR: MAPSSWE or MacNemar\nAs with other language processing algorithms, we need to know whether a particular\nimprovement in word error rate is signi\ufb01cant or not.\nThe standard statistical tests for determining if two word error rates are different\nis the Matched-Pair Sentence Segment Word Error (MAPSSWE) test, introduced in\nGillick and Cox (1989).\nThe MAPSSWE test is a parametric test that looks at the difference between\nthe number of word errors the two systems produce, averaged across a number of\nsegments. The segments may be quite short or as long as an entire utterance; in\ngeneral, we want to have the largest number of (short) segments in order to justify\nthe normality assumption and to maximize power. The test requires that the errors\nin one segment be statistically independent of the errors in another segment. Since\nASR systems tend to use trigram LMs, we can approximate this requirement by\nde\ufb01ning a segment as a region bounded on both sides by words that both recognizers\nget correct (or by turn/utterance boundaries). Here\u2019s an example from NIST (2007)\nwith four regions:\nI II III IV\nREF: |it was|the best|of|times it|was the worst|of times| |it was\n| | | | | | | |\nSYS A:|ITS |the best|of|times it|IS the worst |of times|OR|it was\n| | | | | | | |\nSYS B:|it was|the best| |times it|WON the TEST |of times| |it was\nIn region I, system A has two errors (a deletion and an insertion) and system B\nhas zero; in region III, system A has one error (a substitution) and system B has two.\nLet\u2019s de\ufb01ne a sequence of variables Zrepresenting the difference between the errors\nin the two systems as follows:\nNi\nAthe number of errors made on segment iby system A\nNi\nB the number of errors made on segment iby system B\nZ Ni\nA\u0000Ni\nB;i=1;2;\u0001\u0001\u0001;nwhere nis the number of segments\nIn the example above, the sequence of Zvalues isf2;\u00001;\u00001;1g. Intuitively, if\nthe two systems are identical, we would expect the average difference, that is, the\naverage of the Zvalues, to be zero. If we call the true average of the differences\nmuz, we would thus like to know whether muz=0. Following closely the original\nproposal and notation of Gillick and Cox (1989), we can estimate the true average",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 524,
      "token_count": 776,
      "chapter_title": ""
    }
  },
  {
    "content": "SYS A:|ITS |the best|of|times it|IS the worst |of times|OR|it was\n| | | | | | | |\nSYS B:|it was|the best| |times it|WON the TEST |of times| |it was\nIn region I, system A has two errors (a deletion and an insertion) and system B\nhas zero; in region III, system A has one error (a substitution) and system B has two.\nLet\u2019s de\ufb01ne a sequence of variables Zrepresenting the difference between the errors\nin the two systems as follows:\nNi\nAthe number of errors made on segment iby system A\nNi\nB the number of errors made on segment iby system B\nZ Ni\nA\u0000Ni\nB;i=1;2;\u0001\u0001\u0001;nwhere nis the number of segments\nIn the example above, the sequence of Zvalues isf2;\u00001;\u00001;1g. Intuitively, if\nthe two systems are identical, we would expect the average difference, that is, the\naverage of the Zvalues, to be zero. If we call the true average of the differences\nmuz, we would thus like to know whether muz=0. Following closely the original\nproposal and notation of Gillick and Cox (1989), we can estimate the true average\nfrom our limited sample as \u02c6mz=Pn\ni=1Zi=n. The estimate of the variance of the Zi\u2019s\nis\ns2\nz=1\nn\u00001nX\ni=1(Zi\u0000mz)2(16.22)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 525,
      "token_count": 351,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 356\n\n348 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\nLet\nW=\u02c6mz\nsz=pn(16.23)\nFor a large enough n(>50),Wwill approximately have a normal distribution with\nunit variance. The null hypothesis is H0:mz=0, and it can thus be rejected if\n2\u0003P(Z\u0015jwj)\u00140:05 (two-tailed) or P(Z\u0015jwj)\u00140:05 (one-tailed), where Zis\nstandard normal and wis the realized value W; these probabilities can be looked up\nin the standard tables of the normal distribution.\nEarlier work sometimes used McNemar\u2019s test for signi\ufb01cance, but McNemar\u2019s McNemar\u2019s test\nis only applicable when the errors made by the system are independent, which is not\ntrue in continuous speech recognition, where errors made on a word are extremely\ndependent on errors made on neighboring words.\nCould we improve on word error rate as a metric? It would be nice, for exam-\nple, to have something that didn\u2019t give equal weight to every word, perhaps valuing\ncontent words like Tuesday more than function words like aorof. While researchers\ngenerally agree that this would be a good idea, it has proved dif\ufb01cult to agree on\na metric that works in every application of ASR. For dialogue systems, however,\nwhere the desired semantic output is more clear, a metric called slot error rate or\nconcept error rate has proved extremely useful; it is discussed in Chapter 15 on page\n317.\n16.6 TTS\nThe goal of text-to-speech (TTS) systems is to map from strings of letters to wave-\nforms, a technology that\u2019s important for a variety of applications from dialogue sys-\ntems to games to education.\nLike ASR systems, TTS systems are generally based on the encoder-decoder\narchitecture, either using LSTMs or Transformers. There is a general difference in\ntraining. The default condition for ASR systems is to be speaker-independent: they\nare trained on large corpora with thousands of hours of speech from many speakers\nbecause they must generalize well to an unseen test speaker. By contrast, in TTS, it\u2019s\nless crucial to use multiple voices, and so basic TTS systems are speaker-dependent:\ntrained to have a consistent voice, on much less data, but all from one speaker. For\nexample, one commonly used public domain dataset, the LJ speech corpus, consists\nof 24 hours of one speaker, Linda Johnson, reading audio books in the LibriV ox\nproject (Ito and Johnson, 2017), much smaller than standard ASR corpora which are\nhundreds or thousands of hours.2\nWe generally break up the TTS task into two components. The \ufb01rst component\nis an encoder-decoder model for spectrogram prediction : it maps from strings of\nletters to mel spectrographs: sequences of mel spectral values over time. Thus we\n2There is also recent TTS research on the task of multi-speaker TTS, in which a system is trained on\nspeech from many speakers, and can switch between different voices.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 526,
      "token_count": 677,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 357\n\n16.6 \u2022 TTS 349\nmight map from this string:\nIt's time for lunch!\nto the following mel spectrogram:\nThe second component maps from mel spectrograms to waveforms. Generating\nwaveforms from intermediate representations like spectrograms is called vocoding vocoding\nand this second component is called a vocoder : vocoder\nThese standard encoder-decoder algorithms for TTS are still quite computation-\nally intensive, so a signi\ufb01cant focus of modern research is on ways to speed them\nup.\n16.6.1 TTS Preprocessing: Text normalization\nBefore either of these two steps, however, TTS systems require text normaliza-\ntion preprocessing for handling non-standard words : numbers, monetary amounts,non-standard\nwords\ndates, and other concepts that are verbalized differently than they are spelled. A TTS\nsystem seeing a number like 151needs to know to verbalize it as one hundred \ufb01fty\noneif it occurs as $151 but as one \ufb01fty one if it occurs in the context 151 Chapulte-\npec Ave. . The number 1750 can be spoken in at least four different ways, depending\non the context:\nseventeen fifty: (in\u201cThe European economy in 1750\u201d )\none seven five zero: (in\u201cThe password is 1750\u201d )\nseventeen hundred and fifty: (in\u201c1750 dollars\u201d )\none thousand, seven hundred, and fifty: (in\u201c1750 dollars\u201d )\nOften the verbalization of a non-standard word depends on its meaning (what\nTaylor (2009) calls its semiotic class ). Fig. 16.15 lays out some English non-\nstandard word types.\nMany classes have preferred realizations. A year is generally read as paired\ndigits (e.g., seventeen fifty for 1750). $3.2 billion must be read out with the\nworddollars at the end, as three point two billion dollars . Some ab-\nbreviations like N.Y. are expanded (to New York ), while other acronyms like GPU\nare pronounced as letter sequences. In languages with grammatical gender, normal-\nization may depend on morphological properties. In French, the phrase 1 mangue\n(\u2018one mangue\u2019) is normalized to une mangue , but 1 ananas (\u2018one pineapple\u2019) is\nnormalized to un ananas . In German, Heinrich IV (\u2018Henry IV\u2019) can be normalized\ntoHeinrich der Vierte ,Heinrich des Vierten ,Heinrich dem Vierten , or\nHeinrichdenVierten depending on the grammatical case of the noun (Demberg,\n2006).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 527,
      "token_count": 561,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 358",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 528,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "350 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\nsemiotic class examples verbalization\nabbreviations gov\u2019t , N.Y., mph government\nacronyms read as letters GPU , D.C., PC, UN, IBM G P U\ncardinal numbers 12, 45, 1/2, 0.6 twelve\nordinal numbers May 7, 3rd, Bill Gates III seventh\nnumbers read as digits Room 101 one oh one\ntimes 3.20, 11:45 eleven forty \ufb01ve\ndates 28/02 (or in US, 2/28) February twenty eighth\nyears 1999 , 80s, 1900s, 2045 nineteen ninety nine\nmoney $3.45 ,e250, $200K three dollars forty \ufb01ve\nmoney in tr/m/billions $3.45 billion three point four \ufb01ve billion dollars\npercentage 75% 3.4% seventy \ufb01ve percent\nFigure 16.15 Some types of non-standard words in text normalization; see Sproat et al.\n(2001) and (van Esch and Sproat, 2018) for many more.\nModern end-to-end TTS systems can learn to do some normalization themselves,\nbut TTS systems are only trained on a limited amount of data (like the 220,000 words\nwe mentioned above for the LJ corpus (Ito and Johnson, 2017)), and so a separate\nnormalization step is important.\nNormalization can be done by rule or by an encoder-decoder model. Rule-based\nnormalization is done in two stages: tokenization and verbalization. In the tokeniza-\ntion stage we hand-write rules to detect non-standard words. These can be regular\nexpressions, like the following for detecting years:\n/(1[89][0-9][0-9])j(20[0-9][0-9]/\nA second pass of rules express how to verbalize each semiotic class. Larger TTS\nsystems instead use more complex rule-systems, like the Kestral system of (Ebden\nand Sproat, 2015), which \ufb01rst classi\ufb01es and parses each input into a normal form\nand then produces text using a verbalization grammar. Rules have the advantage\nthat they don\u2019t require training data, and they can be designed for high precision, but\ncan be brittle, and require expert rule-writers so are hard to maintain.\nThe alternative model is to use encoder-decoder models, which have been shown\nto work better than rules for such transduction tasks, but do require expert-labeled\ntraining sets in which non-standard words have been replaced with the appropriate\nverbalization; such training sets for some languages are available (Sproat and Gor-\nman 2018, Zhang et al. 2019).\nIn the simplest encoder-decoder setting, we simply treat the problem like ma-\nchine translation, training a system to map from:\nThey live at 224 Mission St.\nto\nThey live at two twenty four Mission Street\nWhile encoder-decoder algorithms are highly accurate, they occasionally pro-\nduce errors that are egregious; for example normalizing 45 minutes asforty \ufb01ve mil-\nlimeters . To address this, more complex systems use mechanisms like lightweight\ncovering grammars , which enumerate a large set of possible verbalizations but\ndon\u2019t try to disambiguate, to constrain the decoding to avoid such outputs (Zhang\net al., 2019).\n16.6.2 TTS: Spectrogram prediction\nThe exact same architecture we described for ASR\u2014the encoder-decoder with attention\u2013",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 529,
      "token_count": 775,
      "chapter_title": ""
    }
  },
  {
    "content": "that they don\u2019t require training data, and they can be designed for high precision, but\ncan be brittle, and require expert rule-writers so are hard to maintain.\nThe alternative model is to use encoder-decoder models, which have been shown\nto work better than rules for such transduction tasks, but do require expert-labeled\ntraining sets in which non-standard words have been replaced with the appropriate\nverbalization; such training sets for some languages are available (Sproat and Gor-\nman 2018, Zhang et al. 2019).\nIn the simplest encoder-decoder setting, we simply treat the problem like ma-\nchine translation, training a system to map from:\nThey live at 224 Mission St.\nto\nThey live at two twenty four Mission Street\nWhile encoder-decoder algorithms are highly accurate, they occasionally pro-\nduce errors that are egregious; for example normalizing 45 minutes asforty \ufb01ve mil-\nlimeters . To address this, more complex systems use mechanisms like lightweight\ncovering grammars , which enumerate a large set of possible verbalizations but\ndon\u2019t try to disambiguate, to constrain the decoding to avoid such outputs (Zhang\net al., 2019).\n16.6.2 TTS: Spectrogram prediction\nThe exact same architecture we described for ASR\u2014the encoder-decoder with attention\u2013\ncan be used for the \ufb01rst component of TTS. Here we\u2019ll give a simpli\ufb01ed overview",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 530,
      "token_count": 305,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 359",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 531,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "16.6 \u2022 TTS 351\nof the Tacotron2 architecture (Shen et al., 2018), which extends the earlier Tacotron Tacotron2\n(Wang et al., 2017) architecture and the Wavenet vocoder (van den Oord et al., Wavenet\n2016). Fig. 16.16 sketches out the entire architecture.\nThe encoder\u2019s job is to take a sequence of letters and produce a hidden repre-\nsentation representing the letter sequence, which is then used by the attention mech-\nanism in the decoder. The Tacotron2 encoder \ufb01rst maps every input grapheme to\na 512-dimensional character embedding. These are then passed through a stack\nof 3 convolutional layers, each containing 512 \ufb01lters with shape 5 \u00021, i.e. each\n\ufb01lter spanning 5 characters, to model the larger letter context. The output of the\n\ufb01nal convolutional layer is passed through a biLSTM to produce the \ufb01nal encoding.\nIt\u2019s common to use a slightly higher quality (but slower) version of attention called\nlocation-based attention , in which the computation of the avalues (Eq. 8.36 inlocation-based\nattention\nChapter 8) makes use of the avalues from the prior time-state.\nIn the decoder, the predicted mel spectrum from the prior time slot is passed\nthrough a small pre-net as a bottleneck. This prior output is then concatenated with\nthe encoder\u2019s attention vector context and passed through 2 LSTM layers. The out-\nput of this LSTM is used in two ways. First, it is passed through a linear layer, and\nsome output processing, to autoregressively predict one 80-dimensional log-mel \ufb01l-\nterbank vector frame (50 ms, with a 12.5 ms stride) at each step. Second, it is passed\nthrough another linear layer to a sigmoid to make a \u201cstop token prediction\u201d decision\nabout whether to stop producing output.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 532,
      "token_count": 421,
      "chapter_title": ""
    }
  },
  {
    "content": "While linear spectrograms discard phase information (and aretherefore lossy), algorithms such as Grif\ufb01n-Lim [14] are capable ofestimating this discarded information, which enables time-domainconversion via the inverse short-time Fourier transform. Mel spectro-grams discard even more information, presenting a challenging in-verse problem. However, in comparison to the linguistic and acousticfeatures used in WaveNet, the mel spectrogram is a simpler, lower-level acoustic representation of audio signals. It should thereforebe straightforward for a similar WaveNet model conditioned on melspectrograms to generate audio, essentially as a neural vocoder. In-deed, we will show that it is possible to generate high quality audiofrom mel spectrograms using a modi\ufb01ed WaveNet architecture.2.2. Spectrogram Prediction NetworkAs in Tacotron, mel spectrograms are computed through a short-time Fourier transform (STFT) using a 50 ms frame size, 12.5 msframe hop, and a Hann window function. We experimented with a5 ms frame hop to match the frequency of the conditioning inputsin the original WaveNet, but the corresponding increase in temporalresolution resulted in signi\ufb01cantly more pronunciation issues.We transform the STFT magnitude to the mel scale using an 80channel mel \ufb01lterbank spanning 125 Hz to 7.6 kHz, followed by logdynamic range compression. Prior to log compression, the \ufb01lterbankoutput magnitudes are clipped to a minimum value of 0.01 in orderto limit dynamic range in the logarithmic domain.The network is composed of an encoder and a decoder with atten-tion. The encoder converts a character sequence into a hidden featurerepresentation which the decoder consumes to predict a spectrogram.Input characters are represented using a learned 512-dimensionalcharacter embedding, which are passed through a stack of 3 convolu-tional layers each containing 512 \ufb01lters with shape5\u21e51, i.e., whereeach \ufb01lter spans 5 characters, followed by batch normalization [18]and ReLU activations. As in Tacotron, these convolutional layersmodel longer-term context (e.g.,N-grams) in the input charactersequence. The output of the \ufb01nal convolutional layer is passed into asingle bi-directional [19] LSTM [20] layer containing 512 units (256in each direction) to generate the encoded features.The encoder output is consumed by an attention network whichsummarizes the full encoded sequence as a \ufb01xed-length context vectorfor each decoder output step. We use the location-sensitive attentionfrom [21], which extends the additive attention mechanism [22] touse cumulative attention weights from previous decoder time stepsas an additional feature. This encourages the model to move forwardconsistently through the input, mitigating potential failure modeswhere some subsequences are repeated or ignored by the decoder.Attention probabilities are computed after projecting inputs and lo-cation features to 128-dimensional hidden representations. Locationfeatures are computed using 32 1-D convolution \ufb01lters of length 31.The decoder is an autoregressive recurrent neural network whichpredicts a mel spectrogram from the encoded input sequence oneframe at a time. The prediction from the previous time step is \ufb01rstpassed through a smallpre-netcontaining 2 fully connected layersof 256 hidden ReLU units. We found that the pre-net acting as aninformation bottleneck was essential for learning attention. The pre-net output and attention context vector are concatenated and passedthrough a stack of 2 uni-directional LSTM layers with 1024 units.The concatenation of the LSTM output and the attention contextvector is projected through a linear transform to predict the targetspectrogram frame. Finally, the predicted mel spectrogram is passedthrough a 5-layer convolutionalpost-netwhich predicts a residualto add to the prediction to improve the overall reconstruction. Each",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 533,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "'LEVEGXIV\u0004)QFIHHMRK0SGEXMSR\u00047IRWMXMZI\u0004%XXIRXMSR\u0017\u0004'SRZ\u00040E]IVW&MHMVIGXMSREP\u00040781-RTYX\u00048I\\X\u0016\u00040E]IV\u00044VI\u00112IX\u0016\u00040781\u00040E]IVW0MRIEV\u00044VSNIGXMSR0MRIEV\u00044VSNIGXMSR7XST\u00048SOIR\u0019\u0004'SRZ\u00040E]IV\u00044SWX\u00112IX\n0HO\u00036SHFWURJUDP;EZI2IX\u00041S0;EZIJSVQ\u00047EQTPIWFig. 1. Block diagram of the Tacotron 2 system architecture.post-net layer is comprised of 512 \ufb01lters with shape5\u21e51with batchnormalization, followed bytanhactivations on all but the \ufb01nal layer.We minimize the summed mean squared error (MSE) from beforeand after the post-net to aid convergence. We also experimentedwith a log-likelihood loss by modeling the output distribution witha Mixture Density Network [23,24] to avoid assuming a constantvariance over time, but found that these were more dif\ufb01cult to trainand they did not lead to better sounding samples.In parallel to spectrogram frame prediction, the concatenation ofdecoder LSTM output and the attention context is projected downto a scalar and passed through a sigmoid activation to predict theprobability that the output sequence has completed. This \u201cstop token\u201dprediction is used during inference to allow the model to dynamicallydetermine when to terminate generation instead of always generatingfor a \ufb01xed duration. Speci\ufb01cally, generation completes at the \ufb01rstframe for which this probability exceeds a threshold of 0.5.The convolutional layers in the network are regularized usingdropout [25] with probability 0.5, and LSTM layers are regularizedusing zoneout [26] with probability 0.1. In order to introduce outputvariation at inference time, dropout with probability 0.5 is appliedonly to layers in the pre-net of the autoregressive decoder.In contrast to the original Tacotron, our model uses simpler build-ing blocks, using vanilla LSTM and convolutional layers in the en-coder and decoder instead of \u201cCBHG\u201d stacks and GRU recurrentlayers. We do not use a \u201creduction factor\u201d, i.e., each decoder stepcorresponds to a single spectrogram frame.2.3. WaveNet VocoderWe use a modi\ufb01ed version of the WaveNet architecture from [8] toinvert the mel spectrogram feature representation into time-domainwaveform samples. As in the original architecture, there are 30dilated convolution layers, grouped into 3 dilation cycles, i.e., thedilation rate of layer k (k=0...29) is2k(mod 10). To work withthe 12.5 ms frame hop of the spectrogram frames, only 2 upsamplinglayers are used in the conditioning stack instead of 3 layers.Instead of predicting discretized buckets with a softmax layer,we follow PixelCNN++ [27] and Parallel WaveNet [28] and use a 10-component mixture of logistic distributions (MoL) to generate 16-bitsamples at 24 kHz. To compute the logistic mixture distribution, theWaveNet stack output is passed through a ReLU activation followedEncoderDecoderVocoder\nFigure 16.16 The Tacotron2 architecture: An encoder-decoder maps from graphemes to\nmel spectrograms, followed by a vocoder that maps to wave\ufb01les. Figure modi\ufb01ed from Shen\net al. (2018).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 534,
      "token_count": 788,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 16.16 The Tacotron2 architecture: An encoder-decoder maps from graphemes to\nmel spectrograms, followed by a vocoder that maps to wave\ufb01les. Figure modi\ufb01ed from Shen\net al. (2018).\nThe system is trained on gold log-mel \ufb01lterbank features, using teacher forcing,\nthat is the decoder is fed the correct log-model spectral feature at each decoder step\ninstead of the predicted decoder output from the prior step.\n16.6.3 TTS: Vocoding\nThe vocoder for Tacotron 2 is an adaptation of the WaveNet vocoder (van den Oord WaveNet\net al., 2016). Here we\u2019ll give a somewhat simpli\ufb01ed description of vocoding using\nWaveNet.\nRecall that the goal of the vocoding process here will be to invert a log mel spec-\ntrum representations back into a time-domain waveform representation. WaveNet is\nan autoregressive network, like the language models we introduced in Chapter 8. It",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 535,
      "token_count": 218,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 360",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 536,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "352 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\ntakes spectrograms as input and produces audio output represented as sequences of\n8-bit mu-law (page 336). The probability of a waveform , a sequence of 8-bit mu-\nlaw values Y=y1;:::;yt, given an intermediate input mel spectrogram his computed\nas:\np(Y) =tY\nt=1P(ytjy1;:::;yt\u00001;h1;:::;ht) (16.24)\nThis probability distribution is modeled by a stack of special convolution layers,\nwhich include a speci\ufb01c convolutional structure called dilated convolutions , and a\nspeci\ufb01c non-linearity function.\nA dilated convolution is a subtype of causal convolutional layer. Causal or\nmasked convolutions look only at the past input, rather than the future; the pre-\ndiction of yt+1can only depend on y1;:::;yt, useful for autoregressive left-to-right\nprocessing. In dilated convolutions , at each successive layer we apply the convolu-dilated\nconvolutions\ntional \ufb01lter over a span longer than its length by skipping input values. Thus at time\ntwith a dilation value of 1, a convolutional \ufb01lter of length 2 would see input values\nxtandxt\u00001. But a \ufb01lter with a distillation value of 2 would skip an input, so would\nsee input values xtandxt\u00001. Fig. 16.17 shows the computation of the output at time\ntwith 4 dilated convolution layers with dilation values, 1, 2, 4, and 8.\nBecause models with causal convolutions do not have recurrent connections, they are typically faster\nto train than RNNs, especially when applied to very long sequences. One of the problems of causal\nconvolutions is that they require many layers, or large \ufb01lters to increase the receptive \ufb01eld. For\nexample, in Fig. 2 the receptive \ufb01eld is only 5 (= #layers + \ufb01lter length - 1). In this paper we use\ndilated convolutions to increase the receptive \ufb01eld by orders of magnitude, without greatly increasing\ncomputational cost.\nA dilated convolution (also called `a trous , or convolution with holes) is a convolution where the\n\ufb01lter is applied over an area larger than its length by skipping input values with a certain step. It is\nequivalent to a convolution with a larger \ufb01lter derived from the original \ufb01lter by dilating it with zeros,\nbut is signi\ufb01cantly more ef\ufb01cient. A dilated convolution effectively allows the network to operate on\na coarser scale than with a normal convolution. This is similar to pooling or strided convolutions, but\nhere the output has the same size as the input. As a special case, dilated convolution with dilation\n1yields the standard convolution. Fig. 3 depicts dilated causal convolutions for dilations 1,2,4,\nand8. Dilated convolutions have previously been used in various contexts, e.g. signal processing\n(Holschneider et al., 1989; Dutilleux, 1989), and image segmentation (Chen et al., 2015; Yu &\nKoltun, 2016).\nInputHidden LayerDilation = 1Hidden LayerDilation = 2Hidden LayerDilation = 4OutputDilation = 8\nFigure 3: Visualization of a stack of dilated causal convolutional layers.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 537,
      "token_count": 775,
      "chapter_title": ""
    }
  },
  {
    "content": "computational cost.\nA dilated convolution (also called `a trous , or convolution with holes) is a convolution where the\n\ufb01lter is applied over an area larger than its length by skipping input values with a certain step. It is\nequivalent to a convolution with a larger \ufb01lter derived from the original \ufb01lter by dilating it with zeros,\nbut is signi\ufb01cantly more ef\ufb01cient. A dilated convolution effectively allows the network to operate on\na coarser scale than with a normal convolution. This is similar to pooling or strided convolutions, but\nhere the output has the same size as the input. As a special case, dilated convolution with dilation\n1yields the standard convolution. Fig. 3 depicts dilated causal convolutions for dilations 1,2,4,\nand8. Dilated convolutions have previously been used in various contexts, e.g. signal processing\n(Holschneider et al., 1989; Dutilleux, 1989), and image segmentation (Chen et al., 2015; Yu &\nKoltun, 2016).\nInputHidden LayerDilation = 1Hidden LayerDilation = 2Hidden LayerDilation = 4OutputDilation = 8\nFigure 3: Visualization of a stack of dilated causal convolutional layers.\nStacked dilated convolutions enable networks to have very large receptive \ufb01elds with just a few lay-\ners, while preserving the input resolution throughout the network as well as computational ef\ufb01ciency.\nIn this paper, the dilation is doubled for every layer up to a limit and then repeated: e.g.\n1,2,4,..., 512,1,2,4,..., 512,1,2,4,..., 512.\nThe intuition behind this con\ufb01guration is two-fold. First, exponentially increasing the dilation factor\nresults in exponential receptive \ufb01eld growth with depth (Yu & Koltun, 2016). For example each\n1,2,4,..., 512block has receptive \ufb01eld of size 1024 , and can be seen as a more ef\ufb01cient and dis-\ncriminative (non-linear) counterpart of a 1\u21e51024 convolution. Second, stacking these blocks further\nincreases the model capacity and the receptive \ufb01eld size.\n2.2 S OFTMAX DISTRIBUTIONS\nOne approach to modeling the conditional distributions p(xt|x1,...,x t\u00001)over the individual\naudio samples would be to use a mixture model such as a mixture density network (Bishop, 1994)\nor mixture of conditional Gaussian scale mixtures (MCGSM) (Theis & Bethge, 2015). However,\nvan den Oord et al. (2016a) showed that a softmax distribution tends to work better, even when the\ndata is implicitly continuous (as is the case for image pixel intensities or audio sample values). One\nof the reasons is that a categorical distribution is more \ufb02exible and can more easily model arbitrary\ndistributions because it makes no assumptions about their shape.\nBecause raw audio is typically stored as a sequence of 16-bit integer values (one per timestep), a\nsoftmax layer would need to output 65,536 probabilities per timestep to model all possible values.\nTo make this more tractable, we \ufb01rst apply a \u00b5-law companding transformation (ITU-T, 1988) to\nthe data, and then quantize it to 256 possible values:\nf(xt) = sign( xt)ln (1 + \u00b5|xt|)\nln (1 + \u00b5),\n3",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 538,
      "token_count": 769,
      "chapter_title": ""
    }
  },
  {
    "content": "increases the model capacity and the receptive \ufb01eld size.\n2.2 S OFTMAX DISTRIBUTIONS\nOne approach to modeling the conditional distributions p(xt|x1,...,x t\u00001)over the individual\naudio samples would be to use a mixture model such as a mixture density network (Bishop, 1994)\nor mixture of conditional Gaussian scale mixtures (MCGSM) (Theis & Bethge, 2015). However,\nvan den Oord et al. (2016a) showed that a softmax distribution tends to work better, even when the\ndata is implicitly continuous (as is the case for image pixel intensities or audio sample values). One\nof the reasons is that a categorical distribution is more \ufb02exible and can more easily model arbitrary\ndistributions because it makes no assumptions about their shape.\nBecause raw audio is typically stored as a sequence of 16-bit integer values (one per timestep), a\nsoftmax layer would need to output 65,536 probabilities per timestep to model all possible values.\nTo make this more tractable, we \ufb01rst apply a \u00b5-law companding transformation (ITU-T, 1988) to\nthe data, and then quantize it to 256 possible values:\nf(xt) = sign( xt)ln (1 + \u00b5|xt|)\nln (1 + \u00b5),\n3\nFigure 16.17 Dilated convolutions, showing one dilation cycle size of 4, i.e., dilation values\nof 1, 2, 4, 8. Figure from van den Oord et al. (2016).\nThe Tacotron 2 synthesizer uses 12 convolutional layers in two cycles with a\ndilation cycle size of 6, meaning that the \ufb01rst 6 layers have dilations of 1, 2, 4, 8, 16,\nand 32. and the next 6 layers again have dilations of 1, 2, 4, 8, 16, and 32. Dilated\nconvolutions allow the vocoder to grow the receptive \ufb01eld exponentially with depth.\nWaveNet predicts mu-law audio samples. Recall from page 336 that this is a\nstandard compression for audio in which the values at each sampling timestep are\ncompressed into 8-bits. This means that we can predict the value of each sample\nwith a simple 256-way categorical classi\ufb01er. The output of the dilated convolutions\nis thus passed through a softmax which makes this 256-way decision.\nThe spectrogram prediction encoder-decoder and the WaveNet vocoder are trained\nseparately. After the spectrogram predictor is trained, the spectrogram prediction\nnetwork is run in teacher-forcing mode, with each predicted spectral frame condi-\ntioned on the encoded text input and the previous frame from the ground truth spec-\ntrogram. This sequence of ground truth-aligned spectral features and gold audio\noutput is then used to train the vocoder.\nThis has been only a high-level sketch of the TTS process. There are numer-\nous important details that the reader interested in going further with TTS may want",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 539,
      "token_count": 654,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 361\n\n16.7 \u2022 O THER SPEECH TASKS 353\nto look into. For example WaveNet uses a special kind of a gated activation func-\ntion as its non-linearity, and contains residual and skip connections. In practice,\npredicting 8-bit audio values doesn\u2019t as work as well as 16-bit, for which a simple\nsoftmax is insuf\ufb01cient, so decoders use fancier ways as the last step of predicting\naudio sample values, like mixtures of distributions. Finally, the WaveNet vocoder\nas we have described it would be so slow as to be useless; many different kinds of\nef\ufb01ciency improvements are necessary in practice, for example by \ufb01nding ways to\ndo non-autoregressive generation, avoiding the latency of having to wait to generate\neach frame until the prior frame has been generated, and instead making predictions\nin parallel. We encourage the interested reader to consult the original papers and\nvarious version of the code.\n16.6.4 TTS Evaluation\nSpeech synthesis systems are evaluated by human listeners. (The development of a\ngood automatic metric for synthesis evaluation, one that would eliminate the need\nfor expensive and time-consuming human listening experiments, remains an open\nand exciting research topic.)\nWe evaluate the quality of synthesized utterances by playing a sentence to lis-\nteners and ask them to give a mean opinion score (MOS ), a rating of how good MOS\nthe synthesized utterances are, usually on a scale from 1\u20135. We can then compare\nsystems by comparing their MOS scores on the same sentences (using, e.g., paired\nt-tests to test for signi\ufb01cant differences).\nIf we are comparing exactly two systems (perhaps to see if a particular change\nactually improved the system), we can use AB tests . In AB tests, we play the same AB tests\nsentence synthesized by two different systems (an A and a B system). The human\nlisteners choose which of the two utterances they like better. We do this for say\n50 sentences (presented in random order) and compare the number of sentences\npreferred for each system.\n16.7 Other Speech Tasks\nWhile we have focused on speech recognition and TTS in this chapter, there are a\nwide variety of speech-related tasks.\nThe task of wake word detection is to detect a word or short phrase, usually in wake word\norder to wake up a voice-enable assistant like Alexa, Siri, or the Google Assistant.\nThe goal with wake words is build the detection into small devices at the computing\nedge, to maintain privacy by transmitting the least amount of user speech to a cloud-\nbased server. Thus wake word detectors need to be fast, small footprint software that\ncan \ufb01t into embedded devices. Wake word detectors usually use the same frontend\nfeature extraction we saw for ASR, often followed by a whole-word classi\ufb01er.\nSpeaker diarization is the task of determining \u2018who spoke when\u2019 in a longspeaker\ndiarization\nmulti-speaker audio recording, marking the start and end of each speaker\u2019s turns in\nthe interaction. This can be useful for transcribing meetings, classroom speech, or\nmedical interactions. Often diarization systems use voice activity detection (V AD) to\n\ufb01nd segments of continuous speech, extract speaker embedding vectors, and cluster\nthe vectors to group together segments likely from the same speaker. More recent\nwork is investigating end-to-end algorithms to map directly from input speech to a\nsequence of speaker labels for each frame.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 540,
      "token_count": 737,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 362\n\n354 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\nSpeaker recognition , is the task of identifying a speaker. We generally distin-speaker\nrecognition\nguish the subtasks of speaker veri\ufb01cation , where we make a binary decision (is\nthis speaker Xor not?), such as for security when accessing personal information\nover the telephone, and speaker identi\ufb01cation , where we make a one of Ndecision\ntrying to match a speaker\u2019s voice against a database of many speakers . These tasks\nare related to language identi\ufb01cation , in which we are given a wave\ufb01le and mustlanguage\nidenti\ufb01cation\nidentify which language is being spoken; this is useful for example for automatically\ndirecting callers to human operators that speak appropriate languages.\n16.8 Summary\nThis chapter introduced the fundamental algorithms of automatic speech recognition\n(ASR) and text-to-speech (TTS).\n\u2022 The task of speech recognition (or speech-to-text) is to map acoustic wave-\nforms to sequences of graphemes.\n\u2022 The input to a speech recognizer is a series of acoustic waves. that are sam-\npled,quantized , and converted to a spectral representation like the log mel\nspectrum .\n\u2022 Two common paradigms for speech recognition are the encoder-decoder with\nattention model, and models based on the CTC loss function . Attention-\nbased models have higher accuracies, but models based on CTC more easily\nadapt to streaming : outputting graphemes online instead of waiting until the\nacoustic input is complete.\n\u2022 ASR is evaluated using the Word Error Rate; the edit distance between the\nhypothesis and the gold transcription.\n\u2022TTS systems are also based on the encoder-decoder architecture. The en-\ncoder maps letters to an encoding, which is consumed by the decoder which\ngenerates mel spectrogram output. A neural vocoder then reads the spectro-\ngram and generates waveforms.\n\u2022 TTS systems require a \ufb01rst pass of text normalization to deal with numbers\nand abbreviations and other non-standard words.\n\u2022 TTS is evaluated by playing a sentence to human listeners and having them\ngive a mean opinion score (MOS) or by doing AB tests.\nBibliographical and Historical Notes\nASR A number of speech recognition systems were developed by the late 1940s\nand early 1950s. An early Bell Labs system could recognize any of the 10 digits\nfrom a single speaker (Davis et al., 1952). This system had 10 speaker-dependent\nstored patterns, one for each digit, each of which roughly represented the \ufb01rst two\nvowel formants in the digit. They achieved 97%\u201399% accuracy by choosing the pat-\ntern that had the highest relative correlation coef\ufb01cient with the input. Fry (1959)\nand Denes (1959) built a phoneme recognizer at University College, London, that\nrecognized four vowels and nine consonants based on a similar pattern-recognition\nprinciple. Fry and Denes\u2019s system was the \ufb01rst to use phoneme transition probabili-\nties to constrain the recognizer.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 541,
      "token_count": 667,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 363",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 542,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 355\nThe late 1960s and early 1970s produced a number of important paradigm shifts.\nFirst were a number of feature-extraction algorithms, including the ef\ufb01cient fast\nFourier transform (FFT) (Cooley and Tukey, 1965), the application of cepstral pro-\ncessing to speech (Oppenheim et al., 1968), and the development of LPC for speech\ncoding (Atal and Hanauer, 1971). Second were a number of ways of handling warp-\ning; stretching or shrinking the input signal to handle differences in speaking rate warping\nand segment length when matching against stored patterns. The natural algorithm for\nsolving this problem was dynamic programming, and, as we saw in Appendix A, the\nalgorithm was reinvented multiple times to address this problem. The \ufb01rst applica-\ntion to speech processing was by Vintsyuk (1968), although his result was not picked\nup by other researchers, and was reinvented by Velichko and Zagoruyko (1970) and\nSakoe and Chiba (1971) (and 1984). Soon afterward, Itakura (1975) combined this\ndynamic programming idea with the LPC coef\ufb01cients that had previously been used\nonly for speech coding. The resulting system extracted LPC features from incoming\nwords and used dynamic programming to match them against stored LPC templates.\nThe non-probabilistic use of dynamic programming to match a template against in-\ncoming speech is called dynamic time warping .dynamic time\nwarping\nThe third innovation of this period was the rise of the HMM. Hidden Markov\nmodels seem to have been applied to speech independently at two laboratories around\n1972. One application arose from the work of statisticians, in particular Baum and\ncolleagues at the Institute for Defense Analyses in Princeton who applied HMMs\nto various prediction problems (Baum and Petrie 1966, Baum and Eagon 1967).\nJames Baker learned of this work and applied the algorithm to speech processing\n(Baker, 1975a) during his graduate work at CMU. Independently, Frederick Jelinek\nand collaborators (drawing from their research in information-theoretical models\nin\ufb02uenced by the work of Shannon (1948)) applied HMMs to speech at the IBM\nThomas J. Watson Research Center (Jelinek et al., 1975). One early difference was\nthe decoding algorithm; Baker\u2019s DRAGON system used Viterbi (dynamic program-\nming) decoding, while the IBM system applied Jelinek\u2019s stack decoding algorithm\n(Jelinek, 1969). Baker then joined the IBM group for a brief time before founding\nthe speech-recognition company Dragon Systems.\nThe use of the HMM, with Gaussian Mixture Models (GMMs) as the phonetic\ncomponent, slowly spread through the speech community, becoming the dominant\nparadigm by the 1990s. One cause was encouragement by ARPA, the Advanced\nResearch Projects Agency of the U.S. Department of Defense. ARPA started a\n\ufb01ve-year program in 1971 to build 1000-word, constrained grammar, few speaker\nspeech understanding (Klatt, 1977), and funded four competing systems of which\nCarnegie-Mellon University\u2019s Harpy system (Lowerre, 1976), which used a simpli-\n\ufb01ed version of Baker\u2019s HMM-based DRAGON system was the best of the tested sys-\ntems. ARPA (and then DARPA) funded a number of new speech research programs,\nbeginning with 1000-word speaker-independent read-speech tasks like \u201cResource",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 543,
      "token_count": 786,
      "chapter_title": ""
    }
  },
  {
    "content": "Thomas J. Watson Research Center (Jelinek et al., 1975). One early difference was\nthe decoding algorithm; Baker\u2019s DRAGON system used Viterbi (dynamic program-\nming) decoding, while the IBM system applied Jelinek\u2019s stack decoding algorithm\n(Jelinek, 1969). Baker then joined the IBM group for a brief time before founding\nthe speech-recognition company Dragon Systems.\nThe use of the HMM, with Gaussian Mixture Models (GMMs) as the phonetic\ncomponent, slowly spread through the speech community, becoming the dominant\nparadigm by the 1990s. One cause was encouragement by ARPA, the Advanced\nResearch Projects Agency of the U.S. Department of Defense. ARPA started a\n\ufb01ve-year program in 1971 to build 1000-word, constrained grammar, few speaker\nspeech understanding (Klatt, 1977), and funded four competing systems of which\nCarnegie-Mellon University\u2019s Harpy system (Lowerre, 1976), which used a simpli-\n\ufb01ed version of Baker\u2019s HMM-based DRAGON system was the best of the tested sys-\ntems. ARPA (and then DARPA) funded a number of new speech research programs,\nbeginning with 1000-word speaker-independent read-speech tasks like \u201cResource\nManagement\u201d (Price et al., 1988), recognition of sentences read from the Wall Street\nJournal (WSJ), Broadcast News domain (LDC 1998, Graff 1997) (transcription of\nactual news broadcasts, including quite dif\ufb01cult passages such as on-the-street inter-\nviews) and the Switchboard, CallHome, CallFriend, and Fisher domains (Godfrey\net al. 1992, Cieri et al. 2004) (natural telephone conversations between friends or\nstrangers). Each of the ARPA tasks involved an approximately annual bakeoff at bakeoff\nwhich systems were evaluated against each other. The ARPA competitions resulted\nin wide-scale borrowing of techniques among labs since it was easy to see which\nideas reduced errors the previous year, and the competitions were probably an im-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 544,
      "token_count": 454,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 364",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 545,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "356 CHAPTER 16 \u2022 A UTOMATIC SPEECH RECOGNITION AND TEXT-TO-SPEECH\nportant factor in the eventual spread of the HMM paradigm.\nBy around 1990 neural alternatives to the HMM/GMM architecture for ASR\narose, based on a number of earlier experiments with neural networks for phoneme\nrecognition and other speech tasks. Architectures included the time-delay neural\nnetwork ( TDNN )\u2014the \ufb01rst use of convolutional networks for speech\u2014 (Waibel\net al. 1989, Lang et al. 1990), RNNs (Robinson and Fallside, 1991), and the hybrid hybrid\nHMM/MLP architecture in which a feedforward neural network is trained as a pho-\nnetic classi\ufb01er whose outputs are used as probability estimates for an HMM-based\narchitecture (Morgan and Bourlard 1990, Bourlard and Morgan 1994, Morgan and\nBourlard 1995).\nWhile the hybrid systems showed performance close to the standard HMM/GMM\nmodels, the problem was speed: large hybrid models were too slow to train on the\nCPUs of that era. For example, the largest hybrid system, a feedforward network,\nwas limited to a hidden layer of 4000 units, producing probabilities over only a few\ndozen monophones. Yet training this model still required the research group to de-\nsign special hardware boards to do vector processing (Morgan and Bourlard, 1995).\nA later analytic study showed the performance of such simple feedforward MLPs\nfor ASR increases sharply with more than 1 hidden layer, even controlling for the\ntotal number of parameters (Maas et al., 2017). But the computational resources of\nthe time were insuf\ufb01cient for more layers.\nOver the next two decades a combination of Moore\u2019s law and the rise of GPUs\nallowed deep neural networks with many layers. Performance was getting close to\ntraditional systems on smaller tasks like TIMIT phone recognition by 2009 (Mo-\nhamed et al., 2009), and by 2012, the performance of hybrid systems had surpassed\ntraditional HMM/GMM systems (Jaitly et al. 2012, Dahl et al. 2012, inter alia).\nOriginally it seemed that unsupervised pretraining of the networks using a tech-\nnique like deep belief networks was important, but by 2013, it was clear that for\nhybrid HMM/GMM feedforward networks, all that mattered was to use a lot of data\nand enough layers, although a few other components did improve performance: us-\ning log mel features instead of MFCCs, using dropout, and using recti\ufb01ed linear\nunits (Deng et al. 2013, Maas et al. 2013, Dahl et al. 2013).\nMeanwhile early work had proposed the CTC loss function by 2006 (Graves\net al., 2006), and by 2012 the RNN-Transducer was de\ufb01ned and applied to phone\nrecognition (Graves 2012, Graves et al. 2013), and then to end-to-end speech recog-\nnition rescoring (Graves and Jaitly, 2014), and then recognition (Maas et al., 2015),\nwith advances such as specialized beam search (Hannun et al., 2014). (Our de-\nscription of CTC in the chapter draws on Hannun (2017), which we encourage the\ninterested reader to follow).\nThe encoder-decoder architecture was applied to speech at about the same time",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 546,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "Originally it seemed that unsupervised pretraining of the networks using a tech-\nnique like deep belief networks was important, but by 2013, it was clear that for\nhybrid HMM/GMM feedforward networks, all that mattered was to use a lot of data\nand enough layers, although a few other components did improve performance: us-\ning log mel features instead of MFCCs, using dropout, and using recti\ufb01ed linear\nunits (Deng et al. 2013, Maas et al. 2013, Dahl et al. 2013).\nMeanwhile early work had proposed the CTC loss function by 2006 (Graves\net al., 2006), and by 2012 the RNN-Transducer was de\ufb01ned and applied to phone\nrecognition (Graves 2012, Graves et al. 2013), and then to end-to-end speech recog-\nnition rescoring (Graves and Jaitly, 2014), and then recognition (Maas et al., 2015),\nwith advances such as specialized beam search (Hannun et al., 2014). (Our de-\nscription of CTC in the chapter draws on Hannun (2017), which we encourage the\ninterested reader to follow).\nThe encoder-decoder architecture was applied to speech at about the same time\nby two different groups, in the Listen Attend and Spell system of Chan et al. (2016)\nand the attention-based encoder decoder architecture of Chorowski et al. (2014)\nand Bahdanau et al. (2016). By 2018 Transformers were included in this encoder-\ndecoder architecture. Karita et al. (2019) is a nice comparison of RNNs vs Trans-\nformers in encoder-architectures for ASR, TTS, and speech-to-speech translation.\nPopular toolkits for speech processing include Kaldi (Povey et al., 2011) and Kaldi\nESPnet (Watanabe et al. 2018, Hayashi et al. 2020). ESPnet\nTTS As we noted at the beginning of the chapter, speech synthesis is one of the\nearliest \ufb01elds of speech and language processing. The 18th century saw a number\nof physical models of the articulation process, including the von Kempelen model\nmentioned above, as well as the 1773 vowel model of Kratzenstein in Copenhagen",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 547,
      "token_count": 517,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 365\n\nEXERCISES 357\nusing organ pipes.\nThe early 1950s saw the development of three early paradigms of waveform\nsynthesis: formant synthesis, articulatory synthesis, and concatenative synthesis.\nModern encoder-decoder systems are distant descendants of formant synthesiz-\ners. Formant synthesizers originally were inspired by attempts to mimic human\nspeech by generating arti\ufb01cial spectrograms. The Haskins Laboratories Pattern\nPlayback Machine generated a sound wave by painting spectrogram patterns on a\nmoving transparent belt and using re\ufb02ectance to \ufb01lter the harmonics of a wave-\nform (Cooper et al., 1951); other very early formant synthesizers include those of\nLawrence (1953) and Fant (1951). Perhaps the most well-known of the formant\nsynthesizers were the Klatt formant synthesizer and its successor systems, includ-\ning the MITalk system (Allen et al., 1987) and the Klattalk software used in Digital\nEquipment Corporation\u2019s DECtalk (Klatt, 1982). See Klatt (1975) for details.\nA second early paradigm, concatenative synthesis, seems to have been \ufb01rst pro-\nposed by Harris (1953) at Bell Laboratories; he literally spliced together pieces of\nmagnetic tape corresponding to phones. Soon afterwards, Peterson et al. (1958) pro-\nposed a theoretical model based on diphones, including a database with multiple\ncopies of each diphone with differing prosody, each labeled with prosodic features\nincluding F0, stress, and duration, and the use of join costs based on F0 and formant\ndistance between neighboring units. But such diphone synthesis models were not\nactually implemented until decades later (Dixon and Maxey 1968, Olive 1977). The\n1980s and 1990s saw the invention of unit selection synthesis , based on larger units\nof non-uniform length and the use of a target cost, (Sagisaka 1988, Sagisaka et al.\n1992, Hunt and Black 1996, Black and Taylor 1994, Syrdal et al. 2000).\nA third paradigm, articulatory synthesizers attempt to synthesize speech by\nmodeling the physics of the vocal tract as an open tube. Representative models\ninclude Stevens et al. (1953), Flanagan et al. (1975), and Fant (1986). See Klatt\n(1975) and Flanagan (1972) for more details.\nMost early TTS systems used phonemes as input; development of the text anal-\nysis components of TTS came somewhat later, drawing on NLP. Indeed the \ufb01rst\ntrue text-to-speech system seems to have been the system of Umeda and Teranishi\n(Umeda et al. 1968, Teranishi and Umeda 1968, Umeda 1976), which included a\nparser that assigned prosodic boundaries, as well as accent and stress.\nExercises\n16.1 Analyze each of the errors in the incorrectly recognized transcription of \u201cum\nthe phone is I left the. . . \u201d on page 346. For each one, give your best guess as\nto whether you think it is caused by a problem in signal processing, pronun-\nciation modeling, lexicon size, language model, or pruning in the decoding\nsearch.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 548,
      "token_count": 729,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 367\n\nPart III\nANNOTATING LINGUISTIC\nSTRUCTURE\nIn the \ufb01nal part of the book we discuss the task of detecting linguistic structure.\nIn the early history of NLP these structures were an intermediate step toward deeper\nlanguage processing. In modern NLP, we don\u2019t generally make explicit use of parse\nor other structures inside the neural language models we introduced in Part I, or\ndirectly in applications like those we discussed in Part II.\nInstead linguistic structure plays a number of new roles. One important role is for\ninterpretability : to provide a useful interpretive lens on neural networks. Knowing\nthat a particular layer or neuron may be computing something related to a particular\nkind of structure can help us break open the \u2018black box\u2019 and understand what the\ncomponents of our language models are doing.\nA second important role for linguistic structure is as a practical tool for social\nscienti\ufb01c studies of text: knowing which adjective modi\ufb01es which noun, or whether\na particular implicit metaphor is being used, can be important for measuring attitudes\ntoward groups or individuals. Detailed semantic structure can be helpful, for exam-\nple in \ufb01nding particular clauses that have particular meanings in legal contracts.\nWord sense labels can help keep any corpus study from measuring facts about the\nwrong word sense. Relation structures can be used to help build knowledge bases\nfrom text.\nFinally, computation of linguistic structure is an important tool for answering\nquestions about language itself, a research area called computational linguistics\nthat is sometimes distinguished from natural language processing. To answer lin-\nguistic questions about how language changes over time or across individuals we\u2019ll\nneed to be able, for example, to parse entire documents from different time periods.\nTo understand how certain linguistic structures are learned or processed by people,\nit\u2019s necessary to be able to automatically label structures for arbitrary text.\nIn our study of linguistic structure, we begin with one of the oldest tasks in\ncomputational linguistics: the extraction of syntactic structure, and give two sets of\nalgorithms for parsing : extracting syntactic structure, including constituency pars-\ning and dependency parsing. We then introduce a variety of structures related to\nmeaning, including semantic roles, word senses, entity relations, and events. We\n\n## Page 368\n\n360\nconclude with linguistic structures that tend to be related to discourse and meaning\nover larger texts, including coreference and discourse coherence. In each case we\u2019ll\ngive algorithms for automatically annotating the relevant structure.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 549,
      "token_count": 518,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 368\n\n360\nconclude with linguistic structures that tend to be related to discourse and meaning\nover larger texts, including coreference and discourse coherence. In each case we\u2019ll\ngive algorithms for automatically annotating the relevant structure.\n\n## Page 370\n\n362 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\nCHAPTER\n17Sequence Labeling for Parts of\nSpeech and Named Entities\nTo each word a warbling note\nA Midsummer Night\u2019s Dream , V .I\nDionysius Thrax of Alexandria ( c.100 B.C.), or perhaps someone else (it was a long\ntime ago), wrote a grammatical sketch of Greek (a \u201c techn \u00afe\u201d) that summarized the\nlinguistic knowledge of his day. This work is the source of an astonishing proportion\nof modern linguistic vocabulary, including the words syntax ,diphthong ,clitic , and\nanalogy . Also included are a description of eight parts of speech : noun, verb, parts of speech\npronoun, preposition, adverb, conjunction, participle, and article. Although earlier\nscholars (including Aristotle as well as the Stoics) had their own lists of parts of\nspeech, it was Thrax\u2019s set of eight that became the basis for descriptions of European\nlanguages for the next 2000 years. (All the way to the Schoolhouse Rock educational\ntelevision shows of our childhood, which had songs about 8 parts of speech, like the\nlate great Bob Dorough\u2019s Conjunction Junction .) The durability of parts of speech\nthrough two millennia speaks to their centrality in models of human language.\nProper names are another important and anciently studied linguistic category.\nWhile parts of speech are generally assigned to individual words or morphemes, a\nproper name is often an entire multiword phrase, like the name \u201cMarie Curie\u201d, the\nlocation \u201cNew York City\u201d, or the organization \u201cStanford University\u201d. We\u2019ll use the\nterm named entity for, roughly speaking, anything that can be referred to with a named entity\nproper name: a person, a location, an organization, although as we\u2019ll see the term is\ncommonly extended to include things that aren\u2019t entities per se.\nParts of speech (also known as POS ) and named entities are useful clues to POS\nsentence structure and meaning. Knowing whether a word is a noun or a verb tells us\nabout likely neighboring words (nouns in English are preceded by determiners and\nadjectives, verbs by nouns) and syntactic structure (verbs have dependency links to\nnouns), making part-of-speech tagging a key aspect of parsing. Knowing if a named\nentity like Washington is a name of a person, a place, or a university is important to\nmany natural language processing tasks like question answering, stance detection,\nor information extraction.\nIn this chapter we\u2019ll introduce the task of part-of-speech tagging , taking a se-\nquence of words and assigning each word a part of speech like NOUN orVERB , and\nthe task of named entity recognition (NER ), assigning words or phrases tags like\nPERSON ,LOCATION , or ORGANIZATION .\nSuch tasks in which we assign, to each word xiin an input word sequence, a\nlabel yi, so that the output sequence Yhas the same length as the input sequence X\nare called sequence labeling tasks. We\u2019ll introduce classic sequence labeling algo-sequence\nlabeling\nrithms, one generative\u2014 the Hidden Markov Model (HMM)\u2014and one discriminative\u2014\nthe Conditional Random Field (CRF). In following chapters we\u2019ll introduce modern\nsequence labelers based on RNNs and Transformers.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 550,
      "token_count": 767,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 371",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 551,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "17.1 \u2022 (M OSTLY ) ENGLISH WORD CLASSES 363\n17.1 (Mostly) English Word Classes\nUntil now we have been using part-of-speech terms like noun andverb rather freely.\nIn this section we give more complete de\ufb01nitions. While word classes do have\nsemantic tendencies\u2014adjectives, for example, often describe properties and nouns\npeople \u2014 parts of speech are de\ufb01ned instead based on their grammatical relationship\nwith neighboring words or the morphological properties about their af\ufb01xes.\nTag Description ExampleOpen ClassADJ Adjective: noun modi\ufb01ers describing properties red,young ,awesome\nADV Adverb: verb modi\ufb01ers of time, place, manner very,slowly ,home ,yesterday\nNOUN words for persons, places, things, etc. algorithm ,cat,mango ,beauty\nVERB words for actions and processes draw ,provide ,go\nPROPN Proper noun: name of a person, organization, place, etc.. Regina ,IBM,Colorado\nINTJ Interjection: exclamation, greeting, yes/no response, etc. oh,um,yes,helloClosed Class WordsADP Adposition (Preposition/Postposition): marks a noun\u2019s\nspacial, temporal, or other relationin, on, by, under\nAUX Auxiliary: helping verb marking tense, aspect, mood, etc., can, may, should, are\nCCONJ Coordinating Conjunction: joins two phrases/clauses and,or,but\nDET Determiner: marks noun phrase properties a, an, the, this\nNUM Numeral one, two, 2026, 11:00, hundred\nPART Particle: a function word that must be associated with an-\nother word\u2019s, not, (in\ufb01nitive) to\nPRON Pronoun: a shorthand for referring to an entity or event she, who, I, others\nSCONJ Subordinating Conjunction: joins a main clause with a\nsubordinate clause such as a sentential complementwhether ,becauseOtherPUNCT Punctuation\u02d9, , ()\nSYM Symbols like $ or emoji $, %\nX Other asdf, qwfg\nFigure 17.1 The 17 parts of speech in the Universal Dependencies tagset (de Marneffe et al., 2021). Features\ncan be added to make \ufb01ner-grained distinctions (with properties like number, case, de\ufb01niteness, and so on).\nParts of speech fall into two broad categories: closed class andopen class . closed class\nopen class Closed classes are those with relatively \ufb01xed membership, such as prepositions\u2014\nnew prepositions are rarely coined. By contrast, nouns and verbs are open classes\u2014\nnew nouns and verbs like iPhone orto fax are continually being created or borrowed.\nClosed class words are generally function words likeof,it,and, oryou, which tend function word\nto be very short, occur frequently, and often have structuring uses in grammar.\nFour major open classes occur in the languages of the world: nouns (including\nproper nouns), verbs ,adjectives , and adverbs , as well as the smaller open class of\ninterjections . English has all \ufb01ve, although not every language does.\nNouns are words for people, places, or things, but include others as well. Com- noun\nmon nouns include concrete terms like catandmango , abstractions like algorithm common noun\nandbeauty , and verb-like terms like pacing as in His pacing to and fro became quite\nannoying . Nouns in English can occur with determiners ( a goat, this bandwidth )\ntake possessives ( IBM\u2019s annual revenue ), and may occur in the plural ( goats, abaci ).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 552,
      "token_count": 782,
      "chapter_title": ""
    }
  },
  {
    "content": "can be added to make \ufb01ner-grained distinctions (with properties like number, case, de\ufb01niteness, and so on).\nParts of speech fall into two broad categories: closed class andopen class . closed class\nopen class Closed classes are those with relatively \ufb01xed membership, such as prepositions\u2014\nnew prepositions are rarely coined. By contrast, nouns and verbs are open classes\u2014\nnew nouns and verbs like iPhone orto fax are continually being created or borrowed.\nClosed class words are generally function words likeof,it,and, oryou, which tend function word\nto be very short, occur frequently, and often have structuring uses in grammar.\nFour major open classes occur in the languages of the world: nouns (including\nproper nouns), verbs ,adjectives , and adverbs , as well as the smaller open class of\ninterjections . English has all \ufb01ve, although not every language does.\nNouns are words for people, places, or things, but include others as well. Com- noun\nmon nouns include concrete terms like catandmango , abstractions like algorithm common noun\nandbeauty , and verb-like terms like pacing as in His pacing to and fro became quite\nannoying . Nouns in English can occur with determiners ( a goat, this bandwidth )\ntake possessives ( IBM\u2019s annual revenue ), and may occur in the plural ( goats, abaci ).\nMany languages, including English, divide common nouns into count nouns and count noun\nmass nouns . Count nouns can occur in the singular and plural ( goat/goats, rela- mass noun\ntionship/relationships ) and can be counted ( one goat, two goats ). Mass nouns are\nused when something is conceptualized as a homogeneous group. So snow, salt , and\ncommunism are not counted (i.e., *two snows or*two communisms ).Proper nouns , proper noun\nlikeRegina ,Colorado , and IBM, are names of speci\ufb01c persons or entities.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 553,
      "token_count": 418,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 372",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 554,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "364 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\nVerbs refer to actions and processes, including main verbs like draw ,provide , verb\nandgo. English verbs have in\ufb02ections (non-third-person-singular ( eat), third-person-\nsingular ( eats), progressive ( eating ), past participle ( eaten )). While many scholars\nbelieve that all human languages have the categories of noun and verb, others have\nargued that some languages, such as Riau Indonesian and Tongan, don\u2019t even make\nthis distinction (Broschart 1997; Evans 2000; Gil 2000) .\nAdjectives often describe properties or qualities of nouns, like color ( white , adjective\nblack ), age ( old,young ), and value ( good ,bad), but there are languages without\nadjectives. In Korean, for example, the words corresponding to English adjectives\nact as a subclass of verbs, so what is in English an adjective \u201cbeautiful\u201d acts in\nKorean like a verb meaning \u201cto be beautiful\u201d.\nAdverbs are a hodge-podge. All the italicized words in this example are adverbs: adverb\nActually , I ran home extremely quickly yesterday\nAdverbs generally modify something (often verbs, hence the name \u201cadverb\u201d, but\nalso other adverbs and entire verb phrases). Directional adverbs orlocative ad- locative\nverbs (home ,here,downhill ) specify the direction or location of some action; degree degree\nadverbs (extremely ,very,somewhat ) specify the extent of some action, process, or\nproperty; manner adverbs (slowly ,slinkily ,delicately ) describe the manner of some manner\naction or process; and temporal adverbs describe the time that some action or event temporal\ntook place ( yesterday ,Monday ).\nInterjections (oh, hey, alas, uh, um ) are a smaller open class that also includes interjection\ngreetings ( hello, goodbye ) and question responses ( yes, no, uh-huh ).\nEnglish adpositions occur before nouns, hence are called prepositions . They can preposition\nindicate spatial or temporal relations, whether literal ( on it,before then ,by the house )\nor metaphorical ( on time ,with gusto ,beside herself ), and relations like marking the\nagent in Hamlet was written by Shakespeare .\nAparticle resembles a preposition or an adverb and is used in combination with particle\na verb. Particles often have extended meanings that aren\u2019t quite the same as the\nprepositions they resemble, as in the particle over inshe turned the paper over . A\nverb and a particle acting as a single unit is called a phrasal verb . The meaning phrasal verb\nof phrasal verbs is often non-compositional \u2014not predictable from the individual\nmeanings of the verb and the particle. Thus, turn down means \u2018reject\u2019, rule out\n\u2018eliminate\u2019, and go on \u2018continue\u2019.\nDeterminers likethisandthat(this chapter ,that page ) can mark the start of an determiner\nEnglish noun phrase. Articles likea,an, and the, are a type of determiner that mark article\ndiscourse properties of the noun and are quite frequent; theis the most common\nword in written English, with aandanright behind.\nConjunctions join two phrases, clauses, or sentences. Coordinating conjunc- conjunction\ntions like and,or, and butjoin two elements of equal status. Subordinating conjunc-\ntions are used when one of the elements has some embedded status. For example,\nthe subordinating conjunction thatin\u201cI thought that you might like some milk\u201d links",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 555,
      "token_count": 780,
      "chapter_title": ""
    }
  },
  {
    "content": "agent in Hamlet was written by Shakespeare .\nAparticle resembles a preposition or an adverb and is used in combination with particle\na verb. Particles often have extended meanings that aren\u2019t quite the same as the\nprepositions they resemble, as in the particle over inshe turned the paper over . A\nverb and a particle acting as a single unit is called a phrasal verb . The meaning phrasal verb\nof phrasal verbs is often non-compositional \u2014not predictable from the individual\nmeanings of the verb and the particle. Thus, turn down means \u2018reject\u2019, rule out\n\u2018eliminate\u2019, and go on \u2018continue\u2019.\nDeterminers likethisandthat(this chapter ,that page ) can mark the start of an determiner\nEnglish noun phrase. Articles likea,an, and the, are a type of determiner that mark article\ndiscourse properties of the noun and are quite frequent; theis the most common\nword in written English, with aandanright behind.\nConjunctions join two phrases, clauses, or sentences. Coordinating conjunc- conjunction\ntions like and,or, and butjoin two elements of equal status. Subordinating conjunc-\ntions are used when one of the elements has some embedded status. For example,\nthe subordinating conjunction thatin\u201cI thought that you might like some milk\u201d links\nthe main clause I thought with the subordinate clause you might like some milk . This\nclause is called subordinate because this entire clause is the \u201ccontent\u201d of the main\nverb thought . Subordinating conjunctions like thatwhich link a verb to its argument\nin this way are also called complementizers . complementizer\nPronouns act as a shorthand for referring to an entity or event. Personal pro- pronoun\nnouns refer to persons or entities ( you,she,I,it,me, etc.). Possessive pronouns are\nforms of personal pronouns that indicate either actual possession or more often just\nan abstract relation between the person and some object ( my, your, his, her, its, one\u2019s,\nour, their ).Wh-pronouns (what, who, whom, whoever ) are used in certain question wh",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 556,
      "token_count": 458,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 373",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 557,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "17.2 \u2022 P ART-OF-SPEECH TAGGING 365\nforms, or act as complementizers ( Frida, who married Diego. . . ).\nAuxiliary verbs mark semantic features of a main verb such as its tense, whether auxiliary\nit is completed (aspect), whether it is negated (polarity), and whether an action is\nnecessary, possible, suggested, or desired (mood). English auxiliaries include the\ncopula verb be, the two verbs doandhave , forms, as well as modal verbs used to copula\nmodal mark the mood associated with the event depicted by the main verb: canindicates\nability or possibility, may permission or possibility, must necessity.\nAn English-speci\ufb01c tagset, the Penn Treebank tagset (Marcus et al., 1993), shown\nin Fig. 17.2, has been used to label many syntactically annotated corpora like the\nPenn Treebank corpora, so it is worth knowing about.\nTag Description Example Tag Description Example Tag Description Example\nCC coord. conj. and, but, or NNP proper noun, sing. IBM TO in\ufb01nitive to to\nCD cardinal number one, two NNPS proper noun, plu. Carolinas UH interjection ah, oops\nDT determiner a, the NNS noun, plural llamas VB verb base eat\nEX existential \u2018there\u2019 there PDT predeterminer all, both VBD verb past tense ate\nFW foreign word mea culpa POS possessive ending \u2019s VBG verb gerund eating\nIN preposition/\nsubordin-conjof, in, by PRP personal pronoun I, you, he VBN verb past partici-\npleeaten\nJJ adjective yellow PRP$ possess. pronoun your VBP verb non-3sg-pr eat\nJJR comparative adj bigger RB adverb quickly VBZ verb 3sg pres eats\nJJS superlative adj wildest RBR comparative adv faster WDT wh-determ. which, that\nLS list item marker 1, 2, One RBS superlatv. adv fastest WP wh-pronoun what, who\nMD modal can, should RP particle up, off WP$ wh-possess. whose\nNN sing or mass noun llama SYM symbol +,%,& WRB wh-adverb how, where\nFigure 17.2 Penn Treebank core 36 part-of-speech tags.\nBelow we show some examples with each word tagged according to both the UD\n(in blue) and Penn (in red) tagsets. Notice that the Penn tagset distinguishes tense\nand participles on verbs, and has a special tag for the existential there construction in\nEnglish. Note that since London Journal of Medicine is a proper noun, both tagsets\nmark its component nouns as PROPN/NNP, including journal andmedicine , which\nmight otherwise be labeled as common nouns (NOUN/NN).\n(17.1) There/ PRON /EXare/ VERB /VBP 70/NUM /CDchildren/ NOUN /NNS\nthere/ ADV/RB./PUNC /.\n(17.2) Preliminary/ ADJ/JJ\ufb01ndings/ NOUN /NNS were/ AUX/VBD\nreported/ VERB /VBN in/ADP/INtoday/ NOUN /NN\u2019s/PART /POS\nLondon/ PROPN /NNP Journal/ PROPN /NNP of/ADP/INMedicine/ PROPN /NNP\n17.2 Part-of-Speech Tagging\nPart-of-speech tagging is the process of assigning a part-of-speech to each word inpart-of-speech\ntagging",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 558,
      "token_count": 774,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 17.2 Penn Treebank core 36 part-of-speech tags.\nBelow we show some examples with each word tagged according to both the UD\n(in blue) and Penn (in red) tagsets. Notice that the Penn tagset distinguishes tense\nand participles on verbs, and has a special tag for the existential there construction in\nEnglish. Note that since London Journal of Medicine is a proper noun, both tagsets\nmark its component nouns as PROPN/NNP, including journal andmedicine , which\nmight otherwise be labeled as common nouns (NOUN/NN).\n(17.1) There/ PRON /EXare/ VERB /VBP 70/NUM /CDchildren/ NOUN /NNS\nthere/ ADV/RB./PUNC /.\n(17.2) Preliminary/ ADJ/JJ\ufb01ndings/ NOUN /NNS were/ AUX/VBD\nreported/ VERB /VBN in/ADP/INtoday/ NOUN /NN\u2019s/PART /POS\nLondon/ PROPN /NNP Journal/ PROPN /NNP of/ADP/INMedicine/ PROPN /NNP\n17.2 Part-of-Speech Tagging\nPart-of-speech tagging is the process of assigning a part-of-speech to each word inpart-of-speech\ntagging\na text. The input is a sequence x1;x2;:::;xnof (tokenized) words and a tagset, and\nthe output is a sequence y1;y2;:::;ynof tags, each output yicorresponding exactly to\none input xi, as shown in the intuition in Fig. 17.3.\nTagging is a disambiguation task; words are ambiguous \u2014have more than one ambiguous\npossible part-of-speech\u2014and the goal is to \ufb01nd the correct tag for the situation.\nFor example, book can be a verb ( book that \ufb02ight ) or a noun ( hand me that book ).\nThat can be a determiner ( Does that \ufb02ight serve dinner ) or a complementizer ( I",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 559,
      "token_count": 443,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 374\n\n366 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\nwillNOUNAUXVERBDETNOUNJanetbackthebillPart of Speech Taggerx1x2x3x4x5y1y2y3y4y5\nFigure 17.3 The task of part-of-speech tagging: mapping from input words x1;x2;:::;xnto\noutput POS tags y1;y2;:::;yn.\nthought that your \ufb02ight was earlier ). The goal of POS-tagging is to resolve theseambiguity\nresolution\nambiguities, choosing the proper tag for the context.\nThe accuracy of part-of-speech tagging algorithms (the percentage of test set accuracy\ntags that match human gold labels) is extremely high. One study found accuracies\nover 97% across 15 languages from the Universal Dependency (UD) treebank (Wu\nand Dredze, 2019). Accuracies on various English treebanks are also 97% (no matter\nthe algorithm; HMMs, CRFs, BERT perform similarly). This 97% number is also\nabout the human performance on this task, at least for English (Manning, 2011).\nTypes: WSJ Brown\nUnambiguous (1 tag) 44,432 ( 86% ) 45,799 ( 85% )\nAmbiguous (2+ tags) 7,025 ( 14% ) 8,050 ( 15% )\nTokens :\nUnambiguous (1 tag) 577,421 ( 45% ) 384,349 ( 33% )\nAmbiguous (2+ tags) 711,780 ( 55% ) 786,646 ( 67% )\nFigure 17.4 Tag ambiguity in the Brown and WSJ corpora (Treebank-3 45-tag tagset).\nWe\u2019ll introduce algorithms for the task in the next few sections, but \ufb01rst let\u2019s\nexplore the task. Exactly how hard is it? Fig. 17.4 shows that most word types\n(85-86%) are unambiguous ( Janet is always NNP, hesitantly is always RB). But the\nambiguous words, though accounting for only 14-15% of the vocabulary, are very\ncommon, and 55-67% of word tokens in running text are ambiguous. Particularly\nambiguous common words include that,back,down ,putandset; here are some\nexamples of the 6 different parts of speech for the word back:\nearnings growth took a back/JJ seat\na small building in the back/NN\na clear majority of senators back/VBP the bill\nDave began to back/VB toward the door\nenable the country to buy back/RP debt\nI was twenty-one back/RB then\nNonetheless, many words are easy to disambiguate, because their different tags\naren\u2019t equally likely. For example, acan be a determiner or the letter a, but the\ndeterminer sense is much more likely.\nThis idea suggests a useful baseline : given an ambiguous word, choose the tag\nwhich is most frequent in the training corpus. This is a key concept:\nMost Frequent Class Baseline: Always compare a classi\ufb01er against a baseline at\nleast as good as the most frequent class baseline (assigning each token to the class\nit occurred in most often in the training set).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 560,
      "token_count": 730,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 375\n\n17.3 \u2022 N AMED ENTITIES AND NAMED ENTITY TAGGING 367\nThe most-frequent-tag baseline has an accuracy of about 92%1. The baseline\nthus differs from the state-of-the-art and human ceiling (97%) by only 5%.\n17.3 Named Entities and Named Entity Tagging\nPart of speech tagging can tell us that words like Janet ,Stanford University , and\nColorado are all proper nouns; being a proper noun is a grammatical property of\nthese words. But viewed from a semantic perspective, these proper nouns refer to\ndifferent kinds of entities: Janet is a person, Stanford University is an organization,\nand Colorado is a location.\nHere we re-introduce the concept of a named entity , which was also introduced named entity\nin Section 11.5 for readers who haven\u2019t yet read Chapter 11.\nAnamed entity is, roughly speaking, anything that can be referred to with a named entity\nproper name: a person, a location, an organization. The task of named entity recog-\nnition (NER ) is to \ufb01nd spans of text that constitute proper names and tag the type ofnamed entity\nrecognition\nNER the entity. Four entity tags are most common: PER (person), LOC (location), ORG\n(organization), or GPE (geo-political entity). However, the term named entity is\ncommonly extended to include things that aren\u2019t entities per se, including dates,\ntimes, and other kinds of temporal expressions, and even numerical expressions like\nprices. Here\u2019s an example of the output of an NER tagger:\nCiting high fuel prices, [ ORG United Airlines ] said [ TIME Friday ] it\nhas increased fares by [ MONEY $6] per round trip on \ufb02ights to some\ncities also served by lower-cost carriers. [ ORG American Airlines ], a\nunit of [ ORG AMR Corp.] , immediately matched the move, spokesman\n[PER Tim Wagner ] said. [ ORG United] , a unit of [ ORG UAL Corp.] ,\nsaid the increase took effect [ TIME Thursday] and applies to most\nroutes where it competes against discount carriers, such as [ LOC Chicago]\nto [LOC Dallas] and [ LOC Denver] to [LOC San Francisco] .\nThe text contains 13 mentions of named entities including 5 organizations, 4 loca-\ntions, 2 times, 1 person, and 1 mention of money. Figure 17.5 shows typical generic\nnamed entity types. Many applications will also need to use speci\ufb01c entity types like\nproteins, genes, commercial products, or works of art.\nType Tag Sample Categories Example sentences\nPeople PER people, characters Turing is a giant of computer science.\nOrganization ORG companies, sports teams The IPCC warned about the cyclone.\nLocation LOC regions, mountains, seas Mt. Sanitas is in Sunshine Canyon .\nGeo-Political Entity GPE countries, states Palo Alto is raising the fees for parking.\nFigure 17.5 A list of generic named entity types with the kinds of entities they refer to.\nNamed entity tagging is a useful \ufb01rst step in lots of natural language processing\ntasks. In sentiment analysis we might want to know a consumer\u2019s sentiment toward a\nparticular entity. Entities are a useful \ufb01rst stage in question answering, or for linking\ntext to information in structured knowledge sources like Wikipedia. And named\nentity tagging is also central to tasks involving building semantic representations,\nlike extracting events and the relationship between participants.\n1In English, on the WSJ corpus, tested on sections 22-24.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 561,
      "token_count": 757,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 376\n\n368 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\nUnlike part-of-speech tagging, where there is no segmentation problem since\neach word gets one tag, the task of named entity recognition is to \ufb01nd and label\nspans of text, and is dif\ufb01cult partly because of the ambiguity of segmentation; we\nneed to decide what\u2019s an entity and what isn\u2019t, and where the boundaries are. Indeed,\nmost words in a text will not be named entities. Another dif\ufb01culty is caused by type\nambiguity. The mention JFKcan refer to a person, the airport in New York, or any\nnumber of schools, bridges, and streets around the United States. Some examples of\nthis kind of cross-type confusion are given in Figure 17.6.\n[PER Washington] was born into slavery on the farm of James Burroughs.\n[ORG Washington] went up 2 games to 1 in the four-game series.\nBlair arrived in [ LOC Washington] for what may well be his last state visit.\nIn June, [ GPE Washington] passed a primary seatbelt law.\nFigure 17.6 Examples of type ambiguities in the use of the name Washington .\nThe standard approach to sequence labeling for a span-recognition problem like\nNER is BIO tagging (Ramshaw and Marcus, 1995). This is a method that allows us\nto treat NER like a word-by-word sequence labeling task, via tags that capture both\nthe boundary and the named entity type. Consider the following sentence:\n[PER Jane Villanueva ] of [ ORG United ] , a unit of [ ORG United Airlines\nHolding ] , said the fare applies to the [ LOC Chicago ] route.\nFigure 17.7 shows the same excerpt represented with BIO tagging, as well as BIO\nvariants called IOtagging and BIOES tagging. In BIO tagging we label any token\nthatbegins a span of interest with the label B, tokens that occur inside a span are\ntagged with an I, and any tokens outside of any span of interest are labeled O. While\nthere is only one Otag, we\u2019ll have distinct Band Itags for each named entity class.\nThe number of tags is thus 2 n+1 tags, where nis the number of entity types. BIO\ntagging can represent exactly the same information as the bracketed notation, but has\nthe advantage that we can represent the task in the same simple sequence modeling\nway as part-of-speech tagging: assigning a single label yito each input word xi:\nWords IO Label BIO Label BIOES Label\nJane I-PER B-PER B-PER\nVillanueva I-PER I-PER E-PER\nof O O O\nUnited I-ORG B-ORG B-ORG\nAirlines I-ORG I-ORG I-ORG\nHolding I-ORG I-ORG E-ORG\ndiscussed O O O\nthe O O O\nChicago I-LOC B-LOC S-LOC\nroute O O O\n. O O O\nFigure 17.7 NER as a sequence model, showing IO, BIO, and BIOES taggings.\nWe\u2019ve also shown two variant tagging schemes: IO tagging, which loses some\ninformation by eliminating the B tag, and BIOES tagging, which adds an end tag\nEfor the end of a span, and a span tag Sfor a span consisting of only one word.\nA sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label each\ntoken in a text with tags that indicate the presence (or absence) of particular kinds\nof named entities.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 562,
      "token_count": 776,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 377\n\n17.4 \u2022 HMM P ART-OF-SPEECH TAGGING 369\n17.4 HMM Part-of-Speech Tagging\nIn this section we introduce our \ufb01rst sequence labeling algorithm, the Hidden Markov\nModel, and show how to apply it to part-of-speech tagging. Recall that a sequence\nlabeler is a model whose job is to assign a label to each unit in a sequence, thus\nmapping a sequence of observations to a sequence of labels of the same length.\nThe HMM is a classic model that introduces many of the key concepts of sequence\nmodeling that we will see again in more modern models.\nAn HMM is a probabilistic sequence model: given a sequence of units (words,\nletters, morphemes, sentences, whatever), it computes a probability distribution over\npossible sequences of labels and chooses the best label sequence.\n17.4.1 Markov Chains\nThe HMM is based on augmenting the Markov chain. A Markov chain is a model Markov chain\nthat tells us something about the probabilities of sequences of random variables,\nstates , each of which can take on values from some set. These sets can be words, or\ntags, or symbols representing anything, for example the weather. A Markov chain\nmakes a very strong assumption that if we want to predict the future in the sequence,\nall that matters is the current state. All the states before the current state have no im-\npact on the future except via the current state. It\u2019s as if to predict tomorrow\u2019s weather\nyou could examine today\u2019s weather but you weren\u2019t allowed to look at yesterday\u2019s\nweather.\nWARM3HOT1COLD2.8.6.1.1.3.6.1.1.3\ncharminguniformlyare.1.4.5.5.5.2.6.2\n(a) (b)\nFigure 17.8 A Markov chain for weather (a) and one for words (b), showing states and\ntransitions. A start distribution pis required; setting p= [0:1;0:7;0:2]for (a) would mean a\nprobability 0.7 of starting in state 2 (cold), probability 0.1 of starting in state 1 (hot), etc.\nMore formally, consider a sequence of state variables q1;q2;:::;qi. A Markov\nmodel embodies the Markov assumption on the probabilities of this sequence: thatMarkov\nassumption\nwhen predicting the future, the past doesn\u2019t matter, only the present.\nMarkov Assumption: P(qi=ajq1:::qi\u00001) =P(qi=ajqi\u00001) (17.3)\nFigure 17.8a shows a Markov chain for assigning a probability to a sequence of\nweather events, for which the vocabulary consists of HOT,COLD , and WARM . The\nstates are represented as nodes in the graph, and the transitions, with their probabil-\nities, as edges. The transitions are probabilities: the values of arcs leaving a given\nstate must sum to 1. Figure 17.8b shows a Markov chain for assigning a probabil-\nity to a sequence of words w1:::wt. This Markov chain should be familiar; in fact,\nit represents a bigram language model, with each edge expressing the probability\np(wijwj)! Given the two models in Fig. 17.8, we can assign a probability to any\nsequence from our vocabulary.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 563,
      "token_count": 742,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 378",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 564,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "370 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\nFormally, a Markov chain is speci\ufb01ed by the following components:\nQ=q1q2:::qN a set of Nstates\nA=a11a12:::aN1:::aNN atransition probability matrix A, each ai jrepresent-\ning the probability of moving from state ito state j, s.t.Pn\nj=1ai j=18i\np=p1;p2;:::;pN aninitial probability distribution over states. piis the\nprobability that the Markov chain will start in state i.\nSome states jmay have pj=0, meaning that they cannot\nbe initial states. Also,Pn\ni=1pi=1\nBefore you go on, use the sample probabilities in Fig. 17.8a (with p= [0:1;0:7;0:2])\nto compute the probability of each of the following sequences:\n(17.4) hot hot hot hot\n(17.5) cold hot cold hot\nWhat does the difference in these probabilities tell you about a real-world weather\nfact encoded in Fig. 17.8a?\n17.4.2 The Hidden Markov Model\nA Markov chain is useful when we need to compute a probability for a sequence\nof observable events. In many cases, however, the events we are interested in are\nhidden : we don\u2019t observe them directly. For example we don\u2019t normally observe hidden\npart-of-speech tags in a text. Rather, we see words, and must infer the tags from the\nword sequence. We call the tags hidden because they are not observed.\nAhidden Markov model (HMM ) allows us to talk about both observed eventshidden Markov\nmodel\n(like words that we see in the input) and hidden events (like part-of-speech tags) that\nwe think of as causal factors in our probabilistic model. An HMM is speci\ufb01ed by\nthe following components:\nQ=q1q2:::qN a set of Nstates\nA=a11:::ai j:::aNNatransition probability matrix A, each ai jrepresenting the probability\nof moving from state ito state j, s.t.PN\nj=1ai j=18i\nB=bi(ot) a sequence of observation likelihoods , also called emission probabili-\nties, each expressing the probability of an observation ot(drawn from a\nvocabulary V=v1;v2;:::;vV) being generated from a state qi\np=p1;p2;:::;pN aninitial probability distribution over states. piis the probability that\nthe Markov chain will start in state i. Some states jmay have pj=0,\nmeaning that they cannot be initial states. Also,Pn\ni=1pi=1\nThe HMM is given as input O=o1o2:::oT: a sequence of Tobservations , each\none drawn from the vocabulary V.\nA \ufb01rst-order hidden Markov model instantiates two simplifying assumptions.\nFirst, as with a \ufb01rst-order Markov chain, the probability of a particular state depends\nonly on the previous state:\nMarkov Assumption: P(qijq1;:::;qi\u00001) =P(qijqi\u00001) (17.6)\nSecond, the probability of an output observation oidepends only on the state that\nproduced the observation qiand not on any other states or any other observations:\nOutput Independence: P(oijq1;:::qi;:::; qT;o1;:::; oi;:::; oT) =P(oijqi)(17.7)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 565,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 379\n\n17.4 \u2022 HMM P ART-OF-SPEECH TAGGING 371\n17.4.3 The components of an HMM tagger\nAn HMM has two components, the AandBprobabilities, both estimated by counting\non a tagged training corpus. (For this example we\u2019ll use the tagged WSJ corpus.)\nTheAmatrix contains the tag transition probabilities P(tijti\u00001)which represent\nthe probability of a tag occurring given the previous tag. For example, modal verbs\nlikewillare very likely to be followed by a verb in the base form, a VB, like race, so\nwe expect this probability to be high. We compute the maximum likelihood estimate\nof this transition probability by counting, out of the times we see the \ufb01rst tag in a\nlabeled corpus, how often the \ufb01rst tag is followed by the second:\nP(tijti\u00001) =C(ti\u00001;ti)\nC(ti\u00001)(17.8)\nIn the WSJ corpus, for example, MD occurs 13124 times of which it is followed\nby VB 10471, for an MLE estimate of\nP(V BjMD) =C(MD;V B)\nC(MD)=10471\n13124=:80 (17.9)\nTheBemission probabilities, P(wijti), represent the probability, given a tag (say\nMD), that it will be associated with a given word (say will). The MLE of the emis-\nsion probability is\nP(wijti) =C(ti;wi)\nC(ti)(17.10)\nOf the 13124 occurrences of MD in the WSJ corpus, it is associated with will4046\ntimes:\nP(willjMD) =C(MD;will)\nC(MD)=4046\n13124=:31 (17.11)\nWe saw this kind of Bayesian modeling in Chapter 4; recall that this likelihood\nterm is not asking \u201cwhich is the most likely tag for the word will?\u201d That would be\nthe posterior P(MDjwill). Instead, P(willjMD)answers the slightly counterintuitive\nquestion \u201cIf we were going to generate a MD, how likely is it that this modal would\nbewill?\u201d\nNN3VB1MD2a22\na11a12a21a13a33a32a23a31P(\"aardvark\" | NN)...P(\u201cwill\u201d | NN)...P(\"the\" | NN)...P(\u201cback\u201d | NN)...P(\"zebra\" | NN)B3P(\"aardvark\" | VB)...P(\u201cwill\u201d | VB)...P(\"the\" | VB)...P(\u201cback\u201d | VB)...P(\"zebra\" | VB)B1P(\"aardvark\" | MD)...P(\u201cwill\u201d | MD)...P(\"the\" | MD)...P(\u201cback\u201d | MD)...P(\"zebra\" | MD)B2\nFigure 17.9 An illustration of the two parts of an HMM representation: the Atransition\nprobabilities used to compute the prior probability, and the Bobservation likelihoods that are\nassociated with each state, one likelihood for each possible observation word.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 566,
      "token_count": 686,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 380\n\n372 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\nThe Atransition probabilities, and Bobservation likelihoods of the HMM are\nillustrated in Fig. 17.9 for three states in an HMM part-of-speech tagger; the full\ntagger would have one state for each tag.\n17.4.4 HMM tagging as decoding\nFor any model, such as an HMM, that contains hidden variables, the task of deter-\nmining the hidden variables sequence corresponding to the sequence of observations\nis called decoding . More formally, decoding\nDecoding : Given as input an HMM l= (A;B)and a sequence of ob-\nservations O=o1;o2;:::;oT, \ufb01nd the most probable sequence of states\nQ=q1q2q3:::qT.\nFor part-of-speech tagging, the goal of HMM decoding is to choose the tag\nsequence t1:::tnthat is most probable given the observation sequence of nwords\nw1:::wn:\n\u02c6t1:n=argmax\nt1:::tnP(t1:::tnjw1:::wn) (17.12)\nThe way we\u2019ll do this in the HMM is to use Bayes\u2019 rule to instead compute:\n\u02c6t1:n=argmax\nt1:::tnP(w1:::wnjt1:::tn)P(t1:::tn)\nP(w1:::wn)(17.13)\nFurthermore, we simplify Eq. 17.13 by dropping the denominator P(wn\n1):\n\u02c6t1:n=argmax\nt1:::tnP(w1:::wnjt1:::tn)P(t1:::tn) (17.14)\nHMM taggers make two further simplifying assumptions. The \ufb01rst (output in-\ndependence, from Eq. 17.7) is that the probability of a word appearing depends only\non its own tag and is independent of neighboring words and tags:\nP(w1:::wnjt1:::tn)\u0019nY\ni=1P(wijti) (17.15)\nThe second assumption (the Markov assumption, Eq. 17.6) is that the probability of\na tag is dependent only on the previous tag, rather than the entire tag sequence;\nP(t1:::tn)\u0019nY\ni=1P(tijti\u00001) (17.16)\nPlugging the simplifying assumptions from Eq. 17.15 and Eq. 17.16 into Eq. 17.14\nresults in the following equation for the most probable tag sequence from a bigram\ntagger:\n\u02c6t1:n=argmax\nt1:::tnP(t1:::tnjw1:::wn)\u0019argmax\nt1:::tnnY\ni=1emissionz}|{\nP(wijti)transitionz}|{\nP(tijti\u00001) (17.17)\nThe two parts of Eq. 17.17 correspond neatly to the Bemission probability andA\ntransition probability that we just de\ufb01ned above!",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 567,
      "token_count": 682,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 381\n\n17.4 \u2022 HMM P ART-OF-SPEECH TAGGING 373\nfunction VITERBI (observations of len T,state-graph of len N)returns best-path ,path-prob\ncreate a path probability matrix viterbi[N,T]\nforeach state sfrom 1toNdo ; initialization step\nviterbi [s,1] ps\u0003bs(o1)\nbackpointer [s,1] 0\nforeach time step tfrom 2toTdo ; recursion step\nforeach state sfrom 1toNdo\nviterbi [s,t] Nmax\ns0=1viterbi [s0;t\u00001]\u0003as0;s\u0003bs(ot)\nbackpointer [s,t] Nargmax\ns0=1viterbi [s0;t\u00001]\u0003as0;s\u0003bs(ot)\nbestpathprob Nmax\ns=1viterbi [s;T] ; termination step\nbestpathpointer Nargmax\ns=1viterbi [s;T] ; termination step\nbestpath the path starting at state bestpathpointer , that follows backpointer[] to states back in time\nreturn bestpath ,bestpathprob\nFigure 17.10 Viterbi algorithm for \ufb01nding the optimal sequence of tags. Given an observation sequence and\nan HMM l= (A;B), the algorithm returns the state path through the HMM that assigns maximum likelihood\nto the observation sequence.\n17.4.5 The Viterbi Algorithm\nThe decoding algorithm for HMMs is the Viterbi algorithm shown in Fig. 17.10.Viterbi\nalgorithm\nAs an instance of dynamic programming , Viterbi resembles the dynamic program-\nming minimum edit distance algorithm of Chapter 2.\nThe Viterbi algorithm \ufb01rst sets up a probability matrix or lattice , with one col-\numn for each observation otand one row for each state in the state graph. Each col-\numn thus has a cell for each state qiin the single combined automaton. Figure 17.11\nshows an intuition of this lattice for the sentence Janet will back the bill .\nEach cell of the lattice, vt(j), represents the probability that the HMM is in state\njafter seeing the \ufb01rst tobservations and passing through the most probable state\nsequence q1;:::;qt\u00001, given the HMM l. The value of each cell vt(j)is computed\nby recursively taking the most probable path that could lead us to this cell. Formally,\neach cell expresses the probability\nvt(j) = max\nq1;:::;qt\u00001P(q1:::qt\u00001;o1;o2:::ot;qt=jjl) (17.18)\nWe represent the most probable path by taking the maximum over all possible\nprevious state sequences max\nq1;:::;qt\u00001. Like other dynamic programming algorithms,\nViterbi \ufb01lls each cell recursively. Given that we had already computed the probabil-\nity of being in every state at time t\u00001, we compute the Viterbi probability by taking\nthe most probable of the extensions of the paths that lead to the current cell. For a\ngiven state qjat time t, the value vt(j)is computed as\nvt(j) =Nmax\ni=1vt\u00001(i)ai jbj(ot) (17.19)\nThe three factors that are multiplied in Eq. 17.19 for extending the previous paths to\ncompute the Viterbi probability at time tare",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 568,
      "token_count": 752,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 382\n\n374 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\nJJNNPNNPNNPMDMDMDMDVBVBJJJJJJNNNNRBRBRBRBDTDTDTDT\nNNPJanetwillbackthebillNNVBMDNNVBJJRB\nNNPDTNNVB\nFigure 17.11 A sketch of the lattice for Janet will back the bill , showing the possible tags\n(qi) for each word and highlighting the path corresponding to the correct tag sequence through\nthe hidden states. States (parts of speech) which have a zero probability of generating a\nparticular word according to the Bmatrix (such as the probability that a determiner DT will\nbe realized as Janet ) are greyed out.\nvt\u00001(i)theprevious Viterbi path probability from the previous time step\nai j thetransition probability from previous state qito current state qj\nbj(ot) thestate observation likelihood of the observation symbol otgiven\nthe current state j\n17.4.6 Working through an example\nLet\u2019s tag the sentence Janet will back the bill ; the goal is the correct series of tags\n(see also Fig. 17.11):\n(17.20) Janet/NNP will/MD back/VB the/DT bill/NN\nNNP MD VB JJ NN RB DT\n<s> 0.2767 0.0006 0.0031 0.0453 0.0449 0.0510 0.2026\nNNP 0.3777 0.0110 0.0009 0.0084 0.0584 0.0090 0.0025\nMD 0.0008 0.0002 0.7968 0.0005 0.0008 0.1698 0.0041\nVB 0.0322 0.0005 0.0050 0.0837 0.0615 0.0514 0.2231\nJJ 0.0366 0.0004 0.0001 0.0733 0.4509 0.0036 0.0036\nNN 0.0096 0.0176 0.0014 0.0086 0.1216 0.0177 0.0068\nRB 0.0068 0.0102 0.1011 0.1012 0.0120 0.0728 0.0479\nDT 0.1147 0.0021 0.0002 0.2157 0.4744 0.0102 0.0017\nFigure 17.12 TheAtransition probabilities P(tijti\u00001)computed from the WSJ corpus with-\nout smoothing. Rows are labeled with the conditioning event; thus P(V BjMD)is 0.7968.\n<s>is the start token.\nLet the HMM be de\ufb01ned by the two tables in Fig. 17.12 and Fig. 17.13. Fig-\nure 17.12 lists the ai jprobabilities for transitioning between the hidden states (part-\nof-speech tags). Figure 17.13 expresses the bi(ot)probabilities, the observation\nlikelihoods of words given tags. This table is (slightly simpli\ufb01ed) from counts in the\nWSJ corpus. So the word Janet only appears as an NNP, back has 4 possible parts",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 569,
      "token_count": 764,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 383\n\n17.4 \u2022 HMM P ART-OF-SPEECH TAGGING 375\nJanet will back the bill\nNNP 0.000032 0 0 0.000048 0\nMD 0 0.308431 0 0 0\nVB 0 0.000028 0.000672 0 0.000028\nJJ 0 0 0.000340 0 0\nNN 0 0.000200 0.000223 0 0.002337\nRB 0 0 0.010446 0 0\nDT 0 0 0 0.506099 0\nFigure 17.13 Observation likelihoods Bcomputed from the WSJ corpus without smooth-\ning, simpli\ufb01ed slightly.\nof speech, and the word thecan appear as a determiner or as an NNP (in titles like\n\u201cSomewhere Over the Rainbow\u201d all words are tagged as NNP).\n\u03c0P(NNP|start) = .28* P(MD|MD)= 0*  P(MD|NNP).000009*.01  = .9e-8 v1(2)=.0006 x 0 = 0v1(1) = .28* .000032 = .000009\ntMDq2q1\no1Janetbillwillo2o3backVBJJv1(3)=.0031 x 0 = 0v1(4)= .045*0=0\no4  *  P(MD|VB) = 0 * P(MD|JJ)= 0P(VB|start) = .0031P(JJ |start) =.045\nbacktraceq3q4\ntheNNq5RBq6DTq7\nv2(2) =max * .308 =2.772e-8v2(5)=max * .0002 = .0000000001\nv2(3)=max * .000028 =     2.5e-13v3(6)=max * .0104v3(5)=max * .000223v3(4)=max * .00034v3(3)=max * .00067v1(5)v1(6)v1(7)\nv2(1)v2(4)v2(6)v2(7)\nbacktrace* P(RB|NN)* P(NN|NN)\nstartstartstartstartstarto5NNPP(MD|start) = .0006\nFigure 17.14 The \ufb01rst few entries in the individual state columns for the Viterbi algorithm. Each cell keeps\nthe probability of the best path so far and a pointer to the previous cell along that path. We have only \ufb01lled out\ncolumns 1 and 2; to avoid clutter most cells with value 0 are left empty. The rest is left as an exercise for the\nreader. After the cells are \ufb01lled in, backtracing from the endstate, we should be able to reconstruct the correct\nstate sequence NNP MD VB DT NN.\nFigure 17.14 shows a \ufb02eshed-out version of the sketch we saw in Fig. 17.11,\nthe Viterbi lattice for computing the best hidden state sequence for the observation\nsequence Janet will back the bill .\nThere are N=5 state columns. We begin in column 1 (for the word Janet ) by\nsetting the Viterbi value in each cell to the product of the ptransition probability (the\nstart probability for that state i, which we get from the <s>entry of Fig. 17.12), and",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 570,
      "token_count": 791,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 384\n\n376 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\nthe observation likelihood of the word Janet given the tag for that cell. Most of the\ncells in the column are zero since the word Janet cannot be any of those tags. The\nreader should \ufb01nd this in Fig. 17.14.\nNext, each cell in the willcolumn gets updated. For each state, we compute the\nvalue viterbi [s;t]by taking the maximum over the extensions of all the paths from\nthe previous column that lead to the current cell according to Eq. 17.19. We have\nshown the values for the MD, VB, and NN cells. Each cell gets the max of the 7 val-\nues from the previous column, multiplied by the appropriate transition probability;\nas it happens in this case, most of them are zero from the previous column. The re-\nmaining value is multiplied by the relevant observation probability, and the (trivial)\nmax is taken. In this case the \ufb01nal value, 2.772e-8, comes from the NNP state at the\nprevious column. The reader should \ufb01ll in the rest of the lattice in Fig. 17.14 and\nbacktrace to see whether or not the Viterbi algorithm returns the gold state sequence\nNNP MD VB DT NN.\n17.5 Conditional Random Fields (CRFs)\nWhile the HMM is a useful and powerful model, it turns out that HMMs need a\nnumber of augmentations to achieve high accuracy. For example, in POS tagging\nas in other tasks, we often run into unknown words : proper names and acronymsunknown\nwords\nare created very often, and even new common nouns and verbs enter the language\nat a surprising rate. It would be great to have ways to add arbitrary features to\nhelp with this, perhaps based on capitalization or morphology (words starting with\ncapital letters are likely to be proper nouns, words ending with -edtend to be past\ntense (VBD or VBN), etc.) Or knowing the previous or following words might be a\nuseful feature (if the previous word is the, the current tag is unlikely to be a verb).\nAlthough we could try to hack the HMM to \ufb01nd ways to incorporate some of\nthese, in general it\u2019s hard for generative models like HMMs to add arbitrary features\ndirectly into the model in a clean way. We\u2019ve already seen a model for combining\narbitrary features in a principled way: log-linear models like the logistic regression\nmodel of Chapter 5! But logistic regression isn\u2019t a sequence model; it assigns a class\nto a single observation.\nLuckily, there is a discriminative sequence model based on log-linear models:\ntheconditional random \ufb01eld (CRF ). We\u2019ll describe here the linear chain CRF , CRF\nthe version of the CRF most commonly used for language processing, and the one\nwhose conditioning closely matches the HMM.\nAssuming we have a sequence of input words X=x1:::xnand want to compute\na sequence of output tags Y=y1:::yn. In an HMM to compute the best tag sequence\nthat maximizes P(YjX)we rely on Bayes\u2019 rule and the likelihood P(XjY):\n\u02c6Y=argmax\nYp(YjX)\n=argmax\nYp(XjY)p(Y)\n=argmax\nYY\nip(xijyi)Y\nip(yijyi\u00001) (17.21)\nIn a CRF, by contrast, we compute the posterior p(YjX)directly, training the CRF",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 571,
      "token_count": 777,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 385\n\n17.5 \u2022 C ONDITIONAL RANDOM FIELDS (CRF S)377\nto discriminate among the possible tag sequences:\n\u02c6Y=argmax\nY2YP(YjX) (17.22)\nHowever, the CRF does not compute a probability for each tag at each time step. In-\nstead, at each time step the CRF computes log-linear functions over a set of relevant\nfeatures, and these local features are aggregated and normalized to produce a global\nprobability for the whole sequence.\nLet\u2019s introduce the CRF more formally, again using XandYas the input and\noutput sequences. A CRF is a log-linear model that assigns a probability to an\nentire output (tag) sequence Y, out of all possible sequences Y, given the entire input\n(word) sequence X. We can think of a CRF as like a giant sequential version of\nthe multinomial logistic regression algorithm we saw for text categorization. Recall\nthat we introduced the feature function fin regular multinomial logistic regression\nfor text categorization as a function of a tuple: the input text xand a single class y\n(page 86). In a CRF, we\u2019re dealing with a sequence, so the function Fmaps an entire\ninput sequence Xand an entire output sequence Yto a feature vector. Let\u2019s assume\nwe have Kfeatures, with a weight wkfor each feature Fk:\np(YjX) =exp KX\nk=1wkFk(X;Y)!\nX\nY02Yexp KX\nk=1wkFk(X;Y0)! (17.23)\nIt\u2019s common to also describe the same equation by pulling out the denominator into\na function Z(X):\np(YjX) =1\nZ(X)exp KX\nk=1wkFk(X;Y)!\n(17.24)\nZ(X) =X\nY02Yexp KX\nk=1wkFk(X;Y0)!\n(17.25)\nWe\u2019ll call these Kfunctions Fk(X;Y)global features , since each one is a property\nof the entire input sequence Xand output sequence Y. We compute them by decom-\nposing into a sum of local features for each position iinY:\nFk(X;Y) =nX\ni=1fk(yi\u00001;yi;X;i) (17.26)\nEach of these local features fkin a linear-chain CRF is allowed to make use of the\ncurrent output token yi, the previous output token yi\u00001, the entire input string X(or\nany subpart of it), and the current position i. This constraint to only depend on\nthe current and previous output tokens yiandyi\u00001are what characterizes a linear\nchain CRF . As we will see, this limitation makes it possible to use versions of thelinear chain\nCRF\nef\ufb01cient Viterbi and Forward-Backwards algorithms from the HMM. A general CRF,\nby contrast, allows a feature to make use of any output token, and are thus necessary\nfor tasks in which the decision depend on distant output tokens, like yi\u00004. General\nCRFs require more complex inference, and are less commonly used for language\nprocessing.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 572,
      "token_count": 681,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 386",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 573,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "378 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\n17.5.1 Features in a CRF POS Tagger\nLet\u2019s look at some of these features in detail, since the reason to use a discriminative\nsequence model is that it\u2019s easier to incorporate a lot of features.2\nAgain, in a linear-chain CRF, each local feature fkat position ican depend on\nany information from: (yi\u00001;yi;X;i). So some legal features representing common\nsituations might be the following:\n1fxi=the;yi=DETg\n1fyi=PROPN, xi+1=Street ,yi\u00001=NUMg\n1fyi=VERB, yi\u00001=AUXg\nFor simplicity, we\u2019ll assume all CRF features take on the value 1 or 0. Above, we\nexplicitly use the notation 1fxgto mean \u201c1 if xis true, and 0 otherwise\u201d. From now\non, we\u2019ll leave off the 1when we de\ufb01ne features, but you can assume each feature\nhas it there implicitly.\nAlthough the idea of what features to use is done by the system designer by hand,\nthe speci\ufb01c features are automatically populated by using feature templates as wefeature\ntemplates\nbrie\ufb02y mentioned in Chapter 5. Here are some templates that only use information\nfrom (yi\u00001;yi;X;i):\nhyi;xii;hyi;yi\u00001i;hyi;xi\u00001;xi+2i\nThese templates automatically populate the set of features from every instance in\nthe training and test set. Thus for our example Janet/NNP will/MD back/VB the/DT\nbill/NN , when xiis the word back, the following features would be generated and\nhave the value 1 (we\u2019ve assigned them arbitrary feature numbers):\nf3743:yi= VB and xi= back\nf156:yi= VB and yi\u00001= MD\nf99732 :yi= VB and xi\u00001= will and xi+2= bill\nIt\u2019s also important to have features that help with unknown words. One of the\nmost important is word shape features, which represent the abstract letter pattern word shape\nof the word by mapping lower-case letters to \u2018x\u2019, upper-case to \u2018X\u2019, numbers to\n\u2019d\u2019, and retaining punctuation. Thus for example I.M.F. would map to X.X.X. and\nDC10-30 would map to XXdd-dd. A second class of shorter word shape features is\nalso used. In these features consecutive character types are removed, so words in all\ncaps map to X, words with initial-caps map to Xx, DC10-30 would be mapped to\nXd-d but I.M.F would still map to X.X.X. Pre\ufb01x and suf\ufb01x features are also useful.\nIn summary, here are some sample feature templates that help with unknown words:\nxicontains a particular pre\ufb01x (perhaps from all pre\ufb01xes of length \u00142)\nxicontains a particular suf\ufb01x (perhaps from all suf\ufb01xes of length \u00142)\nxi\u2019s word shape\nxi\u2019s short word shape\nFor example the word well-dressed might generate the following non-zero val-\nued feature values:\n2Because in HMMs all computation is based on the two probabilities P(tagjtag)andP(wordjtag), if\nwe want to include some source of knowledge into the tagging process, we must \ufb01nd a way to encode\nthe knowledge into one of these two probabilities. Each time we add a feature we have to do a lot of",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 574,
      "token_count": 783,
      "chapter_title": ""
    }
  },
  {
    "content": "\u2019d\u2019, and retaining punctuation. Thus for example I.M.F. would map to X.X.X. and\nDC10-30 would map to XXdd-dd. A second class of shorter word shape features is\nalso used. In these features consecutive character types are removed, so words in all\ncaps map to X, words with initial-caps map to Xx, DC10-30 would be mapped to\nXd-d but I.M.F would still map to X.X.X. Pre\ufb01x and suf\ufb01x features are also useful.\nIn summary, here are some sample feature templates that help with unknown words:\nxicontains a particular pre\ufb01x (perhaps from all pre\ufb01xes of length \u00142)\nxicontains a particular suf\ufb01x (perhaps from all suf\ufb01xes of length \u00142)\nxi\u2019s word shape\nxi\u2019s short word shape\nFor example the word well-dressed might generate the following non-zero val-\nued feature values:\n2Because in HMMs all computation is based on the two probabilities P(tagjtag)andP(wordjtag), if\nwe want to include some source of knowledge into the tagging process, we must \ufb01nd a way to encode\nthe knowledge into one of these two probabilities. Each time we add a feature we have to do a lot of\ncomplicated conditioning which gets harder and harder as we have more and more such features.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 575,
      "token_count": 293,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 387\n\n17.5 \u2022 C ONDITIONAL RANDOM FIELDS (CRF S)379\npre\ufb01x( xi) =w\npre\ufb01x( xi) =we\nsuf\ufb01x( xi) =ed\nsuf\ufb01x( xi) =d\nword-shape( xi) =xxxx-xxxxxxx\nshort-word-shape( xi) =x-x\nThe known-word templates are computed for every word seen in the training\nset; the unknown word features can also be computed for all words in training, or\nonly on training words whose frequency is below some threshold. The result of the\nknown-word templates and word-signature features is a very large set of features.\nGenerally a feature cutoff is used in which features are thrown out if they have count\n<5 in the training set.\nRemember that in a CRF we don\u2019t learn weights for each of these local features\nfk. Instead, we \ufb01rst sum the values of each local feature (for example feature f3743)\nover the entire sentence, to create each global feature (for example F3743). It is those\nglobal features that will then be multiplied by weight w3743. Thus for training and\ninference there is always a \ufb01xed set of Kfeatures with Kweights, even though the\nlength of each sentence is different.\n17.5.2 Features for CRF Named Entity Recognizers\nA CRF for NER makes use of very similar features to a POS tagger, as shown in\nFigure 17.15.\nidentity of wi, identity of neighboring words\nembeddings for wi, embeddings for neighboring words\npart of speech of wi, part of speech of neighboring words\npresence of wiin agazetteer\nwicontains a particular pre\ufb01x (from all pre\ufb01xes of length \u00144)\nwicontains a particular suf\ufb01x (from all suf\ufb01xes of length \u00144)\nword shape of wi, word shape of neighboring words\nshort word shape of wi, short word shape of neighboring words\ngazetteer features\nFigure 17.15 Typical features for a feature-based NER system.\nOne feature that is especially useful for locations is a gazetteer , a list of place gazetteer\nnames, often providing millions of entries for locations with detailed geographical\nand political information.3This can be implemented as a binary feature indicating a\nphrase appears in the list. Other related resources like name-lists , for example from\nthe United States Census Bureau4, can be used, as can other entity dictionaries like\nlists of corporations or products, although they may not be as helpful as a gazetteer\n(Mikheev et al., 1999).\nThe sample named entity token L\u2019Occitane would generate the following non-\nzero valued feature values (assuming that L\u2019Occitane is neither in the gazetteer nor\nthe census).\n3www.geonames.org\n4www.census.gov",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 576,
      "token_count": 623,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 388\n\n380 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\npre\ufb01x( xi) =L suf\ufb01x( xi) =tane\npre\ufb01x( xi) =L' suf\ufb01x( xi) =ane\npre\ufb01x( xi) =L'O suf\ufb01x( xi) =ne\npre\ufb01x( xi) =L'Oc suf\ufb01x( xi) =e\nword-shape( xi) =X'Xxxxxxxx short-word-shape( xi) =X'Xx\nFigure 17.16 illustrates the result of adding part-of-speech tags and some shape\ninformation to our earlier example.\nWords POS Short shape Gazetteer BIO Label\nJane NNP Xx 0 B-PER\nVillanueva NNP Xx 1 I-PER\nof IN x 0 O\nUnited NNP Xx 0 B-ORG\nAirlines NNP Xx 0 I-ORG\nHolding NNP Xx 0 I-ORG\ndiscussed VBD x 0 O\nthe DT x 0 O\nChicago NNP Xx 1 B-LOC\nroute NN x 0 O\n. . . 0 O\nFigure 17.16 Some NER features for a sample sentence, assuming that Chicago and Vil-\nlanueva are listed as locations in a gazetteer. We assume features only take on the values 0 or\n1, so the \ufb01rst POS feature, for example, would be represented as 1fPOS=NNPg.\n17.5.3 Inference and Training for CRFs\nHow do we \ufb01nd the best tag sequence \u02c6Yfor a given input X? We start with Eq. 17.22:\n\u02c6Y=argmax\nY2YP(YjX)\n=argmax\nY2Y1\nZ(X)exp KX\nk=1wkFk(X;Y)!\n(17.27)\n=argmax\nY2Yexp KX\nk=1wknX\ni=1fk(yi\u00001;yi;X;i)!\n(17.28)\n=argmax\nY2YKX\nk=1wknX\ni=1fk(yi\u00001;yi;X;i) (17.29)\n=argmax\nY2YnX\ni=1KX\nk=1wkfk(yi\u00001;yi;X;i) (17.30)\nWe can ignore the exp function and the denominator Z(X), as we do above, because\nexp doesn\u2019t change the argmax, and the denominator Z(X)is constant for a given\nobservation sequence X.\nHow should we decode to \ufb01nd this optimal tag sequence \u02c6 y? Just as with HMMs,\nwe\u2019ll turn to the Viterbi algorithm, which works because, like the HMM, the linear-\nchain CRF depends at each timestep on only one previous output token yi\u00001.\nConcretely, this involves \ufb01lling an N\u0002Tarray with the appropriate values, main-\ntaining backpointers as we proceed. As with HMM Viterbi, when the table is \ufb01lled,\nwe simply follow pointers back from the maximum value in the \ufb01nal column to\nretrieve the desired set of labels.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 577,
      "token_count": 719,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 389\n\n17.6 \u2022 E VALUATION OF NAMED ENTITY RECOGNITION 381\nThe requisite changes from HMM Viterbi have to do only with how we \ufb01ll each\ncell. Recall from Eq. 17.19 that the recursive step of the Viterbi equation computes\nthe Viterbi value of time tfor state jas\nvt(j) =Nmax\ni=1vt\u00001(i)ai jbj(ot); 1\u0014j\u0014N;1<t\u0014T (17.31)\nwhich is the HMM implementation of\nvt(j) =Nmax\ni=1vt\u00001(i)P(sjjsi)P(otjsj)1\u0014j\u0014N;1<t\u0014T (17.32)\nThe CRF requires only a slight change to this latter formula, replacing the aandb\nprior and likelihood probabilities with the CRF features:\nvt(j) =Nmax\ni=1\"\nvt\u00001(i)+KX\nk=1wkfk(yt\u00001;yt;X;t)1\u0014j\u0014N;1<t\u0014T#\n(17.33)\nLearning in CRFs relies on the same supervised learning algorithms we presented\nfor logistic regression. Given a sequence of observations, feature functions, and cor-\nresponding outputs, we use stochastic gradient descent to train the weights to maxi-\nmize the log-likelihood of the training corpus. The local nature of linear-chain CRFs\nmeans that the forward-backward algorithm introduced for HMMs in Appendix A\ncan be extended to a CRF version that will ef\ufb01ciently compute the necessary deriva-\ntives. As with logistic regression, L1 or L2 regularization is important.\n17.6 Evaluation of Named Entity Recognition\nPart-of-speech taggers are evaluated by the standard metric of accuracy . Named\nentity recognizers are evaluated by recall ,precision , and F1measure . Recall that\nrecall is the ratio of the number of correctly labeled responses to the total that should\nhave been labeled; precision is the ratio of the number of correctly labeled responses\nto the total labeled; and F-measure is the harmonic mean of the two.\nTo know if the difference between the F 1scores of two NER systems is a signif-\nicant difference, we use the paired bootstrap test, or the similar randomization test\n(Section 4.9).\nFor named entity tagging, the entity rather than the word is the unit of response.\nThus in the example in Fig. 17.16, the two entities Jane Villanueva andUnited Air-\nlines Holding and the non-entity discussed would each count as a single response.\nThe fact that named entity tagging has a segmentation component which is not\npresent in tasks like text categorization or part-of-speech tagging causes some prob-\nlems with evaluation. For example, a system that labeled Jane but not Jane Vil-\nlanueva as a person would cause two errors, a false positive for O and a false nega-\ntive for I-PER. In addition, using entities as the unit of response but words as the unit\nof training means that there is a mismatch between the training and test conditions.\n17.7 Further Details\nIn this section we summarize a few remaining details of the data and models for\npart-of-speech tagging and NER, beginning with data. Since the algorithms we have",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 578,
      "token_count": 708,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 390",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 579,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "382 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\npresented are supervised, having labeled data is essential for training and testing. A\nwide variety of datasets exist for part-of-speech tagging and/or NER. The Universal\nDependencies (UD) dataset (de Marneffe et al., 2021) has POS tagged corpora in\nover a hundred languages, as do the Penn Treebanks in English, Chinese, and Arabic.\nOntoNotes has corpora labeled for named entities in English, Chinese, and Arabic\n(Hovy et al., 2006). Named entity tagged corpora are also available in particular\ndomains, such as for biomedical (Bada et al., 2012) and literary text (Bamman et al.,\n2019).\n17.7.1 Rule-based Methods\nWhile machine learned (neural or CRF) sequence models are the norm in academic\nresearch, commercial approaches to NER are often based on pragmatic combina-\ntions of lists and rules, with some smaller amount of supervised machine learning\n(Chiticariu et al., 2013). For example in the IBM System T architecture, a user\nspeci\ufb01es declarative constraints for tagging tasks in a formal query language that\nincludes regular expressions, dictionaries, semantic constraints, and other operators,\nwhich the system compiles into an ef\ufb01cient extractor (Chiticariu et al., 2018).\nOne common approach is to make repeated rule-based passes over a text, starting\nwith rules with very high precision but low recall, and, in subsequent stages, using\nmachine learning methods that take the output of the \ufb01rst pass into account (an\napproach \ufb01rst worked out for coreference (Lee et al., 2017a)):\n1. First, use high-precision rules to tag unambiguous entity mentions.\n2. Then, search for substring matches of the previously detected names.\n3. Use application-speci\ufb01c name lists to \ufb01nd likely domain-speci\ufb01c mentions.\n4. Finally, apply supervised sequence labeling techniques that use tags from pre-\nvious stages as additional features.\nRule-based methods were also the earliest methods for part-of-speech tagging.\nRule-based taggers like the English Constraint Grammar system (Karlsson et al.\n1995, V outilainen 1999) use a two-stage formalism invented in the 1950s and 1960s:\n(1) a morphological analyzer with tens of thousands of word stem entries returns all\nparts of speech for a word, then (2) a large set of thousands of constraints are applied\nto the input sentence to rule out parts of speech inconsistent with the context.\n17.7.2 POS Tagging for Morphologically Rich Languages\nAugmentations to tagging algorithms become necessary when dealing with lan-\nguages with rich morphology like Czech, Hungarian and Turkish.\nThese productive word-formation processes result in a large vocabulary for these\nlanguages: a 250,000 word token corpus of Hungarian has more than twice as many\nword types as a similarly sized corpus of English (Oravecz and Dienes, 2002), while\na 10 million word token corpus of Turkish contains four times as many word types\nas a similarly sized English corpus (Hakkani-T \u00a8ur et al., 2002). Large vocabular-\nies mean many unknown words, and these unknown words cause signi\ufb01cant per-\nformance degradations in a wide variety of languages (including Czech, Slovene,\nEstonian, and Romanian) (Haji \u02c7c, 2000).\nHighly in\ufb02ectional languages also have much more information than English",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 580,
      "token_count": 775,
      "chapter_title": ""
    }
  },
  {
    "content": "1995, V outilainen 1999) use a two-stage formalism invented in the 1950s and 1960s:\n(1) a morphological analyzer with tens of thousands of word stem entries returns all\nparts of speech for a word, then (2) a large set of thousands of constraints are applied\nto the input sentence to rule out parts of speech inconsistent with the context.\n17.7.2 POS Tagging for Morphologically Rich Languages\nAugmentations to tagging algorithms become necessary when dealing with lan-\nguages with rich morphology like Czech, Hungarian and Turkish.\nThese productive word-formation processes result in a large vocabulary for these\nlanguages: a 250,000 word token corpus of Hungarian has more than twice as many\nword types as a similarly sized corpus of English (Oravecz and Dienes, 2002), while\na 10 million word token corpus of Turkish contains four times as many word types\nas a similarly sized English corpus (Hakkani-T \u00a8ur et al., 2002). Large vocabular-\nies mean many unknown words, and these unknown words cause signi\ufb01cant per-\nformance degradations in a wide variety of languages (including Czech, Slovene,\nEstonian, and Romanian) (Haji \u02c7c, 2000).\nHighly in\ufb02ectional languages also have much more information than English\ncoded in word morphology, like case (nominative, accusative, genitive) or gender\n(masculine, feminine). Because this information is important for tasks like pars-\ning and coreference resolution, part-of-speech taggers for morphologically rich lan-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 581,
      "token_count": 348,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 391\n\n17.8 \u2022 S UMMARY 383\nguages need to label words with case and gender information. Tagsets for morpho-\nlogically rich languages are therefore sequences of morphological tags rather than a\nsingle primitive tag. Here\u2019s a Turkish example, in which the word izinhas three pos-\nsible morphological/part-of-speech tags and meanings (Hakkani-T \u00a8ur et al., 2002):\n1. Yerdeki izintemizlenmesi gerek. iz + Noun+A3sg+Pnon+Gen\nThe trace on the \ufb02oor should be cleaned.\n2.\u00a8Uzerinde parmak izinkalmis \u00b8. iz + Noun+A3sg+P2sg+Nom\nYour \ufb01nger print is left on (it).\n3. Ic \u00b8eri girmek ic \u00b8in izinalman gerekiyor. izin + Noun+A3sg+Pnon+Nom\nYou need permission to enter.\nUsing a morphological parse sequence like Noun+A3sg+Pnon+Gen as the part-\nof-speech tag greatly increases the number of parts of speech, and so tagsets can\nbe 4 to 10 times larger than the 50\u2013100 tags we have seen for English. With such\nlarge tagsets, each word needs to be morphologically analyzed to generate the list\nof possible morphological tag sequences (part-of-speech tags) for the word. The\nrole of the tagger is then to disambiguate among these tags. This method also helps\nwith unknown words since morphological parsers can accept unknown stems and\nstill segment the af\ufb01xes properly.\n17.8 Summary\nThis chapter introduced parts of speech andnamed entities , and the tasks of part-\nof-speech tagging andnamed entity recognition :\n\u2022 Languages generally have a small set of closed class words that are highly\nfrequent, ambiguous, and act as function words , and open-class words like\nnouns ,verbs ,adjectives . Various part-of-speech tagsets exist, of between 40\nand 200 tags.\n\u2022Part-of-speech tagging is the process of assigning a part-of-speech label to\neach of a sequence of words.\n\u2022Named entities are words for proper nouns referring mainly to people, places,\nand organizations, but extended to many other types that aren\u2019t strictly entities\nor even proper nouns.\n\u2022 Two common approaches to sequence modeling are a generative approach,\nHMM tagging, and a discriminative approach, CRF tagging. We will see a\nneural approach in following chapters.\n\u2022 The probabilities in HMM taggers are estimated by maximum likelihood es-\ntimation on tag-labeled training corpora. The Viterbi algorithm is used for\ndecoding , \ufb01nding the most likely tag sequence\n\u2022Conditional Random Fields orCRF taggers train a log-linear model that can\nchoose the best tag sequence given an observation sequence, based on features\nthat condition on the output tag, the prior output tag, the entire input sequence,\nand the current timestep. They use the Viterbi algorithm for inference, to\nchoose the best sequence of tags, and a version of the Forward-Backward\nalgorithm (see Appendix A) for training,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 582,
      "token_count": 686,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 392",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 583,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "384 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\nBibliographical and Historical Notes\nWhat is probably the earliest part-of-speech tagger was part of the parser in Zellig\nHarris\u2019s Transformations and Discourse Analysis Project (TDAP), implemented be-\ntween June 1958 and July 1959 at the University of Pennsylvania (Harris, 1962),\nalthough earlier systems had used part-of-speech dictionaries. TDAP used 14 hand-\nwritten rules for part-of-speech disambiguation; the use of part-of-speech tag se-\nquences and the relative frequency of tags for a word pre\ufb01gures modern algorithms.\nThe parser was implemented essentially as a cascade of \ufb01nite-state transducers; see\nJoshi and Hopely (1999) and Karttunen (1999) for a reimplementation.\nThe Computational Grammar Coder (CGC) of Klein and Simmons (1963) had\nthree components: a lexicon, a morphological analyzer, and a context disambigua-\ntor. The small 1500-word lexicon listed only function words and other irregular\nwords. The morphological analyzer used in\ufb02ectional and derivational suf\ufb01xes to as-\nsign part-of-speech classes. These were run over words to produce candidate parts\nof speech which were then disambiguated by a set of 500 context rules by relying on\nsurrounding islands of unambiguous words. For example, one rule said that between\nan ARTICLE and a VERB, the only allowable sequences were ADJ-NOUN, NOUN-\nADVERB, or NOUN-NOUN. The TAGGIT tagger (Greene and Rubin, 1971) used\nthe same architecture as Klein and Simmons (1963), with a bigger dictionary and\nmore tags (87). TAGGIT was applied to the Brown corpus and, according to Francis\nand Ku \u02c7cera (1982, p. 9), accurately tagged 77% of the corpus; the remainder of the\nBrown corpus was then tagged by hand. All these early algorithms were based on\na two-stage architecture in which a dictionary was \ufb01rst used to assign each word a\nset of potential parts of speech, and then lists of handwritten disambiguation rules\nwinnowed the set down to a single part of speech per word.\nProbabilities were used in tagging by Stolz et al. (1965) and a complete proba-\nbilistic tagger with Viterbi decoding was sketched by Bahl and Mercer (1976). The\nLancaster-Oslo/Bergen (LOB) corpus, a British English equivalent of the Brown cor-\npus, was tagged in the early 1980\u2019s with the CLAWS tagger (Marshall 1983; Mar-\nshall 1987; Garside 1987), a probabilistic algorithm that approximated a simpli\ufb01ed\nHMM tagger. The algorithm used tag bigram probabilities, but instead of storing the\nword likelihood of each tag, the algorithm marked tags either as rare (P(tagjword)<\n:01)infrequent (P(tagjword)<:10) or normally frequent (P(tagjword)>:10).\nDeRose (1988) developed a quasi-HMM algorithm, including the use of dy-\nnamic programming, although computing P(tjw)P(w)instead of P(wjt)P(w). The\nsame year, the probabilistic PARTS tagger of Church 1988, 1989 was probably the\n\ufb01rst implemented HMM tagger, described correctly in Church (1989), although",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 584,
      "token_count": 776,
      "chapter_title": ""
    }
  },
  {
    "content": "winnowed the set down to a single part of speech per word.\nProbabilities were used in tagging by Stolz et al. (1965) and a complete proba-\nbilistic tagger with Viterbi decoding was sketched by Bahl and Mercer (1976). The\nLancaster-Oslo/Bergen (LOB) corpus, a British English equivalent of the Brown cor-\npus, was tagged in the early 1980\u2019s with the CLAWS tagger (Marshall 1983; Mar-\nshall 1987; Garside 1987), a probabilistic algorithm that approximated a simpli\ufb01ed\nHMM tagger. The algorithm used tag bigram probabilities, but instead of storing the\nword likelihood of each tag, the algorithm marked tags either as rare (P(tagjword)<\n:01)infrequent (P(tagjword)<:10) or normally frequent (P(tagjword)>:10).\nDeRose (1988) developed a quasi-HMM algorithm, including the use of dy-\nnamic programming, although computing P(tjw)P(w)instead of P(wjt)P(w). The\nsame year, the probabilistic PARTS tagger of Church 1988, 1989 was probably the\n\ufb01rst implemented HMM tagger, described correctly in Church (1989), although\nChurch (1988) also described the computation incorrectly as P(tjw)P(w)instead\nofP(wjt)P(w). Church (p.c.) explained that he had simpli\ufb01ed for pedagogical pur-\nposes because using the probability P(tjw)made the idea seem more understandable\nas \u201cstoring a lexicon in an almost standard form\u201d.\nLater taggers explicitly introduced the use of the hidden Markov model (Kupiec\n1992; Weischedel et al. 1993; Sch \u00a8utze and Singer 1994). Merialdo (1994) showed\nthat fully unsupervised EM didn\u2019t work well for the tagging task and that reliance\non hand-labeled data was important. Charniak et al. (1993) showed the importance\nof the most frequent tag baseline; the 92.3% number we give above was from Abney\net al. (1999). See Brants (2000) for HMM tagger implementation details, includ-\ning the extension to trigram contexts, and the use of sophisticated unknown word\nfeatures; its performance is still close to state of the art taggers.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 585,
      "token_count": 537,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 393",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 586,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "EXERCISES 385\nLog-linear models for POS tagging were introduced by Ratnaparkhi (1996),\nwho introduced a system called MXPOST which implemented a maximum entropy\nMarkov model (MEMM), a slightly simpler version of a CRF. Around the same\ntime, sequence labelers were applied to the task of named entity tagging, \ufb01rst with\nHMMs (Bikel et al., 1997) and MEMMs (McCallum et al., 2000), and then once\nCRFs were developed (Lafferty et al. 2001), they were also applied to NER (Mc-\nCallum and Li, 2003). A wide exploration of features followed (Zhou et al., 2005).\nNeural approaches to NER mainly follow from the pioneering results of Collobert\net al. (2011), who applied a CRF on top of a convolutional net. BiLSTMs with word\nand character-based embeddings as input followed shortly and became a standard\nneural algorithm for NER (Huang et al. 2015, Ma and Hovy 2016, Lample et al.\n2016) followed by the more recent use of Transformers and BERT.\nThe idea of using letter suf\ufb01xes for unknown words is quite old; the early Klein\nand Simmons (1963) system checked all \ufb01nal letter suf\ufb01xes of lengths 1-5. The un-\nknown word features described on page 378 come mainly from Ratnaparkhi (1996),\nwith augmentations from Toutanova et al. (2003) and Manning (2011).\nState of the art POS taggers use neural algorithms, either bidirectional RNNs or\nTransformers like BERT; see Chapter 8 to Chapter 11. HMM (Brants 2000; Thede\nand Harper 1999) and CRF tagger accuracies are likely just a tad lower.\nManning (2011) investigates the remaining 2.7% of errors in a high-performing\ntagger (Toutanova et al., 2003). He suggests that a third or half of these remaining\nerrors are due to errors or inconsistencies in the training data, a third might be solv-\nable with richer linguistic models, and for the remainder the task is underspeci\ufb01ed\nor unclear.\nSupervised tagging relies heavily on in-domain training data hand-labeled by\nexperts. Ways to relax this assumption include unsupervised algorithms for cluster-\ning words into part-of-speech-like classes, summarized in Christodoulopoulos et al.\n(2010), and ways to combine labeled and unlabeled data, for example by co-training\n(Clark et al. 2003; S\u00f8gaard 2010).\nSee Householder (1995) for historical notes on parts of speech, and Sampson\n(1987) and Garside et al. (1997) on the provenance of the Brown and other tagsets.\nExercises\n17.1 Find one tagging error in each of the following sentences that are tagged with\nthe Penn Treebank tagset:\n1. I/PRP need/VBP a/DT \ufb02ight/NN from/IN Atlanta/NN\n2. Does/VBZ this/DT \ufb02ight/NN serve/VB dinner/NNS\n3. I/PRP have/VB a/DT friend/NN living/VBG in/IN Denver/NNP\n4. Can/VBP you/PRP list/VB the/DT nonstop/JJ afternoon/NN \ufb02ights/NNS\n17.2 Use the Penn Treebank tagset to tag each word in the following sentences",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 587,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "or unclear.\nSupervised tagging relies heavily on in-domain training data hand-labeled by\nexperts. Ways to relax this assumption include unsupervised algorithms for cluster-\ning words into part-of-speech-like classes, summarized in Christodoulopoulos et al.\n(2010), and ways to combine labeled and unlabeled data, for example by co-training\n(Clark et al. 2003; S\u00f8gaard 2010).\nSee Householder (1995) for historical notes on parts of speech, and Sampson\n(1987) and Garside et al. (1997) on the provenance of the Brown and other tagsets.\nExercises\n17.1 Find one tagging error in each of the following sentences that are tagged with\nthe Penn Treebank tagset:\n1. I/PRP need/VBP a/DT \ufb02ight/NN from/IN Atlanta/NN\n2. Does/VBZ this/DT \ufb02ight/NN serve/VB dinner/NNS\n3. I/PRP have/VB a/DT friend/NN living/VBG in/IN Denver/NNP\n4. Can/VBP you/PRP list/VB the/DT nonstop/JJ afternoon/NN \ufb02ights/NNS\n17.2 Use the Penn Treebank tagset to tag each word in the following sentences\nfrom Damon Runyon\u2019s short stories. You may ignore punctuation. Some of\nthese are quite dif\ufb01cult; do your best.\n1. It is a nice night.\n2. This crap game is over a garage in Fifty-second Street. . .\n3. . . . Nobody ever takes the newspapers she sells . . .\n4. He is a tall, skinny guy with a long, sad, mean-looking kisser, and a\nmournful voice.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 588,
      "token_count": 380,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 394\n\n386 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\n5. . . . I am sitting in Mindy\u2019s restaurant putting on the ge\ufb01llte \ufb01sh, which is\na dish I am very fond of, . . .\n6. When a guy and a doll get to taking peeks back and forth at each other,\nwhy there you are indeed.\n17.3 Now compare your tags from the previous exercise with one or two friend\u2019s\nanswers. On which words did you disagree the most? Why?\n17.4 Implement the \u201cmost likely tag\u201d baseline. Find a POS-tagged training set,\nand use it to compute for each word the tag that maximizes p(tjw). You will\nneed to implement a simple tokenizer to deal with sentence boundaries. Start\nby assuming that all unknown words are NN and compute your error rate on\nknown and unknown words. Now write at least \ufb01ve rules to do a better job of\ntagging unknown words, and show the difference in error rates.\n17.5 Build a bigram HMM tagger. You will need a part-of-speech-tagged corpus.\nFirst split the corpus into a training set and test set. From the labeled training\nset, train the transition and observation probabilities of the HMM tagger di-\nrectly on the hand-tagged data. Then implement the Viterbi algorithm so you\ncan decode a test sentence. Now run your algorithm on the test set. Report its\nerror rate and compare its performance to the most frequent tag baseline.\n17.6 Do an error analysis of your tagger. Build a confusion matrix and investigate\nthe most frequent errors. Propose some features for improving the perfor-\nmance of your tagger on these errors.\n17.7 Develop a set of regular expressions to recognize the character shape features\ndescribed on page 378.\n17.8 The BIO and other labeling schemes given in this chapter aren\u2019t the only\npossible one. For example, the Btag can be reserved only for those situations\nwhere an ambiguity exists between adjacent entities. Propose a new set of\nBIOtags for use with your NER system. Experiment with it and compare its\nperformance with the schemes presented in this chapter.\n17.9 Names of works of art (books, movies, video games, etc.) are quite different\nfrom the kinds of named entities we\u2019ve discussed in this chapter. Collect a\nlist of names of works of art from a particular category from a Web-based\nsource (e.g., gutenberg.org, amazon.com, imdb.com, etc.). Analyze your list\nand give examples of ways that the names in it are likely to be problematic for\nthe techniques described in this chapter.\n17.10 Develop an NER system speci\ufb01c to the category of names that you collected\nin the last exercise. Evaluate your system on a collection of text likely to\ncontain instances of these named entities.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 589,
      "token_count": 621,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 395\n\nCHAPTER\n18Context-Free Grammars and\nConstituency Parsing\nBecause the Night by Bruce Springsteen and Patti Smith\nThe Fire Next Time by James Baldwin\nIf on a winter\u2019s night a traveler by Italo Calvino\nLove Actually by Richard Curtis\nSuddenly Last Summer by Tennessee Williams\nA Scanner Darkly by Philip K. Dick\nSix titles that are not constituents, from Geoffrey K. Pullum on\nLanguage Log (who was pointing out their incredible rarity).\nOne morning I shot an elephant in my pajamas.\nHow he got into my pajamas I don\u2019t know.\nGroucho Marx, Animal Crackers , 1930\nThe study of grammar has an ancient pedigree. The grammar of Sanskrit was\ndescribed by the Indian grammarian P \u00afan.ini sometime between the 7th and 4th cen-\nturies BCE, in his famous treatise the As .t.\u00afadhy \u00afay\u00af\u0131 (\u20188 books\u2019). And our word syntax syntax\ncomes from the Greek s\u00b4yntaxis , meaning \u201csetting out together or arrangement\u201d, and\nrefers to the way words are arranged together. We have seen syntactic notions in pre-\nvious chapters like the use of part-of-speech categories (Chapter 17). In this chapter\nand the next one we introduce formal models for capturing more sophisticated no-\ntions of grammatical structure and algorithms for parsing these structures.\nOur focus in this chapter is context-free grammars and the CKY algorithm\nfor parsing them. Context-free grammars are the backbone of many formal mod-\nels of the syntax of natural language (and, for that matter, of computer languages).\nSyntactic parsing is the task of assigning a syntactic structure to a sentence. Parse\ntrees (whether for context-free grammars or for the dependency or CCG formalisms\nwe introduce in following chapters) can be used in applications such as grammar\nchecking : sentence that cannot be parsed may have grammatical errors (or at least\nbe hard to read). Parse trees can be an intermediate stage of representation for for-\nmal semantic analysis . And parsers and the grammatical structure they assign a\nsentence are a useful text analysis tool for text data science applications that require\nmodeling the relationship of elements in sentences.\nIn this chapter we introduce context-free grammars, give a small sample gram-\nmar of English, introduce more formal de\ufb01nitions of context-free grammars and\ngrammar normal form, and talk about treebanks : corpora that have been anno-\ntated with syntactic structure. We then discuss parse ambiguity and the problems\nit presents, and turn to parsing itself, giving the famous Cocke-Kasami-Younger\n(CKY) algorithm (Kasami 1965, Younger 1967), the standard dynamic program-\nming approach to syntactic parsing. The CKY algorithm returns an ef\ufb01cient repre-\nsentation of the set of parse trees for a sentence, but doesn\u2019t tell us which parse tree\nis the right one. For that, we need to augment CKY with scores for each possible\nconstituent. We\u2019ll see how to do this with neural span-based parsers. Finally, we\u2019ll\nintroduce the standard set of metrics for evaluating parser accuracy.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 590,
      "token_count": 683,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 396\n\n388 CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\n18.1 Constituency\nSyntactic constituency is the idea that groups of words can behave as single units,\nor constituents. Part of developing a grammar involves building an inventory of the\nconstituents in the language. How do words group together in English? Consider\nthenoun phrase , a sequence of words surrounding at least one noun. Here are some noun phrase\nexamples of noun phrases (thanks to Damon Runyon):\nHarry the Horse a high-class spot such as Mindy\u2019s\nthe Broadway coppers the reason he comes into the Hot Box\nthey three parties from Brooklyn\nWhat evidence do we have that these words group together (or \u201cform constituents\u201d)?\nOne piece of evidence is that they can all appear in similar syntactic environments,\nfor example, before a verb.\nthree parties from Brooklyn arrive . . .\na high-class spot such as Mindy\u2019s attracts . . .\nthe Broadway coppers love. . .\nthey sit\nBut while the whole noun phrase can occur before a verb, this is not true of each\nof the individual words that make up a noun phrase. The following are not grammat-\nical sentences of English (recall that we use an asterisk (*) to mark fragments that\nare not grammatical English sentences):\n*from arrive . . .*asattracts . . .\n*the is. . . *spot sat. . .\nThus, to correctly describe facts about the ordering of these words in English, we\nmust be able to say things like \u201c Noun Phrases can occur before verbs \u201d. Let\u2019s now\nsee how to do this in a more formal way!\n18.2 Context-Free Grammars\nA widely used formal system for modeling constituent structure in natural lan-\nguage is the context-free grammar , orCFG . Context-free grammars are also called CFG\nphrase-structure grammars , and the formalism is equivalent to Backus-Naur form ,\norBNF . The idea of basing a grammar on constituent structure dates back to the psy-\nchologist Wilhelm Wundt (1900) but was not formalized until Chomsky (1956) and,\nindependently, Backus (1959).\nA context-free grammar consists of a set of rules orproductions , each of which rules\nexpresses the ways that symbols of the language can be grouped and ordered to-\ngether, and a lexicon of words and symbols. For example, the following productions lexicon\nexpress that an NP(ornoun phrase ) can be composed of either a ProperNoun or NP\na determiner ( Det) followed by a Nominal ; aNominal in turn can consist of one or",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 591,
      "token_count": 568,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 397\n\n18.2 \u2022 C ONTEXT -FREE GRAMMARS 389\nmore Noun s.1\nNP!Det Nominal\nNP!ProperNoun\nNominal!NounjNominal Noun\nContext-free rules can be hierarchically embedded, so we can combine the previous\nrules with others, like the following, that express facts about the lexicon:\nDet!a\nDet!the\nNoun!\ufb02ight\nThe symbols that are used in a CFG are divided into two classes. The symbols\nthat correspond to words in the language (\u201cthe\u201d, \u201cnightclub\u201d) are called terminal terminal\nsymbols; the lexicon is the set of rules that introduce these terminal symbols. The\nsymbols that express abstractions over these terminals are called non-terminals . In non-terminal\neach context-free rule, the item to the right of the arrow ( !) is an ordered list of one\nor more terminals and non-terminals; to the left of the arrow is a single non-terminal\nsymbol expressing some cluster or generalization. The non-terminal associated with\neach word in the lexicon is its lexical category, or part of speech.\nA CFG can be thought of in two ways: as a device for generating sentences\nand as a device for assigning a structure to a given sentence. Viewing a CFG as a\ngenerator, we can read the !arrow as \u201crewrite the symbol on the left with the string\nof symbols on the right\u201d.\nSo starting from the symbol: NP\nwe can use our \ufb01rst rule to rewrite NPas: Det Nominal\nand then rewrite Nominal as: Noun\nand \ufb01nally rewrite these parts-of-speech as: a \ufb02ight\nWe say the string a \ufb02ight can be derived from the non-terminal NP. Thus, a CFG\ncan be used to generate a set of strings. This sequence of rule expansions is called a\nderivation of the string of words. It is common to represent a derivation by a parse derivation\ntree (commonly shown inverted with the root at the top). Figure 18.1 shows the tree parse tree\nrepresentation of this derivation.\nNP\nNom\nNoun\n\ufb02ightDet\na\nFigure 18.1 A parse tree for \u201ca \ufb02ight\u201d.\nIn the parse tree shown in Fig. 18.1, we can say that the node NPdominates dominates\nall the nodes in the tree ( Det,Nom ,Noun ,a,\ufb02ight ). We can say further that it\nimmediately dominates the nodes DetandNom .\nThe formal language de\ufb01ned by a CFG is the set of strings that are derivable\nfrom the designated start symbol . Each grammar must have one designated start start symbol\n1When talking about these rules we can pronounce the rightarrow !as \u201cgoes to\u201d, and so we might\nread the \ufb01rst rule above as \u201cNP goes to Det Nominal\u201d.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 592,
      "token_count": 624,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 398\n\n390 CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\nsymbol, which is often called S. Since context-free grammars are often used to de\ufb01ne\nsentences, Sis usually interpreted as the \u201csentence\u201d node, and the set of strings that\nare derivable from Sis the set of sentences in some simpli\ufb01ed version of English.\nLet\u2019s add a few additional rules to our inventory. The following rule expresses\nthe fact that a sentence can consist of a noun phrase followed by a verb phrase : verb phrase\nS!NP VP I prefer a morning \ufb02ight\nA verb phrase in English consists of a verb followed by assorted other things;\nfor example, one kind of verb phrase consists of a verb followed by a noun phrase:\nVP!Verb NP prefer a morning \ufb02ight\nOr the verb may be followed by a noun phrase and a prepositional phrase:\nVP!Verb NP PP leave Boston in the morning\nOr the verb phrase may have a verb followed by a prepositional phrase alone:\nVP!Verb PP leaving on Thursday\nA prepositional phrase generally has a preposition followed by a noun phrase.\nFor example, a common type of prepositional phrase in the ATIS corpus is used to\nindicate location or direction:\nPP!Preposition NP from Los Angeles\nTheNPinside a PPneed not be a location; PPs are often used with times and\ndates, and with other nouns as well; they can be arbitrarily complex. Here are ten\nexamples from the ATIS corpus:\nto Seattle on these \ufb02ights\nin Minneapolis about the ground transportation in Chicago\non Wednesday of the round trip \ufb02ight on United Airlines\nin the evening of the AP \ufb01fty seven \ufb02ight\non the ninth of July with a stopover in Nashville\nFigure 18.2 gives a sample lexicon, and Fig. 18.3 summarizes the grammar rules\nwe\u2019ve seen so far, which we\u2019ll call L0. Note that we can use the or-symbol jto\nindicate that a non-terminal has alternate possible expansions.\nNoun!\ufb02ightsj\ufb02ightjbreezejtripjmorning\nVerb!isjpreferjlikejneedjwantj\ufb02yjdo\nAdjective!cheapestjnon-stopj\ufb01rstjlatest\njotherjdirect\nPronoun!mejIjyoujit\nProper-Noun!AlaskajBaltimorejLos Angeles\njChicagojUnitedjAmerican\nDeterminer!thejajanjthisjthesejthat\nPreposition!fromjtojonjnearjin\nConjunction!andjorjbut\nFigure 18.2 The lexicon for L0.\nWe can use this grammar to generate sentences of this \u201cATIS-language\u201d. We\nstart with S, expand it to NP VP , then choose a random expansion of NP(let\u2019s say, to",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 593,
      "token_count": 628,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 399\n\n18.2 \u2022 C ONTEXT -FREE GRAMMARS 391\nGrammar Rules Examples\nS!NP VP I + want a morning \ufb02ight\nNP!Pronoun I\njProper-Noun Los Angeles\njDet Nominal a + \ufb02ight\nNominal!Nominal Noun morning + \ufb02ight\njNoun \ufb02ights\nVP!Verb do\njVerb NP want + a \ufb02ight\njVerb NP PP leave + Boston + in the morning\njVerb PP leaving + on Thursday\nPP!Preposition NP from + Los Angeles\nFigure 18.3 The grammar for L0, with example phrases for each rule.\nS\nVP\nNP\nNom\nNoun\n\ufb02ightNom\nNoun\nmorningDet\naVerb\npreferNP\nPro\nI\nFigure 18.4 The parse tree for \u201cI prefer a morning \ufb02ight\u201d according to grammar L0.\nI), and a random expansion of VP(let\u2019s say, to Verb NP ), and so on until we generate\nthe string I prefer a morning \ufb02ight . Figure 18.4 shows a parse tree that represents a\ncomplete derivation of I prefer a morning \ufb02ight .\nWe can also represent a parse tree in a more compact format called bracketed\nnotation ; here is the bracketed representation of the parse tree of Fig. 18.4:bracketed\nnotation\n(18.1) [ S[NP[ProI]] [VP[Vprefer] [ NP[Deta] [Nom [Nmorning] [ Nom [N\ufb02ight]]]]]]\nA CFG like that of L0de\ufb01nes a formal language. Sentences (strings of words)\nthat can be derived by a grammar are in the formal language de\ufb01ned by that gram-\nmar, and are called grammatical sentences. Sentences that cannot be derived by grammatical\na given formal grammar are not in the language de\ufb01ned by that grammar and are\nreferred to as ungrammatical . This hard line between \u201cin\u201d and \u201cout\u201d characterizes ungrammatical\nall formal languages but is only a very simpli\ufb01ed model of how natural languages\nreally work. This is because determining whether a given sentence is part of a given\nnatural language (say, English) often depends on the context. In linguistics, the use\nof formal languages to model natural languages is called generative grammar sincegenerative\ngrammar\nthe language is de\ufb01ned by the set of possible sentences \u201cgenerated\u201d by the grammar.\n(Note that this is a different sense of the word \u2018generate\u2019 than when we talk about",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 594,
      "token_count": 571,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 400\n\n392 CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\nlanguage models generating text.)\n18.2.1 Formal De\ufb01nition of Context-Free Grammar\nWe conclude this section with a quick, formal description of a context-free gram-\nmar and the language it generates. A context-free grammar Gis de\ufb01ned by four\nparameters: N;S;R;S(technically it is a \u201c4-tuple\u201d).\nNa set of non-terminal symbols (orvariables )\nSa set of terminal symbols (disjoint from N)\nRa set of rules or productions, each of the form A!b,\nwhere Ais a non-terminal,\nbis a string of symbols from the in\ufb01nite set of strings (S[N)\u0003\nSa designated start symbol and a member of N\nFor the remainder of the book we adhere to the following conventions when dis-\ncussing the formal properties of context-free grammars (as opposed to explaining\nparticular facts about English or other languages).\nCapital letters like A,B, and S Non-terminals\nS The start symbol\nLower-case Greek letters like a,b, and g Strings drawn from (S[N)\u0003\nLower-case Roman letters like u,v, and w Strings of terminals\nA language is de\ufb01ned through the concept of derivation. One string derives an-\nother one if it can be rewritten as the second one by some series of rule applications.\nMore formally, following Hopcroft and Ullman (1979),\nifA!bis a production of Randaandgare any strings in the set\n(S[N)\u0003, then we say that aAgdirectly derives abg , oraAg)abg . directly derives\nDerivation is then a generalization of direct derivation:\nLeta1;a2;:::; ambe strings in (S[N)\u0003;m\u00151, such that\na1)a2;a2)a3;:::;am\u00001)am\nWe say that a1derives am, ora1\u0003)am. derives\nWe can then formally de\ufb01ne the language LGgenerated by a grammar Gas the\nset of strings composed of terminal symbols that can be derived from the designated\nstart symbol S.\nLG=fwjwis inS\u0003andS\u0003)wg\nThe problem of mapping from a string of words to its parse tree is called syn-\ntactic parsing , as we\u2019ll see in Section 18.6.syntactic\nparsing\n18.3 Treebanks\nA corpus in which every sentence is annotated with a parse tree is called a treebank . treebank",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 595,
      "token_count": 550,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 401\n\n18.3 \u2022 T REEBANKS 393\nTreebanks play an important role in parsing as well as in linguistic investigations of\nsyntactic phenomena.\nTreebanks are generally made by running a parser over each sentence and then\nhaving the resulting parse hand-corrected by human linguists. Figure 18.5 shows\nsentences from the Penn Treebank project, which includes various treebanks in Penn Treebank\nEnglish, Arabic, and Chinese. The Penn Treebank part-of-speech tagset was de\ufb01ned\nin Chapter 17, but we\u2019ll see minor formatting differences across treebanks. The use\nof LISP-style parenthesized notation for trees is extremely common and resembles\nthe bracketed notation we saw earlier in (18.1). For those who are not familiar with\nit we show a standard node-and-line tree representation in Fig. 18.6.\n((S\n(NP-SBJ (DT That)\n(JJ cold) (, ,)\n(JJ empty) (NN sky) )\n(VP (VBD was)\n(ADJP-PRD (JJ full)\n(PP (IN of)\n(NP (NN fire)\n(CC and)\n(NN light) ))))\n(. .) ))((S\n(NP-SBJ The/DT flight/NN )\n(VP should/MD\n(VP arrive/VB\n(PP-TMP at/IN\n(NP eleven/CD a.m/RB ))\n(NP-TMP tomorrow/NN )))))\n(a) (b)\nFigure 18.5 Parses from the LDC Treebank3 for (a) Brown and (b) ATIS sentences.\nS\n.\n.VP\nADJP-PRD\nPP\nNP\nNN\nlightCC\nandNN\n\ufb01reIN\nofJJ\nfullVBD\nwasNP-SBJ\nNN\nskyJJ\nempty,\n,JJ\ncoldDT\nThat\nFigure 18.6 The tree corresponding to the Brown corpus sentence in the previous \ufb01gure.\nThe sentences in a treebank implicitly constitute a grammar of the language. For\nexample, from the parsed sentences in Fig. 18.5 we can extract the CFG rules shown\nin Fig. 18.7 (with rule suf\ufb01xes ( -SBJ ) stripped for simplicity). The grammar used\nto parse the Penn Treebank is very \ufb02at, resulting in very many rules. For example,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 596,
      "token_count": 505,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 402\n\n394 CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\nGrammar Lexicon\nS!NP VP . DT!thejthat\nS!NP VP JJ!coldjemptyjfull\nNP!CD RB NN!skyj\ufb01rejlightj\ufb02ightjtomorrow\nNP!DT NN CC!and\nNP!NN CC NN IN!ofjat\nNP!DT JJ , JJ NN CD!eleven\nNP!NN RB!a.m.\nVP!MD VP VB!arrive\nVP!VBD ADJP VBD!wasjsaid\nVP!MD VP MD!shouldjwould\nVP!VB PP NP\nADJP!JJ PP\nPP!IN NP\nFigure 18.7 CFG grammar rules and lexicon from the treebank sentences in Fig. 18.5.\namong the approximately 4,500 different rules for expanding VPs are separate rules\nfor PP sequences of any length and every possible arrangement of verb arguments:\nVP!VBD PP\nVP!VBD PP PP\nVP!VBD PP PP PP\nVP!VBD PP PP PP PP\nVP!VB ADVP PP\nVP!VB PP ADVP\nVP!ADVP VB PP\n18.4 Grammar Equivalence and Normal Form\nA formal language is de\ufb01ned as a (possibly in\ufb01nite) set of strings of words. This sug-\ngests that we could ask if two grammars are equivalent by asking if they generate the\nsame set of strings. In fact, it is possible to have two distinct context-free grammars\ngenerate the same language. We say that two grammars are strongly equivalent ifstrongly\nequivalent\nthey generate the same set of strings andif they assign the same phrase structure\nto each sentence (allowing merely for renaming of the non-terminal symbols). Two\ngrammars are weakly equivalent if they generate the same set of strings but do notweakly\nequivalent\nassign the same phrase structure to each sentence.\nIt is sometimes useful to have a normal form for grammars, in which each of normal form\nthe productions takes a particular form. For example, a context-free grammar is in\nChomsky normal form (CNF) (Chomsky, 1963) if it is \u000f-free and if in additionChomsky\nnormal form\neach production is either of the form A!B C orA!a. That is, the right-hand side\nof each rule either has two non-terminal symbols or one terminal symbol. Chomsky\nnormal form grammars are binary branching , that is they have binary trees (downbinary\nbranching\nto the prelexical nodes). We make use of this binary branching property in the CKY\nparsing algorithm in Section 18.6.\nAny context-free grammar can be converted into a weakly equivalent Chomsky\nnormal form grammar. For example, a rule of the form\nA!B C D\ncan be converted into the following two CNF rules (Exercise 18.1 asks the reader to",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 597,
      "token_count": 652,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 403\n\n18.5 \u2022 A MBIGUITY 395\nGrammar Lexicon\nS!NP VP Det!thatjthisjtheja\nS!Aux NP VP Noun!bookj\ufb02ightjmealjmoney\nS!VP Verb!bookjincludejprefer\nNP!Pronoun Pronoun!Ijshejme\nNP!Proper-Noun Proper-Noun!HoustonjUnited\nNP!Det Nominal Aux!does\nNominal!Noun Preposition!fromjtojonjnearjthrough\nNominal!Nominal Noun\nNominal!Nominal PP\nVP!Verb\nVP!Verb NP\nVP!Verb NP PP\nVP!Verb PP\nVP!VP PP\nPP!Preposition NP\nFigure 18.8 TheL1miniature English grammar and lexicon.\nformulate the complete algorithm):\nA!B X\nX!C D\nSometimes using binary branching can actually produce smaller grammars. For\nexample, the sentences that might be characterized as\nVP -> VBD NP PP*\nare represented in the Penn Treebank by this series of rules:\nVP!VBD NP PP\nVP!VBD NP PP PP\nVP!VBD NP PP PP PP\nVP!VBD NP PP PP PP PP\n...\nbut could also be generated by the following two-rule grammar:\nVP!VBD NP PP\nVP!VP PP\nThe generation of a symbol A with a potentially in\ufb01nite sequence of symbols B with\na rule of the form A!A Bis known as Chomsky-adjunction .Chomsky-\nadjunction\n18.5 Ambiguity\nAmbiguity is the most serious problem faced by syntactic parsers. Chapter 17 intro-\nduced the notions of part-of-speech ambiguity andpart-of-speech disambigua-\ntion. Here, we introduce a new kind of ambiguity, called structural ambiguity ,structural\nambiguity\nillustrated with a new toy grammar L1, shown in Figure 18.8, which adds a few\nrules to the L0grammar.\nStructural ambiguity occurs when the grammar can assign more than one parse\nto a sentence. Groucho Marx\u2019s well-known line as Captain Spaulding in Animal",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 598,
      "token_count": 470,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 404\n\n396 CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\nS\nVP\nNP\nNominal\nPP\nin my pajamasNominal\nNoun\nelephantDet\nanVerb\nshotNP\nPronoun\nIS\nVP\nPP\nin my pajamasVP\nNP\nNominal\nNoun\nelephantDet\nanVerb\nshotNP\nPronoun\nI\nFigure 18.9 Two parse trees for an ambiguous sentence. The parse on the left corresponds to the humorous\nreading in which the elephant is in the pajamas, the parse on the right corresponds to the reading in which\nCaptain Spaulding did the shooting in his pajamas.\nCrackers is ambiguous because the phrase in my pajamas can be part of the NP\nheaded by elephant or a part of the verb phrase headed by shot. Figure 18.9 illus-\ntrates these two analyses of Marx\u2019s line using rules from L1.\nStructural ambiguity, appropriately enough, comes in many forms. Two common\nkinds of ambiguity are attachment ambiguity andcoordination ambiguity . A\nsentence has an attachment ambiguity if a particular constituent can be attached toattachment\nambiguity\nthe parse tree at more than one place. The Groucho Marx sentence is an example\nofPP-attachment ambiguity : the preposition phrase can be attached either as partPP-attachment\nambiguity\nof the NP or as part of the VP. Various kinds of adverbial phrases are also subject\nto this kind of ambiguity. For instance, in the following example the gerundive-VP\n\ufb02ying to Paris can be part of a gerundive sentence whose subject is the Eiffel Tower\nor it can be an adjunct modifying the VP headed by saw:\n(18.2) We saw the Eiffel Tower \ufb02ying to Paris.\nIncoordination ambiguity phrases can be conjoined by a conjunction like and.coordination\nambiguity\nFor example, the phrase old men and women can be bracketed as [old [men and\nwomen]] , referring to old men andold women , or as [old men] and [women] , in\nwhich case it is only the men who are old. These ambiguities combine in complex\nways in real sentences, like the following news sentence from the Brown corpus:\n(18.3) President Kennedy today pushed aside other White House business to\ndevote all his time and attention to working on the Berlin crisis address he\nwill deliver tomorrow night to the American people over nationwide\ntelevision and radio.\nThis sentence has a number of ambiguities, although since they are semantically\nunreasonable, it requires a careful reading to see them. The last noun phrase could be\nparsed [nationwide [television and radio]] or[[nationwide television] and radio] .\nThe direct object of pushed aside should be other White House business but could\nalso be the bizarre phrase [other White House business to devote all his time and\nattention to working] (i.e., a structure like Kennedy af\ufb01rmed [his intention to propose\na new budget to address the de\ufb01cit] ). Then the phrase on the Berlin crisis address he",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 599,
      "token_count": 677,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 405\n\n18.6 \u2022 CKY P ARSING : A D YNAMIC PROGRAMMING APPROACH 397\nwill deliver tomorrow night to the American people could be an adjunct modifying\nthe verb pushed . APPlikeover nationwide television and radio could be attached\nto any of the higher VPs orNPs (e.g., it could modify people ornight ).\nThe fact that there are many grammatically correct but semantically unreason-\nable parses for naturally occurring sentences is an irksome problem that affects all\nparsers. Fortunately, the CKY algorithm below is designed to ef\ufb01ciently handle\nstructural ambiguities. And as we\u2019ll see in the following section, we can augment\nCKY with neural methods to choose a single correct parse by syntactic disambigua-\ntion.syntactic\ndisambiguation\n18.6 CKY Parsing: A Dynamic Programming Approach\nDynamic programming provides a powerful framework for addressing the prob-\nlems caused by ambiguity in grammars. Recall that a dynamic programming ap-\nproach systematically \ufb01lls in a table of solutions to subproblems. The complete\ntable has the solution to all the subproblems needed to solve the problem as a whole.\nIn the case of syntactic parsing, these subproblems represent parse trees for all the\nconstituents detected in the input.\nThe dynamic programming advantage arises from the context-free nature of our\ngrammar rules\u2014once a constituent has been discovered in a segment of the input we\ncan record its presence and make it available for use in any subsequent derivation\nthat might require it. This provides both time and storage ef\ufb01ciencies since subtrees\ncan be looked up in a table, not reanalyzed. This section presents the Cocke-Kasami-\nYounger (CKY) algorithm, the most widely used dynamic-programming based ap-\nproach to parsing. Chart parsing (Kaplan 1973, Kay 1982) is a related approach,\nand dynamic programming methods are often referred to as chart parsing methods. chart parsing\n18.6.1 Conversion to Chomsky Normal Form\nThe CKY algorithm requires grammars to \ufb01rst be in Chomsky Normal Form (CNF).\nRecall from Section 18.4 that grammars in CNF are restricted to rules of the form\nA!B CorA!w. That is, the right-hand side of each rule must expand either to\ntwo non-terminals or to a single terminal. Restricting a grammar to CNF does not\nlead to any loss in expressiveness, since any context-free grammar can be converted\ninto a corresponding CNF grammar that accepts exactly the same set of strings as\nthe original grammar.\nLet\u2019s start with the process of converting a generic CFG into one represented in\nCNF. Assuming we\u2019re dealing with an \u000f-free grammar, there are three situations we\nneed to address in any generic grammar: rules that mix terminals with non-terminals\non the right-hand side, rules that have a single non-terminal on the right-hand side,\nand rules in which the length of the right-hand side is greater than 2.\nThe remedy for rules that mix terminals and non-terminals is to simply introduce\na new dummy non-terminal that covers only the original terminal. For example, a\nrule for an in\ufb01nitive verb phrase such as INF-VP!to VP would be replaced by the\ntwo rules INF-VP!TO VP andTO!to.\nRules with a single non-terminal on the right are called unit productions . WeUnit\nproductions\ncan eliminate unit productions by rewriting the right-hand side of the original rules\nwith the right-hand side of all the non-unit production rules that they ultimately lead\nto. More formally, if A\u0003)Bby a chain of one or more unit productions and B!g",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 600,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 406\n\n398 CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\nis a non-unit production in our grammar, then we add A!gfor each such rule in\nthe grammar and discard all the intervening unit productions. As we demonstrate\nwith our toy grammar, this can lead to a substantial \ufb02attening of the grammar and a\nconsequent promotion of terminals to fairly high levels in the resulting trees.\nRules with right-hand sides longer than 2 are normalized through the introduc-\ntion of new non-terminals that spread the longer sequences over several new rules.\nFormally, if we have a rule like\nA!B Cg\nwe replace the leftmost pair of non-terminals with a new non-terminal and introduce\na new production, resulting in the following new rules:\nA!X1g\nX1!B C\nIn the case of longer right-hand sides, we simply iterate this process until the of-\nfending rule has been replaced by rules of length 2. The choice of replacing the\nleftmost pair of non-terminals is purely arbitrary; any systematic scheme that results\nin binary rules would suf\ufb01ce.\nIn our current grammar, the rule S!Aux NP VP would be replaced by the two\nrules S!X1 VP andX1!Aux NP .\nThe entire conversion process can be summarized as follows:\n1. Copy all conforming rules to the new grammar unchanged.\n2. Convert terminals within rules to dummy non-terminals.\n3. Convert unit productions.\n4. Make all rules binary and add them to new grammar.\nFigure 18.10 shows the results of applying this entire conversion procedure to\ntheL1grammar introduced earlier on page 395. Note that this \ufb01gure doesn\u2019t show\nthe original lexical rules; since these original lexical rules are already in CNF, they\nall carry over unchanged to the new grammar. Figure 18.10 does, however, show\nthe various places where the process of eliminating unit productions has, in effect,\ncreated new lexical rules. For example, all the original verbs have been promoted to\nboth VPs and to Ss in the converted grammar.\n18.6.2 CKY Recognition\nWith our grammar now in CNF, each non-terminal node above the part-of-speech\nlevel in a parse tree will have exactly two daughters. A two-dimensional matrix can\nbe used to encode the structure of an entire tree. For a sentence of length n, we will\nwork with the upper-triangular portion of an (n+1)\u0002(n+1)matrix. Each cell [i;j]\nin this matrix contains the set of non-terminals that represent all the constituents that\nspan positions ithrough jof the input. Since our indexing scheme begins with 0, it\u2019s\nnatural to think of the indexes as pointing at the gaps between the input words (as in\n0Book 1that 2\ufb02ight 3). These gaps are often called fenceposts , on the metaphor of fenceposts\nthe posts between segments of fencing. It follows then that the cell that represents\nthe entire input resides in position [0;n]in the matrix.\nSince each non-terminal entry in our table has two daughters in the parse, it fol-\nlows that for each constituent represented by an entry [i;j], there must be a position\nin the input, k, where it can be split into two parts such that i<k<j. Given such",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 601,
      "token_count": 728,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 407\n\n18.6 \u2022 CKY P ARSING : A D YNAMIC PROGRAMMING APPROACH 399\nL1Grammar L1in CNF\nS!NP VP S!NP VP\nS!Aux NP VP S!X1 VP\nX1!Aux NP\nS!VP S!bookjincludejprefer\nS!Verb NP\nS!X2 PP\nS!Verb PP\nS!VP PP\nNP!Pronoun NP!Ijshejme\nNP!Proper-Noun NP!UnitedjHouston\nNP!Det Nominal NP!Det Nominal\nNominal!Noun Nominal!bookj\ufb02ightjmealjmoney\nNominal!Nominal Noun Nominal!Nominal Noun\nNominal!Nominal PP Nominal!Nominal PP\nVP!Verb VP!bookjincludejprefer\nVP!Verb NP VP!Verb NP\nVP!Verb NP PP VP!X2 PP\nX2!Verb NP\nVP!Verb PP VP!Verb PP\nVP!VP PP VP!VP PP\nPP!Preposition NP PP!Preposition NP\nFigure 18.10 L1Grammar and its conversion to CNF. Note that although they aren\u2019t shown\nhere, all the original lexical entries from L1carry over unchanged as well.\na position k, the \ufb01rst constituent [i;k]must lie to the left of entry [i;j]somewhere\nalong row i, and the second entry [k;j]must lie beneath it, along column j.\nTo make this more concrete, consider the following example with its completed\nparse matrix, shown in Fig. 18.11.\n(18.4) Book the \ufb02ight through Houston.\nThe superdiagonal row in the matrix contains the parts of speech for each word in\nthe input. The subsequent diagonals above that superdiagonal contain constituents\nthat cover all the spans of increasing length in the input.\nGiven this setup, CKY recognition consists of \ufb01lling the parse table in the right\nway. To do this, we\u2019ll proceed in a bottom-up fashion so that at the point where we\nare \ufb01lling any cell [i;j], the cells containing the parts that could contribute to this\nentry (i.e., the cells to the left and the cells below) have already been \ufb01lled. The\nalgorithm given in Fig. 18.12 \ufb01lls the upper-triangular matrix a column at a time\nworking from left to right, with each column \ufb01lled from bottom to top, as the right\nside of Fig. 18.11 illustrates. This scheme guarantees that at each point in time we\nhave all the information we need (to the left, since all the columns to the left have\nalready been \ufb01lled, and below since we\u2019re \ufb01lling bottom to top). It also mirrors on-\nline processing, since \ufb01lling the columns from left to right corresponds to processing\neach word one at a time.\nThe outermost loop of the algorithm given in Fig. 18.12 iterates over the columns,\nand the second loop iterates over the rows, from the bottom up. The purpose of the\ninnermost loop is to range over all the places where a substring spanning itojin\nthe input might be split in two. As kranges over the places where the string can be\nsplit, the pairs of cells we consider move, in lockstep, to the right along row iand\ndown along column j. Figure 18.13 illustrates the general case of \ufb01lling cell [i;j].",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 602,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 408\n\n400 CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\nBookthe flight throughHoustonS, VP, Verb, Nominal, NounS,VP,X2S,VP,X2DetNPNPNominal,NounNominalPrepPPNP,Proper-Noun[0,1][0,2][0,3][0,4][0,5][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]\nFigure 18.11 Completed parse table for Book the \ufb02ight through Houston.\nfunction CKY-P ARSE (words, grammar )returns table\nforj from 1toLENGTH (words )do\nfor all fAjA!words [j]2grammar g\ntable [j\u00001;j] table [j\u00001;j][A\nfori from j\u00002down to 0do\nfork i+1toj\u00001do\nfor all fAjA!BC2grammar andB2table[i;k]andC2table[k;j]g\ntable [i,j] table [i,j][A\nFigure 18.12 The CKY algorithm.\nAt each such split, the algorithm considers whether the contents of the two cells can\nbe combined in a way that is sanctioned by a rule in the grammar. If such a rule\nexists, the non-terminal on its left-hand side is entered into the table.\nFigure 18.14 shows how the \ufb01ve cells of column 5 of the table are \ufb01lled after the\nword Houston is read. The arrows point out the two spans that are being used to add\nan entry to the table. Note that the action in cell [0;5]indicates the presence of three\nalternative parses for this input, one where the PPmodi\ufb01es the \ufb02ight , one where\nit modi\ufb01es the booking, and one that captures the second argument in the original\nVP!Verb NP PP rule, now captured indirectly with the VP!X2 PP rule.\n18.6.3 CKY Parsing\nThe algorithm given in Fig. 18.12 is a recognizer, not a parser. That is, it can tell\nus whether a valid parse exists for a given sentence based on whether or not if \ufb01nds\nanSin cell [0;n], but it can\u2019t provide the derivation, which is the actual job for a\nparser. To turn it into a parser capable of returning all possible parses for a given\ninput, we can make two simple changes to the algorithm: the \ufb01rst change is to\naugment the entries in the table so that each non-terminal is paired with pointers to\nthe table entries from which it was derived (more or less as shown in Fig. 18.14), the\nsecond change is to permit multiple versions of the same non-terminal to be entered\ninto the table (again as shown in Fig. 18.14). With these changes, the completed\ntable contains all the possible parses for a given input. Returning an arbitrary single",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 603,
      "token_count": 673,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 409\n\n18.6 \u2022 CKY P ARSING : A D YNAMIC PROGRAMMING APPROACH 401\n...\n...[0,n]\n[i,i+1][i,i+2][i,j-2][i,j-1][i+1,j][i+2,j]\n[j-1,j][j-2,j][i,j]...[0,1]\n[n-1, n]\nFigure 18.13 All the ways to \ufb01ll the [ i,j]th cell in the CKY table.\nparse consists of choosing an Sfrom cell [0;n]and then recursively retrieving its\ncomponent constituents from the table. Of course, instead of returning every parse\nfor a sentence, we usually want just the best parse; we\u2019ll see how to do that in the\nnext section.\n18.6.4 CKY in Practice\nFinally, we should note that while the restriction to CNF does not pose a problem\ntheoretically, it does pose some non-trivial problems in practice. The returned CNF\ntrees may not be consistent with the original grammar built by the grammar devel-\nopers, and will complicate any syntax-driven approach to semantic analysis.\nOne approach to getting around these problems is to keep enough information\naround to transform our trees back to the original grammar as a post-processing step\nof the parse. This is trivial in the case of the transformation used for rules with length\ngreater than 2. Simply deleting the new dummy non-terminals and promoting their\ndaughters restores the original tree.\nIn the case of unit productions, it turns out to be more convenient to alter the ba-\nsic CKY algorithm to handle them directly than it is to store the information needed\nto recover the correct trees. Exercise 18.3 asks you to make this change. Many of\nthe probabilistic parsers presented in Appendix C use the CKY algorithm altered in",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 604,
      "token_count": 394,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 410\n\n402 CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\nBookthe flight throughHoustonS, VP, Verb, Nominal, NounS,VP,X2DetNPNominal,NounNominalPrepNP,Proper-Noun[0,1][0,2][0,3][0,4][0,5][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]Bookthe flight throughHoustonS, VP, Verb, Nominal, NounS,VP,X2DetNPNPNominal,NounPrepPPNP,Proper-Noun[0,1][0,2][0,3][0,4][0,5][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]\nBookthe flight throughHoustonS, VP, Verb, Nominal, NounS,VP,X2DetNPNPNominal,NounNominalPrepPPNP,Proper-Noun[0,1][0,2][0,3][0,4][0,5][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]Bookthe flight throughHoustonS, VP, Verb, Nominal, NounS,VP,X2DetNPNPNominal,NounNominalPrepPPNP,Proper-Noun[0,1][0,2][0,3][0,4][0,5][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]\nBookthe flight throughHoustonS, VP, Verb, Nominal, NounS,VP,X2DetNPNPNominal,NounNominalPrepPPNP,Proper-Noun[0,1][0,2][0,3][0,4][1,2][1,3][2,3][1,4][2,5][2,4][3,4][4,5][3,5][1,5]S2, VPS3S1,VP, X2\nFigure 18.14 Filling the cells of column 5 after reading the word Houston .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 605,
      "token_count": 558,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 411\n\n18.7 \u2022 S PAN-BASED NEURAL CONSTITUENCY PARSING 403\njust this manner.\n18.7 Span-Based Neural Constituency Parsing\nWhile the CKY parsing algorithm we\u2019ve seen so far does great at enumerating all\nthe possible parse trees for a sentence, it has a large problem: it doesn\u2019t tell us which\nparse is the correct one! That is, it doesn\u2019t disambiguate among the possible parses.\nTo solve the disambiguation problem we\u2019ll use a simple neural extension of the\nCKY algorithm. The intuition of such parsing algorithms (often called span-based\nconstituency parsing , orneural CKY ), is to train a neural classi\ufb01er to assign a\nscore to each constituent, and then use a modi\ufb01ed version of CKY to combine these\nconstituent scores to \ufb01nd the best-scoring parse tree.\nHere we\u2019ll describe a version of the algorithm from Kitaev et al. (2019). This\nparser learns to map a span of words to a constituent, and, like CKY , hierarchically\ncombines larger and larger spans to build the parse-tree bottom-up. But unlike clas-\nsic CKY , this parser doesn\u2019t use the hand-written grammar to constrain what con-\nstituents can be combined, instead just relying on the learned neural representations\nof spans to encode likely combinations.\n18.7.1 Computing Scores for a Span\nLet\u2019s begin by considering just the constituent (we\u2019ll call it a span ) that lies between span\nfencepost positions iand jwith non-terminal symbol label l. We\u2019ll build a system\nto assign a score s(i;j;l)to this constituent span.\nENCODER[START]BooktheflightthroughHouston[END]map to subwordsmap back to words013245MLPi=1hj-hij=3NPCompute score for spanRepresent spanCKY for computing best parse\npostprocessing layers\nFigure 18.15 A simpli\ufb01ed outline of computing the span score for the span the \ufb02ight with\nthe label NP.\nFig. 18.15 sketches the architecture. The input word tokens are embedded by",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 606,
      "token_count": 467,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 412\n\n404 CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\npassing them through a pretrained language model like BERT. Because BERT oper-\nates on the level of subword (wordpiece) tokens rather than words, we\u2019ll \ufb01rst need to\nconvert the BERT outputs to word representations. One standard way of doing this\nis to simply use the \ufb01rst subword unit as the representation for the entire word; us-\ning the last subword unit, or the sum of all the subword units are also common. The\nembeddings can then be passed through some postprocessing layers; Kitaev et al.\n(2019), for example, use 8 Transformer layers.\nThe resulting word encoder outputs ytare then used to compute a span score.\nFirst, we must map the word encodings (indexed by word positions) to span encod-\nings (indexed by fenceposts). We do this by representing each fencepost with two\nseparate values; the intuition is that a span endpoint to the right of a word represents\ndifferent information than a span endpoint to the left of a word. We convert each\nword output ytinto a (leftward-pointing) value for spans ending at this fencepost, \u0000yt, and a (rightward-pointing) value\u0000 !ytfor spans beginning at this fencepost, by\nsplitting ytinto two halves. Each span then stretches from one double-vector fence-\npost to another, as in the following representation of the \ufb02ight , which is span (1;3):\nSTART 0 Book the \ufb02ight through\ny0\u0000 !y0 \u0000y1y1\u0000 !y1 \u0000y2 y2\u0000 !y2 \u0000y3y3\u0000 !y3 \u0000y4 y4\u0000 !y4 \u0000y5:::\n0\n 1\n 2\n 3\n 4\nspan(1,3)\nA traditional way to represent a span, developed originally for RNN-based models\n(Wang and Chang, 2016), but extended also to Transformers, is to take the differ-\nence between the embeddings of its start and end, i.e., representing span (i;j)by\nsubtracting the embedding of ifrom the embedding of j. Here we represent a span\nby concatenating the difference of each of its fencepost components:\nv(i;j) = [\u0000 !yj\u0000\u0000 !yi; \u0000\u0000yj+1\u0000 \u0000\u0000yi+1] (18.5)\nThe span vector vis then passed through an MLP span classi\ufb01er, with two fully-\nconnected layers and one ReLU activation function, whose output dimensionality is\nthe number of possible non-terminal labels:\ns(i;j;\u0001) =W2ReLU (LayerNorm (W1v(i;j))) (18.6)\nThe MLP then outputs a score for each possible non-terminal.\n18.7.2 Integrating Span Scores into a Parse\nNow we have a score for each labeled constituent span s(i;j;l). But we need a score\nfor an entire parse tree. Formally a tree Tis represented as a set of jTjsuch labeled\nspans, with the tthspan starting at position itand ending at position jt, with label lt:\nT=f(it;jt;lt):t=1;:::;jTjg (18.7)\nThus once we have a score for each span, the parser can compute a score for the\nwhole tree s(T)simply by summing over the scores of its constituent spans:\ns(T) =X\n(i;j;l)2Ts(i;j;l) (18.8)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 607,
      "token_count": 783,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 413\n\n18.8 \u2022 E VALUATING PARSERS 405\nAnd we can choose the \ufb01nal parse tree as the tree with the maximum score:\n\u02c6T=argmax\nTs(T) (18.9)\nThe simplest method to produce the most likely parse is to greedily choose the\nhighest scoring label for each span. This greedy method is not guaranteed to produce\na tree, since the best label for a span might not \ufb01t into a complete tree. In practice,\nhowever, the greedy method tends to \ufb01nd trees; in their experiments Gaddy et al.\n(2018) \ufb01nds that 95% of predicted bracketings form valid trees.\nNonetheless it is more common to use a variant of the CKY algorithm to \ufb01nd the\nfull parse. The variant de\ufb01ned in Gaddy et al. (2018) works as follows. Let\u2019s de\ufb01ne\nsbest(i;j)as the score of the best subtree spanning (i;j). For spans of length one, we\nchoose the best label:\nsbest(i;i+1) =max\nls(i;i+1;l) (18.10)\nFor other spans (i;j), the recursion is:\nsbest(i;j) = max\nls(i;j;l)\n+max\nk[sbest(i;k)+sbest(k;j)] (18.11)\nNote that the parser is using the max label for span (i;j)+ the max labels for spans\n(i;k)and(k;j)without worrying about whether those decisions make sense given a\ngrammar. The role of the grammar in classical parsing is to help constrain possible\ncombinations of constituents (NPs like to be followed by VPs). By contrast, the\nneural model seems to learn these kinds of contextual constraints during its mapping\nfrom spans to non-terminals.\nFor more details on span-based parsing, including the margin-based training al-\ngorithm, see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and\nKitaev et al. (2019).\n18.8 Evaluating Parsers\nThe standard tool for evaluating parsers that assign a single parse tree to a sentence\nis the PARSEV AL metrics (Black et al., 1991). The PARSEV AL metric measures PARSEV AL\nhow much the constituents in the hypothesis parse tree look like the constituents in a\nhand-labeled, reference parse. PARSEV AL thus requires a human-labeled reference\n(or \u201cgold standard\u201d) parse tree for each sentence in the test set; we generally draw\nthese reference parses from a treebank like the Penn Treebank.\nA constituent in a hypothesis parse Chof a sentence sis labeled correct if there\nis a constituent in the reference parse Crwith the same starting point, ending point,\nand non-terminal symbol. We can then measure the precision and recall just as for\ntasks we\u2019ve seen already like named entity tagging:\nlabeled recall: =# of correct constituents in hypothesis parse of s\n# of total constituents in reference parse of s\nlabeled precision: =# of correct constituents in hypothesis parse of s\n# of total constituents in hypothesis parse of s",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 608,
      "token_count": 673,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 414\n\n406 CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\nS(dumped)\nVP(dumped)\nPP(into)\nNP(bin)\nNN(bin)\nbinDT(a)\naP\nintoNP(sacks)\nNNS(sacks)\nsacksVBD(dumped)\ndumpedNP(workers)\nNNS(workers)\nworkers\nFigure 18.16 A lexicalized tree from Collins (1999).\nAs usual, we often report a combination of the two, F 1:\nF1=2PR\nP+R(18.12)\nWe additionally use a new metric, crossing brackets, for each sentence s:\ncross-brackets: the number of constituents for which the reference parse has a\nbracketing such as ((A B) C) but the hypothesis parse has a bracketing such\nas (A (B C)).\nFor comparing parsers that use different grammars, the PARSEV AL metric in-\ncludes a canonicalization algorithm for removing information likely to be grammar-\nspeci\ufb01c (auxiliaries, pre-in\ufb01nitival \u201cto\u201d, etc.) and for computing a simpli\ufb01ed score\n(Black et al., 1991). The canonical implementation of the PARSEV AL metrics is\ncalled evalb (Sekine and Collins, 1997). evalb\n18.9 Heads and Head-Finding\nSyntactic constituents can be associated with a lexical head ;Nis the head of an NP,\nVis the head of a VP. This idea of a head for each constituent dates back to Bloom-\n\ufb01eld 1914, and is central to the dependency grammars and dependency parsing we\u2019ll\nintroduce in Chapter 19. Indeed, heads can be used as a way to map between con-\nstituency and dependency parses. Heads are also important in probabilistic pars-\ning (Appendix C) and in constituent-based grammar formalisms like Head-Driven\nPhrase Structure Grammar (Pollard and Sag, 1994)..\nIn one simple model of lexical heads, each context-free rule is associated with\na head (Charniak 1997, Collins 1999). The head is the word in the phrase that is\ngrammatically the most important. Heads are passed up the parse tree; thus, each\nnon-terminal in a parse tree is annotated with a single word, which is its lexical head.\nFigure 18.16 shows an example of such a tree from Collins (1999), in which each\nnon-terminal is annotated with its head.\nFor the generation of such a tree, each CFG rule must be augmented to identify\none right-side constituent to be the head child. The headword for a node is then set to\nthe headword of its head child. Choosing these head children is simple for textbook\nexamples ( NNis the head of NP) but is complicated and indeed controversial for",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 609,
      "token_count": 608,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 415\n\n18.10 \u2022 S UMMARY 407\nmost phrases. (Should the complementizer toor the verb be the head of an in\ufb01nite\nverb phrase?) Modern linguistic theories of syntax generally include a component\nthat de\ufb01nes heads (see, e.g., (Pollard and Sag, 1994)).\nAn alternative approach to \ufb01nding a head is used in most practical computational\nsystems. Instead of specifying head rules in the grammar itself, heads are identi\ufb01ed\ndynamically in the context of trees for speci\ufb01c sentences. In other words, once\na sentence is parsed, the resulting tree is walked to decorate each node with the\nappropriate head. Most current systems rely on a simple set of handwritten rules,\nsuch as a practical one for Penn Treebank grammars given in Collins (1999) but\ndeveloped originally by Magerman (1995). For example, the rule for \ufb01nding the\nhead of an NPis as follows (Collins, 1999, p. 238):\n\u2022 If the last word is tagged POS, return last-word.\n\u2022 Else search from right to left for the \ufb01rst child which is an NN, NNP, NNPS, NX, POS,\nor JJR.\n\u2022 Else search from left to right for the \ufb01rst child which is an NP.\n\u2022 Else search from right to left for the \ufb01rst child which is a $, ADJP, or PRN.\n\u2022 Else search from right to left for the \ufb01rst child which is a CD.\n\u2022 Else search from right to left for the \ufb01rst child which is a JJ, JJS, RB or QP.\n\u2022 Else return the last word\nSelected other rules from this set are shown in Fig. 18.17. For example, for VP\nrules of the form VP!Y1\u0001\u0001\u0001Yn, the algorithm would start from the left of Y1\u0001\u0001\u0001\nYnlooking for the \ufb01rst Yiof type TO; if no TOs are found, it would search for the\n\ufb01rstYiof type VBD; if no VBDs are found, it would search for a VBN, and so on.\nSee Collins (1999) for more details.\nParent Direction Priority List\nADJP Left NNS QP NN $ ADVP JJ VBN VBG ADJP JJR NP JJS DT FW RBR RBS\nSBAR RB\nADVP Right RB RBR RBS FW ADVP TO CD JJR JJ IN NP JJS NN\nPRN Left\nPRT Right RP\nQP Left $ IN NNS NN JJ RB DT CD NCD QP JJR JJS\nS Left TO IN VP S SBAR ADJP UCP NP\nSBAR Left WHNP WHPP WHADVP WHADJP IN DT S SQ SINV SBAR FRAG\nVP Left TO VBD VBN MD VBZ VB VBG VBP VP ADJP NN NNS NP\nFigure 18.17 Some head rules from Collins (1999). The head rules are also called a head percolation table .\n18.10 Summary\nThis chapter introduced constituency parsing. Here\u2019s a summary of the main points:\n\u2022 In many languages, groups of consecutive words act as a group or a con-\nstituent , which can be modeled by context-free grammars (which are also\nknown as phrase-structure grammars ).\n\u2022 A context-free grammar consists of a set of rules orproductions , expressed\nover a set of non-terminal symbols and a set of terminal symbols. Formally,\na particular context-free language is the set of strings that can be derived\nfrom a particular context-free grammar .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 610,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 416\n\n408 CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\n\u2022Structural ambiguity is a signi\ufb01cant problem for parsers. Common sources\nof structural ambiguity include PP-attachment andcoordination ambiguity .\n\u2022Dynamic programming parsing algorithms, such as CKY , use a table of\npartial parses to ef\ufb01ciently parse ambiguous sentences.\n\u2022CKY restricts the form of the grammar to Chomsky normal form (CNF).\n\u2022 The basic CKY algorithm compactly represents all possible parses of the sen-\ntence but doesn\u2019t choose a single best parse.\n\u2022 Choosing a single parse from all possible parses ( disambiguation ) can be\ndone by neural constituency parsers .\n\u2022 Span-based neural constituency parses train a neural classi\ufb01er to assign a score\nto each constituent, and then use a modi\ufb01ed version of CKY to combine these\nconstituent scores to \ufb01nd the best-scoring parse tree.\n\u2022 Parsers are evaluated with three metrics: labeled recall ,labeled precision ,\nandcross-brackets .\n\u2022Partial parsing andchunking are methods for identifying shallow syntac-\ntic constituents in a text. They are solved by sequence models trained on\nsyntactically-annotated data.\nBibliographical and Historical Notes\nAccording to Percival (1976), the idea of breaking up a sentence into a hierarchy of\nconstituents appeared in the V\u00a8olkerpsychologie of the groundbreaking psychologist\nWilhelm Wundt (Wundt, 1900):\n...den sprachlichen Ausdruck f \u00a8ur die willk \u00a8urliche Gliederung einer Ge-\nsammtvorstellung in ihre in logische Beziehung zueinander gesetzten\nBestandteile\n[the linguistic expression for the arbitrary division of a total idea\ninto its constituent parts placed in logical relations to one another]\nWundt\u2019s idea of constituency was taken up into linguistics by Leonard Bloom-\n\ufb01eld in his early book An Introduction to the Study of Language (Bloom\ufb01eld, 1914).\nBy the time of his later book, Language (Bloom\ufb01eld, 1933), what was then called\n\u201cimmediate-constituent analysis\u201d was a well-established method of syntactic study\nin the United States. By contrast, traditional European grammar, dating from the\nClassical period, de\ufb01ned relations between words rather than constituents, and Eu-\nropean syntacticians retained this emphasis on such dependency grammars, the sub-\nject of Chapter 19. (And indeed, both dependency and constituency grammars have\nbeen in vogue in computational linguistics at different times).\nAmerican Structuralism saw a number of speci\ufb01c de\ufb01nitions of the immediate\nconstituent, couched in terms of their search for a \u201cdiscovery procedure\u201d: a method-\nological algorithm for describing the syntax of a language. In general, these attempt\nto capture the intuition that \u201cThe primary criterion of the immediate constituent\nis the degree in which combinations behave as simple units\u201d (Bazell, 1952/1966, p.\n284). The most well known of the speci\ufb01c de\ufb01nitions is Harris\u2019 idea of distributional\nsimilarity to individual units, with the substitutability test. Essentially, the method\nproceeded by breaking up a construction into constituents by attempting to substitute\nsimple structures for possible constituents\u2014if a substitution of a simple form, say,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 611,
      "token_count": 742,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 417",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 612,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "EXERCISES 409\nman, was substitutable in a construction for a more complex set (like intense young\nman), then the form intense young man was probably a constituent. Harris\u2019s test was\nthe beginning of the intuition that a constituent is a kind of equivalence class.\nThe context-free grammar was a formalization of this idea of hierarchical\nconstituency de\ufb01ned in Chomsky (1956) and further expanded upon (and argued\nagainst) in Chomsky (1957) and Chomsky (1956/1975). Shortly after Chomsky\u2019s\ninitial work, the context-free grammar was reinvented by Backus (1959) and inde-\npendently by Naur et al. (1960) in their descriptions of the ALGOL programming\nlanguage; Backus (1996) noted that he was in\ufb02uenced by the productions of Emil\nPost and that Naur\u2019s work was independent of his (Backus\u2019) own. After this early\nwork, a great number of computational models of natural language processing were\nbased on context-free grammars because of the early development of ef\ufb01cient pars-\ning algorithms.\nDynamic programming parsing has a history of independent discovery. Ac-\ncording to the late Martin Kay (personal communication), a dynamic programming\nparser containing the roots of the CKY algorithm was \ufb01rst implemented by John\nCocke in 1960. Later work extended and formalized the algorithm, as well as prov-\ning its time complexity (Kay 1967, Younger 1967, Kasami 1965). The related well-\nformed substring table (WFST ) seems to have been independently proposed by WFST\nKuno (1965) as a data structure that stores the results of all previous computations\nin the course of the parse. Based on a generalization of Cocke\u2019s work, a similar\ndata structure had been independently described in Kay (1967) (and Kay 1973). The\ntop-down application of dynamic programming to parsing was described in Earley\u2019s\nPh.D. dissertation (Earley 1968, Earley 1970). Sheil (1976) showed the equivalence\nof the WFST and the Earley algorithm. Norvig (1991) shows that the ef\ufb01ciency of-\nfered by dynamic programming can be captured in any language with a memoization\nfunction (such as in LISP) simply by wrapping the memoization operation around a\nsimple top-down parser.\nThe earliest disambiguation algorithms for parsing were based on probabilistic\ncontext-free grammars , \ufb01rst worked out by Booth (1969) and Salomaa (1969); seeprobabilistic\ncontext-free\ngrammarsAppendix C for more history. Neural methods were \ufb01rst applied to parsing at around\nthe same time as statistical parsing methods were developed (Henderson, 1994). In\nthe earliest work neural networks were used to estimate some of the probabilities for\nstatistical constituency parsers (Henderson, 2003, 2004; Emami and Jelinek, 2005)\n. The next decades saw a wide variety of neural parsing algorithms, including re-\ncursive neural architectures (Socher et al., 2011, 2013), encoder-decoder models\n(Vinyals et al., 2015; Choe and Charniak, 2016), and the idea of focusing on spans\n(Cross and Huang, 2016). For more on the span-based self-attention approach we\ndescribe in this chapter see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 613,
      "token_count": 771,
      "chapter_title": ""
    }
  },
  {
    "content": "fered by dynamic programming can be captured in any language with a memoization\nfunction (such as in LISP) simply by wrapping the memoization operation around a\nsimple top-down parser.\nThe earliest disambiguation algorithms for parsing were based on probabilistic\ncontext-free grammars , \ufb01rst worked out by Booth (1969) and Salomaa (1969); seeprobabilistic\ncontext-free\ngrammarsAppendix C for more history. Neural methods were \ufb01rst applied to parsing at around\nthe same time as statistical parsing methods were developed (Henderson, 1994). In\nthe earliest work neural networks were used to estimate some of the probabilities for\nstatistical constituency parsers (Henderson, 2003, 2004; Emami and Jelinek, 2005)\n. The next decades saw a wide variety of neural parsing algorithms, including re-\ncursive neural architectures (Socher et al., 2011, 2013), encoder-decoder models\n(Vinyals et al., 2015; Choe and Charniak, 2016), and the idea of focusing on spans\n(Cross and Huang, 2016). For more on the span-based self-attention approach we\ndescribe in this chapter see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein\n(2018), and Kitaev et al. (2019). See Chapter 20 for the parallel history of neural\ndependency parsing.\nThe classic reference for parsing algorithms is Aho and Ullman (1972); although\nthe focus of that book is on computer languages, most of the algorithms have been\napplied to natural language.\nExercises\n18.1 Implement the algorithm to convert arbitrary context-free grammars to CNF.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 614,
      "token_count": 380,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 418\n\n410 CHAPTER 18 \u2022 C ONTEXT -FREE GRAMMARS AND CONSTITUENCY PARSING\nApply your program to the L1grammar.\n18.2 Implement the CKY algorithm and test it with your converted L1grammar.\n18.3 Rewrite the CKY algorithm given in Fig. 18.12 on page 400 so that it can\naccept grammars that contain unit productions.\n18.4 Discuss how to augment a parser to deal with input that may be incorrect, for\nexample, containing spelling errors or mistakes arising from automatic speech\nrecognition.\n18.5 Implement the PARSEV AL metrics described in Section 18.8. Next, use a\nparser and a treebank, compare your metrics against a standard implementa-\ntion. Analyze the errors in your approach.\n\n## Page 419\n\nCHAPTER\n19Dependency Parsing\nTout mot qui fait partie d\u2019une phrase... Entre lui et ses voisins, l\u2019esprit aperc \u00b8oit\ndes connexions, dont l\u2019ensemble forme la charpente de la phrase.\n[Between each word in a sentence and its neighbors, the mind perceives con-\nnections . These connections together form the scaffolding of the sentence.]\nLucien Tesni `ere. 1959. \u00b4El\u00b4ements de syntaxe structurale, A.1.\u00a74\nThe focus of the last chapter was on context-free grammars and constituent-\nbased representations. Here we present another important family of grammar for-\nmalisms called dependency grammars . In dependency formalisms, phrasal con-dependency\ngrammars\nstituents and phrase-structure rules do not play a direct role. Instead, the syntactic\nstructure of a sentence is described solely in terms of directed binary grammatical\nrelations between the words , as in the following dependency parse:\nIprefer themorning \ufb02ight through Denvernsubjobj\ndet\ncompoundnmod\ncaseroot\n(19.1)\nRelations among the words are illustrated above the sentence with directed, labeled\narcs from heads todependents . We call this a typed dependency structure becausetyped\ndependency\nthe labels are drawn from a \ufb01xed inventory of grammatical relations. A root node\nexplicitly marks the root of the tree, the head of the entire structure.\nFigure 19.1 on the next page shows the dependency analysis from Eq. 19.1 but\nvisualized as a tree, alongside its corresponding phrase-structure analysis of the kind\ngiven in the prior chapter. Note the absence of nodes corresponding to phrasal con-\nstituents or lexical categories in the dependency parse; the internal structure of the\ndependency parse consists solely of directed relations between words. These head-\ndependent relationships directly encode important information that is often buried in\nthe more complex phrase-structure parses. For example, the arguments to the verb\nprefer are directly linked to it in the dependency structure, while their connection\nto the main verb is more distant in the phrase-structure tree. Similarly, morning\nandDenver , modi\ufb01ers of \ufb02ight , are linked to it directly in the dependency structure.\nThis fact that the head-dependent relations are a good proxy for the semantic rela-\ntionship between predicates and their arguments is an important reason why depen-\ndency grammars are currently more common than constituency grammars in natural\nlanguage processing.\nAnother major advantage of dependency grammars is their ability to deal with\nlanguages that have a relatively free word order . For example, word order in Czech free word order\ncan be much more \ufb02exible than in English; a grammatical object might occur before\nor after a location adverbial . A phrase-structure grammar would need a separate rule",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 615,
      "token_count": 782,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 420\n\n412 CHAPTER 19 \u2022 D EPENDENCY PARSING\nprefer\n\ufb02ight\nDenver\nthroughmorning theIS\nVP\nNP\nNom\nPP\nNP\nPro\nDenverP\nthroughNom\nNoun\n\ufb02ightNom\nNoun\nmorningDet\ntheVerb\npreferNP\nPro\nI\nFigure 19.1 Dependency and constituent analyses for I prefer the morning \ufb02ight through Denver.\nfor each possible place in the parse tree where such an adverbial phrase could occur.\nA dependency-based approach can have just one link type representing this particu-\nlar adverbial relation; dependency grammar approaches can thus abstract away a bit\nmore from word order information.\nIn the following sections, we\u2019ll give an inventory of relations used in dependency\nparsing, discuss two families of parsing algorithms (transition-based, and graph-\nbased), and discuss evaluation.\n19.1 Dependency Relations\nThe traditional linguistic notion of grammatical relation provides the basis for thegrammatical\nrelation\nbinary relations that comprise these dependency structures. The arguments to these\nrelations consist of a head and a dependent . The head plays the role of the central head\ndependent organizing word, and the dependent as a kind of modi\ufb01er. The head-dependent rela-\ntionship is made explicit by directly linking heads to the words that are immediately\ndependent on them.\nIn addition to specifying the head-dependent pairs, dependency grammars allow\nus to classify the kinds of grammatical relations, or grammatical function that thegrammatical\nfunction\ndependent plays with respect to its head. These include familiar notions such as\nsubject ,direct object andindirect object . In English these notions strongly corre-\nlate with, but by no means determine, both position in a sentence and constituent\ntype and are therefore somewhat redundant with the kind of information found in\nphrase-structure trees. However, in languages with more \ufb02exible word order, the\ninformation encoded directly in these grammatical relations is critical since phrase-\nbased constituent syntax provides little help.\nLinguists have developed taxonomies of relations that go well beyond the famil-\niar notions of subject and object. While there is considerable variation from theory",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 616,
      "token_count": 461,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 421\n\n19.1 \u2022 D EPENDENCY RELATIONS 413\nClausal Argument Relations Description\nNSUBJ Nominal subject\nOBJ Direct object\nIOBJ Indirect object\nCCOMP Clausal complement\nNominal Modi\ufb01er Relations Description\nNMOD Nominal modi\ufb01er\nAMOD Adjectival modi\ufb01er\nAPPOS Appositional modi\ufb01er\nDET Determiner\nCASE Prepositions, postpositions and other case markers\nOther Notable Relations Description\nCONJ Conjunct\nCC Coordinating conjunction\nFigure 19.2 Some of the Universal Dependency relations (de Marneffe et al., 2021).\nto theory, there is enough commonality that cross-linguistic standards have been\ndeveloped. The Universal Dependencies (UD) project (de Marneffe et al., 2021),Universal\nDependencies\nan open community effort to annotate dependencies and other aspects of grammar\nacross more than 100 languages, provides an inventory of 37 dependency relations.\nFig. 19.2 shows a subset of the UD relations and Fig. 19.3 provides some examples.\nThe motivation for all of the relations in the Universal Dependency scheme is\nbeyond the scope of this chapter, but the core set of frequently used relations can be\nbroken into two sets: clausal relations that describe syntactic roles with respect to a\npredicate (often a verb), and modi\ufb01er relations that categorize the ways that words\ncan modify their heads.\nConsider, for example, the following sentence:\nUnited canceled themorning \ufb02ights toHoustonnsubjobj\ndet\ncompoundnmod\ncaseroot\n(19.2)\nHere the clausal relations NSUBJ and OBJidentify the subject and direct object of\nthe predicate cancel , while the NMOD ,DET, and CASE relations denote modi\ufb01ers of\nthe nouns \ufb02ights andHouston .\n19.1.1 Dependency Formalisms\nA dependency structure can be represented as a directed graph G= (V;A), consisting\nof a set of vertices V, and a set of ordered pairs of vertices A, which we\u2019ll call arcs.\nFor the most part we will assume that the set of vertices, V, corresponds exactly\nto the set of words in a given sentence. However, they might also correspond to\npunctuation, or when dealing with morphologically complex languages the set of\nvertices might consist of stems and af\ufb01xes. The set of arcs, A, captures the head-\ndependent and grammatical function relationships between the elements in V.\nDifferent grammatical theories or formalisms may place further constraints on\nthese dependency structures. Among the more frequent restrictions are that the struc-\ntures must be connected, have a designated root node, and be acyclic or planar. Of\nmost relevance to the parsing approaches discussed in this chapter is the common,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 617,
      "token_count": 604,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 422\n\n414 CHAPTER 19 \u2022 D EPENDENCY PARSING\nRelation Examples with head anddependent\nNSUBJ United canceled the \ufb02ight.\nOBJ United diverted the\ufb02ight to Reno.\nWebooked her the \ufb01rst \ufb02ight to Miami.\nIOBJ Webooked herthe \ufb02ight to Miami.\nCOMPOUND We took the morning \ufb02ight .\nNMOD \ufb02ight toHouston .\nAMOD Book the cheapest \ufb02ight .\nAPPOS United , aunit of UAL, matched the fares.\nDET The \ufb02ight was canceled.\nWhich \ufb02ight was delayed?\nCONJ We\ufb02ewto Denver and drove to Steamboat.\nCC We \ufb02ew to Denver anddrove to Steamboat.\nCASE Book the \ufb02ight through Houston .\nFigure 19.3 Examples of some Universal Dependency relations.\ncomputationally-motivated, restriction to rooted trees. That is, a dependency treedependency\ntree\nis a directed graph that satis\ufb01es the following constraints:\n1. There is a single designated root node that has no incoming arcs.\n2. With the exception of the root node, each vertex has exactly one incoming arc.\n3. There is a unique path from the root node to each vertex in V.\nTaken together, these constraints ensure that each word has a single head, that the\ndependency structure is connected, and that there is a single root node from which\none can follow a unique directed path to each of the words in the sentence.\n19.1.2 Projectivity\nThe notion of projectivity imposes an additional constraint that is derived from the\norder of the words in the input. An arc from a head to a dependent is said to be\nprojective if there is a path from the head to every word that lies between the head projective\nand the dependent in the sentence. A dependency tree is then said to be projective if\nall the arcs that make it up are projective. All the dependency trees we\u2019ve seen thus\nfar have been projective. There are, however, many valid constructions which lead\nto non-projective trees, particularly in languages with relatively \ufb02exible word order.\nConsider the following example.\nJetBlue canceled our \ufb02ight this morning which was already latensubjobjobl\ndetacl:relcl\ndet nsubjcop\nadvroot\n(19.3)\nIn this example, the arc from \ufb02ight to its modi\ufb01er lateis non-projective since there\nis no path from \ufb02ight to the intervening words thisandmorning . As we can see from\nthis diagram, projectivity (and non-projectivity) can be detected in the way we\u2019ve\nbeen drawing our trees. A dependency tree is projective if it can be drawn with\nno crossing edges. Here there is no way to link \ufb02ight to its dependent latewithout\ncrossing the arc that links morning to its head.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 618,
      "token_count": 630,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 423\n\n19.1 \u2022 D EPENDENCY RELATIONS 415\nOur concern with projectivity arises from two related issues. First, the most\nwidely used English dependency treebanks were automatically derived from phrase-\nstructure treebanks through the use of head-\ufb01nding rules. The trees generated in such\na fashion will always be projective, and hence will be incorrect when non-projective\nexamples like this one are encountered.\nSecond, there are computational limitations to the most widely used families of\nparsing algorithms. The transition-based approaches discussed in Section 19.2 can\nonly produce projective trees, hence any sentences with non-projective structures\nwill necessarily contain some errors. This limitation is one of the motivations for\nthe more \ufb02exible graph-based parsing approach described in Section 19.3.\n19.1.3 Dependency Treebanks\nTreebanks play a critical role in the development and evaluation of dependency\nparsers. They are used for training parsers, they act as the gold labels for evaluating\nparsers, and they also provide useful information for corpus linguistics studies.\nDependency treebanks are created by having human annotators directly generate\ndependency structures for a given corpus, or by hand-correcting the output of an\nautomatic parser. A few early treebanks were also based on using a deterministic\nprocess to translate existing constituent-based treebanks into dependency trees.\nThe largest open community project for building dependency trees is the Univer-\nsal Dependencies project at https://universaldependencies.org/ introduced\nabove, which currently has almost 200 dependency treebanks in more than 100 lan-\nguages (de Marneffe et al., 2021). Here are a few UD examples showing dependency\ntrees for sentences in Spanish, Basque, and Mandarin Chinese:\nVERB ADP DET NOUN ADP DET NUM PUNCT\nSubiremos a el tren a las cinco .\nwe-will-board on the train at the \ufb01ve .obl\ndetcase\ndetobl:tmod\ncasepunct\n[Spanish] Subiremos al tren a las cinco. \u201cWe will be boarding the train at \ufb01ve.\u201d (19.4)\nNOUN NOUN VERB AUX PUNCT\nEkaitzak itsasontzia hondoratu du .\nstorm (Erg.) ship (Abs.) sunk has .nsubj\nobj auxpunct\n[Basque] Ekaitzak itsasontzia hondoratu du. \u201cThe storm has sunk the ship.\u201d (19.5)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 619,
      "token_count": 522,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 424\n\n416 CHAPTER 19 \u2022 D EPENDENCY PARSING\nADV PRON NOUN ADV VERB VERB NOUN\n\u4f46\u6211\u6628\u5929 \u624d \u6536 \u5230\u4fe1\nbut I yesterday only-then receive arrive letter .adv\nnsubj\nobj:tmod\nadvmod compound:vvobj\n[Chinese]\u4f46\u6211\u6628\u5929\u624d\u6536\u5230\u4fe1\u201cBut I didn\u2019t receive the letter until yesterday\u201d (19.6)\n19.2 Transition-Based Dependency Parsing\nOur \ufb01rst approach to dependency parsing is called transition-based parsing. This transition-based\narchitecture draws on shift-reduce parsing , a paradigm originally developed for\nanalyzing programming languages (Aho and Ullman, 1972). In transition-based\nparsing we\u2019ll have a stack on which we build the parse, a buffer of tokens to be\nparsed, and a parser which takes actions on the parse via a predictor called an oracle ,\nas illustrated in Fig. 19.4.\nwnw1w2s2...s1snParserInput bu\ufb00erStackOracleLEFTARCRIGHTARCSHIFTActionDependencyRelationsw3w2\nFigure 19.4 Basic transition-based parser. The parser examines the top two elements of the\nstack and selects an action by consulting an oracle that examines the current con\ufb01guration.\nThe parser walks through the sentence left-to-right, successively shifting items\nfrom the buffer onto the stack. At each time point we examine the top two elements\non the stack, and the oracle makes a decision about what transition to apply to build\nthe parse. The possible transitions correspond to the intuitive actions one might take\nin creating a dependency tree by examining the words in a single pass over the input\nfrom left to right (Covington, 2001):\n\u2022 Assign the current word as the head of some previously seen word,\n\u2022 Assign some previously seen word as the head of the current word,\n\u2022 Postpone dealing with the current word, storing it for later processing.\nWe\u2019ll formalize this intuition with the following three transition operators that\nwill operate on the top two elements of the stack:\n\u2022LEFT ARC: Assert a head-dependent relation between the word at the top of\nthe stack and the second word; remove the second word from the stack.\n\u2022RIGHT ARC: Assert a head-dependent relation between the second word on\nthe stack and the word at the top; remove the top word from the stack;",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 620,
      "token_count": 510,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 425\n\n19.2 \u2022 T RANSITION -BASED DEPENDENCY PARSING 417\n\u2022SHIFT : Remove the word from the front of the input buffer and push it onto\nthe stack.\nWe\u2019ll sometimes call operations like LEFT ARCand RIGHT ARCreduce operations,\nbased on a metaphor from shift-reduce parsing, in which reducing means combin-\ning elements on the stack. There are some preconditions for using operators. The\nLEFT ARCoperator cannot be applied when ROOT is the second element of the stack\n(since by de\ufb01nition the ROOT node cannot have any incoming arcs). And both the\nLEFT ARCand RIGHT ARCoperators require two elements to be on the stack to be\napplied.\nThis particular set of operators implements what is known as the arc standard arc standard\napproach to transition-based parsing (Covington 2001, Nivre 2003). In arc standard\nparsing the transition operators only assert relations between elements at the top of\nthe stack, and once an element has been assigned its head it is removed from the\nstack and is not available for further processing. As we\u2019ll see, there are alterna-\ntive transition systems which demonstrate different parsing behaviors, but the arc\nstandard approach is quite effective and is simple to implement.\nThe speci\ufb01cation of a transition-based parser is quite simple, based on repre-\nsenting the current state of the parse as a con\ufb01guration : the stack, an input buffer con\ufb01guration\nof words or tokens, and a set of relations representing a dependency tree. Parsing\nmeans making a sequence of transitions through the space of possible con\ufb01gura-\ntions. We start with an initial con\ufb01guration in which the stack contains the ROOT\nnode, the buffer has the tokens in the sentence, and an empty set of relations repre-\nsents the parse. In the \ufb01nal goal state, the stack and the word list should be empty,\nand the set of relations will represent the \ufb01nal parse. Fig. 19.5 gives the algorithm.\nfunction DEPENDENCY PARSE (words )returns dependency tree\nstate f[root], [ words ], []g; initial con\ufb01guration\nwhile state not \ufb01nal\nt ORACLE (state ) ; choose a transition operator to apply\nstate APPLY (t,state ) ; apply it, creating a new state\nreturn state\nFigure 19.5 A generic transition-based dependency parser\nAt each step, the parser consults an oracle (we\u2019ll come back to this shortly) that\nprovides the correct transition operator to use given the current con\ufb01guration. It then\napplies that operator to the current con\ufb01guration, producing a new con\ufb01guration.\nThe process ends when all the words in the sentence have been consumed and the\nROOT node is the only element remaining on the stack.\nThe ef\ufb01ciency of transition-based parsers should be apparent from the algorithm.\nThe complexity is linear in the length of the sentence since it is based on a single\nleft to right pass through the words in the sentence. (Each word must \ufb01rst be shifted\nonto the stack and then later reduced.)\nNote that unlike the dynamic programming and search-based approaches dis-\ncussed in Chapter 18, this approach is a straightforward greedy algorithm\u2014the or-\nacle provides a single choice at each step and the parser proceeds with that choice,\nno other options are explored, no backtracking is employed, and a single parse is\nreturned in the end.\nFigure 19.6 illustrates the operation of the parser with the sequence of transitions",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 621,
      "token_count": 756,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 426\n\n418 CHAPTER 19 \u2022 D EPENDENCY PARSING\nleading to a parse for the following example.\nBook methemorning \ufb02ightiobjobj\ndet\ncompoundroot\n(19.7)\nLet\u2019s consider the state of the con\ufb01guration at Step 2, after the word mehas been\npushed onto the stack.\nStack Word List Relations\n[root, book, me] [the, morning, \ufb02ight]\nThe correct operator to apply here is RIGHT ARCwhich assigns book as the head of\nmeand pops mefrom the stack resulting in the following con\ufb01guration.\nStack Word List Relations\n[root, book] [the, morning, \ufb02ight] (book!me)\nAfter several subsequent applications of the SHIFT operator, the con\ufb01guration in\nStep 6 looks like the following:\nStack Word List Relations\n[root, book, the, morning, \ufb02ight] [] (book!me)\nHere, all the remaining words have been passed onto the stack and all that is left\nto do is to apply the appropriate reduce operators. In the current con\ufb01guration, we\nemploy the LEFT ARCoperator resulting in the following state.\nStack Word List Relations\n[root, book, the, \ufb02ight] [] (book!me)\n(morning \ufb02ight)\nAt this point, the parse for this sentence consists of the following structure.\nBook methemorning \ufb02ightiobj compound\n(19.8)\nThere are several important things to note when examining sequences such as\nthe one in Figure 19.6. First, the sequence given is not the only one that might lead\nto a reasonable parse. In general, there may be more than one path that leads to the\nsame result, and due to ambiguity, there may be other transition sequences that lead\nto different equally valid parses.\nSecond, we are assuming that the oracle always provides the correct operator\nat each point in the parse\u2014an assumption that is unlikely to be true in practice.\nAs a result, given the greedy nature of this algorithm, incorrect choices will lead to\nincorrect parses since the parser has no opportunity to go back and pursue alternative\nchoices. Section 19.2.4 will introduce several techniques that allow transition-based\napproaches to explore the search space more fully.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 622,
      "token_count": 489,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 427",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 623,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "19.2 \u2022 T RANSITION -BASED DEPENDENCY PARSING 419\nStep Stack Word List Action Relation Added\n0 [root] [book, me, the, morning, \ufb02ight] SHIFT\n1 [root, book] [me, the, morning, \ufb02ight] SHIFT\n2 [root, book, me] [the, morning, \ufb02ight] RIGHT ARC (book!me)\n3 [root, book] [the, morning, \ufb02ight] SHIFT\n4 [root, book, the] [morning, \ufb02ight] SHIFT\n5 [root, book, the, morning] [\ufb02ight] SHIFT\n6 [root, book, the, morning, \ufb02ight] [] LEFT ARC (morning \ufb02ight)\n7 [root, book, the, \ufb02ight] [] LEFT ARC (the \ufb02ight)\n8 [root, book, \ufb02ight] [] RIGHT ARC (book!\ufb02ight)\n9 [root, book] [] RIGHT ARC (root!book)\n10 [root] [] Done\nFigure 19.6 Trace of a transition-based parse.\nFinally, for simplicity, we have illustrated this example without the labels on\nthe dependency relations. To produce labeled trees, we can parameterize the LEFT -\nARCand RIGHT ARCoperators with dependency labels, as in LEFT ARC(NSUBJ ) or\nRIGHT ARC(OBJ). This is equivalent to expanding the set of transition operators from\nour original set of three to a set that includes LEFT ARCand RIGHT ARCoperators for\neach relation in the set of dependency relations being used, plus an additional one\nfor the SHIFT operator. This, of course, makes the job of the oracle more dif\ufb01cult\nsince it now has a much larger set of operators from which to choose.\n19.2.1 Creating an Oracle\nThe oracle for greedily selecting the appropriate transition is trained by supervised\nmachine learning. As with all supervised machine learning methods, we will need\ntraining data: con\ufb01gurations annotated with the correct transition to take. We can\ndraw these from dependency trees. And we need to extract features of the con-\n\ufb01guration. We\u2019ll introduce neural classi\ufb01ers that represent the con\ufb01guration via\nembeddings, as well as classic systems that use hand-designed features.\nGenerating Training Data\nThe oracle from the algorithm in Fig. 19.5 takes as input a con\ufb01guration and returns a\ntransition operator. Therefore, to train a classi\ufb01er, we will need con\ufb01gurations paired\nwith transition operators (i.e., LEFT ARC,RIGHT ARC, or SHIFT ). Unfortunately,\ntreebanks pair entire sentences with their corresponding trees, not con\ufb01gurations\nwith transitions.\nTo generate the required training data, we employ the oracle-based parsing algo-\nrithm in a clever way. We supply our oracle with the training sentences to be parsed\nalong with their corresponding reference parses from the treebank. To produce train-\ning instances, we then simulate the operation of the parser by running the algorithm\nand relying on a new training oracle to give us correct transition operators for each training oracle\nsuccessive con\ufb01guration.\nTo see how this works, let\u2019s \ufb01rst review the operation of our parser. It begins with\na default initial con\ufb01guration where the stack contains the ROOT , the input list is just\nthe list of words, and the set of relations is empty. The LEFT ARCand RIGHT ARC\noperators each add relations between the words at the top of the stack to the set of\nrelations being accumulated for a given sentence. Since we have a gold-standard",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 624,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "embeddings, as well as classic systems that use hand-designed features.\nGenerating Training Data\nThe oracle from the algorithm in Fig. 19.5 takes as input a con\ufb01guration and returns a\ntransition operator. Therefore, to train a classi\ufb01er, we will need con\ufb01gurations paired\nwith transition operators (i.e., LEFT ARC,RIGHT ARC, or SHIFT ). Unfortunately,\ntreebanks pair entire sentences with their corresponding trees, not con\ufb01gurations\nwith transitions.\nTo generate the required training data, we employ the oracle-based parsing algo-\nrithm in a clever way. We supply our oracle with the training sentences to be parsed\nalong with their corresponding reference parses from the treebank. To produce train-\ning instances, we then simulate the operation of the parser by running the algorithm\nand relying on a new training oracle to give us correct transition operators for each training oracle\nsuccessive con\ufb01guration.\nTo see how this works, let\u2019s \ufb01rst review the operation of our parser. It begins with\na default initial con\ufb01guration where the stack contains the ROOT , the input list is just\nthe list of words, and the set of relations is empty. The LEFT ARCand RIGHT ARC\noperators each add relations between the words at the top of the stack to the set of\nrelations being accumulated for a given sentence. Since we have a gold-standard\nreference parse for each training sentence, we know which dependency relations are\nvalid for a given sentence. Therefore, we can use the reference parse to guide the",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 625,
      "token_count": 324,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 428\n\n420 CHAPTER 19 \u2022 D EPENDENCY PARSING\nStep Stack Word List Predicted Action\n0 [root] [book, the, \ufb02ight, through, houston] SHIFT\n1 [root, book] [the, \ufb02ight, through, houston] SHIFT\n2 [root, book, the] [\ufb02ight, through, houston] SHIFT\n3 [root, book, the, \ufb02ight] [through, houston] LEFT ARC\n4 [root, book, \ufb02ight] [through, houston] SHIFT\n5 [root, book, \ufb02ight, through] [houston] SHIFT\n6 [root, book, \ufb02ight, through, houston] [] LEFT ARC\n7 [root, book, \ufb02ight, houston ] [] RIGHT ARC\n8 [root, book, \ufb02ight] [] RIGHT ARC\n9 [root, book] [] RIGHT ARC\n10 [root] [] Done\nFigure 19.7 Generating training items consisting of con\ufb01guration/predicted action pairs by simulating a parse\nwith a given reference parse.\nselection of operators as the parser steps through a sequence of con\ufb01gurations.\nTo be more precise, given a reference parse and a con\ufb01guration, the training\noracle proceeds as follows:\n\u2022 Choose LEFT ARCif it produces a correct head-dependent relation given the\nreference parse and the current con\ufb01guration,\n\u2022 Otherwise, choose RIGHT ARCif (1) it produces a correct head-dependent re-\nlation given the reference parse and (2) all of the dependents of the word at\nthe top of the stack have already been assigned,\n\u2022 Otherwise, choose SHIFT .\nThe restriction on selecting the RIGHT ARCoperator is needed to ensure that a\nword is not popped from the stack, and thus lost to further processing, before all its\ndependents have been assigned to it.\nMore formally, during training the oracle has access to the following:\n\u2022 A current con\ufb01guration with a stack Sand a set of dependency relations Rc\n\u2022 A reference parse consisting of a set of vertices Vand a set of dependency\nrelations Rp\nGiven this information, the oracle chooses transitions as follows:\nLEFT ARC(r):if(S1r S2)2Rp\nRIGHT ARC(r):if(S2r S1)2Rpand8r0;w s:t:(S1r0w)2Rpthen(S1r0w)2Rc\nSHIFT :otherwise\nLet\u2019s walk through the processing of the following example as shown in Fig. 19.7.\nBook the\ufb02ight through Houstonobj\ndetnmod\ncaseroot\n(19.9)\nAt Step 1, LEFT ARCis not applicable in the initial con\ufb01guration since it asserts\na relation, (root book), not in the reference answer; RIGHT ARCdoes assert a\nrelation contained in the \ufb01nal answer (root !book), however book has not been\nattached to any of its dependents yet, so we have to defer, leaving SHIFT as the only",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 626,
      "token_count": 658,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 429\n\n19.2 \u2022 T RANSITION -BASED DEPENDENCY PARSING 421\npossible action. The same conditions hold in the next two steps. In step 3, LEFT ARC\nis selected to link theto its head.\nNow consider the situation in Step 4.\nStack Word buffer Relations\n[root, book, \ufb02ight] [through, Houston] (the \ufb02ight)\nHere, we might be tempted to add a dependency relation between book and\ufb02ight ,\nwhich is present in the reference parse. But doing so now would prevent the later\nattachment of Houston since \ufb02ight would have been removed from the stack. For-\ntunately, the precondition on choosing RIGHT ARCprevents this choice and we\u2019re\nagain left with SHIFT as the only viable option. The remaining choices complete the\nset of operators needed for this example.\nTo recap, we derive appropriate training instances consisting of con\ufb01guration-\ntransition pairs from a treebank by simulating the operation of a parser in the con-\ntext of a reference dependency tree. We can deterministically record correct parser\nactions at each step as we progress through each training example, thereby creating\nthe training set we require.\n19.2.2 A feature-based classi\ufb01er\nWe\u2019ll now introduce two classi\ufb01ers for choosing transitions, here a classic feature-\nbased algorithm and in the next section a neural classi\ufb01er using embedding features.\nFeatured-based classi\ufb01ers generally use the same features we\u2019ve seen with part-\nof-speech tagging and partial parsing: Word forms, lemmas, parts of speech, the\nhead, and the dependency relation to the head. Other features may be relevant for\nsome languages, for example morphosyntactic features like case marking on subjects\nor objects. The features are extracted from the training con\ufb01gurations , which consist\nof the stack, the buffer and the current set of relations. Most useful are features\nreferencing the top levels of the stack, the words near the front of the buffer, and the\ndependency relations already associated with any of those elements.\nWe\u2019ll use a feature template as we did for sentiment analysis and part-of-speechfeature\ntemplate\ntagging. Feature templates allow us to automatically generate large numbers of spe-\nci\ufb01c features from a training set. For example, consider the following feature tem-\nplates that are based on single positions in a con\ufb01guration.\nhs1:w;opi;hs2:w;opihs1:t;opi;hs2:t;opi\nhb1:w;opi;hb1:t;opihs1:wt;opi (19.10)\nHere features are denoted as location:property , where s= stack, b= the word\nbuffer, w= word forms, t= part-of-speech, and op= operator. Thus the feature for\nthe word form at the top of the stack would be s1:w, the part of speech tag at the\nfront of the buffer b1:t, and the concatenated feature s1:wtrepresents the word form\nconcatenated with the part of speech of the word at the top of the stack. Consider\napplying these templates to the following intermediate con\ufb01guration derived from a\ntraining oracle for (19.2).\nStack Word buffer Relations\n[root, canceled, \ufb02ights] [to Houston] (canceled!United)\n(\ufb02ights!morning)\n(\ufb02ights!the)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 627,
      "token_count": 736,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 430\n\n422 CHAPTER 19 \u2022 D EPENDENCY PARSING\nThe correct transition here is SHIFT (you should convince yourself of this before\nproceeding). The application of our set of feature templates to this con\ufb01guration\nwould result in the following set of instantiated features.\nhs1:w=\ufb02ights;op=shifti (19.11)\nhs2:w=canceled;op=shifti\nhs1:t=NNS;op=shifti\nhs2:t=VBD;op=shifti\nhb1:w=to;op=shifti\nhb1:t=TO;op=shifti\nhs1:wt=\ufb02ightsNNS;op=shifti\nGiven that the left and right arc transitions operate on the top two elements of the\nstack, features that combine properties from these positions are even more useful.\nFor example, a feature like s1:t\u000es2:tconcatenates the part of speech tag of the word\nat the top of the stack with the tag of the word beneath it.\nhs1:t\u000es2:t=NNSVBD;op=shifti (19.12)\nGiven the training data and features, any classi\ufb01er, like multinomial logistic re-\ngression or support vector machines, can be used.\n19.2.3 A neural classi\ufb01er\nThe oracle can also be implemented by a neural classi\ufb01er. A standard architecture\nis simply to pass the sentence through an encoder, then take the presentation of the\ntop 2 words on the stack and the \ufb01rst word of the buffer, concatenate them, and\npresent to a feedforward network that predicts the transition to take (Kiperwasser\nand Goldberg, 2016; Kulmizev et al., 2019). Fig. 19.8 sketches this model. Learning\ncan be done with cross-entropy loss.\nw\u2026s2...s1Input bu\ufb00erStackLEFTARCRIGHTARCSHIFTActionDependencyRelationsw3w2ENCODERw1w2w3w4w5w6Parser OracleSoftmaxFFNws1s2e(w)e(s1)e(s2)\nFigure 19.8 Neural classi\ufb01er for the oracle for the transition-based parser. The parser takes\nthe top 2 words on the stack and the \ufb01rst word of the buffer, represents them by their encodings\n(from running the whole sentence through the encoder), concatenates the embeddings and\npasses through a softmax to choose a parser action (transition).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 628,
      "token_count": 543,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 431\n\n19.2 \u2022 T RANSITION -BASED DEPENDENCY PARSING 423\n19.2.4 Advanced Methods in Transition-Based Parsing\nThe basic transition-based approach can be elaborated in a number of ways to im-\nprove performance by addressing some of the most obvious \ufb02aws in the approach.\nAlternative Transition Systems\nThe arc-standard transition system described above is only one of many possible sys-\ntems. A frequently used alternative is the arc eager transition system. The arc eager arc eager\napproach gets its name from its ability to assert rightward relations much sooner\nthan in the arc standard approach. To see this, let\u2019s revisit the arc standard trace of\nExample 19.9, repeated here.\nBook the\ufb02ight through Houstonobj\ndetnmod\ncaseroot\nConsider the dependency relation between book and\ufb02ight in this analysis. As\nis shown in Fig. 19.7, an arc-standard approach would assert this relation at Step 8,\ndespite the fact that book and\ufb02ight \ufb01rst come together on the stack much earlier at\nStep 4. The reason this relation can\u2019t be captured at this point is due to the presence\nof the postnominal modi\ufb01er through Houston . In an arc-standard approach, depen-\ndents are removed from the stack as soon as they are assigned their heads. If \ufb02ight\nhad been assigned book as its head in Step 4, it would no longer be available to serve\nas the head of Houston .\nWhile this delay doesn\u2019t cause any issues in this example, in general the longer\na word has to wait to get assigned its head the more opportunities there are for\nsomething to go awry. The arc-eager system addresses this issue by allowing words\nto be attached to their heads as early as possible, before all the subsequent words\ndependent on them have been seen. This is accomplished through minor changes to\ntheLEFT ARCand RIGHT ARCoperators and the addition of a new REDUCE operator.\n\u2022LEFT ARC: Assert a head-dependent relation between the word at the front of\nthe input buffer and the word at the top of the stack; pop the stack.\n\u2022RIGHT ARC: Assert a head-dependent relation between the word on the top of\nthe stack and the word at the front of the input buffer; shift the word at the\nfront of the input buffer to the stack.\n\u2022SHIFT : Remove the word from the front of the input buffer and push it onto\nthe stack.\n\u2022REDUCE : Pop the stack.\nThe LEFT ARCand RIGHT ARCoperators are applied to the top of the stack and\nthe front of the input buffer, instead of the top two elements of the stack as in the\narc-standard approach. The RIGHT ARCoperator now moves the dependent to the\nstack from the buffer rather than removing it, thus making it available to serve as the\nhead of following words. The new REDUCE operator removes the top element from\nthe stack. Together these changes permit a word to be eagerly assigned its head and\nstill allow it to serve as the head for later dependents. The trace shown in Fig. 19.9\nillustrates the new decision sequence for this example.\nIn addition to demonstrating the arc-eager transition system, this example demon-\nstrates the power and \ufb02exibility of the overall transition-based approach. We were\nable to swap in a new transition system without having to make any changes to the",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 629,
      "token_count": 724,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 432",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 630,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "424 CHAPTER 19 \u2022 D EPENDENCY PARSING\nStep Stack Word List Action Relation Added\n0 [root] [book, the, \ufb02ight, through, houston] RIGHT ARC (root!book)\n1 [root, book] [the, \ufb02ight, through, houston] SHIFT\n2 [root, book, the] [\ufb02ight, through, houston] LEFT ARC (the \ufb02ight)\n3 [root, book] [\ufb02ight, through, houston] RIGHT ARC (book!\ufb02ight)\n4 [root, book, \ufb02ight] [through, houston] SHIFT\n5 [root, book, \ufb02ight, through] [houston] LEFT ARC (through houston)\n6 [root, book, \ufb02ight] [houston] RIGHT ARC (\ufb02ight!houston)\n7 [root, book, \ufb02ight, houston] [] REDUCE\n8 [root, book, \ufb02ight] [] REDUCE\n9 [root, book] [] REDUCE\n10 [root] [] Done\nFigure 19.9 A processing trace of Book the \ufb02ight through Houston using the arc-eager transition operators.\nunderlying parsing algorithm. This \ufb02exibility has led to the development of a di-\nverse set of transition systems that address different aspects of syntax and semantics\nincluding: assigning part of speech tags (Choi and Palmer, 2011a), allowing the\ngeneration of non-projective dependency structures (Nivre, 2009), assigning seman-\ntic roles (Choi and Palmer, 2011b), and parsing texts containing multiple languages\n(Bhat et al., 2017).\nBeam Search\nThe computational ef\ufb01ciency of the transition-based approach discussed earlier de-\nrives from the fact that it makes a single pass through the sentence, greedily making\ndecisions without considering alternatives. Of course, this is also a weakness \u2013 once\na decision has been made it can not be undone, even in the face of overwhelming\nevidence arriving later in a sentence. We can use beam search to explore alterna- beam search\ntive decision sequences. Recall from Chapter 9 that beam search uses a breadth-\ufb01rst\nsearch strategy with a heuristic \ufb01lter that prunes the search frontier to stay within a\n\ufb01xed-size beam width . beam width\nIn applying beam search to transition-based parsing, we\u2019ll elaborate on the al-\ngorithm given in Fig. 19.5. Instead of choosing the single best transition operator\nat each iteration, we\u2019ll apply all applicable operators to each state on an agenda and\nthen score the resulting con\ufb01gurations. We then add each of these new con\ufb01gura-\ntions to the frontier, subject to the constraint that there has to be room within the\nbeam. As long as the size of the agenda is within the speci\ufb01ed beam width, we can\nadd new con\ufb01gurations to the agenda. Once the agenda reaches the limit, we only\nadd new con\ufb01gurations that are better than the worst con\ufb01guration on the agenda\n(removing the worst element so that we stay within the limit). Finally, to insure that\nwe retrieve the best possible state on the agenda, the while loop continues as long as\nthere are non-\ufb01nal states on the agenda.\nThe beam search approach requires a more elaborate notion of scoring than we\nused with the greedy algorithm. There, we assumed that the oracle would be a\nsupervised classi\ufb01er that chose the best transition operator based on features of the",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 631,
      "token_count": 776,
      "chapter_title": ""
    }
  },
  {
    "content": "search strategy with a heuristic \ufb01lter that prunes the search frontier to stay within a\n\ufb01xed-size beam width . beam width\nIn applying beam search to transition-based parsing, we\u2019ll elaborate on the al-\ngorithm given in Fig. 19.5. Instead of choosing the single best transition operator\nat each iteration, we\u2019ll apply all applicable operators to each state on an agenda and\nthen score the resulting con\ufb01gurations. We then add each of these new con\ufb01gura-\ntions to the frontier, subject to the constraint that there has to be room within the\nbeam. As long as the size of the agenda is within the speci\ufb01ed beam width, we can\nadd new con\ufb01gurations to the agenda. Once the agenda reaches the limit, we only\nadd new con\ufb01gurations that are better than the worst con\ufb01guration on the agenda\n(removing the worst element so that we stay within the limit). Finally, to insure that\nwe retrieve the best possible state on the agenda, the while loop continues as long as\nthere are non-\ufb01nal states on the agenda.\nThe beam search approach requires a more elaborate notion of scoring than we\nused with the greedy algorithm. There, we assumed that the oracle would be a\nsupervised classi\ufb01er that chose the best transition operator based on features of the\ncurrent con\ufb01guration. This choice can be viewed as assigning a score to all the\npossible transitions and picking the best one.\n\u02c6T(c) =argmaxScore (t;c)\nWith beam search we are now searching through the space of decision sequences,\nso it makes sense to base the score for a con\ufb01guration on its entire history. So we\ncan de\ufb01ne the score for a new con\ufb01guration as the score of its predecessor plus the",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 632,
      "token_count": 393,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 433\n\n19.3 \u2022 G RAPH -BASED DEPENDENCY PARSING 425\nscore of the operator used to produce it.\nCon\ufb01gScore (c0) = 0:0\nCon\ufb01gScore (ci) = Con\ufb01gScore (ci\u00001)+Score (ti;ci\u00001)\nThis score is used both in \ufb01ltering the agenda and in selecting the \ufb01nal answer. The\nnew beam search version of transition-based parsing is given in Fig. 19.10.\nfunction DEPENDENCY BEAM PARSE (words ,width )returns dependency tree\nstate f[root], [ words ], [], 0.0 g;initial con\ufb01guration\nagenda hstatei ;initial agenda\nwhile agenda contains non-\ufb01nal states\nnewagenda hi\nfor each state2agenda do\nfor all ftjt2VALID OPERATORS (state )gdo\nchild APPLY (t,state )\nnewagenda ADDTOBEAM (child ,newagenda ,width )\nagenda newagenda\nreturn BESTOF(agenda )\nfunction ADDTOBEAM (state ,agenda ,width )returns updated agenda\nifLENGTH (agenda )<width then\nagenda INSERT (state ,agenda )\nelse if SCORE (state )>SCORE (WORST OF(agenda ))\nagenda REMOVE (WORST OF(agenda ))\nagenda INSERT (state ,agenda )\nreturn agenda\nFigure 19.10 Beam search applied to transition-based dependency parsing.\n19.3 Graph-Based Dependency Parsing\nGraph-based methods are the second important family of dependency parsing algo-\nrithms. Graph-based parsers are more accurate than transition-based parsers, espe-\ncially on long sentences; transition-based methods have trouble when the heads are\nvery far from the dependents (McDonald and Nivre, 2011). Graph-based methods\navoid this dif\ufb01culty by scoring entire trees, rather than relying on greedy local de-\ncisions. Furthermore, unlike transition-based approaches, graph-based parsers can\nproduce non-projective trees. Although projectivity is not a signi\ufb01cant issue for\nEnglish, it is de\ufb01nitely a problem for many of the world\u2019s languages.\nGraph-based dependency parsers search through the space of possible trees for a\ngiven sentence for a tree (or trees) that maximize some score. These methods encode\nthe search space as directed graphs and employ methods drawn from graph theory\nto search the space for optimal solutions. More formally, given a sentence Swe\u2019re\nlooking for the best dependency tree in Gs, the space of all possible trees for that\nsentence, that maximizes some score.\n\u02c6T(S) =argmax\nt2GSScore (t;S)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 633,
      "token_count": 568,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 434\n\n426 CHAPTER 19 \u2022 D EPENDENCY PARSING\nWe\u2019ll make the simplifying assumption that this score can be edge-factored , edge-factored\nmeaning that the overall score for a tree is the sum of the scores of each of the scores\nof the edges that comprise the tree.\nScore (t;S) =X\ne2tScore (e)\nGraph-based algorithms have to solve two problems: (1) assigning a score to\neach edge, and (2) \ufb01nding the best parse tree given the scores of all potential edges.\nIn the next few sections we\u2019ll introduce solutions to these two problems, beginning\nwith the second problem of \ufb01nding trees, and then giving a feature-based and a\nneural algorithm for solving the \ufb01rst problem of assigning scores.\n19.3.1 Parsing via \ufb01nding the maximum spanning tree\nIn graph-based parsing, given a sentence Swe start by creating a graph Gwhich is a\nfully-connected, weighted, directed graph where the vertices are the input words and\nthe directed edges represent all possible head-dependent assignments. We\u2019ll include\nan additional ROOT node with outgoing edges directed at all of the other vertices.\nThe weights of each edge in Gre\ufb02ect the score for each possible head-dependent\nrelation assigned by some scoring algorithm.\nIt turns out that \ufb01nding the best dependency parse for Sis equivalent to \ufb01nding\nthemaximum spanning tree over G. A spanning tree over a graph Gis a subsetmaximum\nspanning tree\nofGthat is a tree and covers all the vertices in G; a spanning tree over Gthat starts\nfrom the ROOT is a valid parse of S. A maximum spanning tree is the spanning tree\nwith the highest score. Thus a maximum spanning tree of Gemanating from the\nROOT is the optimal dependency parse for the sentence.\nA directed graph for the example Book that \ufb02ight is shown in Fig. 19.11, with the\nmaximum spanning tree corresponding to the desired parse shown in blue. For ease\nof exposition, we\u2019ll describe here the algorithm for unlabeled dependency parsing.\nrootBookthat\ufb02ight1244568757\nFigure 19.11 Initial rooted, directed graph for Book that \ufb02ight .\nBefore describing the algorithm it\u2019s useful to consider two intuitions about di-\nrected graphs and their spanning trees. The \ufb01rst intuition begins with the fact that\nevery vertex in a spanning tree has exactly one incoming edge. It follows from this\nthat every connected component of a spanning tree (i.e., every set of vertices that\nare linked to each other by paths over edges) will also have one incoming edge.\nThe second intuition is that the absolute values of the edge scores are not critical\nto determining its maximum spanning tree. Instead, it is the relative weights of the\nedges entering each vertex that matters. If we were to subtract a constant amount\nfrom each edge entering a given vertex it would have no impact on the choice of",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 634,
      "token_count": 631,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 435",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 635,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "19.3 \u2022 G RAPH -BASED DEPENDENCY PARSING 427\nthe maximum spanning tree since every possible spanning tree would decrease by\nexactly the same amount.\nThe \ufb01rst step of the algorithm itself is quite straightforward. For each vertex\nin the graph, an incoming edge (representing a possible head assignment) with the\nhighest score is chosen. If the resulting set of edges produces a spanning tree then\nwe\u2019re done. More formally, given the original fully-connected graph G= (V;E), a\nsubgraph T= (V;F)is a spanning tree if it has no cycles and each vertex (other than\nthe root) has exactly one edge entering it. If the greedy selection process produces\nsuch a tree then it is the best possible one.\nUnfortunately, this approach doesn\u2019t always lead to a tree since the set of edges\nselected may contain cycles. Fortunately, in yet another case of multiple discovery,\nthere is a straightforward way to eliminate cycles generated during the greedy se-\nlection phase. Chu and Liu (1965) and Edmonds (1967) independently developed\nan approach that begins with greedy selection and follows with an elegant recursive\ncleanup phase that eliminates cycles.\nThe cleanup phase begins by adjusting all the weights in the graph by subtracting\nthe score of the maximum edge entering each vertex from the score of all the edges\nentering that vertex. This is where the intuitions mentioned earlier come into play.\nWe have scaled the values of the edges so that the weights of the edges in the cycle\nhave no bearing on the weight of anyof the possible spanning trees. Subtracting the\nvalue of the edge with maximum weight from each edge entering a vertex results\nin a weight of zero for all of the edges selected during the greedy selection phase,\nincluding all of the edges involved in the cycle .\nHaving adjusted the weights, the algorithm creates a new graph by selecting a\ncycle and collapsing it into a single new node. Edges that enter or leave the cycle\nare altered so that they now enter or leave the newly collapsed node. Edges that do\nnot touch the cycle are included and edges within the cycle are dropped.\nNow, if we knew the maximum spanning tree of this new graph, we would have\nwhat we need to eliminate the cycle. The edge of the maximum spanning tree di-\nrected towards the vertex representing the collapsed cycle tells us which edge to\ndelete in order to eliminate the cycle. How do we \ufb01nd the maximum spanning tree\nof this new graph? We recursively apply the algorithm to the new graph. This will\neither result in a spanning tree or a graph with a cycle. The recursions can continue\nas long as cycles are encountered. When each recursion completes we expand the\ncollapsed vertex, restoring all the vertices and edges from the cycle with the excep-\ntion of the single edge to be deleted .\nPutting all this together, the maximum spanning tree algorithm consists of greedy\nedge selection, re-scoring of edge costs and a recursive cleanup phase when needed.\nThe full algorithm is shown in Fig. 19.12.\nFig. 19.13 steps through the algorithm with our Book that \ufb02ight example. The\n\ufb01rst row of the \ufb01gure illustrates greedy edge selection with the edges chosen shown\nin blue (corresponding to the set Fin the algorithm). This results in a cycle between\nthatand\ufb02ight . The scaled weights using the maximum value entering each node are\nshown in the graph to the right.\nCollapsing the cycle between that and\ufb02ight to a single node (labelled tf) and\nrecursing with the newly scaled costs is shown in the second row. The greedy selec-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 636,
      "token_count": 771,
      "chapter_title": ""
    }
  },
  {
    "content": "rected towards the vertex representing the collapsed cycle tells us which edge to\ndelete in order to eliminate the cycle. How do we \ufb01nd the maximum spanning tree\nof this new graph? We recursively apply the algorithm to the new graph. This will\neither result in a spanning tree or a graph with a cycle. The recursions can continue\nas long as cycles are encountered. When each recursion completes we expand the\ncollapsed vertex, restoring all the vertices and edges from the cycle with the excep-\ntion of the single edge to be deleted .\nPutting all this together, the maximum spanning tree algorithm consists of greedy\nedge selection, re-scoring of edge costs and a recursive cleanup phase when needed.\nThe full algorithm is shown in Fig. 19.12.\nFig. 19.13 steps through the algorithm with our Book that \ufb02ight example. The\n\ufb01rst row of the \ufb01gure illustrates greedy edge selection with the edges chosen shown\nin blue (corresponding to the set Fin the algorithm). This results in a cycle between\nthatand\ufb02ight . The scaled weights using the maximum value entering each node are\nshown in the graph to the right.\nCollapsing the cycle between that and\ufb02ight to a single node (labelled tf) and\nrecursing with the newly scaled costs is shown in the second row. The greedy selec-\ntion step in this recursion yields a spanning tree that links roottobook , as well as an\nedge that links book to the contracted node. Expanding the contracted node, we can\nsee that this edge corresponds to the edge from book to\ufb02ight in the original graph.\nThis in turn tells us which edge to drop to eliminate the cycle.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 637,
      "token_count": 360,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 436\n\n428 CHAPTER 19 \u2022 D EPENDENCY PARSING\nfunction MAXSPANNING TREE(G=(V ,E) ,root,score )returns spanning tree\nF []\nT\u2019 []\nscore\u2019 []\nfor each v2Vdo\nbestInEdge argmaxe=(u;v)2Escore[e]\nF F[bestInEdge\nfor each e=(u,v)2Edo\nscore\u2019[e] score[e]\u0000score[bestInEdge]\nifT=(V ,F) is a spanning tree then return it\nelse\nC a cycle in F\nG\u2019 CONTRACT (G,C)\nT\u2019 MAXSPANNING TREE(G\u2019,root,score\u2019 )\nT EXPAND (T\u2019,C)\nreturn T\nfunction CONTRACT (G,C)returns contracted graph\nfunction EXPAND (T,C)returns expanded graph\nFigure 19.12 The Chu-Liu Edmonds algorithm for \ufb01nding a maximum spanning tree in a\nweighted directed graph.\nOn arbitrary directed graphs, this version of the CLE algorithm runs in O(mn)\ntime, where mis the number of edges and nis the number of nodes. Since this par-\nticular application of the algorithm begins by constructing a fully connected graph\nm=n2yielding a running time of O(n3). Gabow et al. (1986) present a more ef\ufb01-\ncient implementation with a running time of O(m+nlogn ).\n19.3.2 A feature-based algorithm for assigning scores\nRecall that given a sentence, S, and a candidate tree, T, edge-factored parsing models\nmake the simpli\ufb01cation that the score for the tree is the sum of the scores of the edges\nthat comprise the tree:\nscore(S;T) =X\ne2Tscore(S;e)\nIn a feature-based algorithm we compute the edge score as a weighted sum of fea-\ntures extracted from it:\nscore(S;e) =NX\ni=1wifi(S;e)\nOr more succinctly.\nscore(S;e) = w\u0001f\nGiven this formulation, we need to identify relevant features and train the weights.\nThe features (and feature combinations) used to train edge-factored models mir-\nror those used in training transition-based parsers, such as",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 638,
      "token_count": 478,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 437\n\n19.3 \u2022 G RAPH -BASED DEPENDENCY PARSING 429\nrootBooktf\nrootBookthat\ufb02ight0-3-4\n-7-1-6-2rootBook12that7\ufb02ight8-4-30-2-6-1-700\nrootBook0tf-10-3-4\n-7-1-6-2rootBook12that7\ufb02ight81244568757\nDeleted from cycle\nFigure 19.13 Chu-Liu-Edmonds graph-based example for Book that \ufb02ight\n\u2022 Wordforms, lemmas, and parts of speech of the headword and its dependent.\n\u2022 Corresponding features from the contexts before, after and between the words.\n\u2022 Word embeddings.\n\u2022 The dependency relation itself.\n\u2022 The direction of the relation (to the right or left).\n\u2022 The distance from the head to the dependent.\nGiven a set of features, our next problem is to learn a set of weights correspond-\ning to each. Unlike many of the learning problems discussed in earlier chapters,\nhere we are not training a model to associate training items with class labels, or\nparser actions. Instead, we seek to train a model that assigns higher scores to cor-\nrect trees than to incorrect ones. An effective framework for problems like this is to\nuseinference-based learning combined with the perceptron learning rule. In thisinference-based\nlearning\nframework, we parse a sentence (i.e, perform inference) from the training set using\nsome initially random set of initial weights. If the resulting parse matches the cor-\nresponding tree in the training data, we do nothing to the weights. Otherwise, we\n\ufb01nd those features in the incorrect parse that are notpresent in the reference parse\nand we lower their weights by a small amount based on the learning rate. We do this\nincrementally for each sentence in our training data until the weights converge.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 639,
      "token_count": 409,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 438\n\n430 CHAPTER 19 \u2022 D EPENDENCY PARSING\n19.3.3 A neural algorithm for assigning scores\nState-of-the-art graph-based multilingual parsers are based on neural networks. In-\nstead of extracting hand-designed features to represent each edge between words wi\nandwj, these parsers run the sentence through an encoder, and then pass the encoded\nrepresentation of the two words wiandwjthrough a network that estimates a score\nfor the edge i!j.\nbookthat\ufb02ightr1score(h1head, h3dep)Bia\ufb03neb\nENCODERUh1 headFFNheadFFNheadFFNdepFFNdeph1 depFFNheadFFNdeph2 headh2 deph3 headh3 depWr2r3\u2211+\nFigure 19.14 Computing scores for a single edge (book !\ufb02ight) in the biaf\ufb01ne parser of\nDozat and Manning (2017); Dozat et al. (2017). The parser uses distinct feedforward net-\nworks to turn the encoder output for each word into a head and dependent representation for\nthe word. The biaf\ufb01ne function turns the head embedding of the head and the dependent\nembedding of the dependent into a score for the dependency edge.\nHere we\u2019ll sketch the biaf\ufb01ne algorithm of Dozat and Manning (2017) and Dozat\net al. (2017) shown in Fig. 19.14, drawing on the work of Gr \u00a8unewald et al. (2021)\nwho tested many versions of the algorithm via their STEPS system. The algorithm\n\ufb01rst runs the sentence X=x1;:::;xnthrough an encoder to produce a contextual\nembedding representation for each token R=r1;:::;rn. The embedding for each\ntoken is now passed through two separate feedforward networks, one to produce a\nrepresentation of this token as a head, and one to produce a representation of this\ntoken as a dependent:\nhhead\ni=FFNhead(ri) (19.13)\nhdep\ni=FFNdep(ri) (19.14)\nNow to assign a score to the directed edge i!j, (wiis the head and wjis the depen-\ndent), we feed the head representation of i,hhead\ni, and the dependent representation\nofj,hdep\nj, into a biaf\ufb01ne scoring function:\nScore (i!j) = Biaff(hhead\ni;hdep\nj) (19.15)\nBiaff(x;y) = x|Uy+W(x\by)+b (19.16)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 640,
      "token_count": 578,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 439\n\n19.4 \u2022 E VALUATION 431\nwhere U,W, and bare weights learned by the model. The idea of using a biaf\ufb01ne\nfunction is to allow the system to learn multiplicative interactions between the vec-\ntorsxandy.\nIf we pass Score (i!j)through a softmax, we end up with a probability distri-\nbution, for each token j, over potential heads i(all other tokens in the sentence):\np(i!j) =softmax ([Score (k!j);8k6=j;1\u0014k\u0014n]) (19.17)\nThis probability can then be passed to the maximum spanning tree algorithm of\nSection 19.3.1 to \ufb01nd the best tree.\nThis p(i!j)classi\ufb01er is trained by optimizing the cross-entropy loss.\nNote that the algorithm as we\u2019ve described it is unlabeled. To make this into\na labeled algorithm, the Dozat and Manning (2017) algorithm actually trains two\nclassi\ufb01ers. The \ufb01rst classi\ufb01er, the edge-scorer , the one we described above, assigns\na probability p(i!j)to each word wiandwj. Then the Maximum Spanning Tree\nalgorithm is run to get a single best dependency parse tree for the second. We then\napply a second classi\ufb01er, the label-scorer , whose job is to \ufb01nd the maximum prob-\nability label for each edge in this parse. This second classi\ufb01er has the same form\nas (19.15-19.17), but instead of being trained to predict with binary softmax the\nprobability of an edge existing between two words, it is trained with a softmax over\ndependency labels to predict the dependency label between the words.\n19.4 Evaluation\nAs with phrase structure-based parsing, the evaluation of dependency parsers pro-\nceeds by measuring how well they work on a test set. An obvious metric would be\nexact match (EM)\u2014how many sentences are parsed correctly. This metric is quite\npessimistic, with most sentences being marked wrong. Such measures are not \ufb01ne-\ngrained enough to guide the development process. Our metrics need to be sensitive\nenough to tell if actual improvements are being made.\nFor these reasons, the most common method for evaluating dependency parsers\nare labeled and unlabeled attachment accuracy. Labeled attachment refers to the\nproper assignment of a word to its head along with the correct dependency relation.\nUnlabeled attachment simply looks at the correctness of the assigned head, ignor-\ning the dependency relation. Given a system output and a corresponding reference\nparse, accuracy is simply the percentage of words in an input that are assigned the\ncorrect head with the correct relation. These metrics are usually referred to as the\nlabeled attachment score (LAS) and unlabeled attachment score (UAS). Finally, we\ncan make use of a label accuracy score (LS), the percentage of tokens with correct\nlabels, ignoring where the relations are coming from.\nAs an example, consider the reference parse and system parse for the following\nexample shown in Fig. 19.15.\n(19.18) Book me the \ufb02ight through Houston.\nThe system correctly \ufb01nds 4 of the 6 dependency relations present in the reference\nparse and receives an LAS of 2/3. However, one of the 2 incorrect relations found\nby the system holds between book and\ufb02ight , which are in a head-dependent relation\nin the reference parse; the system therefore achieves a UAS of 5/6.\nBeyond attachment scores, we may also be interested in how well a system is\nperforming on a particular kind of dependency relation, for example NSUBJ , across",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 641,
      "token_count": 790,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 440\n\n432 CHAPTER 19 \u2022 D EPENDENCY PARSING\nBook methe \ufb02ight through Houston\n(a) Referenceiobjobj\ndetnmod\ncaseroot\nBook methe \ufb02ight through Houston\n(b) Systemxcomp\nnsubj\ndetnmod\ncaseroot\nFigure 19.15 Reference and system parses for Book me the \ufb02ight through Houston , resulting in an LAS of\n2/3 and an UAS of 5/6.\na development corpus. Here we can make use of the notions of precision and recall\nintroduced in Chapter 17, measuring the percentage of relations labeled NSUBJ by\nthe system that were correct (precision), and the percentage of the NSUBJ relations\npresent in the development set that were in fact discovered by the system (recall).\nWe can employ a confusion matrix to keep track of how often each dependency type\nwas confused for another.\n19.5 Summary\nThis chapter has introduced the concept of dependency grammars and dependency\nparsing. Here\u2019s a summary of the main points that we covered:\n\u2022 In dependency-based approaches to syntax, the structure of a sentence is de-\nscribed in terms of a set of binary relations that hold between the words in a\nsentence. Larger notions of constituency are not directly encoded in depen-\ndency analyses.\n\u2022 The relations in a dependency structure capture the head-dependent relation-\nship among the words in a sentence.\n\u2022 Dependency-based analysis provides information directly useful in further\nlanguage processing tasks including information extraction, semantic parsing\nand question answering.\n\u2022 Transition-based parsing systems employ a greedy stack-based algorithm to\ncreate dependency structures.\n\u2022 Graph-based methods for creating dependency structures are based on the use\nof maximum spanning tree methods from graph theory.\n\u2022 Both transition-based and graph-based approaches are developed using super-\nvised machine learning techniques.\n\u2022 Treebanks provide the data needed to train these systems. Dependency tree-\nbanks can be created directly by human annotators or via automatic transfor-\nmation from phrase-structure treebanks.\n\u2022 Evaluation of dependency parsers is based on labeled and unlabeled accuracy\nscores as measured against withheld development and test corpora.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 642,
      "token_count": 451,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 441",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 643,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 433\nBibliographical and Historical Notes\nThe dependency-based approach to grammar is much older than the relatively recent\nphrase-structure or constituency grammars, which date only to the 20th century. De-\npendency grammar dates back to the Indian grammarian P \u00afan.ini sometime between\nthe 7th and 4th centuries BCE, as well as the ancient Greek linguistic traditions.\nContemporary theories of dependency grammar all draw heavily on the 20th cen-\ntury work of Tesni `ere (1959).\nAutomatic parsing using dependency grammars was \ufb01rst introduced into compu-\ntational linguistics by early work on machine translation at the RAND Corporation\nled by David Hays. This work on dependency parsing closely paralleled work on\nconstituent parsing and made explicit use of grammars to guide the parsing process.\nAfter this early period, computational work on dependency parsing remained inter-\nmittent over the following decades. Notable implementations of dependency parsers\nfor English during this period include Link Grammar (Sleator and Temperley, 1993),\nConstraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003).\nDependency parsing saw a major resurgence in the late 1990\u2019s with the appear-\nance of large dependency-based treebanks and the associated advent of data driven\napproaches described in this chapter. Eisner (1996) developed an ef\ufb01cient dynamic\nprogramming approach to dependency parsing based on bilexical grammars derived\nfrom the Penn Treebank. Covington (2001) introduced the deterministic word by\nword approach underlying current transition-based approaches. Yamada and Mat-\nsumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce\nparadigm and the use of supervised machine learning in the form of support vector\nmachines to dependency parsing.\nTransition-based parsing is based on the shift-reduce parsing algorithm orig-\ninally developed for analyzing programming languages (Aho and Ullman, 1972).\nShift-reduce parsing also makes use of a context-free grammar. Input tokens are\nsuccessively shifted onto the stack and the top two elements of the stack are matched\nagainst the right-hand side of the rules in the grammar; when a match is found the\nmatched elements are replaced on the stack (reduced) by the non-terminal from the\nleft-hand side of the rule being matched. In transition-based dependency parsing\nwe skip the grammar, and alter the reduce operation to add a dependency relation\nbetween a word and its head.\nNivre (2003) de\ufb01ned the modern, deterministic, transition-based approach to\ndependency parsing. Subsequent work by Nivre and his colleagues formalized and\nanalyzed the performance of numerous transition systems, training methods, and\nmethods for dealing with non-projective language (Nivre and Scholz 2004, Nivre\n2006, Nivre and Nilsson 2005, Nivre et al. 2007b, Nivre 2007). The neural ap-\nproach was pioneered by Chen and Manning (2014) and extended by Kiperwasser\nand Goldberg (2016); Kulmizev et al. (2019).\nThe graph-based maximum spanning tree approach to dependency parsing was\nintroduced by McDonald et al. 2005a, McDonald et al. 2005b. The neural classi\ufb01er\nwas introduced by (Kiperwasser and Goldberg, 2016).\nThe long-running Prague Dependency Treebank project (Haji \u02c7c, 1998) is the most",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 644,
      "token_count": 763,
      "chapter_title": ""
    }
  },
  {
    "content": "against the right-hand side of the rules in the grammar; when a match is found the\nmatched elements are replaced on the stack (reduced) by the non-terminal from the\nleft-hand side of the rule being matched. In transition-based dependency parsing\nwe skip the grammar, and alter the reduce operation to add a dependency relation\nbetween a word and its head.\nNivre (2003) de\ufb01ned the modern, deterministic, transition-based approach to\ndependency parsing. Subsequent work by Nivre and his colleagues formalized and\nanalyzed the performance of numerous transition systems, training methods, and\nmethods for dealing with non-projective language (Nivre and Scholz 2004, Nivre\n2006, Nivre and Nilsson 2005, Nivre et al. 2007b, Nivre 2007). The neural ap-\nproach was pioneered by Chen and Manning (2014) and extended by Kiperwasser\nand Goldberg (2016); Kulmizev et al. (2019).\nThe graph-based maximum spanning tree approach to dependency parsing was\nintroduced by McDonald et al. 2005a, McDonald et al. 2005b. The neural classi\ufb01er\nwas introduced by (Kiperwasser and Goldberg, 2016).\nThe long-running Prague Dependency Treebank project (Haji \u02c7c, 1998) is the most\nsigni\ufb01cant effort to directly annotate a corpus with multiple layers of morphological,\nsyntactic and semantic information. PDT 3.0 contains over 1.5 M tokens (Bej \u02c7cek\net al., 2013).\nUniversal Dependencies (UD) (de Marneffe et al., 2021) is an open community",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 645,
      "token_count": 370,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 442\n\n434 CHAPTER 19 \u2022 D EPENDENCY PARSING\nproject to create a framework for dependency treebank annotation, with nearly 200\ntreebanks in over 100 languages. The UD annotation scheme evolved out of several\ndistinct efforts including Stanford dependencies (de Marneffe et al. 2006, de Marn-\neffe and Manning 2008, de Marneffe et al. 2014), Google\u2019s universal part-of-speech\ntags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets\n(Zeman, 2008).\nThe Conference on Natural Language Learning (CoNLL) has conducted an in-\n\ufb02uential series of shared tasks related to dependency parsing over the years (Buch-\nholz and Marsi 2006, Nivre et al. 2007a, Surdeanu et al. 2008, Haji \u02c7c et al. 2009).\nMore recent evaluations have focused on parser robustness with respect to morpho-\nlogically rich languages (Seddah et al., 2013), and non-canonical language forms\nsuch as social media, texts, and spoken language (Petrov and McDonald, 2012).\nChoi et al. (2015) presents a performance analysis of 10 dependency parsers across\na range of metrics, as well as DEPEND ABLE, a robust parser evaluation tool.\nExercises",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 646,
      "token_count": 312,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 443\n\nCHAPTER\n20Information Extraction:\nRelations, Events, and Time\nTime will explain.\nJane Austen, Persuasion\nImagine that you are an analyst with an investment \ufb01rm that tracks airline stocks.\nYou\u2019re given the task of determining the relationship (if any) between airline an-\nnouncements of fare increases and the behavior of their stocks the next day. His-\ntorical data about stock prices is easy to come by, but what about the airline an-\nnouncements? You will need to know at least the name of the airline, the nature of\nthe proposed fare hike, the dates of the announcement, and possibly the response of\nother airlines. Fortunately, these can be all found in news articles like this one:\nCiting high fuel prices, United Airlines said Friday it has increased fares\nby $6 per round trip on \ufb02ights to some cities also served by lower-\ncost carriers. American Airlines, a unit of AMR Corp., immediately\nmatched the move, spokesman Tim Wagner said. United, a unit of UAL\nCorp., said the increase took effect Thursday and applies to most routes\nwhere it competes against discount carriers, such as Chicago to Dallas\nand Denver to San Francisco.\nThis chapter presents techniques for extracting limited kinds of semantic con-\ntent from text. This process of information extraction (IE) turns the unstructuredinformation\nextraction\ninformation embedded in texts into structured data, for example for populating a\nrelational database to enable further processing.\nWe begin with the task of relation extraction : \ufb01nding and classifying semanticrelation\nextraction\nrelations among entities mentioned in a text, like child-of (X is the child-of Y), or\npart-whole or geospatial relations. Relation extraction has close links to populat-\ning a relational database, and knowledge graphs , datasets of structured relationalknowledge\ngraphs\nknowledge, are a useful way for search engines to present information to users.\nNext, we discuss event extraction , the task of \ufb01nding events in which these en-event\nextraction\ntities participate, like, in our sample text, the fare increases by United andAmerican\nand the reporting events said andcite. Events are also situated in time , occurring at\na particular date or time, and events can be related temporally, happening before or\nafter or simultaneously with each other. We\u2019ll need to recognize temporal expres-\nsions like Friday ,Thursday ortwo days from now and times such as 3:30 P .M. , and\nnormalize them onto speci\ufb01c calendar dates or times. We\u2019ll need to link Friday to\nthe time of United\u2019s announcement, Thursday to the previous day\u2019s fare increase,\nand we\u2019ll need to produce a timeline in which United\u2019s announcement follows the\nfare increase and American\u2019s announcement follows both of those events.\nThe related task of template \ufb01lling is to \ufb01nd recurring stereotypical events or template \ufb01lling\nsituations in documents and \ufb01ll in the template slots. These slot-\ufb01llers may consist\nof text segments extracted directly from the text, or concepts like times, amounts, or\nontology entities that have been inferred through additional processing. Our airline",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 647,
      "token_count": 667,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 444\n\n436 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\nARTIFACTGENERALAFFILIATIONORGAFFILIATIONPART-WHOLEPERSON-SOCIALPHYSICALLocatedNearBusinessFamilyLasting PersonalCitizen-Resident-Ethnicity-ReligionOrg-Location-OriginFounderEmploymentMembershipOwnershipStudent-AlumInvestorUser-Owner-Inventor-ManufacturerGeographicalSubsidiary\nSports-Affiliation\nFigure 20.1 The 17 relations used in the ACE relation extraction task.\ntext presents such a stereotypical situation since airlines often raise fares and then\nwait to see if competitors follow along. Here we can identify United as a lead air-\nline that initially raised its fares, $6 as the amount, Thursday as the increase date,\nandAmerican as an airline that followed along, leading to a \ufb01lled template like the\nfollowing:\nFARE-RAISE ATTEMPT :2\n6664LEAD AIRLINE : U NITED AIRLINES\nAMOUNT : $6\nEFFECTIVE DATE: 2006-10-26\nFOLLOWER : A MERICAN AIRLINES3\n7775\n20.1 Relation Extraction\nLet\u2019s assume that we have detected the named entities in our sample text (perhaps\nusing the techniques of Chapter 17), and would like to discern the relationships that\nexist among the detected entities:\nCiting high fuel prices, [ ORG United Airlines ] said [ TIME Friday ] it\nhas increased fares by [ MONEY $6] per round trip on \ufb02ights to some\ncities also served by lower-cost carriers. [ ORG American Airlines ], a\nunit of [ ORG AMR Corp.] , immediately matched the move, spokesman\n[PER Tim Wagner ] said. [ ORG United] , a unit of [ ORG UAL Corp.] ,\nsaid the increase took effect [ TIME Thursday] and applies to most\nroutes where it competes against discount carriers, such as [ LOC Chicago]\nto [LOC Dallas] and [ LOC Denver] to [LOC San Francisco] .\nThe text tells us, for example, that Tim Wagner is a spokesman for American\nAirlines , that United is a unit of UAL Corp. , and that American is a unit of AMR .\nThese binary relations are instances of more generic relations such as part-of or\nemploys that are fairly frequent in news-style texts. Figure 20.1 lists the 17 relations\nused in the ACE relation extraction evaluations and Fig. 20.2 shows some sample\nrelations. We might also extract more domain-speci\ufb01c relations such as the notion of\nan airline route. For example from this text we can conclude that United has routes\nto Chicago, Dallas, Denver, and San Francisco.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 648,
      "token_count": 584,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 445\n\n20.1 \u2022 R ELATION EXTRACTION 437\nRelations Types Examples\nPhysical-Located PER-GPE Hewas in Tennessee\nPart-Whole-Subsidiary ORG-ORG XYZ , the parent company of ABC\nPerson-Social-Family PER-PER Yoko \u2019s husband John\nOrg-AFF-Founder PER-ORG Steve Jobs , co-founder of Apple ...\nFigure 20.2 Semantic relations with examples and the named entity types they involve.\nSets of relations have been de\ufb01ned for many other domains as well. For example\nUMLS, the Uni\ufb01ed Medical Language System from the US National Library of\nMedicine has a network that de\ufb01nes 134 broad subject categories, entity types, and\n54 relations between the entities, such as the following:\nEntity Relation Entity\nInjury disrupts Physiological Function\nBodily Location location-of Biologic Function\nAnatomical Structure part-of Organism\nPharmacologic Substance causes Pathological Function\nPharmacologic Substance treats Pathologic Function\nGiven a medical sentence like this one:\n(20.1) Doppler echocardiography can be used to diagnose left anterior descending\nartery stenosis in patients with type 2 diabetes\nWe could thus extract the UMLS relation:\nEchocardiography, Doppler Diagnoses Acquired stenosis\nWikipedia also offers a large supply of relations, drawn from infoboxes , struc- infoboxes\ntured tables associated with certain Wikipedia articles. For example, the Wikipedia\ninfobox for Stanford includes structured facts like state = \"California\" or\npresident = \"Marc Tessier-Lavigne\" . These facts can be turned into rela-\ntions like president-of orlocated-in . or into relations in a metalanguage called RDF RDF\n(Resource Description Framework). An RDF triple is a tuple of entity-relation- RDF triple\nentity, called a subject-predicate-object expression. Here\u2019s a sample RDF triple:\nsubject predicate object\nGolden Gate Park location San Francisco\nFor example the crowdsourced DBpedia (Bizer et al., 2009) is an ontology de-\nrived from Wikipedia containing over 2 billion RDF triples. Another dataset from\nWikipedia infoboxes, Freebase (Bollacker et al., 2008), now part of Wikidata (Vrande \u02c7ci\u00b4c Freebase\nand Kr \u00a8otzsch, 2014), has relations between people and their nationality, or locations,\nand other locations they are contained in.\nWordNet or other ontologies offer useful ontological relations that express hier-\narchical relations between words or concepts. For example WordNet has the is-aor is-a\nhypernym relation between classes, hypernym\nGiraffe is-a ruminant is-a ungulate is-a mammal is-a vertebrate ...\nWordNet also has Instance-of relation between individuals and classes, so that for\nexample San Francisco is in the Instance-of relation with city. Extracting these\nrelations is an important step in extending or building ontologies.\nFinally, there are large datasets that contain sentences hand-labeled with their\nrelations, designed for training and testing relation extractors. The TACRED dataset\n(Zhang et al., 2017) contains 106,264 examples of relation triples about particular\npeople or organizations, labeled in sentences from news and web text drawn from the",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 649,
      "token_count": 708,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 446\n\n438 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\nannual TAC Knowledge Base Population (TAC KBP) challenges. TACRED contains\n41 relation types (like per:city of birth, org:subsidiaries, org:member of, per:spouse),\nplus a no relation tag; examples are shown in Fig. 20.3. About 80% of all examples\nare annotated as no relation; having suf\ufb01cient negative data is important for training\nsupervised classi\ufb01ers.\nExample Entity Types & Label\nCarey will succeed Cathleen P. Black, who held the position for 15\nyears and will take on a new role as chairwoman of Hearst Maga-\nzines, the company said.PERSON /TITLE\nRelation: per:title\nIrene Morgan Kirkaldy, who was born and reared in Baltimore, lived\non Long Island and ran a child-care center in Queens with her second\nhusband, Stanley Kirkaldy.PERSON /CITY\nRelation: per:city ofbirth\nBaldwin declined further comment, and said JetBlue chief executive\nDave Barger was unavailable.Types: PERSON /TITLE\nRelation: norelation\nFigure 20.3 Example sentences and labels from the TACRED dataset (Zhang et al., 2017).\nA standard dataset was also produced for the SemEval 2010 Task 8, detecting\nrelations between nominals (Hendrickx et al., 2009). The dataset has 10,717 exam-\nples, each with a pair of nominals (untyped) hand-labeled with one of 9 directed\nrelations like product-producer ( afactory manufactures suits) orcomponent-whole\n(myapartment has a large kitchen ).\n20.2 Relation Extraction Algorithms\nThere are \ufb01ve main classes of algorithms for relation extraction: handwritten pat-\nterns ,supervised machine learning ,semi-supervised (viabootstrapping ordis-\ntant supervision ), and unsupervised . We\u2019ll introduce each of these in the next\nsections.\n20.2.1 Using Patterns to Extract Relations\nThe earliest and still common algorithm for relation extraction is lexico-syntactic\npatterns, \ufb01rst developed by Hearst (1992a), and therefore often called Hearst pat-\nterns . Consider the following sentence: Hearst patterns\nAgar is a substance prepared from a mixture of red algae, such as Ge-\nlidium, for laboratory or industrial use.\nHearst points out that most human readers will not know what Gelidium is, but that\nthey can readily infer that it is a kind of (a hyponym of)red algae , whatever that is.\nShe suggests that the following lexico-syntactic pattern\nNP0such as NP 1f;NP2:::;(andjor)NPig;i\u00151 (20.2)\nimplies the following semantics\n8NPi;i\u00151;hyponym (NPi;NP0) (20.3)\nallowing us to infer\nhyponym (Gelidium;red algae ) (20.4)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 650,
      "token_count": 668,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 447\n\n20.2 \u2022 R ELATION EXTRACTION ALGORITHMS 439\nNPf, NPg*f,g(andjor) other NP H temples, treasuries, and other important civic buildings\nNPHsuch asfNP,g*f(orjand)gNP red algae such as Gelidium\nsuch NP HasfNP,g*f(orjand)gNP such authors as Herrick, Goldsmith, and Shakespeare\nNPHf,gincludingfNP,g*f(orjand)gNP common-law countries, including Canada and England\nNPHf,gespeciallyfNPg*f(orjand)gNP European countries, especially France, England, and Spain\nFigure 20.4 Hand-built lexico-syntactic patterns for \ufb01nding hypernyms, using fgto mark optionality (Hearst\n1992a, Hearst 1998).\nFigure 20.4 shows \ufb01ve patterns Hearst (1992a, 1998) suggested for inferring\nthe hyponym relation; we\u2019ve shown NPHas the parent/hyponym. Modern versions\nof the pattern-based approach extend it by adding named entity constraints. For\nexample if our goal is to answer questions about \u201cWho holds what of\ufb01ce in which\norganization?\u201d, we can use patterns like the following:\nPER, POSITION of ORG:\nGeorge Marshall, Secretary of State of the United States\nPER (namedjappointedjchosejetc.) PER Prep? POSITION\nTruman appointed Marshall Secretary of State\nPER [be]? (namedjappointedjetc.) Prep? ORG POSITION\nGeorge Marshall was named US Secretary of State\nHand-built patterns have the advantage of high-precision and they can be tailored\nto speci\ufb01c domains. On the other hand, they are often low-recall, and it\u2019s a lot of\nwork to create them for all possible patterns.\n20.2.2 Relation Extraction via Supervised Learning\nSupervised machine learning approaches to relation extraction follow a scheme that\nshould be familiar by now. A \ufb01xed set of relations and entities is chosen, a training\ncorpus is hand-annotated with the relations and entities, and the annotated texts are\nthen used to train classi\ufb01ers to annotate an unseen test set.\nThe most straightforward approach, illustrated in Fig. 20.5 is: (1) Find pairs of\nnamed entities (usually in the same sentence). (2): Apply a relation-classi\ufb01cation\non each pair. The classi\ufb01er can use any supervised technique (logistic regression,\nRNN, Transformer, random forest, etc.).\nAn optional intermediate \ufb01ltering classi\ufb01er can be used to speed up the process-\ning by making a binary decision on whether a given pair of named entities are related\n(by any relation). It\u2019s trained on positive examples extracted directly from all rela-\ntions in the annotated corpus, and negative examples generated from within-sentence\nentity pairs that are not annotated with a relation.\nFeature-based supervised relation classi\ufb01ers. Let\u2019s consider sample features for\na feature-based classi\ufb01er (like logistic regression or random forests), classifying the\nrelationship between American Airlines (Mention 1, or M1) and Tim Wagner (Men-\ntion 2, M2) from this sentence:\n(20.5) American Airlines , a unit of AMR, immediately matched the move,\nspokesman Tim Wagner said\nThese include word features (as embeddings, or 1-hot, stemmed or not):\n\u2022 The headwords of M1 and M2 and their concatenation\nAirlines Wagner Airlines-Wagner",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 651,
      "token_count": 770,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 448\n\n440 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\nfunction FINDRELATIONS (words )returns relations\nrelations nil\nentities FINDENTITIES (words )\nforall entity pairs he1,e2iinentities do\nifRELATED ?(e1,e2)\nrelations relations +CLASSIFY RELATION (e1,e2)\nFigure 20.5 Finding and classifying the relations among entities in a text.\n\u2022 Bag-of-words and bigrams in M1 and M2\nAmerican, Airlines, Tim, Wagner, American Airlines, Tim Wagner\n\u2022 Words or bigrams in particular positions\nM2: -1 spokesman\nM2: +1 said\n\u2022 Bag of words or bigrams between M1 and M2:\na, AMR, of, immediately, matched, move, spokesman, the, unit\nNamed entity features:\n\u2022 Named-entity types and their concatenation\n(M1: ORG, M2: PER, M1M2: ORG-PER)\n\u2022 Entity Level of M1 and M2 (from the set NAME, NOMINAL, PRONOUN)\nM1: NAME [it or he would be PRONOUN]\nM2: NAME [the company would be NOMINAL]\n\u2022 Number of entities between the arguments (in this case 1, for AMR)\nSyntactic structure is a useful signal, often represented as the dependency or\nconstituency syntactic path traversed through the tree between the entities.\n\u2022 Constituent paths between M1 and M2\nNP\"NP\"S\"S#NP\n\u2022 Dependency-tree paths\nAirlines sub jmatched comp said!sub jWagner\nNeural supervised relation classi\ufb01ers Neural models for relation extraction sim-\nilarly treat the task as supervised classi\ufb01cation. Let\u2019s consider a typical system ap-\nplied to the TACRED relation extraction dataset and task (Zhang et al., 2017). In\nTACRED we are given a sentence and two spans within it: a subject, which is a\nperson or organization, and an object, which is any other entity. The task is to assign\na relation from the 42 TAC relations, or no relation.\nA typical Transformer-encoder algorithm, shown in Fig. 20.6, simply takes a\npretrained encoder like BERT and adds a linear layer on top of the sentence repre-\nsentation (for example the BERT [CLS] token), a linear layer that is \ufb01netuned as a\n1-of-N classi\ufb01er to assign one of the 43 labels. The input to the BERT encoder is\npartially de-lexi\ufb01ed; the subject and object entities are replaced in the input by their\nNER tags. This helps keep the system from over\ufb01tting to the individual lexical items\n(Zhang et al., 2017). When using BERT-type Transformers for relation extraction, it\nhelps to use versions of BERT like RoBERTa (Liu et al., 2019) or spanBERT (Joshi\net al., 2020) that don\u2019t have two sequences separated by a [SEP] token, but instead\nform the input from a single long sequence of sentences.\nIn general, if the test set is similar enough to the training set, and if there is\nenough hand-labeled data, supervised relation extraction systems can get high ac-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 652,
      "token_count": 717,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 449\n\n20.2 \u2022 R ELATION EXTRACTION ALGORITHMS 441\nENCODER[CLS][SUBJ_PERSON]wasbornin[OBJ_LOC],MichiganLinearClassi\ufb01erp(relation|SUBJ,OBJ)\nFigure 20.6 Relation extraction as a linear layer on top of an encoder (in this case BERT),\nwith the subject and object entities replaced in the input by their NER tags (Zhang et al. 2017,\nJoshi et al. 2020).\ncuracies. But labeling a large training set is extremely expensive and supervised\nmodels are brittle: they don\u2019t generalize well to different text genres. For this rea-\nson, much research in relation extraction has focused on the semi-supervised and\nunsupervised approaches we turn to next.\n20.2.3 Semisupervised Relation Extraction via Bootstrapping\nSupervised machine learning assumes that we have lots of labeled data. Unfortu-\nnately, this is expensive. But suppose we just have a few high-precision seed pat-\nterns , like those in Section 20.2.1, or perhaps a few seed tuples . That\u2019s enough seed patterns\nseed tuples to bootstrap a classi\ufb01er! Bootstrapping proceeds by taking the entities in the seed\nbootstrapping pair, and then \ufb01nding sentences (on the web, or whatever dataset we are using) that\ncontain both entities. From all such sentences, we extract and generalize the context\naround the entities to learn new patterns. Fig. 20.7 sketches a basic algorithm.\nfunction BOOTSTRAP (Relation R )returns new relation tuples\ntuples Gather a set of seed tuples that have relation R\niterate\nsentences \ufb01nd sentences that contain entities in tuples\npatterns generalize the context between and around entities in sentences\nnewpairs usepatterns to identify more tuples\nnewpairs newpairs with high con\ufb01dence\ntuples tuples +newpairs\nreturn tuples\nFigure 20.7 Bootstrapping from seed entity pairs to learn relations.\nSuppose, for example, that we need to create a list of airline/hub pairs, and we\nknow only that Ryanair has a hub at Charleroi. We can use this seed fact to discover\nnew patterns by \ufb01nding other mentions of this relation in our corpus. We search\nfor the terms Ryanair ,Charleroi andhubin some proximity. Perhaps we \ufb01nd the\nfollowing set of sentences:\n(20.6) Budget airline Ryanair, which uses Charleroi as a hub, scrapped all\nweekend \ufb02ights out of the airport.\n(20.7) All \ufb02ights in and out of Ryanair\u2019s hub at Charleroi airport were grounded on\nFriday...\n(20.8) A spokesman at Charleroi, a main hub for Ryanair, estimated that 8000\npassengers had already been affected.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 653,
      "token_count": 614,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 450\n\n442 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\nFrom these results, we can use the context of words between the entity mentions,\nthe words before mention one, the word after mention two, and the named entity\ntypes of the two mentions, and perhaps other features, to extract general patterns\nsuch as the following:\n/ [ORG], which uses [LOC] as a hub /\n/ [ORG]'s hub at [LOC] /\n/ [LOC], a main hub for [ORG] /\nThese new patterns can then be used to search for additional tuples.\nBootstrapping systems also assign con\ufb01dence values to new tuples to avoid se-con\ufb01dence\nvalues\nmantic drift . In semantic drift, an erroneous pattern leads to the introduction of semantic drift\nerroneous tuples, which, in turn, lead to the creation of problematic patterns and the\nmeaning of the extracted relations \u2018drifts\u2019. Consider the following example:\n(20.9) Sydney has a ferry hub at Circular Quay.\nIf accepted as a positive example, this expression could lead to the incorrect in-\ntroduction of the tuple hSydney;CircularQuayi. Patterns based on this tuple could\npropagate further errors into the database.\nCon\ufb01dence values for patterns are based on balancing two factors: the pattern\u2019s\nperformance with respect to the current set of tuples and the pattern\u2019s productivity\nin terms of the number of matches it produces in the document collection. More\nformally, given a document collection D, a current set of tuples T, and a proposed\npattern p, we need to track two factors:\n\u2022hits(p): the set of tuples in Tthatpmatches while looking in D\n\u2022\ufb01nds (p): The total set of tuples that p\ufb01nds in D\nThe following equation balances these considerations (Riloff and Jones, 1999).\nConfRlogF(p) =jhits(p)j\nj\ufb01nds(p)jlog(j\ufb01nds(p)j) (20.10)\nThis metric is generally normalized to produce a probability.\nWe can assess the con\ufb01dence in a proposed new tuple by combining the evidence\nsupporting it from all the patterns P0that match that tuple in D(Agichtein and Gra-\nvano, 2000). One way to combine such evidence is the noisy-or technique. Assume noisy-or\nthat a given tuple is supported by a subset of the patterns in P, each with its own\ncon\ufb01dence assessed as above. In the noisy-or model, we make two basic assump-\ntions. First, that for a proposed tuple to be false, allof its supporting patterns must\nhave been in error, and second, that the sources of their individual failures are all\nindependent. If we loosely treat our con\ufb01dence measures as probabilities, then the\nprobability of any individual pattern pfailing is 1\u0000Conf(p); the probability of all\nof the supporting patterns for a tuple being wrong is the product of their individual\nfailure probabilities, leaving us with the following equation for our con\ufb01dence in a\nnew tuple.\nConf(t) =1\u0000Y\np2P0(1\u0000Conf(p)) (20.11)\nSetting conservative con\ufb01dence thresholds for the acceptance of new patterns\nand tuples during the bootstrapping process helps prevent the system from drifting\naway from the targeted relation.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 654,
      "token_count": 727,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 451\n\n20.2 \u2022 R ELATION EXTRACTION ALGORITHMS 443\n20.2.4 Distant Supervision for Relation Extraction\nAlthough hand-labeling text with relation labels is expensive to produce, there are\nways to \ufb01nd indirect sources of training data. The distant supervision methoddistant\nsupervision\n(Mintz et al., 2009) combines the advantages of bootstrapping with supervised learn-\ning. Instead of just a handful of seeds, distant supervision uses a large database to\nacquire a huge number of seed examples, creates lots of noisy pattern features from\nall these examples and then combines them in a supervised classi\ufb01er.\nFor example suppose we are trying to learn the place-of-birth relationship be-\ntween people and their birth cities. In the seed-based approach, we might have only\n5 examples to start with. But Wikipedia-based databases like DBPedia or Freebase\nhave tens of thousands of examples of many relations; including over 100,000 ex-\namples of place-of-birth , (<Edwin Hubble, Marshfield> ,<Albert Einstein,\nUlm> , etc.,). The next step is to run named entity taggers on large amounts of text\u2014\nMintz et al. (2009) used 800,000 articles from Wikipedia\u2014and extract all sentences\nthat have two named entities that match the tuple, like the following:\n...Hubble was born in Marsh\ufb01eld...\n...Einstein, born (1879), Ulm...\n...Hubble\u2019s birthplace in Marsh\ufb01eld...\nTraining instances can now be extracted from this data, one training instance\nfor each identical tuple <relation, entity1, entity2> . Thus there will be one\ntraining instance for each of:\n<born-in, Edwin Hubble, Marshfield>\n<born-in, Albert Einstein, Ulm>\n<born-year, Albert Einstein, 1879>\nand so on.\nWe can then apply feature-based or neural classi\ufb01cation. For feature-based\nclassi\ufb01cation, we can use standard supervised relation extraction features like the\nnamed entity labels of the two mentions, the words and dependency paths in be-\ntween the mentions, and neighboring words. Each tuple will have features col-\nlected from many training instances; the feature vector for a single training instance\nlike (<born-in,Albert Einstein, Ulm> will have lexical and syntactic features\nfrom many different sentences that mention Einstein and Ulm.\nBecause distant supervision has very large training sets, it is also able to use very\nrich features that are conjunctions of these individual features. So we will extract\nthousands of patterns that conjoin the entity types with the intervening words or\ndependency paths like these:\nPER was born in LOC\nPER, born (XXXX), LOC\nPER\u2019s birthplace in LOC\nTo return to our running example, for this sentence:\n(20.12) American Airlines , a unit of AMR, immediately matched the move,\nspokesman Tim Wagner said\nwe would learn rich conjunction features like this one:\nM1 = ORG & M2 = PER & nextword=\u201csaid\u201d& path= NP\"NP\"S\"S#NP\nThe result is a supervised classi\ufb01er that has a huge rich set of features to use\nin detecting relations. Since not every test sentence will have one of the training",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 655,
      "token_count": 705,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 452\n\n444 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\nrelations, the classi\ufb01er will also need to be able to label an example as no-relation .\nThis label is trained by randomly selecting entity pairs that do not appear in any\nFreebase relation, extracting features for them, and building a feature vector for\neach such tuple. The \ufb01nal algorithm is sketched in Fig. 20.8.\nfunction DISTANT SUPERVISION (Database D, Text T )returns relation classi\ufb01er C\nforeach relation R\nforeach tuple ( e1,e2 ) of entities with relation RinD\nsentences Sentences in Tthat contain e1ande2\nf Frequent features in sentences\nobservations observations + new training tuple ( e1, e2, f, R )\nC Train supervised classi\ufb01er on observations\nreturn C\nFigure 20.8 The distant supervision algorithm for relation extraction. A neural classi\ufb01er\nwould skip the feature set f.\nDistant supervision shares advantages with each of the methods we\u2019ve exam-\nined. Like supervised classi\ufb01cation, distant supervision uses a classi\ufb01er with lots\nof features, and supervised by detailed hand-created knowledge. Like pattern-based\nclassi\ufb01ers, it can make use of high-precision evidence for the relation between en-\ntities. Indeed, distance supervision systems learn patterns just like the hand-built\npatterns of early relation extractors. For example the is-aorhypernym extraction\nsystem of Snow et al. (2005) used hypernym/hyponym NP pairs from WordNet as\ndistant supervision, and then learned new patterns from large amounts of text. Their\nsystem induced exactly the original 5 template patterns of Hearst (1992a), but also\n70,000 additional patterns including these four:\nNPHlike NP Many hormones like leptin...\nNPHcalled NP ...using a markup language called XHTML\nNP is a NP H Ruby is a programming language...\nNP, a NP H IBM, a company with a long...\nThis ability to use a large number of features simultaneously means that, un-\nlike the iterative expansion of patterns in seed-based systems, there\u2019s no semantic\ndrift. Like unsupervised classi\ufb01cation, it doesn\u2019t use a labeled training corpus of\ntexts, so it isn\u2019t sensitive to genre issues in the training corpus, and relies on very\nlarge amounts of unlabeled data. Distant supervision also has the advantage that it\ncan create training tuples to be used with neural classi\ufb01ers, where features are not\nrequired.\nThe main problem with distant supervision is that it tends to produce low-precision\nresults, and so current research focuses on ways to improve precision. Furthermore,\ndistant supervision can only help in extracting relations for which a large enough\ndatabase already exists. To extract new relations without datasets, or relations for\nnew domains, purely unsupervised methods must be used.\n20.2.5 Unsupervised Relation Extraction\nThe goal of unsupervised relation extraction is to extract relations from the web\nwhen we have no labeled training data, and not even any list of relations. This task\nis often called open information extraction orOpen IE . In Open IE, the relationsopen\ninformation\nextraction",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 656,
      "token_count": 693,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 453\n\n20.2 \u2022 R ELATION EXTRACTION ALGORITHMS 445\nare simply strings of words (usually beginning with a verb).\nFor example, the ReVerb system (Fader et al., 2011) extracts a relation from a\nsentence sin 4 steps:\n1. Run a part-of-speech tagger and entity chunker over s\n2. For each verb in s, \ufb01nd the longest sequence of words wthat start with a verb\nand satisfy syntactic and lexical constraints, merging adjacent matches.\n3. For each phrase w, \ufb01nd the nearest noun phrase xto the left which is not a\nrelative pronoun, wh-word or existential \u201cthere\u201d. Find the nearest noun phrase\nyto the right.\n4. Assign con\ufb01dence cto the relation r= (x;w;y)using a con\ufb01dence classi\ufb01er\nand return it.\nA relation is only accepted if it meets syntactic and lexical constraints. The\nsyntactic constraints ensure that it is a verb-initial sequence that might also include\nnouns (relations that begin with light verbs like make ,have , ordooften express the\ncore of the relation with a noun, like have a hub in ):\nVjVPjVW*P\nV = verb particle? adv?\nW = (nounjadjjadvjpronjdet )\nP = (prepjparticlejin\ufb01nitive \u201cto\u201d)\nThe lexical constraints are based on a dictionary Dthat is used to prune very rare,\nlong relation strings. The intuition is to eliminate candidate relations that don\u2019t oc-\ncur with suf\ufb01cient number of distinct argument types and so are likely to be bad\nexamples. The system \ufb01rst runs the above relation extraction algorithm of\ufb02ine on\n500 million web sentences and extracts a list of all the relations that occur after nor-\nmalizing them (removing in\ufb02ection, auxiliary verbs, adjectives, and adverbs). Each\nrelation ris added to the dictionary if it occurs with at least 20 different arguments.\nFader et al. (2011) used a dictionary of 1.7 million normalized relations.\nFinally, a con\ufb01dence value is computed for each relation using a logistic re-\ngression classi\ufb01er. The classi\ufb01er is trained by taking 1000 random web sentences,\nrunning the extractor, and hand labeling each extracted relation as correct or incor-\nrect. A con\ufb01dence classi\ufb01er is then trained on this hand-labeled data, using features\nof the relation and the surrounding words. Fig. 20.9 shows some sample features\nused in the classi\ufb01cation.\n(x,r,y) covers all words in s\nthe last preposition in risfor\nthe last preposition in rison\nlen(s)\u001410\nthere is a coordinating conjunction to the left of rins\nrmatches a lone V in the syntactic constraints\nthere is preposition to the left of xins\nthere is an NP to the right of yins\nFigure 20.9 Features for the classi\ufb01er that assigns con\ufb01dence to relations extracted by the\nOpen Information Extraction system REVERB (Fader et al., 2011).\nFor example the following sentence:\n(20.13) United has a hub in Chicago, which is the headquarters of United\nContinental Holdings.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 657,
      "token_count": 717,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 454\n\n446 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\nhas the relation phrases has a hub in andis the headquarters of (it also has hasand\nis, but longer phrases are preferred). Step 3 \ufb01nds United to the left and Chicago to\nthe right of has a hub in , and skips over which to \ufb01nd Chicago to the left of is the\nheadquarters of . The \ufb01nal output is:\nr1: <United, has a hub in, Chicago>\nr2: <Chicago, is the headquarters of, United Continental Holdings>\nThe great advantage of unsupervised relation extraction is its ability to handle\na huge number of relations without having to specify them in advance. The dis-\nadvantage is the need to map all the strings into some canonical form for adding\nto databases or knowledge graphs. Current methods focus heavily on relations ex-\npressed with verbs, and so will miss many relations that are expressed nominally.\n20.2.6 Evaluation of Relation Extraction\nSupervised relation extraction systems are evaluated by using test sets with human-\nannotated, gold-standard relations and computing precision, recall, and F-measure.\nLabeled precision and recall require the system to classify the relation correctly,\nwhereas unlabeled methods simply measure a system\u2019s ability to detect entities that\nare related.\nSemi-supervised andunsupervised methods are much more dif\ufb01cult to evalu-\nate, since they extract totally new relations from the web or a large text. Because\nthese methods use very large amounts of text, it is generally not possible to run them\nsolely on a small labeled test set, and as a result it\u2019s not possible to pre-annotate a\ngold set of correct instances of relations.\nFor these methods it\u2019s possible to approximate (only) precision by drawing a\nrandom sample of relations from the output, and having a human check the accuracy\nof each of these relations. Usually this approach focuses on the tuples to be extracted\nfrom a body of text rather than on the relation mentions ; systems need not detect\nevery mention of a relation to be scored correctly. Instead, the evaluation is based\non the set of tuples occupying the database when the system is \ufb01nished. That is,\nwe want to know if the system can discover that Ryanair has a hub at Charleroi; we\ndon\u2019t really care how many times it discovers it. The estimated precision \u02c6Pis then\n\u02c6P=# of correctly extracted relation tuples in the sample\ntotal # of extracted relation tuples in the sample.(20.14)\nAnother approach that gives us a little bit of information about recall is to com-\npute precision at different levels of recall. Assuming that our system is able to\nrank the relations it produces (by probability, or con\ufb01dence) we can separately com-\npute precision for the top 1000 new relations, the top 10,000 new relations, the top\n100,000, and so on. In each case we take a random sample of that set. This will\nshow us how the precision curve behaves as we extract more and more tuples. But\nthere is no way to directly evaluate recall.\n20.3 Extracting Events\nThe task of event extraction is to identify mentions of events in texts. For theevent\nextraction\npurposes of this task, an event mention is any expression denoting an event or state\nthat can be assigned to a particular point, or interval, in time. The following markup\nof the sample text on page 435 shows all the events in this text.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 658,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 455\n\n20.4 \u2022 R EPRESENTING TIME 447\n[EVENT Citing] high fuel prices, United Airlines [ EVENT said] Fri-\nday it has [ EVENT increased] fares by $6 per round trip on \ufb02ights to\nsome cities also served by lower-cost carriers. American Airlines, a unit\nof AMR Corp., immediately [ EVENT matched] [ EVENT the move],\nspokesman Tim Wagner [ EVENT said]. United, a unit of UAL Corp.,\n[EVENT said] [ EVENT the increase] took effect Thursday and [ EVENT\napplies] to most routes where it [ EVENT competes] against discount\ncarriers, such as Chicago to Dallas and Denver to San Francisco.\nIn English, most event mentions correspond to verbs, and most verbs introduce\nevents. However, as we can see from our example, this is not always the case. Events\ncan be introduced by noun phrases, as in the move andthe increase , and some verbs\nfail to introduce events, as in the phrasal verb took effect , which refers to when the\nevent began rather than to the event itself. Similarly, light verbs such as make ,take, light verbs\nandhave often fail to denote events. A light verb is a verb that has very little meaning\nitself, and the associated event is instead expressed by its direct object noun. In light\nverb examples like took a \ufb02ight , it\u2019s the word \ufb02ight that de\ufb01nes the event; these light\nverbs just provide a syntactic structure for the noun\u2019s arguments.\nVarious versions of the event extraction task exist, depending on the goal. For\nexample in the TempEval shared tasks (Verhagen et al. 2009) the goal is to extract\nevents and aspects like their aspectual and temporal properties. Events are to be\nclassi\ufb01ed as actions, states, reporting events (say, report, tell, explain ), perceptionreporting\nevents\nevents, and so on. The aspect, tense, and modality of each event also needs to be\nextracted. Thus for example the various said events in the sample text would be\nannotated as (class=REPORTING, tense=PAST, aspect=PERFECTIVE).\nEvent extraction is generally modeled via supervised learning, detecting events\nvia IOB sequence models and assigning event classes and attributes with multi-class\nclassi\ufb01ers. The input can be neural models starting from encoders; or classic feature-\nbased models using features like those in Fig. 20.10.\nFeature Explanation\nCharacter af\ufb01xes Character-level pre\ufb01xes and suf\ufb01xes of target word\nNominalization suf\ufb01x Character-level suf\ufb01xes for nominalizations (e.g., -tion )\nPart of speech Part of speech of the target word\nLight verb Binary feature indicating that the target is governed by a light verb\nSubject syntactic category Syntactic category of the subject of the sentence\nMorphological stem Stemmed version of the target word\nVerb root Root form of the verb basis for a nominalization\nWordNet hypernyms Hypernym set for the target\nFigure 20.10 Features commonly used in classic feature-based approaches to event detection.\n20.4 Representing Time\nLet\u2019s begin by introducing the basics of temporal logic and how human languages temporal logic\nconvey temporal information. The most straightforward theory of time holds that it\n\ufb02ows inexorably forward and that events are associated with either points or inter-\nvals in time, as on a timeline. We can order distinct events by situating them on the\ntimeline; one event precedes another if the \ufb02ow of time leads from the \ufb01rst event",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 659,
      "token_count": 774,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 456\n\n448 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\nto the second. Accompanying these notions in most theories is the idea of the cur-\nrent moment in time. Combining this notion with the idea of a temporal ordering\nrelationship yields the familiar notions of past, present, and future.\nVarious kinds of temporal representation systems can be used to talk about tem-\nporal ordering relationship. One of the most commonly used in computational mod-\neling is the interval algebra of Allen (1984). Allen models all events and time interval algebra\nexpressions as intervals there is no representation for points (although intervals can\nbe very short). In order to deal with intervals without points, he identi\ufb01es 13 primi-\ntive relations that can hold between these temporal intervals. Fig. 20.11 shows these\n13Allen relations . Allen relations\nBABA\nBA\nAAB\nBAB\nTime A  before BB after  AA overlaps BB overlaps' AA meets BB meets' AA equals B(B equals A)A starts BB starts' AA finishes BB finishes' A\nBA during BB during' AA\nFigure 20.11 The 13 temporal relations from Allen (1984).\n20.4.1 Reichenbach\u2019s reference point\nThe relation between simple verb tenses and points in time is by no means straight-\nforward. The present tense can be used to refer to a future event, as in this example:\n(20.15) Ok, we \ufb02y from San Francisco to Boston at 10.\nOr consider the following examples:\n(20.16) Flight 1902 arrived late.\n(20.17) Flight 1902 had arrived late.\nAlthough both refer to events in the past, representing them in the same way seems\nwrong. The second example seems to have another unnamed event lurking in the\nbackground (e.g., Flight 1902 had already arrived late when something else hap-\npened).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 660,
      "token_count": 411,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 457\n\n20.4 \u2022 R EPRESENTING TIME 449\nTo account for this phenomena, Reichenbach (1947) introduced the notion of\nareference point . In our simple temporal scheme, the current moment in time is reference point\nequated with the time of the utterance and is used as a reference point for when\nthe event occurred (before, at, or after). In Reichenbach\u2019s approach, the notion of\nthe reference point is separated from the utterance time and the event time. The\nfollowing examples illustrate the basics of this approach:\n(20.18) When Mary\u2019s \ufb02ight departed, I ate lunch.\n(20.19) When Mary\u2019s \ufb02ight departed, I had eaten lunch.\nIn both of these examples, the eating event has happened in the past, that is, prior\nto the utterance. However, the verb tense in the \ufb01rst example indicates that the eating\nevent began when the \ufb02ight departed, while the second example indicates that the\neating was accomplished prior to the \ufb02ight\u2019s departure. Therefore, in Reichenbach\u2019s\nterms the departure event speci\ufb01es the reference point. These facts can be accom-\nmodated by additional constraints relating the eating anddeparture events. In the\n\ufb01rst example, the reference point precedes the eating event, and in the second exam-\nple, the eating precedes the reference point. Figure 20.12 illustrates Reichenbach\u2019s\napproach with the primary English tenses. Exercise 20.4 asks you to represent these\nexamples in FOL.\nPast PerfectSimple PastPresent Perfect\nSimple FutureFuture PerfectPresentEE\nEER\nRUR,EUR,U\nU,R,EU,RU\nFigure 20.12 Reichenbach\u2019s approach applied to various English tenses. In these diagrams,\ntime \ufb02ows from left to right, Edenotes the time of the event, Rdenotes the reference time,\nandUdenotes the time of the utterance.\nLanguages have many other ways to convey temporal information besides tense.\nMost useful for our purposes will be temporal expressions like in the morning or\n6:45 orafterwards .\n(20.20) I\u2019d like to go at 6:45 in the morning.\n(20.21) Somewhere around noon, please.\n(20.22) I want to take the train back afterwards.\nIncidentally, temporal expressions display a fascinating metaphorical conceptual\norganization. Temporal expressions in English are frequently expressed in spatial\nterms, as is illustrated by the various uses of at,in,somewhere , and near in these\nexamples (Lakoff and Johnson 1980, Jackendoff 1983). Metaphorical organizations\nsuch as these, in which one domain is systematically expressed in terms of another,\nare very common in languages of the world.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 661,
      "token_count": 598,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 458\n\n450 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\n20.5 Representing Aspect\nA related notion to time is aspect , which is what we call the way events can be aspect\ncategorized by their internal temporal structure or temporal contour. By this we\nmean questions like whether events are ongoing or have ended, or whether they are\nconceptualized as happening at a point in time or over some interval. Such notions\nof temporal contour have been used to divide event expressions into classes since\nAristotle, although the set of four classes we\u2019ll introduce here is due to Vendler\n(1967) (you may also see the German term aktionsart used to refer to these classes). aktionsart\nThe most basic aspectual distinction is between events (which involve change) events\nandstates (which do not involve change). Stative expressions represent the notion states\nstative of an event participant being in a state , or having a particular property, at a given\npoint in time. Stative expressions capture aspects of the world at a single point in\ntime, and conceptualize the participant as unchanging and continuous. Consider the\nfollowing ATIS examples.\n(20.23) I like express trains.\n(20.24) I need the cheapest fare.\n(20.25) I want to go \ufb01rst class.\nIn examples like these, the event participant denoted by the subject can be seen as\nexperiencing something at a speci\ufb01c point in time, and don\u2019t involve any kind of\ninternal change over time (the liking or needing is conceptualized as continuous and\nunchanging).\nNon-states (which we\u2019ll refer to as events ) are divided into subclasses; we\u2019ll\nintroduce three here. Activity expressions describe events undertaken by a partic- activity\nipant that occur over a span of time (rather than being conceptualized as a single\npoint in time like stative expressions), and have no particular end point. Of course\nin practice all things end, but the meaning of the expression doesn\u2019t represent this\nfact. Consider the following examples:\n(20.26) She drove a Mazda.\n(20.27) I live in Brooklyn.\nThese examples both specify that the subject is engaged in, or has engaged in, the\nactivity speci\ufb01ed by the verb for some period of time, but doesn\u2019t specify when the\ndriving or living might have stopped.\nTwo more classes of expressions, achievement expressions and accomplish-\nment expressions, describe events that take place over time, but also conceptualize\nthe event as having a particular kind of endpoint or goal. The Greek word telos\nmeans \u2018end\u2019 or \u2019goal\u2019 and so the events described by these kinds of expressions are\noften called telic events. telic\nAccomplishment expressions describe events that have a natural end point andaccomplishment\nexpressions\nresult in a particular state. Consider the following examples:\n(20.28) He booked me a reservation.\n(20.29) The 7:00 train got me to New York City.\nIn these examples, an event is seen as occurring over some period of time that ends\nwhen the intended state is accomplished (i.e., the state of me having a reservation,\nor me being in New York City).\nThe \ufb01nal aspectual class, achievement expressions , is only subtly different thanachievement\nexpressions\naccomplishments. Consider the following:",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 662,
      "token_count": 720,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 459\n\n20.6 \u2022 T EMPORALLY ANNOTATED DATASETS : TIMEBANK 451\n(20.30) She found her gate.\n(20.31) I reached New York.\nLike accomplishment expressions, achievement expressions result in a state. But\nunlike accomplishments, achievement events are \u2018punctual\u2019: they are thought of as\nhappening in an instant and the verb doesn\u2019t conceptualize the process or activ-\nity leading up the state. Thus the events in these examples may in fact have been\npreceded by extended searching ortraveling events, but the verb doesn\u2019t conceptu-\nalize these preceding processes, but rather conceptualizes the events corresponding\nto\ufb01nding andreaching as points, not intervals.\nIn summary, a standard way of categorizing event expressions by their temporal\ncontours is via these four general classes:\nStative: I know my departure gate.\nActivity: John is \ufb02ying.\nAccomplishment: Sally booked her \ufb02ight.\nAchievement: She found her gate.\nBefore moving on, note that event expressions can easily be shifted from one\nclass to another. Consider the following examples:\n(20.32) I \ufb02ew.\n(20.33) I \ufb02ew to New York.\nThe \ufb01rst example is a simple activity; it has no natural end point. The second ex-\nample is clearly an accomplishment event since it has an end point, and results in a\nparticular state. Clearly, the classi\ufb01cation of an event is not solely governed by the\nverb, but by the semantics of the entire expression in context.\n20.6 Temporally Annotated Datasets: TimeBank\nTheTimeBank corpus consists of American English text annotated with temporal TimeBank\ninformation (Pustejovsky et al., 2003). The annotations use TimeML (Saur \u00b4\u0131 et al.,\n2006), a markup language for time based on Allen\u2019s interval algebra discussed above\n(Allen, 1984). There are three types of TimeML objects: an E VENT represent events\nand states, a T IME represents time expressions like dates, and a L INK represents\nvarious relationships between events and times (event-event, event-time, and time-\ntime). The links include temporal links (TL INK) for the 13 Allen relations, aspec-\ntual links (AL INK) for aspectual relationships between events and subevents, and\nSLINKS which mark factuality.\nConsider the following sample sentence and its corresponding markup shown in\nFig. 20.13, selected from one of the TimeBank documents.\n(20.34) Delta Air Lines earnings soared 33% to a record in the \ufb01scal \ufb01rst quarter,\nbucking the industry trend toward declining pro\ufb01ts.\nThis text has three events and two temporal expressions (including the creation\ntime of the article, which serves as the document time), and four temporal links that\ncapture the using the Allen relations:\n\u2022 Soaring e1isincluded in the \ufb01scal \ufb01rst quarter t58\n\u2022 Soaring e1isbefore 1989-10-26 t57\n\u2022 Soaring e1issimultaneous with the bucking e3",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 663,
      "token_count": 684,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 460\n\n452 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\n<TIMEX3 tid=\"t57\" type=\"DATE\" value=\"1989-10-26\" functionInDocument=\"CREATION_TIME\">\n10/26/89 </TIMEX3>\nDelta Air Lines earnings <EVENT eid=\"e1\" class=\"OCCURRENCE\"> soared </EVENT> 33% to a\nrecord in <TIMEX3 tid=\"t58\" type=\"DATE\" value=\"1989-Q1\" anchorTimeID=\"t57\"> the\nfiscal first quarter </TIMEX3>, <EVENT eid=\"e3\" class=\"OCCURRENCE\">bucking</EVENT>\nthe industry trend toward <EVENT eid=\"e4\" class=\"OCCURRENCE\">declining</EVENT>\nprofits.\nFigure 20.13 Example from the TimeBank corpus.\n\u2022 Declining e4includes soaring e1\nWe can also visualize the links as a graph. The TimeBank snippet in Eq. 20.35\nwould be represented with a graph like Fig. 20.14.\n(20.35) [DCT:11/02/891] 1: Paci\ufb01c First Financial Corp. said 2shareholders\napproved 3itsacquisition 4by Royal Trustco Ltd. of Toronto for $27 a share,\nor $212 million. The thrift holding company said 5itexpects 6toobtain 7\nregulatory approval 8andcomplete 9thetransaction 10byyear-end 11.\n1234567811910BEFOREBEFOREAFTERSIMULTANEOUSENDSCULMINATESBEFOREEVIDENTIALMODALFACTIVEMODALEVIDENTIALMODAL\nFigure 20.14 A graph of the text in Eq. 20.35, adapted from (Ocal et al., 2022). TL INKS\nare shown in blue, AL INKS in red, and SL INKS in green.\n20.7 Automatic Temporal Analysis\nHere we introduce the three common steps used in analyzing time in text:\n1. Extracting temporal expressions\n2.Normalizing these expressions, by converting them to a standard format.\n3.Linking events to times and extracting time graphs and timelines\n20.7.1 Extracting Temporal Expressions\nTemporal expressions are phrases that refer to absolute points in time, relative times,\ndurations, and sets of these. Absolute temporal expressions are those that can be absolute\nmapped directly to calendar dates, times of day, or both. Relative temporal expres- relative\nsions map to particular times through some other reference point (as in a week from\nlast Tuesday ). Finally, durations denote spans of time at varying levels of granular- duration\nity (seconds, minutes, days, weeks, centuries, etc.). Figure 20.15 lists some sample\ntemporal expressions in each of these categories.\nTemporal expressions are grammatical constructions that often have temporal\nlexical triggers as their heads, making them easy to \ufb01nd. Lexical triggers might lexical triggers",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 664,
      "token_count": 646,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 461\n\n20.7 \u2022 A UTOMATIC TEMPORAL ANALYSIS 453\nAbsolute Relative Durations\nApril 24, 1916 yesterday four hours\nThe summer of \u201977 next semester three weeks\n10:15 AM two weeks from yesterday six days\nThe 3rd quarter of 2006 last quarter the last three quarters\nFigure 20.15 Examples of absolute, relational and durational temporal expressions.\nbe nouns, proper nouns, adjectives, and adverbs; full temporal expressions consist\nof their phrasal projections: noun phrases, adjective phrases, and adverbial phrases\n(Figure 20.16).\nCategory Examples\nNoun morning ,noon ,night ,winter ,dusk,dawn\nProper Noun January, Monday, Ides, Easter, Rosh Hashana, Ramadan, Tet\nAdjective recent, past, annual, former\nAdverb hourly, daily, monthly, yearly\nFigure 20.16 Examples of temporal lexical triggers.\nThe task is to detect temporal expressions in running text, like this examples,\nshown with TIMEX3 tags (Pustejovsky et al. 2005, Ferro et al. 2005).\nA fare increase initiated <TIMEX3>last week</TIMEX3>by UAL\nCorp\u2019s United Airlines was matched by competitors over <TIMEX3>the\nweekend</TIMEX3>, marking the second successful fare increase in\n<TIMEX3>two weeks</TIMEX3>.\nRule-based approaches use cascades of regular expressions to recognize larger\nand larger chunks from previous stages, based on patterns containing parts of speech,\ntrigger words (e.g., February ) or classes (e.g., MONTH ) (Chang and Manning, 2012;\nStr\u00a8otgen and Gertz, 2013; Chambers, 2013). Here\u2019s a rule from SUTime (Chang and\nManning, 2012) for detecting expressions like 3 years old :\n/(\\d+)[-\\s]($TEUnits)(s)?([-\\s]old)?/\nSequence-labeling approaches use the standard IOB scheme, marking words\nthat are either (I)nside, (O)utside or at the (B)eginning of a temporal expression:\nA\nOfare\nOincrease\nOinitiated\nOlast\nBweek\nIby\nOUAL\nOCorp\u2019s...\nO\nA statistical sequence labeler is trained, using either embeddings or a \ufb01ne-tuned\nencoder, or classic features extracted from the token and context including words,\nlexical triggers, and POS.\nTemporal expression recognizers are evaluated with the usual recall, precision,\nandF-measures. A major dif\ufb01culty for all of these very lexicalized approaches is\navoiding expressions that trigger false positives:\n(20.36) 1984 tells the story of Winston Smith...\n(20.37) ...U2\u2019s classic Sunday Bloody Sunday\n20.7.2 Temporal Normalization\nTemporal normalization is the task of mapping a temporal expression to a pointtemporal\nnormalization\nin time or to a duration. Points in time correspond to calendar dates, to times of\nday, or both. Durations primarily consist of lengths of time. Normalized times",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 665,
      "token_count": 677,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 462",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 666,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "454 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\n<TIMEX3 i d =\u201d t1 \u2019 \u2019 t y p e =\u201dDATE\u201d v a l u e =\u201d 2007 \u221207 \u221202 \u201d f u n c t i o n I n D o c u m e n t =\u201dCREATION TIME\u201d >\nJ u l y 2 , 2007 </TIMEX3 >A f a r e i n c r e a s e i n i t i a t e d <TIMEX3 i d =\u201d t 2 \u201d t y p e =\u201dDATE\u201d\nv a l u e =\u201d 2007 \u2212W26\u201d anchorTimeID=\u201d t 1 \u201d >l a s t week </TIMEX3 >by U n i t e d A i r l i n e s was\nmatched by c o m p e t i t o r s o ve r <TIMEX3 i d =\u201d t 3 \u201d t y p e =\u201dDURATION\u201d v a l u e =\u201dP1WE\u201d\nanchorTimeID=\u201d t 1 \u201d >t h e weekend </TIMEX3 >, marking t h e second s u c c e s s f u l f a r e\ni n c r e a s e i n <TIMEX3 i d =\u201d t 4 \u201d t y p e =\u201dDURATION\u201d v a l u e =\u201dP2W\u201d anchorTimeID=\u201d t 1 \u201d >two\nweeks </TIMEX3 >.\nFigure 20.17 TimeML markup including normalized values for temporal expressions.\nare represented via the ISO 8601 standard for encoding temporal values (ISO8601,\n2004). Fig. 20.17 reproduces our earlier example with these value attributes.\nThe dateline, or document date, for this text was July 2, 2007 . The ISO repre-\nsentation for this kind of expression is YYYY-MM-DD, or in this case, 2007-07-02.\nThe encodings for the temporal expressions in our sample text all follow from this\ndate, and are shown here as values for the VALUE attribute.\nThe \ufb01rst temporal expression in the text proper refers to a particular week of the\nyear. In the ISO standard, weeks are numbered from 01 to 53, with the \ufb01rst week\nof the year being the one that has the \ufb01rst Thursday of the year. These weeks are\nrepresented with the template YYYY-Wnn. The ISO week for our document date is\nweek 27; thus the value for last week is represented as \u201c2007-W26\u201d.\nThe next temporal expression is the weekend . ISO weeks begin on Monday;\nthus, weekends occur at the end of a week and are fully contained within a single\nweek. Weekends are treated as durations, so the value of the VALUE attribute has\nto be a length. Durations are represented according to the pattern P nx, where nis\nan integer denoting the length and xrepresents the unit, as in P3Y for three years\nor P2D for two days . In this example, one weekend is captured as P1WE. In this\ncase, there is also suf\ufb01cient information to anchor this particular weekend as part of\na particular week. Such information is encoded in the ANCHOR TIMEID attribute.\nFinally, the phrase two weeks also denotes a duration captured as P2W. Figure 20.18\ngive some more examples, but there is a lot more to the various temporal annotation",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 667,
      "token_count": 754,
      "chapter_title": ""
    }
  },
  {
    "content": "The \ufb01rst temporal expression in the text proper refers to a particular week of the\nyear. In the ISO standard, weeks are numbered from 01 to 53, with the \ufb01rst week\nof the year being the one that has the \ufb01rst Thursday of the year. These weeks are\nrepresented with the template YYYY-Wnn. The ISO week for our document date is\nweek 27; thus the value for last week is represented as \u201c2007-W26\u201d.\nThe next temporal expression is the weekend . ISO weeks begin on Monday;\nthus, weekends occur at the end of a week and are fully contained within a single\nweek. Weekends are treated as durations, so the value of the VALUE attribute has\nto be a length. Durations are represented according to the pattern P nx, where nis\nan integer denoting the length and xrepresents the unit, as in P3Y for three years\nor P2D for two days . In this example, one weekend is captured as P1WE. In this\ncase, there is also suf\ufb01cient information to anchor this particular weekend as part of\na particular week. Such information is encoded in the ANCHOR TIMEID attribute.\nFinally, the phrase two weeks also denotes a duration captured as P2W. Figure 20.18\ngive some more examples, but there is a lot more to the various temporal annotation\nstandards; consult ISO8601 (2004), Ferro et al. (2005), and Pustejovsky et al. (2005)\nfor more details.\nUnit Pattern Sample Value\nFully speci\ufb01ed dates YYYY-MM-DD 1991-09-28\nWeeks YYYY-Wnn 2007-W27\nWeekends PnWE P1WE\n24-hour clock times HH:MM:SS 11:13:45\nDates and times YYYY-MM-DDTHH:MM:SS 1991-09-28T11:00:00\nFinancial quarters Qn 1999-Q3\nFigure 20.18 Sample ISO patterns for representing various times and durations.\nMost current approaches to temporal normalization are rule-based (Chang and\nManning 2012, Str \u00a8otgen and Gertz 2013). Patterns that match temporal expressions\nare associated with semantic analysis procedures. For example, the pattern above for\nrecognizing phrases like 3 years old can be associated with the predicate Duration\nthat takes two arguments, the length and the unit of time:\npattern: /(\\d+)[-\\s]($TEUnits)(s)?([-\\s]old)?/\nresult: Duration($1, $2)\nThe task is dif\ufb01cult because fully quali\ufb01ed temporal expressions are fairly rare\nin real texts. Most temporal expressions in news articles are incomplete and are only\nimplicitly anchored, often with respect to the dateline of the article, which we refer",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 668,
      "token_count": 603,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 463\n\n20.7 \u2022 A UTOMATIC TEMPORAL ANALYSIS 455\nto as the document\u2019s temporal anchor . The values of temporal expressions suchtemporal\nanchor\nastoday ,yesterday , ortomorrow can all be computed with respect to this temporal\nanchor. The semantic procedure for today simply assigns the anchor, and the attach-\nments for tomorrow andyesterday add a day and subtract a day from the anchor,\nrespectively. Of course, given the cyclic nature of our representations for months,\nweeks, days, and times of day, our temporal arithmetic procedures must use modulo\narithmetic appropriate to the time unit being used.\nUnfortunately, even simple expressions such as the weekend orWednesday in-\ntroduce a fair amount of complexity. In our current example, the weekend clearly\nrefers to the weekend of the week that immediately precedes the document date. But\nthis won\u2019t always be the case, as is illustrated in the following example.\n(20.38) Random security checks that began yesterday at Sky Harbor will continue\nat least through the weekend.\nIn this case, the expression the weekend refers to the weekend of the week that the\nanchoring date is part of (i.e., the coming weekend). The information that signals\nthis meaning comes from the tense of continue , the verb governing the weekend .\nRelative temporal expressions are handled with temporal arithmetic similar to\nthat used for today andyesterday . The document date indicates that our example\narticle is ISO week 27, so the expression last week normalizes to the current week\nminus 1. To resolve ambiguous next andlastexpressions we consider the distance\nfrom the anchoring date to the nearest unit. Next Friday can refer either to the\nimmediately next Friday or to the Friday following that, but the closer the document\ndate is to a Friday, the more likely it is that the phrase will skip the nearest one. Such\nambiguities are handled by encoding language and domain-speci\ufb01c heuristics into\nthe temporal attachments.\n20.7.3 Temporal Ordering of Events\nThe goal of temporal analysis, is to link times to events and then \ufb01t all these events\ninto a complete timeline. This ambitious task is the subject of considerable current\nresearch but solving it with a high level of accuracy is beyond the capabilities of\ncurrent systems. A somewhat simpler, but still useful, task is to impose a partial or-\ndering on the events and temporal expressions mentioned in a text. Such an ordering\ncan provide many of the same bene\ufb01ts as a true timeline. An example of such a par-\ntial ordering is the determination that the fare increase by American Airlines came\nafter the fare increase by United in our sample text. Determining such an ordering\ncan be viewed as a binary relation detection and classi\ufb01cation task.\nEven this partial ordering task assumes that in addition to the detecting and nor-\nmalizing time expressions steps described above, we have already detected all the\nevents in the text. Indeed, many temporal expressions are anchored to events men-\ntioned in a text and not directly to other temporal expressions. Consider the follow-\ning example:\n(20.39) One week after the storm, JetBlue issued its customer bill of rights.\nTo determine when JetBlue issued its customer bill of rights we need to determine\nthe time of the storm event, and then we need to modify that time by the temporal\nexpression one week after .\nThus once the events and times have been detected, our goal next is to assert links\nbetween all the times and events: i.e. creating event-event, event-time, time-time,\nDCT-event, and DCT-time TimeML TL INKS . This can be done by training time\nrelation classi\ufb01ers to predict the correct T: INKbetween each pair of times/events,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 669,
      "token_count": 793,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 464\n\n456 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\nsupervised by the gold labels in the TimeBank corpus with features like words/em-\nbeddings, parse paths, tense and aspect The sieve-based architecture using precision-\nranked sets of classi\ufb01ers, which we\u2019ll introduce in Chapter 23, is also commonly\nused.\nSystems that perform all 4 tasks (time extraction creation and normalization,\nevent extraction, and time/event linking) include TARSQI (Verhagen et al., 2005)\nCLEAR TK (Bethard, 2013), CAEVO (Chambers et al., 2014), and CATENA (Mirza\nand Tonelli, 2016).\n20.8 Template Filling\nMany texts contain reports of events, and possibly sequences of events, that often\ncorrespond to fairly common, stereotypical situations in the world. These abstract\nsituations or stories, related to what have been called scripts (Schank and Abel- scripts\nson, 1977), consist of prototypical sequences of sub-events, participants, and their\nroles. The strong expectations provided by these scripts can facilitate the proper\nclassi\ufb01cation of entities, the assignment of entities into roles and relations, and most\ncritically, the drawing of inferences that \ufb01ll in things that have been left unsaid. In\ntheir simplest form, such scripts can be represented as templates consisting of \ufb01xed templates\nsets of slots that take as values slot-\ufb01llers belonging to particular classes. The task\noftemplate \ufb01lling is to \ufb01nd documents that invoke particular scripts and then \ufb01ll the template \ufb01lling\nslots in the associated templates with \ufb01llers extracted from the text. These slot-\ufb01llers\nmay consist of text segments extracted directly from the text, or they may consist of\nconcepts that have been inferred from text elements through some additional pro-\ncessing.\nA \ufb01lled template from our original airline story might look like the following.\nFARE-RAISE ATTEMPT :2\n6664LEAD AIRLINE : U NITED AIRLINES\nAMOUNT : $6\nEFFECTIVE DATE: 2006-10-26\nFOLLOWER : A MERICAN AIRLINES3\n7775\nThis template has four slots ( LEAD AIRLINE ,AMOUNT ,EFFECTIVE DATE ,FOL-\nLOWER ). The next section describes a standard sequence-labeling approach to \ufb01lling\nslots. Section 20.8.2 then describes an older system based on the use of cascades of\n\ufb01nite-state transducers and designed to address a more complex template-\ufb01lling task\nthat current learning-based systems don\u2019t yet address.\n20.8.1 Machine Learning Approaches to Template Filling\nIn the standard paradigm for template \ufb01lling, we are given training documents with\ntext spans annotated with prede\ufb01ned templates and their slot \ufb01llers. Our goal is to\ncreate one template for each event in the input, \ufb01lling in the slots with text spans.\nThe task is generally modeled by training two separate supervised systems. The\n\ufb01rst system decides whether the template is present in a particular sentence. This\ntask is called template recognition or sometimes, in a perhaps confusing bit oftemplate\nrecognition\nterminology, event recognition . Template recognition can be treated as a text classi-\n\ufb01cation task, with features extracted from every sequence of words that was labeled\nin training documents as \ufb01lling any slot from the template being detected. The usual",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 670,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 465",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 671,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "20.8 \u2022 T EMPLATE FILLING 457\nset of features can be used: tokens, embeddings, word shapes, part-of-speech tags,\nsyntactic chunk tags, and named entity tags.\nThe second system has the job of role-\ufb01ller extraction . A separate classi\ufb01er isrole-\ufb01ller\nextraction\ntrained to detect each role ( LEAD -AIRLINE ,AMOUNT , and so on). This can be a\nbinary classi\ufb01er that is run on every noun-phrase in the parsed input sentence, or a\nsequence model run over sequences of words. Each role classi\ufb01er is trained on the\nlabeled data in the training set. Again, the usual set of features can be used, but now\ntrained only on an individual noun phrase or the \ufb01llers of a single slot.\nMultiple non-identical text segments might be labeled with the same slot la-\nbel. For example in our sample text, the strings United orUnited Airlines might be\nlabeled as the L EAD AIRLINE . These are not incompatible choices and the corefer-\nence resolution techniques introduced in Chapter 23 can provide a path to a solution.\nA variety of annotated collections have been used to evaluate this style of ap-\nproach to template \ufb01lling, including sets of job announcements, conference calls for\npapers, restaurant guides, and biological texts. A key open question is extracting\ntemplates in cases where there is no training data or even prede\ufb01ned templates, by\ninducing templates as sets of linked events (Chambers and Jurafsky, 2011).\n20.8.2 Earlier Finite-State Template-Filling Systems\nThe templates above are relatively simple. But consider the task of producing a\ntemplate that contained all the information in a text like this one (Grishman and\nSundheim, 1995):\nBridgestone Sports Co. said Friday it has set up a joint venture in Taiwan\nwith a local concern and a Japanese trading house to produce golf clubs to be\nshipped to Japan. The joint venture, Bridgestone Sports Taiwan Co., capital-\nized at 20 million new Taiwan dollars, will start production in January 1990\nwith production of 20,000 iron and \u201cmetal wood\u201d clubs a month.\nThe MUC-5 \u2018joint venture\u2019 task (the Message Understanding Conferences were\na series of U.S. government-organized information-extraction evaluations) was to\nproduce hierarchically linked templates describing joint ventures. Figure 20.19\nshows a structure produced by the FASTUS system (Hobbs et al., 1997). Note how\nthe \ufb01ller of the ACTIVITY slot of the TIE-UPtemplate is itself a template with slots.\nTie-up-1 Activity-1 :\nRELATIONSHIP tie-up C OMPANY Bridgestone Sports Taiwan Co.\nENTITIES Bridgestone Sports Co. P RODUCT iron and \u201cmetal wood\u201d clubs\na local concern S TART DATE DURING: January 1990\na Japanese trading house\nJOINT VENTURE Bridgestone Sports Taiwan Co.\nACTIVITY Activity-1\nAMOUNT NT$20000000\nFigure 20.19 The templates produced by FASTUS given the input text on page 457.\nEarly systems for dealing with these complex templates were based on cascades\nof transducers based on handwritten rules, as sketched in Fig. 20.20.\nThe \ufb01rst four stages use handwritten regular expression and grammar rules to\ndo basic tokenization, chunking, and parsing. Stage 5 then recognizes entities and",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 672,
      "token_count": 760,
      "chapter_title": ""
    }
  },
  {
    "content": "with production of 20,000 iron and \u201cmetal wood\u201d clubs a month.\nThe MUC-5 \u2018joint venture\u2019 task (the Message Understanding Conferences were\na series of U.S. government-organized information-extraction evaluations) was to\nproduce hierarchically linked templates describing joint ventures. Figure 20.19\nshows a structure produced by the FASTUS system (Hobbs et al., 1997). Note how\nthe \ufb01ller of the ACTIVITY slot of the TIE-UPtemplate is itself a template with slots.\nTie-up-1 Activity-1 :\nRELATIONSHIP tie-up C OMPANY Bridgestone Sports Taiwan Co.\nENTITIES Bridgestone Sports Co. P RODUCT iron and \u201cmetal wood\u201d clubs\na local concern S TART DATE DURING: January 1990\na Japanese trading house\nJOINT VENTURE Bridgestone Sports Taiwan Co.\nACTIVITY Activity-1\nAMOUNT NT$20000000\nFigure 20.19 The templates produced by FASTUS given the input text on page 457.\nEarly systems for dealing with these complex templates were based on cascades\nof transducers based on handwritten rules, as sketched in Fig. 20.20.\nThe \ufb01rst four stages use handwritten regular expression and grammar rules to\ndo basic tokenization, chunking, and parsing. Stage 5 then recognizes entities and\nevents with a recognizer based on \ufb01nite-state transducers (FSTs), and inserts the rec-\nognized objects into the appropriate slots in templates. This FST recognizer is based",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 673,
      "token_count": 330,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 466\n\n458 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\nNo. Step Description\n1 Tokens Tokenize input stream of characters\n2 Complex Words Multiword phrases, numbers, and proper names.\n3 Basic phrases Segment sentences into noun and verb groups\n4 Complex phrases Identify complex noun groups and verb groups\n5 Semantic Patterns Identify entities and events, insert into templates.\n6 Merging Merge references to the same entity or event\nFigure 20.20 Levels of processing in FASTUS (Hobbs et al., 1997). Each level extracts a\nspeci\ufb01c type of information which is then passed on to the next higher level.\non hand-built regular expressions like the following (NG indicates Noun-Group and\nVG Verb-Group), which matches the \ufb01rst sentence of the news story above.\nNG(Company/ies) VG(Set-up) NG(Joint-Venture) with NG(Company/ies)\nVG(Produce) NG(Product)\nThe result of processing these two sentences is the \ufb01ve draft templates (Fig. 20.21)\nthat must then be merged into the single hierarchical structure shown in Fig. 20.19.\nThe merging algorithm, after performing coreference resolution, merges two activi-\nties that are likely to be describing the same events.\n#Template/Slot Value\n1RELATIONSHIP : TIE-UP\nENTITIES : Bridgestone Co., a local concern, a Japanese trading house\n2ACTIVITY : PRODUCTION\nPRODUCT : \u201cgolf clubs\u201d\n3RELATIONSHIP : TIE-UP\nJOINT VENTURE : \u201cBridgestone Sports Taiwan Co.\u201d\nAMOUNT : NT$20000000\n4ACTIVITY : PRODUCTION\nCOMPANY : \u201cBridgestone Sports Taiwan Co.\u201d\nSTART DATE: DURING : January 1990\n5ACTIVITY : PRODUCTION\nPRODUCT : \u201ciron and \u201cmetal wood\u201d clubs\u201d\nFigure 20.21 The \ufb01ve partial templates produced by stage 5 of FASTUS . These templates\nare merged in stage 6 to produce the \ufb01nal template shown in Fig. 20.19 on page 457.\n20.9 Summary\nThis chapter has explored techniques for extracting limited forms of semantic con-\ntent from texts.\n\u2022Relations among entities can be extracted by pattern-based approaches, su-\npervised learning methods when annotated training data is available, lightly\nsupervised bootstrapping methods when small numbers of seed tuples or\nseed patterns are available, distant supervision when a database of relations\nis available, and unsupervised orOpen IE methods.\n\u2022 Reasoning about time can be facilitated by detection and normalization of\ntemporal expressions .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 674,
      "token_count": 561,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 467",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 675,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 459\n\u2022Events can be ordered in time using sequence models and classi\ufb01ers trained\non temporally- and event-labeled data like the TimeBank corpus .\n\u2022Template-\ufb01lling applications can recognize stereotypical situations in texts\nand assign elements from the text to roles represented as \ufb01xed sets of slots .\nBibliographical and Historical Notes\nThe earliest work on information extraction addressed the template-\ufb01lling task in the\ncontext of the Frump system (DeJong, 1982). Later work was stimulated by the U.S.\ngovernment-sponsored MUC conferences (Sundheim 1991, Sundheim 1992, Sund-\nheim 1993, Sundheim 1995). Early MUC systems like CIRCUS system (Lehnert\net al., 1991) and SCISOR (Jacobs and Rau, 1990) were quite in\ufb02uential and inspired\nlater systems like FASTUS (Hobbs et al., 1997). Chinchor et al. (1993) describe the\nMUC evaluation techniques.\nDue to the dif\ufb01culty of porting systems from one domain to another, attention\nshifted to machine learning approaches. Early supervised learning approaches to\nIE (Cardie 1993, Cardie 1994, Riloff 1993, Soderland et al. 1995, Huffman 1996)\nfocused on automating the knowledge acquisition process, mainly for \ufb01nite-state\nrule-based systems. Their success, and the earlier success of HMM-based speech\nrecognition, led to the use of sequence labeling (HMMs: Bikel et al. 1997; MEMMs\nMcCallum et al. 2000; CRFs: Lafferty et al. 2001), and a wide exploration of fea-\ntures (Zhou et al., 2005). Neural approaches followed from the pioneering results of\nCollobert et al. (2011), who applied a CRF on top of a convolutional net.\nProgress in this area continues to be stimulated by formal evaluations with shared\nbenchmark datasets, including the Automatic Content Extraction (ACE) evaluations\nof 2000-2007 on named entity recognition, relation extraction, and temporal ex-\npressions1, the KBP (Knowledge Base Population ) evaluations (Ji et al. 2010, Sur- KBP\ndeanu 2013) of relation extraction tasks like slot \ufb01lling (extracting attributes (\u2018slots\u2019) slot \ufb01lling\nlike age, birthplace, and spouse for a given entity) and a series of SemEval work-\nshops (Hendrickx et al., 2009).\nSemisupervised relation extraction was \ufb01rst proposed by Hearst (1992b), and\nextended by systems like AutoSlog-TS (Riloff, 1996), DIPRE (Brin, 1998), SNOW-\nBALL (Agichtein and Gravano, 2000), and Jones et al. (1999). The distant super-\nvision algorithm we describe was drawn from Mintz et al. (2009), who \ufb01rst used\nthe term \u2018distant supervision\u2019 (which was suggested to them by Chris Manning)\nbut similar ideas had occurred in earlier systems like Craven and Kumlien (1999)\nand Morgan et al. (2004) under the name weakly labeled data , as well as in Snow\net al. (2005) and Wu and Weld (2007). Among the many extensions are Wu and",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 676,
      "token_count": 765,
      "chapter_title": ""
    }
  },
  {
    "content": "pressions1, the KBP (Knowledge Base Population ) evaluations (Ji et al. 2010, Sur- KBP\ndeanu 2013) of relation extraction tasks like slot \ufb01lling (extracting attributes (\u2018slots\u2019) slot \ufb01lling\nlike age, birthplace, and spouse for a given entity) and a series of SemEval work-\nshops (Hendrickx et al., 2009).\nSemisupervised relation extraction was \ufb01rst proposed by Hearst (1992b), and\nextended by systems like AutoSlog-TS (Riloff, 1996), DIPRE (Brin, 1998), SNOW-\nBALL (Agichtein and Gravano, 2000), and Jones et al. (1999). The distant super-\nvision algorithm we describe was drawn from Mintz et al. (2009), who \ufb01rst used\nthe term \u2018distant supervision\u2019 (which was suggested to them by Chris Manning)\nbut similar ideas had occurred in earlier systems like Craven and Kumlien (1999)\nand Morgan et al. (2004) under the name weakly labeled data , as well as in Snow\net al. (2005) and Wu and Weld (2007). Among the many extensions are Wu and\nWeld (2010), Riedel et al. (2010), and Ritter et al. (2013). Open IE systems include\nKNOW ITALLEtzioni et al. (2005), TextRunner (Banko et al., 2007), and R EVERB\n(Fader et al., 2011). See Riedel et al. (2013) for a universal schema that combines\nthe advantages of distant supervision and Open IE.\n1www.nist.gov/speech/tests/ace/",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 677,
      "token_count": 383,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 468\n\n460 CHAPTER 20 \u2022 I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME\nExercises\n20.1 Acronym expansion, the process of associating a phrase with an acronym, can\nbe accomplished by a simple form of relational analysis. Develop a system\nbased on the relation analysis approaches described in this chapter to populate\na database of acronym expansions. If you focus on English Three Letter\nAcronyms (TLAs) you can evaluate your system\u2019s performance by comparing\nit to Wikipedia\u2019s TLA page.\n20.2 Acquire the CMU seminar corpus and develop a template-\ufb01lling system by\nusing any of the techniques mentioned in Section 20.8. Analyze how well\nyour system performs as compared with state-of-the-art results on this corpus.\n20.3 A useful functionality in newer email and calendar applications is the ability\nto associate temporal expressions connected with events in email (doctor\u2019s\nappointments, meeting planning, party invitations, etc.) with speci\ufb01c calendar\nentries. Collect a corpus of email containing temporal expressions related to\nevent planning. How do these expressions compare to the kinds of expressions\ncommonly found in news text that we\u2019ve been discussing in this chapter?\n20.4 For the following sentences, give FOL translations that capture the temporal\nrelationships between the events.\n1. When Mary\u2019s \ufb02ight departed, I ate lunch.\n2. When Mary\u2019s \ufb02ight departed, I had eaten lunch.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 678,
      "token_count": 311,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 469\n\nCHAPTER\n21Semantic Role Labeling\n\u201cWho, What, Where, When, With what, Why, How\u201d\nThe seven circumstances, associated with Hermagoras and Aristotle (Sloan, 2010)\nSometime between the 7th and 4th centuries BCE, the Indian grammarian P \u00afan.ini1\nwrote a famous treatise on Sanskrit grammar, the As .t.\u00afadhy \u00afay\u00af\u0131 (\u20188 books\u2019), a treatise\nthat has been called \u201cone of the greatest monuments of hu-\nman intelligence\u201d (Bloom\ufb01eld, 1933, 11). The work de-\nscribes the linguistics of the Sanskrit language in the form\nof 3959 sutras, each very ef\ufb01ciently (since it had to be\nmemorized!) expressing part of a formal rule system that\nbrilliantly pre\ufb01gured modern mechanisms of formal lan-\nguage theory (Penn and Kiparsky, 2012). One set of rules\ndescribes the k\u00afarakas , semantic relationships between a\nverb and noun arguments, roles like agent ,instrument , or\ndestination . P\u00afan.ini\u2019s work was the earliest we know of\nthat modeled the linguistic realization of events and their\nparticipants. This task of understanding how participants relate to events\u2014being\nable to answer the question \u201cWho did what to whom\u201d (and perhaps also \u201cwhen and\nwhere\u201d)\u2014is a central question of natural language processing.\nLet\u2019s move forward 2.5 millennia to the present and consider the very mundane\ngoal of understanding text about a purchase of stock by XYZ Corporation. This\npurchasing event and its participants can be described by a wide variety of surface\nforms. The event can be described by a verb ( sold, bought ) or a noun ( purchase ),\nand XYZ Corp can be the syntactic subject (of bought ), the indirect object (of sold),\nor in a genitive or noun compound relation (with the noun purchase ) despite having\nnotionally the same role in all of them:\n\u2022 XYZ corporation bought the stock.\n\u2022 They sold the stock to XYZ corporation.\n\u2022 The stock was bought by XYZ corporation.\n\u2022 The purchase of the stock by XYZ corporation...\n\u2022 The stock purchase by XYZ corporation...\nIn this chapter we introduce a level of representation that captures the common-\nality between these sentences: there was a purchase event, the participants were\nXYZ Corp and some stock, and XYZ Corp was the buyer. These shallow semantic\nrepresentations , semantic roles , express the role that arguments of a predicate take\nin the event, codi\ufb01ed in databases like PropBank and FrameNet. We\u2019ll introduce\nsemantic role labeling , the task of assigning roles to spans in sentences, and selec-\ntional restrictions , the preferences that predicates express about their arguments,\nsuch as the fact that the theme of eatis generally something edible.\n1Figure shows a birch bark manuscript from Kashmir of the Rupavatra, a grammatical textbook based\non the Sanskrit grammar of Panini. Image from the Wellcome Collection.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 679,
      "token_count": 652,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 470\n\n462 CHAPTER 21 \u2022 S EMANTIC ROLE LABELING\n21.1 Semantic Roles\nConsider the meanings of the arguments Sasha ,Pat,the window , and the door in\nthese two sentences.\n(21.1) Sasha broke the window.\n(21.2) Pat opened the door.\nThe subjects Sasha andPat, what we might call the breaker of the window-\nbreaking event and the opener of the door-opening event, have something in com-\nmon. They are both volitional actors, often animate, and they have direct causal\nresponsibility for their events.\nThematic roles are a way to capture this semantic commonality between break- thematic roles\nersandopeners . We say that the subjects of both these verbs are agents . Thus, agents\nAGENT is the thematic role that represents an abstract idea such as volitional causa-\ntion. Similarly, the direct objects of both these verbs, the BrokenThing andOpenedThing ,\nare both prototypically inanimate objects that are affected in some way by the action.\nThe semantic role for these participants is theme . theme\nThematic Role De\ufb01nition\nAGENT The volitional causer of an event\nEXPERIENCER The experiencer of an event\nFORCE The non-volitional causer of the event\nTHEME The participant most directly affected by an event\nRESULT The end product of an event\nCONTENT The proposition or content of a propositional event\nINSTRUMENT An instrument used in an event\nBENEFICIARY The bene\ufb01ciary of an event\nSOURCE The origin of the object of a transfer event\nGOAL The destination of an object of a transfer event\nFigure 21.1 Some commonly used thematic roles with their de\ufb01nitions.\nAlthough thematic roles are one of the oldest linguistic models, as we saw above,\ntheir modern formulation is due to Fillmore (1968) and Gruber (1965). Although\nthere is no universally agreed-upon set of roles, Figs. 21.1 and 21.2 list some the-\nmatic roles that have been used in various computational papers, together with rough\nde\ufb01nitions and examples. Most thematic role sets have about a dozen roles, but we\u2019ll\nsee sets with smaller numbers of roles with even more abstract meanings, and sets\nwith very large numbers of roles that are speci\ufb01c to situations. We\u2019ll use the general\nterm semantic roles for all sets of roles, whether small or large. semantic roles\n21.2 Diathesis Alternations\nThe main reason computational systems use semantic roles is to act as a shallow\nmeaning representation that can let us make simple inferences that aren\u2019t possible\nfrom the pure surface string of words, or even from the parse tree. To extend the\nearlier examples, if a document says that Company A acquired Company B , we\u2019d\nlike to know that this answers the query Was Company B acquired? despite the fact\nthat the two sentences have very different surface syntax. Similarly, this shallow\nsemantics might act as a useful intermediate language in machine translation.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 680,
      "token_count": 640,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 471\n\n21.2 \u2022 D IATHESIS ALTERNATIONS 463\nThematic Role Example\nAGENT The waiter spilled the soup.\nEXPERIENCER John has a headache.\nFORCE The wind blows debris from the mall into our yards.\nTHEME Only after Benjamin Franklin broke the ice ...\nRESULT The city built a regulation-size baseball diamond ...\nCONTENT Mona asked \u201cYou met Mary Ann at a supermarket?\u201d\nINSTRUMENT He poached cat\ufb01sh, stunning them with a shocking device ...\nBENEFICIARY Whenever Ann Callahan makes hotel reservations for her boss ...\nSOURCE I \ufb02ew in from Boston .\nGOAL I drove to Portland .\nFigure 21.2 Some prototypical examples of various thematic roles.\nSemantic roles thus help generalize over different surface realizations of pred-\nicate arguments. For example, while the AGENT is often realized as the subject of\nthe sentence, in other cases the THEME can be the subject. Consider these possible\nrealizations of the thematic arguments of the verb break :\n(21.3) John\nAGENTbroke the window.\nTHEME\n(21.4) John\nAGENTbroke the window\nTHEMEwith a rock.\nINSTRUMENT\n(21.5) The rock\nINSTRUMENTbroke the window.\nTHEME\n(21.6) The window\nTHEMEbroke.\n(21.7) The window\nTHEMEwas broken by John.\nAGENT\nThese examples suggest that break has (at least) the possible arguments AGENT ,\nTHEME , and INSTRUMENT . The set of thematic role arguments taken by a verb is\noften called the thematic grid ,q-grid, or case frame . We can see that there are thematic grid\ncase frame (among others) the following possibilities for the realization of these arguments of\nbreak :\nAGENT /Subject, THEME /Object\nAGENT /Subject, THEME /Object, INSTRUMENT /PPwith\nINSTRUMENT /Subject, THEME /Object\nTHEME /Subject\nIt turns out that many verbs allow their thematic roles to be realized in various\nsyntactic positions. For example, verbs like give can realize the THEME and GOAL\narguments in two different ways:\n(21.8) a. Doris\nAGENTgave the book\nTHEMEto Cary.\nGOAL\nb.Doris\nAGENTgave Cary\nGOALthe book.\nTHEME\nThese multiple argument structure realizations (the fact that break can take AGENT ,\nINSTRUMENT , or THEME as subject, and give can realize its THEME and GOAL in\neither order) are called verb alternations ordiathesis alternations . The alternationverb\nalternation\nwe showed above for give, the dative alternation , seems to occur with particular se-dative\nalternation\nmantic classes of verbs, including \u201cverbs of future having\u201d ( advance ,allocate ,offer,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 681,
      "token_count": 598,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 472",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 682,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "464 CHAPTER 21 \u2022 S EMANTIC ROLE LABELING\nowe), \u201csend verbs\u201d ( forward ,hand ,mail), \u201cverbs of throwing\u201d ( kick,pass,throw ),\nand so on. Levin (1993) lists for 3100 English verbs the semantic classes to which\nthey belong (47 high-level classes, divided into 193 more speci\ufb01c classes) and the\nvarious alternations in which they participate. These lists of verb classes have been\nincorporated into the online resource VerbNet (Kipper et al., 2000), which links each\nverb to both WordNet and FrameNet entries.\n21.3 Semantic Roles: Problems with Thematic Roles\nRepresenting meaning at the thematic role level seems like it should be useful in\ndealing with complications like diathesis alternations. Yet it has proved quite dif\ufb01-\ncult to come up with a standard set of roles, and equally dif\ufb01cult to produce a formal\nde\ufb01nition of roles like AGENT ,THEME , or INSTRUMENT .\nFor example, researchers attempting to de\ufb01ne role sets often \ufb01nd they need to\nfragment a role like AGENT orTHEME into many speci\ufb01c roles. Levin and Rappa-\nport Hovav (2005) summarize a number of such cases, such as the fact there seem\nto be at least two kinds of INSTRUMENTS ,intermediary instruments that can appear\nas subjects and enabling instruments that cannot:\n(21.9) a. Shelly cut the banana with a knife.\nb. The knife cut the banana.\n(21.10) a. Shelly ate the sliced banana with a fork.\nb. *The fork ate the sliced banana.\nIn addition to the fragmentation problem, there are cases in which we\u2019d like to\nreason about and generalize across semantic roles, but the \ufb01nite discrete lists of roles\ndon\u2019t let us do this.\nFinally, it has proved dif\ufb01cult to formally de\ufb01ne the thematic roles. Consider the\nAGENT role; most cases of AGENTS are animate, volitional, sentient, causal, but any\nindividual noun phrase might not exhibit all of these properties.\nThese problems have led to alternative semantic role models that use either semantic role\nmany fewer or many more roles.\nThe \ufb01rst of these options is to de\ufb01ne generalized semantic roles that abstract\nover the speci\ufb01c thematic roles. For example, PROTO -AGENT and PROTO -PATIENT proto-agent\nproto-patient are generalized roles that express roughly agent-like and roughly patient-like mean-\nings. These roles are de\ufb01ned, not by necessary and suf\ufb01cient conditions, but rather\nby a set of heuristic features that accompany more agent-like or more patient-like\nmeanings. Thus, the more an argument displays agent-like properties (being voli-\ntionally involved in the event, causing an event or a change of state in another par-\nticipant, being sentient or intentionally involved, moving) the greater the likelihood\nthat the argument can be labeled a PROTO -AGENT . The more patient-like the proper-\nties (undergoing change of state, causally affected by another participant, stationary\nrelative to other participants, etc.), the greater the likelihood that the argument can\nbe labeled a PROTO -PATIENT .\nThe second direction is instead to de\ufb01ne semantic roles that are speci\ufb01c to a\nparticular verb or a particular group of semantically related verbs or nouns.\nIn the next two sections we describe two commonly used lexical resources that\nmake use of these alternative versions of semantic roles. PropBank uses both proto-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 683,
      "token_count": 767,
      "chapter_title": ""
    }
  },
  {
    "content": "many fewer or many more roles.\nThe \ufb01rst of these options is to de\ufb01ne generalized semantic roles that abstract\nover the speci\ufb01c thematic roles. For example, PROTO -AGENT and PROTO -PATIENT proto-agent\nproto-patient are generalized roles that express roughly agent-like and roughly patient-like mean-\nings. These roles are de\ufb01ned, not by necessary and suf\ufb01cient conditions, but rather\nby a set of heuristic features that accompany more agent-like or more patient-like\nmeanings. Thus, the more an argument displays agent-like properties (being voli-\ntionally involved in the event, causing an event or a change of state in another par-\nticipant, being sentient or intentionally involved, moving) the greater the likelihood\nthat the argument can be labeled a PROTO -AGENT . The more patient-like the proper-\nties (undergoing change of state, causally affected by another participant, stationary\nrelative to other participants, etc.), the greater the likelihood that the argument can\nbe labeled a PROTO -PATIENT .\nThe second direction is instead to de\ufb01ne semantic roles that are speci\ufb01c to a\nparticular verb or a particular group of semantically related verbs or nouns.\nIn the next two sections we describe two commonly used lexical resources that\nmake use of these alternative versions of semantic roles. PropBank uses both proto-\nroles and verb-speci\ufb01c semantic roles. FrameNet uses semantic roles that are spe-\nci\ufb01c to a general semantic idea called a frame .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 684,
      "token_count": 322,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 473\n\n21.4 \u2022 T HEPROPOSITION BANK 465\n21.4 The Proposition Bank\nThe Proposition Bank , generally referred to as PropBank , is a resource of sen- PropBank\ntences annotated with semantic roles. The English PropBank labels all the sentences\nin the Penn TreeBank; the Chinese PropBank labels sentences in the Penn Chinese\nTreeBank. Because of the dif\ufb01culty of de\ufb01ning a universal set of thematic roles,\nthe semantic roles in PropBank are de\ufb01ned with respect to an individual verb sense.\nEach sense of each verb thus has a speci\ufb01c set of roles, which are given only numbers\nrather than names: Arg0 ,Arg1 ,Arg2 , and so on. In general, Arg0 represents the\nPROTO -AGENT , and Arg1 , the PROTO -PATIENT . The semantics of the other roles\nare less consistent, often being de\ufb01ned speci\ufb01cally for each verb. Nonetheless there\nare some generalization; the Arg2 is often the benefactive, instrument, attribute, or\nend state, the Arg3 the start point, benefactive, instrument, or attribute, and the Arg4\nthe end point.\nHere are some slightly simpli\ufb01ed PropBank entries for one sense each of the\nverbs agree andfall. Such PropBank entries are called frame \ufb01les ; note that the\nde\ufb01nitions in the frame \ufb01le for each role (\u201cOther entity agreeing\u201d, \u201cExtent, amount\nfallen\u201d) are informal glosses intended to be read by humans, rather than being formal\nde\ufb01nitions.\n(21.11) agree.01\nArg0: Agreer\nArg1: Proposition\nArg2: Other entity agreeing\nEx1: [ Arg0 The group] agreed [Arg1 it wouldn\u2019t make an offer].\nEx2: [ ArgM-TMP Usually] [ Arg0 John] agrees [Arg2 with Mary]\n[Arg1 on everything].\n(21.12) fall.01\nArg1: Logical subject, patient, thing falling\nArg2: Extent, amount fallen\nArg3: start point\nArg4: end point, end state of arg1\nEx1: [ Arg1 Sales] fell[Arg4 to $25 million] [ Arg3 from $27 million].\nEx2: [ Arg1 The average junk bond] fell[Arg2 by 4.2%].\nNote that there is no Arg0 role for fall, because the normal subject of fallis a\nPROTO -PATIENT .\nThe PropBank semantic roles can be useful in recovering shallow semantic in-\nformation about verbal arguments. Consider the verb increase :\n(21.13) increase.01 \u201cgo up incrementally\u201d\nArg0: causer of increase\nArg1: thing increasing\nArg2: amount increased by, EXT, or MNR\nArg3: start point\nArg4: end point\nA PropBank semantic role labeling would allow us to infer the commonality in\nthe event structures of the following three examples, that is, that in each case Big\nFruit Co. is the AGENT andthe price of bananas is the THEME , despite the differing\nsurface forms.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 685,
      "token_count": 677,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 474\n\n466 CHAPTER 21 \u2022 S EMANTIC ROLE LABELING\n(21.14) [ Arg0 Big Fruit Co. ] increased [ Arg1 the price of bananas].\n(21.15) [ Arg1 The price of bananas] was increased again [ Arg0 by Big Fruit Co. ]\n(21.16) [ Arg1 The price of bananas] increased [ Arg2 5%].\nPropBank also has a number of non-numbered arguments called ArgMs , (ArgM-\nTMP, ArgM-LOC, etc.) which represent modi\ufb01cation or adjunct meanings. These\nare relatively stable across predicates, so aren\u2019t listed with each frame \ufb01le. Data\nlabeled with these modi\ufb01ers can be helpful in training systems to detect temporal,\nlocation, or directional modi\ufb01cation across predicates. Some of the ArgM\u2019s include:\nTMP when? yesterday evening, now\nLOC where? at the museum, in San Francisco\nDIR where to/from? down, to Bangkok\nMNR how? clearly, with much enthusiasm\nPRP/CAU why? because ... , in response to the ruling\nREC themselves, each other\nADV miscellaneous\nPRD secondary predication ...ate the meat raw\nWhile PropBank focuses on verbs, a related project, NomBank (Meyers et al., NomBank\n2004) adds annotations to noun predicates. For example the noun agreement in\nApple\u2019s agreement with IBM would be labeled with Apple as the Arg0 and IBM as\nthe Arg2. This allows semantic role labelers to assign labels to arguments of both\nverbal and nominal predicates.\n21.5 FrameNet\nWhile making inferences about the semantic commonalities across different sen-\ntences with increase is useful, it would be even more useful if we could make such\ninferences in many more situations, across different verbs, and also between verbs\nand nouns. For example, we\u2019d like to extract the similarity among these three sen-\ntences:\n(21.17) [ Arg1 The price of bananas] increased [ Arg2 5%].\n(21.18) [ Arg1 The price of bananas] rose [ Arg2 5%].\n(21.19) There has been a [ Arg2 5%] rise [ Arg1 in the price of bananas].\nNote that the second example uses the different verb rise, and the third example\nuses the noun rather than the verb rise. We\u2019d like a system to recognize that the\nprice of bananas is what went up, and that 5%is the amount it went up, no matter\nwhether the 5%appears as the object of the verb increased or as a nominal modi\ufb01er\nof the noun rise.\nThe FrameNet project is another semantic-role-labeling project that attempts FrameNet\nto address just these kinds of problems (Baker et al. 1998, Fillmore et al. 2003,\nFillmore and Baker 2009, Ruppenhofer et al. 2016). Whereas roles in the PropBank\nproject are speci\ufb01c to an individual verb, roles in the FrameNet project are speci\ufb01c\nto aframe .\nWhat is a frame? Consider the following set of words:\nreservation, \ufb02ight, travel, buy, price, cost, fare, rates, meal, plane\nThere are many individual lexical relations of hyponymy, synonymy, and so on\nbetween many of the words in this list. The resulting set of relations does not,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 686,
      "token_count": 740,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 475\n\n21.5 \u2022 F RAME NET 467\nhowever, add up to a complete account of how these words are related. They are\nclearly all de\ufb01ned with respect to a coherent chunk of common-sense background\ninformation concerning air travel.\nWe call the holistic background knowledge that unites these words a frame (Fill- frame\nmore, 1985). The idea that groups of words are de\ufb01ned with respect to some back-\nground information is widespread in arti\ufb01cial intelligence and cognitive science,\nwhere besides frame we see related works like a model (Johnson-Laird, 1983), or model\neven script (Schank and Abelson, 1977). script\nA frame in FrameNet is a background knowledge structure that de\ufb01nes a set of\nframe-speci\ufb01c semantic roles, called frame elements , and includes a set of predi- frame elements\ncates that use these roles. Each word evokes a frame and pro\ufb01les some aspect of the\nframe and its elements. The FrameNet dataset includes a set of frames and frame\nelements, the lexical units associated with each frame, and a set of labeled exam-\nple sentences. For example, the change position onascale frame is de\ufb01ned as\nfollows:\nThis frame consists of words that indicate the change of an Item\u2019s posi-\ntion on a scale (the Attribute) from a starting point (Initial value) to an\nend point (Final value).\nSome of the semantic roles (frame elements) in the frame are de\ufb01ned as in\nFig. 21.3. Note that these are separated into core roles , which are frame speci\ufb01c, and core roles\nnon-core roles , which are more like the Arg-M arguments in PropBank, expressing non-core roles\nmore general properties of time, location, and so on.\nCore Roles\nATTRIBUTE The A TTRIBUTE is a scalar property that the I TEM possesses.\nDIFFERENCE The distance by which an I TEM changes its position on the scale.\nFINAL STATE A description that presents the I TEM\u2019s state after the change in the A TTRIBUTE \u2019s\nvalue as an independent predication.\nFINAL VALUE The position on the scale where the I TEM ends up.\nINITIAL STATE A description that presents the I TEM\u2019s state before the change in the A T-\nTRIBUTE \u2019s value as an independent predication.\nINITIAL VALUE The initial position on the scale from which the I TEM moves away.\nITEM The entity that has a position on the scale.\nVALUE RANGE A portion of the scale, typically identi\ufb01ed by its end points, along which the\nvalues of the A TTRIBUTE \ufb02uctuate.\nSome Non-Core Roles\nDURATION The length of time over which the change takes place.\nSPEED The rate of change of the V ALUE .\nGROUP The G ROUP in which an I TEM changes the value of an\nATTRIBUTE in a speci\ufb01ed way.\nFigure 21.3 The frame elements in the change position onascale frame from the FrameNet Labelers\nGuide (Ruppenhofer et al., 2016).\nHere are some example sentences:\n(21.20) [ ITEM Oil]rose [ATTRIBUTE in price] [ DIFFERENCE by 2%].\n(21.21) [ ITEM It] has increased [FINAL STATE to having them 1 day a month].\n(21.22) [ ITEM Microsoft shares] fell[FINAL VALUE to 7 5/8].\n(21.23) [ ITEM Colon cancer incidence] fell[DIFFERENCE by 50%] [ GROUP among\nmen].",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 687,
      "token_count": 763,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 476\n\n468 CHAPTER 21 \u2022 S EMANTIC ROLE LABELING\n(21.24) a steady increase [INITIAL VALUE from 9.5] [ FINAL VALUE to 14.3] [ ITEM\nin dividends]\n(21.25) a [ DIFFERENCE 5%] [ ITEM dividend] increase ...\nNote from these example sentences that the frame includes target words like rise,\nfall, and increase . In fact, the complete frame consists of the following words:\nVERBS: dwindle move soar escalation shift\nadvance edge mushroom swell explosion tumble\nclimb explode plummet swing fall\ndecline fall reach triple \ufb02uctuation ADVERBS:\ndecrease \ufb02uctuate rise tumble gain increasingly\ndiminish gain rocket growth\ndip grow shift NOUNS: hike\ndouble increase skyrocket decline increase\ndrop jump slide decrease rise\nFrameNet also codes relationships between frames, allowing frames to inherit\nfrom each other, or representing relations between frames like causation (and gen-\neralizations among frame elements in different frames can be represented by inheri-\ntance as well). Thus, there is a Cause change ofposition onascale frame that is\nlinked to the Change ofposition onascale frame by the cause relation, but that\nadds an A GENT role and is used for causative examples such as the following:\n(21.26) [ AGENT They] raised [ITEM the price of their soda] [ DIFFERENCE by 2%].\nTogether, these two frames would allow an understanding system to extract the\ncommon event semantics of all the verbal and nominal causative and non-causative\nusages.\nFrameNets have also been developed for many other languages including Span-\nish, German, Japanese, Portuguese, Italian, and Chinese.\n21.6 Semantic Role Labeling\nSemantic role labeling (sometimes shortened as SRL ) is the task of automaticallysemantic role\nlabeling\n\ufb01nding the semantic roles of each argument of each predicate in a sentence. Cur-\nrent approaches to semantic role labeling are based on supervised machine learning,\noften using the FrameNet and PropBank resources to specify what counts as a pred-\nicate, de\ufb01ne the set of roles used in the task, and provide training and test sets.\nRecall that the difference between these two models of semantic roles is that\nFrameNet (21.27) employs many frame-speci\ufb01c frame elements as roles, while Prop-\nBank (21.28) uses a smaller number of numbered argument labels that can be inter-\npreted as verb-speci\ufb01c labels, along with the more general ARGM labels. Some\nexamples:\n(21.27)[You] can\u2019t [blame] [the program] [for being unable to identify it]\nCOGNIZER TARGET EVALUEE REASON\n(21.28)[The San Francisco Examiner] issued [a special edition] [yesterday]\nARG0 TARGET ARG 1 ARGM -TMP\n21.6.1 A Feature-based Algorithm for Semantic Role Labeling\nA simpli\ufb01ed feature-based semantic role labeling algorithm is sketched in Fig. 21.4.\nFeature-based algorithms\u2014from the very earliest systems like (Simmons, 1973)\u2014\nbegin by parsing, using broad-coverage parsers to assign a parse to the input string.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 688,
      "token_count": 689,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 477\n\n21.6 \u2022 S EMANTIC ROLE LABELING 469\nFigure 21.5 shows a parse of (21.28) above. The parse is then traversed to \ufb01nd all\nwords that are predicates.\nFor each of these predicates, the algorithm examines each node in the parse\ntree and uses supervised classi\ufb01cation to decide the semantic role (if any) it plays\nfor this predicate. Given a labeled training set such as PropBank or FrameNet, a\nfeature vector is extracted for each node, using feature templates described in the\nnext subsection. A 1-of-N classi\ufb01er is then trained to predict a semantic role for\neach constituent given these features, where N is the number of potential semantic\nroles plus an extra NONE role for non-role constituents. Any standard classi\ufb01cation\nalgorithms can be used. Finally, for each test sentence to be labeled, the classi\ufb01er is\nrun on each relevant constituent.\nfunction SEMANTIC ROLELABEL (words )returns labeled tree\nparse PARSE (words )\nfor each predicate inparse do\nfor each node inparse do\nfeaturevector EXTRACT FEATURES (node ,predicate ,parse )\nCLASSIFY NODE(node ,featurevector ,parse )\nFigure 21.4 A generic semantic-role-labeling algorithm. C LASSIFY NODE is a 1-of- Nclas-\nsi\ufb01er that assigns a semantic role (or NONE for non-role constituents), trained on labeled data\nsuch as FrameNet or PropBank.\nS\nNP-SBJ =A R G 0 VP\nDT NNP NNP NNP\nThe San Francisco Examiner\nVBD = TARGET NP=A R G 1 PP-TMP =A R G M - T M P\nissued DT JJ NN IN NP\nas p e c i a l e d i t i o n a r o u n d N N N P - T M P\nnoon yesterday\nFigure 21.5 Parse tree for a PropBank sentence, showing the PropBank argument labels. The dotted line\nshows the path feature NP\"S#VP#VBD for ARG0, the NP-SBJ constituent The San Francisco Examiner.\nInstead of training a single-stage classi\ufb01er as in Fig. 21.5, the node-level classi-\n\ufb01cation task can be broken down into multiple steps:\n1.Pruning: Since only a small number of the constituents in a sentence are\narguments of any given predicate, many systems use simple heuristics to prune\nunlikely constituents.\n2.Identi\ufb01cation: a binary classi\ufb01cation of each node as an argument to be la-\nbeled or a NONE .\n3.Classi\ufb01cation: a 1-of- Nclassi\ufb01cation of all the constituents that were labeled\nas arguments by the previous stage",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 689,
      "token_count": 595,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 478\n\n470 CHAPTER 21 \u2022 S EMANTIC ROLE LABELING\nThe separation of identi\ufb01cation and classi\ufb01cation may lead to better use of fea-\ntures (different features may be useful for the two tasks) or to computational ef\ufb01-\nciency.\nGlobal Optimization\nThe classi\ufb01cation algorithm of Fig. 21.5 classi\ufb01es each argument separately (\u2018lo-\ncally\u2019), making the simplifying assumption that each argument of a predicate can be\nlabeled independently. This assumption is false; there are interactions between argu-\nments that require a more \u2018global\u2019 assignment of labels to constituents. For example,\nconstituents in FrameNet and PropBank are required to be non-overlapping. More\nsigni\ufb01cantly, the semantic roles of constituents are not independent. For example\nPropBank does not allow multiple identical arguments; two constituents of the same\nverb cannot both be labeled ARG0 .\nRole labeling systems thus often add a fourth step to deal with global consistency\nacross the labels in a sentence. For example, the local classi\ufb01ers can return a list of\npossible labels associated with probabilities for each constituent, and a second-pass\nViterbi decoding or re-ranking approach can be used to choose the best consensus\nlabel. Integer linear programming (ILP) is another common way to choose a solution\nthat conforms best to multiple constraints.\nFeatures for Semantic Role Labeling\nMost systems use some generalization of the core set of features introduced by\nGildea and Jurafsky (2000). Common basic features templates (demonstrated on\ntheNP-SBJ constituent The San Francisco Examiner in Fig. 21.5) include:\n\u2022 The governing predicate , in this case the verb issued . The predicate is a cru-\ncial feature since labels are de\ufb01ned only with respect to a particular predicate.\n\u2022 The phrase type of the constituent, in this case, NP(orNP-SBJ ). Some se-\nmantic roles tend to appear as NPs, others as SorPP, and so on.\n\u2022 The headword of the constituent, Examiner . The headword of a constituent\ncan be computed with standard head rules, such as those given in Appendix D\nin Fig. 18.17. Certain headwords (e.g., pronouns) place strong constraints on\nthe possible semantic roles they are likely to \ufb01ll.\n\u2022 The headword part of speech of the constituent, NNP .\n\u2022 The path in the parse tree from the constituent to the predicate. This path is\nmarked by the dotted line in Fig. 21.5. Following Gildea and Jurafsky (2000),\nwe can use a simple linear representation of the path, NP \"S#VP#VBD.\"and\n#represent upward and downward movement in the tree, respectively. The\npath is very useful as a compact representation of many kinds of grammatical\nfunction relationships between the constituent and the predicate.\n\u2022 The voice of the clause in which the constituent appears, in this case, active\n(as contrasted with passive ). Passive sentences tend to have strongly different\nlinkings of semantic roles to surface form than do active ones.\n\u2022 The binary linear position of the constituent with respect to the predicate,\neither before orafter .\n\u2022 The subcategorization of the predicate, the set of expected arguments that\nappear in the verb phrase. We can extract this information by using the phrase-\nstructure rule that expands the immediate parent of the predicate; VP !VBD\nNP PP for the predicate in Fig. 21.5.\n\u2022 The named entity type of the constituent.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 690,
      "token_count": 756,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 479\n\n21.6 \u2022 S EMANTIC ROLE LABELING 471\n\u2022 The \ufb01rst words and the last word of the constituent.\nThe following feature vector thus represents the \ufb01rst NP in our example (recall\nthat most observations will have the value NONE rather than, for example, ARG0,\nsince most constituents in the parse tree will not bear a semantic role):\nARG0: [issued, NP, Examiner, NNP, NP \"S#VP#VBD, active, before, VP !NP PP,\nORG, The, Examiner]\nOther features are often used in addition, such as sets of n-grams inside the\nconstituent, or more complex versions of the path features (the upward or downward\nhalves, or whether particular nodes occur in the path).\nIt\u2019s also possible to use dependency parses instead of constituency parses as the\nbasis of features, for example using dependency parse paths instead of constituency\npaths.\n21.6.2 A Neural Algorithm for Semantic Role Labeling\nA simple neural approach to SRL is to treat it as a sequence labeling task like named-\nentity recognition, using the BIO approach. Let\u2019s assume that we are given the\npredicate and the task is just detecting and labeling spans. Recall that with BIO\ntagging, we have a begin and end tag for each possible role ( B-ARG0,I-ARG0;B-\nARG1,I-ARG1, and so on), plus an outside tag O.\nENCODER[CLS]thecatslovehats[SEP]love[SEP]FFNB-ARG0I-ARG0B-PREDconcatenate with predicateB-ARG1FFNSoftmaxFFNFFNFFN\nFigure 21.6 A simple neural approach to semantic role labeling. The input sentence is\nfollowed by [SEP] and an extra input for the predicate, in this case love. The encoder outputs\nare concatenated to an indicator variable which is 1 for the predicate and 0 for all other words\nAfter He et al. (2017) and Shi and Lin (2019).\nAs with all the taggers, the goal is to compute the highest probability tag se-\nquence \u02c6 y, given the input sequence of words w:\n\u02c6y=argmax\ny2TP(yjw)\nFig. 21.6 shows a sketch of a standard algorithm from He et al. (2017). Here each\ninput word is mapped to pretrained embeddings, and then each token is concatenated\nwith the predicate embedding and then passed through a feedforward network with\na softmax which outputs a distribution over each SRL label. For decoding, a CRF\nlayer can be used instead of the MLP layer on top of the biLSTM output to do global\ninference, but in practice this doesn\u2019t seem to provide much bene\ufb01t.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 691,
      "token_count": 594,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 480\n\n472 CHAPTER 21 \u2022 S EMANTIC ROLE LABELING\n21.6.3 Evaluation of Semantic Role Labeling\nThe standard evaluation for semantic role labeling is to require that each argument\nlabel must be assigned to the exactly correct word sequence or parse constituent, and\nthen compute precision, recall, and F-measure. Identi\ufb01cation and classi\ufb01cation can\nalso be evaluated separately. Two common datasets used for evaluation are CoNLL-\n2005 (Carreras and M `arquez, 2005) and CoNLL-2012 (Pradhan et al., 2013).\n21.7 Selectional Restrictions\nWe turn in this section to another way to represent facts about the relationship be-\ntween predicates and arguments. A selectional restriction is a semantic type con-selectional\nrestriction\nstraint that a verb imposes on the kind of concepts that are allowed to \ufb01ll its argument\nroles. Consider the two meanings associated with the following example:\n(21.29) I want to eat someplace nearby.\nThere are two possible parses and semantic interpretations for this sentence. In\nthe sensible interpretation, eatis intransitive and the phrase someplace nearby is\nan adjunct that gives the location of the eating event. In the nonsensical speaker-as-\nGodzilla interpretation, eatis transitive and the phrase someplace nearby is the direct\nobject and the THEME of the eating, like the NP Malaysian food in the following\nsentences:\n(21.30) I want to eat Malaysian food.\nHow do we know that someplace nearby isn\u2019t the direct object in this sentence?\nOne useful cue is the semantic fact that the THEME of E ATING events tends to be\nsomething that is edible . This restriction placed by the verb eaton the \ufb01ller of its\nTHEME argument is a selectional restriction.\nSelectional restrictions are associated with senses, not entire lexemes. We can\nsee this in the following examples of the lexeme serve :\n(21.31) The restaurant serves green-lipped mussels.\n(21.32) Which airlines serve Denver?\nExample (21.31) illustrates the offering-food sense of serve , which ordinarily re-\nstricts its THEME to be some kind of food Example (21.32) illustrates the provides a\ncommercial service to sense of serve , which constrains its THEME to be some type\nof appropriate location.\nSelectional restrictions vary widely in their speci\ufb01city. The verb imagine , for\nexample, imposes strict requirements on its AGENT role (restricting it to humans\nand other animate entities) but places very few semantic requirements on its THEME\nrole. A verb like diagonalize , on the other hand, places a very speci\ufb01c constraint\non the \ufb01ller of its THEME role: it has to be a matrix, while the arguments of the\nadjective odorless are restricted to concepts that could possess an odor:\n(21.33) In rehearsal, I often ask the musicians to imagine a tennis game.\n(21.34) Radon is an odorless gas that can\u2019t be detected by human senses.\n(21.35) To diagonalize a matrix is to \ufb01nd its eigenvalues.\nThese examples illustrate that the set of concepts we need to represent selectional\nrestrictions (being a matrix, being able to possess an odor, etc) is quite open ended.\nThis distinguishes selectional restrictions from other features for representing lexical\nknowledge, like parts-of-speech, which are quite limited in number.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 692,
      "token_count": 733,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 481\n\n21.7 \u2022 S ELECTIONAL RESTRICTIONS 473\n21.7.1 Representing Selectional Restrictions\nOne way to capture the semantics of selectional restrictions is to use and extend the\nevent representation of Appendix F. Recall that the neo-Davidsonian representation\nof an event consists of a single variable that stands for the event, a predicate denoting\nthe kind of event, and variables and relations for the event roles. Ignoring the issue of\nthel-structures and using thematic roles rather than deep event roles, the semantic\ncontribution of a verb like eatmight look like the following:\n9e;x;y Eating (e)^Agent (e;x)^T heme (e;y)\nWith this representation, all we know about y, the \ufb01ller of the THEME role, is that\nit is associated with an Eating event through the Theme relation. To stipulate the\nselectional restriction that ymust be something edible, we simply add a new term to\nthat effect:\n9e;x;y Eating (e)^Agent (e;x)^T heme (e;y)^EdibleT hing (y)\nWhen a phrase like ate a hamburger is encountered, a semantic analyzer can form\nthe following kind of representation:\n9e;x;y Eating (e)^Eater (e;x)^T heme (e;y)^EdibleT hing (y)^Hamburger (y)\nThis representation is perfectly reasonable since the membership of yin the category\nHamburger is consistent with its membership in the category EdibleThing , assuming\na reasonable set of facts in the knowledge base. Correspondingly, the representation\nfor a phrase such as ate a takeoff would be ill-formed because membership in an\nevent-like category such as Takeoff would be inconsistent with membership in the\ncategory EdibleThing .\nWhile this approach adequately captures the semantics of selectional restrictions,\nthere are two problems with its direct use. First, using FOL to perform the simple\ntask of enforcing selectional restrictions is overkill. Other, far simpler, formalisms\ncan do the job with far less computational cost. The second problem is that this\napproach presupposes a large, logical knowledge base of facts about the concepts\nthat make up selectional restrictions. Unfortunately, although such common-sense\nknowledge bases are being developed, none currently have the kind of coverage\nnecessary to the task.\nA more practical approach is to state selectional restrictions in terms of WordNet\nsynsets rather than as logical concepts. Each predicate simply speci\ufb01es a WordNet\nsynset as the selectional restriction on each of its arguments. A meaning representa-\ntion is well-formed if the role \ufb01ller word is a hyponym (subordinate) of this synset.\nFor our ate a hamburger example, for instance, we could set the selectional\nrestriction on the THEME role of the verb eatto the synset ffood, nutrient g, glossed\nasany substance that can be metabolized by an animal to give energy and build\ntissue . Luckily, the chain of hypernyms for hamburger shown in Fig. 21.7 reveals\nthat hamburgers are indeed food. Again, the \ufb01ller of a role need not match the\nrestriction synset exactly; it just needs to have the synset as one of its superordinates.\nWe can apply this approach to the THEME roles of the verbs imagine ,lift, and di-\nagonalize , discussed earlier. Let us restrict imagine \u2019sTHEME to the synsetfentityg,\nlift\u2019sTHEME tofphysical entityg, and diagonalize tofmatrixg. This arrangement\ncorrectly permits imagine a hamburger andlift a hamburger , while also correctly\nruling out diagonalize a hamburger .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 693,
      "token_count": 778,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 482\n\n474 CHAPTER 21 \u2022 S EMANTIC ROLE LABELING\nSense 1\nhamburger, beefburger --\n(a fried cake of minced beef served on a bun)\n=> sandwich\n=> snack food\n=> dish\n=> nutriment, nourishment, nutrition...\n=> food, nutrient\n=> substance\n=> matter\n=> physical entity\n=> entity\nFigure 21.7 Evidence from WordNet that hamburgers are edible.\n21.7.2 Selectional Preferences\nIn the earliest implementations, selectional restrictions were considered strict con-\nstraints on the kind of arguments a predicate could take (Katz and Fodor 1963,\nHirst 1987). For example, the verb eatmight require that its THEME argument be\n[+FOOD] . Early word sense disambiguation systems used this idea to rule out senses\nthat violated the selectional restrictions of their governing predicates.\nVery quickly, however, it became clear that these selectional restrictions were\nbetter represented as preferences rather than strict constraints (Wilks 1975b, Wilks\n1975a). For example, selectional restriction violations (like inedible arguments of\neat) often occur in well-formed sentences, for example because they are negated\n(21.36), or because selectional restrictions are overstated (21.37):\n(21.36) But it fell apart in 1931, perhaps because people realized you can\u2019t eat\ngold for lunch if you\u2019re hungry.\n(21.37) In his two championship trials, Mr. Kulkarni ateglass on an empty\nstomach, accompanied only by water and tea.\nModern systems for selectional preferences therefore specify the relation be-\ntween a predicate and its possible arguments with soft constraints of some kind.\nSelectional Association\nOne of the most in\ufb02uential has been the selectional association model of Resnik\n(1993). Resnik de\ufb01nes the idea of selectional preference strength as the generalselectional\npreference\nstrengthamount of information that a predicate tells us about the semantic class of its argu-\nments. For example, the verb eattells us a lot about the semantic class of its direct\nobjects, since they tend to be edible. The verb be, by contrast, tells us less about\nits direct objects. The selectional preference strength can be de\ufb01ned by the differ-\nence in information between two distributions: the distribution of expected semantic\nclasses P(c)(how likely is it that a direct object will fall into class c) and the dis-\ntribution of expected semantic classes for the particular verb P(cjv)(how likely is\nit that the direct object of the speci\ufb01c verb vwill fall into semantic class c). The\ngreater the difference between these distributions, the more information the verb\nis giving us about possible objects. The difference between these two distributions\ncan be quanti\ufb01ed by relative entropy , or the Kullback-Leibler divergence (Kullback relative entropy\nand Leibler, 1951). The Kullback-Leibler or KL divergence D(PjjQ)expresses the KL divergence",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 694,
      "token_count": 658,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 483\n\n21.7 \u2022 S ELECTIONAL RESTRICTIONS 475\ndifference between two probability distributions PandQ\nD(PjjQ) =X\nxP(x)logP(x)\nQ(x)(21.38)\nThe selectional preference SR(v)uses the KL divergence to express how much in-\nformation, in bits, the verb vexpresses about the possible semantic class of its argu-\nment.\nSR(v) = D(P(cjv)jjP(c))\n=X\ncP(cjv)logP(cjv)\nP(c)(21.39)\nResnik then de\ufb01nes the selectional association of a particular class and verb as theselectional\nassociation\nrelative contribution of that class to the general selectional preference of the verb:\nAR(v;c) =1\nSR(v)P(cjv)logP(cjv)\nP(c)(21.40)\nThe selectional association is thus a probabilistic measure of the strength of asso-\nciation between a predicate and a class dominating the argument to the predicate.\nResnik estimates the probabilities for these associations by parsing a corpus, count-\ning all the times each predicate occurs with each argument word, and assuming\nthat each word is a partial observation of all the WordNet concepts containing the\nword. The following table from Resnik (1996) shows some sample high and low\nselectional associations for verbs and some WordNet semantic classes of their direct\nobjects.\nDirect Object Direct Object\nVerb Semantic Class Assoc Semantic Class Assoc\nread WRITING 6.80 ACTIVITY -.20\nwrite WRITING 7.26 COMMERCE 0\nsee ENTITY 5.79 METHOD -0.01\nSelectional Preference via Conditional Probability\nAn alternative to using selectional association between a verb and the WordNet class\nof its arguments is to use the conditional probability of an argument word given a\npredicate verb, directly modeling the strength of association of one verb (predicate)\nwith one noun (argument).\nThe conditional probability model can be computed by parsing a very large cor-\npus (billions of words), and computing co-occurrence counts: how often a given\nverb occurs with a given noun in a given relation. The conditional probability of an\nargument noun given a verb for a particular relation P(njv;r)can then be used as a\nselectional preference metric for that pair of words (Brockmann and Lapata 2003,\nKeller and Lapata 2003):\nP(njv;r) =(\nC(n;v;r)\nC(v;r)ifC(n;v;r)>0\n0 otherwise\nThe inverse probability P(vjn;r)was found to have better performance in some cases\n(Brockmann and Lapata, 2003):\nP(vjn;r) =(\nC(n;v;r)\nC(n;r)ifC(n;v;r)>0\n0 otherwise",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 695,
      "token_count": 606,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 484\n\n476 CHAPTER 21 \u2022 S EMANTIC ROLE LABELING\nAn even simpler approach is to use the simple log co-occurrence frequency of\nthe predicate with the argument log count (v;n;r)instead of conditional probability;\nthis seems to do better for extracting preferences for syntactic subjects rather than\nobjects (Brockmann and Lapata, 2003).\nEvaluating Selectional Preferences\nOne way to evaluate models of selectional preferences is to use pseudowords (Gale pseudowords\net al. 1992b, Sch \u00a8utze 1992a). A pseudoword is an arti\ufb01cial word created by concate-\nnating a test word in some context (say banana ) with a confounder word (say door )\nto create banana-door ). The task of the system is to identify which of the two words\nis the original word. To evaluate a selectional preference model (for example on the\nrelationship between a verb and a direct object) we take a test corpus and select all\nverb tokens. For each verb token (say drive ) we select the direct object (e.g., car),\nconcatenated with a confounder word that is its nearest neighbor , the noun with the\nfrequency closest to the original (say house ), to make car/house ). We then use the\nselectional preference model to choose which of carandhouse are more preferred\nobjects of drive , and compute how often the model chooses the correct original ob-\nject (e.g., car) (Chambers and Jurafsky, 2010).\nAnother evaluation metric is to get human preferences for a test set of verb-\nargument pairs, and have them rate their degree of plausibility. This is usually done\nby using magnitude estimation, a technique from psychophysics, in which subjects\nrate the plausibility of an argument proportional to a modulus item. A selectional\npreference model can then be evaluated by its correlation with the human prefer-\nences (Keller and Lapata, 2003).\n21.8 Primitive Decomposition of Predicates\nOne way of thinking about the semantic roles we have discussed through the chapter\nis that they help us de\ufb01ne the roles that arguments play in a decompositional way,\nbased on \ufb01nite lists of thematic roles (agent, patient, instrument, proto-agent, proto-\npatient, etc.). This idea of decomposing meaning into sets of primitive semantic\nelements or features, called primitive decomposition orcomponential analysis ,componential\nanalysis\nhas been taken even further, and focused particularly on predicates.\nConsider these examples of the verb kill:\n(21.41) Jim killed his philodendron.\n(21.42) Jim did something to cause his philodendron to become not alive.\nThere is a truth-conditional (\u2018propositional semantics\u2019) perspective from which these\ntwo sentences have the same meaning. Assuming this equivalence, we could repre-\nsent the meaning of killas:\n(21.43) KILL (x,y),CAUSE (x,BECOME (NOT(ALIVE (y))))\nthus using semantic primitives like do,cause ,become not , and alive .\nIndeed, one such set of potential semantic primitives has been used to account\nfor some of the verbal alternations discussed in Section 21.2 (Lakoff 1965, Dowty\n1979). Consider the following examples.\n(21.44) John opened the door. )CAUSE (John, BECOME (OPEN (door)))\n(21.45) The door opened. )BECOME (OPEN (door))",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 696,
      "token_count": 751,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 485\n\n21.9 \u2022 S UMMARY 477\n(21.46) The door is open. )OPEN (door)\nThe decompositional approach asserts that a single state-like predicate associ-\nated with open underlies all of these examples. The differences among the meanings\nof these examples arises from the combination of this single predicate with the prim-\nitives CAUSE and BECOME .\nWhile this approach to primitive decomposition can explain the similarity be-\ntween states and actions or causative and non-causative predicates, it still relies on\nhaving a large number of predicates like open . More radical approaches choose to\nbreak down these predicates as well. One such approach to verbal predicate decom-\nposition that played a role in early natural language systems is conceptual depen-\ndency (CD), a set of ten primitive predicates, shown in Fig. 21.8.conceptual\ndependency\nPrimitive De\ufb01nition\nATRANS The abstract transfer of possession or control from one entity to\nanother\nPTRANS The physical transfer of an object from one location to another\nMTRANS The transfer of mental concepts between entities or within an\nentity\nMBUILD The creation of new information within an entity\nPROPEL The application of physical force to move an object\nMOVE The integral movement of a body part by an animal\nINGEST The taking in of a substance by an animal\nEXPEL The expulsion of something from an animal\nSPEAK The action of producing a sound\nATTEND The action of focusing a sense organ\nFigure 21.8 A set of conceptual dependency primitives.\nBelow is an example sentence along with its CDrepresentation. The verb brought\nis translated into the two primitives ATRANS and PTRANS to indicate that the waiter\nboth physically conveyed the check to Mary and passed control of it to her. Note\nthat CDalso associates a \ufb01xed set of thematic roles with each primitive to represent\nthe various participants in the action.\n(21.47) The waiter brought Mary the check.\n9x;y Atrans (x)^Actor (x;Waiter )^Ob ject (x;Check )^To(x;Mary )\n^Ptrans (y)^Actor (y;Waiter )^Ob ject (y;Check )^To(y;Mary )\n21.9 Summary\n\u2022Semantic roles are abstract models of the role an argument plays in the event\ndescribed by the predicate.\n\u2022Thematic roles are a model of semantic roles based on a single \ufb01nite list of\nroles. Other semantic role models include per-verb semantic role lists and\nproto-agent /proto-patient , both of which are implemented in PropBank ,\nand per-frame role lists, implemented in FrameNet .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 697,
      "token_count": 563,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 486\n\n478 CHAPTER 21 \u2022 S EMANTIC ROLE LABELING\n\u2022Semantic role labeling is the task of assigning semantic role labels to the\nconstituents of a sentence. The task is generally treated as a supervised ma-\nchine learning task, with models trained on PropBank or FrameNet. Algo-\nrithms generally start by parsing a sentence and then automatically tag each\nparse tree node with a semantic role. Neural models map straight from words\nend-to-end.\n\u2022 Semantic selectional restrictions allow words (particularly predicates) to post\nconstraints on the semantic properties of their argument words. Selectional\npreference models (like selectional association or simple conditional proba-\nbility) allow a weight or probability to be assigned to the association between\na predicate and an argument word or class.\nBibliographical and Historical Notes\nAlthough the idea of semantic roles dates back to P \u00afan.ini, they were re-introduced\ninto modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fill-\nmore had become interested in argument structure by studying Lucien Tesni `ere\u2019s\ngroundbreaking \u00b4El\u00b4ements de Syntaxe Structurale (Tesni `ere, 1959) in which the term\n\u2018dependency\u2019 was introduced and the foundations were laid for dependency gram-\nmar. Following Tesni `ere\u2019s terminology, Fillmore \ufb01rst referred to argument roles as\nactants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003))\nand proposed a universal list of semantic roles or cases (Agent, Patient, Instrument,\netc.), that could be taken on by the arguments of predicates. Verbs would be listed in\nthe lexicon with their case frame , the list of obligatory (or optional) case arguments.\nThe idea that semantic roles could provide an intermediate level of semantic\nrepresentation that could help map from syntactic parse structures to deeper, more\nfully-speci\ufb01ed representations of meaning was quickly adopted in natural language\nprocessing, and systems for extracting case frames were created for machine transla-\ntion (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language pro-\ncessing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). General-\npurpose semantic role labelers were developed. The earliest ones (Simmons, 1973)\n\ufb01rst parsed a sentence by means of an ATN (Augmented Transition Network) parser.\nEach verb then had a set of rules specifying how the parse should be mapped to se-\nmantic roles. These rules mainly made reference to grammatical functions (subject,\nobject, complement of speci\ufb01c prepositions) but also checked constituent internal\nfeatures such as the animacy of head nouns. Later systems assigned roles from pre-\nbuilt parse trees, again by using dictionaries with verb-speci\ufb01c case frames (Levin\n1977, Marcus 1980).\nBy 1977 case representation was widely used and taught in AI and NLP courses,\nand was described as a standard of natural language processing in the \ufb01rst edition of\nWinston\u2019s 1977 textbook Arti\ufb01cial Intelligence .\nIn the 1980s Fillmore proposed his model of frame semantics , later describing\nthe intuition as follows:\n\u201cThe idea behind frame semantics is that speakers are aware of possi-\nbly quite complex situation types, packages of connected expectations,\nthat go by various names\u2014frames, schemas, scenarios, scripts, cultural\nnarratives, memes\u2014and the words in our language are understood with\nsuch frames as their presupposed background.\u201d (Fillmore, 2012, p. 712)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 698,
      "token_count": 787,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 487",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 699,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 479\nThe word frame seemed to be in the air for a suite of related notions proposed at\nabout the same time by Minsky (1974), Hymes (1974), and Goffman (1974), as\nwell as related notions with other names like scripts (Schank and Abelson, 1975)\nandschemata (Bobrow and Norman, 1975) (see Tannen (1979) for a comparison).\nFillmore was also in\ufb02uenced by the semantic \ufb01eld theorists and by a visit to the Yale\nAI lab where he took notice of the lists of slots and \ufb01llers used by early information\nextraction systems like DeJong (1982) and Schank and Abelson (1977). In the 1990s\nFillmore drew on these insights to begin the FrameNet corpus annotation project.\nAt the same time, Beth Levin drew on her early case frame dictionaries (Levin,\n1977) to develop her book which summarized sets of verb classes de\ufb01ned by shared\nargument realizations (Levin, 1993). The VerbNet project built on this work (Kipper\net al., 2000), leading soon afterwards to the PropBank semantic-role-labeled corpus\ncreated by Martha Palmer and colleagues (Palmer et al., 2005).\nThe combination of rich linguistic annotation and corpus-based approach in-\nstantiated in FrameNet and PropBank led to a revival of automatic approaches to\nsemantic role labeling, \ufb01rst on FrameNet (Gildea and Jurafsky, 2000) and then on\nPropBank data (Gildea and Palmer, 2002, inter alia). The problem \ufb01rst addressed in\nthe 1970s by handwritten rules was thus now generally recast as one of supervised\nmachine learning enabled by large and consistent databases. Many popular features\nused for role labeling are de\ufb01ned in Gildea and Jurafsky (2002), Surdeanu et al.\n(2003), Xue and Palmer (2004), Pradhan et al. (2005), Che et al. (2009), and Zhao\net al. (2009). The use of dependency rather than constituency parses was introduced\nin the CoNLL-2008 shared task (Surdeanu et al., 2008). For surveys see Palmer\net al. (2010) and M `arquez et al. (2008).\nThe use of neural approaches to semantic role labeling was pioneered by Col-\nlobert et al. (2011), who applied a CRF on top of a convolutional net. Early work\nlike Foland, Jr. and Martin (2015) focused on using dependency features. Later work\neschewed syntactic features altogether; Zhou and Xu (2015b) introduced the use of\na stacked (6-8 layer) biLSTM architecture, and (He et al., 2017) showed how to\naugment the biLSTM architecture with highway networks and also replace the CRF\nwith A* decoding that make it possible to apply a wide variety of global constraints\nin SRL decoding.\nMost semantic role labeling schemes only work within a single sentence, fo-\ncusing on the object of the verbal (or nominal, in the case of NomBank) predicate.\nHowever, in many cases, a verbal or nominal predicate may have an implicit argu-\nment : one that appears only in a contextual sentence, or perhaps not at all and mustimplicit\nargument\nbe inferred. In the two sentences This house has a new owner. The sale was \ufb01nalized",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 700,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "in the CoNLL-2008 shared task (Surdeanu et al., 2008). For surveys see Palmer\net al. (2010) and M `arquez et al. (2008).\nThe use of neural approaches to semantic role labeling was pioneered by Col-\nlobert et al. (2011), who applied a CRF on top of a convolutional net. Early work\nlike Foland, Jr. and Martin (2015) focused on using dependency features. Later work\neschewed syntactic features altogether; Zhou and Xu (2015b) introduced the use of\na stacked (6-8 layer) biLSTM architecture, and (He et al., 2017) showed how to\naugment the biLSTM architecture with highway networks and also replace the CRF\nwith A* decoding that make it possible to apply a wide variety of global constraints\nin SRL decoding.\nMost semantic role labeling schemes only work within a single sentence, fo-\ncusing on the object of the verbal (or nominal, in the case of NomBank) predicate.\nHowever, in many cases, a verbal or nominal predicate may have an implicit argu-\nment : one that appears only in a contextual sentence, or perhaps not at all and mustimplicit\nargument\nbe inferred. In the two sentences This house has a new owner. The sale was \ufb01nalized\n10 days ago. thesale in the second sentence has no A RG1, but a reasonable reader\nwould infer that the Arg1 should be the house mentioned in the prior sentence. Find-\ning these arguments, implicit argument detection (sometimes shortened as iSRL ) iSRL\nwas introduced by Gerber and Chai (2010) and Ruppenhofer et al. (2010). See Do\net al. (2017) for more recent neural models.\nTo avoid the need for huge labeled training sets, unsupervised approaches for\nsemantic role labeling attempt to induce the set of semantic roles by clustering over\narguments. The task was pioneered by Riloff and Schmelzenbach (1998) and Swier\nand Stevenson (2004); see Grenager and Manning (2006), Titov and Klementiev\n(2012), Lang and Lapata (2014), Woodsend and Lapata (2015), and Titov and Khod-\ndam (2014).\nRecent innovations in frame labeling include connotation frames , which mark\nricher information about the argument of predicates. Connotation frames mark the",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 701,
      "token_count": 523,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 488\n\n480 CHAPTER 21 \u2022 S EMANTIC ROLE LABELING\nsentiment of the writer or reader toward the arguments (for example using the verb\nsurvive inhe survived a bombing expresses the writer\u2019s sympathy toward the subject\nheand negative sentiment toward the bombing. See Chapter 22 for more details.\nSelectional preference has been widely studied beyond the selectional associa-\ntion models of Resnik (1993) and Resnik (1996). Methods have included clustering\n(Rooth et al., 1999), discriminative learning (Bergsma et al., 2008a), and topic mod-\nels (S \u00b4eaghdha 2010, Ritter et al. 2010b), and constraints can be expressed at the level\nof words or classes (Agirre and Martinez, 2001). Selectional preferences have also\nbeen successfully integrated into semantic role labeling (Erk 2007, Zapirain et al.\n2013, Do et al. 2017).\nExercises",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 702,
      "token_count": 218,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 489\n\nCHAPTER\n22Lexicons for Sentiment, Affect,\nand Connotation\nSome day we\u2019ll be able to measure the power of words\nMaya Angelou\nIn this chapter we turn to tools for interpreting affective meaning, extending our affective\nstudy of sentiment analysis in Chapter 4. We use the word \u2018affective\u2019, following the\ntradition in affective computing (Picard, 1995) to mean emotion, sentiment, per-\nsonality, mood, and attitudes. Affective meaning is closely related to subjectivity , subjectivity\nthe study of a speaker or writer\u2019s evaluations, opinions, emotions, and speculations\n(Wiebe et al., 1999).\nHow should affective meaning be de\ufb01ned? One in\ufb02uential typology of affec-\ntive states comes from Scherer (2000), who de\ufb01nes each class of affective states by\nfactors like its cognitive realization and time course (Fig. 22.1).\nEmotion: Relatively brief episode of response to the evaluation of an external\nor internal event as being of major signi\ufb01cance.\n(angry, sad, joyful, fearful, ashamed, proud, elated, desperate )\nMood: Diffuse affect state, most pronounced as change in subjective feeling, of\nlow intensity but relatively long duration, often without apparent cause.\n(cheerful, gloomy, irritable, listless, depressed, buoyant )\nInterpersonal stance: Affective stance taken toward another person in a spe-\nci\ufb01c interaction, coloring the interpersonal exchange in that situation.\n(distant, cold, warm, supportive, contemptuous, friendly )\nAttitude: Relatively enduring, affectively colored beliefs, preferences, and pre-\ndispositions towards objects or persons.\n(liking, loving, hating, valuing, desiring )\nPersonality traits: Emotionally laden, stable personality dispositions and be-\nhavior tendencies, typical for a person.\n(nervous, anxious, reckless, morose, hostile, jealous )\nFigure 22.1 The Scherer typology of affective states (Scherer, 2000).\nWe can design extractors for each of these kinds of affective states. Chapter 4\nalready introduced sentiment analysis , the task of extracting the positive or negative\norientation that a writer expresses in a text. This corresponds in Scherer\u2019s typology\nto the extraction of attitudes : \ufb01guring out what people like or dislike, from affect-\nrich texts like consumer reviews of books or movies, newspaper editorials, or public\nsentiment in blogs or tweets.\nDetecting emotion andmoods is useful for detecting whether a student is con-\nfused, engaged, or certain when interacting with a tutorial system, whether a caller\nto a help line is frustrated, whether someone\u2019s blog posts or tweets indicated depres-\nsion. Detecting emotions like fear in novels, for example, could help us trace what\ngroups or situations are feared and how that changes over time.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 703,
      "token_count": 634,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 490",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 704,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "482 CHAPTER 22 \u2022 L EXICONS FOR SENTIMENT , AFFECT ,AND CONNOTATION\nDetecting different interpersonal stances can be useful when extracting infor-\nmation from human-human conversations. The goal here is to detect stances like\nfriendliness or awkwardness in interviews or friendly conversations, for example for\nsummarizing meetings or \ufb01nding parts of a conversation where people are especially\nexcited or engaged, conversational hot spots that can help in meeting summariza-\ntion. Detecting the personality of a user\u2014such as whether the user is an extrovert\nor the extent to which they are open to experience \u2014 can help improve conversa-\ntional agents, which seem to work better if they match users\u2019 personality expecta-\ntions (Mairesse and Walker, 2008). And affect is important for generation as well\nas recognition; synthesizing affect is important for conversational agents in various\ndomains, including literacy tutors such as children\u2019s storybooks, or computer games.\nIn Chapter 4 we introduced the use of naive Bayes classi\ufb01cation to classify a\ndocument\u2019s sentiment. Various classi\ufb01ers have been successfully applied to many of\nthese tasks, using all the words in the training set as input to a classi\ufb01er which then\ndetermines the affect status of the text.\nIn this chapter we focus on an alternative model, in which instead of using every\nword as a feature, we focus only on certain words, ones that carry particularly strong\ncues to affect or sentiment. We call these lists of words affective lexicons orsenti-\nment lexicons . These lexicons presuppose a fact about semantics: that words have\naffective meanings orconnotations . The word connotation has different meanings connotations\nin different \ufb01elds, but here we use it to mean the aspects of a word\u2019s meaning that\nare related to a writer or reader\u2019s emotions, sentiment, opinions, or evaluations. In\naddition to their ability to help determine the affective status of a text, connotation\nlexicons can be useful features for other kinds of affective tasks, and for computa-\ntional social science analysis.\nIn the next sections we introduce basic theories of emotion, show how sentiment\nlexicons are a special case of emotion lexicons, and mention some useful lexicons.\nWe then survey three ways for building lexicons: human labeling, semi-supervised,\nand supervised. Finally, we talk about how to detect affect toward a particular entity,\nand introduce connotation frames.\n22.1 De\ufb01ning Emotion\nOne of the most important affective classes is emotion , which Scherer (2000) de\ufb01nes emotion\nas a \u201crelatively brief episode of response to the evaluation of an external or internal\nevent as being of major signi\ufb01cance\u201d.\nDetecting emotion has the potential to improve a number of language processing\ntasks. Emotion recognition could help dialogue systems like tutoring systems detect\nthat a student was unhappy, bored, hesitant, con\ufb01dent, and so on. Automatically\ndetecting emotions in reviews or customer responses (anger, dissatisfaction, trust)\ncould help businesses recognize speci\ufb01c problem areas or ones that are going well.\nEmotion can play a role in medical NLP tasks like helping diagnose depression or\nsuicidal intent. Detecting emotions expressed toward characters in novels might\nplay a role in understanding how different social groups were viewed by society at\ndifferent times.\nComputational models of emotion in NLP have mainly been based on two fami-\nlies of theories of emotion (out of the many studied in the \ufb01eld of affective science).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 705,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "We then survey three ways for building lexicons: human labeling, semi-supervised,\nand supervised. Finally, we talk about how to detect affect toward a particular entity,\nand introduce connotation frames.\n22.1 De\ufb01ning Emotion\nOne of the most important affective classes is emotion , which Scherer (2000) de\ufb01nes emotion\nas a \u201crelatively brief episode of response to the evaluation of an external or internal\nevent as being of major signi\ufb01cance\u201d.\nDetecting emotion has the potential to improve a number of language processing\ntasks. Emotion recognition could help dialogue systems like tutoring systems detect\nthat a student was unhappy, bored, hesitant, con\ufb01dent, and so on. Automatically\ndetecting emotions in reviews or customer responses (anger, dissatisfaction, trust)\ncould help businesses recognize speci\ufb01c problem areas or ones that are going well.\nEmotion can play a role in medical NLP tasks like helping diagnose depression or\nsuicidal intent. Detecting emotions expressed toward characters in novels might\nplay a role in understanding how different social groups were viewed by society at\ndifferent times.\nComputational models of emotion in NLP have mainly been based on two fami-\nlies of theories of emotion (out of the many studied in the \ufb01eld of affective science).\nIn one of these families, emotions are viewed as \ufb01xed atomic units, limited in num-\nber, and from which others are generated, often called basic emotions (Tomkins basic emotions",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 706,
      "token_count": 313,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 491\n\n22.1 \u2022 D EFINING EMOTION 483\n1962, Plutchik 1962), a model dating back to Darwin. Perhaps the most well-known\nof this family of theories are the 6 emotions proposed by Ekman (e.g., Ekman 1999)\nto be universally present in all cultures: surprise, happiness, anger, fear, disgust,\nsadness . Another atomic theory is the Plutchik (1980) wheel of emotion, consisting\nof 8 basic emotions in four opposing pairs: joy\u2013sadness ,anger\u2013fear ,trust\u2013disgust ,\nandanticipation\u2013surprise , together with the emotions derived from them, shown in\nFig. 22.2.\nFigure 22.2 Plutchik wheel of emotion.\nThe second class of emotion theories widely used in NLP views emotion as a\nspace in 2 or 3 dimensions (Russell, 1980). Most models include the two dimensions\nvalence andarousal , and many add a third, dominance . These can be de\ufb01ned as:\nvalence: the pleasantness of the stimulus\narousal: the level of alertness, activeness, or energy provoked by the stimulus\ndominance: the degree of control or dominance exerted by the stimulus or the\nemotion\nSentiment can be viewed as a special case of this second view of emotions as points\nin space. In particular, the valence dimension, measuring how pleasant or unpleasant\na word is, is often used directly as a measure of sentiment.\nIn these lexicon-based models of affect, the affective meaning of a word is gen-\nerally \ufb01xed, irrespective of the linguistic context in which a word is used, or the\ndialect or culture of the speaker. By contrast, other models in affective science repre-\nsent emotions as much richer processes involving cognition (Barrett et al., 2007). In\nappraisal theory , for example, emotions are complex processes, in which a person\nconsiders how an event is congruent with their goals, taking into account variables\nlike the agency, certainty, urgency, novelty and control associated with the event\n(Moors et al., 2013). Computational models in NLP taking into account these richer\ntheories of emotion will likely play an important role in future work.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 707,
      "token_count": 494,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 492",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 708,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "484 CHAPTER 22 \u2022 L EXICONS FOR SENTIMENT , AFFECT ,AND CONNOTATION\n22.2 Available Sentiment and Affect Lexicons\nA wide variety of affect lexicons have been created and released. The most basic\nlexicons label words along one dimension of semantic variability, generally called\n\u201csentiment\u201d or \u201cvalence\u201d.\nIn the simplest lexicons this dimension is represented in a binary fashion, with\na wordlist for positive words and a wordlist for negative words. The oldest is the\nGeneral Inquirer (Stone et al., 1966), which drew on content analysis and on earlyGeneral\nInquirer\nwork in the cognitive psychology of word meaning (Osgood et al., 1957). The Gen-\neral Inquirer has a lexicon of 1915 positive words and a lexicon of 2291 negative\nwords (as well as other lexicons discussed below). The MPQA Subjectivity lexicon\n(Wilson et al., 2005) has 2718 positive and 4912 negative words drawn from prior\nlexicons plus a bootstrapped list of subjective words and phrases (Riloff and Wiebe,\n2003). Each entry in the lexicon is hand-labeled for sentiment and also labeled for\nreliability (strongly subjective or weakly subjective). The polarity lexicon of Hu\nand Liu (2004b) gives 2006 positive and 4783 negative words, drawn from product\nreviews, labeled using a bootstrapping method from WordNet.\nPositive admire, amazing, assure, celebration, charm, eager, enthusiastic, excellent, fancy, fan-\ntastic, frolic, graceful, happy, joy, luck, majesty, mercy, nice, patience, perfect, proud,\nrejoice, relief, respect, satisfactorily, sensational, super, terri\ufb01c, thank, vivid, wise, won-\nderful, zest\nNegative abominable, anger, anxious, bad, catastrophe, cheap, complaint, condescending, deceit,\ndefective, disappointment, embarrass, fake, fear, \ufb01lthy, fool, guilt, hate, idiot, in\ufb02ict, lazy,\nmiserable, mourn, nervous, objection, pest, plot, reject, scream, silly, terrible, unfriendly,\nvile, wicked\nFigure 22.3 Some words with consistent sentiment across the General Inquirer (Stone et al., 1966), the\nMPQA Subjectivity lexicon (Wilson et al., 2005), and the polarity lexicon of Hu and Liu (2004b).\nSlightly more general than these sentiment lexicons are lexicons that assign each\nword a value on all three affective dimensions. The NRC Valence, Arousal, and\nDominance (V AD) lexicon (Mohammad, 2018a) assigns valence, arousal, and dom-\ninance scores to 20,000 words. Some examples are shown in Fig. 22.4.\nValence Arousal Dominance\nvacation .840 enraged .962 powerful .991\ndelightful .918 party .840 authority .935\nwhistle .653 organized .337 saxophone .482\nconsolation .408 effortless .120 discouraged .0090\ntorture .115 napping .046 weak .045\nFigure 22.4 Values of sample words on the emotional dimensions of Mohammad (2018a).\nThe NRC Word-Emotion Association Lexicon, also called EmoLex (Moham- EmoLex\nmad and Turney, 2013), uses the Plutchik (1980) 8 basic emotions de\ufb01ned above.\nThe lexicon includes around 14,000 words including words from prior lexicons as",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 709,
      "token_count": 783,
      "chapter_title": ""
    }
  },
  {
    "content": "MPQA Subjectivity lexicon (Wilson et al., 2005), and the polarity lexicon of Hu and Liu (2004b).\nSlightly more general than these sentiment lexicons are lexicons that assign each\nword a value on all three affective dimensions. The NRC Valence, Arousal, and\nDominance (V AD) lexicon (Mohammad, 2018a) assigns valence, arousal, and dom-\ninance scores to 20,000 words. Some examples are shown in Fig. 22.4.\nValence Arousal Dominance\nvacation .840 enraged .962 powerful .991\ndelightful .918 party .840 authority .935\nwhistle .653 organized .337 saxophone .482\nconsolation .408 effortless .120 discouraged .0090\ntorture .115 napping .046 weak .045\nFigure 22.4 Values of sample words on the emotional dimensions of Mohammad (2018a).\nThe NRC Word-Emotion Association Lexicon, also called EmoLex (Moham- EmoLex\nmad and Turney, 2013), uses the Plutchik (1980) 8 basic emotions de\ufb01ned above.\nThe lexicon includes around 14,000 words including words from prior lexicons as\nwell as frequent nouns, verbs, adverbs and adjectives. Values from the lexicon for\nsome sample words:",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 710,
      "token_count": 296,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 493\n\n22.3 \u2022 C REATING AFFECT LEXICONS BY HUMAN LABELING 485\nWord\nanger\nanticipation\ndisgust\nfear\njoy\nsadness\nsurprise\ntrust\npositive\nnegative\nreward 0100101110\nworry 0101010001\ntenderness 0000100010\nsweetheart 0100110110\nsuddenly 0000001000\nthirst 0100011000\ngarbage 0010000001\nFor a smaller set of 5,814 words, the NRC Emotion/Affect Intensity Lexicon\n(Mohammad, 2018b) contains real-valued scores of association for anger, fear, joy,\nand sadness; Fig. 22.5 shows examples.\nAnger Fear Joy Sadness\noutraged 0.964 horror 0.923 superb 0.864 sad 0.844\nviolence 0.742 anguish 0.703 cheered 0.773 guilt 0.750\ncoup 0.578 pestilence 0.625 rainbow 0.531 unkind 0.547\noust 0.484 stressed 0.531 gesture 0.387 dif\ufb01culties 0.421\nsuspicious 0.484 failing 0.531 warms 0.391 beggar 0.422\nnurture 0.059 con\ufb01dent 0.094 hardship .031 sing 0.017\nFigure 22.5 Sample emotional intensities for words for anger, fear, joy, and sadness from\nMohammad (2018b).\nLIWC ,Linguistic Inquiry and Word Count , is a widely used set of 73 lex- LIWC\nicons containing over 2300 words (Pennebaker et al., 2007), designed to capture\naspects of lexical meaning relevant for social psychological tasks. In addition to\nsentiment-related lexicons like ones for negative emotion ( bad, weird, hate, prob-\nlem, tough ) and positive emotion ( love, nice, sweet ), LIWC includes lexicons for\ncategories like anger, sadness, cognitive mechanisms, perception, tentative, and in-\nhibition, shown in Fig. 22.6.\nThere are various other hand-built affective lexicons. The General Inquirer in-\ncludes additional lexicons for dimensions like strong vs. weak, active vs. passive,\noverstated vs. understated, as well as lexicons for categories like pleasure, pain,\nvirtue, vice, motivation, and cognitive orientation.\nAnother useful feature for various tasks is the distinction between concrete concrete\nwords like banana orbathrobe andabstract words like belief andalthough . The abstract\nlexicon in Brysbaert et al. (2014) used crowdsourcing to assign a rating from 1 to 5\nof the concreteness of 40,000 words, thus assigning banana ,bathrobe , and bagel 5,\nbelief 1.19, although 1.07, and in between words like brisk a 2.5.\n22.3 Creating Affect Lexicons by Human Labeling\nThe earliest method used to build affect lexicons, and still in common use, is to have\nhumans label each word. This is now most commonly done via crowdsourcing : crowdsourcing\nbreaking the task into small pieces and distributing them to a large number of anno-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 711,
      "token_count": 718,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 494\n\n486 CHAPTER 22 \u2022 L EXICONS FOR SENTIMENT , AFFECT ,AND CONNOTATION\nPositive Negative\nEmotion Emotion Insight Inhibition Family Negate\nappreciat* anger* aware* avoid* brother* aren\u2019t\ncomfort* bore* believe careful* cousin* cannot\ngreat cry decid* hesitat* daughter* didn\u2019t\nhappy despair* feel limit* family neither\ninterest fail* \ufb01gur* oppos* father* never\njoy* fear know prevent* grandf* no\nperfect* griev* knew reluctan* grandm* nobod*\nplease* hate* means safe* husband none\nsafe* panic* notice* stop mom nor\nterri\ufb01c suffers recogni* stubborn* mother nothing\nvalue terrify sense wait niece* nowhere\nwow* violent* think wary wife without\nFigure 22.6 Samples from 5 of the 73 lexical categories in LIWC (Pennebaker et al., 2007).\nThe * means the previous letters are a word pre\ufb01x and all words with that pre\ufb01x are included\nin the category.\ntators. Let\u2019s take a look at some of the methodological choices for two crowdsourced\nemotion lexicons.\nThe NRC Emotion Lexicon (EmoLex) (Mohammad and Turney, 2013), labeled\nemotions in two steps. To ensure that the annotators were judging the correct sense\nof the word, they \ufb01rst answered a multiple-choice synonym question that primed\nthe correct sense of the word (without requiring the annotator to read a potentially\nconfusing sense de\ufb01nition). These were created automatically using the headwords\nassociated with the thesaurus category of the sense in question in the Macquarie\ndictionary and the headwords of 3 random distractor categories. An example:\nWhich word is closest in meaning (most related) to startle?\n\u2022automobile\n\u2022shake\n\u2022honesty\n\u2022entertain\nFor each word (e.g. startle ), the annotator was then asked to rate how associated\nthat word is with each of the 8 emotions ( joy,fear,anger , etc.). The associations\nwere rated on a scale of not,weakly ,moderately , and strongly associated. Outlier\nratings were removed, and then each term was assigned the class chosen by the ma-\njority of the annotators, with ties broken by choosing the stronger intensity, and then\nthe 4 levels were mapped into a binary label for each word (no and weak mapped to\n0, moderate and strong mapped to 1).\nThe NRC V AD Lexicon (Mohammad, 2018a) was built by selecting words and\nemoticons from prior lexicons and annotating them with crowd-sourcing using best-\nworst scaling (Louviere et al. 2015, Kiritchenko and Mohammad 2017). In best-best-worst\nscaling\nworst scaling, annotators are given N items (usually 4) and are asked which item is\nthebest (highest) and which is the worst (lowest) in terms of some property. The\nset of words used to describe the ends of the scales are taken from prior literature.\nFor valence, for example, the raters were asked:\nQ1. Which of the four words below is associated with the MOST happi-\nness / pleasure / positiveness / satisfaction / contentedness / hopefulness\nOR LEAST unhappiness / annoyance / negativeness / dissatisfaction /",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 712,
      "token_count": 745,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 495\n\n22.4 \u2022 S EMI-SUPERVISED INDUCTION OF AFFECT LEXICONS 487\nmelancholy / despair? (Four words listed as options.)\nQ2. Which of the four words below is associated with the LEAST hap-\npiness / pleasure / positiveness / satisfaction / contentedness / hopeful-\nness OR MOST unhappiness / annoyance / negativeness / dissatisfaction\n/ melancholy / despair? (Four words listed as options.)\nThe score for each word in the lexicon is the proportion of times the item was chosen\nas the best (highest V/A/D) minus the proportion of times the item was chosen as the\nworst (lowest V/A/D). The agreement between annotations are evaluated by split-\nhalf reliability : split the corpus in half and compute the correlations between thesplit-half\nreliability\nannotations in the two halves.\n22.4 Semi-supervised Induction of Affect Lexicons\nAnother common way to learn sentiment lexicons is to start from a set of seed words\nthat de\ufb01ne two poles of a semantic axis (words like good orbad), and then \ufb01nd ways\nto label each word wby its similarity to the two seed sets. Here we summarize two\nfamilies of seed-based semi-supervised lexicon induction algorithms, axis-based and\ngraph-based.\n22.4.1 Semantic Axis Methods\nOne of the most well-known lexicon induction methods, the Turney and Littman\n(2003) algorithm, is given seed words like good orbad, and then for each word wto\nbe labeled, measures both how similar it is to good and how different it is from bad.\nHere we describe a slight extension of the algorithm due to An et al. (2018), which\nis based on computing a semantic axis .\nIn the \ufb01rst step, we choose seed words by hand. There are two methods for\ndealing with the fact that the affect of a word is different in different contexts: (1)\nstart with a single large seed lexicon and rely on the induction algorithm to \ufb01ne-tune\nit to the domain, or (2) choose different seed words for different genres. Hellrich\net al. (2019) suggests that for modeling affect across different historical time periods,\nstarting with a large modern affect dictionary is better than small seedsets tuned to be\nstable across time. As an example of the second approach, Hamilton et al. (2016a)\nde\ufb01ne one set of seed words for general sentiment analysis, a different set for Twitter,\nand yet another set for sentiment in \ufb01nancial text:\nDomain Positive seeds Negative seeds\nGeneral good, lovely, excellent, fortunate, pleas-\nant, delightful, perfect, loved, love,\nhappybad, horrible, poor, unfortunate, un-\npleasant, disgusting, evil, hated, hate,\nunhappy\nTwitter love, loved, loves, awesome, nice,\namazing, best, fantastic, correct, happyhate, hated, hates, terrible, nasty, awful,\nworst, horrible, wrong, sad\nFinance successful, excellent, pro\ufb01t, bene\ufb01cial,\nimproving, improved, success, gains,\npositivenegligent, loss, volatile, wrong, losses,\ndamages, bad, litigation, failure, down,\nnegative\nIn the second step, we compute embeddings for each of the pole words. These\nembeddings can be off-the-shelf word2vec embeddings, or can be computed directly",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 713,
      "token_count": 732,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 496\n\n488 CHAPTER 22 \u2022 L EXICONS FOR SENTIMENT , AFFECT ,AND CONNOTATION\non a speci\ufb01c corpus (for example using a \ufb01nancial corpus if a \ufb01nance lexicon is the\ngoal), or we can \ufb01ne-tune off-the-shelf embeddings to a corpus. Fine-tuning is espe-\ncially important if we have a very speci\ufb01c genre of text but don\u2019t have enough data\nto train good embeddings. In \ufb01ne-tuning, we begin with off-the-shelf embeddings\nlike word2vec, and continue training them on the small target corpus.\nOnce we have embeddings for each pole word, we create an embedding that\nrepresents each pole by taking the centroid of the embeddings of each of the seed\nwords; recall that the centroid is the multidimensional version of the mean. Given\na set of embeddings for the positive seed words S+=fE(w+\n1);E(w+\n2);:::;E(w+\nn)g,\nand embeddings for the negative seed words S\u0000=fE(w\u0000\n1);E(w\u0000\n2);:::;E(w\u0000\nm)g, the\npole centroids are:\nV+=1\nnnX\n1E(w+\ni)\nV\u0000=1\nmmX\n1E(w\u0000\ni) (22.1)\nThe semantic axis de\ufb01ned by the poles is computed just by subtracting the two vec-\ntors:\nVaxis=V+\u0000V\u0000(22.2)\nVaxis, the semantic axis, is a vector in the direction of positive sentiment. Finally,\nwe compute (via cosine similarity) the angle between the vector in the direction of\npositive sentiment and the direction of w\u2019s embedding. A higher cosine means that\nwis more aligned with S+than S\u0000.\nscore(w) = cos\u0000\nE(w);Vaxis\u0001\n=E(w)\u0001Vaxis\nkE(w)kkVaxisk(22.3)\nIf a dictionary of words with sentiment scores is suf\ufb01cient, we\u2019re done! Or if we\nneed to group words into a positive and a negative lexicon, we can use a threshold\nor other method to give us discrete lexicons.\n22.4.2 Label Propagation\nAn alternative family of methods de\ufb01nes lexicons by propagating sentiment labels\non graphs, an idea suggested in early work by Hatzivassiloglou and McKeown\n(1997). We\u2019ll describe the simple SentProp (Sentiment Propagation) algorithm of\nHamilton et al. (2016a), which has four steps:\n1.De\ufb01ne a graph : Given word embeddings, build a weighted lexical graph by\nconnecting each word with its knearest neighbors (according to cosine simi-\nlarity). The weights of the edge between words wiandwjare set as:\nEi;j=arccos\u0012\n\u0000wi>wj\nkwikkwjk\u0013\n: (22.4)\n2.De\ufb01ne a seed set: Choose positive and negative seed words.\n3.Propagate polarities from the seed set: Now we perform a random walk on\nthis graph, starting at the seed set. In a random walk, we start at a node and",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 714,
      "token_count": 689,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 497\n\n22.4 \u2022 S EMI-SUPERVISED INDUCTION OF AFFECT LEXICONS 489\nthen choose a node to move to with probability proportional to the edge prob-\nability. A word\u2019s polarity score for a seed set is proportional to the probability\nof a random walk from the seed set landing on that word (Fig. 22.7).\n4.Create word scores : We walk from both positive and negative seed sets,\nresulting in positive (rawscore+(wi)) and negative (rawscore\u0000(wi)) raw label\nscores. We then combine these values into a positive-polarity score as:\nscore+(wi) =rawscore+(wi)\nrawscore+(wi)+rawscore\u0000(wi)(22.5)\nIt\u2019s often helpful to standardize the scores to have zero mean and unit variance\nwithin a corpus.\n5.Assign con\ufb01dence to each score: Because sentiment scores are in\ufb02uenced by\nthe seed set, we\u2019d like to know how much the score of a word would change if\na different seed set is used. We can use bootstrap sampling to get con\ufb01dence\nregions, by computing the propagation Btimes over random subsets of the\npositive and negative seed sets (for example using B=50 and choosing 7 of\nthe 10 seed words each time). The standard deviation of the bootstrap sampled\npolarity scores gives a con\ufb01dence measure.\nidolize\nlove\nadore\nappreciate\nlike\n\ufb01nd\ndislike\nsee\nnotice\ndisapprove\nabhor\nhate\nloathe\ndespise\nuncover\nidolize\nlove\nadore\nappreciate\nlike\n\ufb01nd\ndislike\nsee\nnotice\ndisapprove\nabhor\nhate\nloathe\ndespise\nuncover\n(a) (b)\nFigure 22.7 Intuition of the S ENTPROP algorithm. (a) Run random walks from the seed words. (b) Assign\npolarity scores (shown here as colors green or red) based on the frequency of random walk visits.\n22.4.3 Other Methods\nThe core of semisupervised algorithms is the metric for measuring similarity with\nthe seed words. The Turney and Littman (2003) and Hamilton et al. (2016a) ap-\nproaches above used embedding cosine as the distance metric: words were labeled\nas positive basically if their embeddings had high cosines with positive seeds and\nlow cosines with negative seeds. Other methods have chosen other kinds of distance\nmetrics besides embedding cosine.\nFor example the Hatzivassiloglou and McKeown (1997) algorithm uses syntactic\ncues; two adjectives are considered similar if they were frequently conjoined by and\nand rarely conjoined by but. This is based on the intuition that adjectives conjoined\nby the words andtend to have the same polarity; positive adjectives are generally\ncoordinated with positive, negative with negative:\nfair and legitimate, corrupt and brutal\nbut less often positive adjectives coordinated with negative:\n*fair and brutal, *corrupt and legitimate\nBy contrast, adjectives conjoined by butare likely to be of opposite polarity:",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 715,
      "token_count": 684,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 498\n\n490 CHAPTER 22 \u2022 L EXICONS FOR SENTIMENT , AFFECT ,AND CONNOTATION\nfair but brutal\nAnother cue to opposite polarity comes from morphological negation ( un-,im-,\n-less). Adjectives with the same root but differing in a morphological negative ( ad-\nequate/inadequate ,thoughtful/thoughtless ) tend to be of opposite polarity.\nYet another method for \ufb01nding words that have a similar polarity to seed words is\nto make use of a thesaurus like WordNet (Kim and Hovy 2004, Hu and Liu 2004b).\nA word\u2019s synonyms presumably share its polarity while a word\u2019s antonyms probably\nhave the opposite polarity. After a seed lexicon is built, each lexicon is updated as\nfollows, possibly iterated.\nLex+: Add synonyms of positive words ( well) and antonyms (like \ufb01ne) of negative\nwords\nLex\u0000: Add synonyms of negative words ( awful ) and antonyms (like evil) of positive\nwords\nAn extension of this algorithm assigns polarity to WordNet senses, called Senti-\nWordNet (Baccianella et al., 2010). Fig. 22.8 shows some examples. SentiWordNet\nSynset Pos Neg Obj\ngood#6 \u2018agreeable or pleasing\u2019 1 0 0\nrespectable#2 honorable#4 good#4 estimable#2 \u2018deserving of esteem\u2019 0.75 0 0.25\nestimable#3 computable#1 \u2018may be computed or estimated\u2019 0 0 1\nsting#1 burn#4 bite#2 \u2018cause a sharp or stinging pain\u2019 0 0.875 .125\nacute#6 \u2018of critical importance and consequence\u2019 0.625 0.125 .250\nacute#4 \u2018of an angle; less than 90 degrees\u2019 0 0 1\nacute#1 \u2018having or experiencing a rapid onset and short but severe course\u2019 0 0.5 0.5\nFigure 22.8 Examples from SentiWordNet 3.0 (Baccianella et al., 2010). Note the differences between senses\nof homonymous words: estimable#3 is purely objective, while estimable#2 is positive; acute can be positive\n(acute#6 ), negative ( acute#1 ), or neutral ( acute #4 ).\nIn this algorithm, polarity is assigned to entire synsets rather than words. A\npositive lexicon is built from all the synsets associated with 7 positive words, and a\nnegative lexicon from synsets associated with 7 negative words. A classi\ufb01er is then\ntrained from this data to take a WordNet gloss and decide if the sense being de\ufb01ned\nis positive, negative or neutral. A further step (involving a random-walk algorithm)\nassigns a score to each WordNet synset for its degree of positivity, negativity, and\nneutrality.\nIn summary, semisupervised algorithms use a human-de\ufb01ned set of seed words\nfor the two poles of a dimension, and use similarity metrics like embedding cosine,\ncoordination, morphology, or thesaurus structure to score words by how similar they\nare to the positive seeds and how dissimilar to the negative seeds.\n22.5 Supervised Learning of Word Sentiment\nSemi-supervised methods require only minimal human supervision (in the form of\nseed sets). But sometimes a supervision signal exists in the world and can be made\nuse of. One such signal is the scores associated with online reviews .\nThe web contains an enormous number of online reviews for restaurants, movies,\nbooks, or other products, each of which have the text of the review along with an",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 716,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 499",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 717,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "22.5 \u2022 S UPERVISED LEARNING OF WORD SENTIMENT 491\nMovie review excerpts (IMDb)\n10A great movie. This \ufb01lm is just a wonderful experience. It\u2019s surreal, zany, witty and slapstick\nall at the same time. And terri\ufb01c performances too.\n1This was probably the worst movie I have ever seen. The story went nowhere even though they\ncould have done some interesting stuff with it.\nRestaurant review excerpts (Yelp)\n5The service was impeccable. The food was cooked and seasoned perfectly... The watermelon\nwas perfectly square ... The grilled octopus was ... mouthwatering...\n2...it took a while to get our waters, we got our entree before our starter, and we never received\nsilverware or napkins until we requested them...\nBook review excerpts (GoodReads)\n1I am going to try and stop being deceived by eye-catching titles. I so wanted to like this book\nand was so disappointed by it.\n5This book is hilarious. I would recommend it to anyone looking for a satirical read with a\nromantic twist and a narrator that keeps butting in\nProduct review excerpts (Amazon)\n5The lid on this blender though is probably what I like the best about it... enables you to pour\ninto something without even taking the lid off! ... the perfect pitcher! ... works fantastic.\n1I hate this blender... It is nearly impossible to get frozen fruit and ice to turn into a smoothie...\nYou have to add a TON of liquid. I also wish it had a spout ...\nFigure 22.9 Excerpts from some reviews from various review websites, all on a scale of 1 to 5 stars except\nIMDb, which is on a scale of 1 to 10 stars.\nassociated review score: a value that may range from 1 star to 5 stars, or scoring 1\nto 10. Fig. 22.9 shows samples extracted from restaurant, book, and movie reviews.\nWe can use this review score as supervision: positive words are more likely to\nappear in 5-star reviews; negative words in 1-star reviews. And instead of just a\nbinary polarity, this kind of supervision allows us to assign a word a more complex\nrepresentation of its polarity: its distribution over stars (or other scores).\nThus in a ten-star system we could represent the sentiment of each word as a\n10-tuple, each number a score representing the word\u2019s association with that polarity\nlevel. This association can be a raw count, or a likelihood P(wjc), or some other\nfunction of the count, for each class cfrom 1 to 10.\nFor example, we could compute the IMDb likelihood of a word like disap-\npoint(ed/ing) occurring in a 1 star review by dividing the number of times disap-\npoint(ed/ing) occurs in 1-star reviews in the IMDb dataset (8,557) by the total num-\nber of words occurring in 1-star reviews (25,395,214), so the IMDb estimate of\nP(disappointingj1)is .0003.\nA slight modi\ufb01cation of this weighting, the normalized likelihood, can be used\nas an illuminating visualization (Potts, 2011)1\nP(wjc) =count (w;c)P\nw2Ccount (w;c)\nPottsScore (w) =P(wjc)P\ncP(wjc)(22.6)\nDividing the IMDb estimate P(disappointingj1)of .0003 by the sum of the likeli-\nhood P(wjc)over all categories gives a Potts score of 0.10. The word disappointing",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 718,
      "token_count": 774,
      "chapter_title": ""
    }
  },
  {
    "content": "Thus in a ten-star system we could represent the sentiment of each word as a\n10-tuple, each number a score representing the word\u2019s association with that polarity\nlevel. This association can be a raw count, or a likelihood P(wjc), or some other\nfunction of the count, for each class cfrom 1 to 10.\nFor example, we could compute the IMDb likelihood of a word like disap-\npoint(ed/ing) occurring in a 1 star review by dividing the number of times disap-\npoint(ed/ing) occurs in 1-star reviews in the IMDb dataset (8,557) by the total num-\nber of words occurring in 1-star reviews (25,395,214), so the IMDb estimate of\nP(disappointingj1)is .0003.\nA slight modi\ufb01cation of this weighting, the normalized likelihood, can be used\nas an illuminating visualization (Potts, 2011)1\nP(wjc) =count (w;c)P\nw2Ccount (w;c)\nPottsScore (w) =P(wjc)P\ncP(wjc)(22.6)\nDividing the IMDb estimate P(disappointingj1)of .0003 by the sum of the likeli-\nhood P(wjc)over all categories gives a Potts score of 0.10. The word disappointing\nthus is associated with the vector [.10, .12, .14, .14, .13, .11, .08, .06, .06, .05]. The\n1Each element of the Potts score of a word wand category ccan be shown to be a variant of the\npointwise mutual information pmi (w;c)without the log term; see Exercise 22.1.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 719,
      "token_count": 372,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 500",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 720,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "492 CHAPTER 22 \u2022 L EXICONS FOR SENTIMENT , AFFECT ,AND CONNOTATION\nPotts diagram (Potts, 2011) is a visualization of these word scores, representing the Potts diagram\nprior sentiment of a word as a distribution over the rating categories.\nFig. 22.10 shows the Potts diagrams for 3 positive and 3 negative scalar adjec-\ntives. Note that the curve for strongly positive scalars have the shape of the letter\nJ, while strongly negative scalars look like a reverse J. By contrast, weakly posi-\ntive and negative scalars have a hump-shape, with the maximum either below the\nmean (weakly negative words like disappointing ) or above the mean (weakly pos-\nitive words like good ). These shapes offer an illuminating typology of affective\nmeaning.\nOverviewDataMethodsCategorizationScale inductionLooking aheadExample: attenuators\nIMDB \u2013 53,775 tokens\nCategory-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.050.090.15Cat = 0.33 (p = 0.004)Cat^2 = -4.02 (p < 0.001)OpenTable \u2013 3,890 tokens\nCategory-0.50-0.250.000.250.500.080.38Cat = 0.11 (p = 0.707)Cat^2 = -6.2 (p = 0.014)Goodreads \u2013 3,424 tokens\nCategory-0.50-0.250.000.250.500.080.190.36Cat = -0.55 (p = 0.128)Cat^2 = -5.04 (p = 0.016)Amazon/Tripadvisor \u2013 2,060 tokens\nCategory-0.50-0.250.000.250.500.120.28Cat = 0.42 (p = 0.207)Cat^2 = -2.74 (p = 0.05)somewhat/r\nIMDB \u2013 33,515 tokens\nCategory-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.040.090.17Cat = -0.13 (p = 0.284)Cat^2 = -5.37 (p < 0.001)OpenTable \u2013 2,829 tokens\nCategory-0.50-0.250.000.250.500.080.31Cat = 0.2 (p = 0.265)Cat^2 = -4.16 (p = 0.007)Goodreads \u2013 1,806 tokens\nCategory-0.50-0.250.000.250.500.050.120.180.35Cat = -0.87 (p = 0.016)Cat^2 = -5.74 (p = 0.004)Amazon/Tripadvisor \u2013 2,158 tokens\nCategory-0.50-0.250.000.250.500.110.29Cat = 0.54 (p = 0.183)Cat^2 = -3.32 (p = 0.045)fairly/r\nIMDB \u2013 176,264 tokens",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 721,
      "token_count": 719,
      "chapter_title": ""
    }
  },
  {
    "content": "IMDB \u2013 33,515 tokens\nCategory-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.040.090.17Cat = -0.13 (p = 0.284)Cat^2 = -5.37 (p < 0.001)OpenTable \u2013 2,829 tokens\nCategory-0.50-0.250.000.250.500.080.31Cat = 0.2 (p = 0.265)Cat^2 = -4.16 (p = 0.007)Goodreads \u2013 1,806 tokens\nCategory-0.50-0.250.000.250.500.050.120.180.35Cat = -0.87 (p = 0.016)Cat^2 = -5.74 (p = 0.004)Amazon/Tripadvisor \u2013 2,158 tokens\nCategory-0.50-0.250.000.250.500.110.29Cat = 0.54 (p = 0.183)Cat^2 = -3.32 (p = 0.045)fairly/r\nIMDB \u2013 176,264 tokens\nCategory-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.050.090.13Cat = -0.43 (p < 0.001)Cat^2 = -3.6 (p < 0.001)OpenTable \u2013 8,982 tokens\nCategory-0.50-0.250.000.250.500.080.140.190.32Cat = -0.64 (p = 0.035)Cat^2 = -4.47 (p = 0.007)Goodreads \u2013 11,895 tokens\nCategory-0.50-0.250.000.250.500.070.150.34Cat = -0.71 (p = 0.072)Cat^2 = -4.59 (p = 0.018)Amazon/Tripadvisor \u2013 5,980 tokens",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 722,
      "token_count": 468,
      "chapter_title": ""
    }
  },
  {
    "content": "Category-0.50-0.250.000.250.500.110.29Cat = 0.54 (p = 0.183)Cat^2 = -3.32 (p = 0.045)fairly/r\nIMDB \u2013 176,264 tokens\nCategory-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.050.090.13Cat = -0.43 (p < 0.001)Cat^2 = -3.6 (p < 0.001)OpenTable \u2013 8,982 tokens\nCategory-0.50-0.250.000.250.500.080.140.190.32Cat = -0.64 (p = 0.035)Cat^2 = -4.47 (p = 0.007)Goodreads \u2013 11,895 tokens\nCategory-0.50-0.250.000.250.500.070.150.34Cat = -0.71 (p = 0.072)Cat^2 = -4.59 (p = 0.018)Amazon/Tripadvisor \u2013 5,980 tokens\nCategory-0.50-0.250.000.250.500.150.28Cat = 0.26 (p = 0.496)Cat^2 = -2.23 (p = 0.131)pretty/r\u201cPotts&diagrams\u201dPotts,&Christopher .& 2011.&NSF&workshop&on&restructuring& adjectives.goodgreatexcellentdisappointingbadterribletotallyabsolutelyutterlysomewhatfairlyprettyPositive scalarsNegative scalarsEmphaticsAttenuators1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating\nFigure 22.10 Potts diagrams (Potts, 2011) for positive and negative scalar adjectives, show-\ning the J-shape and reverse J-shape for strongly positive and negative adjectives, and the\nhump-shape for more weakly polarized adjectives.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 723,
      "token_count": 785,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 22.10 Potts diagrams (Potts, 2011) for positive and negative scalar adjectives, show-\ning the J-shape and reverse J-shape for strongly positive and negative adjectives, and the\nhump-shape for more weakly polarized adjectives.\nFig. 22.11 shows the Potts diagrams for emphasizing and attenuating adverbs.\nNote that emphatics tend to have a J-shape (most likely to occur in the most posi-\ntive reviews) or a U-shape (most likely to occur in the strongly positive and nega-\ntive). Attenuators all have the hump-shape, emphasizing the middle of the scale and\ndownplaying both extremes. The diagrams can be used both as a typology of lexical\nsentiment, and also play a role in modeling sentiment compositionality.\nIn addition to functions like posterior P(cjw), likelihood P(wjc), or normalized\nlikelihood (Eq. 22.6) many other functions of the count of a word occurring with a\nsentiment label have been used. We\u2019ll introduce some of these on page 496, includ-\ning ideas like normalizing the counts per writer in Eq. 22.14.\n22.5.1 Log Odds Ratio Informative Dirichlet Prior\nOne thing we often want to do with word polarity is to distinguish between words\nthat are more likely to be used in one category of texts than in another. We may, for\nexample, want to know the words most associated with 1 star reviews versus those\nassociated with 5 star reviews. These differences may not be just related to senti-\nment. We might want to \ufb01nd words used more often by Democratic than Republican\nmembers of Congress, or words used more often in menus of expensive restaurants",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 724,
      "token_count": 373,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 501",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 725,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "22.5 \u2022 S UPERVISED LEARNING OF WORD SENTIMENT 493\nOverviewDataMethodsCategorizationScale inductionLooking aheadExample: attenuators\nIMDB \u2013 53,775 tokens\nCategory-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.050.090.15Cat = 0.33 (p = 0.004)Cat^2 = -4.02 (p < 0.001)OpenTable \u2013 3,890 tokens\nCategory-0.50-0.250.000.250.500.080.38Cat = 0.11 (p = 0.707)Cat^2 = -6.2 (p = 0.014)Goodreads \u2013 3,424 tokens\nCategory-0.50-0.250.000.250.500.080.190.36Cat = -0.55 (p = 0.128)Cat^2 = -5.04 (p = 0.016)Amazon/Tripadvisor \u2013 2,060 tokens\nCategory-0.50-0.250.000.250.500.120.28Cat = 0.42 (p = 0.207)Cat^2 = -2.74 (p = 0.05)somewhat/r\nIMDB \u2013 33,515 tokens\nCategory-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.040.090.17Cat = -0.13 (p = 0.284)Cat^2 = -5.37 (p < 0.001)OpenTable \u2013 2,829 tokens\nCategory-0.50-0.250.000.250.500.080.31Cat = 0.2 (p = 0.265)Cat^2 = -4.16 (p = 0.007)Goodreads \u2013 1,806 tokens\nCategory-0.50-0.250.000.250.500.050.120.180.35Cat = -0.87 (p = 0.016)Cat^2 = -5.74 (p = 0.004)Amazon/Tripadvisor \u2013 2,158 tokens\nCategory-0.50-0.250.000.250.500.110.29Cat = 0.54 (p = 0.183)Cat^2 = -3.32 (p = 0.045)fairly/r\nIMDB \u2013 176,264 tokens\nCategory-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.050.090.13Cat = -0.43 (p < 0.001)Cat^2 = -3.6 (p < 0.001)OpenTable \u2013 8,982 tokens\nCategory-0.50-0.250.000.250.500.080.140.190.32Cat = -0.64 (p = 0.035)Cat^2 = -4.47 (p = 0.007)Goodreads \u2013 11,895 tokens\nCategory-0.50-0.250.000.250.500.070.150.34Cat = -0.71 (p = 0.072)Cat^2 = -4.59 (p = 0.018)Amazon/Tripadvisor \u2013 5,980 tokens",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 726,
      "token_count": 759,
      "chapter_title": ""
    }
  },
  {
    "content": "Category-0.50-0.250.000.250.500.110.29Cat = 0.54 (p = 0.183)Cat^2 = -3.32 (p = 0.045)fairly/r\nIMDB \u2013 176,264 tokens\nCategory-0.50-0.39-0.28-0.17-0.060.060.170.280.390.500.050.090.13Cat = -0.43 (p < 0.001)Cat^2 = -3.6 (p < 0.001)OpenTable \u2013 8,982 tokens\nCategory-0.50-0.250.000.250.500.080.140.190.32Cat = -0.64 (p = 0.035)Cat^2 = -4.47 (p = 0.007)Goodreads \u2013 11,895 tokens\nCategory-0.50-0.250.000.250.500.070.150.34Cat = -0.71 (p = 0.072)Cat^2 = -4.59 (p = 0.018)Amazon/Tripadvisor \u2013 5,980 tokens\nCategory-0.50-0.250.000.250.500.150.28Cat = 0.26 (p = 0.496)Cat^2 = -2.23 (p = 0.131)pretty/r\u201cPotts&diagrams\u201dPotts,&Christopher .& 2011.&NSF&workshop&on&restructuring& adjectives.goodgreatexcellentdisappointingbadterribletotallyabsolutelyutterlysomewhatfairlyprettyPositive scalarsNegative scalarsEmphaticsAttenuators1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating1  2  3  4  5  6  7  8  9  10rating\nFigure 22.11 Potts diagrams (Potts, 2011) for emphatic and attenuating adverbs.\nthan cheap restaurants.\nGiven two classes of documents, to \ufb01nd words more associated with one cate-\ngory than another, we could measure the difference in frequencies (is a word wmore",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 727,
      "token_count": 789,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 22.11 Potts diagrams (Potts, 2011) for emphatic and attenuating adverbs.\nthan cheap restaurants.\nGiven two classes of documents, to \ufb01nd words more associated with one cate-\ngory than another, we could measure the difference in frequencies (is a word wmore\nfrequent in class Aor class B?). Or instead of the difference in frequencies we could\ncompute the ratio of frequencies, or compute the log odds ratio (the log of the ratio\nbetween the odds of the two words). We could then sort words by whichever associ-\nation measure we pick, ranging from words overrepresented in category Ato words\noverrepresented in category B.\nThe problem with simple log-likelihood or log odds methods is that they overem-\nphasize differences in very rare words, and often also in very frequent words. Very\nrare words will seem to occur very differently in the two corpora since with tiny\ncounts there may be statistical \ufb02uctations, or even zero occurrences in one corpus\ncompared to non-zero occurrences in the other. Very frequent words will also seem\ndifferent since all counts are large.\nIn this section we walk through the details of one solution to this problem: the\n\u201clog odds ratio informative Dirichlet prior\u201d method of Monroe et al. (2008) that is a\nparticularly useful method for \ufb01nding words that are statistically overrepresented in\none particular category of texts compared to another. It\u2019s based on the idea of using\nanother large corpus to get a prior estimate of what we expect the frequency of each\nword to be.\nLet\u2019s start with the goal: assume we want to know whether the word horrible\noccurs more in corpus ior corpus j. We could compute the log likelihood ratio ,log likelihood\nratio\nusing fi(w)to mean the frequency of word win corpus i, and nito mean the total\nnumber of words in corpus i:\nllr(horrible ) = logPi(horrible )\nPj(horrible )\n=logPi(horrible )\u0000logPj(horrible )\n=logfi(horrible )\nni\u0000logfj(horrible )\nnj(22.7)\nInstead, let\u2019s compute the log odds ratio : does horrible have higher odds in ior in log odds ratio",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 728,
      "token_count": 478,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 502",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 729,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "494 CHAPTER 22 \u2022 L EXICONS FOR SENTIMENT , AFFECT ,AND CONNOTATION\nj:\nlor(horrible ) = log\u0012Pi(horrible )\n1\u0000Pi(horrible )\u0013\n\u0000log\u0012Pj(horrible )\n1\u0000Pj(horrible )\u0013\n=log0\nB@fi(horrible )\nni\n1\u0000fi(horrible )\nni1\nCA\u0000log0\nB@fj(horrible )\nnj\n1\u0000fj(horrible )\nnj1\nCA\n=log\u0012fi(horrible )\nni\u0000fi(horrible )\u0013\n\u0000log\u0012fj(horrible )\nnj\u0000fj(horrible )\u0013\n(22.8)\nThe Dirichlet intuition is to use a large background corpus to get a prior estimate of\nwhat we expect the frequency of each word wto be. We\u2019ll do this very simply by\nadding the counts from that corpus to the numerator and denominator, so that we\u2019re\nessentially shrinking the counts toward that prior. It\u2019s like asking how large are the\ndifferences between iand jgiven what we would expect given their frequencies in\na well-estimated large background corpus.\nThe method estimates the difference between the frequency of word win two\ncorpora iandjvia the prior-modi\ufb01ed log odds ratio for w,d(i\u0000j)\nw, which is estimated\nas:\nd(i\u0000j)\nw=log\u0012fi\nw+aw\nni+a0\u0000(fiw+aw)\u0013\n\u0000log \nfj\nw+aw\nnj+a0\u0000(fj\nw+aw)!\n(22.9)\n(where niis the size of corpus i,njis the size of corpus j,fi\nwis the count of word\nwin corpus i,fj\nwis the count of word win corpus j,a0is the scaled size of the\nbackground corpus, and awis the scaled count of word win the background corpus.)\nIn addition, Monroe et al. (2008) make use of an estimate for the variance of the\nlog\u2013odds\u2013ratio:\ns2\u0010\n\u02c6d(i\u0000j)\nw\u0011\n\u00191\nfiw+aw+1\nfj\nw+aw(22.10)\nThe \ufb01nal statistic for a word is then the z\u2013score of its log\u2013odds\u2013ratio:\n\u02c6d(i\u0000j)\nwr\ns2\u0010\n\u02c6d(i\u0000j)\nw\u0011(22.11)\nThe Monroe et al. (2008) method thus modi\ufb01es the commonly used log odds ratio\nin two ways: it uses the z-scores of the log odds ratio, which controls for the amount\nof variance in a word\u2019s frequency, and it uses counts from a background corpus to\nprovide a prior count for words.\nFig. 22.12 shows the method applied to a dataset of restaurant reviews from\nYelp, comparing the words used in 1-star reviews to the words used in 5-star reviews\n(Jurafsky et al., 2014). The largest difference is in obvious sentiment words, with the\n1-star reviews using negative sentiment words like worse, bad, awful and the 5-star\nreviews using positive sentiment words like great, best, amazing . But there are other\nilluminating differences. 1-star reviews use logical negation ( no, not ), while 5-star\nreviews use emphatics and emphasize universality ( very, highly, every, always ). 1-\nstar reviews use \ufb01rst person plurals ( we, us, our ) while 5 star reviews use the second",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 730,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "The \ufb01nal statistic for a word is then the z\u2013score of its log\u2013odds\u2013ratio:\n\u02c6d(i\u0000j)\nwr\ns2\u0010\n\u02c6d(i\u0000j)\nw\u0011(22.11)\nThe Monroe et al. (2008) method thus modi\ufb01es the commonly used log odds ratio\nin two ways: it uses the z-scores of the log odds ratio, which controls for the amount\nof variance in a word\u2019s frequency, and it uses counts from a background corpus to\nprovide a prior count for words.\nFig. 22.12 shows the method applied to a dataset of restaurant reviews from\nYelp, comparing the words used in 1-star reviews to the words used in 5-star reviews\n(Jurafsky et al., 2014). The largest difference is in obvious sentiment words, with the\n1-star reviews using negative sentiment words like worse, bad, awful and the 5-star\nreviews using positive sentiment words like great, best, amazing . But there are other\nilluminating differences. 1-star reviews use logical negation ( no, not ), while 5-star\nreviews use emphatics and emphasize universality ( very, highly, every, always ). 1-\nstar reviews use \ufb01rst person plurals ( we, us, our ) while 5 star reviews use the second\nperson. 1-star reviews talk about people ( manager, waiter, customer ) while 5-star\nreviews talk about dessert and properties of expensive restaurants like courses and\natmosphere. See Jurafsky et al. (2014) for more details.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 731,
      "token_count": 340,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 503\n\n22.6 \u2022 U SING LEXICONS FOR SENTIMENT RECOGNITION 495\nClass Words in 1-star reviews Class Words in 5-star reviews\nNegative worst, rude, terrible, horrible, bad,\nawful, disgusting, bland, tasteless,\ngross, mediocre, overpriced, worse,\npoorPositive great, best, love(d), delicious, amazing,\nfavorite, perfect, excellent, awesome,\nfriendly, fantastic, fresh, wonderful, in-\ncredible, sweet, yum(my)\nNegation no, not Emphatics/\nuniversalsvery, highly, perfectly, de\ufb01nitely, abso-\nlutely, everything, every, always\n1Pl pro we, us, our 2 pro you\n3 pro she, he, her, him Articles a, the\nPast verb was, were, asked, told, said, did,\ncharged, waited, left, tookAdvice try, recommend\nSequencers after, then Conjunct also, as, well, with, and\nNouns manager, waitress, waiter, customer,\ncustomers, attitude, waste, poisoning,\nmoney, bill, minutesNouns atmosphere, dessert, chocolate, wine,\ncourse, menu\nIrrealis\nmodalswould, should Auxiliaries is/\u2019s, can, \u2019ve, are\nComp to, that Prep, other in, of, die, city, mouth\nFigure 22.12 The top 50 words associated with one\u2013star and \ufb01ve-star restaurant reviews in a Yelp dataset of\n900,000 reviews, using the Monroe et al. (2008) method (Jurafsky et al., 2014).\n22.6 Using Lexicons for Sentiment Recognition\nIn Chapter 4 we introduced the naive Bayes algorithm for sentiment analysis. The\nlexicons we have focused on throughout the chapter so far can be used in a number\nof ways to improve sentiment detection.\nIn the simplest case, lexicons can be used when we don\u2019t have suf\ufb01cient training\ndata to build a supervised sentiment analyzer; it can often be expensive to have a\nhuman assign sentiment to each document to train the supervised classi\ufb01er.\nIn such situations, lexicons can be used in a rule-based algorithm for classi\ufb01ca-\ntion. The simplest version is just to use the ratio of positive to negative words: if a\ndocument has more positive than negative words (using the lexicon to decide the po-\nlarity of each word in the document), it is classi\ufb01ed as positive. Often a threshold l\nis used, in which a document is classi\ufb01ed as positive only if the ratio is greater than\nl. If the sentiment lexicon includes positive and negative weights for each word,\nq+\nwandq\u0000\nw, these can be used as well. Here\u2019s a simple such sentiment algorithm:\nf+=X\nws.t.w2positivelexiconq+\nwcount (w)\nf\u0000=X\nws.t.w2negativelexiconq\u0000\nwcount (w)\nsentiment =8\n>>><\n>>>:+iff+\nf\u0000>l\n\u0000iff\u0000\nf+>l\n0 otherwise.(22.12)\nIf supervised training data is available, these counts computed from sentiment lex-\nicons, sometimes weighted or normalized in various ways, can also be used as fea-\ntures in a classi\ufb01er along with other lexical or non-lexical features. We return to\nsuch algorithms in Section 22.7.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 732,
      "token_count": 736,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 504\n\n496 CHAPTER 22 \u2022 L EXICONS FOR SENTIMENT , AFFECT ,AND CONNOTATION\n22.7 Using Lexicons for Affect Recognition\nDetection of emotion (and the other kinds of affective meaning described by Scherer\n(2000)) can be done by generalizing the algorithms described above for detecting\nsentiment.\nThe most common algorithms involve supervised classi\ufb01cation: a training set is\nlabeled for the affective meaning to be detected, and a classi\ufb01er is built using features\nextracted from the training set. As with sentiment analysis, if the training set is large\nenough, and the test set is suf\ufb01ciently similar to the training set, simply using all\nthe words or all the bigrams as features in a powerful classi\ufb01er like SVM or logistic\nregression, as described in Fig. 4.2 in Chapter 4, is an excellent algorithm whose\nperformance is hard to beat. Thus we can treat affective meaning classi\ufb01cation of a\ntext sample as simple document classi\ufb01cation.\nSome modi\ufb01cations are nonetheless often necessary for very large datasets. For\nexample, the Schwartz et al. (2013) study of personality, gender, and age using 700\nmillion words of Facebook posts used only a subset of the n-grams of lengths 1-\n3. Only words and phrases used by at least 1% of the subjects were included as\nfeatures, and 2-grams and 3-grams were only kept if they had suf\ufb01ciently high PMI\n(PMI greater than 2 \u0003length , where length is the number of words):\npmi(phrase ) =logp(phrase )Y\nw2phrasep(w)(22.13)\nVarious weights can be used for the features, including the raw count in the training\nset, or some normalized probability or log probability. Schwartz et al. (2013), for\nexample, turn feature counts into phrase likelihoods by normalizing them by each\nsubject\u2019s total word use.\np(phrasejsubject ) =freq(phrase;subject )X\nphrase02vocab (subject )freq(phrase0;subject )(22.14)\nIf the training data is sparser, or not as similar to the test set, any of the lexicons\nwe\u2019ve discussed can play a helpful role, either alone or in combination with all the\nwords and n-grams.\nMany possible values can be used for lexicon features. The simplest is just an\nindicator function, in which the value of a feature fLtakes the value 1 if a particular\ntext has any word from the relevant lexicon L. Using the notation of Chapter 4, in\nwhich a feature value is de\ufb01ned for a particular output class cand document x.\nfL(c;x) =\u001a1 if9w:w2L&w2x&class =c\n0 otherwise\nAlternatively the value of a feature fLfor a particular lexicon Lcan be the total\nnumber of word tokens in the document that occur in L:\nfL=X\nw2Lcount (w)\nFor lexica in which each word is associated with a score or weight, the count can be\nmultiplied by a weight qL\nw:\nfL=X\nw2LqL\nwcount (w)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 733,
      "token_count": 708,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 505\n\n22.8 \u2022 L EXICON -BASED METHODS FOR ENTITY -CENTRIC AFFECT 497\nCounts can alternatively be logged or normalized per writer as in Eq. 22.14.\nHowever they are de\ufb01ned, these lexicon features are then used in a supervised\nclassi\ufb01er to predict the desired affective category for the text or document. Once\na classi\ufb01er is trained, we can examine which lexicon features are associated with\nwhich classes. For a classi\ufb01er like logistic regression the feature weight gives an\nindication of how associated the feature is with the class.\n22.8 Lexicon-based methods for Entity-Centric Affect\nWhat if we want to get an affect score not for an entire document, but for a particular\nentity in the text? The entity-centric method of Field and Tsvetkov (2019) combines\naffect lexicons with contextual embeddings to assign an affect score to an entity in\ntext. In the context of affect about people, they relabel the Valence/Arousal/Domi-\nnance dimension as Sentiment/Agency/Power. The algorithm \ufb01rst trains classi\ufb01ers\nto map embeddings to scores:\n1. For each word win the training corpus:\n(a) Use off-the-shelf pretrained encoders (like BERT) to extract a contextual\nembedding efor each instance of the word. No additional \ufb01ne-tuning is\ndone.\n(b) Average over the eembeddings of each instance of wto obtain a single\nembedding vector for one training point w.\n(c) Use the NRC V AD Lexicon to get S, A, and P scores for w.\n2. Train (three) regression models on all words wto predict V , A, D scores from\na word\u2019s average embedding.\nNow given an entity mention min a text, we assign affect scores as follows:\n1. Use the same pretrained LM to get contextual embeddings for min context.\n2. Feed this embedding through the 3 regression models to get S, A, P scores for\nthe entity.\nThis results in a (S,A,P) tuple for a given entity mention; To get scores for the rep-\nresentation of an entity in a complete document, we can run coreference resolution\nand average the (S,A,P) scores for all the mentions. Fig. 22.13 shows the scores\nfrom their algorithm for characters from the movie The Dark Knight when run on\nWikipedia plot summary texts with gold coreference.\n22.9 Connotation Frames\nThe lexicons we\u2019ve described so far de\ufb01ne a word as a point in affective space. A\nconnotation frame , by contrast, is a lexicon that incorporates a richer kind of gram-connotation\nframe\nmatical structure, by combining affective lexicons with the frame semantic lexicons\nof Chapter 21. The basic insight of connotation frame lexicons is that a predicate\nlike a verb expresses connotations about the verb\u2019s arguments (Rashkin et al. 2016,\nRashkin et al. 2017).\nConsider sentences like:\n(22.15) Country A violated the sovereignty of Country B",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 734,
      "token_count": 666,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 506",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 735,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "498 CHAPTER 22 \u2022 L EXICONS FOR SENTIMENT , AFFECT ,AND CONNOTATION\nPower ScoreweaklyRachelDentGordanBatmanJokerpowerfully\nSentiment ScorenegativeJokerDentGordanRachelBatmanpositive\nAgency ScoredullDentGordanRachelBatmanJokerscaryFigure 1: Power, sentiment, and agency scores for char-\nacters in The Dark Night as learned through the regres-\nsion model with ELMo embeddings. Scores generally\nalign with character archetypes, i.e. the antagonist has\nthe lowest sentiment score.\nment have resulted in his effective removal from\nthe industry. While articles about the #MeToo\nmovement portray men like Weinstein as unpow-\nerful, we can speculate that the corpora used to\ntrain ELMo and BERT portray them as powerful.\nThus, in a corpus where traditional power roles\nhave been inverted, the embeddings extracted\nfrom ELMo and BERT perform worse than ran-\ndom, as they are biased towards the power struc-\ntures in the data they are trained on. Further ev-\nidence of this exists in the performance of the\nBERT-masked embeddings - whereas these em-\nbeddings generally capture power poorly as com-\npared to the unmasked embeddings (Table 2),\nthey outperform the unmasked embeddings on this\ntask, and even outperform the frequency baseline\nin one setting. Nevertheless, they do not outper-\nform Field et al. (2019 ), likely because they do not\ncapture affect information as well as the unmasked\nembeddings (Table 2).\n4.3 Qualitative Document-level Analysis\nFinally, we qualitatively analyze how well our\nmethod captures affect dimensions by analyzing\nsingle documents in detail. We conduct this anal-\nysis in a domain where we expect entities to ful\ufb01ll\ntraditional power roles and where entity portray-\nals are known. Following Bamman et al. (2013 ),\nwe analyze the Wikipedia plot summary of the\nmovie The Dark Knight ,7focusing on Batman\n(protagonist),8the Joker (antagonist), Jim Gordan\n(law enforcement of\ufb01cer, ally to Batman), Har-\n7http://bit.ly/2XmhRDR\n8We consider Batman/Bruce Wayne to be the same entity.Power Scoreweakly Rachel Joker Dent Gordan Batmanpowerfully\nSentiment Scorenegative Joker Gordan Batman Dent Rachel positive",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 736,
      "token_count": 504,
      "chapter_title": ""
    }
  },
  {
    "content": "tures in the data they are trained on. Further ev-\nidence of this exists in the performance of the\nBERT-masked embeddings - whereas these em-\nbeddings generally capture power poorly as com-\npared to the unmasked embeddings (Table 2),\nthey outperform the unmasked embeddings on this\ntask, and even outperform the frequency baseline\nin one setting. Nevertheless, they do not outper-\nform Field et al. (2019 ), likely because they do not\ncapture affect information as well as the unmasked\nembeddings (Table 2).\n4.3 Qualitative Document-level Analysis\nFinally, we qualitatively analyze how well our\nmethod captures affect dimensions by analyzing\nsingle documents in detail. We conduct this anal-\nysis in a domain where we expect entities to ful\ufb01ll\ntraditional power roles and where entity portray-\nals are known. Following Bamman et al. (2013 ),\nwe analyze the Wikipedia plot summary of the\nmovie The Dark Knight ,7focusing on Batman\n(protagonist),8the Joker (antagonist), Jim Gordan\n(law enforcement of\ufb01cer, ally to Batman), Har-\n7http://bit.ly/2XmhRDR\n8We consider Batman/Bruce Wayne to be the same entity.Power Scoreweakly Rachel Joker Dent Gordan Batmanpowerfully\nSentiment Scorenegative Joker Gordan Batman Dent Rachel positive\nAgency Scoredull Rachel Dent GordanBatman Joker scaryFigure 2: Power, sentiment, and agency scores for char-acters inThe Dark Nightas learned through ASP withELMo embeddings. These scores re\ufb02ect the same pat-terns as the regression model with greater separationbetween characters.vey Dent (ally to Batman who turns evil) andRachel Dawes (primary love interest). To facil-itate extracting example sentences, we score eachinstance of these entities in the narrative separatelyand average across instances to obtain an entityscore for the document.9To maximize our databy capturing every mention of an entity, we per-form co-reference resolution by hand. Addition-ally, based on our results from Table3as well asthe use of Wikipedia data in training the ELMomodel (Peters et al.,2018), we use ELMo embed-dings for our analysis.Figures1and2show results. For refer-ence, we show the entity scores as compared toone polar opposite pair identi\ufb01ed by ASP. Boththe regression model and ASP show similar pat-terns. Batman has high power, while Rachel haslow power. Additionally, the Joker is associatedwith the most negative sentiment, but the high-est agency. Throughout the plot summary, themovie progresses by the Joker taking an aggres-sive action and the other characters responding.We can see this dynamic re\ufb02ected in the Joker\u2019spro\ufb01le score, as a high-powered, high-agency,low-sentiment character, who is the primary plot-driver. In general, ASP shows a greater separationbetween characters than the regression model. Wehypothesize that this occurs because ASP isolatesthe dimensions of interest, while the regression ap-proach captures other confounds, such as that hu-9When we used this averaging metric in other evaluations,we found no signi\ufb01cant change in results. Thus, in other sce-narios, we compute scores over averaged embeddings, ratherthan averaging scores separately computed for each embed-ding to reduce computationally complexity.\nFigure 22.13 Power (dominance), sentiment (valence) and agency (arousal) for characters\nin the movie The Dark Knight computed from embeddings trained on the NRC V AD Lexicon.\nNote the protagonist (Batman) and the antagonist (the Joker) have high power and agency",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 737,
      "token_count": 776,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 22.13 Power (dominance), sentiment (valence) and agency (arousal) for characters\nin the movie The Dark Knight computed from embeddings trained on the NRC V AD Lexicon.\nNote the protagonist (Batman) and the antagonist (the Joker) have high power and agency\nscores but differ in sentiment, while the love interest Rachel has low power and agency but\nhigh sentiment.\n(22.16) the teenager ... survived the Boston Marathon bombing\u201d\nBy using the verb violate in (22.15), the author is expressing their sympathies with\nCountry B, portraying Country B as a victim, and expressing antagonism toward\nthe agent Country A. By contrast, in using the verb survive , the author of (22.16) is\nexpressing that the bombing is a negative experience, and the subject of the sentence,\nthe teenager, is a sympathetic character. These aspects of connotation are inherent\nin the meaning of the verbs violate andsurvive , as shown in Fig. 22.14.\nWriter\nRole1\nRole2Role1 is asympathetic victimThere issome typeof hardship\nReader+_+__S(writer\u2192role1)S(writer\u2192role2)Connotation Frame for \u201cRole1 survives Role2\u201d S(role1\u2192role2)\nWriter\nRole1\nRole2Role1 is the antagonistRole2 is asympathetic victim\nReader+_+__S(writer\u2192role1)S(writer\u2192role2)Connotation Frame for \u201cRole1 violates Role2\u201d S(role1\u2192role2)\n(a) (b)\nFigure 22.14 Connotation frames for survive andviolate . (a) For survive , the writer and reader have positive\nsentiment toward Role1, the subject, and negative sentiment toward Role2, the direct object. (b) For violate , the\nwriter and reader have positive sentiment instead toward Role2, the direct object.\nThe connotation frame lexicons of Rashkin et al. (2016) and Rashkin et al.\n(2017) also express other connotative aspects of the predicate toward each argu-\nment, including the effect (something bad happened to x) value : (x is valuable), and\nmental state : (x is distressed by the event). Connotation frames can also mark the\npower differential between the arguments (using the verb implore means that the\ntheme argument has greater power than the agent), and the agency of each argument\n(waited is low agency). Fig. 22.15 shows a visualization from Sap et al. (2017).\nConnotation frames can be built by hand (Sap et al., 2017), or they can be learned\nby supervised learning (Rashkin et al., 2016), for example using hand-labeled train-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 738,
      "token_count": 582,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 507",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 739,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "22.10 \u2022 S UMMARY 499\nAGENTTHEMEpower(AG < TH)VERBimploreHe implored the tribunal to show mercy.The princess waited for her prince.AGENTTHEMEagency(AG) = -VERBwaitFigure 2: The formal notation of the connotation\nframes of power and agency. The \ufb01rst example\nshows the relative power differential implied by\nthe verb \u201cimplored\u201d , i.e., the agent (\u201che\u201d) is in\na position of less power than the theme (\u201cthe tri-\nbunal\u201d). In contrast, \u201cHe demanded the tribunal\nshow mercy\u201d implies that the agent has authority\nover the theme. The second example shows the\nlow level of agency implied by the verb \u201cwaited\u201d .\ninteractive demo website of our \ufb01ndings (see Fig-\nure5in the appendix for a screenshot).2Further-\nmore, as will be seen in Section 4.1, connotation\nframes offer new insights that complement and de-\nviate from the well-known Bechdel test ( Bechdel ,\n1986 ). In particular, we \ufb01nd that high-agency\nwomen through the lens of connotation frames are\nrare in modern \ufb01lms. It is, in part, because some\nmovies (e.g., Snow White) accidentally pass the\nBechdel test and also because even movies with\nstrong female characters are not entirely free from\nthe deeply ingrained biases in social norms.\n2 Connotation Frames of Power and\nAgency\nWe create two new connotation relations, power\nandagency (examples in Figure 3), as an expan-\nsion of the existing connotation frame lexicons.3\nThree AMT crowdworkers annotated the verbs\nwith placeholders to avoid gender bias in the con-\ntext (e.g., Xrescued Y; an example task is shown\nin the appendix in Figure 7). We de\ufb01ne the anno-\ntated constructs as follows:\nPower Differentials Many verbs imply the au-\nthority levels of the agent and theme relative to\n2http://homes .cs.washington .edu/ \u02dcmsap/\nmovie-bias/ .\n3The lexicons and a demo are available at http://\nhomes .cs.washington .edu/ \u02dcmsap/movie-bias/ .power (AG<TH)power (AG>TH)\nagency (AG)=\u0000agency (AG)=+Figure 3: Sample verbs in the connotation frames\nwith high annotator agreement. Size is indicative\nof verb frequency in our corpus (bigger =more\nfrequent), color differences are only for legibility.\none another. For example, if the agent \u201cdom-\ninates\u201d the theme (denoted as power (AG>TH)),\nthen the agent is implied to have a level of control\nover the theme. Alternatively, if the agent \u201chon-\nors\u201d the theme (denoted as power (AG<TH)), the\nwriter implies that the theme is more important or\nauthoritative. We used AMT crowdsourcing to la-\nbel 1700 transitive verbs for power differentials.\nWith three annotators per verb, the inter-annotator\nagreement is 0.34 (Krippendorff\u2019s \u21b5).\nAgency The agency attributed to the agent of the\nverb denotes whether the action being described\nimplies that the agent is powerful, decisive, and\ncapable of pushing forward their own storyline.\nFor example, a person who is described as \u201cex-\nperiencing\u201d things does not seem as active and de-\ncisive as someone who is described as \u201cdetermin-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 740,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "agency (AG)=\u0000agency (AG)=+Figure 3: Sample verbs in the connotation frames\nwith high annotator agreement. Size is indicative\nof verb frequency in our corpus (bigger =more\nfrequent), color differences are only for legibility.\none another. For example, if the agent \u201cdom-\ninates\u201d the theme (denoted as power (AG>TH)),\nthen the agent is implied to have a level of control\nover the theme. Alternatively, if the agent \u201chon-\nors\u201d the theme (denoted as power (AG<TH)), the\nwriter implies that the theme is more important or\nauthoritative. We used AMT crowdsourcing to la-\nbel 1700 transitive verbs for power differentials.\nWith three annotators per verb, the inter-annotator\nagreement is 0.34 (Krippendorff\u2019s \u21b5).\nAgency The agency attributed to the agent of the\nverb denotes whether the action being described\nimplies that the agent is powerful, decisive, and\ncapable of pushing forward their own storyline.\nFor example, a person who is described as \u201cex-\nperiencing\u201d things does not seem as active and de-\ncisive as someone who is described as \u201cdetermin-\ning\u201d things. AMT workers labeled 2000 transi-\ntive verbs for implying high/moderate/low agency\n(inter-annotator agreement of 0.27). We denote\nhigh agency as agency (AG)=+, and low agency\nasagency (AG)=\u0000.\nPairwise agreements on a hard constraint are\n56% and 51% for power and agency, respec-\ntively. Despite this, agreements reach 96% and\n94% when moderate labels are counted as agree-\ning with either high or low labels, showing that an-\nnotators rarely strongly disagree with one another.\nSome contributing factors in the lower KA scores\ninclude the subtlety of choosing between neutral\nFigure 22.15 The connotation frames of Sap et al. (2017), showing that the verb implore\nimplies the agent has lower power than the theme (in contrast, say, with a verb like demanded ),\nand showing the low level of agency of the subject of waited . Figure from Sap et al. (2017).\ning data to supervise classi\ufb01ers for each of the individual relations, e.g., whether\nS(writer!Role1) is + or -, and then improving accuracy via global constraints\nacross all relations.\n22.10 Summary\n\u2022 Many kinds of affective states can be distinguished, including emotions ,moods ,\nattitudes (which include sentiment ),interpersonal stance , and personality .\n\u2022Emotion can be represented by \ufb01xed atomic units often called basic emo-\ntions , or as points in space de\ufb01ned by dimensions like valence andarousal .\n\u2022 Words have connotational aspects related to these affective states, and this\nconnotational aspect of word meaning can be represented in lexicons.\n\u2022 Affective lexicons can be built by hand, using crowd sourcing to label the\naffective content of each word.\n\u2022 Lexicons can be built with semi-supervised , bootstrapping from seed words\nusing similarity metrics like embedding cosine.\n\u2022 Lexicons can be learned in a fully supervised manner, when a convenient\ntraining signal can be found in the world, such as ratings assigned by users on\na review site.\n\u2022 Words can be assigned weights in a lexicon by using various functions of word\ncounts in training texts, and ratio metrics like log odds ratio informative\nDirichlet prior .\n\u2022 Affect can be detected, just like sentiment, by using standard supervised text",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 741,
      "token_count": 764,
      "chapter_title": ""
    }
  },
  {
    "content": "ing data to supervise classi\ufb01ers for each of the individual relations, e.g., whether\nS(writer!Role1) is + or -, and then improving accuracy via global constraints\nacross all relations.\n22.10 Summary\n\u2022 Many kinds of affective states can be distinguished, including emotions ,moods ,\nattitudes (which include sentiment ),interpersonal stance , and personality .\n\u2022Emotion can be represented by \ufb01xed atomic units often called basic emo-\ntions , or as points in space de\ufb01ned by dimensions like valence andarousal .\n\u2022 Words have connotational aspects related to these affective states, and this\nconnotational aspect of word meaning can be represented in lexicons.\n\u2022 Affective lexicons can be built by hand, using crowd sourcing to label the\naffective content of each word.\n\u2022 Lexicons can be built with semi-supervised , bootstrapping from seed words\nusing similarity metrics like embedding cosine.\n\u2022 Lexicons can be learned in a fully supervised manner, when a convenient\ntraining signal can be found in the world, such as ratings assigned by users on\na review site.\n\u2022 Words can be assigned weights in a lexicon by using various functions of word\ncounts in training texts, and ratio metrics like log odds ratio informative\nDirichlet prior .\n\u2022 Affect can be detected, just like sentiment, by using standard supervised text\nclassi\ufb01cation techniques, using all the words or bigrams in a text as features.\nAdditional features can be drawn from counts of words in lexicons.\n\u2022 Lexicons can also be used to detect affect in a rule-based classi\ufb01er by picking\nthe simple majority sentiment based on counts of words in each lexicon.\n\u2022Connotation frames express richer relations of affective meaning that a pred-\nicate encodes about its arguments.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 742,
      "token_count": 381,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 508",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 743,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "500 CHAPTER 22 \u2022 L EXICONS FOR SENTIMENT , AFFECT ,AND CONNOTATION\nBibliographical and Historical Notes\nThe idea of formally representing the subjective meaning of words began with Os-\ngood et al. (1957), the same pioneering study that \ufb01rst proposed the vector space\nmodel of meaning described in Chapter 6. Osgood et al. (1957) had participants rate\nwords on various scales, and ran factor analysis on the ratings. The most signi\ufb01cant\nfactor they uncovered was the evaluative dimension, which distinguished between\npairs like good/bad ,valuable/worthless ,pleasant/unpleasant . This work in\ufb02uenced\nthe development of early dictionaries of sentiment and affective meaning in the \ufb01eld\nofcontent analysis (Stone et al., 1966).\nWiebe (1994) began an in\ufb02uential line of work on detecting subjectivity in text, subjectivity\nbeginning with the task of identifying subjective sentences and the subjective char-\nacters who are described in the text as holding private states, beliefs or attitudes.\nLearned sentiment lexicons such as the polarity lexicons of Hatzivassiloglou and\nMcKeown (1997) were shown to be a useful feature in subjectivity detection (Hatzi-\nvassiloglou and Wiebe 2000, Wiebe 2000).\nThe term sentiment seems to have been introduced in 2001 by Das and Chen\n(2001), to describe the task of measuring market sentiment by looking at the words in\nstock trading message boards. In the same paper Das and Chen (2001) also proposed\nthe use of a sentiment lexicon. The list of words in the lexicon was created by\nhand, but each word was assigned weights according to how much it discriminated\na particular class (say buy versus sell) by maximizing across-class variation and\nminimizing within-class variation. The term sentiment , and the use of lexicons,\ncaught on quite quickly (e.g., inter alia, Turney 2002). Pang et al. (2002) \ufb01rst showed\nthe power of using all the words without a sentiment lexicon; see also Wang and\nManning (2012).\nMost of the semi-supervised methods we describe for extending sentiment dic-\ntionaries drew on the early idea that synonyms and antonyms tend to co-occur in the\nsame sentence (Miller and Charles 1991, Justeson and Katz 1991, Riloff and Shep-\nherd 1997). Other semi-supervised methods for learning cues to affective mean-\ning rely on information extraction techniques, like the AutoSlog pattern extractors\n(Riloff and Wiebe, 2003). Graph based algorithms for sentiment were \ufb01rst sug-\ngested by Hatzivassiloglou and McKeown (1997), and graph propagation became a\nstandard method (Zhu and Ghahramani 2002, Zhu et al. 2003, Zhou et al. 2004a,\nVelikovich et al. 2010). Crowdsourcing can also be used to improve precision by\n\ufb01ltering the result of semi-supervised lexicon learning (Riloff and Shepherd 1997,\nFast et al. 2016).\nMuch recent work focuses on ways to learn embeddings that directly encode sen-\ntiment or other properties, such as the D ENSIFIER algorithm of Rothe et al. (2016)\nthat learns to transform the embedding space to focus on sentiment (or other) infor-\nmation.\nExercises\n22.1 Show that the relationship between a word wand a category cin the Potts",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 744,
      "token_count": 776,
      "chapter_title": ""
    }
  },
  {
    "content": "same sentence (Miller and Charles 1991, Justeson and Katz 1991, Riloff and Shep-\nherd 1997). Other semi-supervised methods for learning cues to affective mean-\ning rely on information extraction techniques, like the AutoSlog pattern extractors\n(Riloff and Wiebe, 2003). Graph based algorithms for sentiment were \ufb01rst sug-\ngested by Hatzivassiloglou and McKeown (1997), and graph propagation became a\nstandard method (Zhu and Ghahramani 2002, Zhu et al. 2003, Zhou et al. 2004a,\nVelikovich et al. 2010). Crowdsourcing can also be used to improve precision by\n\ufb01ltering the result of semi-supervised lexicon learning (Riloff and Shepherd 1997,\nFast et al. 2016).\nMuch recent work focuses on ways to learn embeddings that directly encode sen-\ntiment or other properties, such as the D ENSIFIER algorithm of Rothe et al. (2016)\nthat learns to transform the embedding space to focus on sentiment (or other) infor-\nmation.\nExercises\n22.1 Show that the relationship between a word wand a category cin the Potts\nScore in Eq. 22.6 is a variant of the pointwise mutual information pmi (w;c)\nwithout the log term.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 745,
      "token_count": 298,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 509\n\nCHAPTER\n23Coreference Resolution and\nEntity Linking\nand even Stigand, the patriotic archbishop of Canterbury, found it advisable\u2013\u201d\u2019\n\u2018Found WHAT?\u2019 said the Duck.\n\u2018Found IT, \u2019 the Mouse replied rather crossly: \u2018of course you know what \u201cit\u201dmeans. \u2019\n\u2018I know what \u201cit\u201dmeans well enough, when I \ufb01nd a thing, \u2019 said the Duck: \u2018it\u2019s gener-\nally a frog or a worm. The question is, what did the archbishop \ufb01nd?\u2019\nLewis Carroll, Alice in Wonderland\nAn important component of language processing is knowing who is being talked\nabout in a text. Consider the following passage:\n(23.1) Victoria Chen , CFO of Megabucks Banking, saw her pay jump to $2.3\nmillion, as the 38-year-old became the company\u2019s president. It is widely\nknown that she came to Megabucks from rival Lotsabucks.\nEach of the underlined phrases in this passage is used by the writer to refer to\na person named Victoria Chen. We call linguistic expressions like herorVictoria\nChen mentions orreferring expressions , and the discourse entity that is referred mention\nto (Victoria Chen) the referent . (To distinguish between referring expressions and referent\ntheir referents, we italicize the former.)1Two or more referring expressions that are\nused to refer to the same discourse entity are said to corefer ; thus, Victoria Chen corefer\nandshecorefer in (23.1).\nCoreference is an important component of natural language processing. A dia-\nlogue system that has just told the user \u201cThere is a 2pm \ufb02ight on United and a 4pm\none on Cathay Paci\ufb01c\u201d must know which \ufb02ight the user means by \u201cI\u2019ll take the sec-\nond one\u201d . A question answering system that uses Wikipedia to answer a question\nabout Marie Curie must know who shewas in the sentence \u201cShe was born in War-\nsaw\u201d . And a machine translation system translating from a language like Spanish, in\nwhich pronouns can be dropped, must use coreference from the previous sentence to\ndecide whether the Spanish sentence \u2018 \u201cMe encanta el conocimiento\u201d, dice. \u2019 should\nbe translated as \u2018 \u201cI love knowledge\u201d, he says \u2019, or \u2018 \u201cI love knowledge\u201d, she says \u2019.\nIndeed, this example comes from an actual news article in El Pa \u00b4\u0131sabout a female\nprofessor and was mistranslated as \u201che\u201d in machine translation because of inaccurate\ncoreference resolution (Schiebinger, 2013).\nNatural language processing systems (and humans) interpret linguistic expres-\nsions with respect to a discourse model (Karttunen, 1969). A discourse modeldiscourse\nmodel\n(Fig. 23.1) is a mental model that the understander builds incrementally when in-\nterpreting a text, containing representations of the entities referred to in the text,\nas well as properties of the entities and relations among them. When a referent is\n\ufb01rst mentioned in a discourse, we say that a representation for it is evoked into the evoked\nmodel. Upon subsequent mention, this representation is accessed from the model. accessed\n1As a convenient shorthand, we sometimes speak of a referring expression referring to a referent, e.g.,\nsaying that sherefers to Victoria Chen. However, the reader should keep in mind that what we really\nmean is that the speaker is performing the act of referring to Victoria Chen by uttering she.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 746,
      "token_count": 760,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 510\n\n502 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\nVDiscourse Model\u201cVictoria\u201d\u201cshe\u201dcoreferrefer (evoke)refer (access)$LotsabucksMegabuckspay\nFigure 23.1 How mentions evoke and access discourse entities in a discourse model.\nReference in a text to an entity that has been previously introduced into the\ndiscourse is called anaphora , and the referring expression used is said to be an anaphora\nanaphor , or anaphoric.2In passage (23.1), the pronouns sheandherand the de\ufb01- anaphor\nnite NP the 38-year-old are therefore anaphoric. The anaphor corefers with a prior\nmention (in this case Victoria Chen ) that is called the antecedent . Not every refer- antecedent\nring expression is an antecedent. An entity that has only a single mention in a text\n(like Lotsabucks in (23.1)) is called a singleton . singleton\nIn this chapter we focus on the task of coreference resolution . Coreferencecoreference\nresolution\nresolution is the task of determining whether two mentions corefer , by which we\nmean they refer to the same entity in the discourse model (the same discourse entity ).\nThe set of coreferring expressions is often called a coreference chain or acluster .coreference\nchain\ncluster For example, in processing (23.1), a coreference resolution algorithm would need\nto \ufb01nd at least four coreference chains, corresponding to the four entities in the\ndiscourse model in Fig. 23.1.\n1.fVictoria Chen ,her,the 38-year-old ,Sheg\n2.fMegabucks Banking ,the company ,Megabucksg\n3.fher payg\n4.fLotsabucksg\nNote that mentions can be nested; for example the mention heris syntactically\npart of another mention, her pay , referring to a completely different discourse entity.\nCoreference resolution thus comprises two tasks (although they are often per-\nformed jointly): (1) identifying the mentions, and (2) clustering them into corefer-\nence chains/discourse entities.\nWe said that two mentions corefered if they are associated with the same dis-\ncourse entity . But often we\u2019d like to go further, deciding which real world entity is\nassociated with this discourse entity. For example, the mention Washington might\nrefer to the US state, or the capital city, or the person George Washington; the inter-\npretation of the sentence will of course be very different for each of these. The task\nofentity linking (Ji and Grishman, 2011) or entity resolution is the task of mapping entity linking\na discourse entity to some real-world individual.3We usually operationalize entity\n2We will follow the common NLP usage of anaphor to mean any mention that has an antecedent, rather\nthan the more narrow usage to mean only mentions (like pronouns) whose interpretation depends on the\nantecedent (under the narrower interpretation, repeated names are not anaphors).\n3Computational linguistics/NLP thus differs in its use of the term reference from the \ufb01eld of formal\nsemantics, which uses the words reference andcoreference to describe the relation between a mention\nand a real-world entity. By contrast, we follow the functional linguistics tradition in which a mention\nrefers to adiscourse entity (Webber, 1978) and the relation between a discourse entity and the real world\nindividual requires an additional step of linking .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 747,
      "token_count": 756,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 511",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 748,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "503\nlinking or resolution by mapping to an ontology : a list of entities in the world, like\na gazeteer (Appendix F). Perhaps the most common ontology used for this task is\nWikipedia; each Wikipedia page acts as the unique id for a particular entity. Thus\nthe entity linking task of wiki\ufb01cation (Mihalcea and Csomai, 2007) is the task of de-\nciding which Wikipedia page corresponding to an individual is being referred to by\na mention. But entity linking can be done with any ontology; for example if we have\nan ontology of genes, we can link mentions of genes in text to the disambiguated\ngene name in the ontology.\nIn the next sections we introduce the task of coreference resolution in more de-\ntail, and survey a variety of architectures for resolution. We also introduce two\narchitectures for the task of entity linking.\nBefore turning to algorithms, however, we mention some important tasks we\nwill only touch on brie\ufb02y at the end of this chapter. First are the famous Winograd\nSchema problems (so-called because they were \ufb01rst pointed out by Terry Winograd\nin his dissertation). These entity coreference resolution problems are designed to be\ntoo dif\ufb01cult to be solved by the resolution methods we describe in this chapter, and\nthe kind of real-world knowledge they require has made them a kind of challenge\ntask for natural language processing. For example, consider the task of determining\nthe correct antecedent of the pronoun they in the following example:\n(23.2) The city council denied the demonstrators a permit because\na. they feared violence.\nb. they advocated violence.\nDetermining the correct antecedent for the pronoun they requires understanding\nthat the second clause is intended as an explanation of the \ufb01rst clause, and also\nthat city councils are perhaps more likely than demonstrators to fear violence and\nthat demonstrators might be more likely to advocate violence. Solving Winograd\nSchema problems requires \ufb01nding way to represent or discover the necessary real\nworld knowledge.\nA problem we won\u2019t discuss in this chapter is the related task of event corefer-\nence , deciding whether two event mentions (such as the buyand the acquisition inevent\ncoreference\nthese two sentences from the ECB+ corpus) refer to the same event:\n(23.3) AMD agreed to [ buy] Markham, Ontario-based ATI for around $5.4 billion\nin cash and stock, the companies announced Monday.\n(23.4) The [ acquisition ] would turn AMD into one of the world\u2019s largest providers\nof graphics chips.\nEvent mentions are much harder to detect than entity mentions, since they can be ver-\nbal as well as nominal. Once detected, the same mention-pair and mention-ranking\nmodels used for entities are often applied to events.\nAn even more complex kind of coreference is discourse deixis (Webber, 1988), discourse deixis\nin which an anaphor refers back to a discourse segment, which can be quite hard to\ndelimit or categorize, like the examples in (23.5) adapted from Webber (1991):\n(23.5) According to Soleil, Beau just opened a restaurant\na. But thatturned out to be a lie.\nb. But thatwas false.\nc.That struck me as a funny way to describe the situation.\nThe referent of that is a speech act (see Chapter 15) in (23.5a), a proposition in\n(23.5b), and a manner of description in (23.5c). We don\u2019t give algorithms in this\nchapter for these dif\ufb01cult types of non-nominal antecedents , but see Kolhatkar\net al. (2018) for a survey.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 749,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 512\n\n504 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\n23.1 Coreference Phenomena: Linguistic Background\nWe now offer some linguistic background on reference phenomena. We introduce\nthe four types of referring expressions (de\ufb01nite and inde\ufb01nite NPs, pronouns, and\nnames), describe how these are used to evoke and access entities in the discourse\nmodel, and talk about linguistic features of the anaphor/antecedent relation (like\nnumber/gender agreement, or properties of verb semantics).\n23.1.1 Types of Referring Expressions\nInde\ufb01nite Noun Phrases: The most common form of inde\ufb01nite reference in En-\nglish is marked with the determiner a(oran), but it can also be marked by a quan-\nti\ufb01er such as some or even the determiner this. Inde\ufb01nite reference generally intro-\nduces into the discourse context entities that are new to the hearer.\n(23.6) a. Mrs. Martin was so very kind as to send Mrs. Goddard a beautiful goose .\nb. He had gone round one day to bring her some walnuts .\nc. I saw this beautiful cauli\ufb02ower today.\nDe\ufb01nite Noun Phrases: De\ufb01nite reference, such as via NPs that use the English\narticle the, refers to an entity that is identi\ufb01able to the hearer. An entity can be\nidenti\ufb01able to the hearer because it has been mentioned previously in the text and\nthus is already represented in the discourse model:\n(23.7) It concerns a white stallion which I have sold to an of\ufb01cer. But the pedigree\nofthe white stallion was not fully established.\nAlternatively, an entity can be identi\ufb01able because it is contained in the hearer\u2019s\nset of beliefs about the world, or the uniqueness of the object is implied by the\ndescription itself, in which case it evokes a representation of the referent into the\ndiscourse model, as in (23.9):\n(23.8) I read about it in the New York Times .\n(23.9) Have you seen the car keys?\nThese last uses are quite common; more than half of de\ufb01nite NPs in newswire\ntexts are non-anaphoric, often because they are the \ufb01rst time an entity is mentioned\n(Poesio and Vieira 1998, Bean and Riloff 1999).\nPronouns: Another form of de\ufb01nite reference is pronominalization, used for enti-\nties that are extremely salient in the discourse, (as we discuss below):\n(23.10) Emma smiled and chatted as cheerfully as shecould,\nPronouns can also participate in cataphora , in which they are mentioned before cataphora\ntheir referents are, as in (23.11).\n(23.11) Even before shesawit, Dorothy had been thinking about the Emerald City\nevery day.\nHere, the pronouns sheanditboth occur before their referents are introduced.\nPronouns also appear in quanti\ufb01ed contexts in which they are considered to be\nbound , as in (23.12). bound\n(23.12) Every dancer brought herleft arm forward.\nUnder the relevant reading, herdoes not refer to some woman in context, but instead\nbehaves like a variable bound to the quanti\ufb01ed expression every dancer . We are not\nconcerned with the bound interpretation of pronouns in this chapter.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 750,
      "token_count": 765,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 513\n\n23.1 \u2022 C OREFERENCE PHENOMENA : LINGUISTIC BACKGROUND 505\nIn some languages, pronouns can appear as clitics attached to a word, like lo\n(\u2018it\u2019) in this Spanish example from AnCora (Recasens and Mart \u00b4\u0131, 2010):\n(23.13) La intenci \u00b4on es reconocer el gran prestigio que tiene la marat \u00b4on y unir lo\ncon esta gran carrera.\n\u2018The aim is to recognize the great prestige that the Marathon has and join jit\nwith this great race.\u201d\nDemonstrative Pronouns: Demonstrative pronouns thisandthat can appear ei-\nther alone or as determiners, for instance, this ingredient ,that spice :\n(23.14) I just bought a copy of Thoreau\u2019s Walden . I had bought one \ufb01ve years ago.\nThat one had been very tattered; this one was in much better condition.\nNote that this NP is ambiguous; in colloquial spoken English, it can be inde\ufb01nite,\nas in (23.6), or de\ufb01nite, as in (23.14).\nZero Anaphora: Instead of using a pronoun, in some languages (including Chi-\nnese, Japanese, and Italian) it is possible to have an anaphor that has no lexical\nrealization at all, called a zero anaphor or zero pronoun, as in the following Italian zero anaphor\nand Japanese examples from Poesio et al. (2016):\n(23.15) EN [John] iwent to visit some friends. On the way [he] ibought some\nwine.\nIT [Giovanni] iand`o a far visita a degli amici. Per via ficompr `o del vino.\nJA [John] i-wa yujin-o houmon-sita. Tochu-de fiwain-o ka-tta.\nor this Chinese example:\n(23.16) [ \u6211]\u524d\u4e00\u4f1a\u7cbe\u795e\u4e0a\u592a\u7d27\u5f20\u3002[0]\u73b0\u5728\u6bd4\u8f83\u5e73\u9759\u4e86\n[I] was too nervous a while ago. ... [0] am now calmer.\nZero anaphors complicate the task of mention detection in these languages.\nNames: Names (such as of people, locations, or organizations) can be used to refer\nto both new and old entities in the discourse:\n(23.17) a. Miss Woodhouse certainly had not done him justice.\nb.International Business Machines sought patent compensation\nfrom Amazon; IBM had previously sued other companies.\n23.1.2 Information Status\nThe way referring expressions are used to evoke new referents into the discourse\n(introducing new information), or access old entities from the model (old informa-\ntion), is called their information status orinformation structure . Entities can beinformation\nstatus\ndiscourse-new ordiscourse-old , and indeed it is common to distinguish at least discourse-new\ndiscourse-old three kinds of entities informationally (Prince, 1981):\nnew NPs:\nbrand new NPs: these introduce entities that are discourse-new and hearer-\nnew like a fruit orsome walnuts .\nunused NPs: these introduce entities that are discourse-new but hearer-old\n(like Hong Kong ,Marie Curie , orthe New York Times .\nold NPs: also called evoked NPs , these introduce entities that already in the dis-\ncourse model, hence are both discourse-old and hearer-old, like itin \u201cI went\nto a new restaurant. It was... \u201d.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 751,
      "token_count": 760,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 514",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 752,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "506 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\ninferrables: these introduce entities that are neither hearer-old nor discourse-old,\nbut the hearer can infer their existence by reasoning based on other entities\nthat are in the discourse. Consider the following examples:\n(23.18) I went to a superb restaurant yesterday. The chef had just opened it.\n(23.19) Mix \ufb02our, butter and water. Knead the dough until shiny.\nNeither the chef northe dough were in the discourse model based on the \ufb01rst\nsentence of either example, but the reader can make a bridging inferencebridging\ninference\nthat these entities should be added to the discourse model and associated with\nthe restaurant and the ingredients, based on world knowledge that restaurants\nhave chefs and dough is the result of mixing \ufb02our and liquid (Haviland and\nClark 1974, Webber and Baldwin 1992, Nissim et al. 2004, Hou et al. 2018).\nThe form of an NP gives strong clues to its information status. We often talk\nabout an entity\u2019s position on the given-new dimension, the extent to which the refer- given-new\nent is given (salient in the discourse, easier for the hearer to call to mind, predictable\nby the hearer), versus new (non-salient in the discourse, unpredictable) (Chafe 1976,\nPrince 1981, Gundel et al. 1993). A referent that is very accessible (Ariel, 2001) accessible\ni.e., very salient in the hearer\u2019s mind or easy to call to mind, can be referred to with\nless linguistic material. For example pronouns are used only when the referent has\na high degree of activation or salience in the discourse model.4By contrast, less salience\nsalient entities, like a new referent being introduced to the discourse, will need to be\nintroduced with a longer and more explicit referring expression to help the hearer\nrecover the referent.\nThus when an entity is \ufb01rst introduced into a discourse its mentions are likely\nto have full names, titles or roles, or appositive or restrictive relative clauses, as in\nthe introduction of our protagonist in (23.1): Victoria Chen, CFO of Megabucks\nBanking . As an entity is discussed over a discourse, it becomes more salient to the\nhearer and its mentions on average typically becomes shorter and less informative,\nfor example with a shortened name (for example Ms. Chen ), a de\ufb01nite description\n(the 38-year-old ), or a pronoun ( sheorher) (Hawkins 1978). However, this change\nin length is not monotonic, and is sensitive to discourse structure (Grosz 1977b,\nReichman 1985, Fox 1993).\n23.1.3 Complications: Non-Referring Expressions\nMany noun phrases or other nominals are not referring expressions, although they\nmay bear a confusing super\ufb01cial resemblance. For example in some of the earliest\ncomputational work on reference resolution, Karttunen (1969) pointed out that the\nNPa car in the following example does not create a discourse referent:\n(23.20) Janet doesn\u2019t have a car .\nand cannot be referred back to by anaphoric itorthe car :\n(23.21) * Itis a Toyota.\n(23.22) * The car is red.\nWe summarize here four common types of structures that are not counted as men-\ntions in coreference tasks and hence complicate the task of mention-detection:",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 753,
      "token_count": 776,
      "chapter_title": ""
    }
  },
  {
    "content": "Banking . As an entity is discussed over a discourse, it becomes more salient to the\nhearer and its mentions on average typically becomes shorter and less informative,\nfor example with a shortened name (for example Ms. Chen ), a de\ufb01nite description\n(the 38-year-old ), or a pronoun ( sheorher) (Hawkins 1978). However, this change\nin length is not monotonic, and is sensitive to discourse structure (Grosz 1977b,\nReichman 1985, Fox 1993).\n23.1.3 Complications: Non-Referring Expressions\nMany noun phrases or other nominals are not referring expressions, although they\nmay bear a confusing super\ufb01cial resemblance. For example in some of the earliest\ncomputational work on reference resolution, Karttunen (1969) pointed out that the\nNPa car in the following example does not create a discourse referent:\n(23.20) Janet doesn\u2019t have a car .\nand cannot be referred back to by anaphoric itorthe car :\n(23.21) * Itis a Toyota.\n(23.22) * The car is red.\nWe summarize here four common types of structures that are not counted as men-\ntions in coreference tasks and hence complicate the task of mention-detection:\n4Pronouns also usually (but not always) refer to entities that were introduced no further than one or two\nsentences back in the ongoing discourse, whereas de\ufb01nite noun phrases can often refer further back.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 754,
      "token_count": 325,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 515\n\n23.1 \u2022 C OREFERENCE PHENOMENA : LINGUISTIC BACKGROUND 507\nAppositives: An appositional structure is a noun phrase that appears next to a\nhead noun phrase, describing the head. In English they often appear in commas, like\n\u201ca unit of UAL\u201d appearing in apposition to the NP United , orCFO of Megabucks\nBanking in apposition to Victoria Chen .\n(23.23) Victoria Chen, CFO of Megabucks Banking, saw ...\n(23.24) United, a unit of UAL, matched the fares.\nAppositional NPs are not referring expressions, instead functioning as a kind of\nsupplementary parenthetical description of the head NP. Nonetheless, sometimes it\nis useful to link these phrases to an entity they describe, and so some datasets like\nOntoNotes mark appositional relationships.\nPredicative and Prenominal NPs: Predicative or attributive NPs describe prop-\nerties of the head noun. In United is a unit of UAL , the NP a unit of UAL describes\na property of United, rather than referring to a distinct entity. Thus they are not\nmarked as mentions in coreference tasks; in our example the NPs $2.3 million and\nthe company\u2019s president , are attributive, describing properties of her pay andthe\n38-year-old ; Example (23.27) shows a Chinese example in which the predicate NP\n(\u4e2d\u56fd\u6700\u5927\u7684\u57ce\u5e02; China\u2019s biggest city ) is not a mention.\n(23.25) her pay jumped to $2.3 million\n(23.26) the 38-year-old became the company\u2019s president\n(23.27)\u4e0a\u6d77\u662f[\u4e2d\u56fd\u6700\u5927\u7684\u57ce\u5e02] [Shanghai is China\u2019s biggest city ]\nExpletives: Many uses of pronouns like itin English and corresponding pronouns\nin other languages are not referential. Such expletive orpleonastic cases include expletive\nit is raining , in idioms like hit it off , or in particular syntactic situations like clefts clefts\n(23.28a) or extraposition (23.28b):\n(23.28) a. Itwas Emma Goldman who founded Mother Earth\nb.Itsurprised me that there was a herring hanging on her wall.\nGenerics: Another kind of expression that does not refer back to an entity explic-\nitly evoked in the text is generic reference. Consider (23.29).\n(23.29) I love mangos. They are very tasty.\nHere, they refers, not to a particular mango or set of mangos, but instead to the class\nof mangos in general. The pronoun youcan also be used generically:\n(23.30) In July in San Francisco youhave to wear a jacket.\n23.1.4 Linguistic Properties of the Coreference Relation\nNow that we have seen the linguistic properties of individual referring expressions\nwe turn to properties of the antecedent/anaphor pair. Understanding these properties\nis helpful both in designing novel features and performing error analyses.\nNumber Agreement: Referring expressions and their referents must generally\nagree in number; English she/her/he/him/his/it are singular, we/us/they/them are plu-\nral, and youis unspeci\ufb01ed for number. So a plural antecedent like the chefs cannot\ngenerally corefer with a singular anaphor like she. However, algorithms cannot\nenforce number agreement too strictly. First, semantically plural entities can be re-\nferred to by either itorthey:\n(23.31) IBM announced a new machine translation product yesterday. They have\nbeen working on it for 20 years.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 755,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 516",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 756,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "508 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\nSecond, singular they has become much more common, in which they is used to singular they\ndescribe singular individuals, often useful because they is gender neutral. Although\nrecently increasing, singular they is quite old, part of English for many centuries.5\nPerson Agreement: English distinguishes between \ufb01rst, second, and third person,\nand a pronoun\u2019s antecedent must agree with the pronoun in person. Thus a third\nperson pronoun ( he, she, they, him, her, them, his, her, their ) must have a third person\nantecedent (one of the above or any other noun phrase). However, phenomena like\nquotation can cause exceptions; in this example I,my, and sheare coreferent:\n(23.32) \u201cI voted for Nader because he was most aligned with my values,\u201d she said.\nGender or Noun Class Agreement: In many languages, all nouns have grammat-\nical gender or noun class6and pronouns generally agree with the grammatical gender\nof their antecedent. In English this occurs only with third-person singular pronouns,\nwhich distinguish between male (he, him, his ),female (she, her ), and nonpersonal\n(it) grammatical genders. Non-binary pronouns like zeorhirmay also occur in more\nrecent texts. Knowing which gender to associate with a name in text can be complex,\nand may require world knowledge about the individual. Some examples:\n(23.33) Maryam has a theorem. She is exciting. (she=Maryam, not the theorem)\n(23.34) Maryam has a theorem. It is exciting. (it=the theorem, not Maryam)\nBinding Theory Constraints: The binding theory is a name for syntactic con-\nstraints on the relations between a mention and an antecedent in the same sentence\n(Chomsky, 1981). Oversimplifying a bit, re\ufb02exive pronouns like himself andher- re\ufb02exive\nselfcorefer with the subject of the most immediate clause that contains them (23.35),\nwhereas nonre\ufb02exives cannot corefer with this subject (23.36).\n(23.35) Janet bought herself a bottle of \ufb01sh sauce. [herself =Janet]\n(23.36) Janet bought her a bottle of \ufb01sh sauce. [her 6=Janet]\nRecency: Entities introduced in recent utterances tend to be more salient than\nthose introduced from utterances further back. Thus, in (23.37), the pronoun itis\nmore likely to refer to Jim\u2019s map than the doctor\u2019s map.\n(23.37) The doctor found an old map in the captain\u2019s chest. Jim found an even\nolder map hidden on the shelf. It described an island.\nGrammatical Role: Entities mentioned in subject position are more salient than\nthose in object position, which are in turn more salient than those mentioned in\noblique positions. Thus although the \ufb01rst sentence in (23.38) and (23.39) expresses\nroughly the same propositional content, the preferred referent for the pronoun he\nvaries with the subject\u2014John in (23.38) and Bill in (23.39).\n(23.38) Billy Bones went to the bar with Jim Hawkins. He called for a glass of\nrum. [ he = Billy ]\n(23.39) Jim Hawkins went to the bar with Billy Bones. He called for a glass of\nrum. [ he = Jim ]",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 757,
      "token_count": 764,
      "chapter_title": ""
    }
  },
  {
    "content": "(23.35) Janet bought herself a bottle of \ufb01sh sauce. [herself =Janet]\n(23.36) Janet bought her a bottle of \ufb01sh sauce. [her 6=Janet]\nRecency: Entities introduced in recent utterances tend to be more salient than\nthose introduced from utterances further back. Thus, in (23.37), the pronoun itis\nmore likely to refer to Jim\u2019s map than the doctor\u2019s map.\n(23.37) The doctor found an old map in the captain\u2019s chest. Jim found an even\nolder map hidden on the shelf. It described an island.\nGrammatical Role: Entities mentioned in subject position are more salient than\nthose in object position, which are in turn more salient than those mentioned in\noblique positions. Thus although the \ufb01rst sentence in (23.38) and (23.39) expresses\nroughly the same propositional content, the preferred referent for the pronoun he\nvaries with the subject\u2014John in (23.38) and Bill in (23.39).\n(23.38) Billy Bones went to the bar with Jim Hawkins. He called for a glass of\nrum. [ he = Billy ]\n(23.39) Jim Hawkins went to the bar with Billy Bones. He called for a glass of\nrum. [ he = Jim ]\n5Here\u2019s a bound pronoun example from Shakespeare\u2019s Comedy of Errors :There\u2019s not a man I meet but\ndoth salute me As if I were their well-acquainted friend\n6The word \u201cgender\u201d is generally only used for languages with 2 or 3 noun classes, like most Indo-\nEuropean languages; many languages, like the Bantu languages or Chinese, have a much larger number\nof noun classes.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 758,
      "token_count": 378,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 517\n\n23.2 \u2022 C OREFERENCE TASKS AND DATASETS 509\nVerb Semantics: Some verbs semantically emphasize one of their arguments, bi-\nasing the interpretation of subsequent pronouns. Compare (23.40) and (23.41).\n(23.40) John telephoned Bill. He lost the laptop.\n(23.41) John criticized Bill. He lost the laptop.\nThese examples differ only in the verb used in the \ufb01rst sentence, yet \u201che\u201d in (23.40)\nis typically resolved to John, whereas \u201che\u201d in (23.41) is resolved to Bill. This may\nbe partly due to the link between implicit causality and saliency: the implicit cause\nof a \u201ccriticizing\u201d event is its object, whereas the implicit cause of a \u201ctelephoning\u201d\nevent is its subject. In such verbs, the entity which is the implicit cause may be more\nsalient.\nSelectional Restrictions: Many other kinds of semantic knowledge can play a role\nin referent preference. For example, the selectional restrictions that a verb places on\nits arguments (Chapter 21) can help eliminate referents, as in (23.42).\n(23.42) I ate the soup in my new bowl after cooking it for hours\nThere are two possible referents for it, the soup and the bowl. The verb eat, however,\nrequires that its direct object denote something edible, and this constraint can rule\noutbowl as a possible referent.\n23.2 Coreference Tasks and Datasets\nWe can formulate the task of coreference resolution as follows: Given a text T, \ufb01nd\nall entities and the coreference links between them. We evaluate our task by com-\nparing the links our system creates with those in human-created gold coreference\nannotations on T.\nLet\u2019s return to our coreference example, now using superscript numbers for each\ncoreference chain (cluster), and subscript letters for individual mentions in the clus-\nter:\n(23.43) [Victoria Chen]1\na, CFO of [Megabucks Banking]2\na, saw [[her]1\nbpay]3\najump\nto $2.3 million, as [the 38-year-old]1\ncalso became [[the company]2\nb\u2019s\npresident. It is widely known that [she]1\ndcame to [Megabucks]2\ncfrom rival\n[Lotsabucks]4\na.\nAssuming example (23.43) was the entirety of the article, the chains for her pay and\nLotsabucks are singleton mentions:\n1.fVictoria Chen ,her,the 38-year-old ,Sheg\n2.fMegabucks Banking ,the company ,Megabucksg\n3.fher payg\n4.fLotsabucksg\nFor most coreference evaluation campaigns, the input to the system is the raw\ntext of articles, and systems must detect mentions and then link them into clusters.\nSolving this task requires dealing with pronominal anaphora (\ufb01guring out that her\nrefers to Victoria Chen ), \ufb01ltering out non-referential pronouns like the pleonastic It\ninIt has been ten years ), dealing with de\ufb01nite noun phrases to \ufb01gure out that the\n38-year-old is coreferent with Victoria Chen , and that the company is the same as\nMegabucks . And we need to deal with names, to realize that Megabucks is the same\nasMegabucks Banking .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 759,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 518",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 760,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "510 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\nExactly what counts as a mention and what links are annotated differs from task\nto task and dataset to dataset. For example some coreference datasets do not label\nsingletons, making the task much simpler. Resolvers can achieve much higher scores\non corpora without singletons, since singletons constitute the majority of mentions in\nrunning text, and they are often hard to distinguish from non-referential NPs. Some\ntasks use gold mention-detection (i.e. the system is given human-labeled mention\nboundaries and the task is just to cluster these gold mentions), which eliminates the\nneed to detect and segment mentions from running text.\nCoreference is usually evaluated by the CoNLL F1 score, which combines three\nmetrics: MUC, B3, and CEAF e; Section 23.8 gives the details.\nLet\u2019s mention a few characteristics of one popular coreference dataset, OntoNotes\n(Pradhan et al. 2007c, Pradhan et al. 2007a), and the CoNLL 2012 Shared Task\nbased on it (Pradhan et al., 2012a). OntoNotes contains hand-annotated Chinese\nand English coreference datasets of roughly one million words each, consisting of\nnewswire, magazine articles, broadcast news, broadcast conversations, web data and\nconversational speech data, as well as about 300,000 words of annotated Arabic\nnewswire. The most important distinguishing characteristic of OntoNotes is that\nit does not label singletons, simplifying the coreference task, since singletons rep-\nresent 60%-70% of all entities. In other ways, it is similar to other coreference\ndatasets. Referring expression NPs that are coreferent are marked as mentions, but\ngenerics and pleonastic pronouns are not marked. Appositive clauses are not marked\nas separate mentions, but they are included in the mention. Thus in the NP, \u201cRichard\nGodown, president of the Industrial Biotechnology Association\u201d the mention is the\nentire phrase. Prenominal modi\ufb01ers are annotated as separate entities only if they\nare proper nouns. Thus wheat is not an entity in wheat \ufb01elds , but UNis an entity in\nUN policy (but not adjectives like American inAmerican policy ).\nA number of corpora mark richer discourse phenomena. The ISNotes corpus\nannotates a portion of OntoNotes for information status, include bridging examples\n(Hou et al., 2018). The LitBank coreference corpus (Bamman et al., 2020) contains\ncoreference annotations for 210,532 tokens from 100 different literary novels, in-\ncluding singletons and quanti\ufb01ed and negated noun phrases. The AnCora-CO coref-\nerence corpus (Recasens and Mart \u00b4\u0131, 2010) contains 400,000 words each of Spanish\n(AnCora-CO-Es) and Catalan (AnCora-CO-Ca) news data, and includes labels for\ncomplex phenomena like discourse deixis in both languages. The ARRAU corpus\n(Uryupina et al., 2020) contains 350,000 words of English marking all NPs, which\nmeans singleton clusters are available. ARRAU includes diverse genres like dialog\n(the TRAINS data) and \ufb01ction (the Pear Stories), and has labels for bridging refer-\nences, discourse deixis, generics, and ambiguous anaphoric relations.\n23.3 Mention Detection\nThe \ufb01rst stage of coreference is mention detection : \ufb01nding the spans of text thatmention\ndetection",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 761,
      "token_count": 784,
      "chapter_title": ""
    }
  },
  {
    "content": "A number of corpora mark richer discourse phenomena. The ISNotes corpus\nannotates a portion of OntoNotes for information status, include bridging examples\n(Hou et al., 2018). The LitBank coreference corpus (Bamman et al., 2020) contains\ncoreference annotations for 210,532 tokens from 100 different literary novels, in-\ncluding singletons and quanti\ufb01ed and negated noun phrases. The AnCora-CO coref-\nerence corpus (Recasens and Mart \u00b4\u0131, 2010) contains 400,000 words each of Spanish\n(AnCora-CO-Es) and Catalan (AnCora-CO-Ca) news data, and includes labels for\ncomplex phenomena like discourse deixis in both languages. The ARRAU corpus\n(Uryupina et al., 2020) contains 350,000 words of English marking all NPs, which\nmeans singleton clusters are available. ARRAU includes diverse genres like dialog\n(the TRAINS data) and \ufb01ction (the Pear Stories), and has labels for bridging refer-\nences, discourse deixis, generics, and ambiguous anaphoric relations.\n23.3 Mention Detection\nThe \ufb01rst stage of coreference is mention detection : \ufb01nding the spans of text thatmention\ndetection\nconstitute each mention. Mention detection algorithms are usually very liberal in\nproposing candidate mentions (i.e., emphasizing recall), and only \ufb01ltering later. For\nexample many systems run parsers and named entity taggers on the text and extract\nevery span that is either an NP, apossessive pronoun , or a named entity .\nDoing so from our sample text repeated in (23.44):\n(23.44) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $2.3",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 762,
      "token_count": 392,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 519",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 763,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "23.3 \u2022 M ENTION DETECTION 511\nmillion, as the 38-year-old also became the company\u2019s president. It is\nwidely known that she came to Megabucks from rival Lotsabucks.\nmight result in the following list of 13 potential mentions:\nVictoria Chen $2.3 million she\nCFO of Megabucks Banking the 38-year-old Megabucks\nMegabucks Banking the company Lotsabucks\nher the company\u2019s president\nher pay It\nMore recent mention detection systems are even more generous; the span-based\nalgorithm we will describe in Section 23.6 \ufb01rst extracts literally all n-gram spans\nof words up to N=10. Of course recall from Section 23.1.3 that many NPs\u2014and\nthe overwhelming majority of random n-gram spans\u2014are not referring expressions.\nTherefore all such mention detection systems need to eventually \ufb01lter out pleonas-\ntic/expletive pronouns like Itabove, appositives like CFO of Megabucks Banking\nInc, or predicate nominals like the company\u2019s president or$2.3 million .\nSome of this \ufb01ltering can be done by rules. Early rule-based systems designed\nregular expressions to deal with pleonastic it, like the following rules from Lappin\nand Leass (1994) that use dictionaries of cognitive verbs (e.g., believe ,know ,antic-\nipate ) to capture pleonastic itin \u201cIt is thought that ketchup...\u201d, or modal adjectives\n(e.g., necessary ,possible ,certain ,important ), for, e.g., \u201cIt is likely that I...\u201d. Such\nrules are sometimes used as part of modern systems:\nIt is Modaladjective that S\nIt is Modaladjective (for NP) to VP\nIt is Cogv-ed that S\nIt seems/appears/means/follows (that) S\nMention-detection rules are sometimes designed speci\ufb01cally for particular eval-\nuation campaigns. For OntoNotes, for example, mentions are not embedded within\nlarger mentions, and while numeric quantities are annotated, they are rarely coref-\nerential. Thus for OntoNotes tasks like CoNLL 2012 (Pradhan et al., 2012a), a\ncommon \ufb01rst pass rule-based mention detection algorithm (Lee et al., 2013) is:\n1.Take all NPs, possessive pronouns, and named entities.\n2.Remove numeric quantities (100 dollars, 8%), mentions embedded in\nlarger mentions, adjectival forms of nations, and stop words (like there ).\n3.Remove pleonastic itbased on regular expression patterns.\nRule-based systems, however, are generally insuf\ufb01cient to deal with mention-\ndetection, and so modern systems incorporate some sort of learned mention detec-\ntion component, such as a referentiality classi\ufb01er, an anaphoricity classi\ufb01er \u2014\ndetecting whether an NP is an anaphor\u2014or a discourse-new classi\ufb01er\u2014 detecting\nwhether a mention is discourse-new and a potential antecedent for a future anaphor.\nAnanaphoricity detector , for example, can draw its positive training examplesanaphoricity\ndetector\nfrom any span that is labeled as an anaphoric referring expression in hand-labeled\ndatasets like OntoNotes, ARRAU , or AnCora. Any other NP or named entity can be\nmarked as a negative training example. Anaphoricity classi\ufb01ers use features of the\ncandidate mention such as its head word, surrounding words, de\ufb01niteness, animacy,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 764,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "common \ufb01rst pass rule-based mention detection algorithm (Lee et al., 2013) is:\n1.Take all NPs, possessive pronouns, and named entities.\n2.Remove numeric quantities (100 dollars, 8%), mentions embedded in\nlarger mentions, adjectival forms of nations, and stop words (like there ).\n3.Remove pleonastic itbased on regular expression patterns.\nRule-based systems, however, are generally insuf\ufb01cient to deal with mention-\ndetection, and so modern systems incorporate some sort of learned mention detec-\ntion component, such as a referentiality classi\ufb01er, an anaphoricity classi\ufb01er \u2014\ndetecting whether an NP is an anaphor\u2014or a discourse-new classi\ufb01er\u2014 detecting\nwhether a mention is discourse-new and a potential antecedent for a future anaphor.\nAnanaphoricity detector , for example, can draw its positive training examplesanaphoricity\ndetector\nfrom any span that is labeled as an anaphoric referring expression in hand-labeled\ndatasets like OntoNotes, ARRAU , or AnCora. Any other NP or named entity can be\nmarked as a negative training example. Anaphoricity classi\ufb01ers use features of the\ncandidate mention such as its head word, surrounding words, de\ufb01niteness, animacy,\nlength, position in the sentence/discourse, many of which were \ufb01rst proposed in\nearly work by Ng and Cardie (2002a); see Section 23.5 for more on features.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 765,
      "token_count": 332,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 520",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 766,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "512 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\nReferentiality or anaphoricity detectors can be run as \ufb01lters, in which only men-\ntions that are classi\ufb01ed as anaphoric or referential are passed on to the coreference\nsystem. The end result of such a \ufb01ltering mention detection system on our example\nabove might be the following \ufb01ltered set of 9 potential mentions:\nVictoria Chen her pay she\nMegabucks Bank the 38-year-old Megabucks\nher the company Lotsabucks\nIt turns out, however, that hard \ufb01ltering of mentions based on an anaphoricity\nor referentiality classi\ufb01er leads to poor performance. If the anaphoricity classi\ufb01er\nthreshold is set too high, too many mentions are \ufb01ltered out and recall suffers. If the\nclassi\ufb01er threshold is set too low, too many pleonastic or non-referential mentions\nare included and precision suffers.\nThe modern approach is instead to perform mention detection, anaphoricity, and\ncoreference jointly in a single end-to-end model (Ng 2005b, Denis and Baldridge\n2007, Rahman and Ng 2009). For example mention detection in the Lee et al.\n(2017b),2018 system is based on a single end-to-end neural network that computes\na score for each mention being referential, a score for two mentions being corefer-\nence, and combines them to make a decision, training all these scores with a single\nend-to-end loss. We\u2019ll describe this method in detail in Section 23.6.7\nDespite these advances, correctly detecting referential mentions seems to still be\nan unsolved problem, since systems incorrectly marking pleonastic pronouns like\nitand other non-referential NPs as coreferent is a large source of errors of modern\ncoreference resolution systems (Kummerfeld and Klein 2013, Martschat and Strube\n2014, Martschat and Strube 2015, Wiseman et al. 2015, Lee et al. 2017a).\nMention, referentiality, or anaphoricity detection is thus an important open area\nof investigation. Other sources of knowledge may turn out to be helpful, especially\nin combination with unsupervised and semisupervised algorithms, which also mit-\nigate the expense of labeled datasets. In early work, for example Bean and Riloff\n(1999) learned patterns for characterizing anaphoric or non-anaphoric NPs; (by ex-\ntracting and generalizing over the \ufb01rst NPs in a text, which are guaranteed to be\nnon-anaphoric). Chang et al. (2012) look for head nouns that appear frequently in\nthe training data but never appear as gold mentions to help \ufb01nd non-referential NPs.\nBergsma et al. (2008b) use web counts as a semisupervised way to augment standard\nfeatures for anaphoricity detection for English it, an important task because itis both\ncommon and ambiguous; between a quarter and half itexamples are non-anaphoric.\nConsider the following two examples:\n(23.45) You can make [it] in advance. [anaphoric]\n(23.46) You can make [it] in Hollywood. [non-anaphoric]\nTheitinmake it is non-anaphoric, part of the idiom make it . Bergsma et al. (2008b)\nturn the context around each example into patterns, like \u201cmake * in advance\u201d from",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 767,
      "token_count": 777,
      "chapter_title": ""
    }
  },
  {
    "content": "in combination with unsupervised and semisupervised algorithms, which also mit-\nigate the expense of labeled datasets. In early work, for example Bean and Riloff\n(1999) learned patterns for characterizing anaphoric or non-anaphoric NPs; (by ex-\ntracting and generalizing over the \ufb01rst NPs in a text, which are guaranteed to be\nnon-anaphoric). Chang et al. (2012) look for head nouns that appear frequently in\nthe training data but never appear as gold mentions to help \ufb01nd non-referential NPs.\nBergsma et al. (2008b) use web counts as a semisupervised way to augment standard\nfeatures for anaphoricity detection for English it, an important task because itis both\ncommon and ambiguous; between a quarter and half itexamples are non-anaphoric.\nConsider the following two examples:\n(23.45) You can make [it] in advance. [anaphoric]\n(23.46) You can make [it] in Hollywood. [non-anaphoric]\nTheitinmake it is non-anaphoric, part of the idiom make it . Bergsma et al. (2008b)\nturn the context around each example into patterns, like \u201cmake * in advance\u201d from\n(23.45), and \u201cmake * in Hollywood\u201d from (23.46). They then use Google n-grams to\nenumerate all the words that can replace itin the patterns. Non-anaphoric contexts\ntend to only have itin the wildcard positions, while anaphoric contexts occur with\nmany other NPs (for example make them in advance is just as frequent in their data\n7Some systems try to avoid mention detection or anaphoricity detection altogether. For datasets like\nOntoNotes which don\u2019t label singletons, an alternative to \ufb01ltering out non-referential mentions is to run\ncoreference resolution, and then simply delete any candidate mentions which were not corefered with\nanother mention. This likely doesn\u2019t work as well as explicitly modeling referentiality, and cannot solve\nthe problem of detecting singletons, which is important for tasks like entity linking.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 768,
      "token_count": 465,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 521\n\n23.4 \u2022 A RCHITECTURES FOR COREFERENCE ALGORITHMS 513\nasmake it in advance , but make them in Hollywood did not occur at all). These\nn-gram contexts can be used as features in a supervised anaphoricity classi\ufb01er.\n23.4 Architectures for Coreference Algorithms\nModern systems for coreference are based on supervised neural machine learning,\nsupervised from hand-labeled datasets like OntoNotes. In this section we overview\nthe various architecture of modern systems, using the categorization of Ng (2010),\nwhich distinguishes algorithms based on whether they make each coreference deci-\nsion in a way that is entity-based \u2014representing each entity in the discourse model\u2014\nor only mention-based \u2014considering each mention independently, and whether they\nuseranking models to directly compare potential antecedents. Afterwards, we go\ninto more detail on one state-of-the-art algorithm in Section 23.6.\n23.4.1 The Mention-Pair Architecture\nWe begin with the mention-pair architecture, the simplest and most in\ufb02uential mention-pair\ncoreference architecture, which introduces many of the features of more complex\nalgorithms, even though other architectures perform better. The mention-pair ar- mention-pair\nchitecture is based around a classi\ufb01er that\u2014 as its name suggests\u2014is given a pair\nof mentions, a candidate anaphor and a candidate antecedent, and makes a binary\nclassi\ufb01cation decision: coreferring or not.\nLet\u2019s consider the task of this classi\ufb01er for the pronoun shein our example, and\nassume the slightly simpli\ufb01ed set of potential antecedents in Fig. 23.2.\nVictoria ChenMegabucks Bankingherher paythe 37-year-oldshep(coref|\u201dVictoria Chen\u201d,\u201dshe\u201d)\np(coref|\u201dMegabucks Banking\u201d,\u201dshe\u201d)\nFigure 23.2 For each pair of a mention (like she), and a potential antecedent mention (like\nVictoria Chen orher), the mention-pair classi\ufb01er assigns a probability of a coreference link.\nFor each prior mention ( Victoria Chen ,Megabucks Banking ,her, etc.), the binary\nclassi\ufb01er computes a probability: whether or not the mention is the antecedent of\nshe. We want this probability to be high for actual antecedents ( Victoria Chen ,her,\nthe 38-year-old ) and low for non-antecedents ( Megabucks Banking ,her pay ).\nEarly classi\ufb01ers used hand-built features (Section 23.5); more recent classi\ufb01ers\nuse neural representation learning (Section 23.6)\nFor training, we need a heuristic for selecting training samples; since most pairs\nof mentions in a document are not coreferent, selecting every pair would lead to\na massive overabundance of negative samples. The most common heuristic, from\n(Soon et al., 2001), is to choose the closest antecedent as a positive example, and all\npairs in between as the negative examples. More formally, for each anaphor mention\nmiwe create\n\u2022 one positive instance ( mi;mj) where mjis the closest antecedent to mi, and",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 769,
      "token_count": 702,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 522",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 770,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "514 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\n\u2022 a negative instance ( mi;mk) for each mkbetween mjandmi\nThus for the anaphor she, we would choose ( she,her) as the positive example\nand no negative examples. Similarly, for the anaphor the company we would choose\n(the company ,Megabucks ) as the positive example and ( the company ,she) (the com-\npany ,the 38-year-old ) (the company ,her pay ) and ( the company ,her) as negative\nexamples.\nOnce the classi\ufb01er is trained, it is applied to each test sentence in a clustering\nstep. For each mention iin a document, the classi\ufb01er considers each of the prior i\u00001\nmentions. In closest-\ufb01rst clustering (Soon et al., 2001), the classi\ufb01er is run right to\nleft (from mention i\u00001 down to mention 1) and the \ufb01rst antecedent with probability\n>:5 is linked to i. If no antecedent has probably >0:5, no antecedent is selected for\ni. Inbest-\ufb01rst clustering, the classi\ufb01er is run on all i\u00001 antecedents and the most\nprobable preceding mention is chosen as the antecedent for i. The transitive closure\nof the pairwise relation is taken as the cluster.\nWhile the mention-pair model has the advantage of simplicity, it has two main\nproblems. First, the classi\ufb01er doesn\u2019t directly compare candidate antecedents to\neach other, so it\u2019s not trained to decide, between two likely antecedents, which one\nis in fact better. Second, it ignores the discourse model, looking only at mentions,\nnot entities. Each classi\ufb01er decision is made completely locally to the pair, without\nbeing able to take into account other mentions of the same entity. The next two\nmodels each address one of these two \ufb02aws.\n23.4.2 The Mention-Rank Architecture\nThe mention ranking model directly compares candidate antecedents to each other,\nchoosing the highest-scoring antecedent for each anaphor.\nIn early formulations, for mention i, the classi\ufb01er decides which of the f1;:::;i\u0000\n1gprior mentions is the antecedent (Denis and Baldridge, 2008). But suppose iis\nin fact not anaphoric, and none of the antecedents should be chosen? Such a model\nwould need to run a separate anaphoricity classi\ufb01er on i. Instead, it turns out to be\nbetter to jointly learn anaphoricity detection and coreference together with a single\nloss (Rahman and Ng, 2009).\nSo in modern mention-ranking systems, for the ith mention (anaphor), we have\nan associated random variable yiranging over the values Y(i) =f1;:::;i\u00001;\u000fg. The\nvalue\u000fis a special dummy mention meaning that idoes not have an antecedent (i.e.,\nis either discourse-new and starts a new coref chain, or is non-anaphoric).\nVictoria ChenMegabucks Bankingherher paythe 37-year-oldshep(\u201dVictoria Chen\u201d|\u201dshe\u201d)\np(\u03f5|\u201dshe\u201d)\u03f5One or more of theseshould be highAll of theseshould be low}p(\u201dher pay\u201d|she\u201d)p(\u201dher\u201d|she\u201d)p(\u201dthe 37-year-old\u201d|she\u201d)\np(\u201dMegabucks Banking\u201d|she\u201d)}",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 771,
      "token_count": 785,
      "chapter_title": ""
    }
  },
  {
    "content": "1gprior mentions is the antecedent (Denis and Baldridge, 2008). But suppose iis\nin fact not anaphoric, and none of the antecedents should be chosen? Such a model\nwould need to run a separate anaphoricity classi\ufb01er on i. Instead, it turns out to be\nbetter to jointly learn anaphoricity detection and coreference together with a single\nloss (Rahman and Ng, 2009).\nSo in modern mention-ranking systems, for the ith mention (anaphor), we have\nan associated random variable yiranging over the values Y(i) =f1;:::;i\u00001;\u000fg. The\nvalue\u000fis a special dummy mention meaning that idoes not have an antecedent (i.e.,\nis either discourse-new and starts a new coref chain, or is non-anaphoric).\nVictoria ChenMegabucks Bankingherher paythe 37-year-oldshep(\u201dVictoria Chen\u201d|\u201dshe\u201d)\np(\u03f5|\u201dshe\u201d)\u03f5One or more of theseshould be highAll of theseshould be low}p(\u201dher pay\u201d|she\u201d)p(\u201dher\u201d|she\u201d)p(\u201dthe 37-year-old\u201d|she\u201d)\np(\u201dMegabucks Banking\u201d|she\u201d)}\nFigure 23.3 For each candidate anaphoric mention (like she), the mention-ranking system assigns a proba-\nbility distribution over all previous mentions plus the special dummy mention \u000f.\nAt test time, for a given mention ithe model computes one softmax over all the\nantecedents (plus \u000f) giving a probability for each candidate antecedent (or none).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 772,
      "token_count": 366,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 523",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 773,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "23.5 \u2022 C LASSIFIERS USING HAND -BUILT FEATURES 515\nFig. 23.3 shows an example of the computation for the single candidate anaphor\nshe.\nOnce the antecedent is classi\ufb01ed for each anaphor, transitive closure can be run\nover the pairwise decisions to get a complete clustering.\nTraining is trickier in the mention-ranking model than the mention-pair model,\nbecause for each anaphor we don\u2019t know which of all the possible gold antecedents\nto use for training. Instead, the best antecedent for each mention is latent ; that\nis, for each mention we have a whole cluster of legal gold antecedents to choose\nfrom. Early work used heuristics to choose an antecedent, for example choosing the\nclosest antecedent as the gold antecedent and all non-antecedents in a window of\ntwo sentences as the negative examples (Denis and Baldridge, 2008). Various kinds\nof ways to model latent antecedents exist (Fernandes et al. 2012, Chang et al. 2013,\nDurrett and Klein 2013). The simplest way is to give credit to any legal antecedent\nby summing over all of them, with a loss function that optimizes the likelihood of\nall correct antecedents from the gold clustering (Lee et al., 2017b). We\u2019ll see the\ndetails in Section 23.6.\nMention-ranking models can be implemented with hand-build features or with\nneural representation learning (which might also incorporate some hand-built fea-\ntures). we\u2019ll explore both directions in Section 23.5 and Section 23.6.\n23.4.3 Entity-based Models\nBoth the mention-pair and mention-ranking models make their decisions about men-\ntions . By contrast, entity-based models link each mention not to a previous mention\nbut to a previous discourse entity (cluster of mentions).\nA mention-ranking model can be turned into an entity-ranking model simply\nby having the classi\ufb01er make its decisions over clusters of mentions rather than\nindividual mentions (Rahman and Ng, 2009).\nFor traditional feature-based models, this can be done by extracting features over\nclusters. The size of a cluster is a useful feature, as is its \u2018shape\u2019, which is the\nlist of types of the mentions in the cluster i.e., sequences of the tokens (P)roper,\n(D)e\ufb01nite, (I)nde\ufb01nite, (Pr)onoun, so that a cluster composed of fVictoria ,her,the\n38-year-oldgwould have the shape P-Pr-D (Bj\u00a8orkelund and Kuhn, 2014). An entity-\nbased model that includes a mention-pair classi\ufb01er can use as features aggregates of\nmention-pair probabilities, for example computing the average probability of coref-\nerence over all mention-pairs in the two clusters (Clark and Manning 2015).\nNeural models can learn representations of clusters automatically, for example\nby using an RNN over the sequence of cluster mentions to encode a state correspond-\ning to a cluster representation (Wiseman et al., 2016), or by learning distributed rep-\nresentations for pairs of clusters by pooling over learned representations of mention\npairs (Clark and Manning, 2016b).\nHowever, although entity-based models are more expressive, the use of cluster-\nlevel information in practice has not led to large gains in performance, so mention-\nranking models are still more commonly used.\n23.5 Classi\ufb01ers using hand-built features",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 774,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "clusters. The size of a cluster is a useful feature, as is its \u2018shape\u2019, which is the\nlist of types of the mentions in the cluster i.e., sequences of the tokens (P)roper,\n(D)e\ufb01nite, (I)nde\ufb01nite, (Pr)onoun, so that a cluster composed of fVictoria ,her,the\n38-year-oldgwould have the shape P-Pr-D (Bj\u00a8orkelund and Kuhn, 2014). An entity-\nbased model that includes a mention-pair classi\ufb01er can use as features aggregates of\nmention-pair probabilities, for example computing the average probability of coref-\nerence over all mention-pairs in the two clusters (Clark and Manning 2015).\nNeural models can learn representations of clusters automatically, for example\nby using an RNN over the sequence of cluster mentions to encode a state correspond-\ning to a cluster representation (Wiseman et al., 2016), or by learning distributed rep-\nresentations for pairs of clusters by pooling over learned representations of mention\npairs (Clark and Manning, 2016b).\nHowever, although entity-based models are more expressive, the use of cluster-\nlevel information in practice has not led to large gains in performance, so mention-\nranking models are still more commonly used.\n23.5 Classi\ufb01ers using hand-built features\nFeature-based classi\ufb01ers, use hand-designed features in logistic regression, SVM,\nor random forest classi\ufb01ers for coreference resolution. These classi\ufb01ers don\u2019t per-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 775,
      "token_count": 329,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 524",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 776,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "516 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\nform as well as neural ones. Nonetheless, they are still sometimes useful to build\nlightweight systems when compute or data are sparse, and the features themselves\nare useful for error analysis even in neural systems.\nGiven an anaphor mention and a potential antecedent mention, feature based\nclassi\ufb01ers make use of three types of features: (i) features of the anaphor, (ii) features\nof the candidate antecedent, and (iii) features of the relationship between the pair.\nEntity-based models can make additional use of two additional classes: (iv) feature\nof all mentions from the antecedent\u2019s entity cluster, and (v) features of the relation\nbetween the anaphor and the mentions in the antecedent entity cluster.\nFeatures of the Anaphor or Antecedent Mention\nFirst (last) word Victoria/she First or last word (or embedding) of antecedent/anaphor\nHead word Victoria/she Head word (or head embedding) of antecedent/anaphor\nAttributes Sg-F-A-3-PER/\nSg-F-A-3-PERThe number, gender, animacy, person, named entity type\nattributes of (antecedent/anaphor)\nLength 2/1 length in words of (antecedent/anaphor)\nMention type P/Pr Type: (P)roper, (D)e\ufb01nite, (I)nde\ufb01nite, (Pr)onoun) of an-\ntecedent/anaphor\nFeatures of the Antecedent Entity\nEntity shape P-Pr-D The \u2018shape\u2019 or list of types of the mentions in the\nantecedent entity (cluster), i.e., sequences of (P)roper,\n(D)e\ufb01nite, (I)nde\ufb01nite, (Pr)onoun.\nEntity attributes Sg-F-A-3-PER The number, gender, animacy, person, named entity type\nattributes of the antecedent entity\nAnt. cluster size 3 Number of mentions in the antecedent cluster\nFeatures of the Pair of Mentions\nSentence distance 1 The number of sentences between antecedent and anaphor\nMention distance 4 The number of mentions between antecedent and anaphor\ni-within-i F Anaphor has i-within-i relation with antecedent\nCosine Cosine between antecedent and anaphor embeddings\nFeatures of the Pair of Entities\nExact String Match F True if the strings of any two mentions from the antecedent\nand anaphor clusters are identical.\nHead Word Match F True if any mentions from antecedent cluster has same\nheadword as any mention in anaphor cluster\nWord Inclusion F All words in anaphor cluster included in antecedent cluster\nFigure 23.4 Feature-based coreference: sample feature values for anaphor \u201cshe\u201d and potential antecedent\n\u201cVictoria Chen\u201d.\nFigure 23.4 shows a selection of commonly used features, and shows the value\nthat would be computed for the potential anaphor \u201cshe\u201d and potential antecedent\n\u201cVictoria Chen\u201d in our example sentence, repeated below:\n(23.47) Victoria Chen , CFO of Megabucks Banking, saw her pay jump to $2.3\nmillion, as the 38-year-old also became the company\u2019s president. It is\nwidely known that shecame to Megabucks from rival Lotsabucks.\nFeatures that prior work has found to be particularly useful are exact string\nmatch, entity headword agreement, mention distance, as well as (for pronouns) exact",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 777,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "i-within-i F Anaphor has i-within-i relation with antecedent\nCosine Cosine between antecedent and anaphor embeddings\nFeatures of the Pair of Entities\nExact String Match F True if the strings of any two mentions from the antecedent\nand anaphor clusters are identical.\nHead Word Match F True if any mentions from antecedent cluster has same\nheadword as any mention in anaphor cluster\nWord Inclusion F All words in anaphor cluster included in antecedent cluster\nFigure 23.4 Feature-based coreference: sample feature values for anaphor \u201cshe\u201d and potential antecedent\n\u201cVictoria Chen\u201d.\nFigure 23.4 shows a selection of commonly used features, and shows the value\nthat would be computed for the potential anaphor \u201cshe\u201d and potential antecedent\n\u201cVictoria Chen\u201d in our example sentence, repeated below:\n(23.47) Victoria Chen , CFO of Megabucks Banking, saw her pay jump to $2.3\nmillion, as the 38-year-old also became the company\u2019s president. It is\nwidely known that shecame to Megabucks from rival Lotsabucks.\nFeatures that prior work has found to be particularly useful are exact string\nmatch, entity headword agreement, mention distance, as well as (for pronouns) exact\nattribute match and i-within-i, and (for nominals and proper names) word inclusion\nand cosine. For lexical features (like head words) it is common to only use words\nthat appear enough times ( >20 times).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 778,
      "token_count": 332,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 525",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 779,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "23.6 \u2022 A NEURAL MENTION -RANKING ALGORITHM 517\nIt is crucial in feature-based systems to use conjunctions of features; one exper-\niment suggested that moving from individual features in a classi\ufb01er to conjunctions\nof multiple features increased F1 by 4 points (Lee et al., 2017a). Speci\ufb01c conjunc-\ntions can be designed by hand (Durrett and Klein, 2013), all pairs of features can be\nconjoined (Bengtson and Roth, 2008), or feature conjunctions can be learned using\ndecision tree or random forest classi\ufb01ers (Ng and Cardie 2002a, Lee et al. 2017a).\nFeatures can also be used in neural models as well. Neural systems use contex-\ntual word embeddings so don\u2019t bene\ufb01t from shallow features like string match or or\nmention types. However features like mention length, distance between mentions,\nor genre can complement neural contextual embedding models.\n23.6 A neural mention-ranking algorithm\nIn this section we describe the neural e2e-coref algorithms of Lee et al. (2017b)\n(simpli\ufb01ed and extended a bit, drawing on Joshi et al. (2019) and others). This is\namention-ranking algorithm that considers all possible spans of text in the docu-\nment, assigns a mention-score to each span, prunes the mentions based on this score,\nthen assigns coreference links to the remaining mentions.\nMore formally, given a document Dwith Twords, the model considers all of\ntheT(T+1)\n2text spans in D(unigrams, bigrams, trigrams, 4-grams, etc; in practice\nwe only consider spans up a maximum length around 10). The task is to assign\nto each span ian antecedent yi, a random variable ranging over the values Y(i) =\nf1;:::;i\u00001;\u000fg; each previous span and a special dummy token \u000f. Choosing the\ndummy token means that idoes not have an antecedent, either because iis discourse-\nnew and starts a new coreference chain, or because iis non-anaphoric.\nFor each pair of spans iand j, the system assigns a score s(i;j)for the coref-\nerence link between span iand span j. The system then learns a distribution P(yi)\nover the antecedents for span i:\nP(yi) =exp(s(i;yi))P\ny02Y(i)exp(s(i;y0))(23.48)\nThis score s(i;j)includes three factors that we\u2019ll de\ufb01ne below: m(i); whether span\niis a mention; m(j); whether span jis a mention; and c(i;j); whether jis the\nantecedent of i:\ns(i;j) =m(i)+m(j)+c(i;j) (23.49)\nFor the dummy antecedent \u000f, the score s(i;\u000f)is \ufb01xed to 0. This way if any non-\ndummy scores are positive, the model predicts the highest-scoring antecedent, but if\nall the scores are negative it abstains.\n23.6.1 Computing span representations\nTo compute the two functions m(i)andc(i;j)which score a span ior a pair of spans\n(i;j), we\u2019ll need a way to represent a span. The e2e-coref family of algorithms\nrepresents each span by trying to capture 3 words/tokens: the \ufb01rst word, the last",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 780,
      "token_count": 763,
      "chapter_title": ""
    }
  },
  {
    "content": "For each pair of spans iand j, the system assigns a score s(i;j)for the coref-\nerence link between span iand span j. The system then learns a distribution P(yi)\nover the antecedents for span i:\nP(yi) =exp(s(i;yi))P\ny02Y(i)exp(s(i;y0))(23.48)\nThis score s(i;j)includes three factors that we\u2019ll de\ufb01ne below: m(i); whether span\niis a mention; m(j); whether span jis a mention; and c(i;j); whether jis the\nantecedent of i:\ns(i;j) =m(i)+m(j)+c(i;j) (23.49)\nFor the dummy antecedent \u000f, the score s(i;\u000f)is \ufb01xed to 0. This way if any non-\ndummy scores are positive, the model predicts the highest-scoring antecedent, but if\nall the scores are negative it abstains.\n23.6.1 Computing span representations\nTo compute the two functions m(i)andc(i;j)which score a span ior a pair of spans\n(i;j), we\u2019ll need a way to represent a span. The e2e-coref family of algorithms\nrepresents each span by trying to capture 3 words/tokens: the \ufb01rst word, the last\nword, and the most important word. We \ufb01rst run each paragraph or subdocument\nthrough an encoder (like BERT) to generate embeddings hifor each token i. The\nspan iis then represented by a vector githat is a concatenation of the encoder output",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 781,
      "token_count": 350,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 526\n\n518 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\nembedding for the \ufb01rst (start) token of the span, the encoder output for the last (end)\ntoken of the span, and a third vector which is an attention-based representation:\ngi= [hSTART (i);hEND (i);hATT(i)] (23.50)\nThe goal of the attention vector is to represent which word/token is the likely\nsyntactic head-word of the span; we saw in the prior section that head-words are\na useful feature; a matching head-word is a good indicator of coreference. The\nattention representation is computed as usual; the system learns a weight vector wa,\nand computes its dot product with the hidden state httransformed by a FFN:\nat=wa\u0001FFNa(ht) (23.51)\nThe attention score is normalized into a distribution via a softmax:\nai;t=exp(at)\nPEND (i)\nk=START (i)exp(ak)(23.52)\nAnd then the attention distribution is used to create a vector hATT(i)which is an\nattention-weighted sum of the embeddings etof each of the words in span i:\nhATT(i)=END (i)X\nt=START (i)ai;t\u0001et (23.53)\nFig. 23.5 shows the computation of the span representation and the mention\nscore.\nEncodings (h)\u2026Encoder GeneralElectricsaidthePostalServicecontactedthecompanySpan head (hATT) Span representation (g) Mention score (m)+++++General ElectricElectric said thethe Postal ServiceService contacted thethe company\nFigure 23.5 Computation of the span representation g(and the mention score m) in a BERT version of the\ne2e-coref model (Lee et al. 2017b, Joshi et al. 2019). The model considers all spans up to a maximum width of\nsay 10; the \ufb01gure shows a small subset of the bigram and trigram spans.\n23.6.2 Computing the mention and antecedent scores mandc\nNow that we know how to compute the vector gifor representing span i, we can\nsee the details of the two scoring functions m(i)andc(i;j). Both are computed by\nfeedforward networks:\nm(i) = wm\u0001FFN m(gi) (23.54)\nc(i;j) = wc\u0001FFN c([gi;gj;gi\u000egj;]) (23.55)\nAt inference time, this mention score mis used as a \ufb01lter to keep only the best few\nmentions.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 782,
      "token_count": 561,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 527\n\n23.6 \u2022 A NEURAL MENTION -RANKING ALGORITHM 519\nWe then compute the antecedent score for high-scoring mentions. The antecedent\nscore c(i;j)takes as input a representation of the spans iandj, but also the element-\nwise similarity of the two spans to each other gi\u000egj(here\u000eis element-wise mul-\ntiplication). Fig. 23.6 shows the computation of the score sfor the three possible\nantecedents of the company in the example sentence from Fig. 23.5.\nFigure 23.6 The computation of the score sfor the three possible antecedents of the com-\npany in the example sentence from Fig. 23.5. Figure after Lee et al. (2017b).\nGiven the set of mentions, the joint distribution of antecedents for each docu-\nment is computed in a forward pass, and we can then do transitive closure on the\nantecedents to create a \ufb01nal clustering for the document.\nFig. 23.7 shows example predictions from the model, showing the attention\nweights, which Lee et al. (2017b) \ufb01nd correlate with traditional semantic heads.\nNote that the model gets the second example wrong, presumably because attendants\nandpilot likely have nearby word embeddings.\nFigure 23.7 Sample predictions from the Lee et al. (2017b) model, with one cluster per\nexample, showing one correct example and one mistake. Bold, parenthesized spans are men-\ntions in the predicted cluster. The amount of red color on a word indicates the head-\ufb01nding\nattention weight ai;tin Eq. 23.52. Figure adapted from Lee et al. (2017b).\n23.6.3 Learning\nFor training, we don\u2019t have a single gold antecedent for each mention; instead the\ncoreference labeling only gives us each entire cluster of coreferent mentions; so a\nmention only has a latent antecedent. We therefore use a loss function that maxi-\nmizes the sum of the coreference probability of any of the legal antecedents. For a\ngiven mention iwith possible antecedents Y(i), let GOLD (i) be the set of mentions\nin the gold cluster containing i. Since the set of mentions occurring before iisY(i),\nthe set of mentions in that gold cluster that also occur before iisY(i)\\GOLD (i). We",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 783,
      "token_count": 522,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 528\n\n520 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\ntherefore want to maximize:\nX\n\u02c6y2Y(i)\\GOLD (i)P(\u02c6y) (23.56)\nIf a mention iis not in a gold cluster GOLD (i) =\u000f.\nTo turn this probability into a loss function, we\u2019ll use the cross-entropy loss\nfunction we de\ufb01ned in Eq. 5.23 in Chapter 5, by taking the \u0000log of the probability.\nIf we then sum over all mentions, we get the \ufb01nal loss function for training:\nL=NX\ni=2\u0000logX\n\u02c6y2Y(i)\\GOLD (i)P(\u02c6y) (23.57)\n23.7 Entity Linking\nEntity linking is the task of associating a mention in text with the representation of entity linking\nsome real-world entity in an ontology or knowledge base (Ji and Grishman, 2011). It\nis the natural follow-on to coreference resolution; coreference resolution is the task\nof associating textual mentions that corefer to the same entity. Entity linking takes\nthe further step of identifying who that entity is. It is especially important for any\nNLP task that links to a knowledge base.\nWhile there are all sorts of potential knowledge-bases, we\u2019ll focus in this section\non Wikipedia, since it\u2019s widely used as an ontology for NLP tasks. In this usage,\neach unique Wikipedia page acts as the unique id for a particular entity. This task of\ndeciding which Wikipedia page corresponding to an individual is being referred to\nby a text mention has its own name: wiki\ufb01cation (Mihalcea and Csomai, 2007). wiki\ufb01cation\nSince the earliest systems (Mihalcea and Csomai 2007, Cucerzan 2007, Milne\nand Witten 2008), entity linking is done in (roughly) two stages: mention detec-\ntion andmention disambiguation . We\u2019ll give two algorithms, one simple classic\nbaseline that uses anchor dictionaries and information from the Wikipedia graph\nstructure (Ferragina and Scaiella, 2011) and one modern neural algorithm (Li et al.,\n2020). We\u2019ll focus here mainly on the application of entity linking to questions,\nsince a lot of the literature has been in that context.\n23.7.1 Linking based on Anchor Dictionaries and Web Graph\nAs a simple baseline we introduce the T AGME linker (Ferragina and Scaiella, 2011)\nfor Wikipedia, which itself draws on earlier algorithms (Mihalcea and Csomai 2007,\nCucerzan 2007, Milne and Witten 2008). Wiki\ufb01cation algorithms de\ufb01ne the set of\nentities as the set of Wikipedia pages, so we\u2019ll refer to each Wikipedia page as a\nunique entity e. T AGME \ufb01rst creates a catalog of all entities (i.e. all Wikipedia\npages, removing some disambiguation and other meta-pages) and indexes them in a\nstandard IR engine like Lucene. For each page e, the algorithm computes an in-link\ncount in (e): the total number of in-links from other Wikipedia pages that point to e.\nThese counts can be derived from Wikipedia dumps.\nFinally, the algorithm requires an anchor dictionary . An anchor dictionary\nlists for each Wikipedia page, its anchor texts : the hyperlinked spans of text on anchor texts\nother pages that point to it. For example, the web page for Stanford University,\nhttp://www.stanford.edu , might be pointed to from another page using anchor\ntexts like Stanford orStanford University :",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 784,
      "token_count": 788,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 529",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 785,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "23.7 \u2022 E NTITY LINKING 521\n<a href=\"http://www.stanford.edu\">Stanford University</a>\nWe compute a Wikipedia anchor dictionary by including, for each Wikipedia\npage e,e\u2019s title as well as all the anchor texts from all Wikipedia pages that point to e.\nFor each anchor string awe\u2019ll also compute its total frequency freq (a)in Wikipedia\n(including non-anchor uses), the number of times aoccurs as a link (which we\u2019ll call\nlink(a)), and its link probability linkprob (a) =link(a)=freq(a). Some cleanup of the\n\ufb01nal anchor dictionary is required, for example removing anchor strings composed\nonly of numbers or single characters, that are very rare, or that are very unlikely to\nbe useful entities because they have a very low linkprob.\nMention Detection Given a question (or other text we are trying to link), T AGME\ndetects mentions by querying the anchor dictionary for each token sequence up to\n6 words. This large set of sequences is pruned with some simple heuristics (for\nexample pruning substrings if they have small linkprobs). The question:\nWhen was Ada Lovelace born?\nmight give rise to the anchor Ada Lovelace and possibly Ada, but substrings spans\nlikeLovelace might be pruned as having too low a linkprob, and but spans like born\nhave such a low linkprob that they would not be in the anchor dictionary at all.\nMention Disambiguation If a mention span is unambiguous (points to only one\nentity/Wikipedia page), we are done with entity linking! However, many spans are\nambiguous, matching anchors for multiple Wikipedia entities/pages. The T AGME\nalgorithm uses two factors for disambiguating ambiguous spans, which have been\nreferred to as prior probability andrelatedness/coherence . The \ufb01rst factor is p(eja),\nthe probability with which the span refers to a particular entity. For each page e2\nE(a), the probability p(eja)that anchor apoints to e, is the ratio of the number of\nlinks into ewith anchor text ato the total number of occurrences of aas an anchor:\nprior(a!e) = p(eja) =count (a!e)\nlink(a)(23.58)\nLet\u2019s see how that factor works in linking entities in the following question:\nWhat Chinese Dynasty came before the Yuan?\nThe most common association for the span Yuan in the anchor dictionary is the name\nof the Chinese currency, i.e., the probability p(Yuancurrencyjyuan)is very high.\nRarer Wikipedia associations for Yuan include the common Chinese last name, a\nlanguage spoken in Thailand, and the correct entity in this case, the name of the\nChinese dynasty. So if we chose based only on p(eja), we would make the wrong\ndisambiguation and miss the correct link, Yuandynasty .\nTo help in just this sort of case, T AGME uses a second factor, the relatedness of\nthis entity to other entities in the input question. In our example, the fact that the\nquestion also contains the span Chinese Dynasty , which has a high probability link to\nthe pageDynasties inChinese history , ought to help match Yuandynasty .\nLet\u2019s see how this works. Given a question q, for each candidate anchors span\nadetected in q, we assign a relatedness score to each possible entity e2E(a)ofa.\nThe relatedness score of the link a!eis the weighted average relatedness between\neand all other entities in q. Two entities are considered related to the extent their",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 786,
      "token_count": 766,
      "chapter_title": ""
    }
  },
  {
    "content": "link(a)(23.58)\nLet\u2019s see how that factor works in linking entities in the following question:\nWhat Chinese Dynasty came before the Yuan?\nThe most common association for the span Yuan in the anchor dictionary is the name\nof the Chinese currency, i.e., the probability p(Yuancurrencyjyuan)is very high.\nRarer Wikipedia associations for Yuan include the common Chinese last name, a\nlanguage spoken in Thailand, and the correct entity in this case, the name of the\nChinese dynasty. So if we chose based only on p(eja), we would make the wrong\ndisambiguation and miss the correct link, Yuandynasty .\nTo help in just this sort of case, T AGME uses a second factor, the relatedness of\nthis entity to other entities in the input question. In our example, the fact that the\nquestion also contains the span Chinese Dynasty , which has a high probability link to\nthe pageDynasties inChinese history , ought to help match Yuandynasty .\nLet\u2019s see how this works. Given a question q, for each candidate anchors span\nadetected in q, we assign a relatedness score to each possible entity e2E(a)ofa.\nThe relatedness score of the link a!eis the weighted average relatedness between\neand all other entities in q. Two entities are considered related to the extent their\nWikipedia pages share many in-links. More formally, the relatedness between two\nentities AandBis computed as\nrel(A;B) =log(max(jin(A)j;jin(B)j))\u0000log(jin(A)\\in(B)j)\nlog(jWj)\u0000log(min(jin(A)j;jin(B)j))(23.59)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 787,
      "token_count": 368,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 530\n\n522 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\nwhere in (x)is the set of Wikipedia pages pointing to xandWis the set of all Wiki-\npedia pages in the collection.\nThe vote given by anchor bto the candidate annotation a!Xis the average,\nover all the possible entities of b, of their relatedness to X, weighted by their prior\nprobability:\nvote(b;X) =1\njE(b)jX\nY2E(b)rel(X;Y)p(Yjb) (23.60)\nThe total relatedness score for a!Xis the sum of the votes of all the other anchors\ndetected in q:\nrelatedness (a!X) =X\nb2Xqnavote(b;X) (23.61)\nTo score a!X, we combine relatedness and prior by choosing the entity X\nthat has the highest relatedness (a!X), \ufb01nding other entities within a small \u000fof\nthis value, and from this set, choosing the entity with the highest prior P(Xja). The\nresult of this step is a single entity assigned to each span in q.\nThe T AGME algorithm has one further step of pruning spurious anchor/entity\npairs, assigning a score averaging link probability with the coherence.\ncoherence (a!X) =1\njSj\u00001X\nB2SnXrel(B;X)\nscore(a!X) =coherence (a!X)+linkprob (a)\n2(23.62)\nFinally, pairs are pruned if score (a!X)<l, where the threshold lis set on a\nheld-out set.\n23.7.2 Neural Graph-based linking\nMore recent entity linking models are based on bi-encoders , encoding a candidate\nmention span, encoding an entity, and computing the dot product between the en-\ncodings. This allows embeddings for all the entities in the knowledge base to be\nprecomputed and cached (Wu et al., 2020). Let\u2019s sketch the ELQ linking algorithm\nof Li et al. (2020), which is given a question qand a set of candidate entities from\nWikipedia with associated Wikipedia text, and outputs tuples (e;ms;me)of entity id,\nmention start, and mention end. As Fig. 23.8 shows, it does this by encoding each\nWikipedia entity using text from Wikipedia, encoding each mention span using text\nfrom the question, and computing their similarity, as we describe below.\nEntity Mention Detection To get an h-dimensional embedding for each question\ntoken, the algorithm runs the question through BERT in the normal way:\n[q1\u0001\u0001\u0001qn] =BERT ([CLS]q1\u0001\u0001\u0001qn[SEP]) (23.63)\nIt then computes the likelihood of each span [i;j]inqbeing an entity mention, in\na way similar to the span-based algorithm we saw for the reader above. First we\ncompute the score for i/jbeing the start/end of a mention:\nsstart(i) =wstart\u0001qi;send(j) =wend\u0001qj; (23.64)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 788,
      "token_count": 663,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 531\n\n23.7 \u2022 E NTITY LINKING 523\nFigure 23.8 A sketch of the inference process in the ELQ algorithm for entity linking in\nquestions (Li et al., 2020). Each candidate question mention span and candidate entity are\nseparately encoded, and then scored by the entity/span dot product.\nwhere wstartandwendare vectors learned during training. Next, another trainable\nembedding, wmention is used to compute a score for each token being part of a men-\ntion:\nsmention (t) =wmention\u0001qt (23.65)\nMention probabilities are then computed by combining these three scores:\np([i;j]) =s \nsstart(i)+send(j)+jX\nt=ismention (t)!\n(23.66)\nEntity Linking To link mentions to entities, we next compute embeddings for\neach entity in the set E=e1;\u0001\u0001\u0001;ei;\u0001\u0001\u0001;ewof all Wikipedia entities. For each en-\ntityeiwe\u2019ll get text from the entity\u2019s Wikipedia page, the title t(ei)and the \ufb01rst\n128 tokens of the Wikipedia page which we\u2019ll call the description d(ei). This is\nagain run through BERT, taking the output of the CLStoken BERT [CLS] as the entity\nrepresentation:\nxei=BERT [CLS]([CLS]t(ei)[ENT]d(ei)[SEP]) (23.67)\nMention spans can be linked to entities by computing, for each entity eand span\n[i;j], the dot product similarity between the span encoding (the average of the token\nembeddings) and the entity encoding.\nyi;j=1\n(j\u0000i+1)jX\nt=iqt\ns(e;[i;j]) =x\u0001\neyi;j (23.68)\nFinally, we take a softmax to get a distribution over entities for each span:\np(ej[i;j]) =exp(s(e;[i;j]))P\ne02Eexp(s(e0;[i;j]))(23.69)\nTraining The ELQ mention detection and entity linking algorithm is fully super-\nvised. This means, unlike the anchor dictionary algorithms from Section 23.7.1,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 789,
      "token_count": 471,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 532",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 790,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "524 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\nit requires datasets with entity boundaries marked and linked. Two such labeled\ndatasets are WebQuestionsSP (Yih et al., 2016), an extension of the WebQuestions\n(Berant et al., 2013) dataset derived from Google search questions, and GraphQues-\ntions (Su et al., 2016). Both have had entity spans in the questions marked and\nlinked (Sorokin and Gurevych 2018, Li et al. 2020) resulting in entity-labeled ver-\nsions WebQSP ELand GraphQ EL(Li et al., 2020).\nGiven a training set, the ELQ mention detection and entity linking phases are\ntrained jointly, optimizing the sum of their losses. The mention detection loss is\na binary cross-entropy loss, with Lthe length of the passage and Nthe number of\ncandidates:\nLMD=\u00001\nNX\n1\u0014i\u0014j\u0014min(i+L\u00001;n)\u0000\ny[i;j]logp([i;j])+( 1\u0000y[i;j])log(1\u0000p([i;j]))\u0001\n(23.70)\nwith y[i;j]=1 if[i;j]is a gold mention span, else 0. The entity linking loss is:\nLED=\u0000logp(egj[i;j]) (23.71)\nwhere egis the gold entity for mention [i;j].\n23.8 Evaluation of Coreference Resolution\nWe evaluate coreference algorithms model-theoretically, comparing a set of hypoth-\nesischains or clusters Hproduced by the system against a set of gold or reference\nchains or clusters Rfrom a human labeling, and reporting precision and recall.\nHowever, there are a wide variety of methods for doing this comparison. In fact,\nthere are 5 common metrics used to evaluate coreference algorithms: the link based\nMUC (Vilain et al., 1995) and BLANC (Recasens and Hovy 2011, Luo et al. 2014)\nmetrics, the mention based B3metric (Bagga and Baldwin, 1998), the entity based\nCEAF metric (Luo, 2005), and the link based entity aware LEA metric (Moosavi and\nStrube, 2016).\nLet\u2019s just explore two of the metrics. The MUC F-measure (Vilain et al., 1995)MUC\nF-measure\nis based on the number of coreference links (pairs of mentions) common to Hand\nR. Precision is the number of common links divided by the number of links in H.\nRecall is the number of common links divided by the number of links in R; This\nmakes MUC biased toward systems that produce large chains (and fewer entities),\nand it ignores singletons, since they don\u2019t involve links.\nB3is mention-based rather than link-based. For each mention in the reference B3\nchain, we compute a precision and recall, and then we take a weighted sum over all\nNmentions in the document to compute a precision and recall for the entire task. For\na given mention i, letRbe the reference chain that includes i, and Hthe hypothesis\nchain that has i. The set of correct mentions in HisH\\R. Precision for mention i\nis thusjH\\Rj\njHj, and recall for mention ithusjH\\Rj\njRj. The total precision is the weighted\nsum of the precision for mention i, weighted by a weight wi. The total recall is the\nweighted sum of the recall for mention i, weighted by a weight wi. Equivalently:\nPrecision =NX",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 791,
      "token_count": 778,
      "chapter_title": ""
    }
  },
  {
    "content": "Strube, 2016).\nLet\u2019s just explore two of the metrics. The MUC F-measure (Vilain et al., 1995)MUC\nF-measure\nis based on the number of coreference links (pairs of mentions) common to Hand\nR. Precision is the number of common links divided by the number of links in H.\nRecall is the number of common links divided by the number of links in R; This\nmakes MUC biased toward systems that produce large chains (and fewer entities),\nand it ignores singletons, since they don\u2019t involve links.\nB3is mention-based rather than link-based. For each mention in the reference B3\nchain, we compute a precision and recall, and then we take a weighted sum over all\nNmentions in the document to compute a precision and recall for the entire task. For\na given mention i, letRbe the reference chain that includes i, and Hthe hypothesis\nchain that has i. The set of correct mentions in HisH\\R. Precision for mention i\nis thusjH\\Rj\njHj, and recall for mention ithusjH\\Rj\njRj. The total precision is the weighted\nsum of the precision for mention i, weighted by a weight wi. The total recall is the\nweighted sum of the recall for mention i, weighted by a weight wi. Equivalently:\nPrecision =NX\ni=1wi# of correct mentions in hypothesis chain containing entity i\n# of mentions in hypothesis chain containing entity i\nRecall =NX\ni=1wi# of correct mentions in hypothesis chain containing entity i\n# of mentions in reference chain containing entity i",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 792,
      "token_count": 349,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 533\n\n23.9 \u2022 W INOGRAD SCHEMA PROBLEMS 525\nThe weight wifor each entity can be set to different values to produce different\nversions of the algorithm.\nFollowing a proposal from Denis and Baldridge (2009), the CoNLL coreference\ncompetitions were scored based on the average of MUC, CEAF-e, and B3(Pradhan\net al. 2011, Pradhan et al. 2012b), and so it is common in many evaluation campaigns\nto report an average of these 3 metrics. See Luo and Pradhan (2016) for a detailed\ndescription of the entire set of metrics; reference implementations of these should\nbe used rather than attempting to reimplement from scratch (Pradhan et al., 2014).\nAlternative metrics have been proposed that deal with particular coreference do-\nmains or tasks. For example, consider the task of resolving mentions to named\nentities (persons, organizations, geopolitical entities), which might be useful for in-\nformation extraction or knowledge base completion. A hypothesis chain that cor-\nrectly contains all the pronouns referring to an entity, but has no version of the name\nitself, or is linked with a wrong name, is not useful for this task. We might instead\nwant a metric that weights each mention by how informative it is (with names being\nmost informative) (Chen and Ng, 2013) or a metric that considers a hypothesis to\nmatch a gold chain only if it contains at least one variant of a name (the NEC F1\nmetric of Agarwal et al. (2019)).\n23.9 Winograd Schema problems\nFrom early on in the \ufb01eld, researchers have noted that some cases of coreference\nare quite dif\ufb01cult, seeming to require world knowledge or sophisticated reasoning\nto solve. The problem was most famously pointed out by Winograd (1972) with the\nfollowing example:\n(23.72) The city council denied the demonstrators a permit because\na. they feared violence.\nb. they advocated violence.\nWinograd noticed that the antecedent that most readers preferred for the pro-\nnoun they in continuation (a) was the city council , but in (b) was the demonstrators .\nHe suggested that this requires understanding that the second clause is intended\nas an explanation of the \ufb01rst clause, and also that our cultural frames suggest that\ncity councils are perhaps more likely than demonstrators to fear violence and that\ndemonstrators might be more likely to advocate violence.\nIn an attempt to get the \ufb01eld of NLP to focus more on methods involving world\nknowledge and common-sense reasoning, Levesque (2011) proposed a challenge\ntask called the Winograd Schema Challenge .8The problems in the challenge taskWinograd\nschema\nare coreference problems designed to be easily disambiguated by the human reader,\nbut hopefully not solvable by simple techniques such as selectional restrictions, or\nother basic word association methods.\nThe problems are framed as a pair of statements that differ in a single word or\nphrase, and a coreference question:\n(23.73) The trophy didn\u2019t \ufb01t into the suitcase because it was too large .\nQuestion: What was too large ? Answer: The trophy\n8Levesque\u2019s call was quickly followed up by Levesque et al. (2012) and Rahman and Ng (2012), a\ncompetition at the IJCAI conference (Davis et al., 2017), and a natural language inference version of the\nproblem called WNLI (Wang et al., 2018a).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 793,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 534\n\n526 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\n(23.74) The trophy didn\u2019t \ufb01t into the suitcase because it was too small .\nQuestion: What was too small ? Answer: The suitcase\nThe problems have the following characteristics:\n1. The problems each have two parties\n2. A pronoun preferentially refers to one of the parties, but could grammatically\nalso refer to the other\n3. A question asks which party the pronoun refers to\n4. If one word in the question is changed, the human-preferred answer changes\nto the other party\nThe kind of world knowledge that might be needed to solve the problems can\nvary. In the trophy/suitcase example, it is knowledge about the physical world; that\na bigger object cannot \ufb01t into a smaller object. In the original Winograd sentence,\nit is stereotypes about social actors like politicians and protesters. In examples like\nthe following, it is knowledge about human actions like turn-taking or thanking.\n(23.75) Bill passed the gameboy to John because his turn was [over/next]. Whose\nturn was [over/next]? Answers: Bill/John\n(23.76) Joan made sure to thank Susan for all the help she had [given/received].\nWho had [given/received] help? Answers: Susan/Joan.\nAlthough the Winograd Schema was designed to require common-sense rea-\nsoning, a large percentage of the original set of problems can be solved by pre-\ntrained language models, \ufb01ne-tuned on Winograd Schema sentences (Kocijan et al.,\n2019). Large pretrained language models encode an enormous amount of world or\ncommon-sense knowledge! The current trend is therefore to propose new datasets\nwith increasingly dif\ufb01cult Winograd-like coreference resolution problems like K NOW REF\n(Emami et al., 2019), with examples like:\n(23.77) Marcus is undoubtedly faster than Jarrett right now but in [his] prime the\ngap wasn\u2019t all that big.\nIn the end, it seems likely that some combination of language modeling and knowl-\nedge will prove fruitful; indeed, it seems that knowledge-based models over\ufb01t less\nto lexical idiosyncracies in Winograd Schema training sets (Trichelair et al., 2018),\n23.10 Gender Bias in Coreference\nAs with other aspects of language processing, coreference models exhibit gender and\nother biases (Zhao et al. 2018a, Rudinger et al. 2018, Webster et al. 2018). For exam-\nple the WinoBias dataset (Zhao et al., 2018a) uses a variant of the Winograd Schema\nparadigm to test the extent to which coreference algorithms are biased toward link-\ning gendered pronouns with antecedents consistent with cultural stereotypes. As we\nsummarized in Chapter 6, embeddings replicate societal biases in their training test,\nsuch as associating men with historically sterotypical male occupations like doctors,\nand women with stereotypical female occupations like secretaries (Caliskan et al.\n2017, Garg et al. 2018).\nA WinoBias sentence contain two mentions corresponding to stereotypically-\nmale and stereotypically-female occupations and a gendered pronoun that must be\nlinked to one of them. The sentence cannot be disambiguated by the gender of the\npronoun, but a biased model might be distracted by this cue. Here is an example\nsentence:",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 794,
      "token_count": 744,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 535\n\n23.11 \u2022 S UMMARY 527\n(23.78) The secretary called the physician iand told him iabout a new patient\n[pro-stereotypical]\n(23.79) The secretary called the physician iand told her iabout a new patient\n[anti-stereotypical]\nZhao et al. (2018a) consider a coreference system to be biased if it is more accu-\nrate at linking pronouns consistent with gender stereotypical occupations (e.g., him\nwith physician in (23.78)) than linking pronouns inconsistent with gender-stereotypical\noccupations (e.g., herwith physician in (23.79)). They show that coreference sys-\ntems of all architectures (rule-based, feature-based machine learned, and end-to-\nend-neural) all show signi\ufb01cant bias, performing on average 21 F 1points worse in\nthe anti-stereotypical cases.\nOne possible source of this bias is that female entities are signi\ufb01cantly un-\nderrepresented in the OntoNotes dataset, used to train most coreference systems.\nZhao et al. (2018a) propose a way to overcome this bias: they generate a second\ngender-swapped dataset in which all male entities in OntoNotes are replaced with\nfemale ones and vice versa, and retrain coreference systems on the combined orig-\ninal and swapped OntoNotes data, also using debiased GloVE embeddings (Boluk-\nbasi et al., 2016). The resulting coreference systems no longer exhibit bias on the\nWinoBias dataset, without signi\ufb01cantly impacting OntoNotes coreference accuracy.\nIn a follow-up paper, Zhao et al. (2019) show that the same biases exist in ELMo\ncontextualized word vector representations and coref systems that use them. They\nshowed that retraining ELMo with data augmentation again reduces or removes bias\nin coreference systems on WinoBias.\nWebster et al. (2018) introduces another dataset, GAP, and the task of Gendered\nPronoun Resolution as a tool for developing improved coreference algorithms for\ngendered pronouns. GAP is a gender-balanced labeled corpus of 4,454 sentences\nwith gendered ambiguous pronouns (by contrast, only 20% of the gendered pro-\nnouns in the English OntoNotes training data are feminine). The examples were\ncreated by drawing on naturally occurring sentences from Wikipedia pages to create\nhard to resolve cases with two named entities of the same gender and an ambiguous\npronoun that may refer to either person (or neither), like the following:\n(23.80) In May, Fujisawa joined Mari Motohashi\u2019s rink as the team\u2019s skip, moving\nback from Karuizawa to Kitami where shehad spent her junior days.\nWebster et al. (2018) show that modern coreference algorithms perform signif-\nicantly worse on resolving feminine pronouns than masculine pronouns in GAP.\nKurita et al. (2019) shows that a system based on BERT contextualized word repre-\nsentations shows similar bias.\n23.11 Summary\nThis chapter introduced the task of coreference resolution .\n\u2022 This is the task of linking together mentions in text which corefer , i.e. refer\nto the same discourse entity in the discourse model , resulting in a set of\ncoreference chains (also called clusters orentities ).\n\u2022 Mentions can be de\ufb01nite NPs orinde\ufb01nite NPs ,pronouns (including zero\npronouns ) ornames .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 795,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 536\n\n528 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\n\u2022 The surface form of an entity mention is linked to its information status\n(new,old, orinferrable ), and how accessible orsalient the entity is.\n\u2022 Some NPs are not referring expressions, such as pleonastic itinIt is raining .\n\u2022 Many corpora have human-labeled coreference annotations that can be used\nfor supervised learning, including OntoNotes for English, Chinese, and Ara-\nbic, ARRAU for English, and AnCora for Spanish and Catalan.\n\u2022 Mention detection can start with all nouns and named entities and then use\nanaphoricity classi\ufb01ers orreferentiality classi\ufb01ers to \ufb01lter out non-mentions.\n\u2022 Three common architectures for coreference are mention-pair ,mention-rank ,\nandentity-based , each of which can make use of feature-based or neural clas-\nsi\ufb01ers.\n\u2022 Modern coreference systems tend to be end-to-end, performing mention de-\ntection and coreference in a single end-to-end architecture.\n\u2022 Algorithms learn representations for text spans and heads, and learn to com-\npare anaphor spans with candidate antecedent spans.\n\u2022 Entity linking is the task of associating a mention in text with the representa-\ntion of some real-world entity in an ontology .\n\u2022 Coreference systems are evaluated by comparing with gold entity labels using\nprecision/recall metrics like MUC ,B3,CEAF ,BLANC , orLEA .\n\u2022 The Winograd Schema Challenge problems are dif\ufb01cult coreference prob-\nlems that seem to require world knowledge or sophisticated reasoning to solve.\n\u2022 Coreference systems exhibit gender bias which can be evaluated using datasets\nlike Winobias and GAP.\nBibliographical and Historical Notes\nCoreference has been part of natural language processing since the 1970s (Woods\net al. 1972, Winograd 1972). The discourse model and the entity-centric foundation\nof coreference was formulated by Karttunen (1969) (at the 3rd COLING confer-\nence), playing a role also in linguistic semantics (Heim 1982, Kamp 1981). But\nit was Bonnie Webber\u2019s 1978 dissertation and following work (Webber 1983) that\nexplored the model\u2019s computational aspects, providing fundamental insights into\nhow entities are represented in the discourse model and the ways in which they can\nlicense subsequent reference. Many of the examples she provided continue to chal-\nlenge theories of reference to this day.\nThe Hobbs algorithm9is a tree-search algorithm that was the \ufb01rst in a longHobbs\nalgorithm\nseries of syntax-based methods for identifying reference robustly in naturally occur-\nring text. The input to the Hobbs algorithm is a pronoun to be resolved, together\nwith a syntactic (constituency) parse of the sentences up to and including the cur-\nrent sentence. The details of the algorithm depend on the grammar used, but can be\nunderstood from a simpli\ufb01ed version due to Kehler et al. (2004) that just searches\nthrough the list of NPs in the current and prior sentences. This simpli\ufb01ed Hobbs\nalgorithm searches NPs in the following order: \u201c(i) in the current sentence from\nright-to-left, starting with the \ufb01rst NP to the left of the pronoun, (ii) in the previous\nsentence from left-to-right, (iii) in two sentences prior from left-to-right, and (iv) in\n9The simpler of two algorithms presented originally in Hobbs (1978).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 796,
      "token_count": 769,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 537",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 797,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 529\nthe current sentence from left-to-right, starting with the \ufb01rst noun group to the right\nof the pronoun (for cataphora). The \ufb01rst noun group that agrees with the pronoun\nwith respect to number, gender, and person is chosen as the antecedent\u201d (Kehler\net al., 2004).\nLappin and Leass (1994) was an in\ufb02uential entity-based system that used weights\nto combine syntactic and other features, extended soon after by Kennedy and Bogu-\nraev (1996) whose system avoids the need for full syntactic parses.\nApproximately contemporaneously centering (Grosz et al., 1995) was applied\nto pronominal anaphora resolution by Brennan et al. (1987), and a wide variety of\nwork followed focused on centering\u2019s use in coreference (Kameyama 1986, Di Eu-\ngenio 1990, Walker et al. 1994, Di Eugenio 1996, Strube and Hahn 1996, Kehler\n1997a, Tetreault 2001, Iida et al. 2003). Kehler and Rohde (2013) show how center-\ning can be integrated with coherence-driven theories of pronoun interpretation. See\nChapter 24 for the use of centering in measuring discourse coherence.\nCoreference competitions as part of the US DARPA-sponsored MUC confer-\nences provided early labeled coreference datasets (the 1995 MUC-6 and 1998 MUC-\n7 corpora), and set the tone for much later work, choosing to focus exclusively\non the simplest cases of identity coreference (ignoring dif\ufb01cult cases like bridging,\nmetonymy, and part-whole) and drawing the community toward supervised machine\nlearning and metrics like the MUC metric (Vilain et al., 1995). The later ACE eval-\nuations produced labeled coreference corpora in English, Chinese, and Arabic that\nwere widely used for model training and evaluation.\nThis DARPA work in\ufb02uenced the community toward supervised learning begin-\nning in the mid-90s (Connolly et al. 1994, Aone and Bennett 1995, McCarthy and\nLehnert 1995). Soon et al. (2001) laid out a set of basic features, extended by Ng and\nCardie (2002b), and a series of machine learning models followed over the next 15\nyears. These often focused separately on pronominal anaphora resolution (Kehler\net al. 2004, Bergsma and Lin 2006), full NP coreference (Cardie and Wagstaff 1999,\nNg and Cardie 2002b, Ng 2005a) and de\ufb01nite NP reference (Poesio and Vieira 1998,\nVieira and Poesio 2000), as well as separate anaphoricity detection (Bean and Riloff\n1999, Bean and Riloff 2004, Ng and Cardie 2002a, Ng 2004), or singleton detection\n(de Marneffe et al., 2015).\nThe move from mention-pair to mention-ranking approaches was pioneered by\nYang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods,\nthen extended by Denis and Baldridge (2008) who proposed to do ranking via a soft-\nmax over all prior mentions. The idea of doing mention detection, anaphoricity, and",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 798,
      "token_count": 769,
      "chapter_title": ""
    }
  },
  {
    "content": "Lehnert 1995). Soon et al. (2001) laid out a set of basic features, extended by Ng and\nCardie (2002b), and a series of machine learning models followed over the next 15\nyears. These often focused separately on pronominal anaphora resolution (Kehler\net al. 2004, Bergsma and Lin 2006), full NP coreference (Cardie and Wagstaff 1999,\nNg and Cardie 2002b, Ng 2005a) and de\ufb01nite NP reference (Poesio and Vieira 1998,\nVieira and Poesio 2000), as well as separate anaphoricity detection (Bean and Riloff\n1999, Bean and Riloff 2004, Ng and Cardie 2002a, Ng 2004), or singleton detection\n(de Marneffe et al., 2015).\nThe move from mention-pair to mention-ranking approaches was pioneered by\nYang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods,\nthen extended by Denis and Baldridge (2008) who proposed to do ranking via a soft-\nmax over all prior mentions. The idea of doing mention detection, anaphoricity, and\ncoreference jointly in a single end-to-end model grew out of the early proposal of Ng\n(2005b) to use a dummy antecedent for mention-ranking, allowing \u2018non-referential\u2019\nto be a choice for coreference classi\ufb01ers, Denis and Baldridge\u2019s 2007 joint system\ncombining anaphoricity classi\ufb01er probabilities with coreference probabilities, the\nDenis and Baldridge (2008) ranking model, and the Rahman and Ng (2009) pro-\nposal to train the two models jointly with a single objective.\nSimple rule-based systems for coreference returned to prominence in the 2010s,\npartly because of their ability to encode entity-based features in a high-precision way\n(Zhou et al. 2004b, Haghighi and Klein 2009, Raghunathan et al. 2010, Lee et al.\n2011, Lee et al. 2013, Hajishirzi et al. 2013) but in the end they suffered from an\ninability to deal with the semantics necessary to correctly handle cases of common\nnoun coreference.\nA return to supervised learning led to a number of advances in mention-ranking\nmodels which were also extended into neural architectures, for example using re-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 799,
      "token_count": 545,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 538",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 800,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "530 CHAPTER 23 \u2022 C OREFERENCE RESOLUTION AND ENTITY LINKING\ninforcement learning to directly optimize coreference evaluation models Clark and\nManning (2016a), doing end-to-end coreference all the way from span extraction\n(Lee et al. 2017b, Zhang et al. 2018). Neural models also were designed to take\nadvantage of global entity-level information (Clark and Manning 2016b, Wiseman\net al. 2016, Lee et al. 2018).\nCoreference is also related to the task of entity linking discussed in Chapter 14.\nCoreference can help entity linking by giving more possible surface forms to help\nlink to the right Wikipedia page, and conversely entity linking can help improve\ncoreference resolution. Consider this example from Hajishirzi et al. (2013):\n(23.81) [Michael Eisner] 1and [Donald Tsang] 2announced the grand opening of\n[[Hong Kong] 3Disneyland] 4yesterday. [Eisner] 1thanked [the President] 2\nand welcomed [fans] 5to [the park] 4.\nIntegrating entity linking into coreference can help draw encyclopedic knowl-\nedge (like the fact that Donald Tsang is a president) to help disambiguate the men-\ntionthe President . Ponzetto and Strube (2006) 2007 and Ratinov and Roth (2012)\nshowed that such attributes extracted from Wikipedia pages could be used to build\nricher models of entity mentions in coreference. More recent research shows how to\ndo linking and coreference jointly (Hajishirzi et al. 2013, Zheng et al. 2013) or even\njointly with named entity tagging as well (Durrett and Klein 2014).\nThe coreference task as we introduced it involves a simplifying assumption that\nthe relationship between an anaphor and its antecedent is one of identity : the two\ncoreferring mentions refer to the identical discourse referent. In real texts, the rela-\ntionship can be more complex, where different aspects of a discourse referent can\nbe neutralized or refocused. For example (23.82) (Recasens et al., 2011) shows an\nexample of metonymy , in which the capital city Washington is used metonymically metonymy\nto refer to the US. (23.83-23.84) show other examples (Recasens et al., 2011):\n(23.82) a strict interpretation of a policy requires The U.S. to notify foreign\ndictators of certain coup plots ... Washington rejected the bid ...\n(23.83) I once crossed that border into Ashgh-Abad on Nowruz, the Persian New\nYear. In the South, everyone was celebrating New Year ; to the North, it\nwas a regular day.\n(23.84) In France, the president is elected for a term of seven years, while in the\nUnited States heis elected for a term of four years.\nFor further linguistic discussions of these complications of coreference see Puste-\njovsky (1991), van Deemter and Kibble (2000), Poesio et al. (2006), Fauconnier and\nTurner (2008), Versley (2008), and Barker (2010).\nNg (2017) offers a useful compact history of machine learning models in coref-\nerence resolution. There are three excellent book-length surveys of anaphora/coref-\nerence resolution, covering different time periods: Hirst (1981) (early work until",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 801,
      "token_count": 780,
      "chapter_title": ""
    }
  },
  {
    "content": "example of metonymy , in which the capital city Washington is used metonymically metonymy\nto refer to the US. (23.83-23.84) show other examples (Recasens et al., 2011):\n(23.82) a strict interpretation of a policy requires The U.S. to notify foreign\ndictators of certain coup plots ... Washington rejected the bid ...\n(23.83) I once crossed that border into Ashgh-Abad on Nowruz, the Persian New\nYear. In the South, everyone was celebrating New Year ; to the North, it\nwas a regular day.\n(23.84) In France, the president is elected for a term of seven years, while in the\nUnited States heis elected for a term of four years.\nFor further linguistic discussions of these complications of coreference see Puste-\njovsky (1991), van Deemter and Kibble (2000), Poesio et al. (2006), Fauconnier and\nTurner (2008), Versley (2008), and Barker (2010).\nNg (2017) offers a useful compact history of machine learning models in coref-\nerence resolution. There are three excellent book-length surveys of anaphora/coref-\nerence resolution, covering different time periods: Hirst (1981) (early work until\nabout 1981), Mitkov (2002) (1986-2001), and Poesio et al. (2016) (2001-2015).\nAndy Kehler wrote the Discourse chapter for the 2000 \ufb01rst edition of this text-\nbook, which we used as the starting point for the second-edition chapter, and there\nare some remnants of Andy\u2019s lovely prose still in this third-edition coreference chap-\nter.\nExercises",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 802,
      "token_count": 383,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 539\n\nCHAPTER\n24Discourse Coherence\nAnd even in our wildest and most wandering reveries, nay in our very dreams,\nwe shall \ufb01nd, if we re\ufb02ect, that the imagination ran not altogether at adven-\ntures, but that there was still a connection upheld among the different ideas,\nwhich succeeded each other. Were the loosest and freest conversation to be\ntranscribed, there would immediately be transcribed, there would immediately\nbe observed something which connected it in all its transitions.\nDavid Hume, An enquiry concerning human understanding , 1748\nOrson Welles\u2019 movie Citizen Kane was groundbreaking in many ways, perhaps most\nnotably in its structure. The story of the life of \ufb01ctional media magnate Charles\nFoster Kane, the movie does not proceed in chronological order through Kane\u2019s\nlife. Instead, the \ufb01lm begins with Kane\u2019s death (famously murmuring \u201cRosebud\u201d )\nand is structured around \ufb02ashbacks to his life inserted among scenes of a reporter\ninvestigating his death. The novel idea that the structure of a movie does not have\nto linearly follow the structure of the real timeline made apparent for 20th century\ncinematography the in\ufb01nite possibilities and impact of different kinds of coherent\nnarrative structures.\nBut coherent structure is not just a fact about movies or works of art. Like\nmovies, language does not normally consist of isolated, unrelated sentences, but\ninstead of collocated, structured, coherent groups of sentences. We refer to such\na coherent structured group of sentences as a discourse , and we use the word co- discourse\nherence to refer to the relationship between sentences that makes real discourses coherence\ndifferent than just random assemblages of sentences. The chapter you are now read-\ning is an example of a discourse, as is a news article, a conversation, a thread on\nsocial media, a Wikipedia page, and your favorite novel.\nWhat makes a discourse coherent? If you created a text by taking random sen-\ntences each from many different sources and pasted them together, would that be a\ncoherent discourse? Almost certainly not. Real discourses exhibit both local coher- local\nence andglobal coherence . Let\u2019s consider three ways in which real discourses are global\nlocally coherent;\nFirst, sentences or clauses in real discourses are related to nearby sentences in\nsystematic ways. Consider this example from Hobbs (1979):\n(24.1) John took a train from Paris to Istanbul. He likes spinach.\nThis sequence is incoherent because it is unclear to a reader why the second\nsentence follows the \ufb01rst; what does liking spinach have to do with train trips? In\nfact, a reader might go to some effort to try to \ufb01gure out how the discourse could be\ncoherent; perhaps there is a French spinach shortage? The very fact that hearers try\nto identify such connections suggests that human discourse comprehension involves\nthe need to establish this kind of coherence.\nBy contrast, in the following coherent example:\n(24.2) Jane took a train from Paris to Istanbul. She had to attend a conference.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 803,
      "token_count": 667,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 540",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 804,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "532 CHAPTER 24 \u2022 D ISCOURSE COHERENCE\nthe second sentence gives a REASON for Jane\u2019s action in the \ufb01rst sentence. Struc-\ntured relationships like REASON that hold between text units are called coherence\nrelations , and coherent discourses are structured by many such coherence relations.coherence\nrelations\nCoherence relations are introduced in Section 24.1.\nA second way a discourse can be locally coherent is by virtue of being \u201cabout\u201d\nsomeone or something. In a coherent discourse some entities are salient , and the\ndiscourse focuses on them and doesn\u2019t go back and forth between multiple entities.\nThis is called entity-based coherence . Consider the following incoherent passage,\nin which the salient entity seems to wildly swing from John to Jenny to the piano\nstore to the living room, back to Jenny, then the piano again:\n(24.3) John wanted to buy a piano for his living room.\nJenny also wanted to buy a piano.\nHe went to the piano store.\nIt was nearby.\nThe living room was on the second \ufb02oor.\nShe didn\u2019t \ufb01nd anything she liked.\nThe piano he bought was hard to get up to that \ufb02oor.\nEntity-based coherence models measure this kind of coherence by tracking salient\nentities across a discourse. For example Centering Theory (Grosz et al., 1995), theCentering\nTheory\nmost in\ufb02uential theory of entity-based coherence, keeps track of which entities in\nthe discourse model are salient at any point (salient entities are more likely to be\npronominalized or to appear in prominent syntactic positions like subject or object).\nIn Centering Theory, transitions between sentences that maintain the same salient\nentity are considered more coherent than ones that repeatedly shift between entities.\nThe entity grid model of coherence (Barzilay and Lapata, 2008) is a commonly entity grid\nused model that realizes some of the intuitions of the Centering Theory framework.\nEntity-based coherence is introduced in Section 24.3.\nFinally, discourses can be locally coherent by being topically coherent : nearbytopically\ncoherent\nsentences are generally about the same topic and use the same or similar vocab-\nulary to discuss these topics. Because topically coherent discourses draw from a\nsingle semantic \ufb01eld or topic, they tend to exhibit the surface property known as\nlexical cohesion (Halliday and Hasan, 1976): the sharing of identical or semanti- lexical cohesion\ncally related words in nearby sentences. For example, the fact that the words house ,\nchimney ,garret ,closet , and window \u2014 all of which belong to the same semantic\n\ufb01eld\u2014 appear in the two sentences in (24.4), or that they share the identical word\nshingled , is a cue that the two are tied together as a discourse:\n(24.4) Before winter I built a chimney , and shingled the sides of my house ...\nI have thus a tight shingled and plastered house ... with a garret and a\ncloset , a large window on each side....\nIn addition to the local coherence between adjacent or nearby sentences, dis-\ncourses also exhibit global coherence . Many genres of text are associated with\nparticular conventional discourse structures. Academic articles might have sections\ndescribing the Methodology or Results. Stories might follow conventional plotlines\nor motifs. Persuasive essays have a particular claim they are trying to argue for,\nand an essay might express this claim together with a structured set of premises that\nsupport the argument and demolish potential counterarguments. We\u2019ll introduce\nversions of each of these kinds of global coherence.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 805,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "single semantic \ufb01eld or topic, they tend to exhibit the surface property known as\nlexical cohesion (Halliday and Hasan, 1976): the sharing of identical or semanti- lexical cohesion\ncally related words in nearby sentences. For example, the fact that the words house ,\nchimney ,garret ,closet , and window \u2014 all of which belong to the same semantic\n\ufb01eld\u2014 appear in the two sentences in (24.4), or that they share the identical word\nshingled , is a cue that the two are tied together as a discourse:\n(24.4) Before winter I built a chimney , and shingled the sides of my house ...\nI have thus a tight shingled and plastered house ... with a garret and a\ncloset , a large window on each side....\nIn addition to the local coherence between adjacent or nearby sentences, dis-\ncourses also exhibit global coherence . Many genres of text are associated with\nparticular conventional discourse structures. Academic articles might have sections\ndescribing the Methodology or Results. Stories might follow conventional plotlines\nor motifs. Persuasive essays have a particular claim they are trying to argue for,\nand an essay might express this claim together with a structured set of premises that\nsupport the argument and demolish potential counterarguments. We\u2019ll introduce\nversions of each of these kinds of global coherence.\nWhy do we care about the local or global coherence of a discourse? Since co-\nherence is a property of a well-written text, coherence detection plays a part in any",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 806,
      "token_count": 320,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 541\n\n24.1 \u2022 C OHERENCE RELATIONS 533\ntask that requires measuring the quality of a text. For example coherence can help\nin pedagogical tasks like essay grading or essay quality measurement that are trying\nto grade how well-written a human essay is (Somasundaran et al. 2014, Feng et al.\n2014, Lai and Tetreault 2018). Coherence can also help for summarization; knowing\nthe coherence relationship between sentences can help know how to select informa-\ntion from them. Finally, detecting incoherent text may even play a role in mental\nhealth tasks like measuring symptoms of schizophrenia or other kinds of disordered\nlanguage (Ditman and Kuperberg 2010, Elvev \u02daag et al. 2007, Bedi et al. 2015, Iter\net al. 2018).\n24.1 Coherence Relations\nRecall from the introduction the difference between passages (24.5) and (24.6).\n(24.5) Jane took a train from Paris to Istanbul. She likes spinach.\n(24.6) Jane took a train from Paris to Istanbul. She had to attend a conference.\nThe reason (24.6) is more coherent is that the reader can form a connection be-\ntween the two sentences, in which the second sentence provides a potential REASON\nfor the \ufb01rst sentences. This link is harder to form for (24.5). These connections\nbetween text spans in a discourse can be speci\ufb01ed as a set of coherence relations .coherence\nrelation\nThe next two sections describe two commonly used models of coherence relations\nand associated corpora: Rhetorical Structure Theory (RST), and the Penn Discourse\nTreeBank (PDTB).\n24.1.1 Rhetorical Structure Theory\nThe most commonly used model of discourse organization is Rhetorical Structure\nTheory (RST ) (Mann and Thompson, 1987). In RST relations are de\ufb01ned between RST\ntwo spans of text, generally a nucleus and a satellite . The nucleus is the unit that nucleus\nsatellite is more central to the writer\u2019s purpose and that is interpretable independently; the\nsatellite is less central and generally is only interpretable with respect to the nucleus.\nSome symmetric relations, however, hold between two nuclei.\nBelow are a few examples of RST coherence relations, with de\ufb01nitions adapted\nfrom the RST Treebank Manual (Carlson and Marcu, 2001).\nReason: The nucleus is an action carried out by an animate agent and the satellite\nis the reason for the nucleus.\n(24.7) [ NUC Jane took a train from Paris to Istanbul.] [ SATShe had to attend a\nconference.]\nElaboration: The satellite gives additional information or detail about the situation\npresented in the nucleus.\n(24.8) [ NUC Dorothy was from Kansas.] [ SATShe lived in the midst of the great\nKansas prairies.]\nEvidence: The satellite gives additional information or detail about the situation\npresented in the nucleus. The information is presented with the goal of convince the\nreader to accept the information presented in the nucleus.\n(24.9) [ NUC Kevin must be here.] [ SATHis car is parked outside.]",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 807,
      "token_count": 703,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 542\n\n534 CHAPTER 24 \u2022 D ISCOURSE COHERENCE\nAttribution: The satellite gives the source of attribution for an instance of reported\nspeech in the nucleus.\n(24.10) [ SATAnalysts estimated] [ NUC that sales at U.S. stores declined in the\nquarter, too]\nList: In this multinuclear relation, a series of nuclei is given, without contrast or\nexplicit comparison:\n(24.11) [ NUC Billy Bones was the mate; ] [ NUC Long John, he was quartermaster]\nRST relations are traditionally represented graphically; the asymmetric Nucleus-\nSatellite relation is represented with an arrow from the satellite to the nucleus:\nKevin must be here.His car is parked outsideevidence\nWe can also talk about the coherence of a larger text by considering the hierar-\nchical structure between coherence relations. Figure 24.1 shows the rhetorical struc-\nture of a paragraph from Marcu (2000a) for the text in (24.12) from the Scienti\ufb01c\nAmerican magazine.\n(24.12) With its distant orbit\u201350 percent farther from the sun than Earth\u2013and slim\natmospheric blanket, Mars experiences frigid weather conditions. Surface\ntemperatures typically average about -60 degrees Celsius (-76 degrees\nFahrenheit) at the equator and can dip to -123 degrees C near the poles. Only\nthe midday sun at tropical latitudes is warm enough to thaw ice on occasion,\nbut any liquid water formed in this way would evaporate almost instantly\nbecause of the low atmospheric pressure.\nTitle\n(1)\nMars2-9\nevidence\n2-3\nbackground\n  (2)\nWIth its \ndistant orbit  \n<p> -- 50\n percent \nfarther from \nthe sun than \nEarth -- </p> \nand slim \natmospheric \nblanket,(3)\nMars\nexperiences\nfrigid weather\nconditions.4-9\nelaboration-additional\n(4)\nSurface \ntemperatures\n typically average \nabout -60 \ndegrees Celsius\n <p> (-76 degrees\nFahrenheit)</p>\n at the equator4-5\nList\n(5)\nand can dip\nto -123\ndegrees C\nnear the\npoles.6-9\nContrast\n6-7\n(6)\nOnly the\nmidday sun at\ntropical latitudes\nis warm enough(7)\nto thaw ice\non occasion,purpose8-9\nexplanation-argumentative\n(8)\nbut any liquid water\nformed in this way \nwould evaporate \nalmost instantly(9)\nbecause of\nthe low\natmospheric\npressure.\nFigure 24.1 A discourse tree for the Scienti\ufb01c American text in (24.12), from Marcu (2000a). Note that\nasymmetric relations are represented with a curved arrow from the satellite to the nucleus.\nThe leaves in the Fig. 24.1 tree correspond to text spans of a sentence, clause or\nphrase that are called elementary discourse units orEDU s in RST; these units can EDU\nalso be referred to as discourse segments . Because these units may correspond to\narbitrary spans of text, determining the boundaries of an EDU is an important task\nfor extracting coherence relations. Roughly speaking, one can think of discourse",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 808,
      "token_count": 708,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 543",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 809,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "24.1 \u2022 C OHERENCE RELATIONS 535\nsegments as being analogous to constituents in sentence syntax, and indeed as we\u2019ll\nsee in Section 24.2 we generally draw on parsing algorithms to infer discourse struc-\nture.\nThere are corpora for many discourse coherence models; the RST Discourse\nTreeBank (Carlson et al., 2001) is the largest available discourse corpus. It con-\nsists of 385 English language documents selected from the Penn Treebank, with full\nRST parses for each one, using a large set of 78 distinct relations, grouped into 16\nclasses. RST treebanks exist also for Spanish, German, Basque, Dutch and Brazilian\nPortuguese (Braud et al., 2017).\nNow that we\u2019ve seen examples of coherence, we can see more clearly how a\ncoherence relation can play a role in summarization or information extraction. For\nexample, the nuclei of a text presumably express more important information than\nthe satellites, which might be dropped in a summary.\n24.1.2 Penn Discourse TreeBank (PDTB)\nThe Penn Discourse TreeBank (PDTB ) is a second commonly used dataset that PDTB\nembodies another model of coherence relations (Miltsakaki et al. 2004, Prasad et al.\n2008, Prasad et al. 2014). PDTB labeling is lexically grounded . Instead of asking\nannotators to directly tag the coherence relation between text spans, they were given\na list of discourse connectives , words that signal discourse relations, like because ,discourse\nconnectives\nalthough ,when ,since , oras a result . In a part of a text where these words marked a\ncoherence relation between two text spans, the connective and the spans were then\nannotated, as in Fig. 24.13, where the phrase as a result signals a causal relationship\nbetween what PDTB calls Arg1 (the \ufb01rst two sentences, here in italics) and Arg2\n(the third sentence, here in bold).\n(24.13) Jewelry displays in department stores were often cluttered and uninspired.\nAnd the merchandise was, well, fake. As a result , marketers of faux gems\nsteadily lost space in department stores to more fashionable\nrivals\u2014cosmetics makers.\n(24.14) In July, the Environmental Protection Agency imposed a gradual ban on\nvirtually all uses of asbestos. (implicit=as a result )By 1997, almost all\nremaining uses of cancer-causing asbestos will be outlawed.\nNot all coherence relations are marked by an explicit discourse connective, and\nso the PDTB also annotates pairs of neighboring sentences with no explicit signal,\nlike (24.14). The annotator \ufb01rst chooses the word or phrase that could have been its\nsignal (in this case as a result ), and then labels its sense. For example for the am-\nbiguous discourse connective since annotators marked whether it is using a C AUSAL\nor a T EMPORAL sense.\nThe \ufb01nal dataset contains roughly 18,000 explicit relations and 16,000 implicit\nrelations. Fig. 24.2 shows examples from each of the 4 major semantic classes, while\nFig. 24.3 shows the full tagset.\nUnlike the RST Discourse Treebank, which integrates these pairwise coherence\nrelations into a global tree structure spanning an entire discourse, the PDTB does not\nannotate anything above the span-pair level, making no commitment with respect to\nhigher-level discourse structure.\nThere are also treebanks using similar methods for other languages; (24.15)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 810,
      "token_count": 769,
      "chapter_title": ""
    }
  },
  {
    "content": "rivals\u2014cosmetics makers.\n(24.14) In July, the Environmental Protection Agency imposed a gradual ban on\nvirtually all uses of asbestos. (implicit=as a result )By 1997, almost all\nremaining uses of cancer-causing asbestos will be outlawed.\nNot all coherence relations are marked by an explicit discourse connective, and\nso the PDTB also annotates pairs of neighboring sentences with no explicit signal,\nlike (24.14). The annotator \ufb01rst chooses the word or phrase that could have been its\nsignal (in this case as a result ), and then labels its sense. For example for the am-\nbiguous discourse connective since annotators marked whether it is using a C AUSAL\nor a T EMPORAL sense.\nThe \ufb01nal dataset contains roughly 18,000 explicit relations and 16,000 implicit\nrelations. Fig. 24.2 shows examples from each of the 4 major semantic classes, while\nFig. 24.3 shows the full tagset.\nUnlike the RST Discourse Treebank, which integrates these pairwise coherence\nrelations into a global tree structure spanning an entire discourse, the PDTB does not\nannotate anything above the span-pair level, making no commitment with respect to\nhigher-level discourse structure.\nThere are also treebanks using similar methods for other languages; (24.15)\nshows an example from the Chinese Discourse TreeBank (Zhou and Xue, 2015).\nBecause Chinese has a smaller percentage of explicit discourse connectives than\nEnglish (only 22% of all discourse relations are marked with explicit connectives,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 811,
      "token_count": 336,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 544\n\n536 CHAPTER 24 \u2022 D ISCOURSE COHERENCE\nClass Type Example\nTEMPORAL SYNCHRONOUS The parishioners of St. Michael and All Angels stop to chat at\nthe church door, as members here always have. (Implicit while )\nIn the tower, \ufb01ve men and women pull rhythmically on ropes\nattached to the same \ufb01ve bells that \ufb01rst sounded here in 1614.\nCONTINGENCY REASON Also unlike Mr. Ruder, Mr. Breeden appears to be in a position\nto get somewhere with his agenda. (implicit=because )As a for-\nmer White House aide who worked closely with Congress,\nhe is savvy in the ways of Washington.\nCOMPARISON CONTRAST The U.S. wants the removal of what it perceives as barriers to\ninvestment; Japan denies there are real barriers.\nEXPANSION CONJUNCTION Not only do the actors stand outside their characters and make\nit clear they are at odds with them, but they often literally stand\non their heads.\nFigure 24.2 The four high-level semantic distinctions in the PDTB sense hierarchy\nTemporal Comparison\n\u000fAsynchronous \u000fContrast (Juxtaposition, Opposition)\n\u000fSynchronous (Precedence, Succession) \u000fPragmatic Contrast (Juxtaposition, Opposition)\n\u000fConcession (Expectation, Contra-expectation)\n\u000fPragmatic Concession\nContingency Expansion\n\u000fCause (Reason, Result) \u000fException\n\u000fPragmatic Cause (Justi\ufb01cation) \u000fInstantiation\n\u000fCondition (Hypothetical, General, Unreal\nPresent/Past, Factual Present/Past)\u000fRestatement (Speci\ufb01cation, Equivalence, Generalization)\n\u000fPragmatic Condition (Relevance, Implicit As-\nsertion)\u000fAlternative (Conjunction, Disjunction, Chosen Alterna-\ntive)\n\u000fList\nFigure 24.3 The PDTB sense hierarchy. There are four top-level c\u00aflasses, 16 types, and 23 subtypes (not all\ntypes have subtypes). 11 of the 16 types are commonly used for implicit argument classi\ufb01cation; the 5 types in\nitalics are too rare in implicit labeling to be used.\ncompared to 47% in English), annotators labeled this corpus by directly mapping\npairs of sentences to 11 sense tags, without starting with a lexical discourse connec-\ntor.\n(24.15) [ Conn\u4e3a] [Arg2\u63a8\u52a8\u56fe\u4eec\u6c5f\u5730\u533a\u5f00\u53d1]\uff0c[Arg1\u97e9\u56fd\u6350\u6b3e\u4e00\u767e\u4e07\u7f8e\u5143\n\u8bbe\u7acb\u4e86\u56fe\u4eec\u6c5f\u53d1\u5c55\u57fa\u91d1]\n\u201c[In order to] [ Arg2 promote the development of the Tumen River region],\n[Arg1 South Korea donated one million dollars to establish the Tumen\nRiver Development Fund].\u201d\nThese discourse treebanks have been used for shared tasks on multilingual dis-\ncourse parsing (Xue et al., 2016).\n24.2 Discourse Structure Parsing\nGiven a sequence of sentences, how can we automatically determine the coherence\nrelations between them? This task is often called discourse parsing (even thoughdiscourse\nparsing\nfor PDTB we are only assigning labels to leaf spans and not building a full parse",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 812,
      "token_count": 692,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 545\n\n24.2 \u2022 D ISCOURSE STRUCTURE PARSING 537\ntree as we do for RST).\n24.2.1 EDU segmentation for RST parsing\nRST parsing is generally done in two stages. The \ufb01rst stage, EDU segmentation ,\nextracts the start and end of each EDU. The output of this stage would be a labeling\nlike the following:\n(24.16) [Mr. Rambo says] e1[that a 3.2-acre property] e2[overlooking the San\nFernando Valley] e3[is priced at $4 million] e4[because the late actor Erroll\nFlynn once lived there.] e5\nSince EDUs roughly correspond to clauses, early models of EDU segmentation\n\ufb01rst ran a syntactic parser, and then post-processed the output. Modern systems\ngenerally use neural sequence models supervised by the gold EDU segmentation in\ndatasets like the RST Discourse Treebank. Fig. 24.4 shows an example architecture\nsimpli\ufb01ed from the algorithm of Lukasik et al. (2020) that predicts for each token\nwhether or not it is a break. Here the input sentence is passed through an encoder\nand then passed through a linear layer and a softmax to produce a sequence of 0s\nand 1, where 1 indicates the start of an EDU.\nMr.RambosaysthatENCODER\u20260001linear layersoftmaxEDU break\nFigure 24.4 Predicting EDU segment beginnings from encoded text.\n24.2.2 RST parsing\nTools for building RST coherence structure for a discourse have long been based on\nsyntactic parsing algorithms like shift-reduce parsing (Marcu, 1999). Many modern\nRST parsers since Ji and Eisenstein (2014) draw on the neural syntactic parsers we\nsaw in Chapter 20, using representation learning to build representations for each\nspan, and training a parser to choose the correct shift and reduce actions based on\nthe gold parses in the training set.\nWe\u2019ll describe the shift-reduce parser of Yu et al. (2018). The parser state con-\nsists of a stack and a queue, and produces this structure by taking a series of actions\non the states. Actions include:\n\u2022shift : pushes the \ufb01rst EDU in the queue onto the stack creating a single-node\nsubtree.\n\u2022reduce (l,d): merges the top two subtrees on the stack, where lis the coherence\nrelation label, and dis the nuclearity direction, d2fNN;NS;SNg.\nAs well as the pop root operation, to remove the \ufb01nal tree from the stack.\nFig. 24.6 shows the actions the parser takes to build the structure in Fig. 24.5.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 813,
      "token_count": 602,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 546",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 814,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "538 CHAPTER 24 \u2022 D ISCOURSE COHERENCE\n560e1 e2 e3 e4attr elabelab e1: American Telephone & Telegraph Co. said it\ne2: will lay off 75 to 85 technicians here , effective Nov. 1.\ne3: The workers install , maintain and repair its private branch exchanges,\ne4: which are large intracompany telephone networks.\nFigure 1: An example of RST discourse tree, where {e1,e2,e3,e4}are EDUs, attr andelab are\ndiscourse relation labels, and arrows indicate the nuclearities of discourse relations.\nRST discourse parsing. Other studies still adopt discrete syntax features proposed by statistical models,\nfeeding them into neural network models (Braud et al., 2016; Braud et al., 2017).\nThe above approaches model syntax trees in an explicit way, requiring discrete syntax parsing outputs\nas inputs for RST parsing. These approaches may suffer from the error propagation problem. Syntax trees\nproduced by a supervised syntax parsing model could have errors, which may propagate into discourse\nparsing models. The problem could be extremely serious when inputs of discourse parsing have different\ndistributions with the training data of the supervised syntax parser. Recently, Zhang et al. (2017) suggest\nan alternative method, which extracts syntax features from a Bi-Af\ufb01ne dependency parser (Dozat and\nManning, 2016), and the method gives competitive performances on relation extraction. It actually\nrepresents syntax trees implicitly, thus it can reduce the error propagation problem.\nIn this work, we investigate the implicit syntax feature extraction approach for RST parsing. In ad-\ndition, we propose a transition-based neural model for this task, which is able to incorporate various\nfeatures \ufb02exibly. We exploit hierarchical bi-directional LSTMs (Bi-LSTMs) to encode texts, and further\nenhance the transition-based model with dynamic oracle. Based on the proposed model, we study the\neffectiveness of our proposed implicit syntax features. We conduct experiments on a standard RST dis-\ncourse TreeBank (Carlson et al., 2003). First, we evaluate the performance of our proposed transition-\nbased baseline, \ufb01nding that the model is able to achieve strong performances after applying dynamic\noracle. Then we evaluate the effectiveness of implicit syntax features extracted from a Bi-Af\ufb01ne depen-\ndency parser. Results show that the implicit syntax features are effective, giving better performances than\nexplicit Tree-LSTM (Li et al., 2015b). Our codes will be released for public under the Apache License\n2.0 at https://github.com/yunan4nlp/NNDisParser .\nIn summary, we mainly make the following two contributions in this work: (1) we propose a transition-\nbased neural RST discourse parsing model with dynamic oracle, (2) we compare three different syntactic\nintegration approaches proposed by us. The rest of the paper is organized as follows. Section 2 describes\nour proposed models including the transition-based neural model, the dynamic oracle strategy and the\nimplicit syntax feature extraction approach. Section 3 presents the experiments to evaluate our models.\nSection 4 shows the related work. Finally, section 5 draws conclusions.\n2 Transition-based Discourse Parsing\nWe follow Ji and Eisenstein (2014), exploiting a transition-based framework for RST discourse parsing.\nThe framework is conceptually simple and \ufb02exible to support arbitrary features, which has been widely\nused in a number of NLP tasks (Zhu et al., 2013; Dyer et al., 2015; Zhang et al., 2016). In addition, a",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 815,
      "token_count": 771,
      "chapter_title": ""
    }
  },
  {
    "content": "oracle. Then we evaluate the effectiveness of implicit syntax features extracted from a Bi-Af\ufb01ne depen-\ndency parser. Results show that the implicit syntax features are effective, giving better performances than\nexplicit Tree-LSTM (Li et al., 2015b). Our codes will be released for public under the Apache License\n2.0 at https://github.com/yunan4nlp/NNDisParser .\nIn summary, we mainly make the following two contributions in this work: (1) we propose a transition-\nbased neural RST discourse parsing model with dynamic oracle, (2) we compare three different syntactic\nintegration approaches proposed by us. The rest of the paper is organized as follows. Section 2 describes\nour proposed models including the transition-based neural model, the dynamic oracle strategy and the\nimplicit syntax feature extraction approach. Section 3 presents the experiments to evaluate our models.\nSection 4 shows the related work. Finally, section 5 draws conclusions.\n2 Transition-based Discourse Parsing\nWe follow Ji and Eisenstein (2014), exploiting a transition-based framework for RST discourse parsing.\nThe framework is conceptually simple and \ufb02exible to support arbitrary features, which has been widely\nused in a number of NLP tasks (Zhu et al., 2013; Dyer et al., 2015; Zhang et al., 2016). In addition, a\ntransition-based model formalizes a certain task into predicting a sequence of actions, which is essential\nsimilar to sequence-to-sequence models proposed recently (Bahdanau et al., 2014). In the following,\nwe \ufb01rst describe the transition system for RST discourse parsing, and then introduce our neural network\nmodel by its encoder and decoder parts, respectively. Thirdly, we present our proposed dynamic oracle\nstrategy aiming to enhance the transition-based model. Then we introduce the integration method of\nimplicit syntax features. Finally we describe the training method of our neural network models.\n2.1 The Transition-based System\nThe transition-based framework converts a structural learning problem into a sequence of action predic-\ntions, whose key point is a transition system. A transition system consists of two parts: states and actions.\nThe states are used to store partially-parsed results and the actions are used to control state transitions.\nFigure 24.5 Example RST discourse tree, showing four EDUs. Figure from Yu et al. (2018).\n561Step Stack Queue Action Relation\n1 ?e1,e2,e3,e4 SH ?\n2 e1 e2,e3,e4 SH ?\n3 e1,e2 e3,e4 RD(attr,SN) ?\n4 e1:2 e3,e4 SH de1e2\n5 e1:2,e3 e4 SH de1e2\n6 e1:2,e3,e4 ?RD(elab,NS) de1e2\n7 e1:2,e3:4 ?RD(elab,SN) de1e2,de3e4\n8 e1:4 ? PR de1e2,de3e4,\\e1:2e3:4\nTable 1: An example of the transition-based system for RST discourse parsing.\nThe initial state is an empty state, and the \ufb01nal state represents a full result. There are three kinds of\nactions in our transition system:\n\u2022Shift (SH), which removes the \ufb01rst EDU in the queue onto the stack, forming a single-node subtree.\n\u2022Reduce (RD) ( l,d), which merges the top two subtrees on the stack, where lis a discourse relation\nlabel, and d2{NN,NS,SN}indicates the relation nuclearity (nuclear (N) or satellite (S)).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 816,
      "token_count": 777,
      "chapter_title": ""
    }
  },
  {
    "content": "561Step Stack Queue Action Relation\n1 ?e1,e2,e3,e4 SH ?\n2 e1 e2,e3,e4 SH ?\n3 e1,e2 e3,e4 RD(attr,SN) ?\n4 e1:2 e3,e4 SH de1e2\n5 e1:2,e3 e4 SH de1e2\n6 e1:2,e3,e4 ?RD(elab,NS) de1e2\n7 e1:2,e3:4 ?RD(elab,SN) de1e2,de3e4\n8 e1:4 ? PR de1e2,de3e4,\\e1:2e3:4\nTable 1: An example of the transition-based system for RST discourse parsing.\nThe initial state is an empty state, and the \ufb01nal state represents a full result. There are three kinds of\nactions in our transition system:\n\u2022Shift (SH), which removes the \ufb01rst EDU in the queue onto the stack, forming a single-node subtree.\n\u2022Reduce (RD) ( l,d), which merges the top two subtrees on the stack, where lis a discourse relation\nlabel, and d2{NN,NS,SN}indicates the relation nuclearity (nuclear (N) or satellite (S)).\n\u2022Pop Root (PR), which pops out the top tree on the stack, marking the decoding being completed,\nwhen the stack holds only one subtree and the queue is empty.\nGiven the RST tree as shown in Figure 1, it can be generated by the following action sequence: {SH,\nSH, RD (attr,SN) , SH, SH, RD (elab,NS) , RD (elab,SN) , PR}. Table 1 shows the decoding\nprocess in detail. By this way, we naturally convert RST discourse parsing into predicting a sequence of\ntransition actions, where each line includes a state and next step action referring to the tree.\n2.2 Encoder-Decoder\nPrevious transition-based RST discourse parsing studies exploit statistical models, using manually-\ndesigned discrete features (Sagae, 2009; Heilman and Sagae, 2015; Wang et al., 2017). In this work, we\npropose a transition-based neural model for RST discourse parsing, which follows an encoder-decoder\nframework. Given an input sequence of EDUs {e1,e2,. . . ,e n}, the encoder computes the input represen-\ntations {he\n1,he\n2,. . . ,he\nn}, and the decoder predicts next step actions conditioned on the encoder outputs.\n2.2.1 Encoder\nWe follow Li et al. (2016), using hierarchical Bi-LSTMs to encode the source EDU inputs, where the\n\ufb01rst-layer is used to represent sequencial words inside of EDUs, and the second layer is used to represent\nsequencial EDUs. Given an input sentence {w1,w2,. . . ,w m}, \ufb01rst we represent each word by its form\n(e.g., wi) and POS tag (e.g. ti), concatenating their neural embeddings. By this way, the input vectors\nof the \ufb01rst-layer Bi-LSTM are {xw\n1,xw\n2,. . . ,xw\nm}, where xw\ni=emb (wi)\u0000emb (ti), and then we apply\nBi-LSTM directly, obtaining:\n{hw\n1,hw\n2,. . . ,hw\nm}=Bi-LSTM ({xw\n1,xw\n2,. . . ,xw\nm}) (1)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 817,
      "token_count": 769,
      "chapter_title": ""
    }
  },
  {
    "content": "propose a transition-based neural model for RST discourse parsing, which follows an encoder-decoder\nframework. Given an input sequence of EDUs {e1,e2,. . . ,e n}, the encoder computes the input represen-\ntations {he\n1,he\n2,. . . ,he\nn}, and the decoder predicts next step actions conditioned on the encoder outputs.\n2.2.1 Encoder\nWe follow Li et al. (2016), using hierarchical Bi-LSTMs to encode the source EDU inputs, where the\n\ufb01rst-layer is used to represent sequencial words inside of EDUs, and the second layer is used to represent\nsequencial EDUs. Given an input sentence {w1,w2,. . . ,w m}, \ufb01rst we represent each word by its form\n(e.g., wi) and POS tag (e.g. ti), concatenating their neural embeddings. By this way, the input vectors\nof the \ufb01rst-layer Bi-LSTM are {xw\n1,xw\n2,. . . ,xw\nm}, where xw\ni=emb (wi)\u0000emb (ti), and then we apply\nBi-LSTM directly, obtaining:\n{hw\n1,hw\n2,. . . ,hw\nm}=Bi-LSTM ({xw\n1,xw\n2,. . . ,xw\nm}) (1)\nThe second-layer Bi-LSTM is built over sequential EDUs. We should \ufb01rst obtain a suitable representa-\ntion for each EDU, which is composed by a span of words inside a certain sentence. Assuming an EDU\nwith its words by {ws,ws+1,. . . ,w t}, after applying the \ufb01rst-layer Bi-LSTM, we obtain their representa-\ntions by {hw\ns,hw\ns+1...,hw\nt}, then we calculate the EDU representation by average pooling:\nxe=1\nt\u0000s+1tX\nshw\nk (2)\nWhen the EDU representations are ready, we apply the second-layer Bi-LSTM directly, resulting:\n{he\n1,he\n2,. . . ,he\nn}=Bi-LSTM ({xe\n1,xe\n2,. . . ,xe\nn}) (3)\nFigure 24.6 Parsing the example of Fig. 24.5 using a shift-reduce parser. Figure from Yu\net al. (2018).\nThe Yu et al. (2018) uses an encoder-decoder architecture, where the encoder\nrepresents the input span of words and EDUs using a hierarchical biLSTM. The\n\ufb01rst biLSTM layer represents the words inside an EDU, and the second represents\nthe EDU sequence. Given an input sentence w1;w2;:::;wm, the words can be repre-\nsented as usual (by static embeddings, combinations with character embeddings or\ntags, or contextual embeddings) resulting in an input word representation sequence\nxw\n1;xw\n2;:::;xw\nm. The result of the word-level biLSTM is then a sequence of hwvalues:\nhw\n1;hw\n2;:::;hw\nm=biLSTM (xw\n1;xw\n2;:::;xw\nm) (24.17)\nAn EDU of span ws;ws+1;:::;wtthen has biLSTM output representation hw\ns;hw\ns+1;:::;hw\nt,\nand is represented by average pooling:\nxe=1\nt\u0000s+1tX\nk=shw\nk (24.18)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 818,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 24.6 Parsing the example of Fig. 24.5 using a shift-reduce parser. Figure from Yu\net al. (2018).\nThe Yu et al. (2018) uses an encoder-decoder architecture, where the encoder\nrepresents the input span of words and EDUs using a hierarchical biLSTM. The\n\ufb01rst biLSTM layer represents the words inside an EDU, and the second represents\nthe EDU sequence. Given an input sentence w1;w2;:::;wm, the words can be repre-\nsented as usual (by static embeddings, combinations with character embeddings or\ntags, or contextual embeddings) resulting in an input word representation sequence\nxw\n1;xw\n2;:::;xw\nm. The result of the word-level biLSTM is then a sequence of hwvalues:\nhw\n1;hw\n2;:::;hw\nm=biLSTM (xw\n1;xw\n2;:::;xw\nm) (24.17)\nAn EDU of span ws;ws+1;:::;wtthen has biLSTM output representation hw\ns;hw\ns+1;:::;hw\nt,\nand is represented by average pooling:\nxe=1\nt\u0000s+1tX\nk=shw\nk (24.18)\nThe second layer uses this input to compute a \ufb01nal representation of the sequence of\nEDU representations he:\nhe\n1;he\n2;:::;he\nn=biLSTM (xe\n1;xe\n2;:::;xe\nn) (24.19)\nThe decoder is then a feedforward network Wthat outputs an action obased on a\nconcatenation of the top three subtrees on the stack ( so;s1;s2) plus the \ufb01rst EDU in\nthe queue (q0):\no=W(ht\ns0;ht\ns1;ht\ns2;he\nq0) (24.20)\nwhere the representation of the EDU on the queue he\nq0comes directly from the\nencoder, and the three hidden vectors representing partial trees are computed by\naverage pooling over the encoder output for the EDUs in those trees:\nhts=1\nj\u0000i+1jX\nk=ihe\nk (24.21)",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 819,
      "token_count": 503,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 547\n\n24.2 \u2022 D ISCOURSE STRUCTURE PARSING 539\nTraining \ufb01rst maps each RST gold parse tree into a sequence of oracle actions, and\nthen uses the standard cross-entropy loss (with l2regularization) to train the system\nto take such actions. Give a state Sand oracle action a, we \ufb01rst compute the decoder\noutput using Eq. 24.20, apply a softmax to get probabilities:\npa=exp(oa)P\na02Aexp(oa0)(24.22)\nand then computing the cross-entropy loss:\nLCE() =\u0000log(pa)+l\n2jjQjj2(24.23)\nRST discourse parsers are evaluated on the test section of the RST Discourse Tree-\nbank, either with gold EDUs or end-to-end, using the RST-Pareval metrics (Marcu,\n2000b). It is standard to \ufb01rst transform the gold RST trees into right-branching bi-\nnary trees, and to report four metrics: trees with no labels (S for Span), labeled\nwith nuclei (N), with relations (R), or both (F for Full), for each metric computing\nmicro-averaged F 1over all spans from all documents (Marcu 2000b, Morey et al.\n2017).\n24.2.3 PDTB discourse parsing\nPDTB discourse parsing, the task of detecting PDTB coherence relations between\nspans, is sometimes called shallow discourse parsing because the task just involvesshallow\ndiscourse\nparsing\ufb02at relationships between text spans, rather than the full trees of RST parsing.\nThe set of four subtasks for PDTB discourse parsing was laid out by Lin et al.\n(2014) in the \ufb01rst complete system, with separate tasks for explicit (tasks 1-3) and\nimplicit (task 4) connectives:\n1. Find the discourse connectives (disambiguating them from non-discourse uses)\n2. Find the two spans for each connective\n3. Label the relationship between these spans\n4. Assign a relation between every adjacent pair of sentences\nMany systems have been proposed for Task 4: taking a pair of adjacent sentences\nas input and assign a coherence relation sense label as output. The setup often fol-\nlows Lin et al. (2009) in assuming gold sentence span boundaries and assigning each\nadjacent span one of the 11 second-level PDTB tags or none (removing the 5 very\nrare tags of the 16 shown in italics in Fig. 24.3).\nA simple but very strong algorithm for Task 4 is to represent each of the two\nspans by BERT embeddings and take the last layer hidden state corresponding to\nthe position of the [CLS] token, pass this through a single layer tanh feedforward\nnetwork and then a softmax for sense classi\ufb01cation (Nie et al., 2019).\nEach of the other tasks also have been addressed. Task 1 is to disambiguat-\ning discourse connectives from their non-discourse use. For example as Pitler and\nNenkova (2009) point out, the word andis a discourse connective linking the two\nclauses by an elaboration/expansion relation in (24.24) while it\u2019s a non-discourse\nNP conjunction in (24.25):\n(24.24) Selling picked up as previous buyers bailed out of their positions and\naggressive short sellers\u2014anticipating further declines\u2014moved in.\n(24.25) My favorite colors are blue and green.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 820,
      "token_count": 767,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 548\n\n540 CHAPTER 24 \u2022 D ISCOURSE COHERENCE\nSimilarly, once is a discourse connective indicating a temporal relation in (24.26),\nbut simply a non-discourse adverb meaning \u2018formerly\u2019 and modifying used in (24.27):\n(24.26) The asbestos \ufb01ber, crocidolite, is unusually resilient once it enters the\nlungs, with even brief exposures to it causing symptoms that show up\ndecades later, researchers said.\n(24.27) A form of asbestos once used to make Kent cigarette \ufb01lters has caused a\nhigh percentage of cancer deaths among a group of workers exposed to it\nmore than 30 years ago, researchers reported.\nDetermining whether a word is a discourse connective is thus a special case\nof word sense disambiguation. Early work on disambiguation showed that the 4\nPDTB high-level sense classes could be disambiguated with high (94%) accuracy\nused syntactic features from gold parse trees (Pitler and Nenkova, 2009). Recent\nwork performs the task end-to-end from word inputs using a biLSTM-CRF with\nBIO outputs ( B-CONN ,I-CONN ,O) (Yu et al., 2019).\nFor task 2, PDTB spans can be identi\ufb01ed with the same sequence models used to\n\ufb01nd RST EDUs: a biLSTM sequence model with pretrained contextual embedding\n(BERT) inputs (Muller et al., 2019). Simple heuristics also do pretty well as a base-\nline at \ufb01nding spans, since 93% of relations are either completely within a single\nsentence or span two adjacent sentences, with one argument in each sentence (Biran\nand McKeown, 2015).\n24.3 Centering and Entity-Based Coherence\nA second way a discourse can be coherent is by virtue of being \u201cabout\u201d some entity.\nThis idea that at each point in the discourse some entity is salient, and a discourse\nis coherent by continuing to discuss the same entity, appears early in functional lin-\nguistics and the psychology of discourse (Chafe 1976, Kintsch and Van Dijk 1978),\nand soon made its way to computational models. In this section we introduce two\nmodels of this kind of entity-based coherence :Centering Theory (Grosz et al., entity-based\n1995), and the entity grid model of Barzilay and Lapata (2008).\n24.3.1 Centering\nCentering Theory (Grosz et al., 1995) is a theory of both discourse salience andCentering\nTheory\ndiscourse coherence. As a model of discourse salience, Centering proposes that at\nany given point in the discourse one of the entities in the discourse model is salient:\nit is being \u201ccentered\u201d on. As a model of discourse coherence, Centering proposes\nthat discourses in which adjacent sentences CONTINUE to maintain the same salient\nentity are more coherent than those which SHIFT back and forth between multiple\nentities (we will see that CONTINUE and SHIFT are technical terms in the theory).\nThe following two texts from Grosz et al. (1995) which have exactly the same\npropositional content but different saliences, can help in understanding the main\nCentering intuition.\n(24.28) a. John went to his favorite music store to buy a piano.\nb. He had frequented the store for many years.\nc. He was excited that he could \ufb01nally buy a piano.\nd. He arrived just as the store was closing for the day.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 821,
      "token_count": 775,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 549",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 822,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "24.3 \u2022 C ENTERING AND ENTITY -BASED COHERENCE 541\n(24.29) a. John went to his favorite music store to buy a piano.\nb. It was a store John had frequented for many years.\nc. He was excited that he could \ufb01nally buy a piano.\nd. It was closing just as John arrived.\nWhile these two texts differ only in how the two entities (John and the store) are\nrealized in the sentences, the discourse in (24.28) is intuitively more coherent than\nthe one in (24.29). As Grosz et al. (1995) point out, this is because the discourse\nin (24.28) is clearly about one individual, John, describing his actions and feelings.\nThe discourse in (24.29), by contrast, focuses \ufb01rst on John, then the store, then back\nto John, then to the store again. It lacks the \u201caboutness\u201d of the \ufb01rst discourse.\nCentering Theory realizes this intuition by maintaining two representations for\neach utterance Un. The backward-looking center ofUn, denoted as Cb(Un), rep-backward-\nlooking\ncenterresents the current salient entity, the one being focused on in the discourse after Un\nis interpreted. The forward-looking centers ofUn, denoted as Cf(Un), are a setforward-looking\ncenter\nof potential future salient entities, the discourse entities evoked by Unany of which\ncould serve as Cb(the salient entity) of the following utterance, i.e. Cb(Un+1).\nThe set of forward-looking centers Cf(Un)are ranked according to factors like\ndiscourse salience and grammatical role (for example subjects are higher ranked\nthan objects, which are higher ranked than all other grammatical roles). We call the\nhighest-ranked forward-looking center Cp(for \u201cpreferred center\u201d). Cpis a kind of\nprediction about what entity will be talked about next. Sometimes the next utterance\nindeed talks about this entity, but sometimes another entity becomes salient instead.\nWe\u2019ll use here the algorithm for centering presented in Brennan et al. (1987),\nwhich de\ufb01nes four intersentential relationships between a pair of utterances Unand\nUn+1that depend on the relationship between Cb(Un+1),Cb(Un), and Cp(Un+1);\nthese are shown in Fig. 24.7.\nCb(Un+1) =Cb(Un) Cb(Un+1)6=Cb(Un)\nor unde\ufb01ned Cb(Un)\nCb(Un+1) =Cp(Un+1) Continue Smooth-Shift\nCb(Un+1)6=Cp(Un+1) Retain Rough-Shift\nFigure 24.7 Centering Transitions for Rule 2 from Brennan et al. (1987).\nThe following rules are used by the algorithm:\nRule 1: If any element of Cf(Un)is realized by a pronoun in utterance\nUn+1, then Cb(Un+1)must be realized as a pronoun also.\nRule 2 :Transition states are ordered. Continue is preferred to Retain is\npreferred to Smooth-Shift is preferred to Rough-Shift.\nRule 1 captures the intuition that pronominalization (including zero-anaphora)\nis a common way to mark discourse salience. If there are multiple pronouns in an\nutterance realizing entities from the previous utterance, one of these pronouns must\nrealize the backward center Cb; if there is only one pronoun, it must be Cb.\nRule 2 captures the intuition that discourses that continue to center the same en-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 823,
      "token_count": 765,
      "chapter_title": ""
    }
  },
  {
    "content": "these are shown in Fig. 24.7.\nCb(Un+1) =Cb(Un) Cb(Un+1)6=Cb(Un)\nor unde\ufb01ned Cb(Un)\nCb(Un+1) =Cp(Un+1) Continue Smooth-Shift\nCb(Un+1)6=Cp(Un+1) Retain Rough-Shift\nFigure 24.7 Centering Transitions for Rule 2 from Brennan et al. (1987).\nThe following rules are used by the algorithm:\nRule 1: If any element of Cf(Un)is realized by a pronoun in utterance\nUn+1, then Cb(Un+1)must be realized as a pronoun also.\nRule 2 :Transition states are ordered. Continue is preferred to Retain is\npreferred to Smooth-Shift is preferred to Rough-Shift.\nRule 1 captures the intuition that pronominalization (including zero-anaphora)\nis a common way to mark discourse salience. If there are multiple pronouns in an\nutterance realizing entities from the previous utterance, one of these pronouns must\nrealize the backward center Cb; if there is only one pronoun, it must be Cb.\nRule 2 captures the intuition that discourses that continue to center the same en-\ntity are more coherent than ones that repeatedly shift to other centers. The transition\ntable is based on two factors: whether the backward-looking center Cbis the same\nfrom UntoUn+1and whether this discourse entity is the one that is preferred ( Cp)\nin the new utterance Un+1. If both of these hold, a CONTINUE relation, the speaker\nhas been talking about the same entity and is going to continue talking about that",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 824,
      "token_count": 361,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 550\n\n542 CHAPTER 24 \u2022 D ISCOURSE COHERENCE\nentity. In a RETAIN relation, the speaker intends to SHIFT to a new entity in a future\nutterance and meanwhile places the current entity in a lower rank Cf. In a SHIFT\nrelation, the speaker is shifting to a new salient entity.\nLet\u2019s walk though the start of (24.28) again, repeated as (24.30), showing the\nrepresentations after each utterance is processed.\n(24.30) John went to his favorite music store to buy a piano. ( U1)\nHe was excited that he could \ufb01nally buy a piano. ( U2)\nHe arrived just as the store was closing for the day. ( U3)\nIt was closing just as John arrived ( U4)\nUsing the grammatical role hierarchy to order the C f, for sentence U1we get:\nCf(U1):fJohn, music store, piano g\nCp(U1): John\nCb(U1): unde\ufb01ned\nand then for sentence U2:\nCf(U2):fJohn, pianog\nCp(U2): John\nCb(U2): John\nResult: Continue ( Cp(U2)=Cb(U2);Cb(U1)unde\ufb01ned)\nThe transition from U1toU2is thus a CONTINUE . Completing this example is left\nas exercise (1) for the reader\n24.3.2 Entity Grid model\nCentering embodies a particular theory of how entity mentioning leads to coher-\nence: that salient entities appear in subject position or are pronominalized, and that\ndiscourses are salient by means of continuing to mention the same entity in such\nways.\nThe entity grid model of Barzilay and Lapata (2008) is an alternative way to entity grid\ncapture entity-based coherence: instead of having a top-down theory, the entity-grid\nmodel using machine learning to induce the patterns of entity mentioning that make\na discourse more coherent.\nThe model is based around an entity grid , a two-dimensional array that repre-\nsents the distribution of entity mentions across sentences. The rows represent sen-\ntences, and the columns represent discourse entities (most versions of the entity grid\nmodel focus just on nominal mentions). Each cell represents the possible appearance\nof an entity in a sentence, and the values represent whether the entity appears and its\ngrammatical role. Grammatical roles are subject ( S), object ( O), neither ( X), or ab-\nsent (\u2013); in the implementation of Barzilay and Lapata (2008), subjects of passives\nare represented with O, leading to a representation with some of the characteristics\nof thematic roles.\nFig. 24.8 from Barzilay and Lapata (2008) shows a grid for the text shown in\nFig. 24.9. There is one row for each of the six sentences. The second column, for\nthe entity \u2018trial\u2019, is O\u2013 \u2013 \u2013 X, showing that the trial appears in the \ufb01rst sentence as\ndirect object, in the last sentence as an oblique, and does not appear in the middle\nsentences. The third column, for the entity Microsoft, shows that it appears as sub-\nject in sentence 1 (it also appears as the object of the preposition against , but entities\nthat appear multiple times are recorded with their highest-ranked grammatical func-\ntion). Computing the entity grids requires extracting entities and doing coreference",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 825,
      "token_count": 734,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 551",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 826,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "24.3 \u2022 C ENTERING AND ENTITY -BASED COHERENCE 543\nComputational Linguistics Volume 34, Number 1\nthese patterns can be encoded as feature vectors appropriate for performing coherence-\nrelated ranking and classi\ufb01cation tasks.\n3.1 The Entity-Grid Discourse Representation\nEach text is represented by an entity grid ,at w o - d i m e n s i o n a la r r a yt h a tc a p t u r e s\nthe distribution of discourse entities across text sentences. We follow Miltsakaki and\nKukich (2000) in assuming that our unit of analysis is the traditional sentence (i.e., a\nmain clause with accompanying subordinate and adjunct clauses). The rows of the\ngrid correspond to sentences, and the columns correspond to discourse entities. By\ndiscourse entity we mean a class of coreferent noun phrases (we explain in Section 3.3\nhow coreferent entities are identi\ufb01ed). For each occurrence of a discourse entity in the\ntext, the corresponding grid cell contains information about its presence or absence\nin a sequence of sentences. In addition, for entities present in a given sentence, grid\ncells contain information about their syntactic role. Such information can be expressed\nin many ways (e.g., using constituent labels or thematic role information). Because\ngrammatical relations \ufb01gure prominently in entity-based theories of local coherence (see\nSection 2), they serve as a logical point of departure. Each grid cell thus corresponds to\na string from a set of categories re\ufb02ecting whether the entity in question is a subject ( S),\nobject ( O), or neither ( X). Entities absent from a sentence are signaled by gaps ( \u2013).\nGrammatical role information can be extracted from the output of a broad-coverage\ndependency parser (Lin 2001; Briscoe and Carroll 2002) or any state-of-the art statistical\nparser (Collins 1997; Charniak 2000). We discuss how this information was computed\nfor our experiments in Section 3.3.\nTable 1 illustrates a fragment of an entity grid constructed for the text in Table 2.\nBecause the text contains six sentences, the grid columns are of length six. Consider\nfor instance the grid column for the entity trial ,[O\u2013\u2013\u2013\u2013 X].I tr e c o r d st h a t trial is\npresent in sentences 1 and 6 (as Oand X,r e s p e c t i v e l y )b u ti sa b s e n tf r o mt h er e s to ft h e\nsentences. Also note that the grid in Table 1 takes coreference resolution into account.\nEven though the same entity appears in different linguistic forms, for example, Microsoft\nCorp. ,Microsoft ,a n d the company , it is mapped to a single entry in the grid (see the\ncolumn introduced by Microsoft in Table 1).\nTable 1\nA fragment of the entity grid. Noun phrases are represented by their head nouns. Grid cells\ncorrespond to grammatical roles: subjects ( S), objects ( O), or neither ( X).Department\nTrial\nMicrosoft\nEvidence\nCompetitors\nMarkets\nProducts\nBrands\nCase\nNetscape\nSoftware\nTactics\nGovernment\nSuit\nEarnings\n1SO SXO \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 1\n2\u2013\u2013 O\u2013\u2013 XSO \u2013\u2013\u2013\u2013 \u2013\u2013\u2013 2\n3\u2013\u2013 SO \u2013\u2013\u2013\u2013 SOO \u2013\u2013\u2013 \u2013 3\n4\u2013\u2013 S\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 S\u2013\u2013\u2013 4\n5\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 SO \u20135\n6\u2013XS \u2013\u2013\u2013\u2013\u2013 \u2013\u2013\u2013 \u2013 \u2013 \u2013 O6\n6",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 827,
      "token_count": 785,
      "chapter_title": ""
    }
  },
  {
    "content": "present in sentences 1 and 6 (as Oand X,r e s p e c t i v e l y )b u ti sa b s e n tf r o mt h er e s to ft h e\nsentences. Also note that the grid in Table 1 takes coreference resolution into account.\nEven though the same entity appears in different linguistic forms, for example, Microsoft\nCorp. ,Microsoft ,a n d the company , it is mapped to a single entry in the grid (see the\ncolumn introduced by Microsoft in Table 1).\nTable 1\nA fragment of the entity grid. Noun phrases are represented by their head nouns. Grid cells\ncorrespond to grammatical roles: subjects ( S), objects ( O), or neither ( X).Department\nTrial\nMicrosoft\nEvidence\nCompetitors\nMarkets\nProducts\nBrands\nCase\nNetscape\nSoftware\nTactics\nGovernment\nSuit\nEarnings\n1SO SXO \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 1\n2\u2013\u2013 O\u2013\u2013 XSO \u2013\u2013\u2013\u2013 \u2013\u2013\u2013 2\n3\u2013\u2013 SO \u2013\u2013\u2013\u2013 SOO \u2013\u2013\u2013 \u2013 3\n4\u2013\u2013 S\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 S\u2013\u2013\u2013 4\n5\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013 SO \u20135\n6\u2013XS \u2013\u2013\u2013\u2013\u2013 \u2013\u2013\u2013 \u2013 \u2013 \u2013 O6\n6\nFigure 24.8 Part of the entity grid for the text in Fig. 24.9. Entities are listed by their head\nnoun; each cell represents whether an entity appears as subject ( S), object ( O), neither ( X), or\nis absent (\u2013). Figure from Barzilay and Lapata (2008).\nBarzilay and Lapata Modeling Local Coherence\nTable 2\nSummary augmented with syntactic annotations for grid computation.\n1 [The Justice Department]Sis conducting an [anti-trust trial]Oagainst [Microsoft Corp.]X\nwith [evidence]Xthat [the company]Sis increasingly attempting to crush [competitors]O.\n2[ M i c r o s o f t ]Ois accused of trying to forcefully buy into [markets]Xwhere [its own\nproducts]Sare not competitive enough to unseat [established brands]O.\n3[ T h e c a s e ]Srevolves around [evidence]Oof [Microsoft]Saggressively pressuring\n[Netscape]Ointo merging [browser software]O.\n4[ M i c r o s o f t ]Sclaims [its tactics]Sare commonplace and good economically.\n5 [The government]Smay \ufb01le [a civil suit]Oruling that [conspiracy]Sto curb [competition]O\nthrough [collusion]Xis [a violation of the Sherman Act]O.\n6[ M i c r o s o f t ]Scontinues to show [increased earnings]Odespite [the trial]X.\nWhen a noun is attested more than once with a different grammatical role in the\nsame sentence, we default to the role with the highest grammatical ranking: subjects are\nranked higher than objects, which in turn are ranked higher than the rest. For example,\nthe entity Microsoft is mentioned twice in Sentence 1 with the grammatical roles x(for\nMicrosoft Corp. )a n d s(forthe company ), but is represented only by sin the grid (see\nTables 1 and 2).\n3.2 Entity Grids as Feature Vectors",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 828,
      "token_count": 727,
      "chapter_title": ""
    }
  },
  {
    "content": "products]Sare not competitive enough to unseat [established brands]O.\n3[ T h e c a s e ]Srevolves around [evidence]Oof [Microsoft]Saggressively pressuring\n[Netscape]Ointo merging [browser software]O.\n4[ M i c r o s o f t ]Sclaims [its tactics]Sare commonplace and good economically.\n5 [The government]Smay \ufb01le [a civil suit]Oruling that [conspiracy]Sto curb [competition]O\nthrough [collusion]Xis [a violation of the Sherman Act]O.\n6[ M i c r o s o f t ]Scontinues to show [increased earnings]Odespite [the trial]X.\nWhen a noun is attested more than once with a different grammatical role in the\nsame sentence, we default to the role with the highest grammatical ranking: subjects are\nranked higher than objects, which in turn are ranked higher than the rest. For example,\nthe entity Microsoft is mentioned twice in Sentence 1 with the grammatical roles x(for\nMicrosoft Corp. )a n d s(forthe company ), but is represented only by sin the grid (see\nTables 1 and 2).\n3.2 Entity Grids as Feature Vectors\nAf u n d a m e n t a la s s u m p t i o nu n d e r l y i n go u ra p p r o a c hi st h a tt h ed i s t r i b u t i o no fe n t i t i e s\nin coherent texts exhibits certain regularities re\ufb02ected in grid topology. Some of these\nregularities are formalized in Centering Theory as constraints on transitions of the\nlocal focus in adjacent sentences. Grids of coherent texts are likely to have some dense\ncolumns (i.e., columns with just a few gaps, such as Microsoft in Table 1) and many\nsparse columns which will consist mostly of gaps (see markets andearnings in Table 1).\nOne would further expect that entities corresponding to dense columns are more often\nsubjects or objects. These characteristics will be less pronounced in low-coherence texts.\nInspired by Centering Theory, our analysis revolves around patterns of local entity\ntransitions. A local entity transition is a sequence {S,O,X,\u2013}nthat represents entity\noccurrences and their syntactic roles in nadjacent sentences. Local transitions can be\neasily obtained from a grid as continuous subsequences of each column. Each transition\nwill have a certain probability in a given grid. For instance, the probability of the\ntransition [S\u2013]in the grid from Table 1 is 0 .08 (computed as a ratio of its frequency\n[i.e., six] divided by the total number of transitions of length two [i.e., 75]). Each text\ncan thus be viewed as a distribution de\ufb01ned over transition types.\nWe can now go one step further and represent each text by a \ufb01xed set of transition\nsequences using a standard feature vector notation. Each grid rendering jof a document\ndicorresponds to a feature vector \u03a6(xij)=(p1(xij),p2(xij),... ,pm(xij)), where mis the\nnumber of all prede\ufb01ned entity transitions, and pt(xij)t h ep r o b a b i l i t yo ft r a n s i t i o n t",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 829,
      "token_count": 732,
      "chapter_title": ""
    }
  },
  {
    "content": "One would further expect that entities corresponding to dense columns are more often\nsubjects or objects. These characteristics will be less pronounced in low-coherence texts.\nInspired by Centering Theory, our analysis revolves around patterns of local entity\ntransitions. A local entity transition is a sequence {S,O,X,\u2013}nthat represents entity\noccurrences and their syntactic roles in nadjacent sentences. Local transitions can be\neasily obtained from a grid as continuous subsequences of each column. Each transition\nwill have a certain probability in a given grid. For instance, the probability of the\ntransition [S\u2013]in the grid from Table 1 is 0 .08 (computed as a ratio of its frequency\n[i.e., six] divided by the total number of transitions of length two [i.e., 75]). Each text\ncan thus be viewed as a distribution de\ufb01ned over transition types.\nWe can now go one step further and represent each text by a \ufb01xed set of transition\nsequences using a standard feature vector notation. Each grid rendering jof a document\ndicorresponds to a feature vector \u03a6(xij)=(p1(xij),p2(xij),... ,pm(xij)), where mis the\nnumber of all prede\ufb01ned entity transitions, and pt(xij)t h ep r o b a b i l i t yo ft r a n s i t i o n t\nin grid xij.T h i sf e a t u r ev e c t o rr e p r e s e n t a t i o ni su s e f u l l ya m e n a b l et om a c h i n el e a r n i n g\nalgorithms (see our experiments in Sections 4\u20136). Furthermore, it allows the consid-\neration of large numbers of transitions which could potentially uncover novel entity\ndistribution patterns relevant for coherence assessment or other coherence-related tasks.\nNote that considerable latitude is available when specifying the transition types to\nbe included in a feature vector. These can be all transitions of a given length (e.g., two\nor three) or the most frequent transitions within a document collection. An example of\n7\nFigure 24.9 A discourse with the entities marked and annotated with grammatical func-\ntions. Figure from Barzilay and Lapata (2008).\nresolution to cluster them into discourse entities (Chapter 23) as well as parsing the\nsentences to get grammatical roles.\nIn the resulting grid, columns that are dense (like the column for Microsoft) in-\ndicate entities that are mentioned often in the texts; sparse columns (like the column\nfor earnings) indicate entities that are mentioned rarely.\nIn the entity grid model, coherence is measured by patterns of local entity tran-\nsition . For example, Department is a subject in sentence 1, and then not men-\ntioned in sentence 2; this is the transition [ S\u2013]. The transitions are thus sequences\nfS,O X, \u2013gnwhich can be extracted as continuous cells from each column. Each\ntransition has a probability; the probability of [ S\u2013] in the grid from Fig. 24.8 is 0.08\n(it occurs 6 times out of the 75 total transitions of length two). Fig. 24.10 shows the\ndistribution over transitions of length 2 for the text of Fig. 24.9 (shown as the \ufb01rst\nrowd1), and 2 other documents.\nComputational Linguistics Volume 34, Number 1",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 830,
      "token_count": 739,
      "chapter_title": ""
    }
  },
  {
    "content": "7\nFigure 24.9 A discourse with the entities marked and annotated with grammatical func-\ntions. Figure from Barzilay and Lapata (2008).\nresolution to cluster them into discourse entities (Chapter 23) as well as parsing the\nsentences to get grammatical roles.\nIn the resulting grid, columns that are dense (like the column for Microsoft) in-\ndicate entities that are mentioned often in the texts; sparse columns (like the column\nfor earnings) indicate entities that are mentioned rarely.\nIn the entity grid model, coherence is measured by patterns of local entity tran-\nsition . For example, Department is a subject in sentence 1, and then not men-\ntioned in sentence 2; this is the transition [ S\u2013]. The transitions are thus sequences\nfS,O X, \u2013gnwhich can be extracted as continuous cells from each column. Each\ntransition has a probability; the probability of [ S\u2013] in the grid from Fig. 24.8 is 0.08\n(it occurs 6 times out of the 75 total transitions of length two). Fig. 24.10 shows the\ndistribution over transitions of length 2 for the text of Fig. 24.9 (shown as the \ufb01rst\nrowd1), and 2 other documents.\nComputational Linguistics Volume 34, Number 1\naf e a t u r es p a c ew i t ht r a n s i t i o n so fl e n g t ht w oi si l l u s t r a t e di nT a b l e3 .T h es e c o n dr o w\n(introduced by d1)i st h ef e a t u r ev e c t o rr e p r e s e n t a t i o no ft h eg r i di nT a b l e1 .\n3.3 Grid Construction: Linguistic Dimensions\nOne of the central research issues in developing entity-based models of coherence is\ndetermining what sources of linguistic knowledge are essential for accurate prediction,\nand how to encode them succinctly in a discourse representation. Previous approaches\ntend to agree on the features of entity distribution related to local coherence\u2014the\ndisagreement lies in the way these features are modeled.\nOur study of alternative encodings is not a mere duplication of previous ef-\nforts (Poesio et al. 2004) that focus on linguistic aspects of parameterization. Because we\nare interested in an automatically constructed model, we have to take into account com-\nputational and learning issues when considering alternative representations. Therefore,\nour exploration of the parameter space is guided by three considerations: the linguistic\nimportance of a parameter, the accuracy of its automatic computation, and the size of the\nresulting feature space. From the linguistic side, we focus on properties of entity distri-\nbution that are tightly linked to local coherence, and at the same time allow for multiple\ninterpretations during the encoding process. Computational considerations prevent us\nfrom considering discourse representations that cannot be computed reliably by exist-\ning tools. For instance, we could not experiment with the granularity of an utterance\u2014\nsentence versus clause\u2014because available clause separators introduce substantial noise\ninto a grid construction. Finally, we exclude representations that will explode the size of\nthe feature space, thereby increasing the amount of data required for training the model.\nEntity Ex traction. The accurate computation of entity classes is key to computing mean-\ningful entity grids. In previous implementations of entity-based models, classes of coref-\nerent nouns have been extracted manually (Miltsakaki and Kukich 2000; Karamanis",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 831,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "Our study of alternative encodings is not a mere duplication of previous ef-\nforts (Poesio et al. 2004) that focus on linguistic aspects of parameterization. Because we\nare interested in an automatically constructed model, we have to take into account com-\nputational and learning issues when considering alternative representations. Therefore,\nour exploration of the parameter space is guided by three considerations: the linguistic\nimportance of a parameter, the accuracy of its automatic computation, and the size of the\nresulting feature space. From the linguistic side, we focus on properties of entity distri-\nbution that are tightly linked to local coherence, and at the same time allow for multiple\ninterpretations during the encoding process. Computational considerations prevent us\nfrom considering discourse representations that cannot be computed reliably by exist-\ning tools. For instance, we could not experiment with the granularity of an utterance\u2014\nsentence versus clause\u2014because available clause separators introduce substantial noise\ninto a grid construction. Finally, we exclude representations that will explode the size of\nthe feature space, thereby increasing the amount of data required for training the model.\nEntity Ex traction. The accurate computation of entity classes is key to computing mean-\ningful entity grids. In previous implementations of entity-based models, classes of coref-\nerent nouns have been extracted manually (Miltsakaki and Kukich 2000; Karamanis\net al. 2004; Poesio et al. 2004), but this is not an option for our model. An obvious\nsolution for identifying entity classes is to employ an automatic coreference resolution\ntool that determines which noun phrases refer to the same entity in a document.\nCurrent approaches recast coreference resolution as a classi\ufb01cation task. A pair\nof NPs is classi\ufb01ed as coreferring or not based on constraints that are learned from\nan annotated corpus. A separate clustering mechanism then coordinates the possibly\ncontradictory pairwise classi\ufb01cations and constructs a partition on the set of NPs. In\nour experiments, we employ Ng and Cardie\u2019s (2002) coreference resolution system.\nThe system decides whether two NPs are coreferent by exploiting a wealth of lexical,\ngrammatical, semantic, and positional features. It is trained on the MUC (6\u20137) data sets\nand yields state-of-the-art performance (70.4 F-measure on MUC-6 and 63.4 on MUC-7).\nTable 3\nExample of a feature-vector document representation using all transitions of length two given\nsyntactic categories S,O,X,a n d \u2013.\nSS SO SX S \u2013OS OO OX O \u2013XS XO XX X \u2013\u2013 S\u2013O\u2013X\u2013\u2013\nd1.01 .01 0 .08 .01 0 0 .09 0 0 0 .03 .05 .07 .03 .59\nd2.02 .01 .01 .02 0 .07 0 .02 .14 .14 .06 .04 .03 .07 0.1 .36\nd3.02 0 0 .03 .09 0 .09 .06 0 0 0 .05 .03 .07 .17 .39\n8\nFigure 24.10 A feature vector for representing documents using all transitions of length 2.\nDocument d1is the text in Fig. 24.9. Figure from Barzilay and Lapata (2008).\nThe transitions and their probabilities can then be used as features for a machine\nlearning model. This model can be a text classi\ufb01er trained to produce human-labeled\ncoherence scores (for example from humans labeling each text as coherent or inco-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 832,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "and yields state-of-the-art performance (70.4 F-measure on MUC-6 and 63.4 on MUC-7).\nTable 3\nExample of a feature-vector document representation using all transitions of length two given\nsyntactic categories S,O,X,a n d \u2013.\nSS SO SX S \u2013OS OO OX O \u2013XS XO XX X \u2013\u2013 S\u2013O\u2013X\u2013\u2013\nd1.01 .01 0 .08 .01 0 0 .09 0 0 0 .03 .05 .07 .03 .59\nd2.02 .01 .01 .02 0 .07 0 .02 .14 .14 .06 .04 .03 .07 0.1 .36\nd3.02 0 0 .03 .09 0 .09 .06 0 0 0 .05 .03 .07 .17 .39\n8\nFigure 24.10 A feature vector for representing documents using all transitions of length 2.\nDocument d1is the text in Fig. 24.9. Figure from Barzilay and Lapata (2008).\nThe transitions and their probabilities can then be used as features for a machine\nlearning model. This model can be a text classi\ufb01er trained to produce human-labeled\ncoherence scores (for example from humans labeling each text as coherent or inco-\nherent). But such data is expensive to gather. Barzilay and Lapata (2005) introduced\na simplifying innovation: coherence models can be trained by self-supervision :\ntrained to distinguish the natural original order of sentences in a discourse from",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 833,
      "token_count": 345,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 552\n\n544 CHAPTER 24 \u2022 D ISCOURSE COHERENCE\na modi\ufb01ed order (such as a randomized order). We turn to these evaluations in the\nnext section.\n24.3.3 Evaluating Neural and Entity-based coherence\nEntity-based coherence models, as well as the neural models we introduce in the\nnext section, are generally evaluated in one of two ways.\nFirst, we can have humans rate the coherence of a document and train a classi\ufb01er\nto predict these human ratings, which can be categorial (high/low, or high/mid/low)\nor continuous. This is the best evaluation to use if we have some end task in mind,\nlike essay grading, where human raters are the correct de\ufb01nition of the \ufb01nal label.\nAlternatively, since it\u2019s very expensive to get human labels, and we might not\nyet have an end-task in mind, we can use natural texts to do self-supervision. In\nself-supervision we pair up a natural discourse with a pseudo-document created by\nchanging the ordering. Since naturally-ordered discourses are more coherent than\nrandom permutation (Lin et al., 2011), a successful coherence algorithm should pre-\nfer the original ordering.\nSelf-supervision has been implemented in 3 ways. In the sentence order dis-\ncrimination task (Barzilay and Lapata, 2005), we compare a document to a random\npermutation of its sentences. A model is considered correct for an (original, per-\nmuted) test pair if it ranks the original document higher. Given kdocuments, we can\ncompute npermutations, resulting in knpairs each with one original document and\none permutation, to use in training and testing.\nIn the sentence insertion task (Chen et al., 2007) we take a document, remove\none of the nsentences s, and create n\u00001 copies of the document with sinserted into\neach position. The task is to decide which of the ndocuments is the one with the\noriginal ordering, distinguishing the original position for sfrom all other positions.\nInsertion is harder than discrimination since we are comparing documents that differ\nby only one sentence.\nFinally, in the sentence order reconstruction task (Lapata, 2003), we take a\ndocument, randomize the sentences, and train the model to put them back in the\ncorrect order. Again given kdocuments, we can compute npermutations, resulting\ninknpairs each with one original document and one permutation, to use in training\nand testing. Reordering is of course a much harder task than simple classi\ufb01cation.\n24.4 Representation learning models for local coherence\nThe third kind of local coherence is topical or semantic \ufb01eld coherence. Discourses\ncohere by talking about the same topics and subtopics, and drawing on the same\nsemantic \ufb01elds in doing so.\nThe \ufb01eld was pioneered by a series of unsupervised models in the 1990s of this\nkind of coherence that made use of lexical cohesion (Halliday and Hasan, 1976): lexical cohesion\nthe sharing of identical or semantically related words in nearby sentences. Morris\nand Hirst (1991) computed lexical chains of words (like pine,bush trees ,trunk ) that\noccurred through a discourse and that were related in Roget\u2019s Thesaurus (by being in\nthe same category, or linked categories). They showed that the number and density\nof chain correlated with the topic structure. The TextTiling algorithm of Hearst TextTiling\n(1997) computed the cosine between neighboring text spans (the normalized dot\nproduct of vectors of raw word counts), again showing that sentences or paragraph in",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 834,
      "token_count": 788,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 553\n\n24.4 \u2022 R EPRESENTATION LEARNING MODELS FOR LOCAL COHERENCE 545\na subtopic have high cosine with each other, but not with sentences in a neighboring\nsubtopic.\nA third early model, the LSA Coherence method of Foltz et al. (1998) was the\n\ufb01rst to use embeddings, modeling the coherence between two sentences as the co-\nsine between their LSA sentence embedding vectors1, computing embeddings for a\nsentence sby summing the embeddings of its words w:\nsim(s;t) = cos(s;t)\n=cos(X\nw2sw;X\nw2tw) (24.31)\nand de\ufb01ning the overall coherence of a text as the average similarity over all pairs of\nadjacent sentences siandsi+1:\ncoherence (T) =1\nn\u00001n\u00001X\ni=1cos(si;si+1) (24.32)\nModern neural representation-learning coherence models, beginning with Li et al.\n(2014), draw on the intuitions of these early unsupervised models for learning sen-\ntence representations and measuring how they change between neighboring sen-\ntences. But the new models also draw on the idea pioneered by Barzilay and Lapata\n(2005) of self-supervision. That is, unlike say coherence relation models, which\ntrain on hand-labeled representations for RST or PDTB, these models are trained to\ndistinguish natural discourses from unnatural discourses formed by scrambling the\norder of sentences, thus using representation learning to discover the features that\nmatter for at least the ordering aspect of coherence.\nHere we present one such model, the local coherence discriminator (LCD) (Xu\net al., 2019). Like early models, LCD computes the coherence of a text as the av-\nerage of coherence scores between consecutive pairs of sentences. But unlike the\nearly unsupervised models, LCD is a self-supervised model trained to discriminate\nconsecutive sentence pairs (si;si+1)in the training documents (assumed to be coher-\nent) from (constructed) incoherent pairs (si;s0). All consecutive pairs are positive\nexamples, and the negative (incoherent) partner for a sentence siis another sentence\nuniformly sampled from the same document as si.\nFig. 24.11 describes the architecture of the model fq, which takes a sentence\npair and returns a score, higher scores for more coherent pairs. Given an input\nsentence pair sandt, the model computes sentence embeddings sandt(using any\nsentence embeddings algorithm), and then concatenates four features of the pair: (1)\nthe concatenation of the two vectors (2) their difference s\u0000t; (3) the absolute value\nof their difference js\u0000tj; (4) their element-wise product s\ft. These are passed\nthrough a one-layer feedforward network to output the coherence score.\nThe model is trained to make this coherence score higher for real pairs than for\nnegative pairs. More formally, the training objective for a corpus Cof documents d,\neach of which consists of a list of sentences si, is:\nLq=X\nd2CX\nsi2dE\np(s0jsi)[L(fq(si;si+1);fq(si;s0))] (24.33)\nEp(s0jsi)is the expectation with respect to the negative sampling distribution con-\nditioned on si: given a sentence sithe algorithms samples a negative sentence s0\n1See Chapter 6 for more on LSA embeddings; they are computed by applying SVD to the term-\ndocument matrix (each cell weighted by log frequency and normalized by entropy), and then the \ufb01rst\n300 dimensions are used as the embedding.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 835,
      "token_count": 794,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 554",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 836,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "546 CHAPTER 24 \u2022 D ISCOURSE COHERENCE\n681Loss function: The role of the loss function is\nto encourage f+=f\u2713(si,si+1)to be high while\nf\u0000=f\u2713(si,s0)to be low. Common losses such as\nmargin or log loss can all be used. Through exper-\nimental validation, we found that margin loss to\nbe superior for this problem. Speci\ufb01cally, Ltakes\non the form: L(f+,f\u0000) = max(0 ,\u2318\u0000f++f\u0000)\nwhere \u2318is the margin hyperparameter.\nNegative samples: Technically, we are free to\nchoose any sentence s0to form a negative pair\nwith si. However, because of potential differ-\nences in genre, topic and writing style, such neg-\natives might cause the discriminative model to\nlearn cues unrelated to coherence. Therefore, we\nonly select sentences from the same document to\nconstruct negative pairs. Speci\ufb01cally, suppose si\ncomes from document dkwith length nk, then\np(s0|si)is a uniform distribution over the nk\u00001\nsentences {sj}j6=ifrom dk. For a document with\nnsentences, there are n\u00001positive pairs, and\n(n\u00001)\u21e4(n\u00002)/2negative pairs. It turns out that\nthe quadratic number of negatives provides a rich\nenough learning signal, while at the same time, is\nnot too prohibitively large to be effectively cov-\nered by a sampling procedure. In practice, we\nsample a new set of negatives each time we see\na document, hence after many epochs, we can ef-\nfectively cover the space for even very long doc-\numents. Section 5.7discusses further details on\nsampling.\n4.1 Model Architecture\nThe speci\ufb01c neural architecture that we use for f\u2713\nis illustrated in Figure 1. We assume the use of\nsome pre-trained sentence encoder, which is dis-\ncussed in the next section.\nGiven an input sentence pair, the sentence en-\ncoder maps the sentences to real-valued vectors S\nandT. We then compute the concatenation of the\nfollowing features: (1)concatenation of the two\nvectors (S, T);(2)element-wise difference S\u0000T;\n(3)element-wise product S\u21e4T;(4)absolute value\nof element-wise difference |S\u0000T|. The concate-\nnated feature representation is then fed to a one-\nlayer MLP to output the coherence score.\nIn practice, we make our overall coherence\nmodel bidirectional, by training a forward model\nwith input (S, T)and a backward model with in-\nput(T,S)with the same architecture but separate\nparameters. The coherence score is then the aver-\nage from the two models.\nFigure 1: Generic architecture for our proposed model.\n4.2 Pre-trained Generative Model as the\nSentence Encoder\nOur model can work with any pre-trained sen-\ntence encoder, ranging from the most simplistic\naverage GloVe ( Pennington et al. ,2014 ) embed-\ndings to more sophisticated supervised or unsu-\npervised pre-trained sentence encoders ( Conneau\net al. ,2017 ). As mentioned in the introduction,\nsince generative models can often be turned into\nsentence encoder, generative coherence model can\nbe leveraged by our model to bene\ufb01t from the\nadvantages of both generative and discriminative\ntraining, similar to ( Kiros et al. ,2015 ;Peters et al. ,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 837,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "vectors (S, T);(2)element-wise difference S\u0000T;\n(3)element-wise product S\u21e4T;(4)absolute value\nof element-wise difference |S\u0000T|. The concate-\nnated feature representation is then fed to a one-\nlayer MLP to output the coherence score.\nIn practice, we make our overall coherence\nmodel bidirectional, by training a forward model\nwith input (S, T)and a backward model with in-\nput(T,S)with the same architecture but separate\nparameters. The coherence score is then the aver-\nage from the two models.\nFigure 1: Generic architecture for our proposed model.\n4.2 Pre-trained Generative Model as the\nSentence Encoder\nOur model can work with any pre-trained sen-\ntence encoder, ranging from the most simplistic\naverage GloVe ( Pennington et al. ,2014 ) embed-\ndings to more sophisticated supervised or unsu-\npervised pre-trained sentence encoders ( Conneau\net al. ,2017 ). As mentioned in the introduction,\nsince generative models can often be turned into\nsentence encoder, generative coherence model can\nbe leveraged by our model to bene\ufb01t from the\nadvantages of both generative and discriminative\ntraining, similar to ( Kiros et al. ,2015 ;Peters et al. ,\n2018 ). After initialization, we freeze the genera-\ntive model parameters to avoid over\ufb01tting.\nIn Section 5, we will experimentally show that\nwhile we do bene\ufb01t from strong pre-trained en-\ncoders, the fact that our local discriminative model\nimproves over previous methods is independent of\nthe choice of sentence encoder.\n5 Experiments\n5.1 Evaluation Tasks\nFollowing Nguyen and Joty (2017 ) and other pre-\nvious work, we evaluate our models on the dis-\ncrimination and insertion tasks. Additionally, we\nevaluate on the paragraph reconstruction task in\nopen-domain settings, in a similar manner to Li\nand Jurafsky (2017 ).\nIn the discrimination task, a document is com-\npared to a random permutation of its sentences,\nand the model is considered correct if it scores the\noriginal document higher than the permuted one.\nTwenty permutations are used in the test set in ac-\ncordance with previous work.\nFigure 24.11 The architecture of the LCD model of document coherence, showing the\ncomputation of the score for a pair of sentences sandt. Figure from Xu et al. (2019).\nuniformly over the other sentences in the same document. Lis a loss function that\ntakes two scores, one for a positive pair and one for a negative pair, with the goal of\nencouraging f+=fq(si;si+1)to be high and f\u0000=fq(si;s0))to be low. Fig. 24.11\nuse the margin loss l(f+;f\u0000) =max(0;h\u0000f++f\u0000)where his the margin hyper-\nparameter.\nXu et al. (2019) also give a useful baseline algorithm that itself has quite high\nperformance in measuring perplexity: train an RNN language model on the data,\nand compute the log likelihood of sentence siin two ways, once given the preceding\ncontext (conditional log likelihood) and once with no context (marginal log likeli-\nhood). The difference between these values tells us how much the preceding context\nimproved the predictability of si, a predictability measure of coherence.\nTraining models to predict longer contexts than just consecutive pairs of sen-\ntences can result in even stronger discourse representations. For example a Trans-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 838,
      "token_count": 764,
      "chapter_title": ""
    }
  },
  {
    "content": "cordance with previous work.\nFigure 24.11 The architecture of the LCD model of document coherence, showing the\ncomputation of the score for a pair of sentences sandt. Figure from Xu et al. (2019).\nuniformly over the other sentences in the same document. Lis a loss function that\ntakes two scores, one for a positive pair and one for a negative pair, with the goal of\nencouraging f+=fq(si;si+1)to be high and f\u0000=fq(si;s0))to be low. Fig. 24.11\nuse the margin loss l(f+;f\u0000) =max(0;h\u0000f++f\u0000)where his the margin hyper-\nparameter.\nXu et al. (2019) also give a useful baseline algorithm that itself has quite high\nperformance in measuring perplexity: train an RNN language model on the data,\nand compute the log likelihood of sentence siin two ways, once given the preceding\ncontext (conditional log likelihood) and once with no context (marginal log likeli-\nhood). The difference between these values tells us how much the preceding context\nimproved the predictability of si, a predictability measure of coherence.\nTraining models to predict longer contexts than just consecutive pairs of sen-\ntences can result in even stronger discourse representations. For example a Trans-\nformer language model trained with a contrastive sentence objective to predict text\nup to a distance of \u00062 sentences improves performance on various discourse coher-\nence tasks (Iter et al., 2020).\nLanguage-model style models are generally evaluated by the methods of Sec-\ntion 24.3.3, although they can also be evaluated on the RST and PDTB coherence\nrelation tasks.\n24.5 Global Coherence\nA discourse must also cohere globally rather than just at the level of pairs of sen-\ntences. Consider stories, for example. The narrative structure of stories is one of\nthe oldest kinds of global coherence to be studied. In his in\ufb02uential Morphology of\nthe Folktale , Propp (1968) models the discourse structure of Russian folktales via\na kind of plot grammar. His model includes a set of character categories he called\ndramatis personae , like Hero, Villain, Donor, or Helper, and a set of events he\ncalled functions (like \u201cVillain commits kidnapping\u201d, \u201cDonor tests Hero\u201d, or \u201cHero\nis pursued\u201d) that have to occur in particular order, along with other components.\nPropp shows that the plots of each of the fairy tales he studies can be represented as",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 839,
      "token_count": 547,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 555",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 840,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "24.5 \u2022 G LOBAL COHERENCE 547\na sequence of these functions, different tales choosing different subsets of functions,\nbut always in the same order. Indeed Lakoff (1972) showed that Propp\u2019s model\namounted to a discourse grammar of stories, and in recent computational work Fin-\nlayson (2016) demonstrates that some of these Proppian functions could be induced\nfrom corpora of folktale texts by detecting events that have similar actions across\nstories. Bamman et al. (2013) showed that generalizations over dramatis personae\ncould be induced from movie plot summaries on Wikipedia. Their model induced\nlatent personae from features like the actions the character takes (e.g., Villains stran-\ngle), the actions done to them (e.g., Villains are foiled and arrested) or the descriptive\nwords used of them (Villains are evil).\nIn this section we introduce two kinds of such global discourse structure that\nhave been widely studied computationally. The \ufb01rst is the structure of arguments:\nthe way people attempt to convince each other in persuasive essays by offering\nclaims and supporting premises. The second is somewhat related: the structure of\nscienti\ufb01c papers, and the way authors present their goals, results, and relationship to\nprior work in their papers.\n24.5.1 Argumentation Structure\nThe \ufb01rst type of global discourse structure is the structure of arguments . Analyzing\npeople\u2019s argumentation computationally is often called argumentation mining .argumentation\nmining\nThe study of arguments dates back to Aristotle, who in his Rhetorics described\nthree components of a good argument: pathos (appealing to the emotions of the pathos\nlistener), ethos (appealing to the speaker\u2019s personal character), and logos (the logical ethos\nlogos structure of the argument).\nMost of the discourse structure studies of argumentation have focused on logos ,\nparticularly via building and training on annotated datasets of persuasive essays or\nother arguments (Reed et al. 2008, Stab and Gurevych 2014a, Peldszus and Stede\n2016, Habernal and Gurevych 2017, Musi et al. 2018). Such corpora, for exam-\nple, often include annotations of argumentative components like claims (the central claims\ncomponent of the argument that is controversial and needs support) and premises premises\n(the reasons given by the author to persuade the reader by supporting or attacking\nthe claim or other premises), as well as the argumentative relations between themargumentative\nrelations\nlike SUPPORT and ATTACK .\nConsider the following example of a persuasive essay from Stab and Gurevych\n(2014b). The \ufb01rst sentence (1) presents a claim (in bold). (2) and (3) present two\npremises supporting the claim. (4) gives a premise supporting premise (3).\n\u201c(1) Museums and art galleries provide a better understanding\nabout arts than Internet. (2) In most museums and art galleries, de-\ntailed descriptions in terms of the background, history and author are\nprovided. (3) Seeing an artwork online is not the same as watching it\nwith our own eyes, as (4) the picture online does not show the texture\nor three-dimensional structure of the art, which is important to study.\u201d\nThus this example has three argumentative relations: SUPPORT (2,1), SUPPORT (3,1)\nand SUPPORT (4,3). Fig. 24.12 shows the structure of a much more complex argu-\nment.\nWhile argumentation mining is clearly related to rhetorical structure and other",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 841,
      "token_count": 764,
      "chapter_title": ""
    }
  },
  {
    "content": "ple, often include annotations of argumentative components like claims (the central claims\ncomponent of the argument that is controversial and needs support) and premises premises\n(the reasons given by the author to persuade the reader by supporting or attacking\nthe claim or other premises), as well as the argumentative relations between themargumentative\nrelations\nlike SUPPORT and ATTACK .\nConsider the following example of a persuasive essay from Stab and Gurevych\n(2014b). The \ufb01rst sentence (1) presents a claim (in bold). (2) and (3) present two\npremises supporting the claim. (4) gives a premise supporting premise (3).\n\u201c(1) Museums and art galleries provide a better understanding\nabout arts than Internet. (2) In most museums and art galleries, de-\ntailed descriptions in terms of the background, history and author are\nprovided. (3) Seeing an artwork online is not the same as watching it\nwith our own eyes, as (4) the picture online does not show the texture\nor three-dimensional structure of the art, which is important to study.\u201d\nThus this example has three argumentative relations: SUPPORT (2,1), SUPPORT (3,1)\nand SUPPORT (4,3). Fig. 24.12 shows the structure of a much more complex argu-\nment.\nWhile argumentation mining is clearly related to rhetorical structure and other\nkinds of coherence relations, arguments tend to be much less local; often a persua-\nsive essay will have only a single main claim, with premises spread throughout the\ntext, without the local coherence we see in coherence relations.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 842,
      "token_count": 337,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 556",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 843,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "548 CHAPTER 24 \u2022 D ISCOURSE COHERENCE\nStab and Gurevych Parsing Argumentation Structures\ncloning. This example illustrates that knowing argumentative relations is important for\nseparating several arguments in a paragraph. The example also shows that argument\ncomponents frequently exhibit preceding text units that are not relevant to the argument\nbut helpful for recognizing the argument component type. For example, preceding dis-\ncourse connectors like \u201ctherefore\u201d, \u201cconsequently\u201d, or \u201cthus\u201d can signal a subsequent\nclaim. Discourse markers like \u201cbecause\u201d, \u201csince\u201d, or \u201cfurthermore\u201d could indicate a\npremise. Formally, these preceding tokens of an argument component starting at token\ntiare de\ufb01ned as the tokens ti\u0000m,...,ti\u00001that are not covered by another argument\ncomponent in the sentence s=t1,t2,...,tnwhere 1 \uf8ffi\uf8ffnand i\u0000m\u00001. The third body\nparagraph illustrates a contra argument and argumentative attack relations:\nAdmittedly, [ cloning could bemisused formilitary purposes] Claim 5. For example,\n[:it:::::could:::be:::::used::to::::::::::manipulate:::::::human::::::genes::in::::::order::to::::::create::::::::obedient:::::::soldiers\n::::with::::::::::::extraordinary:::::::abilities] Premise 9. However, because [::::moral::::and:::::::ethical::::::values:::are\n::::::::::::internationally::::::shared] Premise 10,[:it:::is::::very::::::::unlikely::::that:::::::cloning::::will::be::::::::misused:::for\n::::::militant:::::::::objectives] Premise 11.\nThe paragraph begins with Claim 5, which attacks the stance of the author. It is supported\nbyPremise 9in the second sentence. The third sentence includes two premises, both of\nwhich defend the stance of the author. Premise 11is an attack of Claim 5, and Premise 10\nsupports Premise 11. The last paragraph (conclusion) restates the major claim and sum-\nmarizes the main aspects of the essay:\nTo sum up, although [ permitting cloning might bear some risks like misuse for\nmilitary purposes] Claim 6, I strongly believe that [ this technology is bene\ufb01cial to\nhumanity ]MajorClaim 2. It is likely that [ thistechnologybears some important cures which\nwill significantly improve lifeconditions] Claim 7.\nThe conclusion of the essay starts with an attacking claim followed by the restatement of\nthe major claim. The last sentence includes another claim that summarizes the most im-\nportant points of the author\u2019s argumentation. Figure 2 shows the entire argumentation\nstructure of the example essay.\nFigure 2Argumentation structure of the example essay. Arrows indicate argumentative relations.Arrowheads denote argumentative support relations and circleheads attack relations. Dashedlines indicate relations that are encoded in the stance attributes of claims. \u201cP\u201d denotes premises.629\nFigure 24.12 Argumentation structure of a persuasive essay. Arrows indicate argumentation relations, ei-\nther of SUPPORT (with arrowheads) or ATTACK (with circleheads); P denotes premises. Figure from Stab and\nGurevych (2017).\nAlgorithms for detecting argumentation structure often include classi\ufb01ers for\ndistinguishing claims, premises, or non-argumentation, together with relation clas-\nsi\ufb01ers for deciding if two spans have the SUPPORT ,ATTACK , or neither relation\n(Peldszus and Stede, 2013). While these are the main focus of much computational\nwork, there is also preliminary efforts on annotating and detecting richer semantic",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 844,
      "token_count": 785,
      "chapter_title": ""
    }
  },
  {
    "content": "humanity ]MajorClaim 2. It is likely that [ thistechnologybears some important cures which\nwill significantly improve lifeconditions] Claim 7.\nThe conclusion of the essay starts with an attacking claim followed by the restatement of\nthe major claim. The last sentence includes another claim that summarizes the most im-\nportant points of the author\u2019s argumentation. Figure 2 shows the entire argumentation\nstructure of the example essay.\nFigure 2Argumentation structure of the example essay. Arrows indicate argumentative relations.Arrowheads denote argumentative support relations and circleheads attack relations. Dashedlines indicate relations that are encoded in the stance attributes of claims. \u201cP\u201d denotes premises.629\nFigure 24.12 Argumentation structure of a persuasive essay. Arrows indicate argumentation relations, ei-\nther of SUPPORT (with arrowheads) or ATTACK (with circleheads); P denotes premises. Figure from Stab and\nGurevych (2017).\nAlgorithms for detecting argumentation structure often include classi\ufb01ers for\ndistinguishing claims, premises, or non-argumentation, together with relation clas-\nsi\ufb01ers for deciding if two spans have the SUPPORT ,ATTACK , or neither relation\n(Peldszus and Stede, 2013). While these are the main focus of much computational\nwork, there is also preliminary efforts on annotating and detecting richer semantic\nrelationships (Park and Cardie 2014, Hidey et al. 2017) such as detecting argumen-\ntation schemes , larger-scale structures for argument like argument from example ,argumentation\nschemes\norargument from cause to effect , orargument from consequences (Feng and\nHirst, 2011).\nAnother important line of research is studying how these argument structure (or\nother features) are associated with the success or persuasiveness of an argument\n(Habernal and Gurevych 2016, Tan et al. 2016, Hidey et al. 2017. Indeed, while it\nis Aristotle\u2019s logos that is most related to discourse structure, Aristotle\u2019s ethos and\npathos techniques are particularly relevant in the detection of mechanisms of this\nsort of persuasion . For example scholars have investigated the linguistic realization persuasion\nof features studied by social scientists like reciprocity (people return favors), social\nproof (people follow others\u2019 choices), authority (people are in\ufb02uenced by those\nwith power), and scarcity (people value things that are scarce), all of which can\nbe brought up in a persuasive argument (Cialdini, 1984). Rosenthal and McKeown\n(2017) showed that these features could be combined with argumentation structure\nto predict who in\ufb02uences whom on social media, Althoff et al. (2014) found that\nlinguistic models of reciprocity and authority predicted success in online requests,\nwhile the semisupervised model of Yang et al. (2019) detected mentions of scarcity,\ncommitment, and social identity to predict the success of peer-to-peer lending plat-\nforms.\nSee Stede and Schneider (2018) for a comprehensive survey of argument mining.\n24.5.2 The structure of scienti\ufb01c discourse\nScienti\ufb01c papers have a very speci\ufb01c global structure: somewhere in the course of\nthe paper the authors must indicate a scienti\ufb01c goal, develop a method for a solu-\ntion, provide evidence for the solution, and compare to prior work. One popular",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 845,
      "token_count": 740,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 557\n\n24.6 \u2022 S UMMARY 549\nannotation scheme for modeling these rhetorical goals is the argumentative zon-\ningmodel of Teufel et al. (1999) and Teufel et al. (2009), which is informed by theargumentative\nzoning\nidea that each scienti\ufb01c paper tries to make a knowledge claim about a new piece\nof knowledge being added to the repository of the \ufb01eld (Myers, 1992). Sentences\nin a scienti\ufb01c paper can be assigned one of 15 tags; Fig. 24.13 shows 7 (shortened)\nexamples of labeled sentences.\nCategory Description Example\nAIM Statement of speci\ufb01c research goal, or\nhypothesis of current paper\u201cThe aim of this process is to examine the role that\ntraining plays in the tagging process\u201d\nOWNMETHOD New Knowledge claim, own work:\nmethods\u201cIn order for it to be useful for our purposes, the\nfollowing extensions must be made:\u201d\nOWNRESULTS Measurable/objective outcome of own\nwork\u201cAll the curves have a generally upward trend but\nalways lie far below backoff (51% error rate)\u201d\nUSE Other work is used in own work \u201cWe use the framework for the allocation and\ntransfer of control of Whittaker....\u201d\nGAPWEAK Lack of solution in \ufb01eld, problem with\nother solutions\u201cHere, we will produce experimental evidence\nsuggesting that this simple model leads to serious\noverestimates\u201d\nSUPPORT Other work supports current work or is\nsupported by current work\u201cWork similar to that described here has been car-\nried out by Merialdo (1994), with broadly similar\nconclusions.\u201d\nANTISUPPORT Clash with other\u2019s results or theory; su-\nperiority of own work\u201cThis result challenges the claims of...\u201d\nFigure 24.13 Examples for 7 of the 15 labels from the Argumentative Zoning labelset (Teufel et al., 2009).\nTeufel et al. (1999) and Teufel et al. (2009) develop labeled corpora of scienti\ufb01c\narticles from computational linguistics and chemistry, which can be used as supervi-\nsion for training standard sentence-classi\ufb01cation architecture to assign the 15 labels.\n24.6 Summary\nIn this chapter we introduced local and global models for discourse coherence .\n\u2022 Discourses are not arbitrary collections of sentences; they must be coherent .\nAmong the factors that make a discourse coherent are coherence relations\nbetween the sentences, entity-based coherence, and topical coherence.\n\u2022 Various sets of coherence relations andrhetorical relations have been pro-\nposed. The relations in Rhetorical Structure Theory ( RST ) hold between\nspans of text and are structured into a tree. Because of this, shift-reduce\nand other parsing algorithms are generally used to assign these structures.\nThe Penn Discourse Treebank ( PDTB ) labels only relations between pairs of\nspans, and the labels are generally assigned by sequence models.\n\u2022Entity-based coherence captures the intuition that discourses are about an\nentity, and continue mentioning the entity from sentence to sentence. Cen-\ntering Theory is a family of models describing how salience is modeled for\ndiscourse entities, and hence how coherence is achieved by virtue of keeping\nthe same discourse entities salient over the discourse. The entity grid model\ngives a more bottom-up way to compute which entity realization transitions\nlead to coherence.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 846,
      "token_count": 727,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 558",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 847,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "550 CHAPTER 24 \u2022 D ISCOURSE COHERENCE\n\u2022 Many different genres have different types of global coherence . Persuasive\nessays have claims and premises that are extracted in the \ufb01eld of argument\nmining , scienti\ufb01c articles have structure related to aims, methods, results, and\ncomparisons.\nBibliographical and Historical Notes\nCoherence relations arose from the independent development of a number of schol-\nars, including Hobbs (1979) idea that coherence relations play an inferential role for\nthe hearer, and the investigations by Mann and Thompson (1987) of the discourse\nstructure of large texts. Other approaches to coherence relations and their extrac-\ntion include Segmented Discourse Representation Theory ( SDRT ) (Asher and Las- SDRT\ncarides 2003, Baldridge et al. 2007) and the Linguistic Discourse Model (Polanyi\n1988, Scha and Polanyi 1988, Polanyi et al. 2004). Wolf and Gibson (2005) argue\nthat coherence structure includes crossed bracketings, which make it impossible to\nrepresent as a tree, and propose a graph representation instead. A compendium of\nover 350 relations that have been proposed in the literature can be found in Hovy\n(1990).\nRST parsing was \ufb01rst proposed by Marcu (1997), and early work was rule-based,\nfocused on discourse markers (Marcu, 2000a). The creation of the RST Discourse\nTreeBank (Carlson et al. 2001, Carlson and Marcu 2001) enabled a wide variety\nof machine learning algorithms, beginning with the shift-reduce parser of Marcu\n(1999) that used decision trees to choose actions, and continuing with a wide variety\nof machine learned parsing methods (Soricut and Marcu 2003, Sagae 2009, Hernault\net al. 2010, Feng and Hirst 2014, Surdeanu et al. 2015, Joty et al. 2015) and chunkers\n(Sporleder and Lapata, 2005). Subba and Di Eugenio (2009) integrated sophisticated\nsemantic information into RST parsing. Ji and Eisenstein (2014) \ufb01rst applied neural\nmodels to RST parsing neural models, leading to the modern set of neural RST\nmodels (Li et al. 2014, Li et al. 2016b, Braud et al. 2017, Yu et al. 2018, inter alia)\nas well as neural segmenters (Wang et al. 2018b). and neural PDTB parsing models\n(Ji and Eisenstein 2015, Qin et al. 2016, Qin et al. 2017).\nBarzilay and Lapata (2005) pioneered the idea of self-supervision for coher-\nence: training a coherence model to distinguish true orderings of sentences from\nrandom permutations. Li et al. (2014) \ufb01rst applied this paradigm to neural sentence-\nrepresentation, and many neural self-supervised models followed (Li and Jurafsky\n2017, Logeswaran et al. 2018, Lai and Tetreault 2018, Xu et al. 2019, Iter et al.\n2020)\nAnother aspect of global coherence is the global topic structure of a text, the way\nthe topics shift over the course of the document. Barzilay and Lee (2004) introduced\nan HMM model for capturing topics for coherence, and later work expanded this",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 848,
      "token_count": 770,
      "chapter_title": ""
    }
  },
  {
    "content": "models to RST parsing neural models, leading to the modern set of neural RST\nmodels (Li et al. 2014, Li et al. 2016b, Braud et al. 2017, Yu et al. 2018, inter alia)\nas well as neural segmenters (Wang et al. 2018b). and neural PDTB parsing models\n(Ji and Eisenstein 2015, Qin et al. 2016, Qin et al. 2017).\nBarzilay and Lapata (2005) pioneered the idea of self-supervision for coher-\nence: training a coherence model to distinguish true orderings of sentences from\nrandom permutations. Li et al. (2014) \ufb01rst applied this paradigm to neural sentence-\nrepresentation, and many neural self-supervised models followed (Li and Jurafsky\n2017, Logeswaran et al. 2018, Lai and Tetreault 2018, Xu et al. 2019, Iter et al.\n2020)\nAnother aspect of global coherence is the global topic structure of a text, the way\nthe topics shift over the course of the document. Barzilay and Lee (2004) introduced\nan HMM model for capturing topics for coherence, and later work expanded this\nintuition (Soricut and Marcu 2006, Elsner et al. 2007, Louis and Nenkova 2012, Li\nand Jurafsky 2017).\nThe relationship between explicit and implicit discourse connectives has been\na fruitful one for research. Marcu and Echihabi (2002) \ufb01rst proposed to use sen-\ntences with explicit relations to help provide training data for implicit relations, by\nremoving the explicit relations and trying to re-predict them as a way of improv-\ning performance on implicit connectives; this idea was re\ufb01ned by Sporleder and\nLascarides (2005), (Pitler et al., 2009), and Rutherford and Xue (2015). This rela-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 849,
      "token_count": 443,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 559",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 850,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 551\ntionship can also be used as a way to create discourse-aware representations. The\nDisSent algorithm (Nie et al., 2019) creates the task of predicting explicit discourse\nmarkers between two sentences. They show that representations learned to be good\nat this task also function as powerful sentence representations for other discourse\ntasks.\nThe idea of entity-based coherence seems to have arisen in multiple \ufb01elds in the\nmid-1970s, in functional linguistics (Chafe, 1976), in the psychology of discourse\nprocessing (Kintsch and Van Dijk, 1978), and in the roughly contemporaneous work\nof Grosz, Sidner, Joshi, and their colleagues. Grosz (1977a) addressed the focus\nof attention that conversational participants maintain as the discourse unfolds. She\nde\ufb01ned two levels of focus; entities relevant to the entire discourse were said to\nbe in global focus, whereas entities that are locally in focus (i.e., most central to\na particular utterance) were said to be in immediate focus. Sidner 1979; 1983 de-\nscribed a method for tracking (immediate) discourse foci and their use in resolving\npronouns and demonstrative noun phrases. She made a distinction between the cur-\nrent discourse focus and potential foci, which are the predecessors to the backward-\nand forward-looking centers of Centering theory, respectively. The name and further\nroots of the centering approach lie in papers by Joshi and Kuhn (1979) and Joshi and\nWeinstein (1981), who addressed the relationship between immediate focus and the\ninferences required to integrate the current utterance into the discourse model. Grosz\net al. (1983) integrated this work with the prior work of Sidner and Grosz. This led\nto a manuscript on centering which, while widely circulated since 1986, remained\nunpublished until Grosz et al. (1995). A collection of centering papers appears in\nWalker et al. (1998b). See Karamanis et al. (2004) and Poesio et al. (2004) for a\ndeeper exploration of centering and its parameterizations, and the History section of\nChapter 23 for more on the use of centering on coreference.\nThe grid model of entity-based coherence was \ufb01rst proposed by Barzilay and\nLapata (2005) drawing on earlier work by Lapata (2003) and Barzilay, and then\nextended by them Barzilay and Lapata (2008) and others with additional features\n(Elsner and Charniak 2008, 2011, Feng et al. 2014, Lin et al. 2011) a model that\nprojects entities into a global graph for the discourse (Guinaudeau and Strube 2013,\nMesgar and Strube 2016), and a convolutional model to capture longer-range entity\ndependencies (Nguyen and Joty, 2017).\nTheories of discourse coherence have also been used in algorithms for interpret-\ning discourse-level linguistic phenomena, including verb phrase ellipsis and gap-\nping (Asher 1993, Kehler 1993), and tense interpretation (Lascarides and Asher\n1993, Kehler 1994, Kehler 2000). An extensive investigation into the relationship\nbetween coherence relations and discourse connectives can be found in Knott and\nDale (1994).\nUseful surveys of discourse processing and structure include Stede (2011) and\nWebber et al. (2012).",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 851,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "Chapter 23 for more on the use of centering on coreference.\nThe grid model of entity-based coherence was \ufb01rst proposed by Barzilay and\nLapata (2005) drawing on earlier work by Lapata (2003) and Barzilay, and then\nextended by them Barzilay and Lapata (2008) and others with additional features\n(Elsner and Charniak 2008, 2011, Feng et al. 2014, Lin et al. 2011) a model that\nprojects entities into a global graph for the discourse (Guinaudeau and Strube 2013,\nMesgar and Strube 2016), and a convolutional model to capture longer-range entity\ndependencies (Nguyen and Joty, 2017).\nTheories of discourse coherence have also been used in algorithms for interpret-\ning discourse-level linguistic phenomena, including verb phrase ellipsis and gap-\nping (Asher 1993, Kehler 1993), and tense interpretation (Lascarides and Asher\n1993, Kehler 1994, Kehler 2000). An extensive investigation into the relationship\nbetween coherence relations and discourse connectives can be found in Knott and\nDale (1994).\nUseful surveys of discourse processing and structure include Stede (2011) and\nWebber et al. (2012).\nAndy Kehler wrote the Discourse chapter for the 2000 \ufb01rst edition of this text-\nbook, which we used as the starting point for the second-edition chapter, and there\nare some remnants of Andy\u2019s lovely prose still in this third-edition coherence chap-\nter.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 852,
      "token_count": 351,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 560\n\n552 CHAPTER 24 \u2022 D ISCOURSE COHERENCE\nExercises\n24.1 Finish the Centering Theory processing of the last two utterances of (24.30),\nand show how (24.29) would be processed. Does the algorithm indeed mark\n(24.29) as less coherent?\n24.2 Select an editorial column from your favorite newspaper, and determine the\ndiscourse structure for a 10\u201320 sentence portion. What problems did you\nencounter? Were you helped by super\ufb01cial cues the speaker included (e.g.,\ndiscourse connectives) in any places?",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 853,
      "token_count": 129,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 561",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 854,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Bibliography\nAbadi, M., A. Agarwal, P. Barham,\nE. Brevdo, Z. Chen, C. Citro,\nG. S. Corrado, A. Davis, J. Dean,\nM. Devin, S. Ghemawat, I. Good-\nfellow, A. Harp, G. Irving, M. Is-\nard, Y . Jia, R. Jozefowicz, L. Kaiser,\nM. Kudlur, J. Levenberg, D. Man \u00b4e,\nR. Monga, S. Moore, D. Murray,\nC. Olah, M. Schuster, J. Shlens,\nB. Steiner, I. Sutskever, K. Talwar,\nP. Tucker, V . Vanhoucke, V . Vasude-\nvan, F. Vi \u00b4egas, O. Vinyals, P. War-\nden, M. Wattenberg, M. Wicke,\nY . Yu, and X. Zheng. 2015. Tensor-\nFlow: Large-scale machine learning\non heterogeneous systems. Software\navailable from tensor\ufb02ow.org.\nAbney, S. P., R. E. Schapire, and\nY . Singer. 1999. Boosting ap-\nplied to tagging and PP attachment.\nEMNLP/VLC .\nAgarwal, O., S. Subramanian,\nA. Nenkova, and D. Roth. 2019.\nEvaluation of named entity corefer-\nence. Workshop on Computational\nModels of Reference, Anaphora and\nCoreference .\nAggarwal, C. C. and C. Zhai. 2012.\nA survey of text classi\ufb01cation al-\ngorithms. In C. C. Aggarwal and\nC. Zhai, eds, Mining text data , 163\u2013\n222. Springer.\nAgichtein, E. and L. Gravano. 2000.\nSnowball: Extracting relations from\nlarge plain-text collections. Pro-\nceedings of the 5th ACM Interna-\ntional Conference on Digital Li-\nbraries .\nAgirre, E., C. Banea, C. Cardie, D. Cer,\nM. Diab, A. Gonzalez-Agirre,\nW. Guo, I. Lopez-Gazpio, M. Mar-\nitxalar, R. Mihalcea, G. Rigau,\nL. Uria, and J. Wiebe. 2015.\nSemEval-2015 task 2: Semantic\ntextual similarity, English, Span-\nish and pilot on interpretability.\nSemEval-15 .\nAgirre, E., M. Diab, D. Cer,\nand A. Gonzalez-Agirre. 2012.\nSemEval-2012 task 6: A pilot on se-\nmantic textual similarity. SemEval-\n12.\nAgirre, E. and D. Martinez. 2001.\nLearning class-to-class selectional\npreferences. CoNLL .\nAho, A. V . and J. D. Ullman. 1972. The\nTheory of Parsing, Translation, and\nCompiling , volume 1. Prentice Hall.\nAlgoet, P. H. and T. M. Cover. 1988.\nA sandwich proof of the Shannon-\nMcMillan-Breiman theorem. The\nAnnals of Probability , 16(2):899\u2013",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 855,
      "token_count": 740,
      "chapter_title": ""
    }
  },
  {
    "content": "M. Diab, A. Gonzalez-Agirre,\nW. Guo, I. Lopez-Gazpio, M. Mar-\nitxalar, R. Mihalcea, G. Rigau,\nL. Uria, and J. Wiebe. 2015.\nSemEval-2015 task 2: Semantic\ntextual similarity, English, Span-\nish and pilot on interpretability.\nSemEval-15 .\nAgirre, E., M. Diab, D. Cer,\nand A. Gonzalez-Agirre. 2012.\nSemEval-2012 task 6: A pilot on se-\nmantic textual similarity. SemEval-\n12.\nAgirre, E. and D. Martinez. 2001.\nLearning class-to-class selectional\npreferences. CoNLL .\nAho, A. V . and J. D. Ullman. 1972. The\nTheory of Parsing, Translation, and\nCompiling , volume 1. Prentice Hall.\nAlgoet, P. H. and T. M. Cover. 1988.\nA sandwich proof of the Shannon-\nMcMillan-Breiman theorem. The\nAnnals of Probability , 16(2):899\u2013\n909.Allen, J. 1984. Towards a general the-\nory of action and time. Arti\ufb01cial In-\ntelligence , 23(2):123\u2013154.\nAllen, J. and C. R. Perrault. 1980. An-\nalyzing intention in utterances. Arti-\n\ufb01cial Intelligence , 15:143\u2013178.\nAllen, J., M. S. Hunnicut, and D. H.\nKlatt. 1987. From Text to Speech:\nThe MITalk system . Cambridge Uni-\nversity Press.\nAlthoff, T., C. Danescu-Niculescu-\nMizil, and D. Jurafsky. 2014. How\nto ask for a favor: A case study\non the success of altruistic requests.\nICWSM 2014 .\nAn, J., H. Kwak, and Y .-Y . Ahn.\n2018. SemAxis: A lightweight\nframework to characterize domain-\nspeci\ufb01c word semantics beyond sen-\ntiment. ACL.\nAnastasopoulos, A. and G. Neubig.\n2020. Should all cross-lingual em-\nbeddings speak English? ACL.\nAntoniak, M. and D. Mimno.\n2018. Evaluating the stability of\nembedding-based word similarities.\nTACL , 6:107\u2013119.\nAone, C. and S. W. Bennett. 1995. Eval-\nuating automated and manual acqui-\nsition of anaphora resolution strate-\ngies. ACL.\nAriel, M. 2001. Accessibility the-\nory: An overview. In T. Sanders,\nJ. Schilperoord, and W. Spooren,\neds, Text Representation: Linguistic\nand Psycholinguistic Aspects , 29\u2013\n87. Benjamins.\nArora, S., P. Lewis, A. Fan, J. Kahn, and\nC. R \u00b4e. 2023. Reasoning over pub-\nlic and private data in retrieval-based\nsystems. TACL , 11:902\u2013921.\nArtetxe, M. and H. Schwenk. 2019.\nMassively multilingual sentence em-\nbeddings for zero-shot cross-lingual",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 856,
      "token_count": 745,
      "chapter_title": ""
    }
  },
  {
    "content": "framework to characterize domain-\nspeci\ufb01c word semantics beyond sen-\ntiment. ACL.\nAnastasopoulos, A. and G. Neubig.\n2020. Should all cross-lingual em-\nbeddings speak English? ACL.\nAntoniak, M. and D. Mimno.\n2018. Evaluating the stability of\nembedding-based word similarities.\nTACL , 6:107\u2013119.\nAone, C. and S. W. Bennett. 1995. Eval-\nuating automated and manual acqui-\nsition of anaphora resolution strate-\ngies. ACL.\nAriel, M. 2001. Accessibility the-\nory: An overview. In T. Sanders,\nJ. Schilperoord, and W. Spooren,\neds, Text Representation: Linguistic\nand Psycholinguistic Aspects , 29\u2013\n87. Benjamins.\nArora, S., P. Lewis, A. Fan, J. Kahn, and\nC. R \u00b4e. 2023. Reasoning over pub-\nlic and private data in retrieval-based\nsystems. TACL , 11:902\u2013921.\nArtetxe, M. and H. Schwenk. 2019.\nMassively multilingual sentence em-\nbeddings for zero-shot cross-lingual\ntransfer and beyond. TACL , 7:597\u2013\n610.\nArtstein, R., S. Gandhe, J. Gerten,\nA. Leuski, and D. Traum. 2009.\nSemi-formal evaluation of conver-\nsational characters. In Languages:\nFrom Formal to Natural , 22\u201335.\nSpringer.\nAsher, N. 1993. Reference to Abstract\nObjects in Discourse . Studies in Lin-\nguistics and Philosophy (SLAP) 50,\nKluwer.\nAsher, N. and A. Lascarides. 2003. Log-\nics of Conversation . Cambridge Uni-\nversity Press.\nAtal, B. S. and S. Hanauer. 1971.\nSpeech analysis and synthesis by\nprediction of the speech wave. JASA ,\n50:637\u2013655.Austin, J. L. 1962. How to Do Things\nwith Words . Harvard University\nPress.\nAwadallah, A. H., R. G. Kulkarni,\nU. Ozertem, and R. Jones. 2015.\nCharaterizing and predicting voice\nquery reformulation. CIKM-15 .\nBa, J. L., J. R. Kiros, and G. E. Hinton.\n2016. Layer normalization. NeurIPS\nworkshop .\nBaayen, R. H. 2001. Word frequency\ndistributions . Springer.\nBaccianella, S., A. Esuli, and F. Sebas-\ntiani. 2010. Sentiwordnet 3.0: An\nenhanced lexical resource for senti-\nment analysis and opinion mining.\nLREC .\nBach, K. and R. Harnish. 1979. Linguis-\ntic communication and speech acts .\nMIT Press.\nBackus, J. W. 1959. The syntax\nand semantics of the proposed in-\nternational algebraic language of the\nZurich ACM-GAMM Conference.\nInformation Processing: Proceed-\nings of the International Conference\non Information Processing, Paris .\nUNESCO.\nBackus, J. W. 1996. Transcript of ques-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 857,
      "token_count": 734,
      "chapter_title": ""
    }
  },
  {
    "content": "with Words . Harvard University\nPress.\nAwadallah, A. H., R. G. Kulkarni,\nU. Ozertem, and R. Jones. 2015.\nCharaterizing and predicting voice\nquery reformulation. CIKM-15 .\nBa, J. L., J. R. Kiros, and G. E. Hinton.\n2016. Layer normalization. NeurIPS\nworkshop .\nBaayen, R. H. 2001. Word frequency\ndistributions . Springer.\nBaccianella, S., A. Esuli, and F. Sebas-\ntiani. 2010. Sentiwordnet 3.0: An\nenhanced lexical resource for senti-\nment analysis and opinion mining.\nLREC .\nBach, K. and R. Harnish. 1979. Linguis-\ntic communication and speech acts .\nMIT Press.\nBackus, J. W. 1959. The syntax\nand semantics of the proposed in-\nternational algebraic language of the\nZurich ACM-GAMM Conference.\nInformation Processing: Proceed-\nings of the International Conference\non Information Processing, Paris .\nUNESCO.\nBackus, J. W. 1996. Transcript of ques-\ntion and answer session. In R. L.\nWexelblat, ed., History of Program-\nming Languages , page 162. Aca-\ndemic Press.\nBada, M., M. Eckert, D. Evans, K. Gar-\ncia, K. Shipley, D. Sitnikov, W. A.\nBaumgartner, K. B. Cohen, K. Ver-\nspoor, J. A. Blake, and L. E. Hunter.\n2012. Concept annotation in the\ncraft corpus. BMC bioinformatics ,\n13(1):161.\nBagga, A. and B. Baldwin. 1998.\nAlgorithms for scoring coreference\nchains. LREC Workshop on Linguis-\ntic Coreference .\nBahdanau, D., K. H. Cho, and Y . Ben-\ngio. 2015. Neural machine transla-\ntion by jointly learning to align and\ntranslate. ICLR 2015 .\nBahdanau, D., J. Chorowski,\nD. Serdyuk, P. Brakel, and Y . Ben-\ngio. 2016. End-to-end attention-\nbased large vocabulary speech\nrecognition. ICASSP .\nBahl, L. R. and R. L. Mercer. 1976.\nPart of speech assignment by a sta-\ntistical decision algorithm. Proceed-\nings IEEE International Symposium\non Information Theory .\nBahl, L. R., F. Jelinek, and R. L.\nMercer. 1983. A maximum likeli-\nhood approach to continuous speech\nrecognition. IEEE Transactions on\nPattern Analysis and Machine Intel-\nligence , 5(2):179\u2013190.\n553",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 858,
      "token_count": 621,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 562",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 859,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "554 Bibliography\nBajaj, P., D. Campos, N. Craswell,\nL. Deng, J. G. ando Xiaodong Liu,\nR. Majumder, A. McNamara, B. Mi-\ntra, T. Nguye, M. Rosenberg,\nX. Song, A. Stoica, S. Tiwary, and\nT. Wang. 2016. MS MARCO: A\nhuman generated MAchine Reading\nCOmprehension dataset. NeurIPS .\nBaker, C. F., C. J. Fillmore, and\nJ. B. Lowe. 1998. The Berkeley\nFrameNet project. COLING/ACL .\nBaker, J. K. 1975a. The DRAGON sys-\ntem \u2013 An overview. IEEE Transac-\ntions on ASSP , ASSP-23(1):24\u201329.\nBaker, J. K. 1975b. Stochastic\nmodeling for automatic speech un-\nderstanding. In D. R. Reddy,\ned.,Speech Recognition . Academic\nPress.\nBaldridge, J., N. Asher, and J. Hunter.\n2007. Annotation for and robust\nparsing of discourse structure on\nunrestricted texts. Zeitschrift f \u00a8ur\nSprachwissenschaft , 26:213\u2013239.\nBamman, D., O. Lewke, and A. Man-\nsoor. 2020. An annotated dataset\nof coreference in English literature.\nLREC .\nBamman, D., B. O\u2019Connor, and N. A.\nSmith. 2013. Learning latent per-\nsonas of \ufb01lm characters. ACL.\nBamman, D., S. Popat, and S. Shen.\n2019. An annotated dataset of liter-\nary entities. NAACL HLT .\nBanerjee, S. and A. Lavie. 2005. ME-\nTEOR: An automatic metric for MT\nevaluation with improved correla-\ntion with human judgments. Pro-\nceedings of ACL Workshop on In-\ntrinsic and Extrinsic Evaluation\nMeasures for MT and/or Summa-\nrization .\nBanko, M., M. Cafarella, S. Soderland,\nM. Broadhead, and O. Etzioni. 2007.\nOpen information extraction for the\nweb. IJCAI .\nBa\u02dcn\u00b4on, M., P. Chen, B. Haddow,\nK. Hea\ufb01eld, H. Hoang, M. Espl `a-\nGomis, M. L. Forcada, A. Kamran,\nF. Kirefu, P. Koehn, S. Ortiz Ro-\njas, L. Pla Sempere, G. Ram \u00b4\u0131rez-\nS\u00b4anchez, E. Sarr \u00b4\u0131as, M. Strelec,\nB. Thompson, W. Waites, D. Wig-\ngins, and J. Zaragoza. 2020.\nParaCrawl: Web-scale acquisition\nof parallel corpora. ACL.\nBar-Hillel, Y . 1960. The present sta-\ntus of automatic translation of lan-\nguages. In F. Alt, ed., Advances\nin Computers 1 , 91\u2013163. Academic\nPress.\nBarker, C. 2010. Nominals don\u2019t\nprovide criteria of identity. In\nM. Rathert and A. Alexiadou, eds,\nThe Semantics of Nominalizations",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 860,
      "token_count": 746,
      "chapter_title": ""
    }
  },
  {
    "content": "M. Broadhead, and O. Etzioni. 2007.\nOpen information extraction for the\nweb. IJCAI .\nBa\u02dcn\u00b4on, M., P. Chen, B. Haddow,\nK. Hea\ufb01eld, H. Hoang, M. Espl `a-\nGomis, M. L. Forcada, A. Kamran,\nF. Kirefu, P. Koehn, S. Ortiz Ro-\njas, L. Pla Sempere, G. Ram \u00b4\u0131rez-\nS\u00b4anchez, E. Sarr \u00b4\u0131as, M. Strelec,\nB. Thompson, W. Waites, D. Wig-\ngins, and J. Zaragoza. 2020.\nParaCrawl: Web-scale acquisition\nof parallel corpora. ACL.\nBar-Hillel, Y . 1960. The present sta-\ntus of automatic translation of lan-\nguages. In F. Alt, ed., Advances\nin Computers 1 , 91\u2013163. Academic\nPress.\nBarker, C. 2010. Nominals don\u2019t\nprovide criteria of identity. In\nM. Rathert and A. Alexiadou, eds,\nThe Semantics of Nominalizations\nacross Languages and Frameworks ,\n9\u201324. Mouton.Barrett, L. F., B. Mesquita, K. N.\nOchsner, and J. J. Gross. 2007. The\nexperience of emotion. Annual Re-\nview of Psychology , 58:373\u2013403.\nBarzilay, R. and M. Lapata. 2005. Mod-\neling local coherence: An entity-\nbased approach. ACL.\nBarzilay, R. and M. Lapata. 2008. Mod-\neling local coherence: An entity-\nbased approach. Computational Lin-\nguistics , 34(1):1\u201334.\nBarzilay, R. and L. Lee. 2004. Catching\nthe drift: Probabilistic content mod-\nels, with applications to generation\nand summarization. HLT-NAACL .\nBaum, L. E. and J. A. Eagon. 1967. An\ninequality with applications to sta-\ntistical estimation for probabilistic\nfunctions of Markov processes and\nto a model for ecology. Bulletin of\nthe American Mathematical Society ,\n73(3):360\u2013363.\nBaum, L. E. and T. Petrie. 1966. Statis-\ntical inference for probabilistic func-\ntions of \ufb01nite-state Markov chains.\nAnnals of Mathematical Statistics ,\n37(6):1554\u20131563.\nBaum, L. F. 1900. The Wizard of Oz .\nAvailable at Project Gutenberg.\nBayes, T. 1763. An Essay Toward Solv-\ning a Problem in the Doctrine of\nChances , volume 53. Reprinted in\nFacsimiles of Two Papers by Bayes ,\nHafner Publishing, 1963.\nBazell, C. E. 1952/1966. The corre-\nspondence fallacy in structural lin-\nguistics. In E. P. Hamp, F. W.\nHouseholder, and R. Austerlitz, eds,\nStudies by Members of the English\nDepartment, Istanbul University (3),\nreprinted in Readings in Linguistics\nII (1966) , 271\u2013298. University of\nChicago Press.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 861,
      "token_count": 744,
      "chapter_title": ""
    }
  },
  {
    "content": "inequality with applications to sta-\ntistical estimation for probabilistic\nfunctions of Markov processes and\nto a model for ecology. Bulletin of\nthe American Mathematical Society ,\n73(3):360\u2013363.\nBaum, L. E. and T. Petrie. 1966. Statis-\ntical inference for probabilistic func-\ntions of \ufb01nite-state Markov chains.\nAnnals of Mathematical Statistics ,\n37(6):1554\u20131563.\nBaum, L. F. 1900. The Wizard of Oz .\nAvailable at Project Gutenberg.\nBayes, T. 1763. An Essay Toward Solv-\ning a Problem in the Doctrine of\nChances , volume 53. Reprinted in\nFacsimiles of Two Papers by Bayes ,\nHafner Publishing, 1963.\nBazell, C. E. 1952/1966. The corre-\nspondence fallacy in structural lin-\nguistics. In E. P. Hamp, F. W.\nHouseholder, and R. Austerlitz, eds,\nStudies by Members of the English\nDepartment, Istanbul University (3),\nreprinted in Readings in Linguistics\nII (1966) , 271\u2013298. University of\nChicago Press.\nBean, D. and E. Riloff. 1999.\nCorpus-based identi\ufb01cation of non-\nanaphoric noun phrases. ACL.\nBean, D. and E. Riloff. 2004. Unsu-\npervised learning of contextual role\nknowledge for coreference resolu-\ntion. HLT-NAACL .\nBedi, G., F. Carrillo, G. A. Cecchi, D. F.\nSlezak, M. Sigman, N. B. Mota,\nS. Ribeiro, D. C. Javitt, M. Copelli,\nand C. M. Corcoran. 2015. Auto-\nmated analysis of free speech pre-\ndicts psychosis onset in high-risk\nyouths. npj Schizophrenia , 1.\nBej\u02c7cek, E., E. Haji \u02c7cov\u00b4a, J. Haji \u02c7c,\nP. J \u00b4\u0131nov\u00b4a, V . Kettnerov \u00b4a,\nV . Kol \u00b4a\u02c7rov\u00b4a, M. Mikulov \u00b4a,\nJ. M \u00b4\u0131rovsk \u00b4y, A. Nedoluzhko,\nJ. Panevov \u00b4a, L. Pol \u00b4akov \u00b4a,\nM. \u02c7Sev\u02c7c\u00b4\u0131kov\u00b4a, J. \u02c7St\u02c7ep\u00b4anek, and\n\u02c7S. Zik \u00b4anov \u00b4a. 2013. Prague de-\npendency treebank 3.0. Technical\nreport, Institute of Formal and Ap-\nplied Linguistics, Charles Universityin Prague. LINDAT/CLARIN dig-\nital library at Institute of Formal\nand Applied Linguistics, Charles\nUniversity in Prague.\nBellegarda, J. R. 1997. A latent se-\nmantic analysis framework for large-\nspan language modeling. EU-\nROSPEECH .\nBellegarda, J. R. 2000. Exploiting la-\ntent semantic information in statisti-\ncal language modeling. Proceedings\nof the IEEE , 89(8):1279\u20131296.\nBellegarda, J. R. 2013. Natural lan-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 862,
      "token_count": 748,
      "chapter_title": ""
    }
  },
  {
    "content": "P. J \u00b4\u0131nov\u00b4a, V . Kettnerov \u00b4a,\nV . Kol \u00b4a\u02c7rov\u00b4a, M. Mikulov \u00b4a,\nJ. M \u00b4\u0131rovsk \u00b4y, A. Nedoluzhko,\nJ. Panevov \u00b4a, L. Pol \u00b4akov \u00b4a,\nM. \u02c7Sev\u02c7c\u00b4\u0131kov\u00b4a, J. \u02c7St\u02c7ep\u00b4anek, and\n\u02c7S. Zik \u00b4anov \u00b4a. 2013. Prague de-\npendency treebank 3.0. Technical\nreport, Institute of Formal and Ap-\nplied Linguistics, Charles Universityin Prague. LINDAT/CLARIN dig-\nital library at Institute of Formal\nand Applied Linguistics, Charles\nUniversity in Prague.\nBellegarda, J. R. 1997. A latent se-\nmantic analysis framework for large-\nspan language modeling. EU-\nROSPEECH .\nBellegarda, J. R. 2000. Exploiting la-\ntent semantic information in statisti-\ncal language modeling. Proceedings\nof the IEEE , 89(8):1279\u20131296.\nBellegarda, J. R. 2013. Natural lan-\nguage technology in mobile devices:\nTwo grounding frameworks. In\nMobile Speech and Advanced Nat-\nural Language Solutions , 185\u2013196.\nSpringer.\nBellman, R. 1957. Dynamic Program-\nming . Princeton University Press.\nBellman, R. 1984. Eye of the Hurri-\ncane: an autobiography . World Sci-\nenti\ufb01c Singapore.\nBender, E. M. 2019. The #BenderRule:\nOn naming the languages we study\nand why it matters. Blog post.\nBender, E. M., B. Friedman, and\nA. McMillan-Major. 2021. A\nguide for writing data statements\nfor natural language processing.\nhttp://techpolicylab.uw.\nedu/data-statements/ .\nBender, E. M. and A. Koller. 2020.\nClimbing towards NLU: On mean-\ning, form, and understanding in the\nage of data. ACL.\nBengio, Y ., A. Courville, and P. Vin-\ncent. 2013. Representation learn-\ning: A review and new perspec-\ntives. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence ,\n35(8):1798\u20131828.\nBengio, Y ., R. Ducharme, and P. Vin-\ncent. 2000. A neural probabilistic\nlanguage model. NeurIPS .\nBengio, Y ., R. Ducharme, P. Vincent,\nand C. Jauvin. 2003. A neural prob-\nabilistic language model. JMLR ,\n3:1137\u20131155.\nBengio, Y ., P. Lamblin, D. Popovici,\nand H. Larochelle. 2007. Greedy\nlayer-wise training of deep net-\nworks. NeurIPS .\nBengio, Y ., H. Schwenk, J.-S. Sen \u00b4ecal,\nF. Morin, and J.-L. Gauvain. 2006.\nNeural probabilistic language mod-\nels. In Innovations in Machine\nLearning , 137\u2013186. Springer.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 863,
      "token_count": 737,
      "chapter_title": ""
    }
  },
  {
    "content": "Climbing towards NLU: On mean-\ning, form, and understanding in the\nage of data. ACL.\nBengio, Y ., A. Courville, and P. Vin-\ncent. 2013. Representation learn-\ning: A review and new perspec-\ntives. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence ,\n35(8):1798\u20131828.\nBengio, Y ., R. Ducharme, and P. Vin-\ncent. 2000. A neural probabilistic\nlanguage model. NeurIPS .\nBengio, Y ., R. Ducharme, P. Vincent,\nand C. Jauvin. 2003. A neural prob-\nabilistic language model. JMLR ,\n3:1137\u20131155.\nBengio, Y ., P. Lamblin, D. Popovici,\nand H. Larochelle. 2007. Greedy\nlayer-wise training of deep net-\nworks. NeurIPS .\nBengio, Y ., H. Schwenk, J.-S. Sen \u00b4ecal,\nF. Morin, and J.-L. Gauvain. 2006.\nNeural probabilistic language mod-\nels. In Innovations in Machine\nLearning , 137\u2013186. Springer.\nBengtson, E. and D. Roth. 2008. Un-\nderstanding the value of features for\ncoreference resolution. EMNLP .\nBentivogli, L., M. Cettolo, M. Federico,\nand C. Federmann. 2018. Machine\ntranslation human evaluation: an in-\nvestigation of evaluation based on\npost-editing and its relation with di-\nrect assessment. ICSLT .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 864,
      "token_count": 372,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 563",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 865,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Bibliography 555\nBerant, J., A. Chou, R. Frostig, and\nP. Liang. 2013. Semantic parsing\non freebase from question-answer\npairs. EMNLP .\nBerg-Kirkpatrick, T., D. Burkett, and\nD. Klein. 2012. An empirical inves-\ntigation of statistical signi\ufb01cance in\nNLP. EMNLP .\nBerger, A., S. A. Della Pietra, and V . J.\nDella Pietra. 1996. A maximum en-\ntropy approach to natural language\nprocessing. Computational Linguis-\ntics, 22(1):39\u201371.\nBergsma, S. and D. Lin. 2006. Boot-\nstrapping path-based pronoun reso-\nlution. COLING/ACL .\nBergsma, S., D. Lin, and R. Goebel.\n2008a. Discriminative learning of\nselectional preference from unla-\nbeled text. EMNLP .\nBergsma, S., D. Lin, and R. Goebel.\n2008b. Distributional identi\ufb01cation\nof non-referential pronouns. ACL.\nBethard, S. 2013. ClearTK-TimeML:\nA minimalist approach to TempEval\n2013. SemEval-13 .\nBhat, I., R. A. Bhat, M. Shrivastava,\nand D. Sharma. 2017. Joining\nhands: Exploiting monolingual tree-\nbanks for parsing of code-mixing\ndata. EACL .\nBianchi, F., M. Suzgun, G. At-\ntanasio, P. Rottger, D. Jurafsky,\nT. Hashimoto, and J. Zou. 2024.\nSafety-tuned LLaMAs: Lessons\nfrom improving the safety of large\nlanguage models that follow instruc-\ntions. ICLR .\nBickel, B. 2003. Referential density\nin discourse and syntactic typology.\nLanguage , 79(2):708\u2013736.\nBickmore, T. W., H. Trinh, S. Olafsson,\nT. K. O\u2019Leary, R. Asadi, N. M. Rick-\nles, and R. Cruz. 2018. Patient and\nconsumer safety risks when using\nconversational assistants for medical\ninformation: An observational study\nof Siri, Alexa, and Google Assis-\ntant. Journal of Medical Internet Re-\nsearch , 20(9):e11510.\nBikel, D. M., S. Miller, R. Schwartz,\nand R. Weischedel. 1997. Nymble:\nA high-performance learning name-\n\ufb01nder. ANLP .\nBiran, O. and K. McKeown. 2015.\nPDTB discourse parsing as a tagging\ntask: The two taggers approach.\nSIGDIAL .\nBird, S., E. Klein, and E. Loper. 2009.\nNatural Language Processing with\nPython . O\u2019Reilly.\nBisani, M. and H. Ney. 2004. Boot-\nstrap estimates for con\ufb01dence inter-\nvals in ASR performance evaluation.\nICASSP .\nBishop, C. M. 2006. Pattern recogni-\ntion and machine learning . Springer.Bisk, Y ., A. Holtzman, J. Thomason,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 866,
      "token_count": 749,
      "chapter_title": ""
    }
  },
  {
    "content": "T. K. O\u2019Leary, R. Asadi, N. M. Rick-\nles, and R. Cruz. 2018. Patient and\nconsumer safety risks when using\nconversational assistants for medical\ninformation: An observational study\nof Siri, Alexa, and Google Assis-\ntant. Journal of Medical Internet Re-\nsearch , 20(9):e11510.\nBikel, D. M., S. Miller, R. Schwartz,\nand R. Weischedel. 1997. Nymble:\nA high-performance learning name-\n\ufb01nder. ANLP .\nBiran, O. and K. McKeown. 2015.\nPDTB discourse parsing as a tagging\ntask: The two taggers approach.\nSIGDIAL .\nBird, S., E. Klein, and E. Loper. 2009.\nNatural Language Processing with\nPython . O\u2019Reilly.\nBisani, M. and H. Ney. 2004. Boot-\nstrap estimates for con\ufb01dence inter-\nvals in ASR performance evaluation.\nICASSP .\nBishop, C. M. 2006. Pattern recogni-\ntion and machine learning . Springer.Bisk, Y ., A. Holtzman, J. Thomason,\nJ. Andreas, Y . Bengio, J. Chai,\nM. Lapata, A. Lazaridou, J. May,\nA. Nisnevich, N. Pinto, and\nJ. Turian. 2020. Experience grounds\nlanguage. EMNLP .\nBizer, C., J. Lehmann, G. Kobilarov,\nS. Auer, C. Becker, R. Cyganiak,\nand S. Hellmann. 2009. DBpedia\u2014\nA crystallization point for the Web\nof Data. Web Semantics: science,\nservices and agents on the world\nwide web , 7(3):154\u2013165.\nBj\u00a8orkelund, A. and J. Kuhn. 2014.\nLearning structured perceptrons for\ncoreference resolution with latent\nantecedents and non-local features.\nACL.\nBlack, A. W. and P. Taylor. 1994.\nCHATR: A generic speech synthesis\nsystem. COLING .\nBlack, E., S. P. Abney, D. Flickinger,\nC. Gdaniec, R. Grishman, P. Har-\nrison, D. Hindle, R. Ingria, F. Je-\nlinek, J. L. Klavans, M. Y . Liberman,\nM. P. Marcus, S. Roukos, B. San-\ntorini, and T. Strzalkowski. 1991. A\nprocedure for quantitatively compar-\ning the syntactic coverage of English\ngrammars. Speech and Natural Lan-\nguage Workshop .\nBlei, D. M., A. Y . Ng, and M. I. Jor-\ndan. 2003. Latent Dirichlet alloca-\ntion. JMLR , 3(5):993\u20131022.\nBlodgett, S. L., S. Barocas,\nH. Daum \u00b4e III, and H. Wallach. 2020.\nLanguage (technology) is power: A\ncritical survey of \u201cbias\u201d in NLP.\nACL.\nBlodgett, S. L., L. Green, and\nB. O\u2019Connor. 2016. Demographic\ndialectal variation in social media:",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 867,
      "token_count": 749,
      "chapter_title": ""
    }
  },
  {
    "content": "CHATR: A generic speech synthesis\nsystem. COLING .\nBlack, E., S. P. Abney, D. Flickinger,\nC. Gdaniec, R. Grishman, P. Har-\nrison, D. Hindle, R. Ingria, F. Je-\nlinek, J. L. Klavans, M. Y . Liberman,\nM. P. Marcus, S. Roukos, B. San-\ntorini, and T. Strzalkowski. 1991. A\nprocedure for quantitatively compar-\ning the syntactic coverage of English\ngrammars. Speech and Natural Lan-\nguage Workshop .\nBlei, D. M., A. Y . Ng, and M. I. Jor-\ndan. 2003. Latent Dirichlet alloca-\ntion. JMLR , 3(5):993\u20131022.\nBlodgett, S. L., S. Barocas,\nH. Daum \u00b4e III, and H. Wallach. 2020.\nLanguage (technology) is power: A\ncritical survey of \u201cbias\u201d in NLP.\nACL.\nBlodgett, S. L., L. Green, and\nB. O\u2019Connor. 2016. Demographic\ndialectal variation in social media:\nA case study of African-American\nEnglish. EMNLP .\nBlodgett, S. L. and B. O\u2019Connor. 2017.\nRacial disparity in natural language\nprocessing: A case study of so-\ncial media African-American En-\nglish. FAT/ML Workshop, KDD .\nBloom\ufb01eld, L. 1914. An Introduction to\nthe Study of Language . Henry Holt\nand Company.\nBloom\ufb01eld, L. 1933. Language . Uni-\nversity of Chicago Press.\nBobrow, D. G., R. M. Kaplan, M. Kay,\nD. A. Norman, H. Thompson, and\nT. Winograd. 1977. GUS, A frame\ndriven dialog system. Arti\ufb01cial In-\ntelligence , 8:155\u2013173.\nBobrow, D. G. and D. A. Norman.\n1975. Some principles of memory\nschemata. In D. G. Bobrow and\nA. Collins, eds, Representation and\nUnderstanding . Academic Press.\nBojanowski, P., E. Grave, A. Joulin, and\nT. Mikolov. 2017. Enriching word\nvectors with subword information.\nTACL , 5:135\u2013146.Bollacker, K., C. Evans, P. Paritosh,\nT. Sturge, and J. Taylor. 2008.\nFreebase: a collaboratively created\ngraph database for structuring hu-\nman knowledge. SIGMOD 2008 .\nBolukbasi, T., K.-W. Chang, J. Zou,\nV . Saligrama, and A. T. Kalai. 2016.\nMan is to computer programmer as\nwoman is to homemaker? Debiasing\nword embeddings. NeurIPS .\nBommasani, R., D. A. Hudson,\nE. Adeli, R. Altman, S. Arora,\nS. von Arx, M. S. Bernstein,\nJ. Bohg, A. Bosselut, E. Brun-\nskill, E. Brynjolfsson, S. Buch,\nD. Card, R. Castellon, N. S. Chat-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 868,
      "token_count": 756,
      "chapter_title": ""
    }
  },
  {
    "content": "1975. Some principles of memory\nschemata. In D. G. Bobrow and\nA. Collins, eds, Representation and\nUnderstanding . Academic Press.\nBojanowski, P., E. Grave, A. Joulin, and\nT. Mikolov. 2017. Enriching word\nvectors with subword information.\nTACL , 5:135\u2013146.Bollacker, K., C. Evans, P. Paritosh,\nT. Sturge, and J. Taylor. 2008.\nFreebase: a collaboratively created\ngraph database for structuring hu-\nman knowledge. SIGMOD 2008 .\nBolukbasi, T., K.-W. Chang, J. Zou,\nV . Saligrama, and A. T. Kalai. 2016.\nMan is to computer programmer as\nwoman is to homemaker? Debiasing\nword embeddings. NeurIPS .\nBommasani, R., D. A. Hudson,\nE. Adeli, R. Altman, S. Arora,\nS. von Arx, M. S. Bernstein,\nJ. Bohg, A. Bosselut, E. Brun-\nskill, E. Brynjolfsson, S. Buch,\nD. Card, R. Castellon, N. S. Chat-\nterji, A. S. Chen, K. A. Creel,\nJ. Davis, D. Demszky, C. Don-\nahue, M. Doumbouya, E. Durmus,\nS. Ermon, J. Etchemendy, K. Etha-\nyarajh, L. Fei-Fei, C. Finn, T. Gale,\nL. E. Gillespie, K. Goel, N. D.\nGoodman, S. Grossman, N. Guha,\nT. Hashimoto, P. Henderson, J. He-\nwitt, D. E. Ho, J. Hong, K. Hsu,\nJ. Huang, T. F. Icard, S. Jain, D. Ju-\nrafsky, P. Kalluri, S. Karamcheti,\nG. Keeling, F. Khani, O. Khat-\ntab, P. W. Koh, M. S. Krass,\nR. Krishna, R. Kuditipudi, A. Ku-\nmar, F. Ladhak, M. Lee, T. Lee,\nJ. Leskovec, I. Levent, X. L. Li,\nX. Li, T. Ma, A. Malik, C. D. Man-\nning, S. P. Mirchandani, E. Mitchell,\nZ. Munyikwa, S. Nair, A. Narayan,\nD. Narayanan, B. Newman, A. Nie,\nJ. C. Niebles, H. Nilforoshan, J. F.\nNyarko, G. Ogut, L. Orr, I. Papadim-\nitriou, J. S. Park, C. Piech, E. Porte-\nlance, C. Potts, A. Raghunathan,\nR. Reich, H. Ren, F. Rong, Y . H.\nRoohani, C. Ruiz, J. Ryan, C. R\u2019e,\nD. Sadigh, S. Sagawa, K. San-\nthanam, A. Shih, K. P. Srinivasan,\nA. Tamkin, R. Taori, A. W. Thomas,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 869,
      "token_count": 756,
      "chapter_title": ""
    }
  },
  {
    "content": "tab, P. W. Koh, M. S. Krass,\nR. Krishna, R. Kuditipudi, A. Ku-\nmar, F. Ladhak, M. Lee, T. Lee,\nJ. Leskovec, I. Levent, X. L. Li,\nX. Li, T. Ma, A. Malik, C. D. Man-\nning, S. P. Mirchandani, E. Mitchell,\nZ. Munyikwa, S. Nair, A. Narayan,\nD. Narayanan, B. Newman, A. Nie,\nJ. C. Niebles, H. Nilforoshan, J. F.\nNyarko, G. Ogut, L. Orr, I. Papadim-\nitriou, J. S. Park, C. Piech, E. Porte-\nlance, C. Potts, A. Raghunathan,\nR. Reich, H. Ren, F. Rong, Y . H.\nRoohani, C. Ruiz, J. Ryan, C. R\u2019e,\nD. Sadigh, S. Sagawa, K. San-\nthanam, A. Shih, K. P. Srinivasan,\nA. Tamkin, R. Taori, A. W. Thomas,\nF. Tram `er, R. E. Wang, W. Wang,\nB. Wu, J. Wu, Y . Wu, S. M. Xie,\nM. Yasunaga, J. You, M. A. Zaharia,\nM. Zhang, T. Zhang, X. Zhang,\nY . Zhang, L. Zheng, K. Zhou, and\nP. Liang. 2021. On the opportuni-\nties and risks of foundation models.\nArXiv .\nBooth, T. L. 1969. Probabilistic\nrepresentation of formal languages.\nIEEE Conference Record of the 1969\nTenth Annual Symposium on Switch-\ning and Automata Theory .\nBorges, J. L. 1964. The analytical lan-\nguage of john wilkins. In Other\ninquisitions 1937\u20131952 . University\nof Texas Press. Trans. Ruth L. C.\nSimms.\nBostrom, K. and G. Durrett. 2020. Byte\npair encoding is suboptimal for lan-\nguage model pretraining. EMNLP .\nBourlard, H. and N. Morgan. 1994.\nConnectionist Speech Recognition:\nA Hybrid Approach . Kluwer.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 870,
      "token_count": 544,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 564",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 871,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "556 Bibliography\nBrants, T. 2000. TnT: A statistical part-\nof-speech tagger. ANLP .\nBrants, T., A. C. Popat, P. Xu, F. J.\nOch, and J. Dean. 2007. Large lan-\nguage models in machine transla-\ntion. EMNLP/CoNLL .\nBraud, C., M. Coavoux, and\nA. S\u00f8gaard. 2017. Cross-lingual\nRST discourse parsing. EACL .\nBr\u00b4eal, M. 1897. Essai de S \u00b4emantique:\nScience des signi\ufb01cations . Hachette.\nBrennan, S. E., M. W. Friedman, and\nC. Pollard. 1987. A centering ap-\nproach to pronouns. ACL.\nBrin, S. 1998. Extracting patterns and\nrelations from the World Wide Web.\nProceedings World Wide Web and\nDatabases International Workshop,\nNumber 1590 in LNCS . Springer.\nBrockmann, C. and M. Lapata. 2003.\nEvaluating and combining ap-\nproaches to selectional preference\nacquisition. EACL .\nBroschart, J. 1997. Why Tongan does\nit differently. Linguistic Typology ,\n1:123\u2013165.\nBrown, P. F., J. Cocke, S. A.\nDella Pietra, V . J. Della Pietra, F. Je-\nlinek, J. D. Lafferty, R. L. Mercer,\nand P. S. Roossin. 1990. A statis-\ntical approach to machine transla-\ntion. Computational Linguistics ,\n16(2):79\u201385.\nBrown, P. F., S. A. Della Pietra, V . J.\nDella Pietra, and R. L. Mercer. 1993.\nThe mathematics of statistical ma-\nchine translation: Parameter esti-\nmation. Computational Linguistics ,\n19(2):263\u2013311.\nBrown, T., B. Mann, N. Ryder,\nM. Subbiah, J. Kaplan, P. Dhari-\nwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, S. Agar-\nwal, A. Herbert-V oss, G. Krueger,\nT. Henighan, R. Child, A. Ramesh,\nD. M. Ziegler, J. Wu, C. Win-\nter, C. Hesse, M. Chen, E. Sigler,\nM. Litwin, S. Gray, B. Chess,\nJ. Clark, C. Berner, S. McCan-\ndlish, A. Radford, I. Sutskever, and\nD. Amodei. 2020. Language mod-\nels are few-shot learners. NeurIPS ,\nvolume 33.\nBruce, B. C. 1975. Generation as a so-\ncial action. Proceedings of TINLAP-\n1 (Theoretical Issues in Natural\nLanguage Processing) .\nBrysbaert, M., A. B. Warriner, and\nV . Kuperman. 2014. Concrete-\nness ratings for 40 thousand gen-\nerally known English word lem-\nmas. Behavior Research Methods ,\n46(3):904\u2013911.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 872,
      "token_count": 735,
      "chapter_title": ""
    }
  },
  {
    "content": "M. Subbiah, J. Kaplan, P. Dhari-\nwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, S. Agar-\nwal, A. Herbert-V oss, G. Krueger,\nT. Henighan, R. Child, A. Ramesh,\nD. M. Ziegler, J. Wu, C. Win-\nter, C. Hesse, M. Chen, E. Sigler,\nM. Litwin, S. Gray, B. Chess,\nJ. Clark, C. Berner, S. McCan-\ndlish, A. Radford, I. Sutskever, and\nD. Amodei. 2020. Language mod-\nels are few-shot learners. NeurIPS ,\nvolume 33.\nBruce, B. C. 1975. Generation as a so-\ncial action. Proceedings of TINLAP-\n1 (Theoretical Issues in Natural\nLanguage Processing) .\nBrysbaert, M., A. B. Warriner, and\nV . Kuperman. 2014. Concrete-\nness ratings for 40 thousand gen-\nerally known English word lem-\nmas. Behavior Research Methods ,\n46(3):904\u2013911.\nBu, H., J. Du, X. Na, B. Wu, and\nH. Zheng. 2017. AISHELL-1: An\nopen-source Mandarin speech cor-\npus and a speech recognition base-\nline. O-COCOSDA Proceedings .Buchholz, S. and E. Marsi. 2006. Conll-\nx shared task on multilingual depen-\ndency parsing. CoNLL .\nBudanitsky, A. and G. Hirst. 2006.\nEvaluating WordNet-based mea-\nsures of lexical semantic related-\nness. Computational Linguistics ,\n32(1):13\u201347.\nBudzianowski, P., T.-H. Wen, B.-\nH. Tseng, I. Casanueva, S. Ultes,\nO. Ramadan, and M. Ga \u02c7si\u00b4c. 2018.\nMultiWOZ - a large-scale multi-\ndomain wizard-of-Oz dataset for\ntask-oriented dialogue modelling.\nEMNLP .\nBullinaria, J. A. and J. P. Levy. 2007.\nExtracting semantic representations\nfrom word co-occurrence statistics:\nA computational study. Behavior re-\nsearch methods , 39(3):510\u2013526.\nBullinaria, J. A. and J. P. Levy.\n2012. Extracting semantic repre-\nsentations from word co-occurrence\nstatistics: stop-lists, stemming, and\nSVD. Behavior research methods ,\n44(3):890\u2013907.\nBulyko, I., K. Kirchhoff, M. Osten-\ndorf, and J. Goldberg. 2005. Error-\nsensitive response generation in a\nspoken language dialogue system.\nSpeech Communication , 45(3):271\u2013\n288.\nCaliskan, A., J. J. Bryson, and\nA. Narayanan. 2017. Semantics de-\nrived automatically from language\ncorpora contain human-like biases.\nScience , 356(6334):183\u2013186.\nCallison-Burch, C., M. Osborne, and\nP. Koehn. 2006. Re-evaluating the\nrole of BLEU in machine translation",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 873,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "domain wizard-of-Oz dataset for\ntask-oriented dialogue modelling.\nEMNLP .\nBullinaria, J. A. and J. P. Levy. 2007.\nExtracting semantic representations\nfrom word co-occurrence statistics:\nA computational study. Behavior re-\nsearch methods , 39(3):510\u2013526.\nBullinaria, J. A. and J. P. Levy.\n2012. Extracting semantic repre-\nsentations from word co-occurrence\nstatistics: stop-lists, stemming, and\nSVD. Behavior research methods ,\n44(3):890\u2013907.\nBulyko, I., K. Kirchhoff, M. Osten-\ndorf, and J. Goldberg. 2005. Error-\nsensitive response generation in a\nspoken language dialogue system.\nSpeech Communication , 45(3):271\u2013\n288.\nCaliskan, A., J. J. Bryson, and\nA. Narayanan. 2017. Semantics de-\nrived automatically from language\ncorpora contain human-like biases.\nScience , 356(6334):183\u2013186.\nCallison-Burch, C., M. Osborne, and\nP. Koehn. 2006. Re-evaluating the\nrole of BLEU in machine translation\nresearch. EACL .\nCanavan, A., D. Graff, and G. Zip-\nperlen. 1997. CALLHOME Ameri-\ncan English speech LDC97S42. Lin-\nguistic Data Consortium.\nCarbonell, J. R. 1970. AI in\nCAI: An arti\ufb01cial-intelligence ap-\nproach to computer-assisted instruc-\ntion. IEEE transactions on man-\nmachine systems , 11(4):190\u2013202.\nCardie, C. 1993. A case-based approach\nto knowledge acquisition for domain\nspeci\ufb01c sentence analysis. AAAI .\nCardie, C. 1994. Domain-Speci\ufb01c\nKnowledge Acquisition for Concep-\ntual Sentence Analysis . Ph.D. the-\nsis, University of Massachusetts,\nAmherst, MA. Available as CMP-\nSCI Technical Report 94-74.\nCardie, C. and K. Wagstaff. 1999.\nNoun phrase coreference as cluster-\ning. EMNLP/VLC .\nCarlini, N., F. Tramer, E. Wal-\nlace, M. Jagielski, A. Herbert-V oss,\nK. Lee, A. Roberts, T. Brown,\nD. Song, U. Erlingsson, et al. 2021.\nExtracting training data from largelanguage models. 30th USENIX Se-\ncurity Symposium (USENIX Security\n21).\nCarlson, G. N. 1977. Reference to kinds\nin English . Ph.D. thesis, Univer-\nsity of Massachusetts, Amherst. For-\nward.\nCarlson, L. and D. Marcu. 2001. Dis-\ncourse tagging manual. Technical\nReport ISI-TR-545, ISI.\nCarlson, L., D. Marcu, and M. E.\nOkurowski. 2001. Building a\ndiscourse-tagged corpus in the\nframework of rhetorical structure\ntheory. SIGDIAL .\nCarreras, X. and L. M `arquez. 2005.\nIntroduction to the CoNLL-2005\nshared task: Semantic role labeling.\nCoNLL .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 874,
      "token_count": 739,
      "chapter_title": ""
    }
  },
  {
    "content": "Cardie, C. and K. Wagstaff. 1999.\nNoun phrase coreference as cluster-\ning. EMNLP/VLC .\nCarlini, N., F. Tramer, E. Wal-\nlace, M. Jagielski, A. Herbert-V oss,\nK. Lee, A. Roberts, T. Brown,\nD. Song, U. Erlingsson, et al. 2021.\nExtracting training data from largelanguage models. 30th USENIX Se-\ncurity Symposium (USENIX Security\n21).\nCarlson, G. N. 1977. Reference to kinds\nin English . Ph.D. thesis, Univer-\nsity of Massachusetts, Amherst. For-\nward.\nCarlson, L. and D. Marcu. 2001. Dis-\ncourse tagging manual. Technical\nReport ISI-TR-545, ISI.\nCarlson, L., D. Marcu, and M. E.\nOkurowski. 2001. Building a\ndiscourse-tagged corpus in the\nframework of rhetorical structure\ntheory. SIGDIAL .\nCarreras, X. and L. M `arquez. 2005.\nIntroduction to the CoNLL-2005\nshared task: Semantic role labeling.\nCoNLL .\nChafe, W. L. 1976. Givenness, con-\ntrastiveness, de\ufb01niteness, subjects,\ntopics, and point of view. In C. N. Li,\ned.,Subject and Topic , 25\u201355. Aca-\ndemic Press.\nChambers, N. 2013. NavyTime: Event\nand time ordering from raw text.\nSemEval-13 .\nChambers, N., T. Cassidy, B. McDow-\nell, and S. Bethard. 2014. Dense\nevent ordering with a multi-pass ar-\nchitecture. TACL , 2:273\u2013284.\nChambers, N. and D. Jurafsky. 2010.\nImproving the use of pseudo-words\nfor evaluating selectional prefer-\nences. ACL.\nChambers, N. and D. Jurafsky. 2011.\nTemplate-based information extrac-\ntion without the templates. ACL.\nChan, W., N. Jaitly, Q. Le, and\nO. Vinyals. 2016. Listen, at-\ntend and spell: A neural network\nfor large vocabulary conversational\nspeech recognition. ICASSP .\nChandioux, J. 1976. M \u00b4ET\u00b4EO: un\nsyst`eme op \u00b4erationnel pour la tra-\nduction automatique des bulletins\nm\u00b4et\u00b4eorologiques destin \u00b4es au grand\npublic. Meta , 21:127\u2013133.\nChang, A. X. and C. D. Manning. 2012.\nSUTime: A library for recognizing\nand normalizing time expressions.\nLREC .\nChang, K.-W., R. Samdani, and\nD. Roth. 2013. A constrained la-\ntent variable model for coreference\nresolution. EMNLP .\nChang, K.-W., R. Samdani, A. Ro-\nzovskaya, M. Sammons, and\nD. Roth. 2012. Illinois-Coref:\nThe UI system in the CoNLL-2012\nshared task. CoNLL .\nChaplot, D. S. and R. Salakhutdinov.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 875,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "Template-based information extrac-\ntion without the templates. ACL.\nChan, W., N. Jaitly, Q. Le, and\nO. Vinyals. 2016. Listen, at-\ntend and spell: A neural network\nfor large vocabulary conversational\nspeech recognition. ICASSP .\nChandioux, J. 1976. M \u00b4ET\u00b4EO: un\nsyst`eme op \u00b4erationnel pour la tra-\nduction automatique des bulletins\nm\u00b4et\u00b4eorologiques destin \u00b4es au grand\npublic. Meta , 21:127\u2013133.\nChang, A. X. and C. D. Manning. 2012.\nSUTime: A library for recognizing\nand normalizing time expressions.\nLREC .\nChang, K.-W., R. Samdani, and\nD. Roth. 2013. A constrained la-\ntent variable model for coreference\nresolution. EMNLP .\nChang, K.-W., R. Samdani, A. Ro-\nzovskaya, M. Sammons, and\nD. Roth. 2012. Illinois-Coref:\nThe UI system in the CoNLL-2012\nshared task. CoNLL .\nChaplot, D. S. and R. Salakhutdinov.\n2018. Knowledge-based word sense\ndisambiguation using topic models.\nAAAI .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 876,
      "token_count": 301,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 565",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 877,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Bibliography 557\nCharniak, E. 1997. Statistical pars-\ning with a context-free grammar and\nword statistics. AAAI .\nCharniak, E., C. Hendrickson, N. Ja-\ncobson, and M. Perkowitz. 1993.\nEquations for part-of-speech tag-\nging. AAAI .\nChe, W., Z. Li, Y . Li, Y . Guo, B. Qin,\nand T. Liu. 2009. Multilingual\ndependency-based syntactic and se-\nmantic parsing. CoNLL .\nChen, C. and V . Ng. 2013. Linguis-\ntically aware coreference evaluation\nmetrics. IJCNLP .\nChen, D., A. Fisch, J. Weston, and\nA. Bordes. 2017a. Reading Wiki-\npedia to answer open-domain ques-\ntions. ACL.\nChen, D. and C. Manning. 2014. A fast\nand accurate dependency parser us-\ning neural networks. EMNLP .\nChen, E., B. Snyder, and R. Barzi-\nlay. 2007. Incremental text structur-\ning with online hierarchical ranking.\nEMNLP/CoNLL .\nChen, S. F. and J. Goodman. 1999.\nAn empirical study of smoothing\ntechniques for language modeling.\nComputer Speech and Language ,\n13:359\u2013394.\nChen, X., Z. Shi, X. Qiu, and X. Huang.\n2017b. Adversarial multi-criteria\nlearning for Chinese word segmen-\ntation. ACL.\nCheng, J., L. Dong, and M. La-\npata. 2016. Long short-term\nmemory-networks for machine read-\ning. EMNLP .\nCheng, M., E. Durmus, and D. Juraf-\nsky. 2023. Marked personas: Using\nnatural language prompts to mea-\nsure stereotypes in language models.\nACL.\nChiang, D. 2005. A hierarchical phrase-\nbased model for statistical machine\ntranslation. ACL.\nChinchor, N., L. Hirschman, and D. L.\nLewis. 1993. Evaluating Message\nUnderstanding systems: An analy-\nsis of the third Message Understand-\ning Conference. Computational Lin-\nguistics , 19(3):409\u2013449.\nChiticariu, L., M. Danilevsky, Y . Li,\nF. Reiss, and H. Zhu. 2018. Sys-\ntemT: Declarative text understand-\ning for enterprise. NAACL HLT , vol-\nume 3.\nChiticariu, L., Y . Li, and F. R. Reiss.\n2013. Rule-Based Information Ex-\ntraction is Dead! Long Live Rule-\nBased Information Extraction Sys-\ntems! EMNLP .\nChiu, J. P. C. and E. Nichols. 2016.\nNamed entity recognition with bidi-\nrectional LSTM-CNNs. TACL ,\n4:357\u2013370.Cho, K., B. van Merri \u00a8enboer, C. Gul-\ncehre, D. Bahdanau, F. Bougares,\nH. Schwenk, and Y . Bengio. 2014.\nLearning phrase representations us-\ning RNN encoder\u2013decoder for statis-\ntical machine translation. EMNLP .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 878,
      "token_count": 731,
      "chapter_title": ""
    }
  },
  {
    "content": "Chinchor, N., L. Hirschman, and D. L.\nLewis. 1993. Evaluating Message\nUnderstanding systems: An analy-\nsis of the third Message Understand-\ning Conference. Computational Lin-\nguistics , 19(3):409\u2013449.\nChiticariu, L., M. Danilevsky, Y . Li,\nF. Reiss, and H. Zhu. 2018. Sys-\ntemT: Declarative text understand-\ning for enterprise. NAACL HLT , vol-\nume 3.\nChiticariu, L., Y . Li, and F. R. Reiss.\n2013. Rule-Based Information Ex-\ntraction is Dead! Long Live Rule-\nBased Information Extraction Sys-\ntems! EMNLP .\nChiu, J. P. C. and E. Nichols. 2016.\nNamed entity recognition with bidi-\nrectional LSTM-CNNs. TACL ,\n4:357\u2013370.Cho, K., B. van Merri \u00a8enboer, C. Gul-\ncehre, D. Bahdanau, F. Bougares,\nH. Schwenk, and Y . Bengio. 2014.\nLearning phrase representations us-\ning RNN encoder\u2013decoder for statis-\ntical machine translation. EMNLP .\nChoe, D. K. and E. Charniak. 2016.\nParsing as language modeling.\nEMNLP .\nChoi, J. D. and M. Palmer. 2011a. Get-\nting the most out of transition-based\ndependency parsing. ACL.\nChoi, J. D. and M. Palmer. 2011b.\nTransition-based semantic role la-\nbeling using predicate argument\nclustering. Proceedings of the ACL\n2011 Workshop on Relational Mod-\nels of Semantics .\nChoi, J. D., J. Tetreault, and A. Stent.\n2015. It depends: Dependency\nparser comparison using a web-\nbased evaluation tool. ACL.\nChomsky, N. 1956. Three models for\nthe description of language. IRE\nTransactions on Information The-\nory, 2(3):113\u2013124.\nChomsky, N. 1956/1975. The Logi-\ncal Structure of Linguistic Theory .\nPlenum.\nChomsky, N. 1957. Syntactic Struc-\ntures . Mouton.\nChomsky, N. 1963. Formal proper-\nties of grammars. In R. D. Luce,\nR. Bush, and E. Galanter, eds, Hand-\nbook of Mathematical Psychology ,\nvolume 2, 323\u2013418. Wiley.\nChomsky, N. 1981. Lectures on Gov-\nernment and Binding . Foris.\nChorowski, J., D. Bahdanau, K. Cho,\nand Y . Bengio. 2014. End-to-end\ncontinuous speech recognition using\nattention-based recurrent NN: First\nresults. NeurIPS Deep Learning and\nRepresentation Learning Workshop .\nChou, W., C.-H. Lee, and B. H. Juang.\n1993. Minimum error rate train-\ning based on n-best string models.\nICASSP .\nChristiano, P. F., J. Leike, T. Brown,\nM. Martic, S. Legg, and D. Amodei.\n2017. Deep reinforcement learning\nfrom human preferences. NeurIPS ,\nvolume 30.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 879,
      "token_count": 736,
      "chapter_title": ""
    }
  },
  {
    "content": "Chomsky, N. 1956/1975. The Logi-\ncal Structure of Linguistic Theory .\nPlenum.\nChomsky, N. 1957. Syntactic Struc-\ntures . Mouton.\nChomsky, N. 1963. Formal proper-\nties of grammars. In R. D. Luce,\nR. Bush, and E. Galanter, eds, Hand-\nbook of Mathematical Psychology ,\nvolume 2, 323\u2013418. Wiley.\nChomsky, N. 1981. Lectures on Gov-\nernment and Binding . Foris.\nChorowski, J., D. Bahdanau, K. Cho,\nand Y . Bengio. 2014. End-to-end\ncontinuous speech recognition using\nattention-based recurrent NN: First\nresults. NeurIPS Deep Learning and\nRepresentation Learning Workshop .\nChou, W., C.-H. Lee, and B. H. Juang.\n1993. Minimum error rate train-\ning based on n-best string models.\nICASSP .\nChristiano, P. F., J. Leike, T. Brown,\nM. Martic, S. Legg, and D. Amodei.\n2017. Deep reinforcement learning\nfrom human preferences. NeurIPS ,\nvolume 30.\nChristodoulopoulos, C., S. Goldwa-\nter, and M. Steedman. 2010. Two\ndecades of unsupervised POS in-\nduction: How far have we come?\nEMNLP .\nChu, Y .-J. and T.-H. Liu. 1965. On the\nshortest arborescence of a directed\ngraph. Science Sinica , 14:1396\u2013\n1400.\nChu-Carroll, J. 1998. A statistical\nmodel for discourse act recognition\nin dialogue interactions. ApplyingMachine Learning to Discourse Pro-\ncessing. Papers from the 1998 AAAI\nSpring Symposium. Tech. rep. SS-\n98-01. AAAI Press.\nChu-Carroll, J. and S. Carberry. 1998.\nCollaborative response generation in\nplanning dialogues. Computational\nLinguistics , 24(3):355\u2013400.\nChurch, K. W. 1988. A stochastic parts\nprogram and noun phrase parser for\nunrestricted text. ANLP .\nChurch, K. W. 1989. A stochastic parts\nprogram and noun phrase parser for\nunrestricted text. ICASSP .\nChurch, K. W. 1994. Unix for Poets.\nSlides from 2nd ELSNET Summer\nSchool and unpublished paper ms.\nChurch, K. W. and W. A. Gale. 1991. A\ncomparison of the enhanced Good-\nTuring and deleted estimation meth-\nods for estimating probabilities of\nEnglish bigrams. Computer Speech\nand Language , 5:19\u201354.\nChurch, K. W. and P. Hanks. 1989.\nWord association norms, mutual in-\nformation, and lexicography. ACL.\nChurch, K. W. and P. Hanks. 1990.\nWord association norms, mutual in-\nformation, and lexicography. Com-\nputational Linguistics , 16(1):22\u201329.\nCialdini, R. B. 1984. In\ufb02uence: The\npsychology of persuasion . Morrow.\nCieri, C., D. Miller, and K. Walker.\n2004. The Fisher corpus: A resource",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 880,
      "token_count": 751,
      "chapter_title": ""
    }
  },
  {
    "content": "Church, K. W. 1988. A stochastic parts\nprogram and noun phrase parser for\nunrestricted text. ANLP .\nChurch, K. W. 1989. A stochastic parts\nprogram and noun phrase parser for\nunrestricted text. ICASSP .\nChurch, K. W. 1994. Unix for Poets.\nSlides from 2nd ELSNET Summer\nSchool and unpublished paper ms.\nChurch, K. W. and W. A. Gale. 1991. A\ncomparison of the enhanced Good-\nTuring and deleted estimation meth-\nods for estimating probabilities of\nEnglish bigrams. Computer Speech\nand Language , 5:19\u201354.\nChurch, K. W. and P. Hanks. 1989.\nWord association norms, mutual in-\nformation, and lexicography. ACL.\nChurch, K. W. and P. Hanks. 1990.\nWord association norms, mutual in-\nformation, and lexicography. Com-\nputational Linguistics , 16(1):22\u201329.\nCialdini, R. B. 1984. In\ufb02uence: The\npsychology of persuasion . Morrow.\nCieri, C., D. Miller, and K. Walker.\n2004. The Fisher corpus: A resource\nfor the next generations of speech-\nto-text. LREC .\nClark, E. 1987. The principle of con-\ntrast: A constraint on language ac-\nquisition. In B. MacWhinney, ed.,\nMechanisms of language acquisi-\ntion, 1\u201333. LEA.\nClark, H. H. 1996. Using Language .\nCambridge University Press.\nClark, H. H. and J. E. Fox Tree. 2002.\nUsing uh and um in spontaneous\nspeaking. Cognition , 84:73\u2013111.\nClark, H. H. and C. Marshall. 1981.\nDe\ufb01nite reference and mutual\nknowledge. In A. K. Joshi, B. L.\nWebber, and I. A. Sag, eds, Ele-\nments of Discourse Understanding ,\n10\u201363. Cambridge.\nClark, H. H. and D. Wilkes-Gibbs.\n1986. Referring as a collaborative\nprocess. Cognition , 22:1\u201339.\nClark, J. H., E. Choi, M. Collins,\nD. Garrette, T. Kwiatkowski,\nV . Nikolaev, and J. Palomaki.\n2020a. TyDi QA: A benchmark\nfor information-seeking question\nanswering in typologically diverse\nlanguages. TACL , 8:454\u2013470.\nClark, K., M.-T. Luong, Q. V . Le, and\nC. D. Manning. 2020b. Electra: Pre-\ntraining text encoders as discrimina-\ntors rather than generators. ICLR .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 881,
      "token_count": 619,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 566",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 882,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "558 Bibliography\nClark, K. and C. D. Manning. 2015.\nEntity-centric coreference resolution\nwith model stacking. ACL.\nClark, K. and C. D. Manning. 2016a.\nDeep reinforcement learning for\nmention-ranking coreference mod-\nels.EMNLP .\nClark, K. and C. D. Manning. 2016b.\nImproving coreference resolution by\nlearning entity-level distributed rep-\nresentations. ACL.\nClark, S., J. R. Curran, and M. Osborne.\n2003. Bootstrapping POS-taggers\nusing unlabelled data. CoNLL .\nCobbe, K., V . Kosaraju, M. Bavar-\nian, M. Chen, H. Jun, L. Kaiser,\nM. Plappert, J. Tworek, J. Hilton,\nR. Nakano, C. Hesse, and J. Schul-\nman. 2021. Training veri\ufb01ers to\nsolve math word problems. ArXiv\npreprint.\nCoccaro, N. and D. Jurafsky. 1998. To-\nwards better integration of seman-\ntic predictors in statistical language\nmodeling. ICSLP .\nCoenen, A., E. Reif, A. Yuan, B. Kim,\nA. Pearce, F. Vi \u00b4egas, and M. Watten-\nberg. 2019. Visualizing and measur-\ning the geometry of bert. NeurIPS .\nCohen, A. D., A. Roberts, A. Molina,\nA. Butryna, A. Jin, A. Kulshreshtha,\nB. Hutchinson, B. Zevenbergen,\nB. H. Aguera-Arcas, C. ching\nChang, C. Cui, C. Du, D. D. F.\nAdiwardana, D. Chen, D. D. Lep-\nikhin, E. H. Chi, E. Hoffman-John,\nH.-T. Cheng, H. Lee, I. Krivokon,\nJ. Qin, J. Hall, J. Fenton, J. Soraker,\nK. Meier-Hellstern, K. Olson, L. M.\nAroyo, M. P. Bosma, M. J. Pickett,\nM. A. Menegali, M. Croak, M. D \u00b4\u0131az,\nM. Lamm, M. Krikun, M. R. Mor-\nris, N. Shazeer, Q. V . Le, R. Bern-\nstein, R. Rajakumar, R. Kurzweil,\nR. Thoppilan, S. Zheng, T. Bos,\nT. Duke, T. Doshi, V . Y . Zhao,\nV . Prabhakaran, W. Rusch, Y . Li,\nY . Huang, Y . Zhou, Y . Xu, and\nZ. Chen. 2022. Lamda: Lan-\nguage models for dialog applica-\ntions. ArXiv preprint.\nCohen, M. H., J. P. Giangola, and\nJ. Balogh. 2004. Voice User Inter-\nface Design . Addison-Wesley.\nCohen, P. R. and C. R. Perrault. 1979.\nElements of a plan-based theory of\nspeech acts. Cognitive Science ,\n3(3):177\u2013212.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 883,
      "token_count": 737,
      "chapter_title": ""
    }
  },
  {
    "content": "K. Meier-Hellstern, K. Olson, L. M.\nAroyo, M. P. Bosma, M. J. Pickett,\nM. A. Menegali, M. Croak, M. D \u00b4\u0131az,\nM. Lamm, M. Krikun, M. R. Mor-\nris, N. Shazeer, Q. V . Le, R. Bern-\nstein, R. Rajakumar, R. Kurzweil,\nR. Thoppilan, S. Zheng, T. Bos,\nT. Duke, T. Doshi, V . Y . Zhao,\nV . Prabhakaran, W. Rusch, Y . Li,\nY . Huang, Y . Zhou, Y . Xu, and\nZ. Chen. 2022. Lamda: Lan-\nguage models for dialog applica-\ntions. ArXiv preprint.\nCohen, M. H., J. P. Giangola, and\nJ. Balogh. 2004. Voice User Inter-\nface Design . Addison-Wesley.\nCohen, P. R. and C. R. Perrault. 1979.\nElements of a plan-based theory of\nspeech acts. Cognitive Science ,\n3(3):177\u2013212.\nColby, K. M., S. Weber, and F. D. Hilf.\n1971. Arti\ufb01cial paranoia. Arti\ufb01cial\nIntelligence , 2(1):1\u201325.\nCole, R. A., D. G. Novick, P. J. E. Ver-\nmeulen, S. Sutton, M. Fanty, L. F. A.\nWessels, J. H. de Villiers, J. Schalk-\nwyk, B. Hansen, and D. Burnett.\n1997. Experiments with a spo-\nken dialogue system for taking theUS census. Speech Communication ,\n23:243\u2013260.\nCollins, M. 1999. Head-Driven Statis-\ntical Models for Natural Language\nParsing . Ph.D. thesis, University of\nPennsylvania, Philadelphia.\nCollobert, R. and J. Weston. 2007. Fast\nsemantic extraction using a novel\nneural network architecture. ACL.\nCollobert, R. and J. Weston. 2008.\nA uni\ufb01ed architecture for natural\nlanguage processing: Deep neural\nnetworks with multitask learning.\nICML .\nCollobert, R., J. Weston, L. Bottou,\nM. Karlen, K. Kavukcuoglu, and\nP. Kuksa. 2011. Natural language\nprocessing (almost) from scratch.\nJMLR , 12:2493\u20132537.\nComrie, B. 1989. Language Universals\nand Linguistic Typology , 2nd edi-\ntion. Blackwell.\nConneau, A., K. Khandelwal,\nN. Goyal, V . Chaudhary, G. Wen-\nzek, F. Guzm \u00b4an, E. Grave, M. Ott,\nL. Zettlemoyer, and V . Stoyanov.\n2020. Unsupervised cross-lingual\nrepresentation learning at scale.\nACL.\nConnolly, D., J. D. Burger, and D. S.\nDay. 1994. A machine learning ap-\nproach to anaphoric reference. Pro-\nceedings of the International Con-\nference on New Methods in Lan-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 884,
      "token_count": 746,
      "chapter_title": ""
    }
  },
  {
    "content": "semantic extraction using a novel\nneural network architecture. ACL.\nCollobert, R. and J. Weston. 2008.\nA uni\ufb01ed architecture for natural\nlanguage processing: Deep neural\nnetworks with multitask learning.\nICML .\nCollobert, R., J. Weston, L. Bottou,\nM. Karlen, K. Kavukcuoglu, and\nP. Kuksa. 2011. Natural language\nprocessing (almost) from scratch.\nJMLR , 12:2493\u20132537.\nComrie, B. 1989. Language Universals\nand Linguistic Typology , 2nd edi-\ntion. Blackwell.\nConneau, A., K. Khandelwal,\nN. Goyal, V . Chaudhary, G. Wen-\nzek, F. Guzm \u00b4an, E. Grave, M. Ott,\nL. Zettlemoyer, and V . Stoyanov.\n2020. Unsupervised cross-lingual\nrepresentation learning at scale.\nACL.\nConnolly, D., J. D. Burger, and D. S.\nDay. 1994. A machine learning ap-\nproach to anaphoric reference. Pro-\nceedings of the International Con-\nference on New Methods in Lan-\nguage Processing (NeMLaP) .\nCooley, J. W. and J. W. Tukey. 1965.\nAn algorithm for the machine cal-\nculation of complex Fourier se-\nries. Mathematics of Computation ,\n19(90):297\u2013301.\nCooper, F. S., A. M. Liberman, and\nJ. M. Borst. 1951. The interconver-\nsion of audible and visible patterns\nas a basis for research in the per-\nception of speech. Proceedings of\nthe National Academy of Sciences ,\n37(5):318\u2013325.\nCordier, B. 1965. Factor-analysis of\ncorrespondences. COLING 1965 .\nCosta-juss `a, M. R., J. Cross, O. C \u00b8 elebi,\nM. Elbayad, K. Hea\ufb01eld, K. Hef-\nfernan, E. Kalbassi, J. Lam,\nD. Licht, J. Maillard, A. Sun,\nS. Wang, G. Wenzek, A. Young-\nblood, B. Akula, L. Barrault,\nG. M. Gonzalez, P. Hansanti,\nJ. Hoffman, S. Jarrett, K. R.\nSadagopan, D. Rowe, S. Spruit,\nC. Tran, P. Andrews, N. F. Ayan,\nS. Bhosale, S. Edunov, A. Fan,\nC. Gao, V . Goswami, F. Guzm \u00b4an,\nP. Koehn, A. Mourachko, C. Ropers,\nS. Saleem, H. Schwenk, J. Wang,\nand NLLB Team. 2022. No lan-\nguage left behind: Scaling human-\ncentered machine translation. ArXiv.Cover, T. M. and J. A. Thomas. 1991.\nElements of Information Theory .\nWiley.\nCovington, M. 2001. A fundamen-\ntal algorithm for dependency pars-\ning. Proceedings of the 39th Annual\nACM Southeast Conference .\nCox, D. 1969. Analysis of Binary Data .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 885,
      "token_count": 746,
      "chapter_title": ""
    }
  },
  {
    "content": "fernan, E. Kalbassi, J. Lam,\nD. Licht, J. Maillard, A. Sun,\nS. Wang, G. Wenzek, A. Young-\nblood, B. Akula, L. Barrault,\nG. M. Gonzalez, P. Hansanti,\nJ. Hoffman, S. Jarrett, K. R.\nSadagopan, D. Rowe, S. Spruit,\nC. Tran, P. Andrews, N. F. Ayan,\nS. Bhosale, S. Edunov, A. Fan,\nC. Gao, V . Goswami, F. Guzm \u00b4an,\nP. Koehn, A. Mourachko, C. Ropers,\nS. Saleem, H. Schwenk, J. Wang,\nand NLLB Team. 2022. No lan-\nguage left behind: Scaling human-\ncentered machine translation. ArXiv.Cover, T. M. and J. A. Thomas. 1991.\nElements of Information Theory .\nWiley.\nCovington, M. 2001. A fundamen-\ntal algorithm for dependency pars-\ning. Proceedings of the 39th Annual\nACM Southeast Conference .\nCox, D. 1969. Analysis of Binary Data .\nChapman and Hall, London.\nCraven, M. and J. Kumlien. 1999.\nConstructing biological knowledge\nbases by extracting information\nfrom text sources. ISMB-99 .\nCrawford, K. 2017. The trouble with\nbias. Keynote at NeurIPS.\nCroft, W. 1990. Typology and Univer-\nsals. Cambridge University Press.\nCrosbie, J. and E. Shutova. 2022. In-\nduction heads as an essential mech-\nanism for pattern matching in in-\ncontext learning. ArXiv preprint.\nCross, J. and L. Huang. 2016. Span-\nbased constituency parsing with a\nstructure-label system and provably\noptimal dynamic oracles. EMNLP .\nCruse, D. A. 2004. Meaning in Lan-\nguage: an Introduction to Semantics\nand Pragmatics . Oxford University\nPress. Second edition.\nCucerzan, S. 2007. Large-scale\nnamed entity disambiguation based\non Wikipedia data. EMNLP/CoNLL .\nDagan, I., S. Marcus, and\nS. Markovitch. 1993. Contextual\nword similarity and estimation from\nsparse data. ACL.\nDahl, G. E., T. N. Sainath, and G. E.\nHinton. 2013. Improving deep\nneural networks for LVCSR using\nrecti\ufb01ed linear units and dropout.\nICASSP .\nDahl, G. E., D. Yu, L. Deng, and\nA. Acero. 2012. Context-dependent\npre-trained deep neural networks\nfor large-vocabulary speech recog-\nnition. IEEE Transactions on au-\ndio, speech, and language process-\ning, 20(1):30\u201342.\nDahl, M., V . Magesh, M. Suzgun, and\nD. E. Ho. 2024. Large legal \ufb01c-\ntions: Pro\ufb01ling legal hallucinations\nin large language models. Journal of\nLegal Analysis , 16:64\u201393.\nDai, A. M. and Q. V . Le. 2015.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 886,
      "token_count": 752,
      "chapter_title": ""
    }
  },
  {
    "content": "and Pragmatics . Oxford University\nPress. Second edition.\nCucerzan, S. 2007. Large-scale\nnamed entity disambiguation based\non Wikipedia data. EMNLP/CoNLL .\nDagan, I., S. Marcus, and\nS. Markovitch. 1993. Contextual\nword similarity and estimation from\nsparse data. ACL.\nDahl, G. E., T. N. Sainath, and G. E.\nHinton. 2013. Improving deep\nneural networks for LVCSR using\nrecti\ufb01ed linear units and dropout.\nICASSP .\nDahl, G. E., D. Yu, L. Deng, and\nA. Acero. 2012. Context-dependent\npre-trained deep neural networks\nfor large-vocabulary speech recog-\nnition. IEEE Transactions on au-\ndio, speech, and language process-\ning, 20(1):30\u201342.\nDahl, M., V . Magesh, M. Suzgun, and\nD. E. Ho. 2024. Large legal \ufb01c-\ntions: Pro\ufb01ling legal hallucinations\nin large language models. Journal of\nLegal Analysis , 16:64\u201393.\nDai, A. M. and Q. V . Le. 2015.\nSemi-supervised sequence learning.\nNeurIPS .\nDanieli, M. and E. Gerbino. 1995. Met-\nrics for evaluating dialogue strate-\ngies in a spoken language system.\nAAAI Spring Symposium on Empir-\nical Methods in Discourse Interpre-\ntation and Generation .\nDas, S. R. and M. Y . Chen. 2001. Ya-\nhoo! for Amazon: Sentiment pars-\ning from small talk on the web. EFA\n2001 Barcelona Meetings. http://\nssrn.com/abstract=276189 .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 887,
      "token_count": 407,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 567",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 888,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Bibliography 559\nDavid, Jr., E. E. and O. G. Selfridge.\n1962. Eyes and ears for computers.\nProceedings of the IRE (Institute of\nRadio Engineers) , 50:1093\u20131101.\nDavidson, T., D. Bhattacharya, and\nI. Weber. 2019. Racial bias in hate\nspeech and abusive language detec-\ntion datasets. Third Workshop on\nAbusive Language Online .\nDavies, M. 2012. Expanding hori-\nzons in historical linguistics with the\n400-million word Corpus of Histor-\nical American English. Corpora ,\n7(2):121\u2013157.\nDavies, M. 2015. The Wiki-\npedia Corpus: 4.6 million arti-\ncles, 1.9 billion words. Adapted\nfrom Wikipedia. https://www.\nenglish-corpora.org/wiki/ .\nDavies, M. 2020. The Corpus\nof Contemporary American En-\nglish (COCA): One billion words,\n1990-2019. https://www.\nenglish-corpora.org/coca/ .\nDavis, E., L. Morgenstern, and C. L.\nOrtiz. 2017. The \ufb01rst Winograd\nschema challenge at IJCAI-16. AI\nMagazine , 38(3):97\u201398.\nDavis, K. H., R. Biddulph, and S. Bal-\nashek. 1952. Automatic recognition\nof spoken digits. JASA , 24(6):637\u2013\n642.\nDavis, S. and P. Mermelstein. 1980.\nComparison of parametric repre-\nsentations for monosyllabic word\nrecognition in continuously spoken\nsentences. IEEE Transactions on\nASSP , 28(4):357\u2013366.\nDeerwester, S. C., S. T. Dumais, G. W.\nFurnas, R. A. Harshman, T. K.\nLandauer, K. E. Lochbaum, and\nL. Streeter. 1988. Computer infor-\nmation retrieval using latent seman-\ntic structure: US Patent 4,839,853.\nDeerwester, S. C., S. T. Dumais, T. K.\nLandauer, G. W. Furnas, and R. A.\nHarshman. 1990. Indexing by la-\ntent semantics analysis. JASIS ,\n41(6):391\u2013407.\nDeibel, D. and R. Evanhoe. 2021. Con-\nversations with Things: UX Design\nfor Chat and Voice . Rosenfeld.\nDeJong, G. F. 1982. An overview of the\nFRUMP system. In W. G. Lehnert\nand M. H. Ringle, eds, Strategies for\nNatural Language Processing , 149\u2013\n176. LEA.\nDemberg, V . 2006. Letter-to-phoneme\nconversion for a German text-to-\nspeech system. Diplomarbeit Nr. 47,\nUniversit \u00a8at Stuttgart.\nDenes, P. 1959. The design and oper-\nation of the mechanical speech rec-\nognizer at University College Lon-\ndon. Journal of the British Institu-\ntion of Radio Engineers , 19(4):219\u2013\n234. Appears together with compan-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 889,
      "token_count": 734,
      "chapter_title": ""
    }
  },
  {
    "content": "mation retrieval using latent seman-\ntic structure: US Patent 4,839,853.\nDeerwester, S. C., S. T. Dumais, T. K.\nLandauer, G. W. Furnas, and R. A.\nHarshman. 1990. Indexing by la-\ntent semantics analysis. JASIS ,\n41(6):391\u2013407.\nDeibel, D. and R. Evanhoe. 2021. Con-\nversations with Things: UX Design\nfor Chat and Voice . Rosenfeld.\nDeJong, G. F. 1982. An overview of the\nFRUMP system. In W. G. Lehnert\nand M. H. Ringle, eds, Strategies for\nNatural Language Processing , 149\u2013\n176. LEA.\nDemberg, V . 2006. Letter-to-phoneme\nconversion for a German text-to-\nspeech system. Diplomarbeit Nr. 47,\nUniversit \u00a8at Stuttgart.\nDenes, P. 1959. The design and oper-\nation of the mechanical speech rec-\nognizer at University College Lon-\ndon. Journal of the British Institu-\ntion of Radio Engineers , 19(4):219\u2013\n234. Appears together with compan-\nion paper (Fry 1959).Deng, L., G. Hinton, and B. Kingsbury.\n2013. New types of deep neural\nnetwork learning for speech recog-\nnition and related applications: An\noverview. ICASSP .\nDeng, Y . and W. Byrne. 2005. HMM\nword and phrase alignment for sta-\ntistical machine translation. HLT-\nEMNLP .\nDenis, P. and J. Baldridge. 2007. Joint\ndetermination of anaphoricity and\ncoreference resolution using integer\nprogramming. NAACL-HLT .\nDenis, P. and J. Baldridge. 2008. Spe-\ncialized models and ranking for\ncoreference resolution. EMNLP .\nDenis, P. and J. Baldridge. 2009. Global\njoint models for coreference resolu-\ntion and named entity classi\ufb01cation.\nProcesamiento del Lenguaje Natu-\nral, 42.\nDeRose, S. J. 1988. Grammatical cat-\negory disambiguation by statistical\noptimization. Computational Lin-\nguistics , 14:31\u201339.\nDevlin, J., M.-W. Chang, K. Lee, and\nK. Toutanova. 2019. BERT: Pre-\ntraining of deep bidirectional trans-\nformers for language understanding.\nNAACL HLT .\nDi Eugenio, B. 1990. Centering theory\nand the Italian pronominal system.\nCOLING .\nDi Eugenio, B. 1996. The discourse\nfunctions of Italian subjects: A cen-\ntering approach. COLING .\nDias Oliva, T., D. Antonialli, and\nA. Gomes. 2021. Fighting hate\nspeech, silencing drag queens? arti-\n\ufb01cial intelligence in content modera-\ntion and risks to lgbtq voices online.\nSexuality & Culture , 25:700\u2013732.\nDinan, E., G. Abercrombie, A. S.\nBergman, S. Spruit, D. Hovy, Y .-L.\nBoureau, and V . Rieser. 2021. Antic-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 890,
      "token_count": 749,
      "chapter_title": ""
    }
  },
  {
    "content": "ral, 42.\nDeRose, S. J. 1988. Grammatical cat-\negory disambiguation by statistical\noptimization. Computational Lin-\nguistics , 14:31\u201339.\nDevlin, J., M.-W. Chang, K. Lee, and\nK. Toutanova. 2019. BERT: Pre-\ntraining of deep bidirectional trans-\nformers for language understanding.\nNAACL HLT .\nDi Eugenio, B. 1990. Centering theory\nand the Italian pronominal system.\nCOLING .\nDi Eugenio, B. 1996. The discourse\nfunctions of Italian subjects: A cen-\ntering approach. COLING .\nDias Oliva, T., D. Antonialli, and\nA. Gomes. 2021. Fighting hate\nspeech, silencing drag queens? arti-\n\ufb01cial intelligence in content modera-\ntion and risks to lgbtq voices online.\nSexuality & Culture , 25:700\u2013732.\nDinan, E., G. Abercrombie, A. S.\nBergman, S. Spruit, D. Hovy, Y .-L.\nBoureau, and V . Rieser. 2021. Antic-\nipating safety issues in e2e conver-\nsational ai: Framework and tooling.\nArXiv.\nDinan, E., A. Fan, A. Williams, J. Ur-\nbanek, D. Kiela, and J. Weston.\n2020. Queens are powerful too: Mit-\nigating gender bias in dialogue gen-\neration. EMNLP .\nDitman, T. and G. R. Kuperberg.\n2010. Building coherence: A frame-\nwork for exploring the breakdown\nof links across clause boundaries in\nschizophrenia. Journal of neurolin-\nguistics , 23(3):254\u2013269.\nDixon, L., J. Li, J. Sorensen, N. Thain,\nand L. Vasserman. 2018. Measuring\nand mitigating unintended bias in\ntext classi\ufb01cation. 2018 AAAI/ACM\nConference on AI, Ethics, and Soci-\nety.Dixon, N. and H. Maxey. 1968. Termi-\nnal analog synthesis of continuous\nspeech using the diphone method of\nsegment assembly. IEEE Transac-\ntions on Audio and Electroacoustics ,\n16(1):40\u201350.\nDo, Q. N. T., S. Bethard, and M.-F.\nMoens. 2017. Improving implicit\nsemantic role labeling by predicting\nsemantic frame arguments. IJCNLP .\nDoddington, G. 2002. Automatic eval-\nuation of machine translation quality\nusing n-gram co-occurrence statis-\ntics. HLT.\nDodge, J., S. Gururangan, D. Card,\nR. Schwartz, and N. A. Smith. 2019.\nShow your work: Improved report-\ning of experimental results. EMNLP .\nDodge, J., M. Sap, A. Marasovi \u00b4c,\nW. Agnew, G. Ilharco, D. Groen-\neveld, M. Mitchell, and M. Gardner.\n2021. Documenting large webtext\ncorpora: A case study on the colos-\nsal clean crawled corpus. EMNLP .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 891,
      "token_count": 732,
      "chapter_title": ""
    }
  },
  {
    "content": "Conference on AI, Ethics, and Soci-\nety.Dixon, N. and H. Maxey. 1968. Termi-\nnal analog synthesis of continuous\nspeech using the diphone method of\nsegment assembly. IEEE Transac-\ntions on Audio and Electroacoustics ,\n16(1):40\u201350.\nDo, Q. N. T., S. Bethard, and M.-F.\nMoens. 2017. Improving implicit\nsemantic role labeling by predicting\nsemantic frame arguments. IJCNLP .\nDoddington, G. 2002. Automatic eval-\nuation of machine translation quality\nusing n-gram co-occurrence statis-\ntics. HLT.\nDodge, J., S. Gururangan, D. Card,\nR. Schwartz, and N. A. Smith. 2019.\nShow your work: Improved report-\ning of experimental results. EMNLP .\nDodge, J., M. Sap, A. Marasovi \u00b4c,\nW. Agnew, G. Ilharco, D. Groen-\neveld, M. Mitchell, and M. Gardner.\n2021. Documenting large webtext\ncorpora: A case study on the colos-\nsal clean crawled corpus. EMNLP .\nDong, L. and M. Lapata. 2016. Lan-\nguage to logical form with neural at-\ntention. ACL.\nDorr, B. 1994. Machine translation di-\nvergences: A formal description and\nproposed solution. Computational\nLinguistics , 20(4):597\u2013633.\nDostert, L. 1955. The Georgetown-\nI.B.M. experiment. In Machine\nTranslation of Languages: Fourteen\nEssays , 124\u2013135. MIT Press.\nDowty, D. R. 1979. Word Meaning and\nMontague Grammar . D. Reidel.\nDozat, T. and C. D. Manning. 2017.\nDeep biaf\ufb01ne attention for neural de-\npendency parsing. ICLR .\nDozat, T. and C. D. Manning. 2018.\nSimpler but more accurate semantic\ndependency parsing. ACL.\nDozat, T., P. Qi, and C. D. Manning.\n2017. Stanford\u2019s graph-based neu-\nral dependency parser at the CoNLL\n2017 shared task. Proceedings of the\nCoNLL 2017 Shared Task: Multilin-\ngual Parsing from Raw Text to Uni-\nversal Dependencies .\nDror, R., G. Baumer, M. Bogomolov,\nand R. Reichart. 2017. Replicabil-\nity analysis for natural language pro-\ncessing: Testing signi\ufb01cance with\nmultiple datasets. TACL , 5:471\u2013\n\u2013486.\nDror, R., L. Peled-Cohen, S. Shlomov,\nand R. Reichart. 2020. Statisti-\ncal Signi\ufb01cance Testing for Natural\nLanguage Processing , volume 45 of\nSynthesis Lectures on Human Lan-\nguage Technologies . Morgan &\nClaypool.\nDryer, M. S. and M. Haspelmath, eds.\n2013. The World Atlas of Language",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 892,
      "token_count": 698,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 568",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 893,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "560 Bibliography\nStructures Online . Max Planck In-\nstitute for Evolutionary Anthropol-\nogy, Leipzig. Available online at\nhttp://wals.info .\nDu Bois, J. W., W. L. Chafe, C. Meyer,\nS. A. Thompson, R. Englebretson,\nand N. Martey. 2005. Santa Barbara\ncorpus of spoken American English,\nParts 1-4. Philadelphia: Linguistic\nData Consortium.\nDurrett, G. and D. Klein. 2013. Easy\nvictories and uphill battles in coref-\nerence resolution. EMNLP .\nDurrett, G. and D. Klein. 2014. A joint\nmodel for entity analysis: Corefer-\nence, typing, and linking. TACL ,\n2:477\u2013490.\nEarley, J. 1968. An Ef\ufb01cient Context-\nFree Parsing Algorithm . Ph.D.\nthesis, Carnegie Mellon University,\nPittsburgh, PA.\nEarley, J. 1970. An ef\ufb01cient context-\nfree parsing algorithm. CACM ,\n6(8):451\u2013455.\nEbden, P. and R. Sproat. 2015. The\nKestrel TTS text normalization sys-\ntem. Natural Language Engineer-\ning, 21(3):333.\nEdmonds, J. 1967. Optimum branch-\nings. Journal of Research of the\nNational Bureau of Standards B ,\n71(4):233\u2013240.\nEdunov, S., M. Ott, M. Auli, and\nD. Grangier. 2018. Understanding\nback-translation at scale. EMNLP .\nEfron, B. and R. J. Tibshirani. 1993. An\nintroduction to the bootstrap . CRC\npress.\nEgghe, L. 2007. Untangling Herdan\u2019s\nlaw and Heaps\u2019 law: Mathematical\nand informetric arguments. JASIST ,\n58(5):702\u2013709.\nEisner, J. 1996. Three new probabilistic\nmodels for dependency parsing: An\nexploration. COLING .\nEkman, P. 1999. Basic emotions. In\nT. Dalgleish and M. J. Power, eds,\nHandbook of Cognition and Emo-\ntion, 45\u201360. Wiley.\nElhage, N., N. Nanda, C. Olsson,\nT. Henighan, N. Joseph, B. Mann,\nA. Askell, Y . Bai, A. Chen, T. Con-\nerly, N. DasSarma, D. Drain,\nD. Ganguli, Z. Hat\ufb01eld-Dodds,\nD. Hernandez, A. Jones, J. Kernion,\nL. Lovitt, K. Ndousse, D. Amodei,\nT. Brown, J. Clark, J. Kaplan, S. Mc-\nCandlish, and C. Olah. 2021. A\nmathematical framework for trans-\nformer circuits. White paper.\nElman, J. L. 1990. Finding structure in\ntime. Cognitive science , 14(2):179\u2013\n211.\nElsner, M., J. Austerweil, and E. Char-\nniak. 2007. A uni\ufb01ed local and\nglobal model for discourse coher-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 894,
      "token_count": 728,
      "chapter_title": ""
    }
  },
  {
    "content": "models for dependency parsing: An\nexploration. COLING .\nEkman, P. 1999. Basic emotions. In\nT. Dalgleish and M. J. Power, eds,\nHandbook of Cognition and Emo-\ntion, 45\u201360. Wiley.\nElhage, N., N. Nanda, C. Olsson,\nT. Henighan, N. Joseph, B. Mann,\nA. Askell, Y . Bai, A. Chen, T. Con-\nerly, N. DasSarma, D. Drain,\nD. Ganguli, Z. Hat\ufb01eld-Dodds,\nD. Hernandez, A. Jones, J. Kernion,\nL. Lovitt, K. Ndousse, D. Amodei,\nT. Brown, J. Clark, J. Kaplan, S. Mc-\nCandlish, and C. Olah. 2021. A\nmathematical framework for trans-\nformer circuits. White paper.\nElman, J. L. 1990. Finding structure in\ntime. Cognitive science , 14(2):179\u2013\n211.\nElsner, M., J. Austerweil, and E. Char-\nniak. 2007. A uni\ufb01ed local and\nglobal model for discourse coher-\nence. NAACL-HLT .Elsner, M. and E. Charniak. 2008.\nCoreference-inspired coherence\nmodeling. ACL.\nElsner, M. and E. Charniak. 2011. Ex-\ntending the entity grid with entity-\nspeci\ufb01c features. ACL.\nElvev \u02daag, B., P. W. Foltz, D. R.\nWeinberger, and T. E. Goldberg.\n2007. Quantifying incoherence in\nspeech: an automated methodology\nand novel application to schizophre-\nnia. Schizophrenia research , 93(1-\n3):304\u2013316.\nEmami, A. and F. Jelinek. 2005. A neu-\nral syntactic language model. Ma-\nchine learning , 60(1):195\u2013227.\nEmami, A., P. Trichelair, A. Trischler,\nK. Suleman, H. Schulz, and J. C. K.\nCheung. 2019. The KNOWREF\ncoreference corpus: Removing gen-\nder and number cues for dif\ufb01-\ncult pronominal anaphora resolu-\ntion. ACL.\nErk, K. 2007. A simple, similarity-\nbased model for selectional prefer-\nences. ACL.\nvan Esch, D. and R. Sproat. 2018.\nAn expanded taxonomy of semiotic\nclasses for text normalization. IN-\nTERSPEECH .\nEthayarajh, K. 2019. How contextual\nare contextualized word representa-\ntions? Comparing the geometry of\nBERT, ELMo, and GPT-2 embed-\ndings. EMNLP .\nEthayarajh, K., D. Duvenaud, and\nG. Hirst. 2019a. Towards un-\nderstanding linear word analogies.\nACL.\nEthayarajh, K., D. Duvenaud, and\nG. Hirst. 2019b. Understanding un-\ndesirable word embedding associa-\ntions. ACL.\nEthayarajh, K. and D. Jurafsky. 2020.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 895,
      "token_count": 743,
      "chapter_title": ""
    }
  },
  {
    "content": "K. Suleman, H. Schulz, and J. C. K.\nCheung. 2019. The KNOWREF\ncoreference corpus: Removing gen-\nder and number cues for dif\ufb01-\ncult pronominal anaphora resolu-\ntion. ACL.\nErk, K. 2007. A simple, similarity-\nbased model for selectional prefer-\nences. ACL.\nvan Esch, D. and R. Sproat. 2018.\nAn expanded taxonomy of semiotic\nclasses for text normalization. IN-\nTERSPEECH .\nEthayarajh, K. 2019. How contextual\nare contextualized word representa-\ntions? Comparing the geometry of\nBERT, ELMo, and GPT-2 embed-\ndings. EMNLP .\nEthayarajh, K., D. Duvenaud, and\nG. Hirst. 2019a. Towards un-\nderstanding linear word analogies.\nACL.\nEthayarajh, K., D. Duvenaud, and\nG. Hirst. 2019b. Understanding un-\ndesirable word embedding associa-\ntions. ACL.\nEthayarajh, K. and D. Jurafsky. 2020.\nUtility is in the eye of the user:\nA critique of NLP leaderboards.\nEMNLP .\nEtzioni, O., M. Cafarella, D. Downey,\nA.-M. Popescu, T. Shaked, S. Soder-\nland, D. S. Weld, and A. Yates.\n2005. Unsupervised named-entity\nextraction from the web: An experi-\nmental study. Arti\ufb01cial Intelligence ,\n165(1):91\u2013134.\nEvans, N. 2000. Word classes in the\nworld\u2019s languages. In G. Booij,\nC. Lehmann, and J. Mugdan, eds,\nMorphology: A Handbook on In\ufb02ec-\ntion and Word Formation , 708\u2013732.\nMouton.\nFader, A., S. Soderland, and O. Etzioni.\n2011. Identifying relations for open\ninformation extraction. EMNLP .\nFan, A., S. Bhosale, H. Schwenk,\nZ. Ma, A. El-Kishky, S. Goyal,\nM. Baines, O. Celebi, G. Wenzek,V . Chaudhary, N. Goyal, T. Birch,\nV . Liptchinsky, S. Edunov, M. Auli,\nand A. Joulin. 2021. Beyond\nenglish-centric multilingual ma-\nchine translation. JMLR , 22(107):1\u2013\n48.\nFano, R. M. 1961. Transmission of In-\nformation: A Statistical Theory of\nCommunications . MIT Press.\nFant, G. M. 1951. Speech communica-\ntion research. Ing. Vetenskaps Akad.\nStockholm, Sweden , 24:331\u2013337.\nFant, G. M. 1986. Glottal \ufb02ow: Models\nand interaction. Journal of Phonet-\nics, 14:393\u2013399.\nFast, E., B. Chen, and M. S. Bernstein.\n2016. Empath: Understanding Topic\nSignals in Large-Scale Text. CHI.\nFauconnier, G. and M. Turner. 2008.\nThe way we think: Conceptual\nblending and the mind\u2019s hidden",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 896,
      "token_count": 746,
      "chapter_title": ""
    }
  },
  {
    "content": "Z. Ma, A. El-Kishky, S. Goyal,\nM. Baines, O. Celebi, G. Wenzek,V . Chaudhary, N. Goyal, T. Birch,\nV . Liptchinsky, S. Edunov, M. Auli,\nand A. Joulin. 2021. Beyond\nenglish-centric multilingual ma-\nchine translation. JMLR , 22(107):1\u2013\n48.\nFano, R. M. 1961. Transmission of In-\nformation: A Statistical Theory of\nCommunications . MIT Press.\nFant, G. M. 1951. Speech communica-\ntion research. Ing. Vetenskaps Akad.\nStockholm, Sweden , 24:331\u2013337.\nFant, G. M. 1986. Glottal \ufb02ow: Models\nand interaction. Journal of Phonet-\nics, 14:393\u2013399.\nFast, E., B. Chen, and M. S. Bernstein.\n2016. Empath: Understanding Topic\nSignals in Large-Scale Text. CHI.\nFauconnier, G. and M. Turner. 2008.\nThe way we think: Conceptual\nblending and the mind\u2019s hidden\ncomplexities . Basic Books.\nFeldman, J. A. and D. H. Ballard.\n1982. Connectionist models and\ntheir properties. Cognitive Science ,\n6:205\u2013254.\nFellbaum, C., ed. 1998. WordNet: An\nElectronic Lexical Database . MIT\nPress.\nFeng, V . W. and G. Hirst. 2011. Classi-\nfying arguments by scheme. ACL.\nFeng, V . W. and G. Hirst. 2014.\nA linear-time bottom-up discourse\nparser with constraints and post-\nediting. ACL.\nFeng, V . W., Z. Lin, and G. Hirst. 2014.\nThe impact of deep hierarchical dis-\ncourse structures in the evaluation of\ntext coherence. COLING .\nFernandes, E. R., C. N. dos Santos, and\nR. L. Milidi \u00b4u. 2012. Latent struc-\nture perceptron with feature induc-\ntion for unrestricted coreference res-\nolution. CoNLL .\nFerragina, P. and U. Scaiella. 2011.\nFast and accurate annotation of short\ntexts with wikipedia pages. IEEE\nSoftware , 29(1):70\u201375.\nFerro, L., L. Gerber, I. Mani, B. Sund-\nheim, and G. Wilson. 2005. Tides\n2005 standard for the annotation of\ntemporal expressions. Technical re-\nport, MITRE.\nFerrucci, D. A. 2012. Introduction\nto \u201cThis is Watson\u201d. IBM Jour-\nnal of Research and Development ,\n56(3/4):1:1\u20131:15.\nFessler, L. 2017. We tested bots like Siri\nand Alexa to see who would stand\nup to sexual harassment. Quartz .\nFeb 22, 2017. https://qz.com/\n911681/ .\nField, A. and Y . Tsvetkov. 2019. Entity-\ncentric contextual affective analysis.\nACL.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 897,
      "token_count": 710,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 569",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 898,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Bibliography 561\nFikes, R. E. and N. J. Nilsson. 1971.\nSTRIPS: A new approach to the\napplication of theorem proving to\nproblem solving. Arti\ufb01cial Intelli-\ngence , 2:189\u2013208.\nFillmore, C. J. 1966. A proposal con-\ncerning English prepositions. In F. P.\nDinneen, ed., 17th annual Round Ta-\nble, volume 17 of Monograph Series\non Language and Linguistics , 19\u2013\n34. Georgetown University Press.\nFillmore, C. J. 1968. The case for case.\nIn E. W. Bach and R. T. Harms, eds,\nUniversals in Linguistic Theory , 1\u2013\n88. Holt, Rinehart & Winston.\nFillmore, C. J. 1985. Frames and the se-\nmantics of understanding. Quaderni\ndi Semantica , VI(2):222\u2013254.\nFillmore, C. J. 2003. Valency and se-\nmantic roles: the concept of deep\nstructure case. In V . Agel, L. M.\nEichinger, H. W. Eroms, P. Hell-\nwig, H. J. Heringer, and H. Lobin,\neds, Dependenz und Valenz: Ein\ninternationales Handbuch der zeit-\ngen\u00a8ossischen Forschung , chapter 36,\n457\u2013475. Walter de Gruyter.\nFillmore, C. J. 2012. ACL life-\ntime achievement award: Encoun-\nters with language. Computational\nLinguistics , 38(4):701\u2013718.\nFillmore, C. J. and C. F. Baker. 2009. A\nframes approach to semantic analy-\nsis. In B. Heine and H. Narrog, eds,\nThe Oxford Handbook of Linguistic\nAnalysis , 313\u2013340. Oxford Univer-\nsity Press.\nFillmore, C. J., C. R. Johnson, and\nM. R. L. Petruck. 2003. Background\nto FrameNet. International journal\nof lexicography , 16(3):235\u2013250.\nFinkelstein, L., E. Gabrilovich, Y . Ma-\ntias, E. Rivlin, Z. Solan, G. Wolf-\nman, and E. Ruppin. 2002. Placing\nsearch in context: The concept revis-\nited. ACM Transactions on Informa-\ntion Systems , 20(1):116\u2014-131.\nFinlayson, M. A. 2016. Inferring\nPropp\u2019s functions from semantically\nannotated text. The Journal of Amer-\nican Folklore , 129(511):55\u201377.\nFirth, J. R. 1935. The technique of se-\nmantics. Transactions of the philo-\nlogical society , 34(1):36\u201373.\nFirth, J. R. 1957. A synopsis of linguis-\ntic theory 1930\u20131955. In Studies in\nLinguistic Analysis . Philological So-\nciety. Reprinted in Palmer, F. (ed.)\n1968. Selected Papers of J. R. Firth.\nLongman, Harlow.\nFlanagan, J. L. 1972. Speech Analysis,\nSynthesis, and Perception . Springer.\nFlanagan, J. L., K. Ishizaka, and K. L.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 899,
      "token_count": 751,
      "chapter_title": ""
    }
  },
  {
    "content": "Finkelstein, L., E. Gabrilovich, Y . Ma-\ntias, E. Rivlin, Z. Solan, G. Wolf-\nman, and E. Ruppin. 2002. Placing\nsearch in context: The concept revis-\nited. ACM Transactions on Informa-\ntion Systems , 20(1):116\u2014-131.\nFinlayson, M. A. 2016. Inferring\nPropp\u2019s functions from semantically\nannotated text. The Journal of Amer-\nican Folklore , 129(511):55\u201377.\nFirth, J. R. 1935. The technique of se-\nmantics. Transactions of the philo-\nlogical society , 34(1):36\u201373.\nFirth, J. R. 1957. A synopsis of linguis-\ntic theory 1930\u20131955. In Studies in\nLinguistic Analysis . Philological So-\nciety. Reprinted in Palmer, F. (ed.)\n1968. Selected Papers of J. R. Firth.\nLongman, Harlow.\nFlanagan, J. L. 1972. Speech Analysis,\nSynthesis, and Perception . Springer.\nFlanagan, J. L., K. Ishizaka, and K. L.\nShipley. 1975. Synthesis of speech\nfrom a dynamic model of the vocal\ncords and vocal tract. The Bell Sys-\ntem Technical Journal , 54(3):485\u2013\n506.Foland, W. and J. H. Martin. 2016.\nCU-NLP at SemEval-2016 task 8:\nAMR parsing using LSTM-based re-\ncurrent neural networks. SemEval-\n2016 .\nFoland, Jr., W. R. and J. H. Martin.\n2015. Dependency-based seman-\ntic role labeling using convolutional\nneural networks. *SEM 2015 .\nFoltz, P. W., W. Kintsch, and T. K. Lan-\ndauer. 1998. The measurement of\ntextual coherence with latent seman-\ntic analysis. Discourse processes ,\n25(2-3):285\u2013307.\n8, W. Nekoto, V . Marivate, T. Matsila,\nT. Fasubaa, T. Kolawole, T. Fag-\nbohungbe, S. O. Akinola, S. H.\nMuhammad, S. Kabongo, S. Osei,\nS. Freshia, R. A. Niyongabo,\nR. M. P. Ogayo, O. Ahia, M. Mer-\nessa, M. Adeyemi, M. Mokgesi-\nSelinga, L. Okegbemi, L. J. Mar-\ntinus, K. Tajudeen, K. Degila,\nK. Ogueji, K. Siminyu, J. Kreutzer,\nJ. Webster, J. T. Ali, J. A. I.\nOrife, I. Ezeani, I. A. Dangana,\nH. Kamper, H. Elsahar, G. Duru,\nG. Kioko, E. Murhabazi, E. van\nBiljon, D. Whitenack, C. Onye-\nfuluchi, C. Emezue, B. Dossou,\nB. Sibanda, B. I. Bassey, A. Olabiyi,\nA. Ramkilowan, A. \u00a8Oktem, A. Akin-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 900,
      "token_count": 752,
      "chapter_title": ""
    }
  },
  {
    "content": "T. Fasubaa, T. Kolawole, T. Fag-\nbohungbe, S. O. Akinola, S. H.\nMuhammad, S. Kabongo, S. Osei,\nS. Freshia, R. A. Niyongabo,\nR. M. P. Ogayo, O. Ahia, M. Mer-\nessa, M. Adeyemi, M. Mokgesi-\nSelinga, L. Okegbemi, L. J. Mar-\ntinus, K. Tajudeen, K. Degila,\nK. Ogueji, K. Siminyu, J. Kreutzer,\nJ. Webster, J. T. Ali, J. A. I.\nOrife, I. Ezeani, I. A. Dangana,\nH. Kamper, H. Elsahar, G. Duru,\nG. Kioko, E. Murhabazi, E. van\nBiljon, D. Whitenack, C. Onye-\nfuluchi, C. Emezue, B. Dossou,\nB. Sibanda, B. I. Bassey, A. Olabiyi,\nA. Ramkilowan, A. \u00a8Oktem, A. Akin-\nfaderin, and A. Bashir. 2020. Partic-\nipatory research for low-resourced\nmachine translation: A case study\nin African languages. Findings of\nEMNLP .\nFox, B. A. 1993. Discourse Structure\nand Anaphora: Written and Conver-\nsational English . Cambridge.\nFrancis, W. N. and H. Ku \u02c7cera. 1982.\nFrequency Analysis of English Us-\nage. Houghton Mif\ufb02in, Boston.\nFranz, A. and T. Brants. 2006. All our\nn-gram are belong to you. https:\n//research.google/blog/\nall-our-n-gram-are-belong-to-you/ .\nFraser, N. M. and G. N. Gilbert. 1991.\nSimulating speech systems. Com-\nputer Speech and Language , 5:81\u2013\n99.\nFriedman, B. and D. G. Hendry. 2019.\nValue Sensitive Design: Shaping\nTechnology with Moral Imagination .\nMIT Press.\nFriedman, B., D. G. Hendry, and\nA. Borning. 2017. A survey\nof value sensitive design methods.\nFoundations and Trends in Human-\nComputer Interaction , 11(2):63\u2013\n125.\nFry, D. B. 1959. Theoretical as-\npects of mechanical speech recogni-\ntion. Journal of the British Institu-\ntion of Radio Engineers , 19(4):211\u2013\n218. Appears together with compan-\nion paper (Denes 1959).Furnas, G. W., T. K. Landauer, L. M.\nGomez, and S. T. Dumais. 1987.\nThe vocabulary problem in human-\nsystem communication. Commu-\nnications of the ACM , 30(11):964\u2013\n971.\nGabow, H. N., Z. Galil, T. Spencer, and\nR. E. Tarjan. 1986. Ef\ufb01cient algo-\nrithms for \ufb01nding minimum span-\nning trees in undirected and directed\ngraphs. Combinatorica , 6(2):109\u2013",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 901,
      "token_count": 753,
      "chapter_title": ""
    }
  },
  {
    "content": "99.\nFriedman, B. and D. G. Hendry. 2019.\nValue Sensitive Design: Shaping\nTechnology with Moral Imagination .\nMIT Press.\nFriedman, B., D. G. Hendry, and\nA. Borning. 2017. A survey\nof value sensitive design methods.\nFoundations and Trends in Human-\nComputer Interaction , 11(2):63\u2013\n125.\nFry, D. B. 1959. Theoretical as-\npects of mechanical speech recogni-\ntion. Journal of the British Institu-\ntion of Radio Engineers , 19(4):211\u2013\n218. Appears together with compan-\nion paper (Denes 1959).Furnas, G. W., T. K. Landauer, L. M.\nGomez, and S. T. Dumais. 1987.\nThe vocabulary problem in human-\nsystem communication. Commu-\nnications of the ACM , 30(11):964\u2013\n971.\nGabow, H. N., Z. Galil, T. Spencer, and\nR. E. Tarjan. 1986. Ef\ufb01cient algo-\nrithms for \ufb01nding minimum span-\nning trees in undirected and directed\ngraphs. Combinatorica , 6(2):109\u2013\n122.\nGaddy, D., M. Stern, and D. Klein.\n2018. What\u2019s going on in neural\nconstituency parsers? an analysis.\nNAACL HLT .\nGale, W. A. and K. W. Church. 1994.\nWhat is wrong with adding one? In\nN. Oostdijk and P. de Haan, eds,\nCorpus-Based Research into Lan-\nguage , 189\u2013198. Rodopi.\nGale, W. A. and K. W. Church. 1991.\nA program for aligning sentences in\nbilingual corpora. ACL.\nGale, W. A. and K. W. Church. 1993.\nA program for aligning sentences in\nbilingual corpora. Computational\nLinguistics , 19:75\u2013102.\nGale, W. A., K. W. Church, and\nD. Yarowsky. 1992a. One sense per\ndiscourse. HLT.\nGale, W. A., K. W. Church, and\nD. Yarowsky. 1992b. Work on sta-\ntistical methods for word sense dis-\nambiguation. AAAI Fall Symposium\non Probabilistic Approaches to Nat-\nural Language .\nGao, L., T. Hoppe, A. Thite, S. Bi-\nderman, C. Foster, N. Nabeshima,\nS. Black, J. Phang, S. Presser,\nL. Golding, H. He, and C. Leahy.\n2020. The Pile: An 800GB dataset\nof diverse text for language model-\ning. ArXiv preprint.\nGarg, N., L. Schiebinger, D. Jurafsky,\nand J. Zou. 2018. Word embeddings\nquantify 100 years of gender and\nethnic stereotypes. Proceedings of\nthe National Academy of Sciences ,\n115(16):E3635\u2013E3644.\nGarside, R. 1987. The CLAWS word-\ntagging system. In R. Garside,\nG. Leech, and G. Sampson, eds, The",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 902,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "D. Yarowsky. 1992a. One sense per\ndiscourse. HLT.\nGale, W. A., K. W. Church, and\nD. Yarowsky. 1992b. Work on sta-\ntistical methods for word sense dis-\nambiguation. AAAI Fall Symposium\non Probabilistic Approaches to Nat-\nural Language .\nGao, L., T. Hoppe, A. Thite, S. Bi-\nderman, C. Foster, N. Nabeshima,\nS. Black, J. Phang, S. Presser,\nL. Golding, H. He, and C. Leahy.\n2020. The Pile: An 800GB dataset\nof diverse text for language model-\ning. ArXiv preprint.\nGarg, N., L. Schiebinger, D. Jurafsky,\nand J. Zou. 2018. Word embeddings\nquantify 100 years of gender and\nethnic stereotypes. Proceedings of\nthe National Academy of Sciences ,\n115(16):E3635\u2013E3644.\nGarside, R. 1987. The CLAWS word-\ntagging system. In R. Garside,\nG. Leech, and G. Sampson, eds, The\nComputational Analysis of English ,\n30\u201341. Longman.\nGarside, R., G. Leech, and A. McEnery.\n1997. Corpus Annotation . Long-\nman.\nGebru, T., J. Morgenstern, B. Vec-\nchione, J. W. Vaughan, H. Wal-\nlach, H. Daum \u00b4e III, and K. Craw-\nford. 2020. Datasheets for datasets.\nArXiv.\nGehman, S., S. Gururangan, M. Sap,\nY . Choi, and N. A. Smith. 2020. Re-\nalToxicityPrompts: Evaluating neu-\nral toxic degeneration in language\nmodels. Findings of EMNLP .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 903,
      "token_count": 439,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 570",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 904,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "562 Bibliography\nGerber, M. and J. Y . Chai. 2010. Be-\nyond nombank: A study of implicit\narguments for nominal predicates.\nACL.\nGers, F. A., J. Schmidhuber, and\nF. Cummins. 2000. Learning to for-\nget: Continual prediction with lstm.\nNeural computation , 12(10):2451\u2013\n2471.\nGil, D. 2000. Syntactic categories,\ncross-linguistic variation and univer-\nsal grammar. In P. M. V ogel and\nB. Comrie, eds, Approaches to the\nTypology of Word Classes , 173\u2013216.\nMouton.\nGildea, D. and D. Jurafsky. 2000. Au-\ntomatic labeling of semantic roles.\nACL.\nGildea, D. and D. Jurafsky. 2002.\nAutomatic labeling of semantic\nroles. Computational Linguistics ,\n28(3):245\u2013288.\nGildea, D. and M. Palmer. 2002.\nThe necessity of syntactic parsing\nfor predicate argument recognition.\nACL.\nGiles, C. L., G. M. Kuhn, and R. J.\nWilliams. 1994. Dynamic recurrent\nneural networks: Theory and appli-\ncations. IEEE Trans. Neural Netw.\nLearning Syst. , 5(2):153\u2013156.\nGillick, L. and S. J. Cox. 1989. Some\nstatistical issues in the comparison\nof speech recognition algorithms.\nICASSP .\nGirard, G. 1718. La justesse de la\nlangue franc \u00b8oise: ou les diff \u00b4erentes\nsigni\ufb01cations des mots qui passent\npour synonimes . Laurent d\u2019Houry,\nParis.\nGiuliano, V . E. 1965. The inter-\npretation of word associations.\nStatistical Association Methods\nFor Mechanized Documentation.\nSymposium Proceedings. Wash-\nington, D.C., USA, March 17,\n1964 .https://nvlpubs.nist.\ngov/nistpubs/Legacy/MP/\nnbsmiscellaneouspub269.pdf .\nGladkova, A., A. Drozd, and S. Mat-\nsuoka. 2016. Analogy-based de-\ntection of morphological and se-\nmantic relations with word embed-\ndings: what works and what doesn\u2019t.\nNAACL Student Research Workshop .\nGlaese, A., N. McAleese, M. Trebacz,\nJ. Aslanides, V . Firoiu, T. Ewalds,\nM. Rauh, L. Weidinger, M. Chad-\nwick, P. Thacker, L. Campbell-\nGillingham, J. Uesato, P.-S. Huang,\nR. Comanescu, F. Yang, A. See,\nS. Dathathri, R. Greig, C. Chen,\nD. Fritz, J. Sanchez Elias, R. Green,\nS. Mokr \u00b4a, N. Fernando, B. Wu,\nR. Foley, S. Young, I. Gabriel,\nW. Isaac, J. Mellor, D. Hassabis,\nK. Kavukcuoglu, L. A. Hendricks,and G. Irving. 2022. Improving\nalignment of dialogue agents via tar-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 905,
      "token_count": 738,
      "chapter_title": ""
    }
  },
  {
    "content": "gov/nistpubs/Legacy/MP/\nnbsmiscellaneouspub269.pdf .\nGladkova, A., A. Drozd, and S. Mat-\nsuoka. 2016. Analogy-based de-\ntection of morphological and se-\nmantic relations with word embed-\ndings: what works and what doesn\u2019t.\nNAACL Student Research Workshop .\nGlaese, A., N. McAleese, M. Trebacz,\nJ. Aslanides, V . Firoiu, T. Ewalds,\nM. Rauh, L. Weidinger, M. Chad-\nwick, P. Thacker, L. Campbell-\nGillingham, J. Uesato, P.-S. Huang,\nR. Comanescu, F. Yang, A. See,\nS. Dathathri, R. Greig, C. Chen,\nD. Fritz, J. Sanchez Elias, R. Green,\nS. Mokr \u00b4a, N. Fernando, B. Wu,\nR. Foley, S. Young, I. Gabriel,\nW. Isaac, J. Mellor, D. Hassabis,\nK. Kavukcuoglu, L. A. Hendricks,and G. Irving. 2022. Improving\nalignment of dialogue agents via tar-\ngeted human judgements. ArXiv\npreprint.\nGlenberg, A. M. and D. A. Robert-\nson. 2000. Symbol grounding and\nmeaning: A comparison of high-\ndimensional and embodied theories\nof meaning. Journal of memory and\nlanguage , 43(3):379\u2013401.\nGodfrey, J., E. Holliman, and J. Mc-\nDaniel. 1992. SWITCHBOARD:\nTelephone speech corpus for re-\nsearch and development. ICASSP .\nGoel, V . and W. Byrne. 2000. Minimum\nbayes-risk automatic speech recog-\nnition. Computer Speech & Lan-\nguage , 14(2):115\u2013135.\nGoffman, E. 1974. Frame analysis: An\nessay on the organization of experi-\nence. Harvard University Press.\nGoldberg, J., M. Ostendorf, and\nK. Kirchhoff. 2003. The impact of\nresponse wording in error correction\nsubdialogs. ISCA Tutorial and Re-\nsearch Workshop on Error Handling\nin Spoken Dialogue Systems .\nGoldberg, Y . 2017. Neural Network\nMethods for Natural Language Pro-\ncessing , volume 10 of Synthesis Lec-\ntures on Human Language Tech-\nnologies . Morgan & Claypool.\nGonen, H. and Y . Goldberg. 2019. Lip-\nstick on a pig: Debiasing methods\ncover up systematic gender biases in\nword embeddings but do not remove\nthem. NAACL HLT .\nGood, M. D., J. A. Whiteside, D. R.\nWixon, and S. J. Jones. 1984. Build-\ning a user-derived interface. CACM ,\n27(10):1032\u20131043.\nGoodfellow, I., Y . Bengio, and\nA. Courville. 2016. Deep Learn-\ning. MIT Press.\nGoodman, J. 2006. A bit of progress\nin language modeling: Extended\nversion. Technical Report MSR-\nTR-2001-72, Machine Learning and\nApplied Statistics Group, Microsoft\nResearch, Redmond, WA.\nGoodwin, C. 1996. Transparent vi-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 906,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "K. Kirchhoff. 2003. The impact of\nresponse wording in error correction\nsubdialogs. ISCA Tutorial and Re-\nsearch Workshop on Error Handling\nin Spoken Dialogue Systems .\nGoldberg, Y . 2017. Neural Network\nMethods for Natural Language Pro-\ncessing , volume 10 of Synthesis Lec-\ntures on Human Language Tech-\nnologies . Morgan & Claypool.\nGonen, H. and Y . Goldberg. 2019. Lip-\nstick on a pig: Debiasing methods\ncover up systematic gender biases in\nword embeddings but do not remove\nthem. NAACL HLT .\nGood, M. D., J. A. Whiteside, D. R.\nWixon, and S. J. Jones. 1984. Build-\ning a user-derived interface. CACM ,\n27(10):1032\u20131043.\nGoodfellow, I., Y . Bengio, and\nA. Courville. 2016. Deep Learn-\ning. MIT Press.\nGoodman, J. 2006. A bit of progress\nin language modeling: Extended\nversion. Technical Report MSR-\nTR-2001-72, Machine Learning and\nApplied Statistics Group, Microsoft\nResearch, Redmond, WA.\nGoodwin, C. 1996. Transparent vi-\nsion. In E. Ochs, E. A. Schegloff,\nand S. A. Thompson, eds, Interac-\ntion and Grammar , 370\u2013404. Cam-\nbridge University Press.\nGopalakrishnan, K., B. Hedayatnia,\nQ. Chen, A. Gottardi, S. Kwa-\ntra, A. Venkatesh, R. Gabriel, and\nD. Hakkani-T \u00a8ur. 2019. Topical-\nchat: Towards knowledge-grounded\nopen-domain conversations. INTER-\nSPEECH .\nGould, J. D., J. Conti, and T. Ho-\nvanyecz. 1983. Composing let-\nters with a simulated listening type-\nwriter. CACM , 26(4):295\u2013308.Gould, J. D. and C. Lewis. 1985. De-\nsigning for usability: Key principles\nand what designers think. CACM ,\n28(3):300\u2013311.\nGould, S. J. 1980. The Panda\u2019s Thumb .\nPenguin Group.\nGraff, D. 1997. The 1996 Broadcast\nNews speech and language-model\ncorpus. Proceedings DARPA Speech\nRecognition Workshop .\nGravano, A., J. Hirschberg, and\n\u02c7S. Be \u02c7nu\u02c7s. 2012. Af\ufb01rmative cue\nwords in task-oriented dialogue.\nComputational Linguistics , 38(1):1\u2013\n39.\nGraves, A. 2012. Sequence transduc-\ntion with recurrent neural networks.\nICASSP .\nGraves, A. 2013. Generating se-\nquences with recurrent neural net-\nworks. ArXiv.\nGraves, A., S. Fern \u00b4andez, F. Gomez,\nand J. Schmidhuber. 2006. Con-\nnectionist temporal classi\ufb01cation:\nLabelling unsegmented sequence\ndata with recurrent neural networks.\nICML .\nGraves, A., S. Fern \u00b4andez, M. Li-\nwicki, H. Bunke, and J. Schmidhu-\nber. 2007. Unconstrained on-line",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 907,
      "token_count": 748,
      "chapter_title": ""
    }
  },
  {
    "content": "and what designers think. CACM ,\n28(3):300\u2013311.\nGould, S. J. 1980. The Panda\u2019s Thumb .\nPenguin Group.\nGraff, D. 1997. The 1996 Broadcast\nNews speech and language-model\ncorpus. Proceedings DARPA Speech\nRecognition Workshop .\nGravano, A., J. Hirschberg, and\n\u02c7S. Be \u02c7nu\u02c7s. 2012. Af\ufb01rmative cue\nwords in task-oriented dialogue.\nComputational Linguistics , 38(1):1\u2013\n39.\nGraves, A. 2012. Sequence transduc-\ntion with recurrent neural networks.\nICASSP .\nGraves, A. 2013. Generating se-\nquences with recurrent neural net-\nworks. ArXiv.\nGraves, A., S. Fern \u00b4andez, F. Gomez,\nand J. Schmidhuber. 2006. Con-\nnectionist temporal classi\ufb01cation:\nLabelling unsegmented sequence\ndata with recurrent neural networks.\nICML .\nGraves, A., S. Fern \u00b4andez, M. Li-\nwicki, H. Bunke, and J. Schmidhu-\nber. 2007. Unconstrained on-line\nhandwriting recognition with recur-\nrent neural networks. NeurIPS .\nGraves, A. and N. Jaitly. 2014. Towards\nend-to-end speech recognition with\nrecurrent neural networks. ICML .\nGraves, A., A.-r. Mohamed, and\nG. Hinton. 2013. Speech recognition\nwith deep recurrent neural networks.\nICASSP .\nGraves, A. and J. Schmidhuber. 2005.\nFramewise phoneme classi\ufb01cation\nwith bidirectional LSTM and other\nneural network architectures. Neu-\nral Networks , 18(5-6):602\u2013610.\nGraves, A., G. Wayne, and I. Dani-\nhelka. 2014. Neural Turing ma-\nchines. ArXiv.\nGreen, B. F., A. K. Wolf, C. Chom-\nsky, and K. Laughery. 1961. Base-\nball: An automatic question an-\nswerer. Proceedings of the Western\nJoint Computer Conference 19 .\nGreene, B. B. and G. M. Rubin. 1971.\nAutomatic grammatical tagging of\nEnglish. Department of Linguis-\ntics, Brown University, Providence,\nRhode Island.\nGreenwald, A. G., D. E. McGhee, and\nJ. L. K. Schwartz. 1998. Measur-\ning individual differences in implicit\ncognition: the implicit association\ntest. Journal of personality and so-\ncial psychology , 74(6):1464\u20131480.\nGrenager, T. and C. D. Manning. 2006.\nUnsupervised discovery of a statisti-\ncal verb lexicon. EMNLP .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 908,
      "token_count": 640,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 571",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 909,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Bibliography 563\nGrice, H. P. 1975. Logic and conversa-\ntion. In P. Cole and J. L. Morgan,\neds, Speech Acts: Syntax and Se-\nmantics Volume 3 , 41\u201358. Academic\nPress.\nGrice, H. P. 1978. Further notes on\nlogic and conversation. In P. Cole,\ned.,Pragmatics: Syntax and Seman-\ntics Volume 9 , 113\u2013127. Academic\nPress.\nGrishman, R. and B. Sundheim. 1995.\nDesign of the MUC-6 evaluation.\nMUC-6 .\nGrosz, B. J. 1977a. The representation\nand use of focus in a system for un-\nderstanding dialogs. IJCAI-77 . Mor-\ngan Kaufmann.\nGrosz, B. J. 1977b. The Representation\nand Use of Focus in Dialogue Un-\nderstanding . Ph.D. thesis, Univer-\nsity of California, Berkeley.\nGrosz, B. J., A. K. Joshi, and S. Wein-\nstein. 1983. Providing a uni\ufb01ed ac-\ncount of de\ufb01nite noun phrases in En-\nglish. ACL.\nGrosz, B. J., A. K. Joshi, and S. Wein-\nstein. 1995. Centering: A framework\nfor modeling the local coherence of\ndiscourse. Computational Linguis-\ntics, 21(2):203\u2013225.\nGrosz, B. J. and C. L. Sidner. 1980.\nPlans for discourse. In P. R. Cohen,\nJ. Morgan, and M. E. Pollack, eds,\nIntentions in Communication , 417\u2013\n444. MIT Press.\nGruber, J. S. 1965. Studies in Lexical\nRelations . Ph.D. thesis, MIT.\nGr\u00a8unewald, S., A. Friedrich, and\nJ. Kuhn. 2021. Applying Occam\u2019s\nrazor to transformer-based depen-\ndency parsing: What works, what\ndoesn\u2019t, and what is really neces-\nsary. IWPT .\nGuinaudeau, C. and M. Strube. 2013.\nGraph-based local coherence model-\ning. ACL.\nGuindon, R. 1988. A multidisciplinary\nperspective on dialogue structure in\nuser-advisor dialogues. In R. Guin-\ndon, ed., Cognitive Science and Its\nApplications for Human-Computer\nInteraction , 163\u2013200. Lawrence Erl-\nbaum.\nGundel, J. K., N. Hedberg, and\nR. Zacharski. 1993. Cognitive status\nand the form of referring expressions\nin discourse. Language , 69(2):274\u2013\n307.\nGururangan, S., A. Marasovi \u00b4c,\nS. Swayamdipta, K. Lo, I. Belt-\nagy, D. Downey, and N. A. Smith.\n2020. Don\u2019t stop pretraining: Adapt\nlanguage models to domains and\ntasks. ACL.\nGus\ufb01eld, D. 1997. Algorithms on\nStrings, Trees, and Sequences . Cam-\nbridge University Press.Guyon, I. and A. Elisseeff. 2003. An\nintroduction to variable and feature\nselection. JMLR , 3:1157\u20131182.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 910,
      "token_count": 748,
      "chapter_title": ""
    }
  },
  {
    "content": "sary. IWPT .\nGuinaudeau, C. and M. Strube. 2013.\nGraph-based local coherence model-\ning. ACL.\nGuindon, R. 1988. A multidisciplinary\nperspective on dialogue structure in\nuser-advisor dialogues. In R. Guin-\ndon, ed., Cognitive Science and Its\nApplications for Human-Computer\nInteraction , 163\u2013200. Lawrence Erl-\nbaum.\nGundel, J. K., N. Hedberg, and\nR. Zacharski. 1993. Cognitive status\nand the form of referring expressions\nin discourse. Language , 69(2):274\u2013\n307.\nGururangan, S., A. Marasovi \u00b4c,\nS. Swayamdipta, K. Lo, I. Belt-\nagy, D. Downey, and N. A. Smith.\n2020. Don\u2019t stop pretraining: Adapt\nlanguage models to domains and\ntasks. ACL.\nGus\ufb01eld, D. 1997. Algorithms on\nStrings, Trees, and Sequences . Cam-\nbridge University Press.Guyon, I. and A. Elisseeff. 2003. An\nintroduction to variable and feature\nselection. JMLR , 3:1157\u20131182.\nHaber, J. and M. Poesio. 2020. As-\nsessing polyseme sense similarity\nthrough co-predication acceptability\nand contextualised embedding dis-\ntance. *SEM .\nHabernal, I. and I. Gurevych. 2016.\nWhich argument is more convinc-\ning? Analyzing and predicting con-\nvincingness of Web arguments using\nbidirectional LSTM. ACL.\nHabernal, I. and I. Gurevych. 2017.\nArgumentation mining in user-\ngenerated web discourse. Computa-\ntional Linguistics , 43(1):125\u2013179.\nHaghighi, A. and D. Klein. 2009.\nSimple coreference resolution with\nrich syntactic and semantic features.\nEMNLP .\nHajishirzi, H., L. Zilles, D. S. Weld,\nand L. Zettlemoyer. 2013. Joint\ncoreference resolution and named-\nentity linking with multi-pass sieves.\nEMNLP .\nHaji\u02c7c, J. 1998. Building a Syn-\ntactically Annotated Corpus: The\nPrague Dependency Treebank , 106\u2013\n132. Karolinum.\nHaji\u02c7c, J. 2000. Morphological tagging:\nData vs. dictionaries. NAACL .\nHaji\u02c7c, J., M. Ciaramita, R. Johans-\nson, D. Kawahara, M. A. Mart \u00b4\u0131,\nL. M `arquez, A. Meyers, J. Nivre,\nS. Pad \u00b4o, J. \u02c7St\u02c7ep\u00b4anek, P. Stran \u02c7a\u00b4k,\nM. Surdeanu, N. Xue, and Y . Zhang.\n2009. The conll-2009 shared task:\nSyntactic and semantic dependen-\ncies in multiple languages. CoNLL .\nHakkani-T \u00a8ur, D., K. O\ufb02azer, and\nG. T \u00a8ur. 2002. Statistical morpholog-\nical disambiguation for agglutinative\nlanguages. Journal of Computers",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 911,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "and L. Zettlemoyer. 2013. Joint\ncoreference resolution and named-\nentity linking with multi-pass sieves.\nEMNLP .\nHaji\u02c7c, J. 1998. Building a Syn-\ntactically Annotated Corpus: The\nPrague Dependency Treebank , 106\u2013\n132. Karolinum.\nHaji\u02c7c, J. 2000. Morphological tagging:\nData vs. dictionaries. NAACL .\nHaji\u02c7c, J., M. Ciaramita, R. Johans-\nson, D. Kawahara, M. A. Mart \u00b4\u0131,\nL. M `arquez, A. Meyers, J. Nivre,\nS. Pad \u00b4o, J. \u02c7St\u02c7ep\u00b4anek, P. Stran \u02c7a\u00b4k,\nM. Surdeanu, N. Xue, and Y . Zhang.\n2009. The conll-2009 shared task:\nSyntactic and semantic dependen-\ncies in multiple languages. CoNLL .\nHakkani-T \u00a8ur, D., K. O\ufb02azer, and\nG. T \u00a8ur. 2002. Statistical morpholog-\nical disambiguation for agglutinative\nlanguages. Journal of Computers\nand Humanities , 36(4):381\u2013410.\nHalliday, M. A. K. and R. Hasan. 1976.\nCohesion in English . Longman. En-\nglish Language Series, Title No. 9.\nHamilton, W. L., K. Clark, J. Leskovec,\nand D. Jurafsky. 2016a. Inducing\ndomain-speci\ufb01c sentiment lexicons\nfrom unlabeled corpora. EMNLP .\nHamilton, W. L., J. Leskovec, and\nD. Jurafsky. 2016b. Diachronic word\nembeddings reveal statistical laws of\nsemantic change. ACL.\nHannun, A. 2017. Sequence modeling\nwith CTC. Distill , 2(11).\nHannun, A. Y ., A. L. Maas, D. Juraf-\nsky, and A. Y . Ng. 2014. First-pass\nlarge vocabulary continuous speech\nrecognition using bi-directional re-\ncurrent DNNs. ArXiv preprint\narXiv:1408.2873.\nHarris, C. M. 1953. A study of the\nbuilding blocks in speech. JASA ,\n25(5):962\u2013969.Harris, R. A. 2005. Voice Interaction\nDesign: Crafting the New Conver-\nsational Speech Systems . Morgan\nKaufmann.\nHarris, Z. S. 1946. From morpheme\nto utterance. Language , 22(3):161\u2013\n183.\nHarris, Z. S. 1954. Distributional struc-\nture. Word , 10:146\u2013162.\nHarris, Z. S. 1962. String Analysis of\nSentence Structure . Mouton, The\nHague.\nHashimoto, T., M. Srivastava,\nH. Namkoong, and P. Liang. 2018.\nFairness without demographics in\nrepeated loss minimization. ICML .\nHastie, T., R. J. Tibshirani, and J. H.\nFriedman. 2001. The Elements of\nStatistical Learning . Springer.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 912,
      "token_count": 753,
      "chapter_title": ""
    }
  },
  {
    "content": "sky, and A. Y . Ng. 2014. First-pass\nlarge vocabulary continuous speech\nrecognition using bi-directional re-\ncurrent DNNs. ArXiv preprint\narXiv:1408.2873.\nHarris, C. M. 1953. A study of the\nbuilding blocks in speech. JASA ,\n25(5):962\u2013969.Harris, R. A. 2005. Voice Interaction\nDesign: Crafting the New Conver-\nsational Speech Systems . Morgan\nKaufmann.\nHarris, Z. S. 1946. From morpheme\nto utterance. Language , 22(3):161\u2013\n183.\nHarris, Z. S. 1954. Distributional struc-\nture. Word , 10:146\u2013162.\nHarris, Z. S. 1962. String Analysis of\nSentence Structure . Mouton, The\nHague.\nHashimoto, T., M. Srivastava,\nH. Namkoong, and P. Liang. 2018.\nFairness without demographics in\nrepeated loss minimization. ICML .\nHastie, T., R. J. Tibshirani, and J. H.\nFriedman. 2001. The Elements of\nStatistical Learning . Springer.\nHatzivassiloglou, V . and K. McKeown.\n1997. Predicting the semantic orien-\ntation of adjectives. ACL.\nHatzivassiloglou, V . and J. Wiebe.\n2000. Effects of adjective orienta-\ntion and gradability on sentence sub-\njectivity. COLING .\nHaviland, S. E. and H. H. Clark. 1974.\nWhat\u2019s new? Acquiring new infor-\nmation as a process in comprehen-\nsion. Journal of Verbal Learning and\nVerbal Behaviour , 13:512\u2013521.\nHawkins, J. A. 1978. De\ufb01niteness\nand inde\ufb01niteness: a study in refer-\nence and grammaticality prediction .\nCroom Helm Ltd.\nHayashi, T., R. Yamamoto, K. In-\noue, T. Yoshimura, S. Watanabe,\nT. Toda, K. Takeda, Y . Zhang,\nand X. Tan. 2020. ESPnet-TTS:\nUni\ufb01ed, reproducible, and integrat-\nable open source end-to-end text-to-\nspeech toolkit. ICASSP .\nHe, L., K. Lee, M. Lewis, and L. Zettle-\nmoyer. 2017. Deep semantic role la-\nbeling: What works and what\u2019s next.\nACL.\nHe, W., K. Liu, J. Liu, Y . Lyu, S. Zhao,\nX. Xiao, Y . Liu, Y . Wang, H. Wu,\nQ. She, X. Liu, T. Wu, and H. Wang.\n2018. DuReader: a Chinese machine\nreading comprehension dataset from\nreal-world applications. Workshop\non Machine Reading for Question\nAnswering .\nHea\ufb01eld, K. 2011. KenLM: Faster\nand smaller language model queries.\nWorkshop on Statistical Machine\nTranslation .\nHea\ufb01eld, K., I. Pouzyrevsky, J. H.\nClark, and P. Koehn. 2013. Scal-\nable modi\ufb01ed Kneser-Ney language\nmodel estimation. ACL.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 913,
      "token_count": 752,
      "chapter_title": ""
    }
  },
  {
    "content": "oue, T. Yoshimura, S. Watanabe,\nT. Toda, K. Takeda, Y . Zhang,\nand X. Tan. 2020. ESPnet-TTS:\nUni\ufb01ed, reproducible, and integrat-\nable open source end-to-end text-to-\nspeech toolkit. ICASSP .\nHe, L., K. Lee, M. Lewis, and L. Zettle-\nmoyer. 2017. Deep semantic role la-\nbeling: What works and what\u2019s next.\nACL.\nHe, W., K. Liu, J. Liu, Y . Lyu, S. Zhao,\nX. Xiao, Y . Liu, Y . Wang, H. Wu,\nQ. She, X. Liu, T. Wu, and H. Wang.\n2018. DuReader: a Chinese machine\nreading comprehension dataset from\nreal-world applications. Workshop\non Machine Reading for Question\nAnswering .\nHea\ufb01eld, K. 2011. KenLM: Faster\nand smaller language model queries.\nWorkshop on Statistical Machine\nTranslation .\nHea\ufb01eld, K., I. Pouzyrevsky, J. H.\nClark, and P. Koehn. 2013. Scal-\nable modi\ufb01ed Kneser-Ney language\nmodel estimation. ACL.\nHeaps, H. S. 1978. Information re-\ntrieval. Computational and theoret-\nical aspects . Academic Press.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 914,
      "token_count": 308,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 572",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 915,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "564 Bibliography\nHearst, M. A. 1992a. Automatic acqui-\nsition of hyponyms from large text\ncorpora. COLING .\nHearst, M. A. 1992b. Automatic acqui-\nsition of hyponyms from large text\ncorpora. COLING .\nHearst, M. A. 1997. Texttiling: Seg-\nmenting text into multi-paragraph\nsubtopic passages. Computational\nLinguistics , 23:33\u201364.\nHearst, M. A. 1998. Automatic discov-\nery of WordNet relations. In C. Fell-\nbaum, ed., WordNet: An Electronic\nLexical Database . MIT Press.\nHeckerman, D., E. Horvitz, M. Sahami,\nand S. T. Dumais. 1998. A bayesian\napproach to \ufb01ltering junk e-mail.\nAAAI-98 Workshop on Learning for\nText Categorization .\nHeim, I. 1982. The semantics of de\ufb01nite\nand inde\ufb01nite noun phrases . Ph.D.\nthesis, University of Massachusetts\nat Amherst.\nHellrich, J., S. Buechel, and U. Hahn.\n2019. Modeling word emotion in\nhistorical language: Quantity beats\nsupposed stability in seed word se-\nlection. 3rd Joint SIGHUM Work-\nshop on Computational Linguistics\nfor Cultural Heritage, Social Sci-\nences, Humanities and Literature .\nHellrich, J. and U. Hahn. 2016. Bad\ncompany\u2014Neighborhoods in neural\nembedding spaces considered harm-\nful.COLING .\nHenderson, J. 1994. Description Based\nParsing in a Connectionist Network .\nPh.D. thesis, University of Pennsyl-\nvania, Philadelphia, PA.\nHenderson, J. 2003. Inducing history\nrepresentations for broad coverage\nstatistical parsing. HLT-NAACL-03 .\nHenderson, J. 2004. Discriminative\ntraining of a neural network statisti-\ncal parser. ACL.\nHenderson, P., J. Hu, J. Romoff,\nE. Brunskill, D. Jurafsky, and\nJ. Pineau. 2020. Towards the sys-\ntematic reporting of the energy and\ncarbon footprints of machine learn-\ning. Journal of Machine Learning\nResearch , 21(248):1\u201343.\nHenderson, P., X. Li, D. Jurafsky,\nT. Hashimoto, M. A. Lemley, and\nP. Liang. 2023. Foundation models\nand fair use. JMLR , 24(400):1\u201379.\nHenderson, P., K. Sinha, N. Angelard-\nGontier, N. R. Ke, G. Fried,\nR. Lowe, and J. Pineau. 2017. Eth-\nical challenges in data-driven dia-\nlogue systems. AAAI/ACM AI Ethics\nand Society Conference .\nHendrickx, I., S. N. Kim, Z. Kozareva,\nP. Nakov, D. \u00b4O S\u00b4eaghdha, S. Pad \u00b4o,\nM. Pennacchiotti, L. Romano, and\nS. Szpakowicz. 2009. Semeval-2010task 8: Multi-way classi\ufb01cation of\nsemantic relations between pairs of",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 916,
      "token_count": 750,
      "chapter_title": ""
    }
  },
  {
    "content": "E. Brunskill, D. Jurafsky, and\nJ. Pineau. 2020. Towards the sys-\ntematic reporting of the energy and\ncarbon footprints of machine learn-\ning. Journal of Machine Learning\nResearch , 21(248):1\u201343.\nHenderson, P., X. Li, D. Jurafsky,\nT. Hashimoto, M. A. Lemley, and\nP. Liang. 2023. Foundation models\nand fair use. JMLR , 24(400):1\u201379.\nHenderson, P., K. Sinha, N. Angelard-\nGontier, N. R. Ke, G. Fried,\nR. Lowe, and J. Pineau. 2017. Eth-\nical challenges in data-driven dia-\nlogue systems. AAAI/ACM AI Ethics\nand Society Conference .\nHendrickx, I., S. N. Kim, Z. Kozareva,\nP. Nakov, D. \u00b4O S\u00b4eaghdha, S. Pad \u00b4o,\nM. Pennacchiotti, L. Romano, and\nS. Szpakowicz. 2009. Semeval-2010task 8: Multi-way classi\ufb01cation of\nsemantic relations between pairs of\nnominals. 5th International Work-\nshop on Semantic Evaluation .\nHendrix, G. G., C. W. Thompson, and\nJ. Slocum. 1973. Language process-\ning via canonical verbs and semantic\nmodels. Proceedings of IJCAI-73 .\nHerdan, G. 1960. Type-token mathe-\nmatics . Mouton.\nHermann, K. M., T. Kocisky, E. Grefen-\nstette, L. Espeholt, W. Kay, M. Su-\nleyman, and P. Blunsom. 2015a.\nTeaching machines to read and com-\nprehend. NeurIPS .\nHermann, K. M., T. Ko \u02c7cisk\u00b4y,\nE. Grefenstette, L. Espeholt, W. Kay,\nM. Suleyman, and P. Blunsom.\n2015b. Teaching machines to read\nand comprehend. NeurIPS .\nHernault, H., H. Prendinger, D. A. du-\nVerle, and M. Ishizuka. 2010. Hilda:\nA discourse parser using support\nvector machine classi\ufb01cation. Dia-\nlogue & Discourse , 1(3).\nHidey, C., E. Musi, A. Hwang, S. Mure-\nsan, and K. McKeown. 2017. Ana-\nlyzing the semantic types of claims\nand premises in an online persuasive\nforum. 4th Workshop on Argument\nMining .\nHill, F., R. Reichart, and A. Korhonen.\n2015. Simlex-999: Evaluating se-\nmantic models with (genuine) sim-\nilarity estimation. Computational\nLinguistics , 41(4):665\u2013695.\nHinkelman, E. A. and J. Allen. 1989.\nTwo constraints on speech act ambi-\nguity. ACL.\nHinton, G. E. 1986. Learning dis-\ntributed representations of concepts.\nCOGSCI .\nHinton, G. E., S. Osindero, and Y .-W.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 917,
      "token_count": 749,
      "chapter_title": ""
    }
  },
  {
    "content": "2015b. Teaching machines to read\nand comprehend. NeurIPS .\nHernault, H., H. Prendinger, D. A. du-\nVerle, and M. Ishizuka. 2010. Hilda:\nA discourse parser using support\nvector machine classi\ufb01cation. Dia-\nlogue & Discourse , 1(3).\nHidey, C., E. Musi, A. Hwang, S. Mure-\nsan, and K. McKeown. 2017. Ana-\nlyzing the semantic types of claims\nand premises in an online persuasive\nforum. 4th Workshop on Argument\nMining .\nHill, F., R. Reichart, and A. Korhonen.\n2015. Simlex-999: Evaluating se-\nmantic models with (genuine) sim-\nilarity estimation. Computational\nLinguistics , 41(4):665\u2013695.\nHinkelman, E. A. and J. Allen. 1989.\nTwo constraints on speech act ambi-\nguity. ACL.\nHinton, G. E. 1986. Learning dis-\ntributed representations of concepts.\nCOGSCI .\nHinton, G. E., S. Osindero, and Y .-W.\nTeh. 2006. A fast learning algorithm\nfor deep belief nets. Neural compu-\ntation , 18(7):1527\u20131554.\nHinton, G. E., N. Srivastava,\nA. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov. 2012. Improv-\ning neural networks by preventing\nco-adaptation of feature detectors.\nArXiv preprint arXiv:1207.0580.\nHirschberg, J., D. J. Litman, and\nM. Swerts. 2001. Identifying user\ncorrections automatically in spoken\ndialogue systems. NAACL .\nHirschman, L., M. Light, E. Breck, and\nJ. D. Burger. 1999. Deep Read:\nA reading comprehension system.\nACL.\nHirst, G. 1981. Anaphora in Natu-\nral Language Understanding: A sur-\nvey. Number 119 in Lecture notes in\ncomputer science. Springer-Verlag.\nHirst, G. 1987. Semantic Interpreta-\ntion and the Resolution of Ambigu-\nity. Cambridge University Press.Hjelmslev, L. 1969. Prologomena to\na Theory of Language . University\nof Wisconsin Press. Translated by\nFrancis J. Whit\ufb01eld; original Danish\nedition 1943.\nHobbs, J. R. 1978. Resolving pronoun\nreferences. Lingua , 44:311\u2013338.\nHobbs, J. R. 1979. Coherence and\ncoreference. Cognitive Science ,\n3:67\u201390.\nHobbs, J. R., D. E. Appelt, J. Bear,\nD. Israel, M. Kameyama, M. E.\nStickel, and M. Tyson. 1997. FAS-\nTUS: A cascaded \ufb01nite-state trans-\nducer for extracting information\nfrom natural-language text. In\nE. Roche and Y . Schabes, eds,\nFinite-State Language Processing ,\n383\u2013406. MIT Press.\nHochreiter, S. and J. Schmidhuber.\n1997. Long short-term memory.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 918,
      "token_count": 753,
      "chapter_title": ""
    }
  },
  {
    "content": "ral Language Understanding: A sur-\nvey. Number 119 in Lecture notes in\ncomputer science. Springer-Verlag.\nHirst, G. 1987. Semantic Interpreta-\ntion and the Resolution of Ambigu-\nity. Cambridge University Press.Hjelmslev, L. 1969. Prologomena to\na Theory of Language . University\nof Wisconsin Press. Translated by\nFrancis J. Whit\ufb01eld; original Danish\nedition 1943.\nHobbs, J. R. 1978. Resolving pronoun\nreferences. Lingua , 44:311\u2013338.\nHobbs, J. R. 1979. Coherence and\ncoreference. Cognitive Science ,\n3:67\u201390.\nHobbs, J. R., D. E. Appelt, J. Bear,\nD. Israel, M. Kameyama, M. E.\nStickel, and M. Tyson. 1997. FAS-\nTUS: A cascaded \ufb01nite-state trans-\nducer for extracting information\nfrom natural-language text. In\nE. Roche and Y . Schabes, eds,\nFinite-State Language Processing ,\n383\u2013406. MIT Press.\nHochreiter, S. and J. Schmidhuber.\n1997. Long short-term memory.\nNeural Computation , 9(8):1735\u2013\n1780.\nHofmann, T. 1999. Probabilistic latent\nsemantic indexing. SIGIR-99 .\nHoltzman, A., J. Buys, L. Du,\nM. Forbes, and Y . Choi. 2020. The\ncurious case of neural text degener-\nation. ICLR .\nHonovich, O., U. Shaham, S. R. Bow-\nman, and O. Levy. 2023. Instruction\ninduction: From few examples to\nnatural language task descriptions.\nACL.\nHopcroft, J. E. and J. D. Ullman.\n1979. Introduction to Automata The-\nory, Languages, and Computation .\nAddison-Wesley.\nHou, Y ., K. Markert, and M. Strube.\n2018. Unrestricted bridging reso-\nlution. Computational Linguistics ,\n44(2):237\u2013284.\nHouseholder, F. W. 1995. Dionysius\nThrax, the technai , and Sextus Em-\npiricus. In E. F. K. Koerner and\nR. E. Asher, eds, Concise History of\nthe Language Sciences , 99\u2013103. El-\nsevier Science.\nHovy, E. H. 1990. Parsimonious\nand pro\ufb02igate approaches to the\nquestion of discourse structure rela-\ntions. Proceedings of the 5th Inter-\nnational Workshop on Natural Lan-\nguage Generation .\nHovy, E. H., M. P. Marcus, M. Palmer,\nL. A. Ramshaw, and R. Weischedel.\n2006. OntoNotes: The 90% solu-\ntion. HLT-NAACL .\nHu, M. and B. Liu. 2004a. Mining\nand summarizing customer reviews.\nKDD .\nHu, M. and B. Liu. 2004b. Mining\nand summarizing customer reviews.\nSIGKDD-04 .\nHuang, E. H., R. Socher, C. D. Man-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 919,
      "token_count": 737,
      "chapter_title": ""
    }
  },
  {
    "content": "2018. Unrestricted bridging reso-\nlution. Computational Linguistics ,\n44(2):237\u2013284.\nHouseholder, F. W. 1995. Dionysius\nThrax, the technai , and Sextus Em-\npiricus. In E. F. K. Koerner and\nR. E. Asher, eds, Concise History of\nthe Language Sciences , 99\u2013103. El-\nsevier Science.\nHovy, E. H. 1990. Parsimonious\nand pro\ufb02igate approaches to the\nquestion of discourse structure rela-\ntions. Proceedings of the 5th Inter-\nnational Workshop on Natural Lan-\nguage Generation .\nHovy, E. H., M. P. Marcus, M. Palmer,\nL. A. Ramshaw, and R. Weischedel.\n2006. OntoNotes: The 90% solu-\ntion. HLT-NAACL .\nHu, M. and B. Liu. 2004a. Mining\nand summarizing customer reviews.\nKDD .\nHu, M. and B. Liu. 2004b. Mining\nand summarizing customer reviews.\nSIGKDD-04 .\nHuang, E. H., R. Socher, C. D. Man-\nning, and A. Y . Ng. 2012. Improving",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 920,
      "token_count": 288,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 573",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 921,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Bibliography 565\nword representations via global con-\ntext and multiple word prototypes.\nACL.\nHuang, Z., W. Xu, and K. Yu. 2015.\nBidirectional LSTM-CRF models\nfor sequence tagging. arXiv preprint\narXiv:1508.01991 .\nHuffman, S. 1996. Learning informa-\ntion extraction patterns from exam-\nples. In S. Wertmer, E. Riloff, and\nG. Scheller, eds, Connectionist, Sta-\ntistical, and Symbolic Approaches\nto Learning Natural Language Pro-\ncessing , 246\u2013260. Springer.\nHunt, A. J. and A. W. Black. 1996.\nUnit selection in a concatenative\nspeech synthesis system using a\nlarge speech database. ICASSP .\nHutchins, W. J. 1986. Machine Trans-\nlation: Past, Present, Future . Ellis\nHorwood, Chichester, England.\nHutchins, W. J. 1997. From \ufb01rst con-\nception to \ufb01rst demonstration: The\nnascent years of machine transla-\ntion, 1947\u20131954. A chronology. Ma-\nchine Translation , 12:192\u2013252.\nHutchins, W. J. and H. L. Somers. 1992.\nAn Introduction to Machine Transla-\ntion. Academic Press.\nHutchinson, B., V . Prabhakaran,\nE. Denton, K. Webster, Y . Zhong,\nand S. Denuyl. 2020. Social bi-\nases in NLP models as barriers for\npersons with disabilities. ACL.\nHymes, D. 1974. Ways of speaking.\nIn R. Bauman and J. Sherzer, eds,\nExplorations in the ethnography of\nspeaking , 433\u2013451. Cambridge Uni-\nversity Press.\nIida, R., K. Inui, H. Takamura, and\nY . Matsumoto. 2003. Incorporating\ncontextual cues in trainable models\nfor coreference resolution. EACL\nWorkshop on The Computational\nTreatment of Anaphora .\nIrsoy, O. and C. Cardie. 2014. Opin-\nion mining with deep recurrent neu-\nral networks. EMNLP .\nIschen, C., T. Araujo, H. V oorveld,\nG. van Noort, and E. Smit. 2019.\nPrivacy concerns in chatbot interac-\ntions. International Workshop on\nChatbot Research and Design .\nISO8601. 2004. Data elements and\ninterchange formats\u2014information\ninterchange\u2014representation of\ndates and times. Technical report,\nInternational Organization for Stan-\ndards (ISO).\nItakura, F. 1975. Minimum prediction\nresidual principle applied to speech\nrecognition. IEEE Transactions on\nASSP , ASSP-32:67\u201372.\nIter, D., K. Guu, L. Lansing, and\nD. Jurafsky. 2020. Pretraining\nwith contrastive sentence objectivesimproves discourse performance of\nlanguage models. ACL.\nIter, D., J. Yoon, and D. Jurafsky. 2018.\nAutomatic detection of incoherent\nspeech for diagnosing schizophre-\nnia. Fifth Workshop on Computa-\ntional Linguistics and Clinical Psy-\nchology .\nIto, K. and L. Johnson. 2017.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 922,
      "token_count": 748,
      "chapter_title": ""
    }
  },
  {
    "content": "Irsoy, O. and C. Cardie. 2014. Opin-\nion mining with deep recurrent neu-\nral networks. EMNLP .\nIschen, C., T. Araujo, H. V oorveld,\nG. van Noort, and E. Smit. 2019.\nPrivacy concerns in chatbot interac-\ntions. International Workshop on\nChatbot Research and Design .\nISO8601. 2004. Data elements and\ninterchange formats\u2014information\ninterchange\u2014representation of\ndates and times. Technical report,\nInternational Organization for Stan-\ndards (ISO).\nItakura, F. 1975. Minimum prediction\nresidual principle applied to speech\nrecognition. IEEE Transactions on\nASSP , ASSP-32:67\u201372.\nIter, D., K. Guu, L. Lansing, and\nD. Jurafsky. 2020. Pretraining\nwith contrastive sentence objectivesimproves discourse performance of\nlanguage models. ACL.\nIter, D., J. Yoon, and D. Jurafsky. 2018.\nAutomatic detection of incoherent\nspeech for diagnosing schizophre-\nnia. Fifth Workshop on Computa-\ntional Linguistics and Clinical Psy-\nchology .\nIto, K. and L. Johnson. 2017.\nThe LJ speech dataset.\nhttps://keithito.com/\nLJ-Speech-Dataset/ .\nIyer, S., I. Konstas, A. Cheung, J. Krish-\nnamurthy, and L. Zettlemoyer. 2017.\nLearning a neural semantic parser\nfrom user feedback. ACL.\nIyer, S., X. V . Lin, R. Pasunuru, T. Mi-\nhaylov, D. Simig, P. Yu, K. Shus-\nter, T. Wang, Q. Liu, P. S. Koura,\nX. Li, B. O\u2019Horo, G. Pereyra,\nJ. Wang, C. Dewan, A. Celikyil-\nmaz, L. Zettlemoyer, and V . Stoy-\nanov. 2022. Opt-iml: Scaling lan-\nguage model instruction meta learn-\ning through the lens of generaliza-\ntion. ArXiv preprint.\nIzacard, G., P. Lewis, M. Lomeli,\nL. Hosseini, F. Petroni, T. Schick,\nJ. Dwivedi-Yu, A. Joulin, S. Riedel,\nand E. Grave. 2022. Few-shot learn-\ning with retrieval augmented lan-\nguage models. ArXiv preprint.\nJackendoff, R. 1983. Semantics and\nCognition . MIT Press.\nJacobs, P. S. and L. F. Rau. 1990.\nSCISOR: A system for extract-\ning information from on-line news.\nCACM , 33(11):88\u201397.\nJaech, A., G. Mulcaire, S. Hathi, M. Os-\ntendorf, and N. A. Smith. 2016.\nHierarchical character-word models\nfor language identi\ufb01cation. ACL\nWorkshop on NLP for Social Media .\nJaitly, N., P. Nguyen, A. Senior, and\nV . Vanhoucke. 2012. Application of\npretrained deep neural networks to\nlarge vocabulary speech recognition.\nINTERSPEECH .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 923,
      "token_count": 746,
      "chapter_title": ""
    }
  },
  {
    "content": "guage model instruction meta learn-\ning through the lens of generaliza-\ntion. ArXiv preprint.\nIzacard, G., P. Lewis, M. Lomeli,\nL. Hosseini, F. Petroni, T. Schick,\nJ. Dwivedi-Yu, A. Joulin, S. Riedel,\nand E. Grave. 2022. Few-shot learn-\ning with retrieval augmented lan-\nguage models. ArXiv preprint.\nJackendoff, R. 1983. Semantics and\nCognition . MIT Press.\nJacobs, P. S. and L. F. Rau. 1990.\nSCISOR: A system for extract-\ning information from on-line news.\nCACM , 33(11):88\u201397.\nJaech, A., G. Mulcaire, S. Hathi, M. Os-\ntendorf, and N. A. Smith. 2016.\nHierarchical character-word models\nfor language identi\ufb01cation. ACL\nWorkshop on NLP for Social Media .\nJaitly, N., P. Nguyen, A. Senior, and\nV . Vanhoucke. 2012. Application of\npretrained deep neural networks to\nlarge vocabulary speech recognition.\nINTERSPEECH .\nJauhiainen, T., M. Lui, M. Zampieri,\nT. Baldwin, and K. Lind \u00b4en. 2019.\nAutomatic language identi\ufb01cation in\ntexts: A survey. JAIR , 65(1):675\u2013\n682.\nJefferson, G. 1972. Side sequences. In\nD. Sudnow, ed., Studies in social in-\nteraction , 294\u2013333. Free Press, New\nYork.\nJeffreys, H. 1948. Theory of Probabil-\nity, 2nd edition. Clarendon Press.\nSection 3.23.\nJelinek, F. 1969. A fast sequential de-\ncoding algorithm using a stack. IBM\nJournal of Research and Develop-\nment , 13:675\u2013685.\nJelinek, F. 1990. Self-organized lan-\nguage modeling for speech recogni-\ntion. In A. Waibel and K.-F. Lee,eds, Readings in Speech Recogni-\ntion, 450\u2013506. Morgan Kaufmann.\nOriginally distributed as IBM tech-\nnical report in 1985.\nJelinek, F. and R. L. Mercer. 1980.\nInterpolated estimation of Markov\nsource parameters from sparse data.\nIn E. S. Gelsema and L. N. Kanal,\neds, Proceedings, Workshop on Pat-\ntern Recognition in Practice , 381\u2013\n397. North Holland.\nJelinek, F., R. L. Mercer, and L. R.\nBahl. 1975. Design of a linguis-\ntic statistical decoder for the recog-\nnition of continuous speech. IEEE\nTransactions on Information The-\nory, IT-21(3):250\u2013256.\nJi, H. and R. Grishman. 2011. Knowl-\nedge base population: Successful\napproaches and challenges. ACL.\nJi, H., R. Grishman, and H. T. Dang.\n2010. Overview of the tac 2011\nknowledge base population track.\nTAC-11 .\nJi, Y . and J. Eisenstein. 2014. Repre-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 924,
      "token_count": 743,
      "chapter_title": ""
    }
  },
  {
    "content": "guage modeling for speech recogni-\ntion. In A. Waibel and K.-F. Lee,eds, Readings in Speech Recogni-\ntion, 450\u2013506. Morgan Kaufmann.\nOriginally distributed as IBM tech-\nnical report in 1985.\nJelinek, F. and R. L. Mercer. 1980.\nInterpolated estimation of Markov\nsource parameters from sparse data.\nIn E. S. Gelsema and L. N. Kanal,\neds, Proceedings, Workshop on Pat-\ntern Recognition in Practice , 381\u2013\n397. North Holland.\nJelinek, F., R. L. Mercer, and L. R.\nBahl. 1975. Design of a linguis-\ntic statistical decoder for the recog-\nnition of continuous speech. IEEE\nTransactions on Information The-\nory, IT-21(3):250\u2013256.\nJi, H. and R. Grishman. 2011. Knowl-\nedge base population: Successful\napproaches and challenges. ACL.\nJi, H., R. Grishman, and H. T. Dang.\n2010. Overview of the tac 2011\nknowledge base population track.\nTAC-11 .\nJi, Y . and J. Eisenstein. 2014. Repre-\nsentation learning for text-level dis-\ncourse parsing. ACL.\nJi, Y . and J. Eisenstein. 2015. One vec-\ntor is not enough: Entity-augmented\ndistributed semantics for discourse\nrelations. TACL , 3:329\u2013344.\nJia, R. and P. Liang. 2016. Data recom-\nbination for neural semantic parsing.\nACL.\nJia, S., T. Meng, J. Zhao, and K.-W.\nChang. 2020. Mitigating gender bias\nampli\ufb01cation in distribution by pos-\nterior regularization. ACL.\nJohnson, J., M. Douze, and H. J \u00b4egou.\n2017. Billion-scale similarity\nsearch with GPUs. ArXiv preprint\narXiv:1702.08734.\nJohnson, W. E. 1932. Probability: de-\nductive and inductive problems (ap-\npendix to). Mind , 41(164):421\u2013423.\nJohnson-Laird, P. N. 1983. Mental\nModels . Harvard University Press,\nCambridge, MA.\nJones, M. P. and J. H. Martin. 1997.\nContextual spelling correction using\nlatent semantic analysis. ANLP .\nJones, R., A. McCallum, K. Nigam, and\nE. Riloff. 1999. Bootstrapping for\ntext learning tasks. IJCAI-99 Work-\nshop on Text Mining: Foundations,\nTechniques and Applications .\nJones, T. 2015. Toward a descrip-\ntion of African American Vernac-\nular English dialect regions using\n\u201cBlack Twitter\u201d. American Speech ,\n90(4):403\u2013440.\nJoos, M. 1950. Description of language\ndesign. JASA , 22:701\u2013708.\nJordan, M. 1986. Serial order: A paral-\nlel distributed processing approach.\nTechnical Report ICS Report 8604,\nUniversity of California, San Diego.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 925,
      "token_count": 709,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 574",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 926,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "566 Bibliography\nJoshi, A. K. and P. Hopely. 1999. A\nparser from antiquity. In A. Kor-\nnai, ed., Extended Finite State Mod-\nels of Language , 6\u201315. Cambridge\nUniversity Press.\nJoshi, A. K. and S. Kuhn. 1979. Cen-\ntered logic: The role of entity cen-\ntered sentence representation in nat-\nural language inferencing. IJCAI-79 .\nJoshi, A. K. and S. Weinstein. 1981.\nControl of inference: Role of some\naspects of discourse structure \u2013 cen-\ntering. IJCAI-81 .\nJoshi, M., D. Chen, Y . Liu, D. S.\nWeld, L. Zettlemoyer, and O. Levy.\n2020. SpanBERT: Improving pre-\ntraining by representing and predict-\ning spans. TACL , 8:64\u201377.\nJoshi, M., O. Levy, D. S. Weld, and\nL. Zettlemoyer. 2019. BERT for\ncoreference resolution: Baselines\nand analysis. EMNLP .\nJoty, S., G. Carenini, and R. T. Ng.\n2015. CODRA: A novel discrimi-\nnative framework for rhetorical anal-\nysis. Computational Linguistics ,\n41(3):385\u2013435.\nJurafsky, D. 2014. The Language of\nFood . W. W. Norton, New York.\nJurafsky, D., V . Chahuneau, B. R. Rout-\nledge, and N. A. Smith. 2014. Narra-\ntive framing of consumer sentiment\nin online restaurant reviews. First\nMonday , 19(4).\nJurafsky, D., C. Wooters, G. Tajchman,\nJ. Segal, A. Stolcke, E. Fosler, and\nN. Morgan. 1994. The Berkeley\nrestaurant project. ICSLP .\nJurgens, D., S. M. Mohammad,\nP. Turney, and K. Holyoak. 2012.\nSemEval-2012 task 2: Measur-\ning degrees of relational similarity.\n*SEM 2012 .\nJurgens, D., Y . Tsvetkov, and D. Juraf-\nsky. 2017. Incorporating dialectal\nvariability for socially equitable lan-\nguage identi\ufb01cation. ACL.\nJusteson, J. S. and S. M. Katz. 1991.\nCo-occurrences of antonymous ad-\njectives and their contexts. Compu-\ntational linguistics , 17(1):1\u201319.\nKalchbrenner, N. and P. Blunsom.\n2013. Recurrent continuous transla-\ntion models. EMNLP .\nKameyama, M. 1986. A property-\nsharing constraint in centering. ACL.\nKamp, H. 1981. A theory of truth and\nsemantic representation. In J. Groe-\nnendijk, T. Janssen, and M. Stokhof,\neds, Formal Methods in the Study\nof Language , 189\u2013222. Mathemati-\ncal Centre, Amsterdam.\nKamphuis, C., A. P. de Vries,\nL. Boytsov, and J. Lin. 2020. Which\nBM25 do you mean? a large-scale",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 927,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "ing degrees of relational similarity.\n*SEM 2012 .\nJurgens, D., Y . Tsvetkov, and D. Juraf-\nsky. 2017. Incorporating dialectal\nvariability for socially equitable lan-\nguage identi\ufb01cation. ACL.\nJusteson, J. S. and S. M. Katz. 1991.\nCo-occurrences of antonymous ad-\njectives and their contexts. Compu-\ntational linguistics , 17(1):1\u201319.\nKalchbrenner, N. and P. Blunsom.\n2013. Recurrent continuous transla-\ntion models. EMNLP .\nKameyama, M. 1986. A property-\nsharing constraint in centering. ACL.\nKamp, H. 1981. A theory of truth and\nsemantic representation. In J. Groe-\nnendijk, T. Janssen, and M. Stokhof,\neds, Formal Methods in the Study\nof Language , 189\u2013222. Mathemati-\ncal Centre, Amsterdam.\nKamphuis, C., A. P. de Vries,\nL. Boytsov, and J. Lin. 2020. Which\nBM25 do you mean? a large-scale\nreproducibility study of scoringvariants. European Conference on\nInformation Retrieval .\nKane, S. K., M. R. Morris, A. Paradiso,\nand J. Campbell. 2017. \u201cat times\navuncular and cantankerous, with\nthe re\ufb02exes of a mongoose\u201d: Un-\nderstanding self-expression through\naugmentative and alternative com-\nmunication devices. CSCW .\nKaplan, J., S. McCandlish,\nT. Henighan, T. B. Brown, B. Chess,\nR. Child, S. Gray, A. Radford, J. Wu,\nand D. Amodei. 2020. Scaling laws\nfor neural language models. ArXiv\npreprint.\nKaplan, R. M. 1973. A general syntac-\ntic processor. In R. Rustin, ed., Natu-\nral Language Processing , 193\u2013241.\nAlgorithmics Press.\nKaramanis, N., M. Poesio, C. Mellish,\nand J. Oberlander. 2004. Evaluat-\ning centering-based metrics of co-\nherence for text structuring using a\nreliably annotated corpus. ACL.\nKarita, S., N. Chen, T. Hayashi,\nT. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y . Soplin, R. Ya-\nmamoto, X. Wang, S. Watanabe,\nT. Yoshimura, and W. Zhang. 2019.\nA comparative study on transformer\nvs RNN in speech applications.\nIEEE ASRU-19 .\nKarlsson, F., A. V outilainen,\nJ. Heikkil \u00a8a, and A. Anttila, eds.\n1995. Constraint Grammar: A\nLanguage-Independent System for\nParsing Unrestricted Text . Mouton\nde Gruyter.\nKarpukhin, V ., B. O \u02d8guz, S. Min,\nP. Lewis, L. Wu, S. Edunov,\nD. Chen, and W.-t. Yih. 2020. Dense\npassage retrieval for open-domain",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 928,
      "token_count": 744,
      "chapter_title": ""
    }
  },
  {
    "content": "Algorithmics Press.\nKaramanis, N., M. Poesio, C. Mellish,\nand J. Oberlander. 2004. Evaluat-\ning centering-based metrics of co-\nherence for text structuring using a\nreliably annotated corpus. ACL.\nKarita, S., N. Chen, T. Hayashi,\nT. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y . Soplin, R. Ya-\nmamoto, X. Wang, S. Watanabe,\nT. Yoshimura, and W. Zhang. 2019.\nA comparative study on transformer\nvs RNN in speech applications.\nIEEE ASRU-19 .\nKarlsson, F., A. V outilainen,\nJ. Heikkil \u00a8a, and A. Anttila, eds.\n1995. Constraint Grammar: A\nLanguage-Independent System for\nParsing Unrestricted Text . Mouton\nde Gruyter.\nKarpukhin, V ., B. O \u02d8guz, S. Min,\nP. Lewis, L. Wu, S. Edunov,\nD. Chen, and W.-t. Yih. 2020. Dense\npassage retrieval for open-domain\nquestion answering. EMNLP .\nKarttunen, L. 1969. Discourse refer-\nents. COLING . Preprint No. 70.\nKarttunen, L. 1999. Comments on\nJoshi. In A. Kornai, ed., Extended\nFinite State Models of Language ,\n16\u201318. Cambridge University Press.\nKasami, T. 1965. An ef\ufb01cient recog-\nnition and syntax analysis algorithm\nfor context-free languages. Tech-\nnical Report AFCRL-65-758, Air\nForce Cambridge Research Labora-\ntory, Bedford, MA.\nKatz, J. J. and J. A. Fodor. 1963. The\nstructure of a semantic theory. Lan-\nguage , 39:170\u2013210.\nKay, M. 1967. Experiments with a pow-\nerful parser. COLING .\nKay, M. 1973. The MIND system.\nIn R. Rustin, ed., Natural Language\nProcessing , 155\u2013188. Algorithmics\nPress.\nKay, M. 1982. Algorithm schemata and\ndata structures in syntactic process-\ning. In S. All \u00b4en, ed., Text Process-\ning: Text Analysis and Generation,Text Typology and Attribution , 327\u2013\n358. Almqvist and Wiksell, Stock-\nholm.\nKay, M. and M. R \u00a8oscheisen. 1988.\nText-translation alignment. Techni-\ncal Report P90-00143, Xerox Palo\nAlto Research Center, Palo Alto,\nCA.\nKay, M. and M. R \u00a8oscheisen. 1993.\nText-translation alignment. Compu-\ntational Linguistics , 19:121\u2013142.\nKehler, A. 1993. The effect of es-\ntablishing coherence in ellipsis and\nanaphora resolution. ACL.\nKehler, A. 1994. Temporal relations:\nReference or discourse coherence?\nACL.\nKehler, A. 1997a. Current theories of\ncentering for pronoun interpretation:\nA critical evaluation. Computational\nLinguistics , 23(3):467\u2013475.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 929,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "Kay, M. 1973. The MIND system.\nIn R. Rustin, ed., Natural Language\nProcessing , 155\u2013188. Algorithmics\nPress.\nKay, M. 1982. Algorithm schemata and\ndata structures in syntactic process-\ning. In S. All \u00b4en, ed., Text Process-\ning: Text Analysis and Generation,Text Typology and Attribution , 327\u2013\n358. Almqvist and Wiksell, Stock-\nholm.\nKay, M. and M. R \u00a8oscheisen. 1988.\nText-translation alignment. Techni-\ncal Report P90-00143, Xerox Palo\nAlto Research Center, Palo Alto,\nCA.\nKay, M. and M. R \u00a8oscheisen. 1993.\nText-translation alignment. Compu-\ntational Linguistics , 19:121\u2013142.\nKehler, A. 1993. The effect of es-\ntablishing coherence in ellipsis and\nanaphora resolution. ACL.\nKehler, A. 1994. Temporal relations:\nReference or discourse coherence?\nACL.\nKehler, A. 1997a. Current theories of\ncentering for pronoun interpretation:\nA critical evaluation. Computational\nLinguistics , 23(3):467\u2013475.\nKehler, A. 1997b. Probabilistic coref-\nerence in information extraction.\nEMNLP .\nKehler, A. 2000. Coherence, Reference,\nand the Theory of Grammar . CSLI\nPublications.\nKehler, A., D. E. Appelt, L. Taylor, and\nA. Simma. 2004. The (non)utility\nof predicate-argument frequencies\nfor pronoun interpretation. HLT-\nNAACL .\nKehler, A. and H. Rohde. 2013. A prob-\nabilistic reconciliation of coherence-\ndriven and centering-driven theories\nof pronoun interpretation. Theoreti-\ncal Linguistics , 39(1-2):1\u201337.\nKeller, F. and M. Lapata. 2003. Using\nthe web to obtain frequencies for un-\nseen bigrams. Computational Lin-\nguistics , 29:459\u2013484.\nKendall, T. and C. Farrington. 2020.\nThe Corpus of Regional African\nAmerican Language. Version\n2020.05. Eugene, OR: The On-\nline Resources for African Amer-\nican Language Project. http:\n//oraal.uoregon.edu/coraal.\nKennedy, C. and B. K. Boguraev. 1996.\nAnaphora for everyone: Pronomi-\nnal anaphora resolution without a\nparser. COLING .\nKhandelwal, U., O. Levy, D. Juraf-\nsky, L. Zettlemoyer, and M. Lewis.\n2019. Generalization through mem-\norization: Nearest neighbor lan-\nguage models. ICLR .\nKhattab, O., C. Potts, and M. Zaharia.\n2021. Relevance-guided supervision\nfor OpenQA with ColBERT. TACL ,\n9:929\u2013944.\nKhattab, O., A. Singhvi, P. Mahesh-\nwari, Z. Zhang, K. Santhanam,\nS. Haq, A. Sharma, T. T. Joshi,\nH. Moazam, H. Miller, M. Zaharia,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 930,
      "token_count": 735,
      "chapter_title": ""
    }
  },
  {
    "content": "seen bigrams. Computational Lin-\nguistics , 29:459\u2013484.\nKendall, T. and C. Farrington. 2020.\nThe Corpus of Regional African\nAmerican Language. Version\n2020.05. Eugene, OR: The On-\nline Resources for African Amer-\nican Language Project. http:\n//oraal.uoregon.edu/coraal.\nKennedy, C. and B. K. Boguraev. 1996.\nAnaphora for everyone: Pronomi-\nnal anaphora resolution without a\nparser. COLING .\nKhandelwal, U., O. Levy, D. Juraf-\nsky, L. Zettlemoyer, and M. Lewis.\n2019. Generalization through mem-\norization: Nearest neighbor lan-\nguage models. ICLR .\nKhattab, O., C. Potts, and M. Zaharia.\n2021. Relevance-guided supervision\nfor OpenQA with ColBERT. TACL ,\n9:929\u2013944.\nKhattab, O., A. Singhvi, P. Mahesh-\nwari, Z. Zhang, K. Santhanam,\nS. Haq, A. Sharma, T. T. Joshi,\nH. Moazam, H. Miller, M. Zaharia,\nand C. Potts. 2024. DSPy: Compil-\ning declarative language model calls\ninto self-improving pipelines. ICLR .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 931,
      "token_count": 307,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 575",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 932,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Bibliography 567\nKhattab, O. and M. Zaharia. 2020. Col-\nBERT: Ef\ufb01cient and effective pas-\nsage search via contextualized late\ninteraction over BERT. SIGIR .\nKiela, D., M. Bartolo, Y . Nie,\nD. Kaushik, A. Geiger, Z. Wu,\nB. Vidgen, G. Prasad, A. Singh,\nP. Ringshia, et al. 2021. Dynabench:\nRethinking benchmarking in nlp.\nNAACL HLT .\nKiela, D. and S. Clark. 2014. A system-\natic study of semantic vector space\nmodel parameters. EACL 2nd Work-\nshop on Continuous Vector Space\nModels and their Compositionality\n(CVSC) .\nKim, E. 2019. Optimize com-\nputational ef\ufb01ciency of skip-\ngram with negative sampling.\nhttps://aegis4048.github.\nio/optimize_computational_\nefficiency_of_skip-gram_\nwith_negative_sampling .\nKim, S. M. and E. H. Hovy. 2004. De-\ntermining the sentiment of opinions.\nCOLING .\nKing, S. 2020. From African Amer-\nican Vernacular English to African\nAmerican Language: Rethinking\nthe study of race and language in\nAfrican Americans\u2019 speech. Annual\nReview of Linguistics , 6:285\u2013300.\nKingma, D. and J. Ba. 2015. Adam: A\nmethod for stochastic optimization.\nICLR 2015 .\nKintsch, W. and T. A. Van Dijk. 1978.\nToward a model of text comprehen-\nsion and production. Psychological\nreview , 85(5):363\u2013394.\nKiperwasser, E. and Y . Goldberg. 2016.\nSimple and accurate dependency\nparsing using bidirectional LSTM\nfeature representations. TACL ,\n4:313\u2013327.\nKipper, K., H. T. Dang, and M. Palmer.\n2000. Class-based construction of a\nverb lexicon. AAAI .\nKiritchenko, S. and S. M. Mohammad.\n2017. Best-worst scaling more re-\nliable than rating scales: A case\nstudy on sentiment intensity annota-\ntion. ACL.\nKiritchenko, S. and S. M. Mohammad.\n2018. Examining gender and race\nbias in two hundred sentiment anal-\nysis systems. *SEM .\nKiss, T. and J. Strunk. 2006. Unsuper-\nvised multilingual sentence bound-\nary detection. Computational Lin-\nguistics , 32(4):485\u2013525.\nKitaev, N., S. Cao, and D. Klein.\n2019. Multilingual constituency\nparsing with self-attention and pre-\ntraining. ACL.\nKitaev, N. and D. Klein. 2018. Con-\nstituency parsing with a self-\nattentive encoder. ACL.Klatt, D. H. 1975. V oice onset time,\nfriction, and aspiration in word-\ninitial consonant clusters. Journal\nof Speech and Hearing Research ,\n18:686\u2013706.\nKlatt, D. H. 1977. Review of the ARPA\nspeech understanding project. JASA ,\n62(6):1345\u20131366.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 933,
      "token_count": 727,
      "chapter_title": ""
    }
  },
  {
    "content": "verb lexicon. AAAI .\nKiritchenko, S. and S. M. Mohammad.\n2017. Best-worst scaling more re-\nliable than rating scales: A case\nstudy on sentiment intensity annota-\ntion. ACL.\nKiritchenko, S. and S. M. Mohammad.\n2018. Examining gender and race\nbias in two hundred sentiment anal-\nysis systems. *SEM .\nKiss, T. and J. Strunk. 2006. Unsuper-\nvised multilingual sentence bound-\nary detection. Computational Lin-\nguistics , 32(4):485\u2013525.\nKitaev, N., S. Cao, and D. Klein.\n2019. Multilingual constituency\nparsing with self-attention and pre-\ntraining. ACL.\nKitaev, N. and D. Klein. 2018. Con-\nstituency parsing with a self-\nattentive encoder. ACL.Klatt, D. H. 1975. V oice onset time,\nfriction, and aspiration in word-\ninitial consonant clusters. Journal\nof Speech and Hearing Research ,\n18:686\u2013706.\nKlatt, D. H. 1977. Review of the ARPA\nspeech understanding project. JASA ,\n62(6):1345\u20131366.\nKlatt, D. H. 1982. The Klattalk text-to-\nspeech conversion system. ICASSP .\nKleene, S. C. 1951. Representation of\nevents in nerve nets and \ufb01nite au-\ntomata. Technical Report RM-704,\nRAND Corporation. RAND Re-\nsearch Memorandum.\nKleene, S. C. 1956. Representation of\nevents in nerve nets and \ufb01nite au-\ntomata. In C. Shannon and J. Mc-\nCarthy, eds, Automata Studies , 3\u201341.\nPrinceton University Press.\nKlein, S. and R. F. Simmons. 1963.\nA computational approach to gram-\nmatical coding of English words.\nJournal of the ACM , 10(3):334\u2013347.\nKnott, A. and R. Dale. 1994. Using\nlinguistic phenomena to motivate a\nset of coherence relations. Discourse\nProcesses , 18(1):35\u201362.\nKocijan, V ., A.-M. Cretu, O.-M.\nCamburu, Y . Yordanov, and\nT. Lukasiewicz. 2019. A surpris-\ningly robust trick for the Winograd\nSchema Challenge. ACL.\nKocmi, T., C. Federmann, R. Grund-\nkiewicz, M. Junczys-Dowmunt,\nH. Matsushita, and A. Menezes.\n2021. To ship or not to ship: An\nextensive evaluation of automatic\nmetrics for machine translation.\nArXiv.\nKoehn, P. 2005. Europarl: A parallel\ncorpus for statistical machine trans-\nlation. MT summit, vol. 5 .\nKoehn, P., H. Hoang, A. Birch,\nC. Callison-Burch, M. Federico,\nN. Bertoldi, B. Cowan, W. Shen,\nC. Moran, R. Zens, C. Dyer, O. Bo-\njar, A. Constantin, and E. Herbst.\n2006. Moses: Open source toolkit\nfor statistical machine translation.\nACL.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 934,
      "token_count": 736,
      "chapter_title": ""
    }
  },
  {
    "content": "linguistic phenomena to motivate a\nset of coherence relations. Discourse\nProcesses , 18(1):35\u201362.\nKocijan, V ., A.-M. Cretu, O.-M.\nCamburu, Y . Yordanov, and\nT. Lukasiewicz. 2019. A surpris-\ningly robust trick for the Winograd\nSchema Challenge. ACL.\nKocmi, T., C. Federmann, R. Grund-\nkiewicz, M. Junczys-Dowmunt,\nH. Matsushita, and A. Menezes.\n2021. To ship or not to ship: An\nextensive evaluation of automatic\nmetrics for machine translation.\nArXiv.\nKoehn, P. 2005. Europarl: A parallel\ncorpus for statistical machine trans-\nlation. MT summit, vol. 5 .\nKoehn, P., H. Hoang, A. Birch,\nC. Callison-Burch, M. Federico,\nN. Bertoldi, B. Cowan, W. Shen,\nC. Moran, R. Zens, C. Dyer, O. Bo-\njar, A. Constantin, and E. Herbst.\n2006. Moses: Open source toolkit\nfor statistical machine translation.\nACL.\nKoehn, P., F. J. Och, and D. Marcu.\n2003. Statistical phrase-based trans-\nlation. HLT-NAACL .\nKolhatkar, V ., A. Roussel, S. Dipper,\nand H. Zinsmeister. 2018. Anaphora\nwith non-nominal antecedents in\ncomputational linguistics: A sur-\nvey. Computational Linguistics ,\n44(3):547\u2013612.\nKreutzer, J., I. Caswell, L. Wang,\nA. Wahab, D. van Esch, N. Ulzii-\nOrshikh, A. Tapo, N. Subra-\nmani, A. Sokolov, C. Sikasote,\nM. Setyawan, S. Sarin, S. Samb,\nB. Sagot, C. Rivera, A. Rios, I. Pa-\npadimitriou, S. Osei, P. O. Suarez,I. Orife, K. Ogueji, A. N. Rubungo,\nT. Q. Nguyen, M. M \u00a8uller, A. M \u00a8uller,\nS. H. Muhammad, N. Muham-\nmad, A. Mnyakeni, J. Mirzakhalov,\nT. Matangira, C. Leong, N. Lawson,\nS. Kudugunta, Y . Jernite, M. Jenny,\nO. Firat, B. F. P. Dossou, S. Dlamini,\nN. de Silva, S. C \u00b8 abuk Ball\u0131, S. Bi-\nderman, A. Battisti, A. Baruwa,\nA. Bapna, P. Baljekar, I. A. Az-\nime, A. Awokoya, D. Ataman,\nO. Ahia, O. Ahia, S. Agrawal, and\nM. Adeyemi. 2022. Quality at a\nglance: An audit of web-crawled\nmultilingual datasets. TACL , 10:50\u2013\n72.\nKrovetz, R. 1993. Viewing morphology",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 935,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "padimitriou, S. Osei, P. O. Suarez,I. Orife, K. Ogueji, A. N. Rubungo,\nT. Q. Nguyen, M. M \u00a8uller, A. M \u00a8uller,\nS. H. Muhammad, N. Muham-\nmad, A. Mnyakeni, J. Mirzakhalov,\nT. Matangira, C. Leong, N. Lawson,\nS. Kudugunta, Y . Jernite, M. Jenny,\nO. Firat, B. F. P. Dossou, S. Dlamini,\nN. de Silva, S. C \u00b8 abuk Ball\u0131, S. Bi-\nderman, A. Battisti, A. Baruwa,\nA. Bapna, P. Baljekar, I. A. Az-\nime, A. Awokoya, D. Ataman,\nO. Ahia, O. Ahia, S. Agrawal, and\nM. Adeyemi. 2022. Quality at a\nglance: An audit of web-crawled\nmultilingual datasets. TACL , 10:50\u2013\n72.\nKrovetz, R. 1993. Viewing morphology\nas an inference process. SIGIR-93 .\nKruskal, J. B. 1983. An overview of se-\nquence comparison. In D. Sankoff\nand J. B. Kruskal, eds, Time\nWarps, String Edits, and Macro-\nmolecules: The Theory and Prac-\ntice of Sequence Comparison , 1\u201344.\nAddison-Wesley.\nKudo, T. 2018. Subword regularization:\nImproving neural network transla-\ntion models with multiple subword\ncandidates. ACL.\nKudo, T. and Y . Matsumoto. 2002.\nJapanese dependency analysis using\ncascaded chunking. CoNLL .\nKudo, T. and J. Richardson. 2018a.\nSentencePiece: A simple and lan-\nguage independent subword tok-\nenizer and detokenizer for neural\ntext processing. EMNLP .\nKudo, T. and J. Richardson. 2018b.\nSentencePiece: A simple and lan-\nguage independent subword tok-\nenizer and detokenizer for neural\ntext processing. EMNLP .\nKullback, S. and R. A. Leibler. 1951.\nOn information and suf\ufb01ciency.\nAnnals of Mathematical Statistics ,\n22:79\u201386.\nKulmizev, A., M. de Lhoneux,\nJ. Gontrum, E. Fano, and J. Nivre.\n2019. Deep contextualized word\nembeddings in transition-based and\ngraph-based dependency parsing\n- a tale of two parsers revisited.\nEMNLP .\nKumar, S. and W. Byrne. 2004. Min-\nimum Bayes-risk decoding for sta-\ntistical machine translation. HLT-\nNAACL .\nKummerfeld, J. K. and D. Klein. 2013.\nError-driven analysis of challenges\nin coreference resolution. EMNLP .\nKuno, S. 1965. The predictive ana-\nlyzer and a path elimination tech-\nnique. CACM , 8(7):453\u2013462.\nKupiec, J. 1992. Robust part-of-speech\ntagging using a hidden Markov",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 936,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "SentencePiece: A simple and lan-\nguage independent subword tok-\nenizer and detokenizer for neural\ntext processing. EMNLP .\nKullback, S. and R. A. Leibler. 1951.\nOn information and suf\ufb01ciency.\nAnnals of Mathematical Statistics ,\n22:79\u201386.\nKulmizev, A., M. de Lhoneux,\nJ. Gontrum, E. Fano, and J. Nivre.\n2019. Deep contextualized word\nembeddings in transition-based and\ngraph-based dependency parsing\n- a tale of two parsers revisited.\nEMNLP .\nKumar, S. and W. Byrne. 2004. Min-\nimum Bayes-risk decoding for sta-\ntistical machine translation. HLT-\nNAACL .\nKummerfeld, J. K. and D. Klein. 2013.\nError-driven analysis of challenges\nin coreference resolution. EMNLP .\nKuno, S. 1965. The predictive ana-\nlyzer and a path elimination tech-\nnique. CACM , 8(7):453\u2013462.\nKupiec, J. 1992. Robust part-of-speech\ntagging using a hidden Markov\nmodel. Computer Speech and Lan-\nguage , 6:225\u2013242.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 937,
      "token_count": 282,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 576",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 938,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "568 Bibliography\nKurita, K., N. Vyas, A. Pareek, A. W.\nBlack, and Y . Tsvetkov. 2019. Quan-\ntifying social biases in contextual\nword representations. 1st ACL Work-\nshop on Gender Bias for Natural\nLanguage Processing .\nKu\u02c7cera, H. and W. N. Francis. 1967.\nComputational Analysis of Present-\nDay American English . Brown Univ.\nPress.\nKwiatkowski, T., J. Palomaki, O. Red-\n\ufb01eld, M. Collins, A. Parikh, C. Al-\nberti, D. Epstein, I. Polosukhin,\nJ. Devlin, K. Lee, K. Toutanova,\nL. Jones, M. Kelcey, M.-W. Chang,\nA. M. Dai, J. Uszkoreit, Q. Le, and\nS. Petrov. 2019. Natural questions:\nA benchmark for question answer-\ning research. TACL , 7:452\u2013466.\nLafferty, J. D., A. McCallum, and\nF. C. N. Pereira. 2001. Conditional\nrandom \ufb01elds: Probabilistic mod-\nels for segmenting and labeling se-\nquence data. ICML .\nLai, A. and J. Tetreault. 2018. Dis-\ncourse coherence in the wild: A\ndataset, evaluation and methods.\nSIGDIAL .\nLake, B. M. and G. L. Murphy. 2021.\nWord meaning in minds and ma-\nchines. Psychological Review . In\npress.\nLakoff, G. 1965. On the Nature of Syn-\ntactic Irregularity . Ph.D. thesis, In-\ndiana University. Published as Irreg-\nularity in Syntax . Holt, Rinehart, and\nWinston, New York, 1970.\nLakoff, G. 1972. Structural complexity\nin fairy tales. In The Study of Man ,\n128\u201350. School of Social Sciences,\nUniversity of California, Irvine, CA.\nLakoff, G. and M. Johnson. 1980.\nMetaphors We Live By . University\nof Chicago Press, Chicago, IL.\nLample, G., M. Ballesteros, S. Subra-\nmanian, K. Kawakami, and C. Dyer.\n2016. Neural architectures for\nnamed entity recognition. NAACL\nHLT.\nLample, G. and A. Conneau. 2019.\nCross-lingual language model pre-\ntraining. NeurIPS , volume 32.\nLan, Z., M. Chen, S. Goodman,\nK. Gimpel, P. Sharma, and R. Sori-\ncut. 2020. ALBERT: A lite BERT\nfor self-supervised learning of lan-\nguage representations. ICLR .\nLandauer, T. K., ed. 1995. The Trou-\nble with Computers: Usefulness, Us-\nability, and Productivity . MIT Press.\nLandauer, T. K. and S. T. Dumais. 1997.\nA solution to Plato\u2019s problem: The\nLatent Semantic Analysis theory of\nacquisition, induction, and represen-\ntation of knowledge. Psychological\nReview , 104:211\u2013240.Landauer, T. K., D. Laham, B. Rehder,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 939,
      "token_count": 740,
      "chapter_title": ""
    }
  },
  {
    "content": "Metaphors We Live By . University\nof Chicago Press, Chicago, IL.\nLample, G., M. Ballesteros, S. Subra-\nmanian, K. Kawakami, and C. Dyer.\n2016. Neural architectures for\nnamed entity recognition. NAACL\nHLT.\nLample, G. and A. Conneau. 2019.\nCross-lingual language model pre-\ntraining. NeurIPS , volume 32.\nLan, Z., M. Chen, S. Goodman,\nK. Gimpel, P. Sharma, and R. Sori-\ncut. 2020. ALBERT: A lite BERT\nfor self-supervised learning of lan-\nguage representations. ICLR .\nLandauer, T. K., ed. 1995. The Trou-\nble with Computers: Usefulness, Us-\nability, and Productivity . MIT Press.\nLandauer, T. K. and S. T. Dumais. 1997.\nA solution to Plato\u2019s problem: The\nLatent Semantic Analysis theory of\nacquisition, induction, and represen-\ntation of knowledge. Psychological\nReview , 104:211\u2013240.Landauer, T. K., D. Laham, B. Rehder,\nand M. E. Schreiner. 1997. How\nwell can passage meaning be derived\nwithout using word order? A com-\nparison of Latent Semantic Analysis\nand humans. COGSCI .\nLang, J. and M. Lapata. 2014.\nSimilarity-driven semantic role in-\nduction via graph partitioning. Com-\nputational Linguistics , 40(3):633\u2013\n669.\nLang, K. J., A. H. Waibel, and G. E.\nHinton. 1990. A time-delay neu-\nral network architecture for isolated\nword recognition. Neural networks ,\n3(1):23\u201343.\nLapata, M. 2003. Probabilistic text\nstructuring: Experiments with sen-\ntence ordering. ACL.\nLapesa, G. and S. Evert. 2014. A large\nscale evaluation of distributional se-\nmantic models: Parameters, interac-\ntions and model selection. TACL ,\n2:531\u2013545.\nLappin, S. and H. Leass. 1994. An algo-\nrithm for pronominal anaphora res-\nolution. Computational Linguistics ,\n20(4):535\u2013561.\nLarsson, S. and D. R. Traum. 2000. In-\nformation state and dialogue man-\nagement in the trindi dialogue move\nengine toolkit. Natural Language\nEngineering , 6(323-340):97\u2013114.\nLascarides, A. and N. Asher. 1993.\nTemporal interpretation, discourse\nrelations, and common sense entail-\nment. Linguistics and Philosophy ,\n16(5):437\u2013493.\nLawrence, W. 1953. The synthesis of\nspeech from signals which have a\nlow information rate. In W. Jackson,\ned., Communication Theory , 460\u2013\n469. Butterworth.\nLDC. 1998. LDC Catalog: Hub4\nproject . University of Penn-\nsylvania. www.ldc.upenn.edu/\nCatalog/LDC98S71.html .\nLeCun, Y ., B. Boser, J. S. Denker,\nD. Henderson, R. E. Howard,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 940,
      "token_count": 735,
      "chapter_title": ""
    }
  },
  {
    "content": "mantic models: Parameters, interac-\ntions and model selection. TACL ,\n2:531\u2013545.\nLappin, S. and H. Leass. 1994. An algo-\nrithm for pronominal anaphora res-\nolution. Computational Linguistics ,\n20(4):535\u2013561.\nLarsson, S. and D. R. Traum. 2000. In-\nformation state and dialogue man-\nagement in the trindi dialogue move\nengine toolkit. Natural Language\nEngineering , 6(323-340):97\u2013114.\nLascarides, A. and N. Asher. 1993.\nTemporal interpretation, discourse\nrelations, and common sense entail-\nment. Linguistics and Philosophy ,\n16(5):437\u2013493.\nLawrence, W. 1953. The synthesis of\nspeech from signals which have a\nlow information rate. In W. Jackson,\ned., Communication Theory , 460\u2013\n469. Butterworth.\nLDC. 1998. LDC Catalog: Hub4\nproject . University of Penn-\nsylvania. www.ldc.upenn.edu/\nCatalog/LDC98S71.html .\nLeCun, Y ., B. Boser, J. S. Denker,\nD. Henderson, R. E. Howard,\nW. Hubbard, and L. D. Jackel. 1989.\nBackpropagation applied to hand-\nwritten zip code recognition. Neural\ncomputation , 1(4):541\u2013551.\nLee, D. D. and H. S. Seung. 1999.\nLearning the parts of objects by non-\nnegative matrix factorization. Na-\nture, 401(6755):788\u2013791.\nLee, H., A. Chang, Y . Peirsman,\nN. Chambers, M. Surdeanu, and\nD. Jurafsky. 2013. Determin-\nistic coreference resolution based\non entity-centric, precision-ranked\nrules. Computational Linguistics ,\n39(4):885\u2013916.\nLee, H., Y . Peirsman, A. Chang,\nN. Chambers, M. Surdeanu, andD. Jurafsky. 2011. Stanford\u2019s multi-\npass sieve coreference resolution\nsystem at the CoNLL-2011 shared\ntask. CoNLL .\nLee, H., M. Surdeanu, and D. Juraf-\nsky. 2017a. A scaffolding approach\nto coreference resolution integrat-\ning statistical and rule-based mod-\nels.Natural Language Engineering ,\n23(5):733\u2013762.\nLee, K., M.-W. Chang, and\nK. Toutanova. 2019. Latent re-\ntrieval for weakly supervised open\ndomain question answering. ACL.\nLee, K., L. He, M. Lewis, and L. Zettle-\nmoyer. 2017b. End-to-end neural\ncoreference resolution. EMNLP .\nLee, K., L. He, and L. Zettlemoyer.\n2018. Higher-order coreference\nresolution with coarse-to-\ufb01ne infer-\nence. NAACL HLT .\nLehnert, W. G., C. Cardie, D. Fisher,\nE. Riloff, and R. Williams. 1991.\nDescription of the CIRCUS system\nas used for MUC-3. MUC-3 .\nLemon, O., K. Georgila, J. Henderson,\nand M. Stuttle. 2006. An ISU di-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 941,
      "token_count": 748,
      "chapter_title": ""
    }
  },
  {
    "content": "pass sieve coreference resolution\nsystem at the CoNLL-2011 shared\ntask. CoNLL .\nLee, H., M. Surdeanu, and D. Juraf-\nsky. 2017a. A scaffolding approach\nto coreference resolution integrat-\ning statistical and rule-based mod-\nels.Natural Language Engineering ,\n23(5):733\u2013762.\nLee, K., M.-W. Chang, and\nK. Toutanova. 2019. Latent re-\ntrieval for weakly supervised open\ndomain question answering. ACL.\nLee, K., L. He, M. Lewis, and L. Zettle-\nmoyer. 2017b. End-to-end neural\ncoreference resolution. EMNLP .\nLee, K., L. He, and L. Zettlemoyer.\n2018. Higher-order coreference\nresolution with coarse-to-\ufb01ne infer-\nence. NAACL HLT .\nLehnert, W. G., C. Cardie, D. Fisher,\nE. Riloff, and R. Williams. 1991.\nDescription of the CIRCUS system\nas used for MUC-3. MUC-3 .\nLemon, O., K. Georgila, J. Henderson,\nand M. Stuttle. 2006. An ISU di-\nalogue system exhibiting reinforce-\nment learning of dialogue policies:\nGeneric slot-\ufb01lling in the TALK in-\ncar system. EACL .\nLevenshtein, V . I. 1966. Binary codes\ncapable of correcting deletions, in-\nsertions, and reversals. Cybernetics\nand Control Theory , 10(8):707\u2013710.\nOriginal in Doklady Akademii Nauk\nSSSR 163(4): 845\u2013848 (1965).\nLevesque, H. 2011. The Winograd\nSchema Challenge. Logical Formal-\nizations of Commonsense Reason-\ning \u2014 Papers from the AAAI 2011\nSpring Symposium (SS-11-06) .\nLevesque, H., E. Davis, and L. Morgen-\nstern. 2012. The Winograd Schema\nChallenge. KR-12 .\nLevesque, H. J., P. R. Cohen, and\nJ. H. T. Nunes. 1990. On acting to-\ngether. AAAI . Morgan Kaufmann.\nLevin, B. 1977. Mapping sentences to\ncase frames. Technical Report 167,\nMIT AI Laboratory. AI Working Pa-\nper 143.\nLevin, B. 1993. English Verb Classes\nand Alternations: A Preliminary In-\nvestigation . University of Chicago\nPress.\nLevin, B. and M. Rappaport Hovav.\n2005. Argument Realization . Cam-\nbridge University Press.\nLevin, E., R. Pieraccini, and W. Eckert.\n2000. A stochastic model of human-\nmachine interaction for learning dia-\nlog strategies. IEEE Transactions on\nSpeech and Audio Processing , 8:11\u2013\n23.\nLevinson, S. C. 1983. Conversational\nAnalysis , chapter 6. Cambridge Uni-\nversity Press.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 942,
      "token_count": 683,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 577",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 943,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Bibliography 569\nLevow, G.-A. 1998. Characterizing\nand recognizing spoken corrections\nin human-computer dialogue. COL-\nING/ACL .\nLevy, O. and Y . Goldberg. 2014a.\nDependency-based word embed-\ndings. ACL.\nLevy, O. and Y . Goldberg. 2014b. Lin-\nguistic regularities in sparse and ex-\nplicit word representations. CoNLL .\nLevy, O. and Y . Goldberg. 2014c. Neu-\nral word embedding as implicit ma-\ntrix factorization. NeurIPS .\nLevy, O., Y . Goldberg, and I. Da-\ngan. 2015. Improving distributional\nsimilarity with lessons learned from\nword embeddings. TACL , 3:211\u2013\n225.\nLi, B. Z., S. Min, S. Iyer, Y . Mehdad,\nand W.-t. Yih. 2020. Ef\ufb01cient one-\npass end-to-end entity linking for\nquestions. EMNLP .\nLi, J., X. Chen, E. H. Hovy, and D. Ju-\nrafsky. 2015. Visualizing and un-\nderstanding neural models in NLP.\nNAACL HLT .\nLi, J. and D. Jurafsky. 2017. Neu-\nral net models of open-domain dis-\ncourse coherence. EMNLP .\nLi, J., R. Li, and E. H. Hovy. 2014.\nRecursive deep models for discourse\nparsing. EMNLP .\nLi, J., W. Monroe, A. Ritter, D. Juraf-\nsky, M. Galley, and J. Gao. 2016a.\nDeep reinforcement learning for di-\nalogue generation. EMNLP .\nLi, M., J. Weston, and S. Roller. 2019a.\nAcute-eval: Improved dialogue eval-\nuation with optimized questions and\nmulti-turn comparisons. NeurIPS19\nWorkshop on Conversational AI .\nLi, Q., T. Li, and B. Chang. 2016b.\nDiscourse parsing with attention-\nbased hierarchical neural networks.\nEMNLP .\nLi, X., Y . Meng, X. Sun, Q. Han,\nA. Yuan, and J. Li. 2019b. Is\nword segmentation necessary for\ndeep learning of Chinese representa-\ntions? ACL.\nLiang, P., R. Bommasani, T. Lee,\nD. Tsipras, D. Soylu, M. Yasunaga,\nY . Zhang, D. Narayanan, Y . Wu,\nA. Kumar, B. Newman, B. Yuan,\nB. Yan, C. Zhang, C. Cosgrove,\nC. D. Manning, C. R \u00b4e, D. Acosta-\nNavas, D. A. Hudson, E. Zelikman,\nE. Durmus, F. Ladhak, F. Rong,\nH. Ren, H. Yao, J. Wang, K. San-\nthanam, L. Orr, L. Zheng, M. Yuk-\nsekgonul, M. Suzgun, N. Kim,\nN. Guha, N. Chatterji, O. Khattab,\nP. Henderson, Q. Huang, R. Chi,\nS. M. Xie, S. Santurkar, S. Ganguli,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 944,
      "token_count": 735,
      "chapter_title": ""
    }
  },
  {
    "content": "Discourse parsing with attention-\nbased hierarchical neural networks.\nEMNLP .\nLi, X., Y . Meng, X. Sun, Q. Han,\nA. Yuan, and J. Li. 2019b. Is\nword segmentation necessary for\ndeep learning of Chinese representa-\ntions? ACL.\nLiang, P., R. Bommasani, T. Lee,\nD. Tsipras, D. Soylu, M. Yasunaga,\nY . Zhang, D. Narayanan, Y . Wu,\nA. Kumar, B. Newman, B. Yuan,\nB. Yan, C. Zhang, C. Cosgrove,\nC. D. Manning, C. R \u00b4e, D. Acosta-\nNavas, D. A. Hudson, E. Zelikman,\nE. Durmus, F. Ladhak, F. Rong,\nH. Ren, H. Yao, J. Wang, K. San-\nthanam, L. Orr, L. Zheng, M. Yuk-\nsekgonul, M. Suzgun, N. Kim,\nN. Guha, N. Chatterji, O. Khattab,\nP. Henderson, Q. Huang, R. Chi,\nS. M. Xie, S. Santurkar, S. Ganguli,\nT. Hashimoto, T. Icard, T. Zhang,\nV . Chaudhary, W. Wang, X. Li,\nY . Mai, Y . Zhang, and Y . Koreeda.2023. Holistic evaluation of lan-\nguage models. Transactions on Ma-\nchine Learning Research .\nLin, C.-Y . 2004. ROUGE: A pack-\nage for automatic evaluation of sum-\nmaries. ACL 2004 Workshop on Text\nSummarization Branches Out .\nLin, D. 2003. Dependency-based eval-\nuation of minipar. Workshop on the\nEvaluation of Parsing Systems .\nLin, Y ., J.-B. Michel, E. Aiden Lieber-\nman, J. Orwant, W. Brockman, and\nS. Petrov. 2012a. Syntactic annota-\ntions for the Google books NGram\ncorpus. ACL.\nLin, Y ., J.-B. Michel, E. Lieber-\nman Aiden, J. Orwant, W. Brock-\nman, and S. Petrov. 2012b. Syntac-\ntic annotations for the Google Books\nNGram corpus. ACL.\nLin, Z., A. Madotto, J. Shin, P. Xu, and\nP. Fung. 2019. MoEL: Mixture of\nempathetic listeners. EMNLP .\nLin, Z., M.-Y . Kan, and H. T. Ng. 2009.\nRecognizing implicit discourse rela-\ntions in the Penn Discourse Tree-\nbank. EMNLP .\nLin, Z., H. T. Ng, and M.-Y . Kan. 2011.\nAutomatically evaluating text coher-\nence using discourse relations. ACL.\nLin, Z., H. T. Ng, and M.-Y . Kan. 2014.\nA pdtb-styled end-to-end discourse\nparser. Natural Language Engineer-\ning, 20(2):151\u2013184.\nLing, W., C. Dyer, A. W. Black,\nI. Trancoso, R. Fermandez, S. Amir,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 945,
      "token_count": 732,
      "chapter_title": ""
    }
  },
  {
    "content": "tions for the Google books NGram\ncorpus. ACL.\nLin, Y ., J.-B. Michel, E. Lieber-\nman Aiden, J. Orwant, W. Brock-\nman, and S. Petrov. 2012b. Syntac-\ntic annotations for the Google Books\nNGram corpus. ACL.\nLin, Z., A. Madotto, J. Shin, P. Xu, and\nP. Fung. 2019. MoEL: Mixture of\nempathetic listeners. EMNLP .\nLin, Z., M.-Y . Kan, and H. T. Ng. 2009.\nRecognizing implicit discourse rela-\ntions in the Penn Discourse Tree-\nbank. EMNLP .\nLin, Z., H. T. Ng, and M.-Y . Kan. 2011.\nAutomatically evaluating text coher-\nence using discourse relations. ACL.\nLin, Z., H. T. Ng, and M.-Y . Kan. 2014.\nA pdtb-styled end-to-end discourse\nparser. Natural Language Engineer-\ning, 20(2):151\u2013184.\nLing, W., C. Dyer, A. W. Black,\nI. Trancoso, R. Fermandez, S. Amir,\nL. Marujo, and T. Lu \u00b4\u0131s. 2015. Find-\ning function in form: Compositional\ncharacter models for open vocabu-\nlary word representation. EMNLP .\nLinzen, T. 2016. Issues in evaluating se-\nmantic spaces using word analogies.\n1st Workshop on Evaluating Vector-\nSpace Representations for NLP .\nLison, P. and J. Tiedemann. 2016.\nOpensubtitles2016: Extracting large\nparallel corpora from movie and tv\nsubtitles. LREC .\nLitman, D. J. 1985. Plan Recognition\nand Discourse Analysis: An Inte-\ngrated Approach for Understanding\nDialogues . Ph.D. thesis, University\nof Rochester, Rochester, NY .\nLitman, D. J. and J. Allen. 1987. A plan\nrecognition model for subdialogues\nin conversation. Cognitive Science ,\n11:163\u2013200.\nLitman, D. J., M. A. Walker, and\nM. Kearns. 1999. Automatic detec-\ntion of poor speech recognition at\nthe dialogue level. ACL.\nLiu, B. and L. Zhang. 2012. A sur-\nvey of opinion mining and sentiment\nanalysis. In C. C. Aggarwal and\nC. Zhai, eds, Mining text data , 415\u2013\n464. Springer.Liu, H., J. Dacon, W. Fan, H. Liu,\nZ. Liu, and J. Tang. 2020. Does gen-\nder matter? Towards fairness in dia-\nlogue systems. COLING .\nLiu, J., S. Min, L. Zettlemoyer, Y . Choi,\nand H. Hajishirzi. 2024. In\ufb01ni-gram:\nScaling unbounded n-gram language\nmodels to a trillion tokens. ArXiv\npreprint.\nLiu, Y ., C. Sun, L. Lin, and X. Wang.\n2016. Learning natural language\ninference using bidirectional LSTM\nmodel and inner-attention. ArXiv.\nLiu, Y ., P. Fung, Y . Yang, C. Cieri,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 946,
      "token_count": 748,
      "chapter_title": ""
    }
  },
  {
    "content": "in conversation. Cognitive Science ,\n11:163\u2013200.\nLitman, D. J., M. A. Walker, and\nM. Kearns. 1999. Automatic detec-\ntion of poor speech recognition at\nthe dialogue level. ACL.\nLiu, B. and L. Zhang. 2012. A sur-\nvey of opinion mining and sentiment\nanalysis. In C. C. Aggarwal and\nC. Zhai, eds, Mining text data , 415\u2013\n464. Springer.Liu, H., J. Dacon, W. Fan, H. Liu,\nZ. Liu, and J. Tang. 2020. Does gen-\nder matter? Towards fairness in dia-\nlogue systems. COLING .\nLiu, J., S. Min, L. Zettlemoyer, Y . Choi,\nand H. Hajishirzi. 2024. In\ufb01ni-gram:\nScaling unbounded n-gram language\nmodels to a trillion tokens. ArXiv\npreprint.\nLiu, Y ., C. Sun, L. Lin, and X. Wang.\n2016. Learning natural language\ninference using bidirectional LSTM\nmodel and inner-attention. ArXiv.\nLiu, Y ., P. Fung, Y . Yang, C. Cieri,\nS. Huang, and D. Graff. 2006.\nHKUST/MTS: A very large scale\nMandarin telephone speech corpus.\nInternational Conference on Chi-\nnese Spoken Language Processing .\nLiu, Y ., M. Ott, N. Goyal, J. Du,\nM. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and\nV . Stoyanov. 2019. RoBERTa:\nA robustly optimized BERT pre-\ntraining approach. ArXiv preprint\narXiv:1907.11692.\nLlama Team. 2024. The llama 3 herd of\nmodels.\nLochbaum, K. E., B. J. Grosz, and\nC. L. Sidner. 2000. Discourse struc-\nture and intention recognition. In\nR. Dale, H. Moisl, and H. L. Somers,\neds,Handbook of Natural Language\nProcessing . Marcel Dekker.\nLogeswaran, L., H. Lee, and D. Radev.\n2018. Sentence ordering and coher-\nence modeling using recurrent neu-\nral networks. AAAI .\nLongpre, S., L. Hou, T. Vu, A. Webson,\nH. W. Chung, Y . Tay, D. Zhou, Q. V .\nLe, B. Zoph, J. Wei, and A. Roberts.\n2023. The Flan collection: Design-\ning data and methods for effective\ninstruction tuning. ICML .\nLongpre, S., R. Mahari, A. Lee,\nC. Lund, H. Oderinwale, W. Bran-\nnon, N. Saxena, N. Obeng-Marnu,\nT. South, C. Hunter, et al. 2024a.\nConsent in crisis: The rapid decline\nof the ai data commons. ArXiv\npreprint.\nLongpre, S., G. Yauney, E. Reif, K. Lee,\nA. Roberts, B. Zoph, D. Zhou,\nJ. Wei, K. Robinson, D. Mimno, and",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 947,
      "token_count": 741,
      "chapter_title": ""
    }
  },
  {
    "content": "R. Dale, H. Moisl, and H. L. Somers,\neds,Handbook of Natural Language\nProcessing . Marcel Dekker.\nLogeswaran, L., H. Lee, and D. Radev.\n2018. Sentence ordering and coher-\nence modeling using recurrent neu-\nral networks. AAAI .\nLongpre, S., L. Hou, T. Vu, A. Webson,\nH. W. Chung, Y . Tay, D. Zhou, Q. V .\nLe, B. Zoph, J. Wei, and A. Roberts.\n2023. The Flan collection: Design-\ning data and methods for effective\ninstruction tuning. ICML .\nLongpre, S., R. Mahari, A. Lee,\nC. Lund, H. Oderinwale, W. Bran-\nnon, N. Saxena, N. Obeng-Marnu,\nT. South, C. Hunter, et al. 2024a.\nConsent in crisis: The rapid decline\nof the ai data commons. ArXiv\npreprint.\nLongpre, S., G. Yauney, E. Reif, K. Lee,\nA. Roberts, B. Zoph, D. Zhou,\nJ. Wei, K. Robinson, D. Mimno, and\nD. Ippolito. 2024b. A pretrainer\u2019s\nguide to training data: Measuring\nthe effects of data age, domain cov-\nerage, quality, & toxicity. NAACL\nHLT.\nLouis, A. and A. Nenkova. 2012. A\ncoherence model based on syntactic\npatterns. EMNLP .\nLoureiro, D. and A. Jorge. 2019.\nLanguage modelling makes sense:\nPropagating representations through\nWordNet for full-coverage word\nsense disambiguation. ACL.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 948,
      "token_count": 396,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 578",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 949,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "570 Bibliography\nLouviere, J. J., T. N. Flynn, and A. A. J.\nMarley. 2015. Best-worst scaling:\nTheory, methods and applications .\nCambridge University Press.\nLovins, J. B. 1968. Development of\na stemming algorithm. Mechanical\nTranslation and Computational Lin-\nguistics , 11(1\u20132):9\u201313.\nLowerre, B. T. 1976. The Harpy Speech\nRecognition System . Ph.D. thesis,\nCarnegie Mellon University, Pitts-\nburgh, PA.\nLuhn, H. P. 1957. A statistical ap-\nproach to the mechanized encoding\nand searching of literary informa-\ntion. IBM Journal of Research and\nDevelopment , 1(4):309\u2013317.\nLui, M. and T. Baldwin. 2011. Cross-\ndomain feature selection for lan-\nguage identi\ufb01cation. IJCNLP .\nLui, M. and T. Baldwin. 2012.\nlangid.py : An off-the-shelf lan-\nguage identi\ufb01cation tool. ACL.\nLukasik, M., B. Dadachev, K. Papineni,\nand G. Sim \u02dcoes. 2020. Text seg-\nmentation by cross segment atten-\ntion. EMNLP .\nLuo, X. 2005. On coreference resolu-\ntion performance metrics. EMNLP .\nLuo, X. and S. Pradhan. 2016. Eval-\nuation metrics. In M. Poesio,\nR. Stuckardt, and Y . Versley, eds,\nAnaphora resolution: Algorithms,\nresources, and applications , 141\u2013\n163. Springer.\nLuo, X., S. Pradhan, M. Recasens, and\nE. H. Hovy. 2014. An extension of\nBLANC to system mentions. ACL.\nMa, X. and E. H. Hovy. 2016. End-\nto-end sequence labeling via bi-\ndirectional LSTM-CNNs-CRF.\nACL.\nMaas, A., Z. Xie, D. Jurafsky, and A. Y .\nNg. 2015. Lexicon-free conversa-\ntional speech recognition with neu-\nral networks. NAACL HLT .\nMaas, A. L., A. Y . Hannun, and A. Y .\nNg. 2013. Recti\ufb01er nonlineari-\nties improve neural network acoustic\nmodels. ICML .\nMaas, A. L., P. Qi, Z. Xie, A. Y . Han-\nnun, C. T. Lengerich, D. Jurafsky,\nand A. Y . Ng. 2017. Building dnn\nacoustic models for large vocabu-\nlary speech recognition. Computer\nSpeech & Language , 41:195\u2013213.\nMagerman, D. M. 1995. Statisti-\ncal decision-tree models for parsing.\nACL.\nMairesse, F. and M. A. Walker. 2008.\nTrainable generation of big-\ufb01ve per-\nsonality styles through data-driven\nparameter estimation. ACL.\nMann, W. C. and S. A. Thompson.\n1987. Rhetorical structure theory: Atheory of text organization. Techni-\ncal Report RS-87-190, Information\nSciences Institute.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 950,
      "token_count": 739,
      "chapter_title": ""
    }
  },
  {
    "content": "Ng. 2015. Lexicon-free conversa-\ntional speech recognition with neu-\nral networks. NAACL HLT .\nMaas, A. L., A. Y . Hannun, and A. Y .\nNg. 2013. Recti\ufb01er nonlineari-\nties improve neural network acoustic\nmodels. ICML .\nMaas, A. L., P. Qi, Z. Xie, A. Y . Han-\nnun, C. T. Lengerich, D. Jurafsky,\nand A. Y . Ng. 2017. Building dnn\nacoustic models for large vocabu-\nlary speech recognition. Computer\nSpeech & Language , 41:195\u2013213.\nMagerman, D. M. 1995. Statisti-\ncal decision-tree models for parsing.\nACL.\nMairesse, F. and M. A. Walker. 2008.\nTrainable generation of big-\ufb01ve per-\nsonality styles through data-driven\nparameter estimation. ACL.\nMann, W. C. and S. A. Thompson.\n1987. Rhetorical structure theory: Atheory of text organization. Techni-\ncal Report RS-87-190, Information\nSciences Institute.\nManning, C. D. 2011. Part-of-speech\ntagging from 97% to 100%: Is it\ntime for some linguistics? CICLing\n2011 .\nManning, C. D., P. Raghavan, and\nH. Sch \u00a8utze. 2008. Introduction to In-\nformation Retrieval . Cambridge.\nManning, C. D., M. Surdeanu, J. Bauer,\nJ. Finkel, S. Bethard, and D. Mc-\nClosky. 2014. The Stanford\nCoreNLP natural language process-\ning toolkit. ACL.\nMarcu, D. 1997. The rhetorical parsing\nof natural language texts. ACL.\nMarcu, D. 1999. A decision-based ap-\nproach to rhetorical parsing. ACL.\nMarcu, D. 2000a. The rhetorical pars-\ning of unrestricted texts: A surface-\nbased approach. Computational Lin-\nguistics , 26(3):395\u2013448.\nMarcu, D., ed. 2000b. The Theory and\nPractice of Discourse Parsing and\nSummarization . MIT Press.\nMarcu, D. and A. Echihabi. 2002. An\nunsupervised approach to recogniz-\ning discourse relations. ACL.\nMarcu, D. and W. Wong. 2002.\nA phrase-based, joint probability\nmodel for statistical machine trans-\nlation. EMNLP .\nMarcus, M. P. 1980. A Theory of Syn-\ntactic Recognition for Natural Lan-\nguage . MIT Press.\nMarcus, M. P., B. Santorini, and M. A.\nMarcinkiewicz. 1993. Building a\nlarge annotated corpus of English:\nThe Penn treebank. Computational\nLinguistics , 19(2):313\u2013330.\nMarie, B., A. Fujita, and R. Rubino.\n2021. Scienti\ufb01c credibility of ma-\nchine translation research: A meta-\nevaluation of 769 papers. ACL.\nMarkov, A. A. 1913. Essai d\u2019une\nrecherche statistique sur le texte du\nroman \u201cEugene Onegin\u201d illustrant la",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 951,
      "token_count": 739,
      "chapter_title": ""
    }
  },
  {
    "content": "based approach. Computational Lin-\nguistics , 26(3):395\u2013448.\nMarcu, D., ed. 2000b. The Theory and\nPractice of Discourse Parsing and\nSummarization . MIT Press.\nMarcu, D. and A. Echihabi. 2002. An\nunsupervised approach to recogniz-\ning discourse relations. ACL.\nMarcu, D. and W. Wong. 2002.\nA phrase-based, joint probability\nmodel for statistical machine trans-\nlation. EMNLP .\nMarcus, M. P. 1980. A Theory of Syn-\ntactic Recognition for Natural Lan-\nguage . MIT Press.\nMarcus, M. P., B. Santorini, and M. A.\nMarcinkiewicz. 1993. Building a\nlarge annotated corpus of English:\nThe Penn treebank. Computational\nLinguistics , 19(2):313\u2013330.\nMarie, B., A. Fujita, and R. Rubino.\n2021. Scienti\ufb01c credibility of ma-\nchine translation research: A meta-\nevaluation of 769 papers. ACL.\nMarkov, A. A. 1913. Essai d\u2019une\nrecherche statistique sur le texte du\nroman \u201cEugene Onegin\u201d illustrant la\nliaison des epreuve en chain (\u2018Ex-\nample of a statistical investigation\nof the text of \u201cEugene Onegin\u201d il-\nlustrating the dependence between\nsamples in chain\u2019). Izvistia Impera-\ntorskoi Akademii Nauk (Bulletin de\nl\u2019Acad \u00b4emie Imp \u00b4eriale des Sciences\nde St.-P \u00b4etersbourg) , 7:153\u2013162.\nde Marneffe, M.-C., T. Dozat, N. Sil-\nveira, K. Haverinen, F. Ginter,\nJ. Nivre, and C. D. Manning. 2014.\nUniversal Stanford dependencies: A\ncross-linguistic typology. LREC .\nde Marneffe, M.-C., B. MacCartney,\nand C. D. Manning. 2006. Gener-\nating typed dependency parses from\nphrase structure parses. LREC .de Marneffe, M.-C. and C. D. Man-\nning. 2008. The Stanford typed de-\npendencies representation. COLING\nWorkshop on Cross-Framework and\nCross-Domain Parser Evaluation .\nde Marneffe, M.-C., C. D. Manning,\nJ. Nivre, and D. Zeman. 2021. Uni-\nversal Dependencies. Computa-\ntional Linguistics , 47(2):255\u2013308.\nde Marneffe, M.-C., M. Recasens, and\nC. Potts. 2015. Modeling the lifes-\npan of discourse entities with ap-\nplication to coreference resolution.\nJAIR , 52:445\u2013475.\nMaron, M. E. 1961. Automatic index-\ning: an experimental inquiry. Jour-\nnal of the ACM , 8(3):404\u2013417.\nM`arquez, L., X. Carreras, K. C.\nLitkowski, and S. Stevenson. 2008.\nSemantic role labeling: An introduc-\ntion to the special issue. Computa-\ntional linguistics , 34(2):145\u2013159.\nMarshall, I. 1983. Choice of grammat-\nical word-class without global syn-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 952,
      "token_count": 751,
      "chapter_title": ""
    }
  },
  {
    "content": "phrase structure parses. LREC .de Marneffe, M.-C. and C. D. Man-\nning. 2008. The Stanford typed de-\npendencies representation. COLING\nWorkshop on Cross-Framework and\nCross-Domain Parser Evaluation .\nde Marneffe, M.-C., C. D. Manning,\nJ. Nivre, and D. Zeman. 2021. Uni-\nversal Dependencies. Computa-\ntional Linguistics , 47(2):255\u2013308.\nde Marneffe, M.-C., M. Recasens, and\nC. Potts. 2015. Modeling the lifes-\npan of discourse entities with ap-\nplication to coreference resolution.\nJAIR , 52:445\u2013475.\nMaron, M. E. 1961. Automatic index-\ning: an experimental inquiry. Jour-\nnal of the ACM , 8(3):404\u2013417.\nM`arquez, L., X. Carreras, K. C.\nLitkowski, and S. Stevenson. 2008.\nSemantic role labeling: An introduc-\ntion to the special issue. Computa-\ntional linguistics , 34(2):145\u2013159.\nMarshall, I. 1983. Choice of grammat-\nical word-class without global syn-\ntactic analysis: Tagging words in the\nLOB corpus. Computers and the Hu-\nmanities , 17:139\u2013150.\nMarshall, I. 1987. Tag selection using\nprobabilistic methods. In R. Garside,\nG. Leech, and G. Sampson, eds, The\nComputational Analysis of English ,\n42\u201356. Longman.\nMartschat, S. and M. Strube. 2014. Re-\ncall error analysis for coreference\nresolution. EMNLP .\nMartschat, S. and M. Strube. 2015. La-\ntent structures for coreference reso-\nlution. TACL , 3:405\u2013418.\nMathis, D. A. and M. C. Mozer. 1995.\nOn the computational utility of con-\nsciousness. NeurIPS . MIT Press.\nMcCallum, A., D. Freitag, and F. C. N.\nPereira. 2000. Maximum entropy\nMarkov models for information ex-\ntraction and segmentation. ICML .\nMcCallum, A. and W. Li. 2003. Early\nresults for named entity recogni-\ntion with conditional random \ufb01elds,\nfeature induction and web-enhanced\nlexicons. CoNLL .\nMcCallum, A. and K. Nigam. 1998.\nA comparison of event models\nfor naive bayes text classi\ufb01cation.\nAAAI/ICML-98 Workshop on Learn-\ning for Text Categorization .\nMcCarthy, J. F. and W. G. Lehnert.\n1995. Using decision trees for coref-\nerence resolution. IJCAI-95 .\nMcClelland, J. L. and J. L. Elman.\n1986. The TRACE model of speech\nperception. Cognitive Psychology ,\n18:1\u201386.\nMcClelland, J. L. and D. E. Rumel-\nhart, eds. 1986. Parallel Dis-\ntributed Processing: Explorations\nin the Microstructure of Cognition ,\nvolume 2: Psychological and Bio-\nlogical Models . MIT Press.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 953,
      "token_count": 739,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 579",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 954,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Bibliography 571\nMcCulloch, W. S. and W. Pitts. 1943. A\nlogical calculus of ideas immanent\nin nervous activity. Bulletin of Math-\nematical Biophysics , 5:115\u2013133.\nMcDonald, R., K. Crammer, and\nF. C. N. Pereira. 2005a. Online\nlarge-margin training of dependency\nparsers. ACL.\nMcDonald, R. and J. Nivre. 2011. An-\nalyzing and integrating dependency\nparsers. Computational Linguistics ,\n37(1):197\u2013230.\nMcDonald, R., F. C. N. Pereira, K. Rib-\narov, and J. Haji \u02c7c. 2005b. Non-\nprojective dependency parsing us-\ning spanning tree algorithms. HLT-\nEMNLP .\nMcGuf\ufb01e, K. and A. Newhouse.\n2020. The radicalization risks of\nGPT-3 and advanced neural lan-\nguage models. ArXiv preprint\narXiv:2009.06807.\nMcLuhan, M. 1964. Understanding\nMedia: The Extensions of Man . New\nAmerican Library.\nMelamud, O., J. Goldberger, and I. Da-\ngan. 2016. context2vec: Learn-\ning generic context embedding with\nbidirectional LSTM. CoNLL .\nMerialdo, B. 1994. Tagging En-\nglish text with a probabilistic\nmodel. Computational Linguistics ,\n20(2):155\u2013172.\nMesgar, M. and M. Strube. 2016. Lexi-\ncal coherence graph modeling using\nword embeddings. ACL.\nMetsis, V ., I. Androutsopoulos, and\nG. Paliouras. 2006. Spam \ufb01lter-\ning with naive bayes-which naive\nbayes? CEAS .\nMeyers, A., R. Reeves, C. Macleod,\nR. Szekely, V . Zielinska, B. Young,\nand R. Grishman. 2004. The nom-\nbank project: An interim report.\nNAACL/HLT Workshop: Frontiers in\nCorpus Annotation .\nMihalcea, R. and A. Csomai. 2007.\nWikify!: Linking documents to en-\ncyclopedic knowledge. CIKM 2007 .\nMikheev, A., M. Moens, and C. Grover.\n1999. Named entity recognition\nwithout gazetteers. EACL .\nMikolov, T. 2012. Statistical lan-\nguage models based on neural net-\nworks . Ph.D. thesis, Brno University\nof Technology.\nMikolov, T., K. Chen, G. S. Corrado,\nand J. Dean. 2013a. Ef\ufb01cient estima-\ntion of word representations in vec-\ntor space. ICLR 2013 .\nMikolov, T., M. Kara\ufb01 \u00b4at, L. Bur-\nget, J. \u02c7Cernock `y, and S. Khudan-\npur. 2010. Recurrent neural net-\nwork based language model. IN-\nTERSPEECH .Mikolov, T., S. Kombrink, L. Burget,\nJ. H. \u02c7Cernock `y, and S. Khudanpur.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 955,
      "token_count": 744,
      "chapter_title": ""
    }
  },
  {
    "content": "NAACL/HLT Workshop: Frontiers in\nCorpus Annotation .\nMihalcea, R. and A. Csomai. 2007.\nWikify!: Linking documents to en-\ncyclopedic knowledge. CIKM 2007 .\nMikheev, A., M. Moens, and C. Grover.\n1999. Named entity recognition\nwithout gazetteers. EACL .\nMikolov, T. 2012. Statistical lan-\nguage models based on neural net-\nworks . Ph.D. thesis, Brno University\nof Technology.\nMikolov, T., K. Chen, G. S. Corrado,\nand J. Dean. 2013a. Ef\ufb01cient estima-\ntion of word representations in vec-\ntor space. ICLR 2013 .\nMikolov, T., M. Kara\ufb01 \u00b4at, L. Bur-\nget, J. \u02c7Cernock `y, and S. Khudan-\npur. 2010. Recurrent neural net-\nwork based language model. IN-\nTERSPEECH .Mikolov, T., S. Kombrink, L. Burget,\nJ. H. \u02c7Cernock `y, and S. Khudanpur.\n2011. Extensions of recurrent neural\nnetwork language model. ICASSP .\nMikolov, T., I. Sutskever, K. Chen,\nG. S. Corrado, and J. Dean. 2013b.\nDistributed representations of words\nand phrases and their compositional-\nity.NeurIPS .\nMikolov, T., W.-t. Yih, and G. Zweig.\n2013c. Linguistic regularities in\ncontinuous space word representa-\ntions. NAACL HLT .\nMiller, G. A. and J. G. Beebe-Center.\n1956. Some psychological methods\nfor evaluating the quality of trans-\nlations. Mechanical Translation ,\n3:73\u201380.\nMiller, G. A. and W. G. Charles. 1991.\nContextual correlates of semantics\nsimilarity. Language and Cognitive\nProcesses , 6(1):1\u201328.\nMiller, G. A. and N. Chomsky. 1963.\nFinitary models of language users.\nIn R. D. Luce, R. R. Bush, and\nE. Galanter, eds, Handbook of Math-\nematical Psychology , volume II,\n419\u2013491. John Wiley.\nMiller, G. A. and J. A. Selfridge.\n1950. Verbal context and the recall\nof meaningful material. American\nJournal of Psychology , 63:176\u2013185.\nMiller, S., R. J. Bobrow, R. Ingria, and\nR. Schwartz. 1994. Hidden under-\nstanding models of natural language.\nACL.\nMilne, D. and I. H. Witten. 2008.\nLearning to link with wikipedia.\nCIKM 2008 .\nMiltsakaki, E., R. Prasad, A. K. Joshi,\nand B. L. Webber. 2004. The Penn\nDiscourse Treebank. LREC .\nMin, S., X. Lyu, A. Holtzman,\nM. Artetxe, M. Lewis, H. Hajishirzi,\nand L. Zettlemoyer. 2022. Rethink-\ning the role of demonstrations: What\nmakes in-context learning work?",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 956,
      "token_count": 745,
      "chapter_title": ""
    }
  },
  {
    "content": "Miller, G. A. and N. Chomsky. 1963.\nFinitary models of language users.\nIn R. D. Luce, R. R. Bush, and\nE. Galanter, eds, Handbook of Math-\nematical Psychology , volume II,\n419\u2013491. John Wiley.\nMiller, G. A. and J. A. Selfridge.\n1950. Verbal context and the recall\nof meaningful material. American\nJournal of Psychology , 63:176\u2013185.\nMiller, S., R. J. Bobrow, R. Ingria, and\nR. Schwartz. 1994. Hidden under-\nstanding models of natural language.\nACL.\nMilne, D. and I. H. Witten. 2008.\nLearning to link with wikipedia.\nCIKM 2008 .\nMiltsakaki, E., R. Prasad, A. K. Joshi,\nand B. L. Webber. 2004. The Penn\nDiscourse Treebank. LREC .\nMin, S., X. Lyu, A. Holtzman,\nM. Artetxe, M. Lewis, H. Hajishirzi,\nand L. Zettlemoyer. 2022. Rethink-\ning the role of demonstrations: What\nmakes in-context learning work?\nEMNLP .\nMinsky, M. 1961. Steps toward arti\ufb01-\ncial intelligence. Proceedings of the\nIRE, 49(1):8\u201330.\nMinsky, M. 1974. A framework for rep-\nresenting knowledge. Technical Re-\nport 306, MIT AI Laboratory. Memo\n306.\nMinsky, M. and S. Papert. 1969. Per-\nceptrons . MIT Press.\nMintz, M., S. Bills, R. Snow, and D. Ju-\nrafsky. 2009. Distant supervision for\nrelation extraction without labeled\ndata. ACL IJCNLP .\nMirza, P. and S. Tonelli. 2016.\nCATENA: CAusal and TEmporal\nrelation extraction from NAtural\nlanguage texts. COLING .Mishra, S., D. Khashabi, C. Baral,\nand H. Hajishirzi. 2022. Cross-task\ngeneralization via natural language\ncrowdsourcing instructions. ACL.\nMitchell, M., S. Wu, A. Zal-\ndivar, P. Barnes, L. Vasserman,\nB. Hutchinson, E. Spitzer, I. D. Raji,\nand T. Gebru. 2019. Model cards for\nmodel reporting. ACM FAccT .\nMitkov, R. 2002. Anaphora Resolution .\nLongman.\nMohamed, A., G. E. Dahl, and G. E.\nHinton. 2009. Deep Belief Networks\nfor phone recognition. NIPS Work-\nshop on Deep Learning for Speech\nRecognition and Related Applica-\ntions .\nMohammad, S. M. 2018a. Obtaining\nreliable human ratings of valence,\narousal, and dominance for 20,000\nEnglish words. ACL.\nMohammad, S. M. 2018b. Word affect\nintensities. LREC .\nMohammad, S. M. and P. D. Tur-\nney. 2013. Crowdsourcing a word-\nemotion association lexicon. Com-\nputational Intelligence , 29(3):436\u2013\n465.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 957,
      "token_count": 742,
      "chapter_title": ""
    }
  },
  {
    "content": "and H. Hajishirzi. 2022. Cross-task\ngeneralization via natural language\ncrowdsourcing instructions. ACL.\nMitchell, M., S. Wu, A. Zal-\ndivar, P. Barnes, L. Vasserman,\nB. Hutchinson, E. Spitzer, I. D. Raji,\nand T. Gebru. 2019. Model cards for\nmodel reporting. ACM FAccT .\nMitkov, R. 2002. Anaphora Resolution .\nLongman.\nMohamed, A., G. E. Dahl, and G. E.\nHinton. 2009. Deep Belief Networks\nfor phone recognition. NIPS Work-\nshop on Deep Learning for Speech\nRecognition and Related Applica-\ntions .\nMohammad, S. M. 2018a. Obtaining\nreliable human ratings of valence,\narousal, and dominance for 20,000\nEnglish words. ACL.\nMohammad, S. M. 2018b. Word affect\nintensities. LREC .\nMohammad, S. M. and P. D. Tur-\nney. 2013. Crowdsourcing a word-\nemotion association lexicon. Com-\nputational Intelligence , 29(3):436\u2013\n465.\nMonroe, B. L., M. P. Colaresi, and\nK. M. Quinn. 2008. Fightin\u2019words:\nLexical feature selection and evalu-\nation for identifying the content of\npolitical con\ufb02ict. Political Analysis ,\n16(4):372\u2013403.\nMoors, A., P. C. Ellsworth, K. R.\nScherer, and N. H. Frijda. 2013. Ap-\npraisal theories of emotion: State\nof the art and future development.\nEmotion Review , 5(2):119\u2013124.\nMoosavi, N. S. and M. Strube. 2016.\nWhich coreference evaluation met-\nric do you trust? A proposal for a\nlink-based entity aware metric. ACL.\nMorey, M., P. Muller, and N. Asher.\n2017. How much progress have we\nmade on RST discourse parsing? a\nreplication study of recent results on\nthe rst-dt. EMNLP .\nMorgan, A. A., L. Hirschman,\nM. Colosimo, A. S. Yeh, and J. B.\nColombe. 2004. Gene name iden-\nti\ufb01cation and normalization using a\nmodel organism database. Journal of\nBiomedical Informatics , 37(6):396\u2013\n410.\nMorgan, N. and H. Bourlard. 1990.\nContinuous speech recognition us-\ning multilayer perceptrons with hid-\nden markov models. ICASSP .\nMorgan, N. and H. A. Bourlard.\n1995. Neural networks for sta-\ntistical recognition of continuous\nspeech. Proceedings of the IEEE ,\n83(5):742\u2013772.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 958,
      "token_count": 649,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 580",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 959,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "572 Bibliography\nMorris, J. and G. Hirst. 1991. Lexical\ncohesion computed by thesaural re-\nlations as an indicator of the struc-\nture of text. Computational Linguis-\ntics, 17(1):21\u201348.\nMosteller, F. and D. L. Wallace. 1963.\nInference in an authorship problem:\nA comparative study of discrimina-\ntion methods applied to the author-\nship of the disputed federalist pa-\npers. Journal of the American Statis-\ntical Association , 58(302):275\u2013309.\nMosteller, F. and D. L. Wallace. 1964.\nInference and Disputed Authorship:\nThe Federalist . Springer-Verlag.\n1984 2nd edition: Applied Bayesian\nand Classical Inference .\nMrk\u02c7si\u00b4c, N., D. \u00b4O S\u00b4eaghdha, T.-H. Wen,\nB. Thomson, and S. Young. 2017.\nNeural belief tracker: Data-driven\ndialogue state tracking. ACL.\nMuller, P., C. Braud, and M. Morey.\n2019. ToNy: Contextual embed-\ndings for accurate multilingual dis-\ncourse segmentation of full docu-\nments. Workshop on Discourse Re-\nlation Parsing and Treebanking .\nMurphy, K. P. 2012. Machine learning:\nA probabilistic perspective . MIT\nPress.\nMusi, E., M. Stede, L. Kriese, S. Mure-\nsan, and A. Rocci. 2018. A multi-\nlayer annotated corpus of argumen-\ntative text: From argument schemes\nto discourse relations. LREC .\nMyers, G. 1992. \u201cIn this paper we\nreport...\u201d: Speech acts and scien-\nti\ufb01c facts. Journal of Pragmatics ,\n17(4):295\u2013313.\nN\u00b4adas, A. 1984. Estimation of prob-\nabilities in the language model of\nthe IBM speech recognition sys-\ntem. IEEE Transactions on ASSP ,\n32(4):859\u2013861.\nNadeem, M., A. Bethke, and S. Reddy.\n2021. StereoSet: Measuring stereo-\ntypical bias in pretrained language\nmodels. ACL.\nNagata, M. and T. Morimoto. 1994.\nFirst steps toward statistical model-\ning of dialogue to predict the speech\nact type of the next utterance. Speech\nCommunication , 15:193\u2013203.\nNallapati, R., B. Zhou, C. dos San-\ntos, C \u00b8 . Gulc \u00b8ehre, and B. Xiang.\n2016. Abstractive text summa-\nrization using sequence-to-sequence\nRNNs and beyond. CoNLL .\nNash-Webber, B. L. 1975. The role of\nsemantics in automatic speech un-\nderstanding. In D. G. Bobrow and\nA. Collins, eds, Representation and\nUnderstanding , 351\u2013382. Academic\nPress.\nNaur, P., J. W. Backus, F. L. Bauer,\nJ. Green, C. Katz, J. McCarthy, A. J.Perlis, H. Rutishauser, K. Samelson,\nB. Vauquois, J. H. Wegstein, A. van\nWijnagaarden, and M. Woodger.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 960,
      "token_count": 740,
      "chapter_title": ""
    }
  },
  {
    "content": "Nadeem, M., A. Bethke, and S. Reddy.\n2021. StereoSet: Measuring stereo-\ntypical bias in pretrained language\nmodels. ACL.\nNagata, M. and T. Morimoto. 1994.\nFirst steps toward statistical model-\ning of dialogue to predict the speech\nact type of the next utterance. Speech\nCommunication , 15:193\u2013203.\nNallapati, R., B. Zhou, C. dos San-\ntos, C \u00b8 . Gulc \u00b8ehre, and B. Xiang.\n2016. Abstractive text summa-\nrization using sequence-to-sequence\nRNNs and beyond. CoNLL .\nNash-Webber, B. L. 1975. The role of\nsemantics in automatic speech un-\nderstanding. In D. G. Bobrow and\nA. Collins, eds, Representation and\nUnderstanding , 351\u2013382. Academic\nPress.\nNaur, P., J. W. Backus, F. L. Bauer,\nJ. Green, C. Katz, J. McCarthy, A. J.Perlis, H. Rutishauser, K. Samelson,\nB. Vauquois, J. H. Wegstein, A. van\nWijnagaarden, and M. Woodger.\n1960. Report on the algorith-\nmic language ALGOL 60. CACM ,\n3(5):299\u2013314. Revised in CACM\n6:1, 1-17, 1963.\nNayak, N., D. Hakkani-T \u00a8ur, M. A.\nWalker, and L. P. Heck. 2017. To\nplan or not to plan? discourse\nplanning in slot-value informed se-\nquence to sequence models for lan-\nguage generation. INTERSPEECH .\nNeff, G. and P. Nagy. 2016. Talking\nto bots: Symbiotic agency and the\ncase of Tay. International Journal\nof Communication , 10:4915\u20134931.\nNg, A. Y . and M. I. Jordan. 2002. On\ndiscriminative vs. generative classi-\n\ufb01ers: A comparison of logistic re-\ngression and naive bayes. NeurIPS .\nNg, H. T., L. H. Teo, and J. L. P. Kwan.\n2000. A machine learning approach\nto answering questions for reading\ncomprehension tests. EMNLP .\nNg, V . 2004. Learning noun phrase\nanaphoricity to improve coreference\nresolution: Issues in representation\nand optimization. ACL.\nNg, V . 2005a. Machine learning for\ncoreference resolution: From lo-\ncal classi\ufb01cation to global ranking.\nACL.\nNg, V . 2005b. Supervised ranking\nfor pronoun resolution: Some recent\nimprovements. AAAI .\nNg, V . 2010. Supervised noun phrase\ncoreference research: The \ufb01rst \ufb01f-\nteen years. ACL.\nNg, V . 2017. Machine learning for en-\ntity coreference resolution: A retro-\nspective look at two decades of re-\nsearch. AAAI .\nNg, V . and C. Cardie. 2002a. Identi-\nfying anaphoric and non-anaphoric\nnoun phrases to improve coreference\nresolution. COLING .\nNg, V . and C. Cardie. 2002b. Improv-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 961,
      "token_count": 757,
      "chapter_title": ""
    }
  },
  {
    "content": "gression and naive bayes. NeurIPS .\nNg, H. T., L. H. Teo, and J. L. P. Kwan.\n2000. A machine learning approach\nto answering questions for reading\ncomprehension tests. EMNLP .\nNg, V . 2004. Learning noun phrase\nanaphoricity to improve coreference\nresolution: Issues in representation\nand optimization. ACL.\nNg, V . 2005a. Machine learning for\ncoreference resolution: From lo-\ncal classi\ufb01cation to global ranking.\nACL.\nNg, V . 2005b. Supervised ranking\nfor pronoun resolution: Some recent\nimprovements. AAAI .\nNg, V . 2010. Supervised noun phrase\ncoreference research: The \ufb01rst \ufb01f-\nteen years. ACL.\nNg, V . 2017. Machine learning for en-\ntity coreference resolution: A retro-\nspective look at two decades of re-\nsearch. AAAI .\nNg, V . and C. Cardie. 2002a. Identi-\nfying anaphoric and non-anaphoric\nnoun phrases to improve coreference\nresolution. COLING .\nNg, V . and C. Cardie. 2002b. Improv-\ning machine learning approaches to\ncoreference resolution. ACL.\nNguyen, D. T. and S. Joty. 2017. A neu-\nral local coherence model. ACL.\nNickerson, R. S. 1976. On con-\nversational interaction with comput-\ners. Proceedings of the ACM/SIG-\nGRAPH workshop on User-oriented\ndesign of interactive graphics sys-\ntems.\nNie, A., E. Bennett, and N. Good-\nman. 2019. DisSent: Learning sen-\ntence representations from explicit\ndiscourse relations. ACL.\nNielsen, J. 1992. The usability engi-\nneering life cycle. IEEE Computer ,\n25(3):12\u201322.Nielsen, M. A. 2015. Neural networks\nand Deep learning . Determination\nPress USA.\nNigam, K., J. D. Lafferty, and A. Mc-\nCallum. 1999. Using maximum en-\ntropy for text classi\ufb01cation. IJCAI-\n99 workshop on machine learning\nfor information \ufb01ltering .\nNirenburg, S., H. L. Somers, and\nY . Wilks, eds. 2002. Readings in\nMachine Translation . MIT Press.\nNissim, M., S. Dingare, J. Carletta, and\nM. Steedman. 2004. An annotation\nscheme for information status in di-\nalogue. LREC .\nNIST. 2005. Speech recognition\nscoring toolkit (sctk) version 2.1.\nhttp://www.nist.gov/speech/\ntools/ .\nNIST. 2007. Matched Pairs Sentence-\nSegment Word Error (MAPSSWE)\nTest.\nNivre, J. 2007. Incremental non-\nprojective dependency parsing.\nNAACL-HLT .\nNivre, J. 2003. An ef\ufb01cient algorithm\nfor projective dependency parsing.\nProceedings of the 8th International\nWorkshop on Parsing Technologies\n(IWPT) .\nNivre, J. 2006. Inductive Dependency\nParsing . Springer.\nNivre, J. 2009. Non-projective de-\npendency parsing in expected linear",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 962,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "tropy for text classi\ufb01cation. IJCAI-\n99 workshop on machine learning\nfor information \ufb01ltering .\nNirenburg, S., H. L. Somers, and\nY . Wilks, eds. 2002. Readings in\nMachine Translation . MIT Press.\nNissim, M., S. Dingare, J. Carletta, and\nM. Steedman. 2004. An annotation\nscheme for information status in di-\nalogue. LREC .\nNIST. 2005. Speech recognition\nscoring toolkit (sctk) version 2.1.\nhttp://www.nist.gov/speech/\ntools/ .\nNIST. 2007. Matched Pairs Sentence-\nSegment Word Error (MAPSSWE)\nTest.\nNivre, J. 2007. Incremental non-\nprojective dependency parsing.\nNAACL-HLT .\nNivre, J. 2003. An ef\ufb01cient algorithm\nfor projective dependency parsing.\nProceedings of the 8th International\nWorkshop on Parsing Technologies\n(IWPT) .\nNivre, J. 2006. Inductive Dependency\nParsing . Springer.\nNivre, J. 2009. Non-projective de-\npendency parsing in expected linear\ntime. ACL IJCNLP .\nNivre, J., J. Hall, S. K \u00a8ubler, R. Mc-\nDonald, J. Nilsson, S. Riedel, and\nD. Yuret. 2007a. The conll 2007\nshared task on dependency parsing.\nEMNLP/CoNLL .\nNivre, J., J. Hall, J. Nilsson, A. Chanev,\nG. Eryigit, S. K \u00a8ubler, S. Mari-\nnov, and E. Marsi. 2007b. Malt-\nparser: A language-independent\nsystem for data-driven dependency\nparsing. Natural Language Engi-\nneering , 13(02):95\u2013135.\nNivre, J. and J. Nilsson. 2005. Pseudo-\nprojective dependency parsing. ACL.\nNivre, J. and M. Scholz. 2004. Deter-\nministic dependency parsing of en-\nglish text. COLING .\nNiwa, Y . and Y . Nitta. 1994. Co-\noccurrence vectors from corpora vs.\ndistance vectors from dictionaries.\nCOLING .\nNoreen, E. W. 1989. Computer Inten-\nsive Methods for Testing Hypothesis .\nWiley.\nNorman, D. A. 1988. The Design of Ev-\neryday Things . Basic Books.\nNorvig, P. 1991. Techniques for au-\ntomatic memoization with applica-\ntions to context-free parsing. Com-\nputational Linguistics , 17(1):91\u201398.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 963,
      "token_count": 619,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 581",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 964,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Bibliography 573\nNosek, B. A., M. R. Banaji, and\nA. G. Greenwald. 2002a. Harvest-\ning implicit group attitudes and be-\nliefs from a demonstration web site.\nGroup Dynamics: Theory, Research,\nand Practice , 6(1):101.\nNosek, B. A., M. R. Banaji, and A. G.\nGreenwald. 2002b. Math=male,\nme=female, therefore math 6=me.\nJournal of personality and social\npsychology , 83(1):44.\nNostalgebraist. 2020. Interpreting gpt:\nthe logit lens. White paper.\nOcal, M., A. Perez, A. Radas, and\nM. Finlayson. 2022. Holistic eval-\nuation of automatic TimeML anno-\ntators. LREC .\nOch, F. J. 1998. Ein beispiels-\nbasierter und statistischer Ansatz\nzum maschinellen Lernen von\nnat\u00a8urlichsprachlicher \u00a8Ubersetzung .\nPh.D. thesis, Universit \u00a8at Erlangen-\nN\u00a8urnberg, Germany. Diplomarbeit\n(diploma thesis).\nOch, F. J. 2003. Minimum error rate\ntraining in statistical machine trans-\nlation. ACL.\nOch, F. J. and H. Ney. 2002. Discrim-\ninative training and maximum en-\ntropy models for statistical machine\ntranslation. ACL.\nOch, F. J. and H. Ney. 2003. A system-\natic comparison of various statistical\nalignment models. Computational\nLinguistics , 29(1):19\u201351.\nOch, F. J. and H. Ney. 2004. The align-\nment template approach to statistical\nmachine translation. Computational\nLinguistics , 30(4):417\u2013449.\nO\u2019Connor, B., M. Krieger, and D. Ahn.\n2010. Tweetmotif: Exploratory\nsearch and topic summarization for\ntwitter. ICWSM .\nOlive, J. P. 1977. Rule synthe-\nsis of speech from dyadic units.\nICASSP77 .\nOlsson, C., N. Elhage, N. Nanda,\nN. Joseph, N. DasSarma,\nT. Henighan, B. Mann, A. Askell,\nY . Bai, A. Chen, et al. 2022. In-\ncontext learning and induction\nheads. ArXiv preprint.\nOlteanu, A., F. Diaz, and G. Kazai.\n2020. When are search completion\nsuggestions problematic? CSCW .\nvan den Oord, A., S. Dieleman, H. Zen,\nK. Simonyan, O. Vinyals, A. Graves,\nN. Kalchbrenner, A. Senior, and\nK. Kavukcuoglu. 2016. WaveNet:\nA Generative Model for Raw Audio.\nISCA Workshop on Speech Synthesis\nWorkshop .\nOppenheim, A. V ., R. W. Schafer, and\nT. G. J. Stockham. 1968. Nonlinear\n\ufb01ltering of multiplied and convolved\nsignals. Proceedings of the IEEE ,\n56(8):1264\u20131291.Oravecz, C. and P. Dienes. 2002. Ef-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 965,
      "token_count": 753,
      "chapter_title": ""
    }
  },
  {
    "content": "sis of speech from dyadic units.\nICASSP77 .\nOlsson, C., N. Elhage, N. Nanda,\nN. Joseph, N. DasSarma,\nT. Henighan, B. Mann, A. Askell,\nY . Bai, A. Chen, et al. 2022. In-\ncontext learning and induction\nheads. ArXiv preprint.\nOlteanu, A., F. Diaz, and G. Kazai.\n2020. When are search completion\nsuggestions problematic? CSCW .\nvan den Oord, A., S. Dieleman, H. Zen,\nK. Simonyan, O. Vinyals, A. Graves,\nN. Kalchbrenner, A. Senior, and\nK. Kavukcuoglu. 2016. WaveNet:\nA Generative Model for Raw Audio.\nISCA Workshop on Speech Synthesis\nWorkshop .\nOppenheim, A. V ., R. W. Schafer, and\nT. G. J. Stockham. 1968. Nonlinear\n\ufb01ltering of multiplied and convolved\nsignals. Proceedings of the IEEE ,\n56(8):1264\u20131291.Oravecz, C. and P. Dienes. 2002. Ef-\n\ufb01cient stochastic part-of-speech tag-\nging for Hungarian. LREC .\nOsgood, C. E., G. J. Suci, and P. H. Tan-\nnenbaum. 1957. The Measurement\nof Meaning . University of Illinois\nPress.\nOuyang, L., J. Wu, X. Jiang,\nD. Almeida, C. Wainwright,\nP. Mishkin, C. Zhang, S. Agar-\nwal, K. Slama, A. Ray, J. Schul-\nman, J. Hilton, F. Kelton, L. Miller,\nM. Simens, A. Askell, P. Welinder,\nP. Christiano, J. Leike, and R. Lowe.\n2022. Training language models\nto follow instructions with human\nfeedback. NeurIPS , volume 35.\nPackard, D. W. 1973. Computer-\nassisted morphological analysis of\nancient Greek. COLING .\nPalmer, D. 2012. Text preprocessing.\nIn N. Indurkhya and F. J. Damerau,\neds,Handbook of Natural Language\nProcessing , 9\u201330. CRC Press.\nPalmer, M., D. Gildea, and N. Xue.\n2010. Semantic role labeling. Syn-\nthesis Lectures on Human Language\nTechnologies , 3(1):1\u2013103.\nPalmer, M., P. Kingsbury, and\nD. Gildea. 2005. The proposi-\ntion bank: An annotated corpus\nof semantic roles. Computational\nLinguistics , 31(1):71\u2013106.\nPanayotov, V ., G. Chen, D. Povey, and\nS. Khudanpur. 2015. Librispeech: an\nASR corpus based on public domain\naudio books. ICASSP .\nPang, B. and L. Lee. 2008. Opin-\nion mining and sentiment analysis.\nFoundations and trends in informa-\ntion retrieval , 2(1-2):1\u2013135.\nPang, B., L. Lee, and S. Vaithyanathan.\n2002. Thumbs up? Sentiment",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 966,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "assisted morphological analysis of\nancient Greek. COLING .\nPalmer, D. 2012. Text preprocessing.\nIn N. Indurkhya and F. J. Damerau,\neds,Handbook of Natural Language\nProcessing , 9\u201330. CRC Press.\nPalmer, M., D. Gildea, and N. Xue.\n2010. Semantic role labeling. Syn-\nthesis Lectures on Human Language\nTechnologies , 3(1):1\u2013103.\nPalmer, M., P. Kingsbury, and\nD. Gildea. 2005. The proposi-\ntion bank: An annotated corpus\nof semantic roles. Computational\nLinguistics , 31(1):71\u2013106.\nPanayotov, V ., G. Chen, D. Povey, and\nS. Khudanpur. 2015. Librispeech: an\nASR corpus based on public domain\naudio books. ICASSP .\nPang, B. and L. Lee. 2008. Opin-\nion mining and sentiment analysis.\nFoundations and trends in informa-\ntion retrieval , 2(1-2):1\u2013135.\nPang, B., L. Lee, and S. Vaithyanathan.\n2002. Thumbs up? Sentiment\nclassi\ufb01cation using machine learn-\ning techniques. EMNLP .\nPaolino, J. 2017. Google Home\nvs Alexa: Two simple user\nexperience design gestures\nthat delighted a female user.\nMedium . Jan 4, 2017. https:\n//medium.com/startup-grind/\ngoogle-home-vs-alexa-56e26f69ac77 .\nPapadimitriou, I., K. Lopez, and D. Ju-\nrafsky. 2023. Multilingual BERT has\nan accent: Evaluating English in-\n\ufb02uences on \ufb02uency in multilingual\nmodels. EACL Findings .\nPapineni, K., S. Roukos, T. Ward, and\nW.-J. Zhu. 2002. Bleu: A method\nfor automatic evaluation of machine\ntranslation. ACL.\nPark, J. H., J. Shin, and P. Fung. 2018.\nReducing gender bias in abusive lan-\nguage detection. EMNLP .\nPark, J. and C. Cardie. 2014. Identify-\ning appropriate support for proposi-\ntions in online user comments. First\nworkshop on argumentation mining .Parrish, A., A. Chen, N. Nangia, V . Pad-\nmakumar, J. Phang, J. Thompson,\nP. M. Htut, and S. Bowman. 2022.\nBBQ: A hand-built bias benchmark\nfor question answering. Findings of\nACL 2022 .\nPaszke, A., S. Gross, S. Chintala,\nG. Chanan, E. Yang, Z. DeVito,\nZ. Lin, A. Desmaison, L. Antiga,\nand A. Lerer. 2017. Automatic dif-\nferentiation in pytorch. NIPS-W .\nPearl, C. 2017. Designing Voice User\nInterfaces: Principles of Conversa-\ntional Experiences . O\u2019Reilly.\nPeldszus, A. and M. Stede. 2013. From\nargument diagrams to argumentation\nmining in texts: A survey. In-\nternational Journal of Cognitive In-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 967,
      "token_count": 754,
      "chapter_title": ""
    }
  },
  {
    "content": "Reducing gender bias in abusive lan-\nguage detection. EMNLP .\nPark, J. and C. Cardie. 2014. Identify-\ning appropriate support for proposi-\ntions in online user comments. First\nworkshop on argumentation mining .Parrish, A., A. Chen, N. Nangia, V . Pad-\nmakumar, J. Phang, J. Thompson,\nP. M. Htut, and S. Bowman. 2022.\nBBQ: A hand-built bias benchmark\nfor question answering. Findings of\nACL 2022 .\nPaszke, A., S. Gross, S. Chintala,\nG. Chanan, E. Yang, Z. DeVito,\nZ. Lin, A. Desmaison, L. Antiga,\nand A. Lerer. 2017. Automatic dif-\nferentiation in pytorch. NIPS-W .\nPearl, C. 2017. Designing Voice User\nInterfaces: Principles of Conversa-\ntional Experiences . O\u2019Reilly.\nPeldszus, A. and M. Stede. 2013. From\nargument diagrams to argumentation\nmining in texts: A survey. In-\nternational Journal of Cognitive In-\nformatics and Natural Intelligence\n(IJCINI) , 7(1):1\u201331.\nPeldszus, A. and M. Stede. 2016. An\nannotated corpus of argumentative\nmicrotexts. 1st European Confer-\nence on Argumentation .\nPenn, G. and P. Kiparsky. 2012. On\nP\u00afan.ini and the generative capacity of\ncontextualized replacement systems.\nCOLING .\nPennebaker, J. W., R. J. Booth, and\nM. E. Francis. 2007. Linguistic In-\nquiry and Word Count: LIWC 2007 .\nAustin, TX.\nPennington, J., R. Socher, and C. D.\nManning. 2014. GloVe: Global\nvectors for word representation.\nEMNLP .\nPercival, W. K. 1976. On the his-\ntorical source of immediate con-\nstituent analysis. In J. D. McCawley,\ned., Syntax and Semantics Volume\n7, Notes from the Linguistic Under-\nground , 229\u2013242. Academic Press.\nPerrault, C. R. and J. Allen. 1980.\nA plan-based analysis of indirect\nspeech acts. American Journal\nof Computational Linguistics , 6(3-\n4):167\u2013182.\nPeters, M., M. Neumann, M. Iyyer,\nM. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer. 2018. Deep\ncontextualized word representations.\nNAACL HLT .\nPeterson, G. E., W. S.-Y . Wang, and\nE. Sivertsen. 1958. Segmenta-\ntion techniques in speech synthesis.\nJASA , 30(8):739\u2013742.\nPeterson, J. C., D. Chen, and T. L. Grif-\n\ufb01ths. 2020. Parallelograms revisited:\nExploring the limitations of vector\nspace models for simple analogies.\nCognition , 205.\nPetroni, F., T. Rockt \u00a8aschel, S. Riedel,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 968,
      "token_count": 738,
      "chapter_title": ""
    }
  },
  {
    "content": "torical source of immediate con-\nstituent analysis. In J. D. McCawley,\ned., Syntax and Semantics Volume\n7, Notes from the Linguistic Under-\nground , 229\u2013242. Academic Press.\nPerrault, C. R. and J. Allen. 1980.\nA plan-based analysis of indirect\nspeech acts. American Journal\nof Computational Linguistics , 6(3-\n4):167\u2013182.\nPeters, M., M. Neumann, M. Iyyer,\nM. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer. 2018. Deep\ncontextualized word representations.\nNAACL HLT .\nPeterson, G. E., W. S.-Y . Wang, and\nE. Sivertsen. 1958. Segmenta-\ntion techniques in speech synthesis.\nJASA , 30(8):739\u2013742.\nPeterson, J. C., D. Chen, and T. L. Grif-\n\ufb01ths. 2020. Parallelograms revisited:\nExploring the limitations of vector\nspace models for simple analogies.\nCognition , 205.\nPetroni, F., T. Rockt \u00a8aschel, S. Riedel,\nP. Lewis, A. Bakhtin, Y . Wu, and\nA. Miller. 2019. Language models\nas knowledge bases? EMNLP .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 969,
      "token_count": 306,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 582",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 970,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "574 Bibliography\nPetrov, S., D. Das, and R. McDonald.\n2012. A universal part-of-speech\ntagset. LREC .\nPetrov, S. and R. McDonald. 2012.\nOverview of the 2012 shared task on\nparsing the web. Notes of the First\nWorkshop on Syntactic Analysis of\nNon-Canonical Language (SANCL) ,\nvolume 59.\nPhillips, A. V . 1960. A question-\nanswering routine. Technical Re-\nport 16, MIT AI Lab.\nPicard, R. W. 1995. Affective comput-\ning. Technical Report 321, MIT Me-\ndia Lab Perceputal Computing Tech-\nnical Report. Revised November 26,\n1995.\nPieraccini, R., E. Levin, and C.-H.\nLee. 1991. Stochastic representation\nof conceptual structure in the ATIS\ntask. Speech and Natural Language\nWorkshop .\nPierce, J. R., J. B. Carroll, E. P.\nHamp, D. G. Hays, C. F. Hockett,\nA. G. Oettinger, and A. J. Perlis.\n1966. Language and Machines:\nComputers in Translation and Lin-\nguistics . ALPAC report. National\nAcademy of Sciences, National Re-\nsearch Council, Washington, DC.\nPilehvar, M. T. and J. Camacho-\nCollados. 2019. WiC: the word-\nin-context dataset for evaluating\ncontext-sensitive meaning represen-\ntations. NAACL HLT .\nPitler, E., A. Louis, and A. Nenkova.\n2009. Automatic sense prediction\nfor implicit discourse relations in\ntext. ACL IJCNLP .\nPitler, E. and A. Nenkova. 2009. Us-\ning syntax to disambiguate explicit\ndiscourse connectives in text. ACL\nIJCNLP .\nPlutchik, R. 1962. The emotions: Facts,\ntheories, and a new model . Random\nHouse.\nPlutchik, R. 1980. A general psycho-\nevolutionary theory of emotion. In\nR. Plutchik and H. Kellerman, eds,\nEmotion: Theory, Research, and Ex-\nperience, Volume 1 , 3\u201333. Academic\nPress.\nPoesio, M., R. Stevenson, B. Di Euge-\nnio, and J. Hitzeman. 2004. Center-\ning: A parametric theory and its in-\nstantiations. Computational Linguis-\ntics, 30(3):309\u2013363.\nPoesio, M., R. Stuckardt, and Y . Ver-\nsley. 2016. Anaphora resolution:\nAlgorithms, resources, and applica-\ntions . Springer.\nPoesio, M., P. Sturt, R. Artstein, and\nR. Filik. 2006. Underspeci\ufb01cation\nand anaphora: Theoretical issues\nand preliminary evidence. Discourse\nprocesses , 42(2):157\u2013175.Poesio, M. and R. Vieira. 1998. A\ncorpus-based investigation of de\ufb01-\nnite description use. Computational\nLinguistics , 24(2):183\u2013216.\nPolanyi, L. 1988. A formal model of",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 971,
      "token_count": 745,
      "chapter_title": ""
    }
  },
  {
    "content": "evolutionary theory of emotion. In\nR. Plutchik and H. Kellerman, eds,\nEmotion: Theory, Research, and Ex-\nperience, Volume 1 , 3\u201333. Academic\nPress.\nPoesio, M., R. Stevenson, B. Di Euge-\nnio, and J. Hitzeman. 2004. Center-\ning: A parametric theory and its in-\nstantiations. Computational Linguis-\ntics, 30(3):309\u2013363.\nPoesio, M., R. Stuckardt, and Y . Ver-\nsley. 2016. Anaphora resolution:\nAlgorithms, resources, and applica-\ntions . Springer.\nPoesio, M., P. Sturt, R. Artstein, and\nR. Filik. 2006. Underspeci\ufb01cation\nand anaphora: Theoretical issues\nand preliminary evidence. Discourse\nprocesses , 42(2):157\u2013175.Poesio, M. and R. Vieira. 1998. A\ncorpus-based investigation of de\ufb01-\nnite description use. Computational\nLinguistics , 24(2):183\u2013216.\nPolanyi, L. 1988. A formal model of\nthe structure of discourse. Journal\nof Pragmatics , 12.\nPolanyi, L., C. Culy, M. van den Berg,\nG. L. Thione, and D. Ahn. 2004.\nA rule based approach to discourse\nparsing. Proceedings of SIGDIAL .\nPollard, C. and I. A. Sag. 1994. Head-\nDriven Phrase Structure Grammar .\nUniversity of Chicago Press.\nPonzetto, S. P. and M. Strube. 2006.\nExploiting semantic role labeling,\nWordNet and Wikipedia for corefer-\nence resolution. HLT-NAACL .\nPonzetto, S. P. and M. Strube. 2007.\nKnowledge derived from Wikipedia\nfor computing semantic relatedness.\nJAIR , 30:181\u2013212.\nPopovi \u00b4c, M. 2015. chrF: charac-\nter n-gram F-score for automatic\nMT evaluation. Proceedings of the\nTenth Workshop on Statistical Ma-\nchine Translation .\nPopp, D., R. A. Donovan, M. Craw-\nford, K. L. Marsh, and M. Peele.\n2003. Gender, race, and speech style\nstereotypes. Sex Roles , 48(7-8):317\u2013\n325.\nPorter, M. F. 1980. An algorithm\nfor suf\ufb01x stripping. Program ,\n14(3):130\u2013137.\nPost, M. 2018. A call for clarity in re-\nporting BLEU scores. WMT 2018 .\nPotts, C. 2011. On the negativity of\nnegation. In N. Li and D. Lutz,\neds, Proceedings of Semantics and\nLinguistic Theory 20 , 636\u2013659. CLC\nPublications, Ithaca, NY .\nPovey, D., A. Ghoshal, G. Boulianne,\nL. Burget, O. Glembek, N. Goel,\nM. Hannemann, P. Motlicek,\nY . Qian, P. Schwarz, J. Silovsk \u00b4y,\nG. Stemmer, and K. Vesel \u00b4y. 2011.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 972,
      "token_count": 750,
      "chapter_title": ""
    }
  },
  {
    "content": "ter n-gram F-score for automatic\nMT evaluation. Proceedings of the\nTenth Workshop on Statistical Ma-\nchine Translation .\nPopp, D., R. A. Donovan, M. Craw-\nford, K. L. Marsh, and M. Peele.\n2003. Gender, race, and speech style\nstereotypes. Sex Roles , 48(7-8):317\u2013\n325.\nPorter, M. F. 1980. An algorithm\nfor suf\ufb01x stripping. Program ,\n14(3):130\u2013137.\nPost, M. 2018. A call for clarity in re-\nporting BLEU scores. WMT 2018 .\nPotts, C. 2011. On the negativity of\nnegation. In N. Li and D. Lutz,\neds, Proceedings of Semantics and\nLinguistic Theory 20 , 636\u2013659. CLC\nPublications, Ithaca, NY .\nPovey, D., A. Ghoshal, G. Boulianne,\nL. Burget, O. Glembek, N. Goel,\nM. Hannemann, P. Motlicek,\nY . Qian, P. Schwarz, J. Silovsk \u00b4y,\nG. Stemmer, and K. Vesel \u00b4y. 2011.\nThe Kaldi speech recognition\ntoolkit. ASRU .\nPradhan, S., E. H. Hovy, M. P. Mar-\ncus, M. Palmer, L. Ramshaw, and\nR. Weischedel. 2007a. OntoNotes:\nA uni\ufb01ed relational semantic repre-\nsentation. Proceedings of ICSC .\nPradhan, S., E. H. Hovy, M. P. Mar-\ncus, M. Palmer, L. A. Ramshaw,\nand R. M. Weischedel. 2007b.\nOntonotes: a uni\ufb01ed relational se-\nmantic representation. Int. J. Seman-\ntic Computing , 1(4):405\u2013419.\nPradhan, S., X. Luo, M. Recasens,\nE. H. Hovy, V . Ng, and M. Strube.\n2014. Scoring coreference partitions\nof predicted mentions: A reference\nimplementation. ACL.Pradhan, S., A. Moschitti, N. Xue, H. T.\nNg, A. Bj \u00a8orkelund, O. Uryupina,\nY . Zhang, and Z. Zhong. 2013. To-\nwards robust linguistic analysis us-\ning OntoNotes. CoNLL .\nPradhan, S., A. Moschitti, N. Xue,\nO. Uryupina, and Y . Zhang. 2012a.\nCoNLL-2012 shared task: Model-\ning multilingual unrestricted coref-\nerence in OntoNotes. CoNLL .\nPradhan, S., A. Moschitti, N. Xue,\nO. Uryupina, and Y . Zhang. 2012b.\nConll-2012 shared task: Model-\ning multilingual unrestricted coref-\nerence in OntoNotes. CoNLL .\nPradhan, S., L. Ramshaw, M. P. Mar-\ncus, M. Palmer, R. Weischedel, and\nN. Xue. 2011. CoNLL-2011 shared\ntask: Modeling unrestricted corefer-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 973,
      "token_count": 746,
      "chapter_title": ""
    }
  },
  {
    "content": "2014. Scoring coreference partitions\nof predicted mentions: A reference\nimplementation. ACL.Pradhan, S., A. Moschitti, N. Xue, H. T.\nNg, A. Bj \u00a8orkelund, O. Uryupina,\nY . Zhang, and Z. Zhong. 2013. To-\nwards robust linguistic analysis us-\ning OntoNotes. CoNLL .\nPradhan, S., A. Moschitti, N. Xue,\nO. Uryupina, and Y . Zhang. 2012a.\nCoNLL-2012 shared task: Model-\ning multilingual unrestricted coref-\nerence in OntoNotes. CoNLL .\nPradhan, S., A. Moschitti, N. Xue,\nO. Uryupina, and Y . Zhang. 2012b.\nConll-2012 shared task: Model-\ning multilingual unrestricted coref-\nerence in OntoNotes. CoNLL .\nPradhan, S., L. Ramshaw, M. P. Mar-\ncus, M. Palmer, R. Weischedel, and\nN. Xue. 2011. CoNLL-2011 shared\ntask: Modeling unrestricted corefer-\nence in OntoNotes. CoNLL .\nPradhan, S., L. Ramshaw, R. Wei-\nschedel, J. MacBride, and L. Mic-\nciulla. 2007c. Unrestricted corefer-\nence: Identifying entities and events\nin OntoNotes. Proceedings of\nICSC 2007 .\nPradhan, S., W. Ward, K. Hacioglu,\nJ. H. Martin, and D. Jurafsky. 2005.\nSemantic role labeling using differ-\nent syntactic views. ACL.\nPrasad, A., P. Hase, X. Zhou, and\nM. Bansal. 2023. GrIPS: Gradient-\nfree, edit-based instruction search\nfor prompting large language mod-\nels.EACL .\nPrasad, R., N. Dinesh, A. Lee, E. Milt-\nsakaki, L. Robaldo, A. K. Joshi, and\nB. L. Webber. 2008. The Penn Dis-\ncourse TreeBank 2.0. LREC .\nPrasad, R., B. L. Webber, and A. Joshi.\n2014. Re\ufb02ections on the Penn Dis-\ncourse Treebank, comparable cor-\npora, and complementary annota-\ntion. Computational Linguistics ,\n40(4):921\u2013950.\nPrates, M. O. R., P. H. Avelar, and L. C.\nLamb. 2019. Assessing gender bias\nin machine translation: a case study\nwith Google Translate. Neural Com-\nputing and Applications , 32:6363\u2013\n6381.\nPrice, P. J., W. Fisher, J. Bern-\nstein, and D. Pallet. 1988. The\nDARPA 1000-word resource man-\nagement database for continuous\nspeech recognition. ICASSP .\nPrince, E. 1981. Toward a taxonomy of\ngiven-new information. In P. Cole,\ned., Radical Pragmatics , 223\u2013255.\nAcademic Press.\nPropp, V . 1968. Morphology of the\nFolktale , 2nd edition. University of\nTexas Press. Original Russian 1928.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 974,
      "token_count": 751,
      "chapter_title": ""
    }
  },
  {
    "content": "B. L. Webber. 2008. The Penn Dis-\ncourse TreeBank 2.0. LREC .\nPrasad, R., B. L. Webber, and A. Joshi.\n2014. Re\ufb02ections on the Penn Dis-\ncourse Treebank, comparable cor-\npora, and complementary annota-\ntion. Computational Linguistics ,\n40(4):921\u2013950.\nPrates, M. O. R., P. H. Avelar, and L. C.\nLamb. 2019. Assessing gender bias\nin machine translation: a case study\nwith Google Translate. Neural Com-\nputing and Applications , 32:6363\u2013\n6381.\nPrice, P. J., W. Fisher, J. Bern-\nstein, and D. Pallet. 1988. The\nDARPA 1000-word resource man-\nagement database for continuous\nspeech recognition. ICASSP .\nPrince, E. 1981. Toward a taxonomy of\ngiven-new information. In P. Cole,\ned., Radical Pragmatics , 223\u2013255.\nAcademic Press.\nPropp, V . 1968. Morphology of the\nFolktale , 2nd edition. University of\nTexas Press. Original Russian 1928.\nTranslated by Laurence Scott.\nPryzant, R., D. Iter, J. Li, Y . Lee,\nC. Zhu, and M. Zeng. 2023. Au-\ntomatic prompt optimization with",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 975,
      "token_count": 321,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 583",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 976,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Bibliography 575\n\u201cgradient descent\u201d and beam search.\nEMNLP .\nPundak, G. and T. N. Sainath. 2016.\nLower frame rate neural network\nacoustic models. INTERSPEECH .\nPustejovsky, J. 1991. The generative\nlexicon. Computational Linguistics ,\n17(4).\nPustejovsky, J., P. Hanks, R. Saur \u00b4\u0131,\nA. See, R. Gaizauskas, A. Setzer,\nD. Radev, B. Sundheim, D. S. Day,\nL. Ferro, and M. Lazo. 2003. The\nTIMEBANK corpus. Proceedings\nof Corpus Linguistics 2003 Confer-\nence. UCREL Technical Paper num-\nber 16.\nPustejovsky, J., R. Ingria,\nR. Saur \u00b4\u0131, J. Casta \u02dcno, J. Littman,\nR. Gaizauskas, A. Setzer, G. Katz,\nand I. Mani. 2005. The Speci\ufb01ca-\ntion Language TimeML , chapter 27.\nOxford.\nQin, L., Z. Zhang, and H. Zhao. 2016.\nA stacking gated neural architecture\nfor implicit discourse relation classi-\n\ufb01cation. EMNLP .\nQin, L., Z. Zhang, H. Zhao, Z. Hu,\nand E. Xing. 2017. Adversarial\nconnective-exploiting networks for\nimplicit discourse relation classi\ufb01ca-\ntion. ACL.\nRadford, A., J. Wu, R. Child, D. Luan,\nD. Amodei, and I. Sutskever. 2019.\nLanguage models are unsupervised\nmultitask learners. OpenAI tech re-\nport.\nRafailov, R., A. Sharma, E. Mitchell,\nS. Ermon, C. D. Manning, and\nC. Finn. 2023. Direct preference op-\ntimization: Your language model is\nsecretly a reward model. NeurIPS .\nRaffel, C., N. Shazeer, A. Roberts,\nK. Lee, S. Narang, M. Matena,\nY . Zhou, W. Li, and P. J. Liu.\n2020. Exploring the limits of trans-\nfer learning with a uni\ufb01ed text-to-\ntext transformer. JMLR , 21(140):1\u2013\n67.\nRaghunathan, K., H. Lee, S. Rangara-\njan, N. Chambers, M. Surdeanu,\nD. Jurafsky, and C. D. Manning.\n2010. A multi-pass sieve for coref-\nerence resolution. EMNLP .\nRahman, A. and V . Ng. 2009. Super-\nvised models for coreference resolu-\ntion. EMNLP .\nRahman, A. and V . Ng. 2012. Resolv-\ning complex cases of de\ufb01nite pro-\nnouns: the Winograd Schema chal-\nlenge. EMNLP .\nRajpurkar, P., R. Jia, and P. Liang.\n2018. Know what you don\u2019t\nknow: Unanswerable questions for\nSQuAD. ACL.\nRajpurkar, P., J. Zhang, K. Lopyrev, and",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 977,
      "token_count": 742,
      "chapter_title": ""
    }
  },
  {
    "content": "K. Lee, S. Narang, M. Matena,\nY . Zhou, W. Li, and P. J. Liu.\n2020. Exploring the limits of trans-\nfer learning with a uni\ufb01ed text-to-\ntext transformer. JMLR , 21(140):1\u2013\n67.\nRaghunathan, K., H. Lee, S. Rangara-\njan, N. Chambers, M. Surdeanu,\nD. Jurafsky, and C. D. Manning.\n2010. A multi-pass sieve for coref-\nerence resolution. EMNLP .\nRahman, A. and V . Ng. 2009. Super-\nvised models for coreference resolu-\ntion. EMNLP .\nRahman, A. and V . Ng. 2012. Resolv-\ning complex cases of de\ufb01nite pro-\nnouns: the Winograd Schema chal-\nlenge. EMNLP .\nRajpurkar, P., R. Jia, and P. Liang.\n2018. Know what you don\u2019t\nknow: Unanswerable questions for\nSQuAD. ACL.\nRajpurkar, P., J. Zhang, K. Lopyrev, and\nP. Liang. 2016. SQuAD: 100,000+questions for machine comprehen-\nsion of text. EMNLP .\nRam, O., Y . Levine, I. Dalmedigos,\nD. Muhlgay, A. Shashua, K. Leyton-\nBrown, and Y . Shoham. 2023.\nIn-context retrieval-augmented lan-\nguage models. ArXiv preprint.\nRamshaw, L. A. and M. P. Mar-\ncus. 1995. Text chunking using\ntransformation-based learning. Pro-\nceedings of the 3rd Annual Work-\nshop on Very Large Corpora .\nRashkin, H., E. Bell, Y . Choi, and\nS. V olkova. 2017. Multilingual con-\nnotation frames: A case study on\nsocial media for targeted sentiment\nanalysis and forecast. ACL.\nRashkin, H., S. Singh, and Y . Choi.\n2016. Connotation frames: A data-\ndriven investigation. ACL.\nRashkin, H., E. M. Smith, M. Li,\nand Y .-L. Boureau. 2019. Towards\nempathetic open-domain conversa-\ntion models: A new benchmark and\ndataset. ACL.\nRatinov, L. and D. Roth. 2012.\nLearning-based multi-sieve co-\nreference resolution with knowl-\nedge. EMNLP .\nRatnaparkhi, A. 1996. A maxi-\nmum entropy part-of-speech tagger.\nEMNLP .\nRatnaparkhi, A. 1997. A linear ob-\nserved time statistical parser based\non maximum entropy models.\nEMNLP .\nRawls, J. 2001. Justice as fairness:\nA restatement . Harvard University\nPress.\nRecasens, M. and E. H. Hovy. 2011.\nBLANC: Implementing the Rand\nindex for coreference evaluation.\nNatural Language Engineering ,\n17(4):485\u2013510.\nRecasens, M., E. H. Hovy, and M. A.\nMart \u00b4\u0131. 2011. Identity, non-identity,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 978,
      "token_count": 737,
      "chapter_title": ""
    }
  },
  {
    "content": "Rashkin, H., S. Singh, and Y . Choi.\n2016. Connotation frames: A data-\ndriven investigation. ACL.\nRashkin, H., E. M. Smith, M. Li,\nand Y .-L. Boureau. 2019. Towards\nempathetic open-domain conversa-\ntion models: A new benchmark and\ndataset. ACL.\nRatinov, L. and D. Roth. 2012.\nLearning-based multi-sieve co-\nreference resolution with knowl-\nedge. EMNLP .\nRatnaparkhi, A. 1996. A maxi-\nmum entropy part-of-speech tagger.\nEMNLP .\nRatnaparkhi, A. 1997. A linear ob-\nserved time statistical parser based\non maximum entropy models.\nEMNLP .\nRawls, J. 2001. Justice as fairness:\nA restatement . Harvard University\nPress.\nRecasens, M. and E. H. Hovy. 2011.\nBLANC: Implementing the Rand\nindex for coreference evaluation.\nNatural Language Engineering ,\n17(4):485\u2013510.\nRecasens, M., E. H. Hovy, and M. A.\nMart \u00b4\u0131. 2011. Identity, non-identity,\nand near-identity: Addressing the\ncomplexity of coreference. Lingua ,\n121(6):1138\u20131152.\nRecasens, M. and M. A. Mart \u00b4\u0131. 2010.\nAnCora-CO: Coreferentially anno-\ntated corpora for Spanish and Cata-\nlan. Language Resources and Eval-\nuation , 44(4):315\u2013345.\nReed, C., R. Mochales Palau, G. Rowe,\nand M.-F. Moens. 2008. Lan-\nguage resources for studying argu-\nment. LREC .\nReeves, B. and C. Nass. 1996. The\nMedia Equation: How People Treat\nComputers, Television, and New Me-\ndia Like Real People and Places .\nCambridge University Press.\nRehder, B., M. E. Schreiner, M. B. W.\nWolfe, D. Laham, T. K. Landauer,\nand W. Kintsch. 1998. Using LatentSemantic Analysis to assess knowl-\nedge: Some technical considera-\ntions. Discourse Processes , 25(2-\n3):337\u2013354.\nRei, R., C. Stewart, A. C. Farinha, and\nA. Lavie. 2020. COMET: A neu-\nral framework for MT evaluation.\nEMNLP .\nReichenbach, H. 1947. Elements of\nSymbolic Logic . Macmillan, New\nYork.\nReichman, R. 1985. Getting Computers\nto Talk Like You and Me . MIT Press.\nResnik, P. 1993. Semantic classes and\nsyntactic ambiguity. HLT.\nResnik, P. 1996. Selectional con-\nstraints: An information-theoretic\nmodel and its computational realiza-\ntion. Cognition , 61:127\u2013159.\nReynolds, L. and K. McDonell. 2021.\nPrompt programming for large lan-\nguage models: Beyond the few-shot\nparadigm. CHI 2021 .\nRiedel, S., L. Yao, and A. McCallum.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 979,
      "token_count": 742,
      "chapter_title": ""
    }
  },
  {
    "content": "Wolfe, D. Laham, T. K. Landauer,\nand W. Kintsch. 1998. Using LatentSemantic Analysis to assess knowl-\nedge: Some technical considera-\ntions. Discourse Processes , 25(2-\n3):337\u2013354.\nRei, R., C. Stewart, A. C. Farinha, and\nA. Lavie. 2020. COMET: A neu-\nral framework for MT evaluation.\nEMNLP .\nReichenbach, H. 1947. Elements of\nSymbolic Logic . Macmillan, New\nYork.\nReichman, R. 1985. Getting Computers\nto Talk Like You and Me . MIT Press.\nResnik, P. 1993. Semantic classes and\nsyntactic ambiguity. HLT.\nResnik, P. 1996. Selectional con-\nstraints: An information-theoretic\nmodel and its computational realiza-\ntion. Cognition , 61:127\u2013159.\nReynolds, L. and K. McDonell. 2021.\nPrompt programming for large lan-\nguage models: Beyond the few-shot\nparadigm. CHI 2021 .\nRiedel, S., L. Yao, and A. McCallum.\n2010. Modeling relations and their\nmentions without labeled text. In\nMachine Learning and Knowledge\nDiscovery in Databases , 148\u2013163.\nSpringer.\nRiedel, S., L. Yao, A. McCallum, and\nB. M. Marlin. 2013. Relation extrac-\ntion with matrix factorization and\nuniversal schemas. NAACL HLT .\nRiloff, E. 1993. Automatically con-\nstructing a dictionary for informa-\ntion extraction tasks. AAAI .\nRiloff, E. 1996. Automatically gen-\nerating extraction patterns from un-\ntagged text. AAAI .\nRiloff, E. and R. Jones. 1999. Learning\ndictionaries for information extrac-\ntion by multi-level bootstrapping.\nAAAI .\nRiloff, E. and M. Schmelzenbach. 1998.\nAn empirical approach to conceptual\ncase frame acquisition. Proceedings\nof the Sixth Workshop on Very Large\nCorpora .\nRiloff, E. and J. Shepherd. 1997. A\ncorpus-based approach for building\nsemantic lexicons. EMNLP .\nRiloff, E. and M. Thelen. 2000. A rule-\nbased question answering system\nfor reading comprehension tests.\nANLP/NAACL workshop on reading\ncomprehension tests .\nRiloff, E. and J. Wiebe. 2003. Learn-\ning extraction patterns for subjective\nexpressions. EMNLP .\nRitter, A., C. Cherry, and B. Dolan.\n2010a. Unsupervised modeling of\ntwitter conversations. NAACL HLT .\nRitter, A., O. Etzioni, and Mausam.\n2010b. A latent dirichlet allocation\nmethod for selectional preferences.\nACL.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 980,
      "token_count": 656,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 584",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 981,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "576 Bibliography\nRitter, A., L. Zettlemoyer, Mausam, and\nO. Etzioni. 2013. Modeling miss-\ning data in distant supervision for in-\nformation extraction. TACL , 1:367\u2013\n378.\nRoberts, A., C. Raffel, and N. Shazeer.\n2020. How much knowledge can\nyou pack into the parameters of a\nlanguage model? EMNLP .\nRobertson, S., S. Walker, S. Jones,\nM. M. Hancock-Beaulieu, and\nM. Gatford. 1995. Okapi at TREC-3.\nOverview of the Third Text REtrieval\nConference (TREC-3) .\nRobinson, T. and F. Fallside. 1991.\nA recurrent error propagation net-\nwork speech recognition system.\nComputer Speech & Language ,\n5(3):259\u2013274.\nRobinson, T., M. Hochberg, and S. Re-\nnals. 1996. The use of recurrent neu-\nral networks in continuous speech\nrecognition. In C.-H. Lee, F. K.\nSoong, and K. K. Paliwal, eds, Au-\ntomatic speech and speaker recogni-\ntion, 233\u2013258. Springer.\nRogers, A., M. Gardner, and I. Au-\ngenstein. 2023. QA dataset explo-\nsion: A taxonomy of NLP resources\nfor question answering and reading\ncomprehension. ACM Computing\nSurveys , 55(10):1\u201345.\nRohde, D. L. T., L. M. Gonnerman, and\nD. C. Plaut. 2006. An improved\nmodel of semantic similarity based\non lexical co-occurrence. CACM ,\n8:627\u2013633.\nRoller, S., E. Dinan, N. Goyal, D. Ju,\nM. Williamson, Y . Liu, J. Xu,\nM. Ott, E. M. Smith, Y .-L. Boureau,\nand J. Weston. 2021. Recipes for\nbuilding an open-domain chatbot.\nEACL .\nRooth, M., S. Riezler, D. Prescher,\nG. Carroll, and F. Beil. 1999. Induc-\ning a semantically annotated lexicon\nvia EM-based clustering. ACL.\nRosenblatt, F. 1958. The percep-\ntron: A probabilistic model for in-\nformation storage and organization\nin the brain. Psychological review ,\n65(6):386\u2013408.\nRosenfeld, R. 1992. Adaptive Statis-\ntical Language Modeling: A Maxi-\nmum Entropy Approach . Ph.D. the-\nsis, Carnegie Mellon University.\nRosenfeld, R. 1996. A maximum en-\ntropy approach to adaptive statisti-\ncal language modeling. Computer\nSpeech and Language , 10:187\u2013228.\nRosenthal, S. and K. McKeown. 2017.\nDetecting in\ufb02uencers in multiple on-\nline genres. ACM Transactions on\nInternet Technology (TOIT) , 17(2).\nRothe, S., S. Ebert, and H. Sch \u00a8utze.\n2016. Ultradense Word Embed-\ndings by Orthogonal Transforma-\ntion. NAACL HLT .Roy, N., J. Pineau, and S. Thrun. 2000.\nSpoken dialogue management using",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 982,
      "token_count": 748,
      "chapter_title": ""
    }
  },
  {
    "content": "G. Carroll, and F. Beil. 1999. Induc-\ning a semantically annotated lexicon\nvia EM-based clustering. ACL.\nRosenblatt, F. 1958. The percep-\ntron: A probabilistic model for in-\nformation storage and organization\nin the brain. Psychological review ,\n65(6):386\u2013408.\nRosenfeld, R. 1992. Adaptive Statis-\ntical Language Modeling: A Maxi-\nmum Entropy Approach . Ph.D. the-\nsis, Carnegie Mellon University.\nRosenfeld, R. 1996. A maximum en-\ntropy approach to adaptive statisti-\ncal language modeling. Computer\nSpeech and Language , 10:187\u2013228.\nRosenthal, S. and K. McKeown. 2017.\nDetecting in\ufb02uencers in multiple on-\nline genres. ACM Transactions on\nInternet Technology (TOIT) , 17(2).\nRothe, S., S. Ebert, and H. Sch \u00a8utze.\n2016. Ultradense Word Embed-\ndings by Orthogonal Transforma-\ntion. NAACL HLT .Roy, N., J. Pineau, and S. Thrun. 2000.\nSpoken dialogue management using\nprobabilistic reasoning. ACL.\nRudinger, R., J. Naradowsky,\nB. Leonard, and B. Van Durme.\n2018. Gender bias in coreference\nresolution. NAACL HLT .\nRumelhart, D. E., G. E. Hinton, and\nR. J. Williams. 1986. Learning in-\nternal representations by error prop-\nagation. In D. E. Rumelhart and\nJ. L. McClelland, eds, Parallel Dis-\ntributed Processing , volume 2, 318\u2013\n362. MIT Press.\nRumelhart, D. E. and J. L. McClelland.\n1986a. On learning the past tense of\nEnglish verbs. In D. E. Rumelhart\nand J. L. McClelland, eds, Parallel\nDistributed Processing , volume 2,\n216\u2013271. MIT Press.\nRumelhart, D. E. and J. L. McClelland,\neds. 1986b. Parallel Distributed\nProcessing . MIT Press.\nRumelhart, D. E. and A. A. Abraham-\nson. 1973. A model for analogi-\ncal reasoning. Cognitive Psychol-\nogy, 5(1):1\u201328.\nRumelhart, D. E. and J. L. McClelland,\neds. 1986c. Parallel Distributed\nProcessing: Explorations in the Mi-\ncrostructure of Cognition , volume\n1:Foundations . MIT Press.\nRuppenhofer, J., M. Ellsworth, M. R. L.\nPetruck, C. R. Johnson, C. F. Baker,\nand J. Scheffczyk. 2016. FrameNet\nII: Extended theory and practice.\nRuppenhofer, J., C. Sporleder,\nR. Morante, C. F. Baker, and\nM. Palmer. 2010. Semeval-2010\ntask 10: Linking events and their\nparticipants in discourse. 5th In-\nternational Workshop on Semantic\nEvaluation .\nRussell, J. A. 1980. A circum-\nplex model of affect. Journal of\npersonality and social psychology ,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 983,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "eds. 1986b. Parallel Distributed\nProcessing . MIT Press.\nRumelhart, D. E. and A. A. Abraham-\nson. 1973. A model for analogi-\ncal reasoning. Cognitive Psychol-\nogy, 5(1):1\u201328.\nRumelhart, D. E. and J. L. McClelland,\neds. 1986c. Parallel Distributed\nProcessing: Explorations in the Mi-\ncrostructure of Cognition , volume\n1:Foundations . MIT Press.\nRuppenhofer, J., M. Ellsworth, M. R. L.\nPetruck, C. R. Johnson, C. F. Baker,\nand J. Scheffczyk. 2016. FrameNet\nII: Extended theory and practice.\nRuppenhofer, J., C. Sporleder,\nR. Morante, C. F. Baker, and\nM. Palmer. 2010. Semeval-2010\ntask 10: Linking events and their\nparticipants in discourse. 5th In-\nternational Workshop on Semantic\nEvaluation .\nRussell, J. A. 1980. A circum-\nplex model of affect. Journal of\npersonality and social psychology ,\n39(6):1161\u20131178.\nRussell, S. and P. Norvig. 2002. Ar-\nti\ufb01cial Intelligence: A Modern Ap-\nproach , 2nd edition. Prentice Hall.\nRutherford, A. and N. Xue. 2015. Im-\nproving the inference of implicit dis-\ncourse relations via classifying ex-\nplicit discourse connectives. NAACL\nHLT.\nSachan, D. S., M. Lewis, D. Yo-\ngatama, L. Zettlemoyer, J. Pineau,\nand M. Zaheer. 2023. Questions are\nall you need to train a dense passage\nretriever. TACL , 11:600\u2013616.\nSacks, H., E. A. Schegloff, and G. Jef-\nferson. 1974. A simplest system-\natics for the organization of turn-\ntaking for conversation. Language ,\n50(4):696\u2013735.Sag, I. A. and M. Y . Liberman. 1975.\nThe intonational disambiguation of\nindirect speech acts. In CLS-75 ,\n487\u2013498. University of Chicago.\nSagae, K. 2009. Analysis of dis-\ncourse structure with syntactic de-\npendencies and data-driven shift-\nreduce parsing. IWPT-09 .\nSagawa, S., P. W. Koh, T. B.\nHashimoto, and P. Liang. 2020. Dis-\ntributionally robust neural networks\nfor group shifts: On the importance\nof regularization for worst-case gen-\neralization. ICLR .\nSagisaka, Y . 1988. Speech synthe-\nsis by rule using an optimal selec-\ntion of non-uniform synthesis units.\nICASSP .\nSagisaka, Y ., N. Kaiki, N. Iwahashi,\nand K. Mimura. 1992. Atr \u2013 n-talk\nspeech synthesis system. ICSLP .\nSahami, M., S. T. Dumais, D. Heck-\nerman, and E. Horvitz. 1998. A\nBayesian approach to \ufb01ltering junk",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 984,
      "token_count": 742,
      "chapter_title": ""
    }
  },
  {
    "content": "taking for conversation. Language ,\n50(4):696\u2013735.Sag, I. A. and M. Y . Liberman. 1975.\nThe intonational disambiguation of\nindirect speech acts. In CLS-75 ,\n487\u2013498. University of Chicago.\nSagae, K. 2009. Analysis of dis-\ncourse structure with syntactic de-\npendencies and data-driven shift-\nreduce parsing. IWPT-09 .\nSagawa, S., P. W. Koh, T. B.\nHashimoto, and P. Liang. 2020. Dis-\ntributionally robust neural networks\nfor group shifts: On the importance\nof regularization for worst-case gen-\neralization. ICLR .\nSagisaka, Y . 1988. Speech synthe-\nsis by rule using an optimal selec-\ntion of non-uniform synthesis units.\nICASSP .\nSagisaka, Y ., N. Kaiki, N. Iwahashi,\nand K. Mimura. 1992. Atr \u2013 n-talk\nspeech synthesis system. ICSLP .\nSahami, M., S. T. Dumais, D. Heck-\nerman, and E. Horvitz. 1998. A\nBayesian approach to \ufb01ltering junk\ne-mail. AAAI Workshop on Learning\nfor Text Categorization .\nSakoe, H. and S. Chiba. 1971. A\ndynamic programming approach to\ncontinuous speech recognition. Pro-\nceedings of the Seventh Interna-\ntional Congress on Acoustics , vol-\nume 3. Akad \u00b4emiai Kiad \u00b4o.\nSakoe, H. and S. Chiba. 1984. Dy-\nnamic programming algorithm opti-\nmization for spoken word recogni-\ntion. IEEE Transactions on ASSP ,\nASSP-26(1):43\u201349.\nSalomaa, A. 1969. Probabilistic and\nweighted grammars. Information\nand Control , 15:529\u2013544.\nSalton, G. 1971. The SMART Re-\ntrieval System: Experiments in Au-\ntomatic Document Processing . Pren-\ntice Hall.\nSalvetti, F., J. B. Lowe, and J. H. Mar-\ntin. 2016. A tangled web: The faint\nsignals of deception in text - boul-\nder lies and truth corpus (BLT-C).\nLREC .\nSampson, G. 1987. Alternative gram-\nmatical coding systems. In R. Gar-\nside, G. Leech, and G. Sampson,\neds, The Computational Analysis of\nEnglish , 165\u2013183. Longman.\nSankoff, D. and W. Labov. 1979. On the\nuses of variable rules. Language in\nsociety , 8(2-3):189\u2013222.\nSap, M., D. Card, S. Gabriel, Y . Choi,\nand N. A. Smith. 2019. The risk of\nracial bias in hate speech detection.\nACL.\nSap, M., M. C. Prasettio, A. Holtzman,\nH. Rashkin, and Y . Choi. 2017. Con-\nnotation frames of power and agency\nin modern \ufb01lms. EMNLP .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 985,
      "token_count": 714,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 585",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 986,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Bibliography 577\nSaur\u00b4\u0131, R., J. Littman, B. Knippen,\nR. Gaizauskas, A. Setzer, and\nJ. Pustejovsky. 2006. TimeML an-\nnotation guidelines version 1.2.1.\nManuscript.\nScha, R. and L. Polanyi. 1988. An\naugmented context free grammar for\ndiscourse. COLING .\nSchank, R. C. and R. P. Abelson. 1975.\nScripts, plans, and knowledge. Pro-\nceedings of IJCAI-75 .\nSchank, R. C. and R. P. Abelson. 1977.\nScripts, Plans, Goals and Under-\nstanding . Lawrence Erlbaum.\nSchegloff, E. A. 1968. Sequencing in\nconversational openings. American\nAnthropologist , 70:1075\u20131095.\nScherer, K. R. 2000. Psychological\nmodels of emotion. In J. C. Borod,\ned., The neuropsychology of emo-\ntion, 137\u2013162. Oxford.\nSchiebinger, L. 2013. Machine\ntranslation: Analyzing gender.\nhttp://genderedinnovations.\nstanford.edu/case-studies/\nnlp.html#tabs-2 .\nSchiebinger, L. 2014. Scienti\ufb01c re-\nsearch must take gender into ac-\ncount. Nature , 507(7490):9.\nSchluter, N. 2018. The word analogy\ntesting caveat. NAACL HLT .\nSchone, P. and D. Jurafsky. 2000.\nKnowlege-free induction of mor-\nphology using latent semantic anal-\nysis. CoNLL .\nSchone, P. and D. Jurafsky. 2001a. Is\nknowledge-free induction of multi-\nword unit dictionary headwords a\nsolved problem? EMNLP .\nSchone, P. and D. Jurafsky. 2001b.\nKnowledge-free induction of in\ufb02ec-\ntional morphologies. NAACL .\nSchuster, M. and K. Nakajima. 2012.\nJapanese and Korean voice search.\nICASSP .\nSchuster, M. and K. K. Paliwal. 1997.\nBidirectional recurrent neural net-\nworks. IEEE Transactions on Signal\nProcessing , 45:2673\u20132681.\nSch\u00a8utze, H. 1992a. Context space.\nAAAI Fall Symposium on Proba-\nbilistic Approaches to Natural Lan-\nguage .\nSch\u00a8utze, H. 1992b. Dimensions of\nmeaning. Proceedings of Supercom-\nputing \u201992 . IEEE Press.\nSch\u00a8utze, H. 1997. Ambiguity Resolu-\ntion in Language Learning \u2013 Com-\nputational and Cognitive Models .\nCSLI, Stanford, CA.\nSch\u00a8utze, H., D. A. Hull, and J. Peder-\nsen. 1995. A comparison of clas-\nsi\ufb01ers and document representations\nfor the routing problem. SIGIR-95 .Sch\u00a8utze, H. and J. Pedersen. 1993. A\nvector model for syntagmatic and\nparadigmatic relatedness. 9th An-\nnual Conference of the UW Centre\nfor the New OED and Text Research .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 987,
      "token_count": 735,
      "chapter_title": ""
    }
  },
  {
    "content": "Schuster, M. and K. Nakajima. 2012.\nJapanese and Korean voice search.\nICASSP .\nSchuster, M. and K. K. Paliwal. 1997.\nBidirectional recurrent neural net-\nworks. IEEE Transactions on Signal\nProcessing , 45:2673\u20132681.\nSch\u00a8utze, H. 1992a. Context space.\nAAAI Fall Symposium on Proba-\nbilistic Approaches to Natural Lan-\nguage .\nSch\u00a8utze, H. 1992b. Dimensions of\nmeaning. Proceedings of Supercom-\nputing \u201992 . IEEE Press.\nSch\u00a8utze, H. 1997. Ambiguity Resolu-\ntion in Language Learning \u2013 Com-\nputational and Cognitive Models .\nCSLI, Stanford, CA.\nSch\u00a8utze, H., D. A. Hull, and J. Peder-\nsen. 1995. A comparison of clas-\nsi\ufb01ers and document representations\nfor the routing problem. SIGIR-95 .Sch\u00a8utze, H. and J. Pedersen. 1993. A\nvector model for syntagmatic and\nparadigmatic relatedness. 9th An-\nnual Conference of the UW Centre\nfor the New OED and Text Research .\nSch\u00a8utze, H. and Y . Singer. 1994. Part-\nof-speech tagging using a variable\nmemory Markov model. ACL.\nSchwartz, H. A., J. C. Eichstaedt,\nM. L. Kern, L. Dziurzynski, S. M.\nRamones, M. Agrawal, A. Shah,\nM. Kosinski, D. Stillwell, M. E. P.\nSeligman, and L. H. Ungar. 2013.\nPersonality, gender, and age in the\nlanguage of social media: The open-\nvocabulary approach. PloS one ,\n8(9):e73791.\nSchwenk, H. 2007. Continuous space\nlanguage models. Computer Speech\n& Language , 21(3):492\u2013518.\nSchwenk, H. 2018. Filtering and min-\ning parallel data in a joint multilin-\ngual space. ACL.\nSchwenk, H., D. Dechelotte, and J.-L.\nGauvain. 2006. Continuous space\nlanguage models for statistical ma-\nchine translation. COLING/ACL .\nSchwenk, H., G. Wenzek, S. Edunov,\nE. Grave, A. Joulin, and A. Fan.\n2021. CCMatrix: Mining billions\nof high-quality parallel sentences on\nthe web. ACL.\nS\u00b4eaghdha, D. O. 2010. Latent vari-\nable models of selectional prefer-\nence. ACL.\nSeddah, D., R. Tsarfaty, S. K \u00a8ubler,\nM. Candito, J. D. Choi, R. Farkas,\nJ. Foster, I. Goenaga, K. Gojenola,\nY . Goldberg, S. Green, N. Habash,\nM. Kuhlmann, W. Maier, J. Nivre,\nA. Przepi \u00b4orkowski, R. Roth,\nW. Seeker, Y . Versley, V . Vincze,\nM. Woli \u00b4nski, A. Wr \u00b4oblewska, and",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 988,
      "token_count": 743,
      "chapter_title": ""
    }
  },
  {
    "content": "ing parallel data in a joint multilin-\ngual space. ACL.\nSchwenk, H., D. Dechelotte, and J.-L.\nGauvain. 2006. Continuous space\nlanguage models for statistical ma-\nchine translation. COLING/ACL .\nSchwenk, H., G. Wenzek, S. Edunov,\nE. Grave, A. Joulin, and A. Fan.\n2021. CCMatrix: Mining billions\nof high-quality parallel sentences on\nthe web. ACL.\nS\u00b4eaghdha, D. O. 2010. Latent vari-\nable models of selectional prefer-\nence. ACL.\nSeddah, D., R. Tsarfaty, S. K \u00a8ubler,\nM. Candito, J. D. Choi, R. Farkas,\nJ. Foster, I. Goenaga, K. Gojenola,\nY . Goldberg, S. Green, N. Habash,\nM. Kuhlmann, W. Maier, J. Nivre,\nA. Przepi \u00b4orkowski, R. Roth,\nW. Seeker, Y . Versley, V . Vincze,\nM. Woli \u00b4nski, A. Wr \u00b4oblewska, and\nE. Villemonte de la Cl \u00b4ergerie.\n2013. Overview of the SPMRL\n2013 shared task: cross-framework\nevaluation of parsing morpholog-\nically rich languages. 4th Work-\nshop on Statistical Parsing of\nMorphologically-Rich Languages .\nSee, A., S. Roller, D. Kiela, and\nJ. Weston. 2019. What makes a\ngood conversation? how control-\nlable attributes affect human judg-\nments. NAACL HLT .\nSekine, S. and M. Collins. 1997.\nThe evalb software. http:\n//cs.nyu.edu/cs/projects/\nproteus/evalb .\nSellam, T., D. Das, and A. Parikh. 2020.\nBLEURT: Learning robust metrics\nfor text generation. ACL.\nSennrich, R., B. Haddow, and A. Birch.\n2016. Neural machine translation of\nrare words with subword units. ACL.\nSeo, M., A. Kembhavi, A. Farhadi, and\nH. Hajishirzi. 2017. Bidirectionalattention \ufb02ow for machine compre-\nhension. ICLR .\nShannon, C. E. 1948. A mathematical\ntheory of communication. Bell Sys-\ntem Technical Journal , 27(3):379\u2013\n423. Continued in the following vol-\nume.\nShannon, C. E. 1951. Prediction and en-\ntropy of printed English. Bell System\nTechnical Journal , 30:50\u201364.\nSheil, B. A. 1976. Observations on con-\ntext free parsing. SMIL: Statistical\nMethods in Linguistics , 1:71\u2013109.\nShen, J., R. Pang, R. J. Weiss,\nM. Schuster, N. Jaitly, Z. Yang,\nZ. Chen, Y . Zhang, Y . Wang,\nR. Skerry-Ryan, R. A. Saurous,\nY . Agiomyrgiannakis, and Y . Wu.\n2018. Natural TTS synthesis by con-\nditioning WaveNet on mel spectro-\ngram predictions. ICASSP .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 989,
      "token_count": 749,
      "chapter_title": ""
    }
  },
  {
    "content": "2016. Neural machine translation of\nrare words with subword units. ACL.\nSeo, M., A. Kembhavi, A. Farhadi, and\nH. Hajishirzi. 2017. Bidirectionalattention \ufb02ow for machine compre-\nhension. ICLR .\nShannon, C. E. 1948. A mathematical\ntheory of communication. Bell Sys-\ntem Technical Journal , 27(3):379\u2013\n423. Continued in the following vol-\nume.\nShannon, C. E. 1951. Prediction and en-\ntropy of printed English. Bell System\nTechnical Journal , 30:50\u201364.\nSheil, B. A. 1976. Observations on con-\ntext free parsing. SMIL: Statistical\nMethods in Linguistics , 1:71\u2013109.\nShen, J., R. Pang, R. J. Weiss,\nM. Schuster, N. Jaitly, Z. Yang,\nZ. Chen, Y . Zhang, Y . Wang,\nR. Skerry-Ryan, R. A. Saurous,\nY . Agiomyrgiannakis, and Y . Wu.\n2018. Natural TTS synthesis by con-\nditioning WaveNet on mel spectro-\ngram predictions. ICASSP .\nSheng, E., K.-W. Chang, P. Natarajan,\nand N. Peng. 2019. The woman\nworked as a babysitter: On biases in\nlanguage generation. EMNLP .\nShi, P. and J. Lin. 2019. Simple BERT\nmodels for relation extraction and\nsemantic role labeling. ArXiv.\nShi, W., S. Min, M. Yasunaga, M. Seo,\nR. James, M. Lewis, L. Zettlemoyer,\nand W.-t. Yih. 2023. REPLUG:\nRetrieval-augmented black-box lan-\nguage models. ArXiv preprint.\nShriberg, E., R. Bates, P. Taylor,\nA. Stolcke, D. Jurafsky, K. Ries,\nN. Coccaro, R. Martin, M. Meteer,\nand C. Van Ess-Dykema. 1998. Can\nprosody aid the automatic classi\ufb01ca-\ntion of dialog acts in conversational\nspeech? Language and Speech (Spe-\ncial Issue on Prosody and Conversa-\ntion) , 41(3-4):439\u2013487.\nSidner, C. L. 1979. Towards a compu-\ntational theory of de\ufb01nite anaphora\ncomprehension in English discourse.\nTechnical Report 537, MIT Arti\ufb01-\ncial Intelligence Laboratory, Cam-\nbridge, MA.\nSidner, C. L. 1983. Focusing in the\ncomprehension of de\ufb01nite anaphora.\nIn M. Brady and R. C. Berwick,\neds, Computational Models of Dis-\ncourse , 267\u2013330. MIT Press.\nSimmons, R. F. 1965. Answering En-\nglish questions by computer: A sur-\nvey. CACM , 8(1):53\u201370.\nSimmons, R. F. 1973. Semantic net-\nworks: Their computation and use\nfor understanding English sentences.\nIn R. C. Schank and K. M. Colby,\neds, Computer Models of Thought\nand Language , 61\u2013113. W.H. Free-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 990,
      "token_count": 751,
      "chapter_title": ""
    }
  },
  {
    "content": "and C. Van Ess-Dykema. 1998. Can\nprosody aid the automatic classi\ufb01ca-\ntion of dialog acts in conversational\nspeech? Language and Speech (Spe-\ncial Issue on Prosody and Conversa-\ntion) , 41(3-4):439\u2013487.\nSidner, C. L. 1979. Towards a compu-\ntational theory of de\ufb01nite anaphora\ncomprehension in English discourse.\nTechnical Report 537, MIT Arti\ufb01-\ncial Intelligence Laboratory, Cam-\nbridge, MA.\nSidner, C. L. 1983. Focusing in the\ncomprehension of de\ufb01nite anaphora.\nIn M. Brady and R. C. Berwick,\neds, Computational Models of Dis-\ncourse , 267\u2013330. MIT Press.\nSimmons, R. F. 1965. Answering En-\nglish questions by computer: A sur-\nvey. CACM , 8(1):53\u201370.\nSimmons, R. F. 1973. Semantic net-\nworks: Their computation and use\nfor understanding English sentences.\nIn R. C. Schank and K. M. Colby,\neds, Computer Models of Thought\nand Language , 61\u2013113. W.H. Free-\nman & Co.\nSimmons, R. F., S. Klein, and K. Mc-\nConlogue. 1964. Indexing and de-\npendency logic for answering En-\nglish questions. American Docu-\nmentation , 15(3):196\u2013204.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 991,
      "token_count": 337,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 586",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 992,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "578 Bibliography\nSimons, G. F. and C. D. Fennig.\n2018. Ethnologue: Languages of\nthe world, 21st edition. SIL Inter-\nnational.\nSingh, S. P., D. J. Litman, M. Kearns,\nand M. A. Walker. 2002. Optimiz-\ning dialogue management with re-\ninforcement learning: Experiments\nwith the NJFun system. JAIR ,\n16:105\u2013133.\nSingh, S., F. Vargus, D. D\u2019souza,\nB. F. Karlsson, A. Mahendiran,\nW.-Y . Ko, H. Shandilya, J. Pa-\ntel, D. Mataciunas, L. O\u2019Mahony,\nM. Zhang, R. Hettiarachchi, J. Wil-\nson, M. Machado, L. S. Moura,\nD. Krzemi \u00b4nski, H. Fadaei, I. Erg \u00a8un,\nI. Okoh, A. Alaagib, O. Mudan-\nnayake, Z. Alyafeai, V . M. Chien,\nS. Ruder, S. Guthikonda, E. A.\nAlghamdi, S. Gehrmann, N. Muen-\nnighoff, M. Bartolo, J. Kreutzer,\nA. \u00a8U\u00a8Ust\u00a8un, M. Fadaee, and\nS. Hooker. 2024. Aya dataset: An\nopen-access collection for multi-\nlingual instruction tuning. ArXiv\npreprint.\nSleator, D. and D. Temperley. 1993.\nParsing English with a link gram-\nmar. IWPT-93 .\nSloan, M. C. 2010. Aristotle\u2019s Nico-\nmachean Ethics as the original lo-\ncus for the Septem Circumstantiae.\nClassical Philology , 105(3):236\u2013\n251.\nSlobin, D. I. 1996. Two ways to\ntravel. In M. Shibatani and S. A.\nThompson, eds, Grammatical Con-\nstructions: Their Form and Mean-\ning, 195\u2013220. Clarendon Press.\nSmith, V . L. and H. H. Clark. 1993. On\nthe course of answering questions.\nJournal of Memory and Language ,\n32:25\u201338.\nSmolensky, P. 1988. On the proper\ntreatment of connectionism. Behav-\nioral and brain sciences , 11(1):1\u2013\n23.\nSmolensky, P. 1990. Tensor product\nvariable binding and the representa-\ntion of symbolic structures in con-\nnectionist systems. Arti\ufb01cial intel-\nligence , 46(1-2):159\u2013216.\nSnover, M., B. Dorr, R. Schwartz,\nL. Micciulla, and J. Makhoul. 2006.\nA study of translation edit rate with\ntargeted human annotation. AMTA-\n2006 .\nSnow, R., D. Jurafsky, and A. Y . Ng.\n2005. Learning syntactic patterns\nfor automatic hypernym discovery.\nNeurIPS .\nSocher, R., J. Bauer, C. D. Man-\nning, and A. Y . Ng. 2013. Pars-\ning with compositional vector gram-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 993,
      "token_count": 738,
      "chapter_title": ""
    }
  },
  {
    "content": "structions: Their Form and Mean-\ning, 195\u2013220. Clarendon Press.\nSmith, V . L. and H. H. Clark. 1993. On\nthe course of answering questions.\nJournal of Memory and Language ,\n32:25\u201338.\nSmolensky, P. 1988. On the proper\ntreatment of connectionism. Behav-\nioral and brain sciences , 11(1):1\u2013\n23.\nSmolensky, P. 1990. Tensor product\nvariable binding and the representa-\ntion of symbolic structures in con-\nnectionist systems. Arti\ufb01cial intel-\nligence , 46(1-2):159\u2013216.\nSnover, M., B. Dorr, R. Schwartz,\nL. Micciulla, and J. Makhoul. 2006.\nA study of translation edit rate with\ntargeted human annotation. AMTA-\n2006 .\nSnow, R., D. Jurafsky, and A. Y . Ng.\n2005. Learning syntactic patterns\nfor automatic hypernym discovery.\nNeurIPS .\nSocher, R., J. Bauer, C. D. Man-\nning, and A. Y . Ng. 2013. Pars-\ning with compositional vector gram-\nmars. ACL.Socher, R., C. C.-Y . Lin, A. Y . Ng, and\nC. D. Manning. 2011. Parsing natu-\nral scenes and natural language with\nrecursive neural networks. ICML .\nSoderland, S., D. Fisher, J. Aseltine,\nand W. G. Lehnert. 1995. CRYS-\nTAL: Inducing a conceptual dictio-\nnary. IJCAI-95 .\nS\u00f8gaard, A. 2010. Simple semi-\nsupervised training of part-of-\nspeech taggers. ACL.\nS\u00f8gaard, A. and Y . Goldberg. 2016.\nDeep multi-task learning with low\nlevel tasks supervised at lower lay-\ners.ACL.\nS\u00f8gaard, A., A. Johannsen, B. Plank,\nD. Hovy, and H. M. Alonso. 2014.\nWhat\u2019s in a p-value in NLP? CoNLL .\nSoldaini, L., R. Kinney, A. Bha-\ngia, D. Schwenk, D. Atkinson,\nR. Authur, B. Bogin, K. Chandu,\nJ. Dumas, Y . Elazar, V . Hofmann,\nA. H. Jha, S. Kumar, L. Lucy,\nX. Lyu, N. Lambert, I. Magnus-\nson, J. Morrison, N. Muennighoff,\nA. Naik, C. Nam, M. E. Pe-\nters, A. Ravichander, K. Richardson,\nZ. Shen, E. Strubell, N. Subramani,\nO. Tafjord, P. Walsh, L. Zettlemoyer,\nN. A. Smith, H. Hajishirzi, I. Belt-\nagy, D. Groeneveld, J. Dodge, and\nK. Lo. 2024. Dolma: An open cor-\npus of three trillion tokens for lan-\nguage model pretraining research.\nArXiv preprint.\nSolorio, T., E. Blair, S. Maharjan,\nS. Bethard, M. Diab, M. Ghoneim,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 994,
      "token_count": 740,
      "chapter_title": ""
    }
  },
  {
    "content": "What\u2019s in a p-value in NLP? CoNLL .\nSoldaini, L., R. Kinney, A. Bha-\ngia, D. Schwenk, D. Atkinson,\nR. Authur, B. Bogin, K. Chandu,\nJ. Dumas, Y . Elazar, V . Hofmann,\nA. H. Jha, S. Kumar, L. Lucy,\nX. Lyu, N. Lambert, I. Magnus-\nson, J. Morrison, N. Muennighoff,\nA. Naik, C. Nam, M. E. Pe-\nters, A. Ravichander, K. Richardson,\nZ. Shen, E. Strubell, N. Subramani,\nO. Tafjord, P. Walsh, L. Zettlemoyer,\nN. A. Smith, H. Hajishirzi, I. Belt-\nagy, D. Groeneveld, J. Dodge, and\nK. Lo. 2024. Dolma: An open cor-\npus of three trillion tokens for lan-\nguage model pretraining research.\nArXiv preprint.\nSolorio, T., E. Blair, S. Maharjan,\nS. Bethard, M. Diab, M. Ghoneim,\nA. Hawwari, F. AlGhamdi,\nJ. Hirschberg, A. Chang, and\nP. Fung. 2014. Overview for the\n\ufb01rst shared task on language iden-\nti\ufb01cation in code-switched data.\nWorkshop on Computational Ap-\nproaches to Code Switching .\nSomasundaran, S., J. Burstein, and\nM. Chodorow. 2014. Lexical chain-\ning for measuring discourse coher-\nence quality in test-taker essays.\nCOLING .\nSoon, W. M., H. T. Ng, and D. C. Y .\nLim. 2001. A machine learning ap-\nproach to coreference resolution of\nnoun phrases. Computational Lin-\nguistics , 27(4):521\u2013544.\nSoricut, R. and D. Marcu. 2003. Sen-\ntence level discourse parsing using\nsyntactic and lexical information.\nHLT-NAACL .\nSoricut, R. and D. Marcu. 2006.\nDiscourse generation using utility-\ntrained coherence models. COL-\nING/ACL .\nSorokin, D. and I. Gurevych. 2018.\nMixing context granularities for im-\nproved entity linking on question\nanswering data across entity cate-\ngories. *SEM .Sparck Jones, K. 1972. A statistical in-\nterpretation of term speci\ufb01city and\nits application in retrieval. Journal\nof Documentation , 28(1):11\u201321.\nSparck Jones, K. 1986. Synonymy and\nSemantic Classi\ufb01cation . Edinburgh\nUniversity Press, Edinburgh. Repub-\nlication of 1964 PhD Thesis.\nSporleder, C. and A. Lascarides. 2005.\nExploiting linguistic cues to classify\nrhetorical relations. RANLP-05 .\nSporleder, C. and M. Lapata. 2005. Dis-\ncourse chunking and its application\nto sentence compression. EMNLP .\nSproat, R., A. W. Black, S. F.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 995,
      "token_count": 737,
      "chapter_title": ""
    }
  },
  {
    "content": "tence level discourse parsing using\nsyntactic and lexical information.\nHLT-NAACL .\nSoricut, R. and D. Marcu. 2006.\nDiscourse generation using utility-\ntrained coherence models. COL-\nING/ACL .\nSorokin, D. and I. Gurevych. 2018.\nMixing context granularities for im-\nproved entity linking on question\nanswering data across entity cate-\ngories. *SEM .Sparck Jones, K. 1972. A statistical in-\nterpretation of term speci\ufb01city and\nits application in retrieval. Journal\nof Documentation , 28(1):11\u201321.\nSparck Jones, K. 1986. Synonymy and\nSemantic Classi\ufb01cation . Edinburgh\nUniversity Press, Edinburgh. Repub-\nlication of 1964 PhD Thesis.\nSporleder, C. and A. Lascarides. 2005.\nExploiting linguistic cues to classify\nrhetorical relations. RANLP-05 .\nSporleder, C. and M. Lapata. 2005. Dis-\ncourse chunking and its application\nto sentence compression. EMNLP .\nSproat, R., A. W. Black, S. F.\nChen, S. Kumar, M. Ostendorf, and\nC. Richards. 2001. Normalization\nof non-standard words. Computer\nSpeech & Language , 15(3):287\u2013\n333.\nSproat, R. and K. Gorman. 2018. A\nbrief summary of the Kaggle text\nnormalization challenge.\nSrivastava, N., G. E. Hinton,\nA. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov. 2014. Dropout:\na simple way to prevent neural net-\nworks from over\ufb01tting. JMLR ,\n15(1):1929\u20131958.\nStab, C. and I. Gurevych. 2014a. Anno-\ntating argument components and re-\nlations in persuasive essays. COL-\nING.\nStab, C. and I. Gurevych. 2014b. Identi-\nfying argumentative discourse struc-\ntures in persuasive essays. EMNLP .\nStab, C. and I. Gurevych. 2017. Parsing\nargumentation structures in persua-\nsive essays. Computational Linguis-\ntics, 43(3):619\u2013659.\nStalnaker, R. C. 1978. Assertion. In\nP. Cole, ed., Pragmatics: Syntax and\nSemantics Volume 9 , 315\u2013332. Aca-\ndemic Press.\nStamatatos, E. 2009. A survey of mod-\nern authorship attribution methods.\nJASIST , 60(3):538\u2013556.\nStanovsky, G., N. A. Smith, and\nL. Zettlemoyer. 2019. Evaluating\ngender bias in machine translation.\nACL.\nStede, M. 2011. Discourse processing .\nMorgan & Claypool.\nStede, M. and J. Schneider. 2018. Argu-\nmentation Mining . Morgan & Clay-\npool.\nStern, M., J. Andreas, and D. Klein.\n2017. A minimal span-based neural\nconstituency parser. ACL.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 996,
      "token_count": 738,
      "chapter_title": ""
    }
  },
  {
    "content": "ING.\nStab, C. and I. Gurevych. 2014b. Identi-\nfying argumentative discourse struc-\ntures in persuasive essays. EMNLP .\nStab, C. and I. Gurevych. 2017. Parsing\nargumentation structures in persua-\nsive essays. Computational Linguis-\ntics, 43(3):619\u2013659.\nStalnaker, R. C. 1978. Assertion. In\nP. Cole, ed., Pragmatics: Syntax and\nSemantics Volume 9 , 315\u2013332. Aca-\ndemic Press.\nStamatatos, E. 2009. A survey of mod-\nern authorship attribution methods.\nJASIST , 60(3):538\u2013556.\nStanovsky, G., N. A. Smith, and\nL. Zettlemoyer. 2019. Evaluating\ngender bias in machine translation.\nACL.\nStede, M. 2011. Discourse processing .\nMorgan & Claypool.\nStede, M. and J. Schneider. 2018. Argu-\nmentation Mining . Morgan & Clay-\npool.\nStern, M., J. Andreas, and D. Klein.\n2017. A minimal span-based neural\nconstituency parser. ACL.\nStevens, K. N., S. Kasowski, and G. M.\nFant. 1953. An electrical analog of\nthe vocal tract. JASA , 25(4):734\u2013\n742.\nStevens, S. S. and J. V olkmann. 1940.\nThe relation of pitch to frequency: A\nrevised scale. The American Journal\nof Psychology , 53(3):329\u2013353.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 997,
      "token_count": 373,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 587",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 998,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Bibliography 579\nStevens, S. S., J. V olkmann, and E. B.\nNewman. 1937. A scale for the mea-\nsurement of the psychological mag-\nnitude pitch. JASA , 8:185\u2013190.\nStifelman, L. J., B. Arons,\nC. Schmandt, and E. A. Hulteen.\n1993. V oiceNotes: A speech inter-\nface for a hand-held voice notetaker.\nINTERCHI 1993 .\nStolcke, A. 1998. Entropy-based prun-\ning of backoff language models.\nProc. DARPA Broadcast News Tran-\nscription and Understanding Work-\nshop .\nStolcke, A. 2002. SRILM \u2013 an exten-\nsible language modeling toolkit. IC-\nSLP.\nStolcke, A., Y . Konig, and M. Wein-\ntraub. 1997. Explicit word error min-\nimization in N-best list rescoring.\nEUROSPEECH , volume 1.\nStolcke, A., K. Ries, N. Coccaro,\nE. Shriberg, R. Bates, D. Jurafsky,\nP. Taylor, R. Martin, M. Meteer,\nand C. Van Ess-Dykema. 2000. Di-\nalogue act modeling for automatic\ntagging and recognition of conversa-\ntional speech. Computational Lin-\nguistics , 26(3):339\u2013371.\nStolz, W. S., P. H. Tannenbaum, and\nF. V . Carstensen. 1965. A stochastic\napproach to the grammatical coding\nof English. CACM , 8(6):399\u2013405.\nStone, P., D. Dunphry, M. Smith, and\nD. Ogilvie. 1966. The General In-\nquirer: A Computer Approach to\nContent Analysis . MIT Press.\nStr\u00a8otgen, J. and M. Gertz. 2013. Mul-\ntilingual and cross-domain temporal\ntagging. Language Resources and\nEvaluation , 47(2):269\u2013298.\nStrube, M. and U. Hahn. 1996. Func-\ntional centering. ACL.\nStrubell, E., A. Ganesh, and A. McCal-\nlum. 2019. Energy and policy con-\nsiderations for deep learning in NLP.\nACL.\nSu, Y ., H. Sun, B. Sadler, M. Srivatsa,\nI. G\u00a8ur, Z. Yan, and X. Yan. 2016. On\ngenerating characteristic-rich ques-\ntion sets for QA evaluation. EMNLP .\nSubba, R. and B. Di Eugenio. 2009. An\neffective discourse parser that uses\nrich linguistic information. NAACL\nHLT.\nSukhbaatar, S., A. Szlam, J. Weston,\nand R. Fergus. 2015. End-to-end\nmemory networks. NeurIPS .\nSundheim, B., ed. 1991. Proceedings of\nMUC-3 .\nSundheim, B., ed. 1992. Proceedings of\nMUC-4 .\nSundheim, B., ed. 1993. Proceedings of\nMUC-5 . Baltimore, MD.\nSundheim, B., ed. 1995. Proceedings of",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 999,
      "token_count": 744,
      "chapter_title": ""
    }
  },
  {
    "content": "Strube, M. and U. Hahn. 1996. Func-\ntional centering. ACL.\nStrubell, E., A. Ganesh, and A. McCal-\nlum. 2019. Energy and policy con-\nsiderations for deep learning in NLP.\nACL.\nSu, Y ., H. Sun, B. Sadler, M. Srivatsa,\nI. G\u00a8ur, Z. Yan, and X. Yan. 2016. On\ngenerating characteristic-rich ques-\ntion sets for QA evaluation. EMNLP .\nSubba, R. and B. Di Eugenio. 2009. An\neffective discourse parser that uses\nrich linguistic information. NAACL\nHLT.\nSukhbaatar, S., A. Szlam, J. Weston,\nand R. Fergus. 2015. End-to-end\nmemory networks. NeurIPS .\nSundheim, B., ed. 1991. Proceedings of\nMUC-3 .\nSundheim, B., ed. 1992. Proceedings of\nMUC-4 .\nSundheim, B., ed. 1993. Proceedings of\nMUC-5 . Baltimore, MD.\nSundheim, B., ed. 1995. Proceedings of\nMUC-6 .Surdeanu, M. 2013. Overview of the\nTAC2013 Knowledge Base Popula-\ntion evaluation: English slot \ufb01lling\nand temporal slot \ufb01lling. TAC-13 .\nSurdeanu, M., S. Harabagiu,\nJ. Williams, and P. Aarseth. 2003.\nUsing predicate-argument structures\nfor information extraction. ACL.\nSurdeanu, M., T. Hicks, and M. A.\nValenzuela-Escarcega. 2015. Two\npractical rhetorical structure theory\nparsers. NAACL HLT .\nSurdeanu, M., R. Johansson, A. Mey-\ners, L. M `arquez, and J. Nivre. 2008.\nThe CoNLL 2008 shared task on\njoint parsing of syntactic and seman-\ntic dependencies. CoNLL .\nSutskever, I., O. Vinyals, and Q. V . Le.\n2014. Sequence to sequence learn-\ning with neural networks. NeurIPS .\nSuzgun, M., L. Melas-Kyriazi, and\nD. Jurafsky. 2023a. Follow the wis-\ndom of the crowd: Effective text\ngeneration via minimum Bayes risk\ndecoding. Findings of ACL 2023 .\nSuzgun, M., N. Scales, N. Sch \u00a8arli,\nS. Gehrmann, Y . Tay, H. W. Chung,\nA. Chowdhery, Q. Le, E. Chi,\nD. Zhou, and J. Wei. 2023b.\nChallenging BIG-bench tasks and\nwhether chain-of-thought can solve\nthem. ACL Findings .\nSwerts, M., D. J. Litman, and J. Hirsch-\nberg. 2000. Corrections in spoken\ndialogue systems. ICSLP .\nSwier, R. and S. Stevenson. 2004. Un-\nsupervised semantic role labelling.\nEMNLP .\nSwitzer, P. 1965. Vector images in doc-\nument retrieval. Statistical Associa-\ntion Methods For Mechanized Docu-\nmentation. Symposium Proceedings.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1000,
      "token_count": 751,
      "chapter_title": ""
    }
  },
  {
    "content": "Sutskever, I., O. Vinyals, and Q. V . Le.\n2014. Sequence to sequence learn-\ning with neural networks. NeurIPS .\nSuzgun, M., L. Melas-Kyriazi, and\nD. Jurafsky. 2023a. Follow the wis-\ndom of the crowd: Effective text\ngeneration via minimum Bayes risk\ndecoding. Findings of ACL 2023 .\nSuzgun, M., N. Scales, N. Sch \u00a8arli,\nS. Gehrmann, Y . Tay, H. W. Chung,\nA. Chowdhery, Q. Le, E. Chi,\nD. Zhou, and J. Wei. 2023b.\nChallenging BIG-bench tasks and\nwhether chain-of-thought can solve\nthem. ACL Findings .\nSwerts, M., D. J. Litman, and J. Hirsch-\nberg. 2000. Corrections in spoken\ndialogue systems. ICSLP .\nSwier, R. and S. Stevenson. 2004. Un-\nsupervised semantic role labelling.\nEMNLP .\nSwitzer, P. 1965. Vector images in doc-\nument retrieval. Statistical Associa-\ntion Methods For Mechanized Docu-\nmentation. Symposium Proceedings.\nWashington, D.C., USA, March 17,\n1964 .https://nvlpubs.nist.\ngov/nistpubs/Legacy/MP/\nnbsmiscellaneouspub269.pdf .\nSyrdal, A. K., C. W. Wightman,\nA. Conkie, Y . Stylianou, M. Beut-\nnagel, J. Schroeter, V . Strom, and\nK.-S. Lee. 2000. Corpus-based\ntechniques in the AT&T NEXTGEN\nsynthesis system. ICSLP .\nTalmy, L. 1985. Lexicalization patterns:\nSemantic structure in lexical forms.\nIn T. Shopen, ed., Language Typol-\nogy and Syntactic Description, Vol-\nume 3 . Cambridge University Press.\nOriginally appeared as UC Berkeley\nCognitive Science Program Report\nNo. 30, 1980.\nTalmy, L. 1991. Path to realization: A\ntypology of event con\ufb02ation. BLS-\n91.\nTan, C., V . Niculae, C. Danescu-\nNiculescu-Mizil, and L. Lee. 2016.\nWinning arguments: Interaction dy-\nnamics and persuasion strategiesin good-faith online discussions.\nWWW-16 .\nTannen, D. 1979. What\u2019s in a frame?\nSurface evidence for underlying ex-\npectations. In R. Freedle, ed., New\nDirections in Discourse Processing ,\n137\u2013181. Ablex.\nTaylor, P. 2009. Text-to-Speech Synthe-\nsis. Cambridge University Press.\nTaylor, W. L. 1953. Cloze procedure: A\nnew tool for measuring readability.\nJournalism Quarterly , 30:415\u2013433.\nTeranishi, R. and N. Umeda. 1968. Use\nof pronouncing dictionary in speech\nsynthesis experiments. 6th Interna-\ntional Congress on Acoustics .\nTesni `ere, L. 1959. \u00b4El\u00b4ements de Syntaxe\nStructurale . Librairie C. Klinck-\nsieck, Paris.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1001,
      "token_count": 739,
      "chapter_title": ""
    }
  },
  {
    "content": "No. 30, 1980.\nTalmy, L. 1991. Path to realization: A\ntypology of event con\ufb02ation. BLS-\n91.\nTan, C., V . Niculae, C. Danescu-\nNiculescu-Mizil, and L. Lee. 2016.\nWinning arguments: Interaction dy-\nnamics and persuasion strategiesin good-faith online discussions.\nWWW-16 .\nTannen, D. 1979. What\u2019s in a frame?\nSurface evidence for underlying ex-\npectations. In R. Freedle, ed., New\nDirections in Discourse Processing ,\n137\u2013181. Ablex.\nTaylor, P. 2009. Text-to-Speech Synthe-\nsis. Cambridge University Press.\nTaylor, W. L. 1953. Cloze procedure: A\nnew tool for measuring readability.\nJournalism Quarterly , 30:415\u2013433.\nTeranishi, R. and N. Umeda. 1968. Use\nof pronouncing dictionary in speech\nsynthesis experiments. 6th Interna-\ntional Congress on Acoustics .\nTesni `ere, L. 1959. \u00b4El\u00b4ements de Syntaxe\nStructurale . Librairie C. Klinck-\nsieck, Paris.\nTetreault, J. R. 2001. A corpus-based\nevaluation of centering and pronoun\nresolution. Computational Linguis-\ntics, 27(4):507\u2013520.\nTeufel, S., J. Carletta, and M. Moens.\n1999. An annotation scheme for\ndiscourse-level argumentation in re-\nsearch articles. EACL .\nTeufel, S., A. Siddharthan, and\nC. Batchelor. 2009. Towards\ndomain-independent argumenta-\ntive zoning: Evidence from chem-\nistry and computational linguistics.\nEMNLP .\nThede, S. M. and M. P. Harper. 1999. A\nsecond-order hidden Markov model\nfor part-of-speech tagging. ACL.\nThompson, B. and P. Koehn. 2019. Ve-\ncalign: Improved sentence align-\nment in linear time and space.\nEMNLP .\nThompson, K. 1968. Regular ex-\npression search algorithm. CACM ,\n11(6):419\u2013422.\nTian, Y ., V . Kulkarni, B. Perozzi,\nand S. Skiena. 2016. On the\nconvergent properties of word em-\nbedding methods. ArXiv preprint\narXiv:1605.03956.\nTibshirani, R. J. 1996. Regression\nshrinkage and selection via the lasso.\nJournal of the Royal Statistical So-\nciety. Series B (Methodological) ,\n58(1):267\u2013288.\nTimkey, W. and M. van Schijndel. 2021.\nAll bark and no bite: Rogue dimen-\nsions in transformer language mod-\nels obscure representational quality.\nEMNLP .\nTitov, I. and E. Khoddam. 2014. Unsu-\npervised induction of semantic roles\nwithin a reconstruction-error mini-\nmization framework. NAACL HLT .\nTitov, I. and A. Klementiev. 2012. A\nBayesian approach to unsupervised\nsemantic role induction. EACL .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1002,
      "token_count": 739,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 588",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1003,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "580 Bibliography\nTomkins, S. S. 1962. Affect, imagery,\nconsciousness: Vol. I. The positive\naffects . Springer.\nToutanova, K., D. Klein, C. D. Man-\nning, and Y . Singer. 2003. Feature-\nrich part-of-speech tagging with a\ncyclic dependency network. HLT-\nNAACL .\nTrichelair, P., A. Emami, J. C. K.\nCheung, A. Trischler, K. Suleman,\nand F. Diaz. 2018. On the eval-\nuation of common-sense reasoning\nin natural language understanding.\nNeurIPS 2018 Workshop on Cri-\ntiquing and Correcting Trends in\nMachine Learning .\nTrnka, K., D. Yarrington, J. McCaw,\nK. F. McCoy, and C. Pennington.\n2007. The effects of word pre-\ndiction on communication rate for\nAAC. NAACL-HLT .\nTurian, J. P., L. Shen, and I. D. Mela-\nmed. 2003. Evaluation of machine\ntranslation and its evaluation. Pro-\nceedings of MT Summit IX .\nTurian, J., L. Ratinov, and Y . Bengio.\n2010. Word representations: a sim-\nple and general method for semi-\nsupervised learning. ACL.\nTurney, P. D. 2002. Thumbs up or\nthumbs down? Semantic orienta-\ntion applied to unsupervised classi-\n\ufb01cation of reviews. ACL.\nTurney, P. D. and M. Littman. 2003.\nMeasuring praise and criticism: In-\nference of semantic orientation from\nassociation. ACM Transactions\non Information Systems (TOIS) ,\n21:315\u2013346.\nTurney, P. D. and M. L. Littman. 2005.\nCorpus-based learning of analogies\nand semantic relations. Machine\nLearning , 60(1-3):251\u2013278.\nUmeda, N. 1976. Linguistic rules for\ntext-to-speech synthesis. Proceed-\nings of the IEEE , 64(4):443\u2013451.\nUmeda, N., E. Matui, T. Suzuki, and\nH. Omura. 1968. Synthesis of fairy\ntale using an analog vocal tract. 6th\nInternational Congress on Acous-\ntics.\nUng, M., J. Xu, and Y .-L. Boureau.\n2022. SaFeRDialogues: Taking\nfeedback gracefully after conversa-\ntional safety failures. ACL.\nUryupina, O., R. Artstein, A. Bristot,\nF. Cavicchio, F. Delogu, K. J. Ro-\ndriguez, and M. Poesio. 2020. An-\nnotating a broad range of anaphoric\nphenomena, in a variety of genres:\nThe ARRAU corpus. Natural Lan-\nguage Engineering , 26(1):1\u201334.\nUszkoreit, J. 2017. Transformer: A\nnovel neural network architecture\nfor language understanding. Google\nResearch blog post, Thursday Au-\ngust 31, 2017.van Deemter, K. and R. Kibble.\n2000. On coreferring: corefer-\nence in MUC and related annotation\nschemes. Computational Linguis-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1004,
      "token_count": 743,
      "chapter_title": ""
    }
  },
  {
    "content": "ings of the IEEE , 64(4):443\u2013451.\nUmeda, N., E. Matui, T. Suzuki, and\nH. Omura. 1968. Synthesis of fairy\ntale using an analog vocal tract. 6th\nInternational Congress on Acous-\ntics.\nUng, M., J. Xu, and Y .-L. Boureau.\n2022. SaFeRDialogues: Taking\nfeedback gracefully after conversa-\ntional safety failures. ACL.\nUryupina, O., R. Artstein, A. Bristot,\nF. Cavicchio, F. Delogu, K. J. Ro-\ndriguez, and M. Poesio. 2020. An-\nnotating a broad range of anaphoric\nphenomena, in a variety of genres:\nThe ARRAU corpus. Natural Lan-\nguage Engineering , 26(1):1\u201334.\nUszkoreit, J. 2017. Transformer: A\nnovel neural network architecture\nfor language understanding. Google\nResearch blog post, Thursday Au-\ngust 31, 2017.van Deemter, K. and R. Kibble.\n2000. On coreferring: corefer-\nence in MUC and related annotation\nschemes. Computational Linguis-\ntics, 26(4):629\u2013637.\nvan der Maaten, L. and G. E. Hinton.\n2008. Visualizing high-dimensional\ndata using t-SNE. JMLR , 9:2579\u2013\n2605.\nvan Rijsbergen, C. J. 1975. Information\nRetrieval . Butterworths.\nVaswani, A., N. Shazeer, N. Parmar,\nJ. Uszkoreit, L. Jones, A. N. Gomez,\n\u0141. Kaiser, and I. Polosukhin. 2017.\nAttention is all you need. NeurIPS .\nVauquois, B. 1968. A survey of for-\nmal grammars and algorithms for\nrecognition and transformation in\nmachine translation. IFIP Congress\n1968 .\nVelichko, V . M. and N. G. Zagoruyko.\n1970. Automatic recognition of\n200 words. International Journal of\nMan-Machine Studies , 2:223\u2013234.\nVelikovich, L., S. Blair-Goldensohn,\nK. Hannan, and R. McDonald. 2010.\nThe viability of web-derived polarity\nlexicons. NAACL HLT .\nVendler, Z. 1967. Linguistics in Philos-\nophy . Cornell University Press.\nVerhagen, M., R. Gaizauskas,\nF. Schilder, M. Hepple, J. Moszkow-\nicz, and J. Pustejovsky. 2009. The\nTempEval challenge: Identifying\ntemporal relations in text. Lan-\nguage Resources and Evaluation ,\n43(2):161\u2013179.\nVerhagen, M., I. Mani, R. Sauri,\nR. Knippen, S. B. Jang, J. Littman,\nA. Rumshisky, J. Phillips, and\nJ. Pustejovsky. 2005. Automating\ntemporal annotation with TARSQI.\nACL.\nVersley, Y . 2008. Vagueness and ref-\nerential ambiguity in a large-scale\nannotated corpus. Research on",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1005,
      "token_count": 752,
      "chapter_title": ""
    }
  },
  {
    "content": "1970. Automatic recognition of\n200 words. International Journal of\nMan-Machine Studies , 2:223\u2013234.\nVelikovich, L., S. Blair-Goldensohn,\nK. Hannan, and R. McDonald. 2010.\nThe viability of web-derived polarity\nlexicons. NAACL HLT .\nVendler, Z. 1967. Linguistics in Philos-\nophy . Cornell University Press.\nVerhagen, M., R. Gaizauskas,\nF. Schilder, M. Hepple, J. Moszkow-\nicz, and J. Pustejovsky. 2009. The\nTempEval challenge: Identifying\ntemporal relations in text. Lan-\nguage Resources and Evaluation ,\n43(2):161\u2013179.\nVerhagen, M., I. Mani, R. Sauri,\nR. Knippen, S. B. Jang, J. Littman,\nA. Rumshisky, J. Phillips, and\nJ. Pustejovsky. 2005. Automating\ntemporal annotation with TARSQI.\nACL.\nVersley, Y . 2008. Vagueness and ref-\nerential ambiguity in a large-scale\nannotated corpus. Research on\nLanguage and Computation , 6(3-\n4):333\u2013353.\nVieira, R. and M. Poesio. 2000. An em-\npirically based system for process-\ning de\ufb01nite descriptions. Computa-\ntional Linguistics , 26(4):539\u2013593.\nVilain, M., J. D. Burger, J. Aberdeen,\nD. Connolly, and L. Hirschman.\n1995. A model-theoretic coreference\nscoring scheme. MUC-6 .\nVintsyuk, T. K. 1968. Speech discrim-\nination by dynamic programming.\nCybernetics , 4(1):52\u201357. Origi-\nnal Russian: Kibernetika 4(1):81-\n88. 1968.\nVinyals, O., \u0141. Kaiser, T. Koo,\nS. Petrov, I. Sutskever, and G. Hin-\nton. 2015. Grammar as a foreign lan-\nguage. NeurIPS .V oorhees, E. M. 1999. TREC-8 ques-\ntion answering track report. Pro-\nceedings of the 8th Text Retrieval\nConference .\nV oorhees, E. M. and D. K. Harman.\n2005. TREC: Experiment and\nEvaluation in Information Retrieval .\nMIT Press.\nV outilainen, A. 1999. Handcrafted\nrules. In H. van Halteren, ed., Syn-\ntactic Wordclass Tagging , 217\u2013246.\nKluwer.\nVrande \u02c7ci\u00b4c, D. and M. Kr \u00a8otzsch. 2014.\nWikidata: a free collaborative\nknowledge base. CACM , 57(10):78\u2013\n85.\nWade, E., E. Shriberg, and P. J. Price.\n1992. User behaviors affecting\nspeech recognition. ICSLP .\nWagner, R. A. and M. J. Fischer. 1974.\nThe string-to-string correction prob-\nlem. Journal of the ACM , 21:168\u2013\n173.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1006,
      "token_count": 739,
      "chapter_title": ""
    }
  },
  {
    "content": "S. Petrov, I. Sutskever, and G. Hin-\nton. 2015. Grammar as a foreign lan-\nguage. NeurIPS .V oorhees, E. M. 1999. TREC-8 ques-\ntion answering track report. Pro-\nceedings of the 8th Text Retrieval\nConference .\nV oorhees, E. M. and D. K. Harman.\n2005. TREC: Experiment and\nEvaluation in Information Retrieval .\nMIT Press.\nV outilainen, A. 1999. Handcrafted\nrules. In H. van Halteren, ed., Syn-\ntactic Wordclass Tagging , 217\u2013246.\nKluwer.\nVrande \u02c7ci\u00b4c, D. and M. Kr \u00a8otzsch. 2014.\nWikidata: a free collaborative\nknowledge base. CACM , 57(10):78\u2013\n85.\nWade, E., E. Shriberg, and P. J. Price.\n1992. User behaviors affecting\nspeech recognition. ICSLP .\nWagner, R. A. and M. J. Fischer. 1974.\nThe string-to-string correction prob-\nlem. Journal of the ACM , 21:168\u2013\n173.\nWaibel, A., T. Hanazawa, G. Hin-\nton, K. Shikano, and K. J. Lang.\n1989. Phoneme recognition using\ntime-delay neural networks. IEEE\nTransactions on ASSP , 37(3):328\u2013\n339.\nWalker, M. A. 2000. An applica-\ntion of reinforcement learning to di-\nalogue strategy selection in a spo-\nken dialogue system for email. JAIR ,\n12:387\u2013416.\nWalker, M. A., J. C. Fromer, and S. S.\nNarayanan. 1998a. Learning optimal\ndialogue strategies: A case study of\na spoken dialogue agent for email.\nCOLING/ACL .\nWalker, M. A., M. Iida, and S. Cote.\n1994. Japanese discourse and the\nprocess of centering. Computational\nLinguistics , 20(2):193\u2013232.\nWalker, M. A., A. K. Joshi, and\nE. Prince, eds. 1998b. Centering in\nDiscourse . Oxford University Press.\nWang, A., A. Singh, J. Michael, F. Hill,\nO. Levy, and S. R. Bowman. 2018a.\nGlue: A multi-task benchmark and\nanalysis platform for natural lan-\nguage understanding. ICLR .\nWang, S. and C. D. Manning. 2012.\nBaselines and bigrams: Simple,\ngood sentiment and topic classi\ufb01ca-\ntion. ACL.\nWang, W. and B. Chang. 2016. Graph-\nbased dependency parsing with bidi-\nrectional LSTM. ACL.\nWang, Y ., S. Li, and J. Yang. 2018b.\nToward fast and accurate neural dis-\ncourse segmentation. EMNLP .\nWang, Y ., S. Mishra, P. Alipoormo-\nlabashi, Y . Kordi, A. Mirzaei,\nA. Naik, A. Ashok, A. S.\nDhanasekaran, A. Arunkumar,\nD. Stap, E. Pathak, G. Kara-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1007,
      "token_count": 744,
      "chapter_title": ""
    }
  },
  {
    "content": "Linguistics , 20(2):193\u2013232.\nWalker, M. A., A. K. Joshi, and\nE. Prince, eds. 1998b. Centering in\nDiscourse . Oxford University Press.\nWang, A., A. Singh, J. Michael, F. Hill,\nO. Levy, and S. R. Bowman. 2018a.\nGlue: A multi-task benchmark and\nanalysis platform for natural lan-\nguage understanding. ICLR .\nWang, S. and C. D. Manning. 2012.\nBaselines and bigrams: Simple,\ngood sentiment and topic classi\ufb01ca-\ntion. ACL.\nWang, W. and B. Chang. 2016. Graph-\nbased dependency parsing with bidi-\nrectional LSTM. ACL.\nWang, Y ., S. Li, and J. Yang. 2018b.\nToward fast and accurate neural dis-\ncourse segmentation. EMNLP .\nWang, Y ., S. Mishra, P. Alipoormo-\nlabashi, Y . Kordi, A. Mirzaei,\nA. Naik, A. Ashok, A. S.\nDhanasekaran, A. Arunkumar,\nD. Stap, E. Pathak, G. Kara-\nmanolakis, H. Lai, I. Purohit,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1008,
      "token_count": 293,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 589",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1009,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Bibliography 581\nI. Mondal, J. Anderson, K. Kuz-\nnia, K. Doshi, K. K. Pal, M. Pa-\ntel, M. Moradshahi, M. Par-\nmar, M. Purohit, N. Varshney,\nP. R. Kaza, P. Verma, R. S. Puri,\nR. Karia, S. Doshi, S. K. Sampat,\nS. Mishra, S. Reddy A, S. Patro,\nT. Dixit, and X. Shen. 2022. Super-\nNaturalInstructions: Generaliza-\ntion via declarative instructions on\n1600+ NLP tasks. EMNLP .\nWang, Y ., R. Skerry-Ryan, D. Stan-\nton, Y . Wu, R. J. Weiss, N. Jaitly,\nZ. Yang, Y . Xiao, Z. Chen, S. Ben-\ngio, Q. Le, Y . Agiomyrgiannakis,\nR. Clark, and R. A. Saurous.\n2017. Tacotron: Towards end-to-end\nspeech synthesis. INTERSPEECH .\nWatanabe, S., T. Hori, S. Karita,\nT. Hayashi, J. Nishitoba, Y . Unno,\nN. E. Y . Soplin, J. Heymann,\nM. Wiesner, N. Chen, A. Renduch-\nintala, and T. Ochiai. 2018. ESP-\nnet: End-to-end speech processing\ntoolkit. INTERSPEECH .\nWeaver, W. 1949/1955. Translation. In\nW. N. Locke and A. D. Boothe, eds,\nMachine Translation of Languages ,\n15\u201323. MIT Press. Reprinted from a\nmemorandum written by Weaver in\n1949.\nWebber, B. L. 1978. A Formal\nApproach to Discourse Anaphora .\nPh.D. thesis, Harvard University.\nWebber, B. L. 1983. So what can we\ntalk about now? In M. Brady and\nR. C. Berwick, eds, Computational\nModels of Discourse , 331\u2013371. The\nMIT Press.\nWebber, B. L. 1991. Structure and os-\ntension in the interpretation of dis-\ncourse deixis. Language and Cogni-\ntive Processes , 6(2):107\u2013135.\nWebber, B. L. and B. Baldwin. 1992.\nAccommodating context change.\nACL.\nWebber, B. L., M. Egg, and V . Kor-\ndoni. 2012. Discourse structure and\nlanguage technology. Natural Lan-\nguage Engineering , 18(4):437\u2013490.\nWebber, B. L. 1988. Discourse deixis:\nReference to discourse segments.\nACL.\nWebson, A. and E. Pavlick. 2022. Do\nprompt-based models really under-\nstand the meaning of their prompts?\nNAACL HLT .\nWebster, K., M. Recasens, V . Axel-\nrod, and J. Baldridge. 2018. Mind\nthe GAP: A balanced corpus of gen-\ndered ambiguous pronouns. TACL ,\n6:605\u2013617.\nWei, J., X. Wang, D. Schuurmans,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1010,
      "token_count": 736,
      "chapter_title": ""
    }
  },
  {
    "content": "R. C. Berwick, eds, Computational\nModels of Discourse , 331\u2013371. The\nMIT Press.\nWebber, B. L. 1991. Structure and os-\ntension in the interpretation of dis-\ncourse deixis. Language and Cogni-\ntive Processes , 6(2):107\u2013135.\nWebber, B. L. and B. Baldwin. 1992.\nAccommodating context change.\nACL.\nWebber, B. L., M. Egg, and V . Kor-\ndoni. 2012. Discourse structure and\nlanguage technology. Natural Lan-\nguage Engineering , 18(4):437\u2013490.\nWebber, B. L. 1988. Discourse deixis:\nReference to discourse segments.\nACL.\nWebson, A. and E. Pavlick. 2022. Do\nprompt-based models really under-\nstand the meaning of their prompts?\nNAACL HLT .\nWebster, K., M. Recasens, V . Axel-\nrod, and J. Baldridge. 2018. Mind\nthe GAP: A balanced corpus of gen-\ndered ambiguous pronouns. TACL ,\n6:605\u2013617.\nWei, J., X. Wang, D. Schuurmans,\nM. Bosma, F. Xia, E. Chi, Q. V .\nLe, D. Zhou, et al. 2022. Chain-of-\nthought prompting elicits reasoning\nin large language models. NeurIPS ,\nvolume 35.Weischedel, R., M. Meteer,\nR. Schwartz, L. A. Ramshaw, and\nJ. Palmucci. 1993. Coping with am-\nbiguity and unknown words through\nprobabilistic models. Computational\nLinguistics , 19(2):359\u2013382.\nWeizenbaum, J. 1966. ELIZA \u2013 A\ncomputer program for the study of\nnatural language communication be-\ntween man and machine. CACM ,\n9(1):36\u201345.\nWeizenbaum, J. 1976. Computer Power\nand Human Reason: From Judge-\nment to Calculation . W.H. Freeman\n& Co.\nWerbos, P. 1974. Beyond regression:\nnew tools for prediction and analy-\nsis in the behavioral sciences . Ph.D.\nthesis, Harvard University.\nWerbos, P. J. 1990. Backpropagation\nthrough time: what it does and how\nto do it. Proceedings of the IEEE ,\n78(10):1550\u20131560.\nWeston, J., S. Chopra, and A. Bordes.\n2015. Memory networks. ICLR\n2015 .\nWidrow, B. and M. E. Hoff. 1960.\nAdaptive switching circuits. IRE\nWESCON Convention Record , vol-\nume 4.\nWiebe, J. 1994. Tracking point of view\nin narrative. Computational Linguis-\ntics, 20(2):233\u2013287.\nWiebe, J. 2000. Learning subjective ad-\njectives from corpora. AAAI .\nWiebe, J., R. F. Bruce, and T. P. O\u2019Hara.\n1999. Development and use of a\ngold-standard data set for subjectiv-\nity classi\ufb01cations. ACL.\nWierzbicka, A. 1992. Semantics, Cul-\nture, and Cognition: University Hu-\nman Concepts in Culture-Speci\ufb01c",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1011,
      "token_count": 744,
      "chapter_title": ""
    }
  },
  {
    "content": "new tools for prediction and analy-\nsis in the behavioral sciences . Ph.D.\nthesis, Harvard University.\nWerbos, P. J. 1990. Backpropagation\nthrough time: what it does and how\nto do it. Proceedings of the IEEE ,\n78(10):1550\u20131560.\nWeston, J., S. Chopra, and A. Bordes.\n2015. Memory networks. ICLR\n2015 .\nWidrow, B. and M. E. Hoff. 1960.\nAdaptive switching circuits. IRE\nWESCON Convention Record , vol-\nume 4.\nWiebe, J. 1994. Tracking point of view\nin narrative. Computational Linguis-\ntics, 20(2):233\u2013287.\nWiebe, J. 2000. Learning subjective ad-\njectives from corpora. AAAI .\nWiebe, J., R. F. Bruce, and T. P. O\u2019Hara.\n1999. Development and use of a\ngold-standard data set for subjectiv-\nity classi\ufb01cations. ACL.\nWierzbicka, A. 1992. Semantics, Cul-\nture, and Cognition: University Hu-\nman Concepts in Culture-Speci\ufb01c\nCon\ufb01gurations . Oxford University\nPress.\nWierzbicka, A. 1996. Semantics:\nPrimes and Universals . Oxford Uni-\nversity Press.\nWilensky, R. 1983. Planning and\nUnderstanding: A Computational\nApproach to Human Reasoning .\nAddison-Wesley.\nWilks, Y . 1973. An arti\ufb01cial intelli-\ngence approach to machine transla-\ntion. In R. C. Schank and K. M.\nColby, eds, Computer Models of\nThought and Language , 114\u2013151.\nW.H. Freeman.\nWilks, Y . 1975a. Preference semantics.\nIn E. L. Keenan, ed., The Formal Se-\nmantics of Natural Language , 329\u2013\n350. Cambridge Univ. Press.\nWilks, Y . 1975b. A preferential,\npattern-seeking, semantics for natu-\nral language inference. Arti\ufb01cial In-\ntelligence , 6(1):53\u201374.Williams, A., N. Nangia, and S. Bow-\nman. 2018. A broad-coverage chal-\nlenge corpus for sentence under-\nstanding through inference. NAACL\nHLT.\nWilliams, J. D., K. Asadi, and\nG. Zweig. 2017. Hybrid code\nnetworks: practical and ef\ufb01cient\nend-to-end dialog control with su-\npervised and reinforcement learning.\nACL.\nWilliams, J. D., A. Raux, and M. Hen-\nderson. 2016. The dialog state track-\ning challenge series: A review. Dia-\nlogue & Discourse , 7(3):4\u201333.\nWilliams, J. D. and S. J. Young. 2007.\nPartially observable markov deci-\nsion processes for spoken dialog sys-\ntems. Computer Speech and Lan-\nguage , 21(1):393\u2013422.\nWilson, T., J. Wiebe, and P. Hoffmann.\n2005. Recognizing contextual polar-\nity in phrase-level sentiment analy-\nsis.EMNLP .\nWinograd, T. 1972. Understanding Nat-\nural Language . Academic Press.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1012,
      "token_count": 742,
      "chapter_title": ""
    }
  },
  {
    "content": "ral language inference. Arti\ufb01cial In-\ntelligence , 6(1):53\u201374.Williams, A., N. Nangia, and S. Bow-\nman. 2018. A broad-coverage chal-\nlenge corpus for sentence under-\nstanding through inference. NAACL\nHLT.\nWilliams, J. D., K. Asadi, and\nG. Zweig. 2017. Hybrid code\nnetworks: practical and ef\ufb01cient\nend-to-end dialog control with su-\npervised and reinforcement learning.\nACL.\nWilliams, J. D., A. Raux, and M. Hen-\nderson. 2016. The dialog state track-\ning challenge series: A review. Dia-\nlogue & Discourse , 7(3):4\u201333.\nWilliams, J. D. and S. J. Young. 2007.\nPartially observable markov deci-\nsion processes for spoken dialog sys-\ntems. Computer Speech and Lan-\nguage , 21(1):393\u2013422.\nWilson, T., J. Wiebe, and P. Hoffmann.\n2005. Recognizing contextual polar-\nity in phrase-level sentiment analy-\nsis.EMNLP .\nWinograd, T. 1972. Understanding Nat-\nural Language . Academic Press.\nWinston, P. H. 1977. Arti\ufb01cial Intelli-\ngence . Addison Wesley.\nWiseman, S., A. M. Rush, and S. M.\nShieber. 2016. Learning global\nfeatures for coreference resolution.\nNAACL HLT .\nWiseman, S., A. M. Rush, S. M.\nShieber, and J. Weston. 2015. Learn-\ning anaphoricity and antecedent\nranking features for coreference res-\nolution. ACL.\nWitten, I. H. and T. C. Bell. 1991.\nThe zero-frequency problem: Es-\ntimating the probabilities of novel\nevents in adaptive text compression.\nIEEE Transactions on Information\nTheory , 37(4):1085\u20131094.\nWitten, I. H. and E. Frank. 2005. Data\nMining: Practical Machine Learn-\ning Tools and Techniques , 2nd edi-\ntion. Morgan Kaufmann.\nWittgenstein, L. 1953. Philosoph-\nical Investigations. (Translated by\nAnscombe, G.E.M.) . Blackwell.\nWolf, F. and E. Gibson. 2005. Rep-\nresenting discourse coherence: A\ncorpus-based analysis. Computa-\ntional Linguistics , 31(2):249\u2013287.\nWolf, M. J., K. W. Miller, and F. S.\nGrodzinsky. 2017. Why we should\nhave seen that coming: Comments\non Microsoft\u2019s Tay \u201cexperiment,\u201d\nand wider implications. The ORBIT\nJournal , 1(2):1\u201312.\nWoods, W. A. 1978. Semantics and\nquanti\ufb01cation in natural language\nquestion answering. In M. Yovits,\ned.,Advances in Computers , 2\u201364.\nAcademic.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1013,
      "token_count": 670,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 590",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1014,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "582 Bibliography\nWoods, W. A., R. M. Kaplan, and B. L.\nNash-Webber. 1972. The lunar sci-\nences natural language information\nsystem: Final report. Technical Re-\nport 2378, BBN.\nWoodsend, K. and M. Lapata. 2015.\nDistributed representations for un-\nsupervised semantic role labeling.\nEMNLP .\nWu, D. 1996. A polynomial-time algo-\nrithm for statistical machine transla-\ntion. ACL.\nWu, F. and D. S. Weld. 2007. Au-\ntonomously semantifying Wiki-\npedia. CIKM-07 .\nWu, F. and D. S. Weld. 2010. Open\ninformation extraction using Wiki-\npedia. ACL.\nWu, L., F. Petroni, M. Josifoski,\nS. Riedel, and L. Zettlemoyer. 2020.\nScalable zero-shot entity linking\nwith dense entity retrieval. EMNLP .\nWu, S. and M. Dredze. 2019. Beto,\nBentz, Becas: The surprising cross-\nlingual effectiveness of BERT.\nEMNLP .\nWu, Y ., M. Schuster, Z. Chen, Q. V .\nLe, M. Norouzi, W. Macherey,\nM. Krikun, Y . Cao, Q. Gao,\nK. Macherey, J. Klingner, A. Shah,\nM. Johnson, X. Liu, \u0141. Kaiser,\nS. Gouws, Y . Kato, T. Kudo,\nH. Kazawa, K. Stevens, G. Kurian,\nN. Patil, W. Wang, C. Young,\nJ. Smith, J. Riesa, A. Rud-\nnick, O. Vinyals, G. S. Corrado,\nM. Hughes, and J. Dean. 2016.\nGoogle\u2019s neural machine translation\nsystem: Bridging the gap between\nhuman and machine translation.\nArXiv preprint arXiv:1609.08144.\nWundt, W. 1900. V\u00a8olkerpsychologie:\neine Untersuchung der Entwick-\nlungsgesetze von Sprache, Mythus,\nund Sitte . W. Engelmann, Leipzig.\nBand II: Die Sprache, Zweiter Teil.\nXu, A., E. Pathak, E. Wallace, S. Gu-\nrurangan, M. Sap, and D. Klein.\n2021. Detoxifying language models\nrisks marginalizing minority voices.\nNAACL HLT .\nXu, J., D. Ju, M. Li, Y .-L. Boureau,\nJ. Weston, and E. Dinan. 2020.\nRecipes for safety in open-\ndomain chatbots. ArXiv preprint\narXiv:2010.07079.\nXu, P., H. Saghir, J. S. Kang, T. Long,\nA. J. Bose, Y . Cao, and J. C. K. Che-\nung. 2019. A cross-domain transfer-\nable neural coherence model. ACL.\nXue, N., H. T. Ng, S. Pradhan,\nA. Rutherford, B. L. Webber,\nC. Wang, and H. Wang. 2016.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1015,
      "token_count": 738,
      "chapter_title": ""
    }
  },
  {
    "content": "Wundt, W. 1900. V\u00a8olkerpsychologie:\neine Untersuchung der Entwick-\nlungsgesetze von Sprache, Mythus,\nund Sitte . W. Engelmann, Leipzig.\nBand II: Die Sprache, Zweiter Teil.\nXu, A., E. Pathak, E. Wallace, S. Gu-\nrurangan, M. Sap, and D. Klein.\n2021. Detoxifying language models\nrisks marginalizing minority voices.\nNAACL HLT .\nXu, J., D. Ju, M. Li, Y .-L. Boureau,\nJ. Weston, and E. Dinan. 2020.\nRecipes for safety in open-\ndomain chatbots. ArXiv preprint\narXiv:2010.07079.\nXu, P., H. Saghir, J. S. Kang, T. Long,\nA. J. Bose, Y . Cao, and J. C. K. Che-\nung. 2019. A cross-domain transfer-\nable neural coherence model. ACL.\nXue, N., H. T. Ng, S. Pradhan,\nA. Rutherford, B. L. Webber,\nC. Wang, and H. Wang. 2016.\nCoNLL 2016 shared task on mul-\ntilingual shallow discourse parsing.\nCoNLL-16 shared task .Xue, N. and M. Palmer. 2004. Calibrat-\ning features for semantic role label-\ning. EMNLP .\nYamada, H. and Y . Matsumoto. 2003.\nStatistical dependency analysis with\nsupport vector machines. IWPT-03 .\nYang, D., J. Chen, Z. Yang, D. Jurafsky,\nand E. H. Hovy. 2019. Let\u2019s make\nyour request more persuasive: Mod-\neling persuasive strategies via semi-\nsupervised neural nets on crowd-\nfunding platforms. NAACL HLT .\nYang, X., G. Zhou, J. Su, and C. L. Tan.\n2003. Coreference resolution us-\ning competition learning approach.\nACL.\nYang, Y . and J. Pedersen. 1997. A com-\nparative study on feature selection in\ntext categorization. ICML .\nYankelovich, N., G.-A. Levow, and\nM. Marx. 1995. Designing\nSpeechActs: Issues in speech user\ninterfaces. CHI-95 .\nYih, W.-t., M. Richardson, C. Meek,\nM.-W. Chang, and J. Suh. 2016. The\nvalue of semantic parse labeling for\nknowledge base question answering.\nACL.\nYoung, S. J., M. Ga \u02c7si\u00b4c, S. Keizer,\nF. Mairesse, J. Schatzmann,\nB. Thomson, and K. Yu. 2010. The\nHidden Information State model:\nA practical framework for POMDP-\nbased spoken dialogue management.\nComputer Speech & Language ,\n24(2):150\u2013174.\nYounger, D. H. 1967. Recognition and\nparsing of context-free languages in\ntime n3.Information and Control ,\n10:189\u2013208.\nYu, N., M. Zhang, and G. Fu. 2018.\nTransition-based neural RST parsing\nwith implicit syntax features. COL-\nING.\nYu, Y ., Y . Zhu, Y . Liu, Y . Liu,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1016,
      "token_count": 745,
      "chapter_title": ""
    }
  },
  {
    "content": "parative study on feature selection in\ntext categorization. ICML .\nYankelovich, N., G.-A. Levow, and\nM. Marx. 1995. Designing\nSpeechActs: Issues in speech user\ninterfaces. CHI-95 .\nYih, W.-t., M. Richardson, C. Meek,\nM.-W. Chang, and J. Suh. 2016. The\nvalue of semantic parse labeling for\nknowledge base question answering.\nACL.\nYoung, S. J., M. Ga \u02c7si\u00b4c, S. Keizer,\nF. Mairesse, J. Schatzmann,\nB. Thomson, and K. Yu. 2010. The\nHidden Information State model:\nA practical framework for POMDP-\nbased spoken dialogue management.\nComputer Speech & Language ,\n24(2):150\u2013174.\nYounger, D. H. 1967. Recognition and\nparsing of context-free languages in\ntime n3.Information and Control ,\n10:189\u2013208.\nYu, N., M. Zhang, and G. Fu. 2018.\nTransition-based neural RST parsing\nwith implicit syntax features. COL-\nING.\nYu, Y ., Y . Zhu, Y . Liu, Y . Liu,\nS. Peng, M. Gong, and A. Zeldes.\n2019. GumDrop at the DISRPT2019\nshared task: A model stacking ap-\nproach to discourse unit segmenta-\ntion and connective detection. Work-\nshop on Discourse Relation Parsing\nand Treebanking 2019 .\nZapirain, B., E. Agirre, L. M `arquez,\nand M. Surdeanu. 2013. Selectional\npreferences for semantic role classi-\n\ufb01cation. Computational Linguistics ,\n39(3):631\u2013663.\nZelle, J. M. and R. J. Mooney. 1996.\nLearning to parse database queries\nusing inductive logic programming.\nAAAI .\nZeman, D. 2008. Reusable tagset con-\nversion using tagset drivers. LREC .\nZens, R. and H. Ney. 2007. Ef\ufb01cient\nphrase-table representation for ma-\nchine translation with applications toonline MT and speech translation.\nNAACL-HLT .\nZettlemoyer, L. and M. Collins. 2005.\nLearning to map sentences to log-\nical form: Structured classi\ufb01cation\nwith probabilistic categorial gram-\nmars. Uncertainty in Arti\ufb01cial Intel-\nligence, UAI\u201905 .\nZettlemoyer, L. and M. Collins. 2007.\nOnline learning of relaxed CCG\ngrammars for parsing to logical\nform. EMNLP/CoNLL .\nZhang, H., R. Sproat, A. H. Ng,\nF. Stahlberg, X. Peng, K. Gorman,\nand B. Roark. 2019. Neural models\nof text normalization for speech ap-\nplications. Computational Linguis-\ntics, 45(2):293\u2013337.\nZhang, R., C. N. dos Santos, M. Ya-\nsunaga, B. Xiang, and D. Radev.\n2018. Neural coreference resolution\nwith deep biaf\ufb01ne attention by joint\nmention detection and mention clus-\ntering. ACL.\nZhang, T., V . Kishore, F. Wu, K. Q.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1017,
      "token_count": 745,
      "chapter_title": ""
    }
  },
  {
    "content": "phrase-table representation for ma-\nchine translation with applications toonline MT and speech translation.\nNAACL-HLT .\nZettlemoyer, L. and M. Collins. 2005.\nLearning to map sentences to log-\nical form: Structured classi\ufb01cation\nwith probabilistic categorial gram-\nmars. Uncertainty in Arti\ufb01cial Intel-\nligence, UAI\u201905 .\nZettlemoyer, L. and M. Collins. 2007.\nOnline learning of relaxed CCG\ngrammars for parsing to logical\nform. EMNLP/CoNLL .\nZhang, H., R. Sproat, A. H. Ng,\nF. Stahlberg, X. Peng, K. Gorman,\nand B. Roark. 2019. Neural models\nof text normalization for speech ap-\nplications. Computational Linguis-\ntics, 45(2):293\u2013337.\nZhang, R., C. N. dos Santos, M. Ya-\nsunaga, B. Xiang, and D. Radev.\n2018. Neural coreference resolution\nwith deep biaf\ufb01ne attention by joint\nmention detection and mention clus-\ntering. ACL.\nZhang, T., V . Kishore, F. Wu, K. Q.\nWeinberger, and Y . Artzi. 2020.\nBERTscore: Evaluating text gener-\nation with BERT. ICLR 2020 .\nZhang, Y ., V . Zhong, D. Chen, G. An-\ngeli, and C. D. Manning. 2017.\nPosition-aware attention and su-\npervised data improve slot \ufb01lling.\nEMNLP .\nZhao, H., W. Chen, C. Kit, and G. Zhou.\n2009. Multilingual dependency\nlearning: A huge feature engineer-\ning method to semantic dependency\nparsing. CoNLL .\nZhao, J., T. Wang, M. Yatskar, R. Cot-\nterell, V . Ordonez, and K.-W. Chang.\n2019. Gender bias in contextualized\nword embeddings. NAACL HLT .\nZhao, J., T. Wang, M. Yatskar, V . Or-\ndonez, and K.-W. Chang. 2017. Men\nalso like shopping: Reducing gender\nbias ampli\ufb01cation using corpus-level\nconstraints. EMNLP .\nZhao, J., T. Wang, M. Yatskar, V . Or-\ndonez, and K.-W. Chang. 2018a.\nGender bias in coreference reso-\nlution: Evaluation and debiasing\nmethods. NAACL HLT .\nZhao, J., Y . Zhou, Z. Li, W. Wang,\nand K.-W. Chang. 2018b. Learn-\ning gender-neutral word embed-\ndings. EMNLP .\nZheng, J., L. Vilnis, S. Singh, J. D.\nChoi, and A. McCallum. 2013.\nDynamic knowledge-base alignment\nfor coreference resolution. CoNLL .\nZhou, D., O. Bousquet, T. N. Lal,\nJ. Weston, and B. Sch \u00a8olkopf. 2004a.\nLearning with local and global con-\nsistency. NeurIPS .\nZhou, G., J. Su, J. Zhang, and\nM. Zhang. 2005. Exploring var-\nious knowledge in relation extrac-",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1018,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "donez, and K.-W. Chang. 2017. Men\nalso like shopping: Reducing gender\nbias ampli\ufb01cation using corpus-level\nconstraints. EMNLP .\nZhao, J., T. Wang, M. Yatskar, V . Or-\ndonez, and K.-W. Chang. 2018a.\nGender bias in coreference reso-\nlution: Evaluation and debiasing\nmethods. NAACL HLT .\nZhao, J., Y . Zhou, Z. Li, W. Wang,\nand K.-W. Chang. 2018b. Learn-\ning gender-neutral word embed-\ndings. EMNLP .\nZheng, J., L. Vilnis, S. Singh, J. D.\nChoi, and A. McCallum. 2013.\nDynamic knowledge-base alignment\nfor coreference resolution. CoNLL .\nZhou, D., O. Bousquet, T. N. Lal,\nJ. Weston, and B. Sch \u00a8olkopf. 2004a.\nLearning with local and global con-\nsistency. NeurIPS .\nZhou, G., J. Su, J. Zhang, and\nM. Zhang. 2005. Exploring var-\nious knowledge in relation extrac-\ntion. ACL.",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1019,
      "token_count": 275,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 591\n\nBibliography 583\nZhou, J. and W. Xu. 2015a. End-to-\nend learning of semantic role label-\ning using recurrent neural networks.\nACL.\nZhou, J. and W. Xu. 2015b. End-to-\nend learning of semantic role label-\ning using recurrent neural networks.\nACL.\nZhou, K., K. Ethayarajh, D. Card, and\nD. Jurafsky. 2022. Problems with\ncosine as a measure of embedding\nsimilarity for high frequency words.\nACL.\nZhou, K., J. Hwang, X. Ren, and\nM. Sap. 2024. Relying on the un-\nreliable: The impact of language\nmodels\u2019 reluctance to express uncer-\ntainty. ACL.\nZhou, L., M. Ticrea, and E. H. Hovy.\n2004b. Multi-document biography\nsummarization. EMNLP .\nZhou, Y ., A. I. Muresanu, Z. Han,\nK. Paster, S. Pitis, H. Chan, and\nJ. Ba. 2023. Large language models\nare human-level prompt engineers.\nThe Eleventh International Confer-\nence on Learning Representations .\nZhou, Y . and N. Xue. 2015. The Chi-\nnese Discourse TreeBank: a Chinese\ncorpus annotated with discourse re-\nlations. Language Resources and\nEvaluation , 49(2):397\u2013431.\nZhu, X. and Z. Ghahramani. 2002.\nLearning from labeled and unlabeled\ndata with label propagation. Techni-\ncal Report CMU-CALD-02, CMU.\nZhu, X., Z. Ghahramani, and J. Laf-\nferty. 2003. Semi-supervised learn-\ning using gaussian \ufb01elds and har-\nmonic functions. ICML .\nZhu, Y ., R. Kiros, R. Zemel,\nR. Salakhutdinov, R. Urtasun,\nA. Torralba, and S. Fidler. 2015.\nAligning books and movies: To-\nwards story-like visual explanations\nby watching movies and reading\nbooks. IEEE International Confer-\nence on Computer Vision .\nZiemski, M., M. Junczys-Dowmunt,\nand B. Pouliquen. 2016. The United\nNations parallel corpus v1.0. LREC .",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1020,
      "token_count": 543,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 593",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1021,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Subject Index\n*?,9\n+?,9\n.wav format, 336\n10-fold cross-validation, 69\n!(derives), 389\n\u02c6,58\n* (RE Kleene *), 7\n+ (RE Kleene +), 7\n. (RE any character), 7\n$(RE end-of-line), 8\n((RE precedence symbol),\n8\n[(RE character\ndisjunction), 6\n\\B(RE non\nword-boundary), 8\n\\b(RE word-boundary), 8\n](RE character\ndisjunction), 6\n^(RE start-of-line), 8\n[^](single-char negation),\n6\n4-gram, 38\n4-tuple, 392\n5-gram, 38\nA-D conversion, 335\nAAC, 32\nAAE, 15\nAB test, 353\nablating, 248\nabsolute position, 198\nabsolute temporal\nexpression, 452\nabstract word, 485\naccessible, 506\naccessing a referent, 501\naccomplishment\nexpressions, 450\naccuracy, 366\nachievement expressions,\n450\nacknowledgment speech\nact, 312\nactivation, 133\nactivity expressions, 450\nacute-eval, 325\nad hoc retrieval, 291\nadd gate, 172\nadd-k, 47\nadd-one smoothing, 46\nadequacy, 280\nadjacency pairs, 313\nAdjectives, 364\nadverb, 364\ndegree, 364\ndirectional, 364\nlocative, 364\nmanner, 364\ntemporal, 364\nAdverbs, 364\nAED, 339\naffective, 481\naf\ufb01x, 24agent, as thematic role, 462\nagglutinative\nlanguage, 267\nAIFF \ufb01le, 336\nAISHELL-1, 334\naktionsart, 450\nALGOL, 409\nalgorithm\nbyte-pair encoding, 22\nCKY , 397\nminimum edit distance,\n28\nnaive Bayes classi\ufb01er, 57\npointwise mutual\ninformation, 114\nsemantic role labeling,\n469\nTextTiling, 544\nViterbi, 373\naligned, 249\nalignment, 25,342\nin ASR, 346\nminimum cost, 27\nstring, 25\nvia minimum edit\ndistance, 27\nAllen relations, 448\nallocational harm, 126\nambiguity\namount of part-of-speech\nin Brown corpus,\n366\nattachment, 396\ncoordination, 396\nof referring expressions,\n503\npart-of-speech, 365\nresolution of tag, 366\nAmerican Structuralism,\n408\nanaphor, 502\nanaphora, 502\nanaphoricity detector, 511\nanchor texts, 520\nanchors in regular\nexpressions, 8, 29\nanisotropy, 234\nantecedent, 502\nApple AIFF, 336\napproximate\nrandomization, 71\narc eager, 423\narc standard, 417\nargmax, 58\nargumentation mining, 547\nargumentation schemes,\n548\nargumentative relations,\n547\nargumentative zoning, 549\nAristotle, 362, 450\nARPA, 355\narticle (part-of-speech), 364\narticulatory synthesis, 357aspect, 450\nASR, 331\ncon\ufb01dence, 320\nassociation, 103\nATIS\ncorpus, 390",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1022,
      "token_count": 785,
      "chapter_title": ""
    }
  },
  {
    "content": "aligned, 249\nalignment, 25,342\nin ASR, 346\nminimum cost, 27\nstring, 25\nvia minimum edit\ndistance, 27\nAllen relations, 448\nallocational harm, 126\nambiguity\namount of part-of-speech\nin Brown corpus,\n366\nattachment, 396\ncoordination, 396\nof referring expressions,\n503\npart-of-speech, 365\nresolution of tag, 366\nAmerican Structuralism,\n408\nanaphor, 502\nanaphora, 502\nanaphoricity detector, 511\nanchor texts, 520\nanchors in regular\nexpressions, 8, 29\nanisotropy, 234\nantecedent, 502\nApple AIFF, 336\napproximate\nrandomization, 71\narc eager, 423\narc standard, 417\nargmax, 58\nargumentation mining, 547\nargumentation schemes,\n548\nargumentative relations,\n547\nargumentative zoning, 549\nAristotle, 362, 450\nARPA, 355\narticle (part-of-speech), 364\narticulatory synthesis, 357aspect, 450\nASR, 331\ncon\ufb01dence, 320\nassociation, 103\nATIS\ncorpus, 390\nATN, 478\nATRANS, 477\nattachment ambiguity, 396\nattention\ncross-attention, 272\nencoder-decoder, 272\nhistory in transformers,\n202\nattention head, 188\nattention mechanism, 179\nAttribution (as coherence\nrelation), 534\naugmentative\ncommunication, 32\nauthorship attribution, 56\nautoregressive generation,\n167,207\nAuxiliary, 365\nB3,524\nBabbage, C., 332\nbackoff, 49\nin smoothing, 48\nbackprop, 147\nbackpropagation through\ntime, 161\nbacktrace\nin minimum edit\ndistance, 29\nbacktranslation, 279\nBackus-Naur form, 388\nbackward-looking center,\n541\nbag of words, 58, 59\nin IR, 291\nbakeoff, 355\nspeech recognition\ncompetition, 355\nbarged in, 326\nbase model, 249\nbasic emotions, 482\nbatch training, 94\nBayes\u2019 rule, 58\ndropping denominator,\n59, 372\nBayesian inference, 58\nBDI, 329\nbeam search, 275,424\nbeam width, 275,424\nBerkeley Restaurant\nProject, 36\nBernoulli naive Bayes, 75\nBERT\nfor affect, 497\nbest-worst scaling, 486\nbias ampli\ufb01cation, 126\nbias term, 79,133\nbidirectional RNN, 170bigram, 34\nbinary branching, 394\nbinary naive Bayes, 63\nbinary tree, 394\nBIO, 238,368\nBIO tagging, 238\nfor NER, 238, 368\nBIOES, 238, 368\nbitext, 270\nbits for measuring entropy,\n49\nblank in CTC, 342\nBM25, 291,293\nBNF (Backus-Naur form),\n388\nbootstrap, 73\nbootstrap algorithm, 73\nbootstrap test, 71\nbootstrapping, 71\nin IE, 441\nbound pronoun, 504\nBPE, 21\nBPE, 22\nbracketed notation, 391\nbridging inference, 506\nbroadcast news",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1023,
      "token_count": 788,
      "chapter_title": ""
    }
  },
  {
    "content": "base model, 249\nbasic emotions, 482\nbatch training, 94\nBayes\u2019 rule, 58\ndropping denominator,\n59, 372\nBayesian inference, 58\nBDI, 329\nbeam search, 275,424\nbeam width, 275,424\nBerkeley Restaurant\nProject, 36\nBernoulli naive Bayes, 75\nBERT\nfor affect, 497\nbest-worst scaling, 486\nbias ampli\ufb01cation, 126\nbias term, 79,133\nbidirectional RNN, 170bigram, 34\nbinary branching, 394\nbinary naive Bayes, 63\nbinary tree, 394\nBIO, 238,368\nBIO tagging, 238\nfor NER, 238, 368\nBIOES, 238, 368\nbitext, 270\nbits for measuring entropy,\n49\nblank in CTC, 342\nBM25, 291,293\nBNF (Backus-Naur form),\n388\nbootstrap, 73\nbootstrap algorithm, 73\nbootstrap test, 71\nbootstrapping, 71\nin IE, 441\nbound pronoun, 504\nBPE, 21\nBPE, 22\nbracketed notation, 391\nbridging inference, 506\nbroadcast news\nspeech recognition of,\n355\nBrown corpus, 13\noriginal tagging of, 384\nbyte-pair encoding, 21\ncalibrated, 290\nCALLHOME, 333\nCandide , 287\nCantonese, 267\ncapture group, 12\ncascade\nregular expression in\nELIZA, 12\ncase\nsensitivity in regular\nexpression search, 6\ncase folding, 23\ncase frame, 463, 478\nCAT, 263\ncataphora, 504\nCD (conceptual\ndependency), 477\nCentering Theory, 532, 540\ncentroid, 117\ncepstrum\nhistory, 355\nCFG, seecontext-free\ngrammar\nchain rule, 99,148\nchain-of-thought, 254\nchannels in stored\nwaveforms, 336\nchart parsing, 397\nChatbots, 309,321\nchatbots, 4\nCHiME, 333\nChinese\n585",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1024,
      "token_count": 502,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 594",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1025,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "586 Subject Index\nas verb-framed language,\n267\nwords for brother, 266\nChomsky normal form, 394\nChomsky-adjunction, 395\nchrF, 281\nCIRCUS, 459\ncitation form, 102\nCitizen Kane , 531\nCKY algorithm, 387\nclaims, 547\nclass-based n-gram, 53\nclassi\ufb01er head, 235\nclefts, 507\nclitic, 19\norigin of term, 362\nclosed book, 304\nclosed class, 363\ncloze task, 226\ncluster, 502\nCNF, seeChomsky normal\nform\nCocke-Kasami-Younger\nalgorithm, seeCKY\ncode switching, 15\ncoherence, 531\nentity-based, 540\nrelations, 533\ncohesion\nlexical, 532, 544\nColBERT, 300\ncold languages, 268\ncollection in IR, 291\ncommissive speech act, 312\ncommon crawl, 211\ncommon ground, 312, 328\nCommon nouns, 363\ncomplementizers, 364\ncomponential analysis, 476\ncompression, 335\nComputational Grammar\nCoder (CGC), 384\nconcatenation, 5, 29\nconceptual dependency, 477\nconcrete word, 485\nconditional generation, 204\nconditional random \ufb01eld,\n376\ncon\ufb01dence, 285\nASR, 320\nin relation extraction, 442\ncon\ufb01dence values, 442\ncon\ufb01guration, 417\nconfusion matrix, 66\nConjunctions, 364\nconnectionist, 157\nconnotation frame, 497\nconnotation frames, 479\nconnotations, 104,482\nconstative speech act, 312\nconstituency, 388\nconstituent, 388\ntitles which are not, 387\nConstraint Grammar, 433\ncontent planning, 319\ncontext embedding, 122\ncontext-free grammar, 388,\n392, 407Chomsky normal form,\n394\ninvention of, 409\nnon-terminal symbol,\n389\nproductions, 388\nrules, 388\nterminal symbol, 389\nweak and strong\nequivalence, 394\ncontextual embeddings,\n186,231\ncontinued pretraining, 214\nconversation, 309\nconversation analysis, 313,\n328\nconversational implicature,\n314\nconversational speech, 333\nconvex, 90\ncoordination ambiguity, 396\ncopula, 365\nCORAAL, 333\ncorefer, 501\ncoreference chain, 502\ncoreference resolution, 502\ngender agreement, 508\nHobbs tree search\nalgorithm, 528\nnumber agreement, 507\nperson agreement, 508\nrecency preferences, 508\nselectional restrictions,\n509\nsyntactic (\u201cbinding\u201d)\nconstraints, 508\nverb semantics, 509\ncorpora, 13\ncorpus, 13\nATIS, 390\nBroadcast news, 355\nBrown, 13, 384\n\ufb01sher, 355\nLOB, 384\nregular expression\nsearching inside, 5\nSwitchboard, 13,333,\n335\nTimeBank, 451\nWall Street Journal, 355\ncorrection act detection,\n319\ncosine\nas a similarity metric,\n110\ncost function, 88\ncount nouns, 363\ncounters, 29\ncounts\ntreating low as zero, 379\nCRF, 376",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1026,
      "token_count": 785,
      "chapter_title": ""
    }
  },
  {
    "content": "weak and strong\nequivalence, 394\ncontextual embeddings,\n186,231\ncontinued pretraining, 214\nconversation, 309\nconversation analysis, 313,\n328\nconversational implicature,\n314\nconversational speech, 333\nconvex, 90\ncoordination ambiguity, 396\ncopula, 365\nCORAAL, 333\ncorefer, 501\ncoreference chain, 502\ncoreference resolution, 502\ngender agreement, 508\nHobbs tree search\nalgorithm, 528\nnumber agreement, 507\nperson agreement, 508\nrecency preferences, 508\nselectional restrictions,\n509\nsyntactic (\u201cbinding\u201d)\nconstraints, 508\nverb semantics, 509\ncorpora, 13\ncorpus, 13\nATIS, 390\nBroadcast news, 355\nBrown, 13, 384\n\ufb01sher, 355\nLOB, 384\nregular expression\nsearching inside, 5\nSwitchboard, 13,333,\n335\nTimeBank, 451\nWall Street Journal, 355\ncorrection act detection,\n319\ncosine\nas a similarity metric,\n110\ncost function, 88\ncount nouns, 363\ncounters, 29\ncounts\ntreating low as zero, 379\nCRF, 376\ncompared to HMM, 376\ninference, 380\nViterbi inference, 380\nCRFs\nlearning, 381\ncross-attention, 272\ncross-brackets, 406cross-entropy, 51\ncross-entropy loss, 88,145\ncross-validation, 69\n10-fold, 69\ncrowdsourcing, 485\nCTC, 341\ndatasheet, 16\ndative alternation, 463\ndebiasing, 127\ndecision boundary, 80,136\ndecoder-only model, 201\ndecoding, 207,372\nViterbi, 372\ndeep\nneural networks, 132\ndeep learning, 132\nde\ufb01nite reference, 504\ndegree adverb, 364\ndelexicalize, 320\ndemonstrations, 246\ndenoising, 226\ndependency\ngrammar, 411\ndependency tree, 414\ndependent, 412\nderivation\ndirect (in a formal\nlanguage), 392\nsyntactic, 389, 389, 392,\n392\nDet, 388\ndeterminer, 364, 388\nDeterminers, 364\ndevelopment set, 38\ndevelopment test set, 69\ndevelopment test set\n(dev-test), 39\ndevset, seedevelopment\ntest set (dev-test), 69\nDFT, 338\ndialogue, 309\ndialogue act\ncorrection, 319\nDialogue acts, 318\ndialogue policy, 319\ndialogue systems, 309\ndesign, 325\ndiathesis alternation, 463\ndiff program, 30\ndigit recognition, 332\ndigital divide, 263\ndigitization, 335\ndilated convolutions, 352\ndimension, 107\ndiphthong\norigin of term, 362\ndirect derivation (in a\nformal language),\n392\ndirectional adverb, 364\ndirective speech act, 312\ndisambiguation\nin parsing, 403\nsyntactic, 397\ndiscount, 47, 49\ndiscounting, 45\ndiscourse, 531segment, 534\ndiscourse connectives, 535\ndiscourse deixis, 503\ndiscourse model, 501\ndiscourse parsing, 536",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1027,
      "token_count": 789,
      "chapter_title": ""
    }
  },
  {
    "content": "dependent, 412\nderivation\ndirect (in a formal\nlanguage), 392\nsyntactic, 389, 389, 392,\n392\nDet, 388\ndeterminer, 364, 388\nDeterminers, 364\ndevelopment set, 38\ndevelopment test set, 69\ndevelopment test set\n(dev-test), 39\ndevset, seedevelopment\ntest set (dev-test), 69\nDFT, 338\ndialogue, 309\ndialogue act\ncorrection, 319\nDialogue acts, 318\ndialogue policy, 319\ndialogue systems, 309\ndesign, 325\ndiathesis alternation, 463\ndiff program, 30\ndigit recognition, 332\ndigital divide, 263\ndigitization, 335\ndilated convolutions, 352\ndimension, 107\ndiphthong\norigin of term, 362\ndirect derivation (in a\nformal language),\n392\ndirectional adverb, 364\ndirective speech act, 312\ndisambiguation\nin parsing, 403\nsyntactic, 397\ndiscount, 47, 49\ndiscounting, 45\ndiscourse, 531segment, 534\ndiscourse connectives, 535\ndiscourse deixis, 503\ndiscourse model, 501\ndiscourse parsing, 536\ndiscourse-new, 505\ndiscourse-old, 505\ndiscovery procedure, 408\ndiscrete Fourier transform,\n338\ndiscriminative model, 78\ndis\ufb02uency, 13\ndisjunction, 29\npipe in regular\nexpressions as, 8\nsquare braces in regular\nexpression as, 6\ndispreferred response, 330\ndistant supervision, 443\ndistributional hypothesis,\n101\ndistributional similarity,\n408\ndivergences between\nlanguages in MT,\n265\ndocument\nin IR, 291\ndocument frequency, 112\ndocument vector, 117\ndomination in syntax, 389\ndot product, 79,110\ndot-product attention, 180\nDragon Systems, 355\ndropout, 151\nduration\ntemporal expression, 452\ndynamic programming, 26\nand parsing, 397\nViterbi as, 373\ndynamic time warping, 355\nedge-factored, 426\nedit distance\nminimum algorithm, 26\nEDU, 534\neffect size, 70\nef\ufb01ciency costs, 317\nElaboration (as coherence\nrelation), 533\nELIZA, 4\nimplementation, 12\nsample conversation, 12\nElman Networks, 158\nELMo\nfor affect, 497\nEM\nfor deleted interpolation,\n48\nembedding layer, 154\nembeddings, 105\ncosine for similarity, 110\nskip-gram, learning, 120\nsparse, 110\ntf-idf, 112\nword2vec, 117\nemission probabilities, 370\nEmoLex, 484",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1028,
      "token_count": 655,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 595",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1029,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Subject Index 587\nemotion, 482\nEncoder-decoder, 175\nencoder-decoder attention,\n272\nend-to-end training, 166\nendpointing, 312\nEnglish\nlexical differences from\nFrench, 267\nsimpli\ufb01ed grammar\nrules, 390\nverb-framed, 267\nentity dictionary, 379\nentity grid, 542\nEntity linking, 520\nentity linking, 502\nentity-based coherence, 540\nentropy, 49\nand perplexity, 49\ncross-entropy, 51\nper-word, 50\nrate, 50\nrelative, 474\nerror backpropagation, 147\nESPnet, 356\nethos, 547\nEuclidean distance\nin L2 regularization, 96\nEugene Onegin , 52\nEuler\u2019s formula, 338\nEuroparl, 270\nevalb, 406\nevaluating parsers, 405\nevaluation\n10-fold cross-validation,\n69\nAB test, 353\ncomparing models, 41\ncross-validation, 69\ndevelopment test set, 39,\n69\ndevset, 69\ndevset or development\ntest set, 39\nextrinsic, 38\n\ufb02uency in MT, 280\nMatched-Pair Sentence\nSegment Word Error\n(MAPSSWE), 347\nmean opinion score, 353\nmost frequent class\nbaseline, 366\nMT, 280\nnamed entity recognition,\n240, 381\nof n-gram, 38\nof n-grams via\nperplexity, 40\npseudoword, 476\nrelation extraction, 446\ntest set, 39\ntraining on the test set, 39\ntraining set, 39\nTTS, 353\nevent coreference, 503\nevent extraction, 435,446\nevents, 450Evidence (as coherence\nrelation), 533\nevoking a referent, 501\nexecution accuracy, 256\nexpansion, 390, 391\nexpletive, 507\nexplicit con\ufb01rmation, 319\nextraposition, 507\nextrinsic evaluation, 38\nF (for F-measure), 67\nF-measure, 67\nF-measure\nin NER, 240, 381\nfactoid question, 289\nFaiss, 301\nfalse negatives, 9\nfalse positives, 9\nFarsi, verb-framed, 267\nfast Fourier transform, 338,\n355\nfasttext, 123\nFASTUS, 457\nfeature cutoff, 379\nfeature interactions, 82\nfeature selection\ninformation gain, 76\nfeature template, 421\nfeature templates, 82\npart-of-speech tagging,\n378\nfeature vectors, 334\nFederalist papers, 75\nfeedforward network, 138\nfenceposts, 398\nfew-shot, 246\nFFT, 338, 355\n\ufb01le format, .wav, 336\n\ufb01lled pause, 13\n\ufb01ller, 13\n\ufb01netuning, 213,235\n\ufb01netuning;supervsed, 249\n\ufb01rst-order co-occurrence,\n124\n\ufb02uency, 280\nin MT, 280\nfold (in cross-validation),\n69\nforget gate, 172\nformal language, 391\nformant synthesis, 357\nforward inference, 153\nforward-looking centers,\n541\nFosler, E., see\nFosler-Lussier, E.\nfoundation model, 222\nfragment of word, 13\nframe, 336",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1030,
      "token_count": 787,
      "chapter_title": ""
    }
  },
  {
    "content": "factoid question, 289\nFaiss, 301\nfalse negatives, 9\nfalse positives, 9\nFarsi, verb-framed, 267\nfast Fourier transform, 338,\n355\nfasttext, 123\nFASTUS, 457\nfeature cutoff, 379\nfeature interactions, 82\nfeature selection\ninformation gain, 76\nfeature template, 421\nfeature templates, 82\npart-of-speech tagging,\n378\nfeature vectors, 334\nFederalist papers, 75\nfeedforward network, 138\nfenceposts, 398\nfew-shot, 246\nFFT, 338, 355\n\ufb01le format, .wav, 336\n\ufb01lled pause, 13\n\ufb01ller, 13\n\ufb01netuning, 213,235\n\ufb01netuning;supervsed, 249\n\ufb01rst-order co-occurrence,\n124\n\ufb02uency, 280\nin MT, 280\nfold (in cross-validation),\n69\nforget gate, 172\nformal language, 391\nformant synthesis, 357\nforward inference, 153\nforward-looking centers,\n541\nFosler, E., see\nFosler-Lussier, E.\nfoundation model, 222\nfragment of word, 13\nframe, 336\nsemantic, 467\nframe elements, 467\nFrameNet, 466\nframes, 314\nfree word order, 411\nFreebase, 437\nfreeze, 155,214\nFrench, 265\nFrump, 459fully-connected, 138\nfunction word, 363, 383\nfusion language, 267\nGaussian\nprior on weights, 96\ngazetteer, 379\nGeneral Inquirer, 64,484\ngeneralize, 95\ngeneralized semantic role,\n464\ngeneration\nof sentences to test a\nCFG grammar, 390\ngenerative AI, 204\ngenerative grammar, 391\ngenerative model, 78\ngenerative models, 59\ngenerator, 389\ngenerics, 507\nGerman, 265\ngiven-new, 506\nGodzilla, speaker as, 472\ngold labels, 66\ngradient, 90\nGrammar\nConstraint, 433\nHead-Driven Phrase\nStructure (HPSG),\n406\nLink, 433\ngrammar\nbinary branching, 394\nchecking, 387\nequivalence, 394\ngenerative, 391\ninversion transduction,\n287\ngrammatical function, 412\ngrammatical relation, 412\ngrammatical sentences, 391\ngreedy decoding, 206\ngreedy RE patterns, 9\ngrep, 5, 5, 30\nGricean maxims, 314\ngrounding, 312\nGUS, 314\nhallucinate, 290\nhallucination, 219\nHamilton, Alexander, 75\nHamming, 337\nHansard, 287\nhanzi, 19\nharmonic mean, 67\nhead, 188,199,406,412\n\ufb01nding, 406\nHead-Driven Phrase\nStructure Grammar\n(HPSG), 406\nHeaps\u2019 Law, 14\nHearst patterns, 438\nheld-out, 48\nHerdan\u2019s Law, 14\nhidden, 370\nhidden layer, 138\nas representation of\ninput, 139hidden units, 138\nHindi, 265\nHindi, verb-framed, 267\nHKUST, 334\nHMM, 370\nformal de\ufb01nition of, 370",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1031,
      "token_count": 789,
      "chapter_title": ""
    }
  },
  {
    "content": "Head-Driven Phrase\nStructure (HPSG),\n406\nLink, 433\ngrammar\nbinary branching, 394\nchecking, 387\nequivalence, 394\ngenerative, 391\ninversion transduction,\n287\ngrammatical function, 412\ngrammatical relation, 412\ngrammatical sentences, 391\ngreedy decoding, 206\ngreedy RE patterns, 9\ngrep, 5, 5, 30\nGricean maxims, 314\ngrounding, 312\nGUS, 314\nhallucinate, 290\nhallucination, 219\nHamilton, Alexander, 75\nHamming, 337\nHansard, 287\nhanzi, 19\nharmonic mean, 67\nhead, 188,199,406,412\n\ufb01nding, 406\nHead-Driven Phrase\nStructure Grammar\n(HPSG), 406\nHeaps\u2019 Law, 14\nHearst patterns, 438\nheld-out, 48\nHerdan\u2019s Law, 14\nhidden, 370\nhidden layer, 138\nas representation of\ninput, 139hidden units, 138\nHindi, 265\nHindi, verb-framed, 267\nHKUST, 334\nHMM, 370\nformal de\ufb01nition of, 370\nhistory in speech\nrecognition, 355\ninitial distribution, 370\nobservation likelihood,\n370\nobservations, 370\nsimplifying assumptions\nfor POS tagging,\n372\nstates, 370\ntransition probabilities,\n370\nHobbs algorithm, 528\nHobbs tree search algorithm\nfor pronoun\nresolution, 528\nhomonymy, 232\nhot languages, 268\nHungarian\npart-of-speech tagging,\n382\nhybrid, 356\nhyperarticulation, 319\nhypernym, 437\nlexico-syntactic patterns\nfor, 438\nhyperparameter, 92\nhyperparameters, 152\nIBM Models, 287\nIBM Thomas J. Watson\nResearch Center,\n53, 355\nidf,113\nidf term weighting, 113,\n292\nimmediately dominates,\n389\nimplicature, 314\nimplicit argument, 479\nimplicit con\ufb01rmation, 320\nin-context learning, 247\ninde\ufb01nite reference, 504\ninduction heads, 247\ninference-based learning,\n429\ninfoboxes, 437\ninformation\nstructure, 505\nstatus, 505\ninformation extraction (IE),\n435\nbootstrapping, 441\ninformation gain, 76\nfor feature selection, 76\nInformation retrieval, 108,\n290\ninformation retrieval, 290\ninitiative, 313\ninner product, 110\ninstance, word, 14",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1032,
      "token_count": 607,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 596",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1033,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "588 Subject Index\nInstitutional Review Board,\n327\nInstruction tuning, 249\nintent determination, 316\nintercept, 79\nInterjections, 364\ninterpolated precision, 296\ninterpolation\nin smoothing, 48\ninterpretable, 98\ninterval algebra, 448\nintrinsic evaluation, 38\ninversion transduction\ngrammar (ITG), 287\ninverted index, 295\nIO, 238, 368\nIOB tagging\nfor temporal expressions,\n453\nIR,290\nidf term weighting, 113,\n292\nterm weighting, 291\nvector space model, 107\nIRB, 327\nis-a, 437\nISO 8601, 454\nisolating language, 267\niSRL, 479\nITG (inversion transduction\ngrammar), 287\nJapanese, 265, 267\nJay, John, 75\njoint intention, 328\nKaldi, 356\nKBP, 459\nKenLM, 38,53\nkey,188\nKL divergence, 474\nKlatt formant synthesizer,\n357\nKleene *, 7\nsneakiness of matching\nzero things, 7\nKleene +, 7\nknowledge claim, 549\nknowledge graphs, 435\nKullback-Leibler\ndivergence, 474\nKV cache, 217\nL1 regularization, 96\nL2 regularization, 96\nlabeled precision, 405\nlabeled recall, 405\nlanguage\nidenti\ufb01cation, 354\nuniversal, 264\nlanguage id, 56\nlanguage model, 32\nlanguage model:coined by,\n53\nlanguage modeling head,\n199\nLaplace smoothing, 45for PMI, 116\nlasso regression, 96\nlatent semantic analysis,\n130\nlayer norm, 192\nLDC, 19\nlearning rate, 91\nlemma, 15,102\nversus wordform, 15\nLemmatization, 23\nlemmatization, 5\nLevenshtein distance, 25\nlexical\ncategory, 389\ncohesion, 532, 544\ngap, 267\nsemantics, 102\ntrigger, in IE, 452\nlexico-syntactic pattern,\n438\nlexicon, 388\nLibriSpeech, 333\nlight verbs, 447\nlikelihood, 59\nlinear chain CRF, 376, 377\nlinear classi\ufb01ers, 60\nlinear interpolation for\nn-grams, 48\nlinearly separable, 136\nLinguistic Data\nConsortium, 19\nLinguistic Discourse\nmodel, 550\nLink Grammar, 433\nList (as coherence relation),\n534\nlisten attend and spell, 339\nLIWC, 64,485\nLM, 32\nLOB corpus, 384\nlocalization, 263\nlocation-based attention,\n351\nlocative, 364\nlocative adverb, 364\nlog\nwhy used for\nprobabilities, 37\nwhy used to compress\nspeech, 336\nlog likelihood ratio, 493\nlog odds ratio, 493\nlog probabilities, 37, 37\nlogistic function, 79\nlogistic regression, 77\nconditional maximum\nlikelihood\nestimation, 88\nGaussian priors, 96\nlearning in, 87\nregularization, 96\nrelation to neural\nnetworks, 140\nlogit, 80,200\nlogit lens, 200\nlogos, 547\nlong short-term memory,",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1034,
      "token_count": 784,
      "chapter_title": ""
    }
  },
  {
    "content": "lexico-syntactic pattern,\n438\nlexicon, 388\nLibriSpeech, 333\nlight verbs, 447\nlikelihood, 59\nlinear chain CRF, 376, 377\nlinear classi\ufb01ers, 60\nlinear interpolation for\nn-grams, 48\nlinearly separable, 136\nLinguistic Data\nConsortium, 19\nLinguistic Discourse\nmodel, 550\nLink Grammar, 433\nList (as coherence relation),\n534\nlisten attend and spell, 339\nLIWC, 64,485\nLM, 32\nLOB corpus, 384\nlocalization, 263\nlocation-based attention,\n351\nlocative, 364\nlocative adverb, 364\nlog\nwhy used for\nprobabilities, 37\nwhy used to compress\nspeech, 336\nlog likelihood ratio, 493\nlog odds ratio, 493\nlog probabilities, 37, 37\nlogistic function, 79\nlogistic regression, 77\nconditional maximum\nlikelihood\nestimation, 88\nGaussian priors, 96\nlearning in, 87\nregularization, 96\nrelation to neural\nnetworks, 140\nlogit, 80,200\nlogit lens, 200\nlogos, 547\nlong short-term memory,\n172lookahead in regex, 13\nLoRA, 218\nloss, 88\nlow frame rate, 340\nLPC (Linear Predictive\nCoding), 355\nLSI, seelatent semantic\nanalysis\nLSTM, 385\nLUNAR, 307\nmachine learning\nfor NER, 382\ntextbooks, 75, 100\nmachine translation, 263\nmacroaveraging, 68\nMadison, James, 75\nMAE, 15\nMandarin, 265\nManhattan distance\nin L1 regularization, 96\nmanner adverb, 364\nMarkov, 34\nassumption, 34\nMarkov assumption, 369\nMarkov chain, 52, 369\nformal de\ufb01nition of, 370\ninitial distribution, 370\nn-gram as, 369\nstates, 370\ntransition probabilities,\n370\nMarkov model, 34\nformal de\ufb01nition of, 370\nhistory, 53\nMarx, G., 387\nMasked Language\nModeling, 226\nmass nouns, 363\nmaxent, 100\nmaxim, Gricean, 314\nmaximum entropy, 100\nmaximum spanning tree,\n426\nMayan, 267\nMBR, 277\nMcNemar\u2019s test, 348\nmean\nelement-wise, 167\nmean average precision,\n297\nmean opinion score, 353\nmean reciprocal rank, 305\nmechanical indexing, 129\nMechanical Turk, 332\nmel, 338\nmemory networks, 202\nmention detection, 510\nmention-pair, 513\nmentions, 501\nMERT, for training in MT,\n287\nMeSH (Medical Subject\nHeadings), 57\nMessage Understanding\nConference, 457\nMETEOR, 288\nmetonymy, 530microaveraging, 68\nMicrosoft .wav format, 336\nmini-batch, 94\nMinimum Bayes risk, 277\nminimum edit distance, 25,\n25, 373\nexample of, 28\nfor speech recognition\nevaluation, 346\nMINIMUM EDIT DISTANCE ,\n28\nminimum edit distance\nalgorithm, 26\nMinimum Error Rate\nTraining, 287\nMLE",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1035,
      "token_count": 788,
      "chapter_title": ""
    }
  },
  {
    "content": "formal de\ufb01nition of, 370\nhistory, 53\nMarx, G., 387\nMasked Language\nModeling, 226\nmass nouns, 363\nmaxent, 100\nmaxim, Gricean, 314\nmaximum entropy, 100\nmaximum spanning tree,\n426\nMayan, 267\nMBR, 277\nMcNemar\u2019s test, 348\nmean\nelement-wise, 167\nmean average precision,\n297\nmean opinion score, 353\nmean reciprocal rank, 305\nmechanical indexing, 129\nMechanical Turk, 332\nmel, 338\nmemory networks, 202\nmention detection, 510\nmention-pair, 513\nmentions, 501\nMERT, for training in MT,\n287\nMeSH (Medical Subject\nHeadings), 57\nMessage Understanding\nConference, 457\nMETEOR, 288\nmetonymy, 530microaveraging, 68\nMicrosoft .wav format, 336\nmini-batch, 94\nMinimum Bayes risk, 277\nminimum edit distance, 25,\n25, 373\nexample of, 28\nfor speech recognition\nevaluation, 346\nMINIMUM EDIT DISTANCE ,\n28\nminimum edit distance\nalgorithm, 26\nMinimum Error Rate\nTraining, 287\nMLE\nfor n-grams, 35\nfor n-grams, intuition, 36\nMLM, 226\nMLP, 138\nMMLU, 258,304\nmodal verb, 365\nmodel alignment, 249\nmodel card, 74\nmorpheme, 23\nMOS (mean opinion score),\n353\nMoses, Michelangelo statue\nof, 309\nMoses, MT toolkit, 287\nMRR, 305\nMS MARCO, 303\nMT, 263\ndivergences, 265\npost-editing, 263\nmu-law, 336\nMUC, 457, 459\nMUC F-measure, 524\nmulti-head attention, 189\nmulti-hop, 303\nmulti-layer perceptrons,\n138\nmultinomial logistic\nregression, 84\nmultinomial naive Bayes,\n57\nmultinomial naive Bayes\nclassi\ufb01er, 57\nmultiword expressions, 130\nMWE, 130\nn-best list, 341\nn-gram, 32,34\nadd-one smoothing, 45\nas approximation, 34\nas generators, 43\nas Markov chain, 369\nequation for, 35\nexample of, 36, 37\nfor Shakespeare, 43\nhistory of, 53\ninterpolation, 48\nKenLM, 38,53\nlogprobs in, 37\nnormalizing, 36\nparameter estimation, 35\nsensitivity to corpus, 43\nsmoothing, 45",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1036,
      "token_count": 628,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 597",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1037,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Subject Index 589\nSRILM, 53\ntest set, 38\ntraining set, 38\nnaive Bayes\nmultinomial, 57\nsimplifying assumptions,\n59\nnaive Bayes assumption, 59\nnaive Bayes classi\ufb01er\nuse in text categorization,\n57\nnamed entity, 237,362,367\nlist of types, 238, 367\nnamed entity recognition,\n237,367\nnatural language inference,\n237\nNatural Questions, 303\nnegative log likelihood loss,\n88,97,146\nNER, 237,367\nneural networks\nrelation to logistic\nregression, 140\nnewline character, 10\nNext Sentence Prediction,\n228\nNIST for MT evaluation,\n288\nnoisy-or, 442\nNomBank, 466\nNominal, 388\nnon-capturing group, 12\nnon-greedy, 9\nnon-standard words, 349\nnon-stationary process, 336\nnon-terminal symbols, 389,\n390\nnormal form, 394, 394\nnormalization\ntemporal, 453\nword, 23\nnormalization of\nprobabilities, 35\nnormalize, 83\nnormalizing, 140\nnoun\nabstract, 363\ncommon, 363\ncount, 363\nmass, 363\nproper, 363\nnoun phrase, 388\nconstituents, 388\nNouns, 363\nNP,388, 390\nnucleus, 533\nnull hypothesis, 70\nNyquist frequency, 335\nobservation likelihood\nrole in Viterbi, 374\none-hot vector, 153,197\nopen book, 304\nopen class, 363\nopen information\nextraction, 444\noperation list, 25operator precedence, 8, 9\noptionality\nuse of ? in regular\nexpressions for, 6\noutput gate, 173\nover\ufb01tting, 95\np-value, 71\nPaired, 71\nparallel corpus, 270\nparallel distributed\nprocessing, 157\nparallelogram model, 124\nparameter-ef\ufb01cient \ufb01ne\ntuning, 217\nparse tree, 389, 391\nPARSEV AL, 405\nparsing\nambiguity, 395\nCKY , 397\nCYK, seeCKY\nevaluation, 405\nrelation to grammars,\n392\nsyntactic, 387\nwell-formed substring\ntable, 409\npart of speech\nas used in CFG, 389\npart-of-speech\nadjective, 364\nadverb, 364\nclosed class, 363\ninterjection, 364\nnoun, 363\nopen class, 363\nparticle, 364\nsubtle distinction\nbetween verb and\nnoun, 364\nverb, 364\npart-of-speech tagger\nPARTS ,384\nTAGGIT, 384\nPart-of-speech tagging, 365\npart-of-speech tagging\nambiguity and, 365\namount of ambiguity in\nBrown corpus, 366\nand morphological\nanalysis, 382\nfeature templates, 378\nhistory of, 384\nHungarian, 382\nTurkish, 382\nunknown words, 376\nparticle, 364\nPARTS tagger, 384\nparts of speech, 362\npathos, 547\npattern, regular expression,\n5\nPCM (Pulse Code\nModulation), 336\nPDP, 157\nPDTB, 535\nPEFT, 217Penn Discourse TreeBank,\n535\nPenn Treebank, 393",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1038,
      "token_count": 787,
      "chapter_title": ""
    }
  },
  {
    "content": "parsing\nambiguity, 395\nCKY , 397\nCYK, seeCKY\nevaluation, 405\nrelation to grammars,\n392\nsyntactic, 387\nwell-formed substring\ntable, 409\npart of speech\nas used in CFG, 389\npart-of-speech\nadjective, 364\nadverb, 364\nclosed class, 363\ninterjection, 364\nnoun, 363\nopen class, 363\nparticle, 364\nsubtle distinction\nbetween verb and\nnoun, 364\nverb, 364\npart-of-speech tagger\nPARTS ,384\nTAGGIT, 384\nPart-of-speech tagging, 365\npart-of-speech tagging\nambiguity and, 365\namount of ambiguity in\nBrown corpus, 366\nand morphological\nanalysis, 382\nfeature templates, 378\nhistory of, 384\nHungarian, 382\nTurkish, 382\nunknown words, 376\nparticle, 364\nPARTS tagger, 384\nparts of speech, 362\npathos, 547\npattern, regular expression,\n5\nPCM (Pulse Code\nModulation), 336\nPDP, 157\nPDTB, 535\nPEFT, 217Penn Discourse TreeBank,\n535\nPenn Treebank, 393\ntagset, 365, 365\nPenn Treebank\ntokenization, 19\nper-word entropy, 50\nperceptron, 135\nperiod disambiguation, 82\nperplexity, 40,52\nas weighted average\nbranching factor, 41\nde\ufb01ned via\ncross-entropy, 52\nperplexity:coined by, 53\npersonal pronoun, 364\npersuasion, 548\nphrasal verb, 364\nphrase-based translation,\n287\nphrase-structure grammar,\n388\nPII,212\npipe, 8\nplanning\nand speech acts, 329\nshared plans, 328\npleonastic, 507\nPointwise mutual\ninformation, 114\npolysynthetic language, 267\npooling, 143,166\nmax, 167\nmean, 166\nPorter stemmer, 24\nPOS, 362\nposition embeddings\nrelative, 199\npositional embeddings, 198\npossessive pronoun, 364\npost-editing, 263\npost-training, 249\npostings, 295\npostposition, 265\nPotts diagram, 492\nPP, 390\nPP-attachment ambiguity,\n396\nPPMI, 115\nprecedence, 8\nprecedence, operator, 8\nPrecision, 67\nprecision\nfor MT evaluation, 288\nin NER, 240, 381\nprecision-recall curve, 296\npremises, 547\nprepositional phrase\nconstituency, 390\nprepositions, 364\npresequences, 313\npretraining, 145,203\nprimitive decomposition,\n476\nprinciple of contrast, 103\nprior probability, 59\npro-drop languages, 268probabilistic context-free\ngrammars, 409\nproductions, 388\nprojective, 414\nprompt, 243\nprompt engineering, 243\npronoun, 364\nbound, 504\ndemonstrative, 505\nnon-binary, 508\npersonal, 364\npossessive, 364\nwh-, 364\nPropBank, 465\nproper noun, 363\nPROTO -AGENT ,464\nPROTO -PATIENT ,464\npseudoword, 476",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1039,
      "token_count": 791,
      "chapter_title": ""
    }
  },
  {
    "content": "position embeddings\nrelative, 199\npositional embeddings, 198\npossessive pronoun, 364\npost-editing, 263\npost-training, 249\npostings, 295\npostposition, 265\nPotts diagram, 492\nPP, 390\nPP-attachment ambiguity,\n396\nPPMI, 115\nprecedence, 8\nprecedence, operator, 8\nPrecision, 67\nprecision\nfor MT evaluation, 288\nin NER, 240, 381\nprecision-recall curve, 296\npremises, 547\nprepositional phrase\nconstituency, 390\nprepositions, 364\npresequences, 313\npretraining, 145,203\nprimitive decomposition,\n476\nprinciple of contrast, 103\nprior probability, 59\npro-drop languages, 268probabilistic context-free\ngrammars, 409\nproductions, 388\nprojective, 414\nprompt, 243\nprompt engineering, 243\npronoun, 364\nbound, 504\ndemonstrative, 505\nnon-binary, 508\npersonal, 364\npossessive, 364\nwh-, 364\nPropBank, 465\nproper noun, 363\nPROTO -AGENT ,464\nPROTO -PATIENT ,464\npseudoword, 476\nPTRANS, 477\npunctuation\nfor numbers\ncross-linguistically,\n19\nfor sentence\nsegmentation, 24\ntokenization, 19\ntreated as words, 13\ntreated as words in LM,\n44\nQA, 289\nquantization, 335\nquery, 188,291\nin IR, 291\nquestion\nfactoid, 289\nquestion answering\nfactoid questions, 289\nRadio Rex, 331\nRAG, 290,302\nrandom sampling, 208\nrange, regular expression, 6\nranking, 281\nrarefaction, 335\nRDF, 437\nRDF triple, 437\nRead speech, 333\nreading comprehension,\n304\nReason (as coherence\nrelation), 533\nRecall, 67\nrecall\nfor MT evaluation, 288\nin NER, 240, 381\nrectangular, 336\nreference\nbound pronouns, 504\ncataphora, 504\nde\ufb01nite, 504\ngenerics, 507\ninde\ufb01nite, 504\nreference point, 449\nreferent, 501\naccessing of, 501\nevoking of, 501\nreferential density, 268",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1040,
      "token_count": 563,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 598",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1041,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "590 Subject Index\nre\ufb02exive, 508\nregex\nregular expression, 5\nregister in regex, 12\nregression\nlasso, 96\nridge, 96\nregular expression, 5, 29\nsubstitutions, 11\nregularization, 95\nrejection\nconversation act, 320\nrelatedness, 103\nrelation extraction, 435\nrelative\ntemporal expression, 452\nrelative entropy, 474\nrelative frequency, 36\nrelevance, 314\nrelexicalize, 321\nReLU, 134\nreporting events, 447\nrepresentation learning, 101\nrepresentational harm, 127\nrepresentational harms, 73\nrescore, 341\nresidual stream, 191\nresolve, 366\nResource Management, 355\nretrieval-augmented\ngeneration, 302\nReVerb, 445\nrewrite, 389\nRhetorical Structure\nTheory, seeRST\nRiau Indonesian, 364\nridge regression, 96\nRLHF, 325\nRNN-T, 345\nrole-\ufb01ller extraction, 457\nRosebud, sled named, 531\nrow vector, 108\nRST, 533\nTreeBank, 535, 550\nrules\ncontext-free, 388\ncontext-free, expansion,\n389\ncontext-free, sample, 390\nRussian\nfusion language, 267\nverb-framed, 267\nSas start symbol in CFG,\n390\nsalience, in discourse\nmodel, 506\nSampling, 42\nsampling\nof analog waveform, 335\nrate, 335\nsatellite, 267,533\nsatellite-framed language,\n267\nsaturated, 135\nscaling laws, 216\nSCISOR, 459\nsclite, 347sclite package, 30\nscript\nSchankian, 467\nscripts, 456\nSDRT (Segmented\nDiscourse\nRepresentation\nTheory), 550\nsearch engine, 290\nsearch tree, 274\nsecond-order\nco-occurrence, 124\nseed pattern in IE, 441\nseed tuples, 441\nsegmentation\nsentence, 24\nword, 18\nselectional association, 475\nselectional preference\nstrength, 474\nselectional preferences\npseudowords for\nevaluation, 476\nselectional restriction, 472\nrepresenting with events,\n473\nviolations in WSD, 474\nself-supervision, 118,163,\n210\nself-training, 155\nsemantic drift in IE, 442\nsemantic feature, 130\nsemantic \ufb01eld, 103\nsemantic frame, 104\nsemantic relations in IE,\n436\ntable, 437\nsemantic role, 462, 462,\n464\nSemantic role labeling, 468\nsemantics\nlexical, 102\nsense\nword, 232\nsentence\nerror rate, 347\nsegmentation, 24\nsentence realization, 320\nsentence segmentation, 5\nsentence separation, 176\nSentencePiece, 270\nsentiment, 104\norigin of term, 500\nsentiment analysis, 56\nsentiment lexicons, 64\nSentiWordNet, 490\nsequence labeling, 362\nSFT, 249\nSGNS, 117\nShakespeare\nn-gram approximations\nto, 43\nshallow discourse parsing,\n539\nshared plans, 328\nside sequence, 313\nsigmoid, 79,133\nsigni\ufb01cance testMAPSSWE for ASR,\n347\nMcNemar\u2019s, 348\nsimilarity, 103",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1042,
      "token_count": 789,
      "chapter_title": ""
    }
  },
  {
    "content": "selectional preferences\npseudowords for\nevaluation, 476\nselectional restriction, 472\nrepresenting with events,\n473\nviolations in WSD, 474\nself-supervision, 118,163,\n210\nself-training, 155\nsemantic drift in IE, 442\nsemantic feature, 130\nsemantic \ufb01eld, 103\nsemantic frame, 104\nsemantic relations in IE,\n436\ntable, 437\nsemantic role, 462, 462,\n464\nSemantic role labeling, 468\nsemantics\nlexical, 102\nsense\nword, 232\nsentence\nerror rate, 347\nsegmentation, 24\nsentence realization, 320\nsentence segmentation, 5\nsentence separation, 176\nSentencePiece, 270\nsentiment, 104\norigin of term, 500\nsentiment analysis, 56\nsentiment lexicons, 64\nSentiWordNet, 490\nsequence labeling, 362\nSFT, 249\nSGNS, 117\nShakespeare\nn-gram approximations\nto, 43\nshallow discourse parsing,\n539\nshared plans, 328\nside sequence, 313\nsigmoid, 79,133\nsigni\ufb01cance testMAPSSWE for ASR,\n347\nMcNemar\u2019s, 348\nsimilarity, 103\ncosine, 110\nsingleton, 502\nsingular they, 508\nskip-gram, 117\nslot error rate, 317\nslot \ufb01lling, 316,459\nslots, 314\nsmoothing, 45, 45\nadd-one, 45\ninterpolation, 48\nLaplace, 45\nlinear interpolation, 48\nsoftmax, 85,140\nSOV language, 265\nspam detection, 56,64\nspan, 403\nSpeaker diarization, 353\nspeaker identi\ufb01cation, 354\nspeaker recognition, 354\nspeaker veri\ufb01cation, 354\nspeech\ntelephone bandwidth,\n335\nspeech acts, 312\nspeech recognition\narchitecture, 332, 339\nhistory of, 354\nspeech synthesis, 332\nsplit-half reliability, 487\nSRILM, 53\nSRL, 468\nStacked RNNs, 169\nstandardize, 82\nstart symbol, 389\nstates, 450\nstatic embeddings, 118\nstationary process, 336\nstationary stochastic\nprocess, 51\nstatistical MT, 287\nstatistical signi\ufb01cance\nMAPSSWE for ASR,\n347\nMcNemar\u2019s test, 348\nstatistically signi\ufb01cant, 71\nstative expressions, 450\nstem, 23\nStemming, 5\nstemming, 24\nstop list, 294\nstop words, 61\nstreaming, 345\nstride, 336\nstructural ambiguity, 395\nstupid backoff, 49\nsubdialogue, 313\nsubjectivity, 481,500\nsubstitutability, 408\nsubstitution operator\n(regular\nexpressions), 11\nsubword tokens, 18\nsubwords, 21\nsupervised \ufb01netuning, 249supervised machine\nlearning, 57\nSVD, 130\nSVO language, 265\nSwedish, verb-framed, 267\nSwitchboard, 333\nSwitchboard Corpus, 13,\n333, 335\nsynchronous grammar, 287\nsynonyms, 103\nsyntactic disambiguation,\n397\nsyntax, 387\norigin of term, 362",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1043,
      "token_count": 788,
      "chapter_title": ""
    }
  },
  {
    "content": "standardize, 82\nstart symbol, 389\nstates, 450\nstatic embeddings, 118\nstationary process, 336\nstationary stochastic\nprocess, 51\nstatistical MT, 287\nstatistical signi\ufb01cance\nMAPSSWE for ASR,\n347\nMcNemar\u2019s test, 348\nstatistically signi\ufb01cant, 71\nstative expressions, 450\nstem, 23\nStemming, 5\nstemming, 24\nstop list, 294\nstop words, 61\nstreaming, 345\nstride, 336\nstructural ambiguity, 395\nstupid backoff, 49\nsubdialogue, 313\nsubjectivity, 481,500\nsubstitutability, 408\nsubstitution operator\n(regular\nexpressions), 11\nsubword tokens, 18\nsubwords, 21\nsupervised \ufb01netuning, 249supervised machine\nlearning, 57\nSVD, 130\nSVO language, 265\nSwedish, verb-framed, 267\nSwitchboard, 333\nSwitchboard Corpus, 13,\n333, 335\nsynchronous grammar, 287\nsynonyms, 103\nsyntactic disambiguation,\n397\nsyntax, 387\norigin of term, 362\nTAC KBP, 438\nTacotron2, 351\nTACRED dataset, 437\nTAGGIT, 384\ntagset\nPenn Treebank, 365, 365\ntable of Penn Treebank\ntags, 365\nTamil, 267\ntanh, 134\ntarget embedding, 122\ntask error rate, 317\nTay, 326\nteacher forcing, 164,178,\n210, 274\ntechnai, 362\ntelephone-bandwidth\nspeech, 335\ntelic, 450\ntemperature sampling, 209\ntemplate, 245\ntemplate \ufb01lling, 435,456\ntemplate recognition, 456\ntemplate, in IE, 456\ntemplates, 244\ntemporal adverb, 364\ntemporal anchor, 455\ntemporal expression\nabsolute, 452\nmetaphor for, 449\nrelative, 452\ntemporal logic, 447\ntemporal normalization,\n453\nterm\nin IR, 291\nweight in IR, 291\nterm frequency, 112\nterm weight, 291\nterm-document matrix, 106\nterm-term matrix, 109\nterminal symbol, 389\ntest set, 38\ndevelopment, 39\nhow to choose, 39\ntext categorization, 56\nbag-of-words\nassumption, 58\nnaive Bayes approach, 57\nunknown words, 61\ntext normalization, 4,16\ntext summarization, 205\ntext-to-speech, 332",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1044,
      "token_count": 619,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 599",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1045,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Subject Index 591\nTextTiling, 544\ntf-idf, 113\nThe Pile, 211\nthematic grid, 463\nthematic role, 462\nand diathesis alternation,\n463\nexamples of, 462\nproblems, 464\ntheme, 462\ntheme, as thematic role, 462\nTimeBank, 451\ntokenization, 4\nsentence, 24\nword, 18\nTop-k sampling, 208\ntop-p sampling, 209\ntopic models, 104\ntoxicity detection, 74\ntraining oracle, 419\ntraining set, 38\ncross-validation, 69\nhow to choose, 39\ntranscription\nof speech, 331\nreference, 346\ntransduction grammars, 287\ntransfer learning, 223\nTransformations and\nDiscourse Analysis\nProject (TDAP),\n384\ntransition probability\nrole in Viterbi, 374\ntransition-based, 416\ntranslation\ndivergences, 265\nTREC, 308\ntreebank, 392\ntrigram, 38\nTTS, 332Turk, Mechanical, 332\nTurkish\nagglutinative, 267\npart-of-speech tagging,\n382\nturns, 311\nTyDi QA, 304\ntyped dependency structure,\n411\ntypes\nword, 14\ntypology, 265\nlinguistic, 265\nunembedding, 200\nungrammatical sentences,\n391\nunigram\nname of tokenization\nalgorithm, 270\nunit production, 397\nunit vector, 111\nUniversal Dependencies,\n413\nuniversal, linguistic, 264\nUnix, 5\nunknown words\nin part-of-speech\ntagging, 376\nin text categorization, 61\nuser-centered design, 325\nutterance, 13\nvalue, 188\nvalue sensitive design, 326\nvanishing gradient, 135\nvanishing gradients, 172\nVauquois triangle, 286\nvector, 107,133\nvector length, 110Vector semantics, 105\nvector semantics, 101\nvector space, 107\nvector space model, 107\nverb\ncopula, 365\nmodal, 365\nphrasal, 364\nverb alternations, 463\nverb phrase, 390\nverb-framed language, 267\nVerbs, 364\nVietnamese, 267\nViterbi\nand beam search, 275\nViterbi algorithm, 26, 373\ninference in CRF, 380\nVITERBI ALGORITHM , 373\nvocoder, 349\nvocoding, 349\nvoice user interface, 325\nVSO language, 265\nwake word, 353\nWall Street Journal\nWall Street Journal\nspeech recognition of,\n355\nwarping, 355\nwave\ufb01le format, 336\nWaveNet, 351\nWavenet, 351\nweight tying, 165,200\nwell-formed substring\ntable, 409\nWFST, 409\nwh-pronoun, 364\nwiki\ufb01cation, 520\nwildcard, regular\nexpression, 7Winograd Schema, 525\nWizard-of-Oz system, 325\nword\nboundary, regular\nexpression notation,\n8\nclosed class, 363\nde\ufb01nition of, 13\nerror rate, 334, 346\nfragment, 13\nfunction, 363, 383\nopen class, 363\npunctuation as, 13\ntokens, 14\ntypes, 14\nword normalization, 23",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1046,
      "token_count": 783,
      "chapter_title": ""
    }
  },
  {
    "content": "phrasal, 364\nverb alternations, 463\nverb phrase, 390\nverb-framed language, 267\nVerbs, 364\nVietnamese, 267\nViterbi\nand beam search, 275\nViterbi algorithm, 26, 373\ninference in CRF, 380\nVITERBI ALGORITHM , 373\nvocoder, 349\nvocoding, 349\nvoice user interface, 325\nVSO language, 265\nwake word, 353\nWall Street Journal\nWall Street Journal\nspeech recognition of,\n355\nwarping, 355\nwave\ufb01le format, 336\nWaveNet, 351\nWavenet, 351\nweight tying, 165,200\nwell-formed substring\ntable, 409\nWFST, 409\nwh-pronoun, 364\nwiki\ufb01cation, 520\nwildcard, regular\nexpression, 7Winograd Schema, 525\nWizard-of-Oz system, 325\nword\nboundary, regular\nexpression notation,\n8\nclosed class, 363\nde\ufb01nition of, 13\nerror rate, 334, 346\nfragment, 13\nfunction, 363, 383\nopen class, 363\npunctuation as, 13\ntokens, 14\ntypes, 14\nword normalization, 23\nword segmentation, 18,20\nword sense, 232\nword sense disambiguation,\n232,seeWSD\nword shape, 378\nword tokenization, 18\nword-word matrix, 109\nword2vec, 117\nwordform, 15\nand lemma, 102\nversus lemma, 15\nWordNet, 232\nwordpiece, 269\nWSD, 232\nYonkers Racetrack, 49\nYupik, 267\nz-score, 82\nzero anaphor, 505\nzero-shot, 246\nzero-width, 13\nzeros, 45",
    "metadata": {
      "source": "ed3book_Jan25",
      "chunk_id": 1047,
      "token_count": 436,
      "chapter_title": ""
    }
  }
]