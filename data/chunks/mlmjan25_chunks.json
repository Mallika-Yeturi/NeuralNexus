[
  {
    "content": "# mlmjan25\n\n## Page 1\n\nMasked Language ModelsBERT\n\n## Page 2\n\nMasked Language Modeling\u2022We've seen autoregressive (causal, left-to-right) LMs.\u2022But what about tasks for which we want to peak at future tokens?\u2022Especially true for tasks where we map each input token to an output token\u2022Bidirectional encoders use masked self-attention to \u2022map sequences of input embeddings (x1,...,xn) \u2022to sequences of output embeddings of the same length (h1,...,hn), \u2022where the output vectors have been contextualized using information from the entire input sequence. \n\n## Page 3\n\nBidirectional Self-Attention\na) A causal self-attention layerb) A bidirectional self-attention layer\nattentionattentionattentionattentionattentiona1a2a3a4a5x3x4x5x1x2\nattentionattentionattentionattentionattentiona1a2a3a4a5x3x4x5x1x2",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 0,
      "token_count": 213,
      "chapter_title": "mlmjan25"
    }
  },
  {
    "content": "## Page 4",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 1,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Easy!  We just remove the maskq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NN\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221eq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NNq1\u2022k2q1\u2022k3q1\u2022k4q2\u2022k3q2\u2022k4q3\u2022k411.1\u2022BIDIRECTIONALTRANSFORMERENCODERS3(repeated from Eq.??for a single attention head):head=softmax\u2713mask\u2713QK|pdk\u25c6\u25c6V(11.1)q1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NN\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221eq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NNq1\u2022k2q1\u2022k3q1\u2022k4q2\u2022k3q2\u2022k4q3\u2022k4(a) (b)Figure 11.2TheN\u21e5NQK|matrix showing theqi\u00b7kjvalues, with the upper-triangleportion of the comparisons matrix zeroed out (set to\u0000\u2022, which the softmax will turn tozero).Fig.11.2shows the masked version ofQK|and the unmasked version. For bidi-rectional attention, we use the unmasked version of Fig.11.2b. Thus the attentioncomputation for bidirectional attention is exactly the same as Eq.11.1but with themask removed:head=softmax\u2713QK|pdk\u25c6V(11.2)Otherwise, the attention computation is identical to what we saw in Chapter 9, as isthe transformer block architecture (the feedforward layer, layer norm, and so on). Asin Chapter 9, the input is also a series of subword tokens, usually computed by one ofthe 3 popular tokenization algorithms (including the BPE algorithm that we alreadysaw in Chapter 2 and two others, the WordPiece algorithm and the SentencePieceUnigram LM algorithm). That means every input sentence \ufb01rst has to be tokenized,and all further processing takes place on subword tokens rather than words. This willrequire, as we\u2019ll see in the third part of the textbook, that for some NLP tasks thatrequire notions of words (like parsing) we will occasionally need to map subwordsback to words.To make this more concrete, the original English-only bidirectional transformerencoder model, BERT (Devlin et al.,2019), consisted of the following:\u2022An English-only subword vocabulary consisting of 30,000 tokens generatedusing the WordPiece algorithm (Schuster and Nakajima,2012).\u2022Input context windowN=512 tokens, and model dimensionalityd=768\u2022SoX, the input to the model, is of shape[N\u21e5d]=[512\u21e5768].\u2022L=12 layers of transformer blocks, each withA=12 (bidirectional) multiheadattention layers.\u2022The resulting model has about 100M parameters.The larger",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 2,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "computation is identical to what we saw in Chapter 9, as isthe transformer block architecture (the feedforward layer, layer norm, and so on). Asin Chapter 9, the input is also a series of subword tokens, usually computed by one ofthe 3 popular tokenization algorithms (including the BPE algorithm that we alreadysaw in Chapter 2 and two others, the WordPiece algorithm and the SentencePieceUnigram LM algorithm). That means every input sentence \ufb01rst has to be tokenized,and all further processing takes place on subword tokens rather than words. This willrequire, as we\u2019ll see in the third part of the textbook, that for some NLP tasks thatrequire notions of words (like parsing) we will occasionally need to map subwordsback to words.To make this more concrete, the original English-only bidirectional transformerencoder model, BERT (Devlin et al.,2019), consisted of the following:\u2022An English-only subword vocabulary consisting of 30,000 tokens generatedusing the WordPiece algorithm (Schuster and Nakajima,2012).\u2022Input context windowN=512 tokens, and model dimensionalityd=768\u2022SoX, the input to the model, is of shape[N\u21e5d]=[512\u21e5768].\u2022L=12 layers of transformer blocks, each withA=12 (bidirectional) multiheadattention layers.\u2022The resulting model has about 100M parameters.The larger multilingual XLM-RoBERTa model, trained on 100 languages, has\u2022A multilingual subword vocabulary with 250,000 tokens generated using theSentencePiece Unigram LM algorithm (Kudo and Richardson,2018).11.1\u2022BIDIRECTIONALTRANSFORMERENCODERS3(repeated from Eq.??for a single attention head):head=softmax\u2713mask\u2713QK|pdk\u25c6\u25c6V(11.1)q1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NN\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221eq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NNq1\u2022k2q1\u2022k3q1\u2022k4q2\u2022k3q2\u2022k4q3\u2022k4(a) (b)Figure 11.2TheN\u21e5NQK|matrix showing theqi\u00b7kjvalues, with the upper-triangleportion of the comparisons matrix zeroed out (set to\u0000\u2022, which the softmax will turn tozero).Fig.11.2shows the masked version ofQK|and the unmasked version. For bidi-rectional attention, we use the unmasked version of Fig.11.2b. Thus the attentioncomputation for bidirectional attention is exactly the same as Eq.11.1but with themask removed:head=softmax\u2713QK|pdk\u25c6V(11.2)Otherwise, the attention computation is identical to what we saw in Chapter 9, as isthe transformer block architecture (the feedforward layer, layer norm, and so on). Asin Chapter 9, the input is also a series of subword tokens, usually computed by one ofthe 3 popular tokenization algorithms (including the BPE algorithm that we alreadysaw in Chapter 2 and two others, the WordPiece algorithm and the SentencePieceUnigram LM algorithm). That means every input sentence \ufb01rst has to be tokenized,and all further processing takes",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 3,
      "token_count": 801,
      "chapter_title": ""
    }
  },
  {
    "content": "(b)Figure 11.2TheN\u21e5NQK|matrix showing theqi\u00b7kjvalues, with the upper-triangleportion of the comparisons matrix zeroed out (set to\u0000\u2022, which the softmax will turn tozero).Fig.11.2shows the masked version ofQK|and the unmasked version. For bidi-rectional attention, we use the unmasked version of Fig.11.2b. Thus the attentioncomputation for bidirectional attention is exactly the same as Eq.11.1but with themask removed:head=softmax\u2713QK|pdk\u25c6V(11.2)Otherwise, the attention computation is identical to what we saw in Chapter 9, as isthe transformer block architecture (the feedforward layer, layer norm, and so on). Asin Chapter 9, the input is also a series of subword tokens, usually computed by one ofthe 3 popular tokenization algorithms (including the BPE algorithm that we alreadysaw in Chapter 2 and two others, the WordPiece algorithm and the SentencePieceUnigram LM algorithm). That means every input sentence \ufb01rst has to be tokenized,and all further processing takes place on subword tokens rather than words. This willrequire, as we\u2019ll see in the third part of the textbook, that for some NLP tasks thatrequire notions of words (like parsing) we will occasionally need to map subwordsback to words.To make this more concrete, the original English-only bidirectional transformerencoder model, BERT (Devlin et al.,2019), consisted of the following:\u2022An English-only subword vocabulary consisting of 30,000 tokens generatedusing the WordPiece algorithm (Schuster and Nakajima,2012).\u2022Input context windowN=512 tokens, and model dimensionalityd=768\u2022SoX, the input to the model, is of shape[N\u21e5d]=[512\u21e5768].\u2022L=12 layers of transformer blocks, each withA=12 (bidirectional) multiheadattention layers.\u2022The resulting model has about 100M parameters.The larger multilingual XLM-RoBERTa model, trained on 100 languages, has\u2022A multilingual subword vocabulary with 250,000 tokens generated using theSentencePiece Unigram LM algorithm (Kudo and Richardson,2018).Casual self-attentionBidirectional self-attention",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 4,
      "token_count": 496,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\nBERT: Bidirectional Encoder Representations from TransformersBERT (Devlin et al., 2019)\u202230,000 English-only tokens (WordPiece tokenizer)\u2022 Input context window N=512 tokens, and model dimensionality d=768 \u2022L=12 layers of transformer blocks, each with A=12 (bidirectional) multihead-attention layers. \u2022The resulting model has about 100M parameters. XLM-RoBERTa (Conneau et al., 2020)\u2022250,000 multilingual tokens (SentencePiece Unigram LM tokenizer)\u2022Input context window N=512 tokens,model dimensionality d=1024 \u2022L=24 layers of transformer blocks, with A=16 multihead attention layers each\u2022 The resulting model has about 550M parameters. \n\n## Page 6\n\nMasked Language ModelsBERT\n\n## Page 7\n\nMasked Language ModelsMasked LM training\n\n## Page 8\n\nMasked training intuition\u2022For left-to-right LMs, the model tries to predict the last word from prior words:   The water of Walden Pond is so beautifully\u2022And we train it to improve its predictions.\u2022For bidirectional masked LMs, the model tries to predict one or more words from all the rest of the words:  The                      of Walden Pond                  so beautifully blue \u2022The model generates a probability distribution over the vocabulary for each missing token\u2022We use the cross-entropy loss from each of the model\u2019s predictions to drive the learning process. \n\n## Page 9\n\nMLM training in BERT15% of the tokens are randomly chosen to be part of the masking Example: \"Lunch was delicious\", if delicious was randomly chosen:Three possibilities:1.80%: Token is replaced with special token [MASK]  Lunch was delicious -> Lunch was [MASK]2.10%: Token is replaced with a random token (sampled from unigram prob)  Lunch was delicious -> Lunch was gasp3.10%: Token is unchanged  Lunch was delicious -> Lunch was delicious\n\n## Page 10\n\nIn detail\nLM Head with Softmax over Vocabulary\nSo[mask]and[mask]for longthanksCE Loss\nall apricot \ufb01shthe\nToken +Positional EmbeddingsSolongandthanksfor all \ufb01shthe\nBidirectional Transformer Encoder+p1+++++++p2p3p4p5p6p7p8z1z2z3z4z5z6z7z8",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 5,
      "token_count": 512,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 6,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "MLM loss6CHAPTER11\u2022MASKEDLANGUAGEMODELSprobabilitiesyover the vocabulary:ui=hLiET(11.3)yi=softmax(ui)(11.4)With a predicted probability distribution for each masked item, we can use cross-entropy to compute the loss for each masked item\u2014the negative log probabilityassigned to the actual masked word, as shown in Fig.11.3. More formally, for agiven vector of input tokens in a sentence or batch bex, let the set of tokens that aremasked beM, the version of that sentence with some tokens replaced by masks bexmask, and the sequence of output vectors beh. For a given input tokenxi, such asthe wordlongin Fig.11.3, the loss is the probability of the correct wordlong, givenxmask(as summarized in the single output vectorhLi):LMLM(xi)=\u0000logP(xi|hLi)The gradients that form the basis for the weight updates are based on the averageloss over the sampled learning items from a single training sequence (or batch ofsequences).LMLM=\u00001|M|Xi2MlogP(xi|hLi)Note that only the tokens inMplay a role in learning; the other words play no rolein the loss function, so in that sense BERT and its descendents are inef\ufb01cient; only15% of the input samples in the training data are actually used for training weights.111.2.2 Next Sentence PredictionThe focus of mask-based learning is on predicting words from surrounding contextswith the goal of producing effective word-level representations. However, an im-portant class of applications involves determining the relationship between pairs ofsentences. These include tasks like paraphrase detection (detecting if two sentenceshave similar meanings), entailment (detecting if the meanings of two sentences en-tail or contradict each other) or discourse coherence (deciding if two neighboringsentences form a coherent discourse).To capture the kind of knowledge required for applications such as these, somemodels in the BERT family include a second learning objective calledNext Sen-tence Prediction(NSP). In this task, the model is presented with pairs of sentencesNext SentencePredictionand is asked to predict whether each pair consists of an actual pair of adjacent sen-tences from the training corpus or a pair of unrelated sentences. In BERT, 50% ofthe training pairs consisted of positive pairs, and in the other 50% the second sen-tence of a pair was randomly selected from elsewhere in the corpus. The NSP lossis based on how well the model can distinguish true pairs from random pairs.To facilitate NSP training, BERT introduces two special tokens to the input rep-resentation (tokens that will prove useful for \ufb01netuning as well). After tokenizingthe input with the subword model, the token[CLS]is prepended to the input sen-tence pair, and the token[SEP]is placed between the sentences and after the \ufb01naltoken of the second sentence. There are actually two more special tokens, a \u2018FirstSegment\u2019 token, and a \u2018Second Segment\u2019 token. These tokens are added in the in-put stage to the word and positional embeddings. That is, each token of the input1ELECTRA, another BERT family member, does use all examples for training (Clark et al.,2020).6CHAPTER11\u2022MASKEDLANGUAGEMODELSprobabilitiesyover the vocabulary:ui=hLiET(11.3)yi=softmax(ui)(11.4)With a predicted probability distribution for each masked item, we can use cross-entropy to compute the loss for each masked item\u2014the negative log probabilityassigned to the actual masked word, as shown in Fig.11.3. More formally, for agiven vector of input tokens in a sentence or batch bex, let the set",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 7,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "and in the other 50% the second sen-tence of a pair was randomly selected from elsewhere in the corpus. The NSP lossis based on how well the model can distinguish true pairs from random pairs.To facilitate NSP training, BERT introduces two special tokens to the input rep-resentation (tokens that will prove useful for \ufb01netuning as well). After tokenizingthe input with the subword model, the token[CLS]is prepended to the input sen-tence pair, and the token[SEP]is placed between the sentences and after the \ufb01naltoken of the second sentence. There are actually two more special tokens, a \u2018FirstSegment\u2019 token, and a \u2018Second Segment\u2019 token. These tokens are added in the in-put stage to the word and positional embeddings. That is, each token of the input1ELECTRA, another BERT family member, does use all examples for training (Clark et al.,2020).6CHAPTER11\u2022MASKEDLANGUAGEMODELSprobabilitiesyover the vocabulary:ui=hLiET(11.3)yi=softmax(ui)(11.4)With a predicted probability distribution for each masked item, we can use cross-entropy to compute the loss for each masked item\u2014the negative log probabilityassigned to the actual masked word, as shown in Fig.11.3. More formally, for agiven vector of input tokens in a sentence or batch bex, let the set of tokens that aremasked beM, the version of that sentence with some tokens replaced by masks bexmask, and the sequence of output vectors beh. For a given input tokenxi, such asthe wordlongin Fig.11.3, the loss is the probability of the correct wordlong, givenxmask(as summarized in the single output vectorhLi):LMLM(xi)=\u0000logP(xi|hLi)The gradients that form the basis for the weight updates are based on the averageloss over the sampled learning items from a single training sequence (or batch ofsequences).LMLM=\u00001|M|Xi2MlogP(xi|hLi)Note that only the tokens inMplay a role in learning; the other words play no rolein the loss function, so in that sense BERT and its descendents are inef\ufb01cient; only15% of the input samples in the training data are actually used for training weights.111.2.2 Next Sentence PredictionThe focus of mask-based learning is on predicting words from surrounding contextswith the goal of producing effective word-level representations. However, an im-portant class of applications involves determining the relationship between pairs ofsentences. These include tasks like paraphrase detection (detecting if two sentenceshave similar meanings), entailment (detecting if the meanings of two sentences en-tail or contradict each other) or discourse coherence (deciding if two neighboringsentences form a coherent discourse).To capture the kind of knowledge required for applications such as these, somemodels in the BERT family include a second learning objective calledNext Sen-tence Prediction(NSP). In this task, the model is presented with pairs of sentencesNext SentencePredictionand is asked to predict whether each pair consists of an actual pair of adjacent sen-tences from the training corpus or a pair of unrelated sentences. In BERT, 50% ofthe training pairs consisted of positive pairs, and in the other 50% the second sen-tence of a pair was randomly selected from elsewhere in the corpus. The NSP lossis based on how well the model can distinguish true pairs from random pairs.To facilitate NSP training, BERT introduces two special tokens to the input rep-resentation (tokens that will prove useful for \ufb01netuning as well). After tokenizingthe input with the subword model, the token[CLS]is prepended to the input sen-tence pair, and the",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 8,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "Next Sentence PredictionThe focus of mask-based learning is on predicting words from surrounding contextswith the goal of producing effective word-level representations. However, an im-portant class of applications involves determining the relationship between pairs ofsentences. These include tasks like paraphrase detection (detecting if two sentenceshave similar meanings), entailment (detecting if the meanings of two sentences en-tail or contradict each other) or discourse coherence (deciding if two neighboringsentences form a coherent discourse).To capture the kind of knowledge required for applications such as these, somemodels in the BERT family include a second learning objective calledNext Sen-tence Prediction(NSP). In this task, the model is presented with pairs of sentencesNext SentencePredictionand is asked to predict whether each pair consists of an actual pair of adjacent sen-tences from the training corpus or a pair of unrelated sentences. In BERT, 50% ofthe training pairs consisted of positive pairs, and in the other 50% the second sen-tence of a pair was randomly selected from elsewhere in the corpus. The NSP lossis based on how well the model can distinguish true pairs from random pairs.To facilitate NSP training, BERT introduces two special tokens to the input rep-resentation (tokens that will prove useful for \ufb01netuning as well). After tokenizingthe input with the subword model, the token[CLS]is prepended to the input sen-tence pair, and the token[SEP]is placed between the sentences and after the \ufb01naltoken of the second sentence. There are actually two more special tokens, a \u2018FirstSegment\u2019 token, and a \u2018Second Segment\u2019 token. These tokens are added in the in-put stage to the word and positional embeddings. That is, each token of the input1ELECTRA, another BERT family member, does use all examples for training (Clark et al.,2020).6CHAPTER11\u2022MASKEDLANGUAGEMODELSprobabilitiesyover the vocabulary:ui=hLiET(11.3)yi=softmax(ui)(11.4)With a predicted probability distribution for each masked item, we can use cross-entropy to compute the loss for each masked item\u2014the negative log probabilityassigned to the actual masked word, as shown in Fig.11.3. More formally, for agiven vector of input tokens in a sentence or batch bex, let the set of tokens that aremasked beM, the version of that sentence with some tokens replaced by masks bexmask, and the sequence of output vectors beh. For a given input tokenxi, such asthe wordlongin Fig.11.3, the loss is the probability of the correct wordlong, givenxmask(as summarized in the single output vectorhLi):LMLM(xi)=\u0000logP(xi|hLi)The gradients that form the basis for the weight updates are based on the averageloss over the sampled learning items from a single training sequence (or batch ofsequences).LMLM=\u00001|M|Xi2MlogP(xi|hLi)Note that only the tokens inMplay a role in learning; the other words play no rolein the loss function, so in that sense BERT and its descendents are inef\ufb01cient; only15% of the input samples in the training data are actually used for training weights.111.2.2 Next Sentence PredictionThe focus of mask-based learning is on predicting words from surrounding contextswith the goal of producing effective word-level representations. However, an im-portant class of applications involves determining the relationship between pairs ofsentences. These include tasks like paraphrase detection (detecting if two sentenceshave similar meanings), entailment (detecting if the meanings of two sentences en-tail or contradict each other) or discourse coherence (deciding if two neighboringsentences form a coherent discourse).To capture the kind of knowledge required for",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 9,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "the version of that sentence with some tokens replaced by masks bexmask, and the sequence of output vectors beh. For a given input tokenxi, such asthe wordlongin Fig.11.3, the loss is the probability of the correct wordlong, givenxmask(as summarized in the single output vectorhLi):LMLM(xi)=\u0000logP(xi|hLi)The gradients that form the basis for the weight updates are based on the averageloss over the sampled learning items from a single training sequence (or batch ofsequences).LMLM=\u00001|M|Xi2MlogP(xi|hLi)Note that only the tokens inMplay a role in learning; the other words play no rolein the loss function, so in that sense BERT and its descendents are inef\ufb01cient; only15% of the input samples in the training data are actually used for training weights.111.2.2 Next Sentence PredictionThe focus of mask-based learning is on predicting words from surrounding contextswith the goal of producing effective word-level representations. However, an im-portant class of applications involves determining the relationship between pairs ofsentences. These include tasks like paraphrase detection (detecting if two sentenceshave similar meanings), entailment (detecting if the meanings of two sentences en-tail or contradict each other) or discourse coherence (deciding if two neighboringsentences form a coherent discourse).To capture the kind of knowledge required for applications such as these, somemodels in the BERT family include a second learning objective calledNext Sen-tence Prediction(NSP). In this task, the model is presented with pairs of sentencesNext SentencePredictionand is asked to predict whether each pair consists of an actual pair of adjacent sen-tences from the training corpus or a pair of unrelated sentences. In BERT, 50% ofthe training pairs consisted of positive pairs, and in the other 50% the second sen-tence of a pair was randomly selected from elsewhere in the corpus. The NSP lossis based on how well the model can distinguish true pairs from random pairs.To facilitate NSP training, BERT introduces two special tokens to the input rep-resentation (tokens that will prove useful for \ufb01netuning as well). After tokenizingthe input with the subword model, the token[CLS]is prepended to the input sen-tence pair, and the token[SEP]is placed between the sentences and after the \ufb01naltoken of the second sentence. There are actually two more special tokens, a \u2018FirstSegment\u2019 token, and a \u2018Second Segment\u2019 token. These tokens are added in the in-put stage to the word and positional embeddings. That is, each token of the input1ELECTRA, another BERT family member, does use all examples for training (Clark et al.,2020).The LM head takes output of final transformer layer L, multiplies it by unembedding layer and turns into probabilities:E.g., for the xi corresponding to \"long\", the loss is the probability of the correct word long, given output hLi ): We get the gradients by taking the average of this loss over the batch",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 10,
      "token_count": 647,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\nNext Sentence PredictionGiven 2 sentences the model predicts if they are a real pair of adjacent sentences from the training corpus or a pair of unrelated sentences. BERT introduces two special tokens \u2022[CLS] is prepended to the input sentence pair, \u2022[SEP] is placed between the sentences, and also after second sentence And two more special tokens\u2022 [1st segment]  and [2nd segment]\u2022These are added to the input embedding and positional embeddinghLCLS from the final layer [CLS] token is input to classifier head (weights WNSP ) that predicts two classes:. 11.2\u2022TRAININGBIDIRECTIONALENCODERS7Xis actually formed by summing 3 embeddings: word, position, and \ufb01rst/secondsegment embeddings.During training, the output vectorhLCLSfrom the \ufb01nal layer associated with the[CLS]token represents the next sentence prediction. As with the MLM objective,we add a special head, in this case an NSP head, which consists of a learned set ofclassi\ufb01cation weightsWNSP2Rd\u21e52that produces a two-class prediction from theraw[CLS]vectorhLCLS:yi=softmax(hLCLSWNSP)Cross entropy is used to compute the NSP loss for each sentence pair presentedto the model. Fig.11.4illustrates the overall NSP training setup. In BERT, the NSPloss was used in conjunction with the MLM training objective to form \ufb01nal loss.\nCancelmy\ufb02ight[SEP] 1CE Loss\nAnd the \nBidirectional Transformer Encoder\np1p2p3p4p5p6p7p8[CLS]++s1NSPHead\nToken +Segment +PositionalEmbeddings hotel p9[SEP] ++s1s1s1s1s2s2s2s2++++++++++++++hCLS\nFigure 11.4An example of the NSP loss calculation.11.2.3 Training RegimesBERT and other early transformer-based language models were trained on about3.3 billion words (a combination of English Wikipedia and a corpus of book textscalled BooksCorpus (Zhu et al.,2015) that is no longer used for intellectual propertyreasons). Modern masked language models are now trained on much larger datasetsof web text, \ufb01ltered a bit, and augmented by higher-quality data like Wikipedia,the same as those we discussed for the causal large language models of Chapter 9.Multilingual models similarly use webtext and multilingual Wikipedia. For examplethe XLM-R model was trained on about 300 billion tokens in 100 languages, takenfrom the web via Common Crawl (https://commoncrawl.org/).To train the original BERT models, pairs of text segments were selected from thetraining corpus according to the next sentence prediction 50/50 scheme. Pairs weresampled so that their combined length was less than the 512 token input. Tokenswithin these sentence pairs were then masked using the MLM approach with thecombined loss from the MLM and NSP objectives used for a \ufb01nal loss. Because this\ufb01nal loss is backpropagated through the entire transformer, the embeddings at eachtransformer layer will learn representations that are useful for predicting words fromtheir neighbors. Since the[CLS]tokens are the direct input to the NSP classi\ufb01er,their learned representations will tend to contain information about the sequence as",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 11,
      "token_count": 723,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\nNSP Loss with classification head\nCancelmy\ufb02ight[SEP] 1CE Loss\nAnd the \nBidirectional Transformer Encoder\np1p2p3p4p5p6p7p8[CLS]++s1NSPHeadToken +Segment +PositionalEmbeddings hotel p9[SEP] ++s1s1s1s1s2s2s2s2++++++++++++++hCLS\n\n## Page 14\n\nMore detailsOriginal model was trained with 40 passes over training dataSome models (like RoBERTa) drop NSP lossTokenizer for multilingual models is trained from stratified sample of languages (some data from each language)Multilingual models are better than monolingual models with small numbers of languages\u2022With large numbers of languages, monolingual models in that language can be better\u2022The \"curse of multilinguality\"\n\n## Page 15\n\nMasked Language ModelsMasked LM training\n\n## Page 16\n\nMasked Language ModelsContextual Embeddings\n\n## Page 17\n\nContextual Embeddings to represent words\n[CLS]SolongandthanksforallhL1hLCLShL2hL3hL4hL5hL6\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 12,
      "token_count": 280,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\nMasked Language ModelsMasked LM training\n\n## Page 16\n\nMasked Language ModelsContextual Embeddings\n\n## Page 17\n\nContextual Embeddings to represent words\n[CLS]SolongandthanksforallhL1hLCLShL2hL3hL4hL5hL6\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\n\n## Page 18\n\nStatic vs Contextual EmbeddingsStatic embeddings represent word types (dictionary entries)Contextual embeddings represent word instances (one for each time the word occurs in any context/sentence)\nFigure 4: Embeddings for the word \"die\" in different contexts, visualized with UMAP. Sample pointsare annotated with corresponding sentences. Overall annotations (blue text) are added as a guide.4.1 Visualization of word sensesOur \ufb01rst experiment is an exploratory visualization of how word sense affects context embeddings.For data on different word senses, we collected all sentences used in the introductions to English-language Wikipedia articles. (Text outside of introductions was frequently fragmentary.) We createdan interactive application, which we plan to make public. A user enters a word, and the systemretrieves 1,000 sentences containing that word. It sends these sentences to BERT-base as input, andfor each one it retrieves the context embedding for the word from a layer of the user\u2019s choosing.The system visualizes these 1,000 context embeddings using UMAP [15], generally showing clearclusters relating to word senses. Different senses of a word are typically spatially separated, andwithin the clusters there is often further structure related to \ufb01ne shades of meaning. In Figure 4, forexample, we not only see crisp, well-separated clusters for three meanings of the word \u201cdie,\u201d butwithin one of these clusters there is a kind of quantitative scale, related to the number of peopledying. See Appendix 6.4 for further examples. The apparent detail in the clusters we visualized raisestwo immediate questions. First, is it possible to \ufb01nd quantitative corroboration that word senses arewell-represented? Second, how can we resolve a seeming contradiction: in the previous section, wesaw how position represented syntax; yet here we see position representing semantics.4.2 Measurement of word sense disambiguation capabilityThe crisp clusters seen in visualizations such as Figure 4 suggest that BERT may create simple,effective internal representations of word senses, putting different meanings in different locations. Totest this hypothesis quantitatively, we test whether a simple classi\ufb01er on these internal representationscan perform well at word-sense disambiguation (WSD).We follow the procedure described in [20], which performed a similar experiment with the ELMomodel. For a given word withnsenses, we make a nearest-neighbor classi\ufb01er where each neighbor isthe centroid of a given word sense\u2019s BERT-base embeddings in the training data. To classify a newword we \ufb01nd the closest of these centroids, defaulting to the most commonly used sense if the wordwas not present in the training data. We used the data and evaluation from [21]: the training data wasSemCor [17] (33,362 senses), and the testing data was the suite described in [21] (3,669 senses).The simple nearest-neighbor classi\ufb01er achieves an F1 score of 71.1, higher than the current state ofthe art (Table 1), with the accuracy monotonically increasing through the layers. This is a strongsignal that context embeddings are representing word-sense information. Additionally, an even higherscore of 71.5 was obtained using the technique described in the following section.6",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 13,
      "token_count": 783,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 14,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Word senseWords are ambiguousA word sense is a discrete representation of one aspect of meaningContextual embeddings offer a continuous high-dimensional model of meaning that is more fine grained than discrete senses.10CHAPTER11\u2022MASKEDLANGUAGEMODELSpolysemous(from Greek \u2018many senses\u2019,poly-\u2018many\u2019 +sema, \u2018sign, mark\u2019).2Asense(orword sense) is a discrete representation of one aspect of the meaningword senseof a word. We can represent each sense with a superscript:bank1andbank2,mouse1andmouse2. These senses can be found listed in online thesauruses (orthesauri) likeWordNet(Fellbaum,1998), which has datasets in many languagesWordNetlisting the senses of many words. In context, it\u2019s easy to see the different meanings:mouse1:.... amousecontrolling a computer system in 1968.mouse2:.... a quiet animal like amousebank1:...abankcan hold the investments in a custodial account ...bank2:...as agriculture burgeons on the eastbank, the river ...This fact that context disambiguates the senses ofmouseandbankabove canalso be visualized geometrically. Fig.11.6shows a two-dimensional projection ofmany instances of the BERT embeddings of the worddiein English and German.Each point in the graph represents the use ofdiein one input sentence. We canclearly see at least two different English senses ofdie(the singular ofdiceand theverbto die, as well as the German article, in the BERT embedding space.",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 15,
      "token_count": 334,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 4: Embeddings for the word \"die\" in different contexts, visualized with UMAP. Sample pointsare annotated with corresponding sentences. Overall annotations (blue text) are added as a guide.4.1 Visualization of word sensesOur \ufb01rst experiment is an exploratory visualization of how word sense affects context embeddings.For data on different word senses, we collected all sentences used in the introductions to English-language Wikipedia articles. (Text outside of introductions was frequently fragmentary.) We createdan interactive application, which we plan to make public. A user enters a word, and the systemretrieves 1,000 sentences containing that word. It sends these sentences to BERT-base as input, andfor each one it retrieves the context embedding for the word from a layer of the user\u2019s choosing.The system visualizes these 1,000 context embeddings using UMAP [15], generally showing clearclusters relating to word senses. Different senses of a word are typically spatially separated, andwithin the clusters there is often further structure related to \ufb01ne shades of meaning. In Figure 4, forexample, we not only see crisp, well-separated clusters for three meanings of the word \u201cdie,\u201d butwithin one of these clusters there is a kind of quantitative scale, related to the number of peopledying. See Appendix 6.4 for further examples. The apparent detail in the clusters we visualized raisestwo immediate questions. First, is it possible to \ufb01nd quantitative corroboration that word senses arewell-represented? Second, how can we resolve a seeming contradiction: in the previous section, wesaw how position represented syntax; yet here we see position representing semantics.4.2 Measurement of word sense disambiguation capabilityThe crisp clusters seen in visualizations such as Figure 4 suggest that BERT may create simple,effective internal representations of word senses, putting different meanings in different locations. Totest this hypothesis quantitatively, we test whether a simple classi\ufb01er on these internal representationscan perform well at word-sense disambiguation (WSD).We follow the procedure described in [20], which performed a similar experiment with the ELMomodel. For a given word withnsenses, we make a nearest-neighbor classi\ufb01er where each neighbor isthe centroid of a given word sense\u2019s BERT-base embeddings in the training data. To classify a newword we \ufb01nd the closest of these centroids, defaulting to the most commonly used sense if the wordwas not present in the training data. We used the data and evaluation from [21]: the training data wasSemCor [17] (33,362 senses), and the testing data was the suite described in [21] (3,669 senses).The simple nearest-neighbor classi\ufb01er achieves an F1 score of 71.1, higher than the current state ofthe art (Table 1), with the accuracy monotonically increasing through the layers. This is a strongsignal that context embeddings are representing word-sense information. Additionally, an even higherscore of 71.5 was obtained using the technique described in the following section.6Figure 11.6Each blue dot shows a BERT contextual embedding for the worddiefrom different sentencesin English and German, projected into two dimensions with the UMAP algorithm. The German and Englishmeanings and the different English senses fall into different clusters. Some sample points are shown with thecontextual sentence they came from. Figure fromCoenen et al.(2019).Thus while thesauruses like WordNet give discrete lists of senses, embeddings(whether static or contextual) offer a continuous high-dimensional model of meaningthat, although it can be clustered, doesn\u2019t divide up into fully discrete senses.Word Sense DisambiguationThe task of selecting the correct sense for a word is calledword sense disambigua-tion,",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 16,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "the closest of these centroids, defaulting to the most commonly used sense if the wordwas not present in the training data. We used the data and evaluation from [21]: the training data wasSemCor [17] (33,362 senses), and the testing data was the suite described in [21] (3,669 senses).The simple nearest-neighbor classi\ufb01er achieves an F1 score of 71.1, higher than the current state ofthe art (Table 1), with the accuracy monotonically increasing through the layers. This is a strongsignal that context embeddings are representing word-sense information. Additionally, an even higherscore of 71.5 was obtained using the technique described in the following section.6Figure 11.6Each blue dot shows a BERT contextual embedding for the worddiefrom different sentencesin English and German, projected into two dimensions with the UMAP algorithm. The German and Englishmeanings and the different English senses fall into different clusters. Some sample points are shown with thecontextual sentence they came from. Figure fromCoenen et al.(2019).Thus while thesauruses like WordNet give discrete lists of senses, embeddings(whether static or contextual) offer a continuous high-dimensional model of meaningthat, although it can be clustered, doesn\u2019t divide up into fully discrete senses.Word Sense DisambiguationThe task of selecting the correct sense for a word is calledword sense disambigua-tion, orWSD. WSD algorithms take as input a word in context and a \ufb01xed inventoryword sensedisambiguationWSDof potential word senses (like the ones in WordNet) and outputs the correct wordsense in context. Fig.11.7sketches out the task.2The wordpolysemyitself is ambiguous; you may see it used in a different way, to refer only to caseswhere a word\u2019s senses are related in some structured way, reserving the wordhomonymyto mean senseambiguities with no relation between the senses (Haber and Poesio,2020). Here we will use \u2018polysemy\u2019to mean any kind of sense ambiguity, and \u2018structured polysemy\u2019 for polysemy with sense relations.",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 17,
      "token_count": 461,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\nWord sense disambiguation (WSD)The task of selecting the correct sense for a word.\nanelectricguitarandbassplayerstando\ufb00toonesideelectric1: using electricityelectric2:  tenseelectric3: thrillingguitar1 bass1: low range\u2026bass4: sea \ufb01sh\u2026 bass7: instrument\u2026player1: in gameplayer2: musician player3: actor\u2026stand1: upright\u2026stand5: bear\u2026 stand10: put upright\u2026side1: relative region\u2026side3: of body\u2026 side11: slope\u2026x1y1x2y2x3y3y4y5y6\nx4x5x6",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 18,
      "token_count": 151,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 19,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "1-nearest neighbor algorithm for WSDAt training time, take a sense-labeled corpus like SEMCORRun corpus through BERT to get contextual embedding for each token\u2022E.g., pooling representations from last 4 BERT transformer layerThen for each sense s of word w for n tokens of that sense, pool embeddings:At test time, given a token of a target word t, compute contextual embedding t and choose its nearest neighbor sense from training set 11.3\u2022CONTEXTUALEMBEDDINGS11\nanelectricguitarandbassplayerstando\ufb00toonesideelectric1: using electricityelectric2:  tenseelectric3: thrillingguitar1 bass1: low range\u2026bass4: sea \ufb01sh\u2026 bass7: instrument\u2026player1: in gameplayer2: musician player3: actor\u2026stand1: upright\u2026stand5: bear\u2026 stand10: put upright\u2026side1: relative region\u2026side3: of body\u2026 side11: slope\u2026x1y1x2y2x3y3y4y5y6\nx4x5x6Figure 11.7The all-words WSD task, mapping from input words (x) to WordNet senses(y). Figure inspired byChaplot and Salakhutdinov(2018).WSD can be a useful analytic tool for text analysis in the humanities and socialsciences, and word senses can play a role in model interpretability for word repre-sentations. Word senses also have interesting distributional properties. For examplea word often is used in roughly the same sense through a discourse, an observationcalled theone sense per discourserule (Gale et al.,1992).one sense perdiscourseThe best performing WSD algorithm is a simple 1-nearest-neighbor algorithmusing contextual word embeddings, due toMelamud et al.(2016) andPeters et al.(2018). At training time we pass each sentence in some sense-labeled dataset (likethe SemCore or SenseEval datasets in various languages) through any contextualembedding (e.g., BERT) resulting in a contextual embedding for each labeled token.(There are various ways to compute this contextual embeddingvifor a tokeni; forBERT it is common to pool multiple layers by summing the vector representationsofifrom the last four BERT layers). Then for each sensesof any word in the corpus,for each of thentokens of that sense, we average theirncontextual representationsvito produce a contextualsense embedding vsfors:vs=1nXivi8vi2tokens(s)(11.6)At test time, given a token of a target wordtin context, we compute its contextualembeddingtand choose its nearest neighbor sense from the training set, i.e., thesense whose sense embedding has the highest cosine witht:sense(t)=argmaxs2senses(t)cosine(t,vs)(11.7)Fig.11.8illustrates the model.11.3.2 Contextual Embeddings and Word SimilarityIn Chapter 6 we introduced the idea that we could measure the similarity of twowords by considering how close they are geometrically, by using the cosine as asimilarity function. The idea of meaning similarity is also clear geometrically in themeaning clusters in Fig.11.6; the representation of a word which has a particularsense in a context is closer to other instances of the same sense of the word. Thus we11.3\u2022CONTEXTUALEMBEDDINGS11",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 20,
      "token_count": 737,
      "chapter_title": ""
    }
  },
  {
    "content": "anelectricguitarandbassplayerstando\ufb00toonesideelectric1: using electricityelectric2:  tenseelectric3: thrillingguitar1 bass1: low range\u2026bass4: sea \ufb01sh\u2026 bass7: instrument\u2026player1: in gameplayer2: musician player3: actor\u2026stand1: upright\u2026stand5: bear\u2026 stand10: put upright\u2026side1: relative region\u2026side3: of body\u2026 side11: slope\u2026x1y1x2y2x3y3y4y5y6\nx4x5x6Figure 11.7The all-words WSD task, mapping from input words (x) to WordNet senses(y). Figure inspired byChaplot and Salakhutdinov(2018).WSD can be a useful analytic tool for text analysis in the humanities and socialsciences, and word senses can play a role in model interpretability for word repre-sentations. Word senses also have interesting distributional properties. For examplea word often is used in roughly the same sense through a discourse, an observationcalled theone sense per discourserule (Gale et al.,1992).one sense perdiscourseThe best performing WSD algorithm is a simple 1-nearest-neighbor algorithmusing contextual word embeddings, due toMelamud et al.(2016) andPeters et al.(2018). At training time we pass each sentence in some sense-labeled dataset (likethe SemCore or SenseEval datasets in various languages) through any contextualembedding (e.g., BERT) resulting in a contextual embedding for each labeled token.(There are various ways to compute this contextual embeddingvifor a tokeni; forBERT it is common to pool multiple layers by summing the vector representationsofifrom the last four BERT layers). Then for each sensesof any word in the corpus,for each of thentokens of that sense, we average theirncontextual representationsvito produce a contextualsense embedding vsfors:vs=1nXivi8vi2tokens(s)(11.6)At test time, given a token of a target wordtin context, we compute its contextualembeddingtand choose its nearest neighbor sense from the training set, i.e., thesense whose sense embedding has the highest cosine witht:sense(t)=argmaxs2senses(t)cosine(t,vs)(11.7)Fig.11.8illustrates the model.11.3.2 Contextual Embeddings and Word SimilarityIn Chapter 6 we introduced the idea that we could measure the similarity of twowords by considering how close they are geometrically, by using the cosine as asimilarity function. The idea of meaning similarity is also clear geometrically in themeaning clusters in Fig.11.6; the representation of a word which has a particularsense in a context is closer to other instances of the same sense of the word. Thus weMelamud et al (2016), Peters et al (2018)",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 21,
      "token_count": 638,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22\n\n1-nearest neighbor algorithm for WSD\nI  found  the  jar  emptycIcfoundfind1vcthecjarcemptyfind9vfind5vfind4vENCODER\n\n## Page 23\n\nSimilarity and contextual embeddings\u2022We generally use cosine as for static embeddings\u2022But some issues:\u2022Contextual embeddings tend to be anisotropic: all point in roughly the same direction so have high inherent cosines (Ethayarajh 2019)\u2022Cosine measure are dominated by a small number of \"rogue\" dimensions with very high values (Timkey and van Schijndel 2021)\u2022Cosine tends to underestimate human judgments on similarity of word meaning for very frequent words (Zhou et al., 2022)\n\n## Page 24\n\nMasked Language ModelsContextual Embeddings\n\n## Page 25\n\nMasked Language ModelsFine-Tuning for Classification\n\n## Page 26\n\nAdding a sentiment classification head\n[CLS]entirelypredictableandlacksenergy\nBidirectional Transformer EncoderhCLS\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+sentimentclassi\ufb01cation headWCy\n\n## Page 27\n\nSequence-Pair classificationAssign a label to pairs of sentences:\u2022paraphrase detection (are the two sentences paraphrases of each other?) \u2022logical entailment (does sentence A logically entail sentence B?) \u2022discourse coherence (how coherent is sentence B as a follow-on to sentence A?)",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 22,
      "token_count": 318,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 26\n\nAdding a sentiment classification head\n[CLS]entirelypredictableandlacksenergy\nBidirectional Transformer EncoderhCLS\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+sentimentclassi\ufb01cation headWCy\n\n## Page 27\n\nSequence-Pair classificationAssign a label to pairs of sentences:\u2022paraphrase detection (are the two sentences paraphrases of each other?) \u2022logical entailment (does sentence A logically entail sentence B?) \u2022discourse coherence (how coherent is sentence B as a follow-on to sentence A?)\n\n## Page 28\n\nExample: Natural Language InferencePairs of sentences are given one of 3 labels\nAlgorithm: pass the premise/hypothesis pairs through a bidirectional encoder and use the output vector for the [CLS] token as the input to the classification head .11.5\u2022FINE-TUNING FORSEQUENCELABELLING:NAMEDENTITYRECOGNITION15As an example, let\u2019s consider an entailment classi\ufb01cation task with the Multi-Genre Natural Language Inference (MultiNLI) dataset (Williams et al.,2018). Inthe task ofnatural language inferenceorNLI, also calledrecognizing textualnaturallanguageinferenceentailment, a model is presented with a pair of sentences and must classify the re-lationship between their meanings. For example in the MultiNLI corpus, pairs ofsentences are given one of 3 labels:entails,contradictsandneutral. These labelsdescribe a relationship between the meaning of the \ufb01rst sentence (the premise) andthe meaning of the second sentence (the hypothesis). Here are representative exam-ples of each class from the corpus:\u2022Neutrala:Jon walked back to the town to the smithy.b:Jon traveled back to his hometown.\u2022Contradictsa:Tourist Information of\ufb01ces can be very helpful.b:Tourist Information of\ufb01ces are never of any help.\u2022Entailsa:I\u2019m confused.b:Not all of it is very clear to me.A relationship ofcontradictsmeans that the premise contradicts the hypothesis;en-tailsmeans that the premise entails the hypothesis;neutralmeans that neither isnecessarily true. The meaning of these labels is looser than strict logical entailmentor contradiction indicating that a typical human reading the sentences would mostlikely interpret the meanings in this way.To \ufb01netune a classi\ufb01er for the MultiNLI task, we pass the premise/hypothesispairs through a bidirectional encoder as described above and use the output vectorfor the[CLS]token as the input to the classi\ufb01cation head. As with ordinary sequenceclassi\ufb01cation, this head provides the input to a three-way classi\ufb01er that can be trainedon the MultiNLI training corpus.11.5 Fine-Tuning for Sequence Labelling: Named En-tity RecognitionIn sequence labeling, the network\u2019s task is to assign a label chosen from a small\ufb01xed set of labels to each token in the sequence. One of the most common sequencelabeling task isnamed entity recognition.11.5.1 Named EntitiesAnamed entityis, roughly speaking, anything that can be referred to with a propernamed entityname: a person, a location, an organization. The task ofnamed entity recognitionnamed entityrecognition(NER) is to \ufb01nd spans of text that constitute proper names and tag the type of theNERentity. Four entity tags are most common:PER(person),LOC(location),ORG(organization), orGPE(geo-political entity). However, the termnamed entityiscommonly extended to include things that aren\u2019t entities per se, including temporalexpressions like dates and times, and even numerical expressions like prices. Here\u2019san example of the output of an NER tagger:",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 23,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 29\n\nFine-tuning for sequence labelingAssign a label from a small fixed set of labels to each token in the sequence. \u2022Named entity recognition\u2022Part of speech tagging.",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 24,
      "token_count": 38,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 30",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 25,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Named Entity RecognitionA named entity is anything that can be referred to with a proper name: a person, a location, an organizationNamed entity recognition (NER): find spans of text that constitute proper names and tag the type of the entity 16CHAPTER11\u2022MASKEDLANGUAGEMODELSCiting high fuel prices,[ORGUnited Airlines]said[TIMEFriday]ithas increased fares by[MONEY$6]per round trip on \ufb02ights to somecities also served by lower-cost carriers.[ORGAmerican Airlines],aunit of[ORGAMR Corp.], immediately matched the move, spokesman[PERTim Wagner]said.[ORGUnited], a unit of[ORGUAL Corp.],said the increase took effect[TIMEThursday]and applies to mostroutes where it competes against discount carriers, such as[LOCChicago]to[LOCDallas]and[LOCDenver]to[LOCSan Francisco].The text contains 13 mentions of named entities including 5 organizations, 4 loca-tions, 2 times, 1 person, and 1 mention of money. Figure11.10shows typical genericnamed entity types. Many applications will also need to use speci\ufb01c entity types likeproteins, genes, commercial products, or works of art.Type Tag Sample Categories Example sentencesPeoplePERpeople, charactersTuringis a giant of computer science.OrganizationORGcompanies, sports teams TheIPCCwarned about the cyclone.LocationLOCregions, mountains, seasMt. Sanitasis inSunshine Canyon.Geo-Political EntityGPEcountries, statesPalo Altois raising the fees for parking.Figure 11.10A list of generic named entity types with the kinds of entities they refer to.Named entity recognition is a useful step in various natural language processingtasks, including linking text to information in structured knowledge sources likeWikipedia, measuring sentiment or attitudes toward a particular entity in text, oreven as part of anonymizing text for privacy. The NER task is is dif\ufb01cult becauseof the ambiguity of segmenting NER spans, \ufb01guring out which tokens are entitiesand which aren\u2019t, since most words in a text will not be named entities. Anotherdif\ufb01culty is caused by type ambiguity. The mentionWashingtoncan refer to aperson, a sports team, a city, or the US government, as we see in Fig.11.11.[PERWashington] was born into slavery on the farm of James Burroughs.[ORGWashington] went up 2 games to 1 in the four-game series.Blair arrived in [LOCWashington] for what may well be his last state visit.In June, [GPEWashington] passed a primary seatbelt law.Figure 11.11Examples of type ambiguities in the use of the nameWashington.11.5.2 BIO TaggingOne standard approach to sequence labeling for a span-recognition problem likeNER isBIO tagging(Ramshaw and Marcus,1995). This is a method that allows usBIO taggingto treat NER like a word-by-word sequence labeling task, via tags that capture boththe boundary and the named entity type. Consider the following sentence:[PERJane Villanueva]of[ORGUnited], a unit of[ORGUnited AirlinesHolding], said the fare applies to the[LOCChicago]route.Figure11.12shows the same excerpt represented withBIOtagging, as well asBIOvariants calledIOtagging andBIOEStagging. In BIO tagging we label any tokenthatbeginsa span of interest with the labelB, tokens that occurinsidea span aretagged with anI, and any tokensoutsideof any span of interest are labeledO. Whilethere is only oneOtag, we\u2019ll have distinctBandItags for each named entity class.The number of tags is thus 2n+1 tags,",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 26,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "was born into slavery on the farm of James Burroughs.[ORGWashington] went up 2 games to 1 in the four-game series.Blair arrived in [LOCWashington] for what may well be his last state visit.In June, [GPEWashington] passed a primary seatbelt law.Figure 11.11Examples of type ambiguities in the use of the nameWashington.11.5.2 BIO TaggingOne standard approach to sequence labeling for a span-recognition problem likeNER isBIO tagging(Ramshaw and Marcus,1995). This is a method that allows usBIO taggingto treat NER like a word-by-word sequence labeling task, via tags that capture boththe boundary and the named entity type. Consider the following sentence:[PERJane Villanueva]of[ORGUnited], a unit of[ORGUnited AirlinesHolding], said the fare applies to the[LOCChicago]route.Figure11.12shows the same excerpt represented withBIOtagging, as well asBIOvariants calledIOtagging andBIOEStagging. In BIO tagging we label any tokenthatbeginsa span of interest with the labelB, tokens that occurinsidea span aretagged with anI, and any tokensoutsideof any span of interest are labeledO. Whilethere is only oneOtag, we\u2019ll have distinctBandItags for each named entity class.The number of tags is thus 2n+1 tags, wherenis the number of entity types. BIO",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 27,
      "token_count": 310,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 31\n\nNamed Entity Recognition16CHAPTER11\u2022MASKEDLANGUAGEMODELSCiting high fuel prices,[ORGUnited Airlines]said[TIMEFriday]ithas increased fares by[MONEY$6]per round trip on \ufb02ights to somecities also served by lower-cost carriers.[ORGAmerican Airlines],aunit of[ORGAMR Corp.], immediately matched the move, spokesman[PERTim Wagner]said.[ORGUnited], a unit of[ORGUAL Corp.],said the increase took effect[TIMEThursday]and applies to mostroutes where it competes against discount carriers, such as[LOCChicago]to[LOCDallas]and[LOCDenver]to[LOCSan Francisco].The text contains 13 mentions of named entities including 5 organizations, 4 loca-tions, 2 times, 1 person, and 1 mention of money. Figure11.10shows typical genericnamed entity types. Many applications will also need to use speci\ufb01c entity types likeproteins, genes, commercial products, or works of art.Type Tag Sample Categories Example sentencesPeoplePERpeople, charactersTuringis a giant of computer science.OrganizationORGcompanies, sports teams TheIPCCwarned about the cyclone.LocationLOCregions, mountains, seasMt. Sanitasis inSunshine Canyon.Geo-Political EntityGPEcountries, statesPalo Altois raising the fees for parking.Figure 11.10A list of generic named entity types with the kinds of entities they refer to.Named entity recognition is a useful step in various natural language processingtasks, including linking text to information in structured knowledge sources likeWikipedia, measuring sentiment or attitudes toward a particular entity in text, oreven as part of anonymizing text for privacy. The NER task is is dif\ufb01cult becauseof the ambiguity of segmenting NER spans, \ufb01guring out which tokens are entitiesand which aren\u2019t, since most words in a text will not be named entities. Anotherdif\ufb01culty is caused by type ambiguity. The mentionWashingtoncan refer to aperson, a sports team, a city, or the US government, as we see in Fig.11.11.[PERWashington] was born into slavery on the farm of James Burroughs.[ORGWashington] went up 2 games to 1 in the four-game series.Blair arrived in [LOCWashington] for what may well be his last state visit.In June, [GPEWashington] passed a primary seatbelt law.Figure 11.11Examples of type ambiguities in the use of the nameWashington.11.5.2 BIO TaggingOne standard approach to sequence labeling for a span-recognition problem likeNER isBIO tagging(Ramshaw and Marcus,1995). This is a method that allows usBIO taggingto treat NER like a word-by-word sequence labeling task, via tags that capture boththe boundary and the named entity type. Consider the following sentence:[PERJane Villanueva]of[ORGUnited], a unit of[ORGUnited AirlinesHolding], said the fare applies to the[LOCChicago]route.Figure11.12shows the same excerpt represented withBIOtagging, as well asBIOvariants calledIOtagging andBIOEStagging. In BIO tagging we label any tokenthatbeginsa span of interest with the labelB, tokens that occurinsidea span aretagged with anI, and any tokensoutsideof any span of interest are labeledO. Whilethere is only oneOtag, we\u2019ll have distinctBandItags for each named entity class.The number of tags is thus 2n+1 tags, wherenis the number of entity types. BIO",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 28,
      "token_count": 769,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 32",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 29,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIO TaggingA method that lets us turn a segmentation task (finding boundaries of entities) into a classification taskRamshaw and Marcus (1995)16CHAPTER11\u2022MASKEDLANGUAGEMODELSCiting high fuel prices,[ORGUnited Airlines]said[TIMEFriday]ithas increased fares by[MONEY$6]per round trip on \ufb02ights to somecities also served by lower-cost carriers.[ORGAmerican Airlines],aunit of[ORGAMR Corp.], immediately matched the move, spokesman[PERTim Wagner]said.[ORGUnited], a unit of[ORGUAL Corp.],said the increase took effect[TIMEThursday]and applies to mostroutes where it competes against discount carriers, such as[LOCChicago]to[LOCDallas]and[LOCDenver]to[LOCSan Francisco].The text contains 13 mentions of named entities including 5 organizations, 4 loca-tions, 2 times, 1 person, and 1 mention of money. Figure11.10shows typical genericnamed entity types. Many applications will also need to use speci\ufb01c entity types likeproteins, genes, commercial products, or works of art.Type Tag Sample Categories Example sentencesPeoplePERpeople, charactersTuringis a giant of computer science.OrganizationORGcompanies, sports teams TheIPCCwarned about the cyclone.LocationLOCregions, mountains, seasMt. Sanitasis inSunshine Canyon.Geo-Political EntityGPEcountries, statesPalo Altois raising the fees for parking.Figure 11.10A list of generic named entity types with the kinds of entities they refer to.Named entity recognition is a useful step in various natural language processingtasks, including linking text to information in structured knowledge sources likeWikipedia, measuring sentiment or attitudes toward a particular entity in text, oreven as part of anonymizing text for privacy. The NER task is is dif\ufb01cult becauseof the ambiguity of segmenting NER spans, \ufb01guring out which tokens are entitiesand which aren\u2019t, since most words in a text will not be named entities. Anotherdif\ufb01culty is caused by type ambiguity. The mentionWashingtoncan refer to aperson, a sports team, a city, or the US government, as we see in Fig.11.11.[PERWashington] was born into slavery on the farm of James Burroughs.[ORGWashington] went up 2 games to 1 in the four-game series.Blair arrived in [LOCWashington] for what may well be his last state visit.In June, [GPEWashington] passed a primary seatbelt law.Figure 11.11Examples of type ambiguities in the use of the nameWashington.11.5.2 BIO TaggingOne standard approach to sequence labeling for a span-recognition problem likeNER isBIO tagging(Ramshaw and Marcus,1995). This is a method that allows usBIO taggingto treat NER like a word-by-word sequence labeling task, via tags that capture boththe boundary and the named entity type. Consider the following sentence:[PERJane Villanueva]of[ORGUnited], a unit of[ORGUnited AirlinesHolding], said the fare applies to the[LOCChicago]route.Figure11.12shows the same excerpt represented withBIOtagging, as well asBIOvariants calledIOtagging andBIOEStagging. In BIO tagging we label any tokenthatbeginsa span of interest with the labelB, tokens that occurinsidea span aretagged with anI, and any tokensoutsideof any span of interest are labeledO. Whilethere is only oneOtag, we\u2019ll have distinctBandItags for each named entity class.The number of tags is thus 2n+1 tags, wherenis the number of entity types.",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 30,
      "token_count": 791,
      "chapter_title": ""
    }
  },
  {
    "content": "Burroughs.[ORGWashington] went up 2 games to 1 in the four-game series.Blair arrived in [LOCWashington] for what may well be his last state visit.In June, [GPEWashington] passed a primary seatbelt law.Figure 11.11Examples of type ambiguities in the use of the nameWashington.11.5.2 BIO TaggingOne standard approach to sequence labeling for a span-recognition problem likeNER isBIO tagging(Ramshaw and Marcus,1995). This is a method that allows usBIO taggingto treat NER like a word-by-word sequence labeling task, via tags that capture boththe boundary and the named entity type. Consider the following sentence:[PERJane Villanueva]of[ORGUnited], a unit of[ORGUnited AirlinesHolding], said the fare applies to the[LOCChicago]route.Figure11.12shows the same excerpt represented withBIOtagging, as well asBIOvariants calledIOtagging andBIOEStagging. In BIO tagging we label any tokenthatbeginsa span of interest with the labelB, tokens that occurinsidea span aretagged with anI, and any tokensoutsideof any span of interest are labeledO. Whilethere is only oneOtag, we\u2019ll have distinctBandItags for each named entity class.The number of tags is thus 2n+1 tags, wherenis the number of entity types. BIO11.5\u2022FINE-TUNING FORSEQUENCELABELLING:NAMEDENTITYRECOGNITION17tagging can represent exactly the same information as the bracketed notation, but hasthe advantage that we can represent the task in the same simple sequence modelingway as part-of-speech tagging: assigning a single labelyito each input wordxi:WordsIO LabelBIO LabelBIOES LabelJaneI-PERB-PERB-PERVillanuevaI-PERI-PERE-PERofOOOUnitedI-ORGB-ORGB-ORGAirlinesI-ORGI-ORGI-ORGHoldingI-ORGI-ORGE-ORGdiscussedOOOtheOOOChicagoI-LOCB-LOCS-LOCrouteOOO.OOOFigure 11.12NER as a sequence model, showing IO, BIO, and BIOES taggings.We\u2019ve also shown two variant tagging schemes: IO tagging, which loses someinformation by eliminating the B tag, and BIOES tagging, which adds an end tagEfor the end of a span, and a span tagSfor a span consisting of only one word.11.5.3 Sequence LabelingIn sequence labeling, we pass the \ufb01nal output vector corresponding to each inputtoken to a classi\ufb01er that produces a softmax distribution over the possible set oftags. For a single feedforward layer classi\ufb01er, the set of weights to be learned isWKof size[d\u21e5k], wherekis the number of possible tags for the task. A greedyapproach, where the argmax tag for each token is taken as a likely answer, can beused to generate the \ufb01nal output tag sequence. Fig.11.13illustrates an example ofthis approach, whereyiis a vector of probabilities over tags, andkindexes the tags.yi=softmax(hLiWK)(11.12)ti=argmaxk(yi)(11.13)Alternatively, the distribution over labels provided by the softmax for each inputtoken can be passed to a conditional random \ufb01eld (CRF) layer which can take globaltag-level transitions into account (see Chapter 17 on CRFs).Tokenization and NERNote that supervised training data for NER is typically in the form of BIO tags as-sociated with text segmented at the word level. For example the following sentencecontaining two named entities:[LOCMt.",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 31,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "B tag, and BIOES tagging, which adds an end tagEfor the end of a span, and a span tagSfor a span consisting of only one word.11.5.3 Sequence LabelingIn sequence labeling, we pass the \ufb01nal output vector corresponding to each inputtoken to a classi\ufb01er that produces a softmax distribution over the possible set oftags. For a single feedforward layer classi\ufb01er, the set of weights to be learned isWKof size[d\u21e5k], wherekis the number of possible tags for the task. A greedyapproach, where the argmax tag for each token is taken as a likely answer, can beused to generate the \ufb01nal output tag sequence. Fig.11.13illustrates an example ofthis approach, whereyiis a vector of probabilities over tags, andkindexes the tags.yi=softmax(hLiWK)(11.12)ti=argmaxk(yi)(11.13)Alternatively, the distribution over labels provided by the softmax for each inputtoken can be passed to a conditional random \ufb01eld (CRF) layer which can take globaltag-level transitions into account (see Chapter 17 on CRFs).Tokenization and NERNote that supervised training data for NER is typically in the form of BIO tags as-sociated with text segmented at the word level. For example the following sentencecontaining two named entities:[LOCMt. Sanitas]is in[LOCSunshine Canyon].would have the following set of per-word BIO tags.(11.14)Mt.B-LOCSanitasI-LOCisOinOSunshineB-LOCCanyonI-LOC.OUnfortunately, the sequence of WordPiece tokens for this sentence doesn\u2019t aligndirectly with BIO tags in the annotation:",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 32,
      "token_count": 378,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 33\n\nSequence labeling11.5\u2022FINE-TUNING FORSEQUENCELABELLING:NAMEDENTITYRECOGNITION17tagging can represent exactly the same information as the bracketed notation, but hasthe advantage that we can represent the task in the same simple sequence modelingway as part-of-speech tagging: assigning a single labelyito each input wordxi:WordsIO LabelBIO LabelBIOES LabelJaneI-PERB-PERB-PERVillanuevaI-PERI-PERE-PERofOOOUnitedI-ORGB-ORGB-ORGAirlinesI-ORGI-ORGI-ORGHoldingI-ORGI-ORGE-ORGdiscussedOOOtheOOOChicagoI-LOCB-LOCS-LOCrouteOOO.OOOFigure 11.12NER as a sequence model, showing IO, BIO, and BIOES taggings.We\u2019ve also shown two variant tagging schemes: IO tagging, which loses someinformation by eliminating the B tag, and BIOES tagging, which adds an end tagEfor the end of a span, and a span tagSfor a span consisting of only one word.11.5.3 Sequence LabelingIn sequence labeling, we pass the \ufb01nal output vector corresponding to each inputtoken to a classi\ufb01er that produces a softmax distribution over the possible set oftags. For a single feedforward layer classi\ufb01er, the set of weights to be learned isWKof size[d\u21e5k], wherekis the number of possible tags for the task. A greedyapproach, where the argmax tag for each token is taken as a likely answer, can beused to generate the \ufb01nal output tag sequence. Fig.11.13illustrates an example ofthis approach, whereyiis a vector of probabilities over tags, andkindexes the tags.yi=softmax(hLiWK)(11.12)ti=argmaxk(yi)(11.13)Alternatively, the distribution over labels provided by the softmax for each inputtoken can be passed to a conditional random \ufb01eld (CRF) layer which can take globaltag-level transitions into account (see Chapter 17 on CRFs).Tokenization and NERNote that supervised training data for NER is typically in the form of BIO tags as-sociated with text segmented at the word level. For example the following sentencecontaining two named entities:[LOCMt. Sanitas]is in[LOCSunshine Canyon].would have the following set of per-word BIO tags.(11.14)Mt.B-LOCSanitasI-LOCisOinOSunshineB-LOCCanyonI-LOC.OUnfortunately, the sequence of WordPiece tokens for this sentence doesn\u2019t aligndirectly with BIO tags in the annotation:\n[CLS]JaneVillanuevaofUnitedAirlines\nBidirectional Transformer EncoderB-PERI-PEROB-ORGI-ORG\nHoldingdiscussedI-ORGOWKNER headhiargmax\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+WKWKWKWKWKWKyi\n\n## Page 34\n\nMore detailsWe need to map between tokens (used by LLM) and words (used in definition of name entities)We evaluate NER with F1 (precision/recall)\n\n## Page 35\n\nMasked Language ModelsFine-Tuning for Classification",
    "metadata": {
      "source": "mlmjan25",
      "chunk_id": 33,
      "token_count": 716,
      "chapter_title": ""
    }
  }
]