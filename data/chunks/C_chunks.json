[
  {
    "content": "# C\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a9\n2024. All rights reserved. Draft of January 12, 2025.\nCHAPTER\nCStatistical Constituency Pars-\ning\nThe characters in Damon Runyon\u2019s short stories are willing to bet \u201con any propo-\nsition whatever\u201d, as Runyon says about Sky Masterson in The Idyll of Miss Sarah\nBrown , from getting aces back-to-back to a man being able to throw a peanut from\nsecond base to home plate. There is a moral here for language processing: with\nenough knowledge we can estimate the probability of just about anything. Chap-\nter 18 introduced constituency structure and the task of parsing it. Here, we show\nhow to build probabilistic models of syntactic knowledge and ef\ufb01cient probabilistic\nparsers.\nOne use of probabilistic parsing is to solve the problem of disambiguation . Re-\ncall from Chapter 18 that sentences on average tend to be syntactically ambiguous\nbecause of phenomena like coordination ambiguity andattachment ambiguity .\nThe CKY parsing algorithm can represent these ambiguities in an ef\ufb01cient way but\nis not equipped to resolve them. There we introduced a neural algorithm for disam-\nbiguation. Here we introduce probabilistic parsers, which offer an alternative solu-\ntion to the problem: compute the probability of each interpretation and choose the\nmost probable interpretation. The most commonly used probabilistic constituency\ngrammar formalism is the probabilistic context-free grammar (PCFG), a prob-\nabilistic augmentation of context-free grammars in which each rule is associated\nwith a probability. We introduce PCFGs in the next section, showing how they can\nbe trained on Treebank grammars and how they can be parsed with a probabilistic\nversion of the CKY algorithm of Chapter 18.\nWe then show a number of ways that we can improve on this basic probabil-\nity model (PCFGs trained on Treebank grammars), such as by modifying the set of\nnon-terminals (making them either more speci\ufb01c or more general), or adding more\nsophisticated conditioning factors like subcategorization or dependencies. Heav-\nily lexicalized grammar formalisms such as Lexical-Functional Grammar (LFG)\n(Bresnan, 1982), Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag,\n1994), Tree-Adjoining Grammar (TAG) (Joshi, 1985), and Combinatory Categorial\nGrammar (CCG) pose additional problems for probabilistic parsers. Section ??in-\ntroduces the task of supertagging and the use of heuristic search methods based on\ntheA* algorithm in the context of CCG parsing.\nC.1 Probabilistic Context-Free Grammars\nThe simplest augmentation of the context-free grammar is the Probabilistic Context-\nFree Grammar (PCFG ), also known as the Stochastic Context-Free Grammar PCFG\n(SCFG ), \ufb01rst proposed by Booth (1969). Recall that a context-free grammar Gis SCFG\nde\ufb01ned by four parameters ( N;S;R;S); a probabilistic context-free grammar is also\nde\ufb01ned by four parameters, with a slight augmentation to each of the rules in R:",
    "metadata": {
      "source": "C",
      "chunk_id": 0,
      "token_count": 696,
      "chapter_title": "C"
    }
  },
  {
    "content": "## Page 2\n\n2APPENDIX C \u2022 S TATISTICAL CONSTITUENCY PARSING\nNa set of non-terminal symbols (orvariables )\nSa set of terminal symbols (disjoint from N)\nRa set of rules or productions, each of the form A!b[p],\nwhere Ais a non-terminal,\nbis a string of symbols from the in\ufb01nite set of strings (S[N)\u0003,\nandpis a number between 0 and 1 expressing P(bjA)\nSa designated start symbol\nThat is, a PCFG differs from a standard CFG by augmenting each rule in Rwith\na conditional probability:\nA!b[p] (C.1)\nHere pexpresses the probability that the given non-terminal Awill be expanded\nto the sequence b. That is, pis the conditional probability of a given expansion b\ngiven the left-hand-side (LHS) non-terminal A. We can represent this probability as\nP(A!b)\nor as\nP(A!bjA)\nor as\nP(RHSjLHS)\nThus, if we consider all the possible expansions of a non-terminal, the sum of their\nprobabilities must be 1:X\nbP(A!b) =1\nFigure C.1 shows a PCFG: a probabilistic augmentation of the L1miniature En-\nglish CFG grammar and lexicon. Note that the probabilities of all of the expansions\nof each non-terminal sum to 1. Also note that these probabilities were made up\nfor pedagogical purposes. A real grammar has a great many more rules for each\nnon-terminal; hence, the probabilities of any particular rule would tend to be much\nsmaller.\nA PCFG is said to be consistent if the sum of the probabilities of all sentences consistent\nin the language equals 1. Certain kinds of recursive rules cause a grammar to be\ninconsistent by causing in\ufb01nitely looping derivations for some sentences. For ex-\nample, a rule S!Swith probability 1 would lead to lost probability mass due to\nderivations that never terminate. See Booth and Thompson (1973) for more details\non consistent and inconsistent grammars.\nHow are PCFGs used? A PCFG can be used to estimate a number of useful\nprobabilities concerning a sentence and its parse tree(s), including the probability of\na particular parse tree (useful in disambiguation) and the probability of a sentence\nor a piece of a sentence (useful in language modeling). Let\u2019s see how this works.\nC.1.1 PCFGs for Disambiguation\nA PCFG assigns a probability to each parse tree T(i.e., each derivation ) of a sen-\ntence S. This attribute is useful in disambiguation . For example, consider the two\nparses of the sentence \u201cBook the dinner \ufb02ight\u201d shown in Fig. C.2. The sensible parse",
    "metadata": {
      "source": "C",
      "chunk_id": 1,
      "token_count": 610,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3",
    "metadata": {
      "source": "C",
      "chunk_id": 2,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "C.1 \u2022 P ROBABILISTIC CONTEXT -FREE GRAMMARS 3\nGrammar Lexicon\nS!NP VP [:80] Det!that[:10]ja[:30]jthe[:60]\nS!Aux NP VP [:15] Noun!book[:10]jtrip[:30]\nS!VP [:05]jmeal[:05]jmoney [:05]\nNP!Pronoun [:35]j\ufb02ight [:40]jdinner [:10]\nNP!Proper-Noun [:30] Verb!book[:30]jinclude [:30]\nNP!Det Nominal [:20]jprefer [:40]\nNP!Nominal [:15] Pronoun!I[:40]jshe[:05]\nNominal!Noun [:75]jme[:15]jyou[:40]\nNominal!Nominal Noun [:20] Proper-Noun!Houston [:60]\nNominal!Nominal PP [:05]jNWA [:40]\nVP!Verb [:35] Aux!does[:60]jcan[:40]\nVP!Verb NP [:20] Preposition!from[:30]jto[:30]\nVP!Verb NP PP [:10]jon[:20]jnear[:15]\nVP!Verb PP [:15]jthrough [:05]\nVP!Verb NP NP [:05]\nVP!VP PP [:15]\nPP!Preposition NP [1:0]\nFigure C.1 A PCFG that is a probabilistic augmentation of the L1miniature English CFG\ngrammar and lexicon of Fig. ??. These probabilities were made up for pedagogical purposes\nand are not based on a corpus (any real corpus would have many more rules, so the true\nprobabilities of each rule would be much smaller).\non the left means \u201cBook a \ufb02ight that serves dinner\u201d. The nonsensical parse on the\nright, however, would have to mean something like \u201cBook a \ufb02ight on behalf of \u2018the\ndinner\u201d\u2019 just as a structurally similar sentence like \u201cCan you book John a \ufb02ight?\u201d\nmeans something like \u201cCan you book a \ufb02ight on behalf of John?\u201d\nThe probability of a particular parse Tis de\ufb01ned as the product of the probabil-\nities of all the nrules used to expand each of the nnon-terminal nodes in the parse\ntreeT, where each rule ican be expressed as LHS i!RHS i:\nP(T;S) =nY\ni=1P(RHS ijLHS i) (C.2)\nThe resulting probability P(T;S)is both the joint probability of the parse and the\nsentence and also the probability of the parse P(T). How can this be true? First, by\nthe de\ufb01nition of joint probability:\nP(T;S) =P(T)P(SjT) (C.3)\nBut since a parse tree includes all the words of the sentence, P(SjT)is 1. Thus,\nP(T;S) =P(T)P(SjT) =P(T) (C.4)\nWe can compute the probability of each of the trees in Fig. C.2 by multiplying the\nprobabilities of each of the rules used in the derivation. For example, the probability\nof the left tree in Fig. C.2a (call it Tle ft) and the right tree (Fig. C.2b or Tright) can be\ncomputed as follows:\nP(Tle ft) = :05\u0003:20\u0003:20\u0003:20\u0003:75\u0003:30\u0003:60\u0003:10\u0003:40=2:2\u000210\u00006",
    "metadata": {
      "source": "C",
      "chunk_id": 3,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "treeT, where each rule ican be expressed as LHS i!RHS i:\nP(T;S) =nY\ni=1P(RHS ijLHS i) (C.2)\nThe resulting probability P(T;S)is both the joint probability of the parse and the\nsentence and also the probability of the parse P(T). How can this be true? First, by\nthe de\ufb01nition of joint probability:\nP(T;S) =P(T)P(SjT) (C.3)\nBut since a parse tree includes all the words of the sentence, P(SjT)is 1. Thus,\nP(T;S) =P(T)P(SjT) =P(T) (C.4)\nWe can compute the probability of each of the trees in Fig. C.2 by multiplying the\nprobabilities of each of the rules used in the derivation. For example, the probability\nof the left tree in Fig. C.2a (call it Tle ft) and the right tree (Fig. C.2b or Tright) can be\ncomputed as follows:\nP(Tle ft) = :05\u0003:20\u0003:20\u0003:20\u0003:75\u0003:30\u0003:60\u0003:10\u0003:40=2:2\u000210\u00006\nP(Tright) = :05\u0003:10\u0003:20\u0003:15\u0003:75\u0003:75\u0003:30\u0003:60\u0003:10\u0003:40=6:1\u000210\u00007",
    "metadata": {
      "source": "C",
      "chunk_id": 4,
      "token_count": 322,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4APPENDIX C \u2022 S TATISTICAL CONSTITUENCY PARSING\nS\nVP\nNP\nNominal\nNoun\n\ufb02ightNominal\nNoun\ndinnerDet\ntheVerb\nBookS\nVP\nNP\nNominal\nNoun\n\ufb02ightNP\nNominal\nNoun\ndinnerDet\ntheVerb\nBook\nRules P Rules P\nS!VP .05 S !VP .05\nVP!Verb NP .20 VP !Verb NP NP .10\nNP!Det Nominal .20 NP !Det Nominal .20\nNominal!Nominal Noun .20 NP !Nominal .15\nNominal!Noun .75 Nominal !Noun .75\nNominal!Noun .75\nVerb!book .30 Verb !book .30\nDet!the .60 Det !the .60\nNoun!dinner .10 Noun !dinner .10\nNoun!\ufb02ight .40 Noun !\ufb02ight .40\nFigure C.2 Two parse trees for an ambiguous sentence. The parse on the left corresponds\nto the sensible meaning \u201cBook a \ufb02ight that serves dinner\u201d, while the parse on the right corre-\nsponds to the nonsensical meaning \u201cBook a \ufb02ight on behalf of \u2018the dinner\u2019 \u201d.\nWe can see that the left tree in Fig. C.2 has a much higher probability than the\ntree on the right. Thus, this parse would correctly be chosen by a disambiguation\nalgorithm that selects the parse with the highest PCFG probability.\nLet\u2019s formalize this intuition that picking the parse with the highest probability\nis the correct way to do disambiguation. Consider all the possible parse trees for a\ngiven sentence S. The string of words Sis called the yield of any parse tree over S. yield\nThus, out of all parse trees with a yield of S, the disambiguation algorithm picks the\nparse tree that is most probable given S:\n\u02c6T(S) = argmax\nT s:t:S=yield (T)P(TjS) (C.5)\nBy de\ufb01nition, the probability P(TjS)can be rewritten as P(T;S)=P(S), thus leading\nto\n\u02c6T(S) = argmax\nT s:t:S=yield (T)P(T;S)\nP(S)(C.6)\nSince we are maximizing over all parse trees for the same sentence, P(S)will be a",
    "metadata": {
      "source": "C",
      "chunk_id": 5,
      "token_count": 543,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\nC.1 \u2022 P ROBABILISTIC CONTEXT -FREE GRAMMARS 5\nconstant for each tree, so we can eliminate it:\n\u02c6T(S) = argmax\nT s:t:S=yield (T)P(T;S) (C.7)\nFurthermore, since we showed above that P(T;S) =P(T), the \ufb01nal equation for\nchoosing the most likely parse neatly simpli\ufb01es to choosing the parse with the high-\nest probability:\n\u02c6T(S) = argmax\nT s:t:S=yield (T)P(T) (C.8)\nC.1.2 PCFGs for Language Modeling\nA second attribute of a PCFG is that it assigns a probability to the string of words\nconstituting a sentence. This is important in language modeling , whether for use\nin speech recognition, machine translation, spelling correction, augmentative com-\nmunication, or other applications. The probability of an unambiguous sentence is\nP(T;S) =P(T)or just the probability of the single parse tree for that sentence. The\nprobability of an ambiguous sentence is the sum of the probabilities of all the parse\ntrees for the sentence:\nP(S) =X\nT s:t:S=yield (T)P(T;S) (C.9)\n=X\nT s:t:S=yield (T)P(T) (C.10)\nAn additional feature of PCFGs that is useful for language modeling is their ability\nto assign a probability to substrings of a sentence. For example, suppose we want\nto know the probability of the next word wiin a sentence given all the words we\u2019ve\nseen so far w1;:::;wi\u00001. The general formula for this is\nP(wijw1;w2;:::;wi\u00001) =P(w1;w2;:::;wi\u00001;wi)\nP(w1;w2;:::;wi\u00001)(C.11)\nWe saw in Chapter 3 a simple approximation of this probability using N-grams,\nconditioning on only the last word or two instead of the entire context; thus, the\nbigram approximation would give us\nP(wijw1;w2;:::;wi\u00001)\u0019P(wi\u00001;wi)\nP(wi\u00001)(C.12)\nBut the fact that the N-gram model can only make use of a couple words of context\nmeans it is ignoring potentially useful prediction cues. Consider predicting the word\nafter in the following sentence from Chelba and Jelinek (2000):\n(C.13) the contract ended with a loss of 7 cents after trading as low as 9 cents\nA trigram grammar must predict after from the words 7 cents , while it seems clear\nthat the verb ended and the subject contract would be useful predictors that a PCFG-\nbased parser could help us make use of. Indeed, it turns out that PCFGs allow us to\ncondition on the entire previous context w1;w2;:::;wi\u00001shown in Eq. C.11.\nIn summary, PCFGs can be applied both to disambiguation in syntactic parsing\nand to word prediction in language modeling. Both of these applications require that\nwe be able to compute the probability of parse tree Tfor a given sentence S. The\nnext few sections introduce some algorithms for computing this probability.",
    "metadata": {
      "source": "C",
      "chunk_id": 6,
      "token_count": 728,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6",
    "metadata": {
      "source": "C",
      "chunk_id": 7,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "6APPENDIX C \u2022 S TATISTICAL CONSTITUENCY PARSING\nC.2 Probabilistic CKY Parsing of PCFGs\nThe parsing problem for PCFGs is to produce the most-likely parse \u02c6Tfor a given\nsentence S, that is,\n\u02c6T(S) = argmax\nT s:t:S=yield (T)P(T) (C.14)\nThe algorithms for computing the most likely parse are simple extensions of the\nstandard algorithms for parsing; most modern probabilistic parsers are based on the\nprobabilistic CKY algorithm, \ufb01rst described by Ney (1991). The probabilistic CKYprobabilistic\nCKY\nalgorithm assumes the PCFG is in Chomsky normal form. Recall from page ??that\nin CNF, the right-hand side of each rule must expand to either two non-terminals or\nto a single terminal, i.e., rules have the form A!B C, orA!w.\nFor the CKY algorithm, we represented each sentence as having indices between\nthe words. Thus, an example sentence like\n(C.15) Book the \ufb02ight through Houston.\nwould assume the following indices between each word:\n(C.16) 0\nBook 1\nthe 2\n\ufb02ight 3\nthrough 4\nHouston 5\nUsing these indices, each constituent in the CKY parse tree is encoded in a\ntwo-dimensional matrix. Speci\ufb01cally, for a sentence of length nand a grammar\nthat contains Vnon-terminals, we use the upper-triangular portion of an (n+1)\u0002\n(n+1)matrix. For CKY , each cell table[i;j]contained a list of constituents that\ncould span the sequence of words from itoj. For probabilistic CKY , it\u2019s slightly\nsimpler to think of the constituents in each cell as constituting a third dimension of\nmaximum length V. This third dimension corresponds to each non-terminal that can\nbe placed in this cell, and the value of the cell is then a probability for that non-\nterminal/constituent rather than a list of constituents. In summary, each cell [i;j;A]\nin this (n+1)\u0002(n+1)\u0002Vmatrix is the probability of a constituent of type Athat\nspans positions ithrough jof the input.\nFigure C.3 gives the probabilistic CKY algorithm.\nfunction PROBABILISTIC -CKY( words,grammar )returns most probable parse\nand its probability\nforj from 1toLENGTH (words )do\nfor allfAjA!words [j]2grammarg\ntable [j\u00001;j;A] P(A!words [j])\nfori from j\u00002downto 0do\nfork i+1toj\u00001do\nfor allfAjA!BC2grammar ;\nandtable[i;k;B]>0andtable[k;j;C]>0g\nif(table [i,j,A]<P(A!BC)\u0002table [i,k,B]\u0002table [k,j,C])then\ntable [i,j,A] P(A!BC)\u0002table [i,k,B]\u0002table [k,j,C]\nback[i,j,A] fk;B;Cg\nreturn BUILD TREE (back[1, L ENGTH (words ),S]),table [1, L ENGTH (words ),S]\nFigure C.3 The probabilistic CKY algorithm for \ufb01nding the maximum probability parse\nof a string of num words words given a PCFG grammar with num rules rules in Chomsky\nnormal form. back is an array of backpointers used to recover the best parse. The build tree\nfunction is left as an exercise to the reader.",
    "metadata": {
      "source": "C",
      "chunk_id": 8,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\nC.2 \u2022 P ROBABILISTIC CKY P ARSING OF PCFG S7\nLike the basic CKY algorithm in Fig. ??, the probabilistic CKY algorithm re-\nquires a grammar in Chomsky normal form. Converting a probabilistic grammar to\nCNF requires that we also modify the probabilities so that the probability of each\nparse remains the same under the new CNF grammar. Exercise C.2 asks you to mod-\nify the algorithm for conversion to CNF in Chapter 18 so that it correctly handles\nrule probabilities.\nIn practice, a generalized CKY algorithm that handles unit productions directly\nis typically used. Recall that Exercise 13.3 asked you to make this change in CKY;\nExercise C.3 asks you to extend this change to probabilistic CKY .\nLet\u2019s see an example of the probabilistic CKY chart, using the following mini-\ngrammar, which is already in CNF:\nS!NP VP .80 Det!the .40\nNP!Det N .30 Det!a .40\nV P!V NP .20 N!meal .01\nV!includes .05 N!f light .02\nGiven this grammar, Fig. C.4 shows the \ufb01rst steps in the probabilistic CKY parse\nof the sentence \u201cThe \ufb02ight includes a meal\u201d.\nTheflight[0,1][0,2][0,3][1,2][1,3][2,3]Det: .40includesameal\n[3,4][4,5]N: .02V: .05NP: .30 *.40 *.02= .0024[0,4][1,4][2,4][3,5][2,5][1,5][0,5]\nDet: .40N: .01\nFigure C.4 The beginning of the probabilistic CKY matrix. Filling out the rest of the chart\nis left as Exercise C.4 for the reader.",
    "metadata": {
      "source": "C",
      "chunk_id": 9,
      "token_count": 420,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8APPENDIX C \u2022 S TATISTICAL CONSTITUENCY PARSING\nC.3 Ways to Learn PCFG Rule Probabilities\nWhere do PCFG rule probabilities come from? There are two ways to learn proba-\nbilities for the rules of a grammar. The simplest way is to use a treebank, a corpus\nof already parsed sentences. Recall that we introduced in Appendix D the idea\nof treebanks and the commonly used Penn Treebank , a collection of parse trees\nin English, Chinese, and other languages that is distributed by the Linguistic Data\nConsortium. Given a treebank, we can compute the probability of each expansion\nof a non-terminal by counting the number of times that expansion occurs and then\nnormalizing.\nP(a!bja) =Count (a!b)P\ngCount (a!g)=Count (a!b)\nCount (a)(C.17)\nIf we don\u2019t have a treebank but we do have a (non-probabilistic) parser, we can\ngenerate the counts we need for computing PCFG rule probabilities by \ufb01rst parsing\na corpus of sentences with the parser. If sentences were unambiguous, it would be\nas simple as this: parse the corpus, increment a counter for every rule in the parse,\nand then normalize to get probabilities.\nBut wait! Since most sentences are ambiguous, that is, have multiple parses, we\ndon\u2019t know which parse to count the rules in. Instead, we need to keep a separate\ncount for each parse of a sentence and weight each of these partial counts by the\nprobability of the parse it appears in. But to get these parse probabilities to weight\nthe rules, we need to already have a probabilistic parser.\nThe intuition for solving this chicken-and-egg problem is to incrementally im-\nprove our estimates by beginning with a parser with equal rule probabilities, then\nparse the sentence, compute a probability for each parse, use these probabilities to\nweight the counts, re-estimate the rule probabilities, and so on, until our proba-\nbilities converge. The standard algorithm for computing this solution is called the\ninside-outside algorithm; it was proposed by Baker (1979) as a generalization of the inside-outside\nforward-backward algorithm for HMMs. Like forward-backward, inside-outside is\na special case of the Expectation Maximization (EM) algorithm, and hence has two\nsteps: the expectation step , and the maximization step . See Lari and Young (1990)\nor Manning and Sch \u00a8utze (1999) for more on the algorithm.\nC.4 Problems with PCFGs\nWhile probabilistic context-free grammars are a natural extension to context-free\ngrammars, they have two main problems as probability estimators:\nPoor independence assumptions: CFG rules impose an independence assumption\non probabilities that leads to poor modeling of structural dependencies across\nthe parse tree.\nLack of lexical conditioning: CFG rules don\u2019t model syntactic facts about speci\ufb01c\nwords, leading to problems with subcategorization ambiguities, preposition\nattachment, and coordinate structure ambiguities.\nBecause of these problems, probabilistic constituent parsing models use some\naugmented version of PCFGs, or modify the Treebank-based grammar in some way.",
    "metadata": {
      "source": "C",
      "chunk_id": 10,
      "token_count": 693,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\nC.4 \u2022 P ROBLEMS WITH PCFG S9\nIn the next few sections after discussing the problems in more detail we introduce\nsome of these augmentations.\nC.4.1 Independence Assumptions Miss Rule Dependencies\nLet\u2019s look at these problems in more detail. Recall that in a CFG the expansion\nof a non-terminal is independent of the context, that is, of the other nearby non-\nterminals in the parse tree. Similarly, in a PCFG, the probability of a particular\nrule like NP!Det N is also independent of the rest of the tree. By de\ufb01nition, the\nprobability of a group of independent events is the product of their probabilities.\nThese two facts explain why in a PCFG we compute the probability of a tree by just\nmultiplying the probabilities of each non-terminal expansion.\nUnfortunately, this CFG independence assumption results in poor probability\nestimates. This is because in English the choice of how a node expands can after all\ndepend on the location of the node in the parse tree. For example, in English it turns\nout that NPs that are syntactic subjects are far more likely to be pronouns, and NPs\nthat are syntactic objects are far more likely to be non-pronominal (e.g., a proper\nnoun or a determiner noun sequence), as shown by these statistics for NPs in the\nSwitchboard corpus (Francis et al., 1999):1\nPronoun Non-Pronoun\nSubject 91% 9%\nObject 34% 66%\nUnfortunately, there is no way to represent this contextual difference in the prob-\nabilities of a PCFG. Consider two expansions of the non-terminal NPas a pronoun\nor as a determiner+noun. How shall we set the probabilities of these two rules? If\nwe set their probabilities to their overall probability in the Switchboard corpus, the\ntwo rules have about equal probability.\nNP!DT NN :28\nNP!PRP :25\nBecause PCFGs don\u2019t allow a rule probability to be conditioned on surrounding\ncontext, this equal probability is all we get; there is no way to capture the fact that in\nsubject position, the probability for NP!PRP should go up to .91, while in object\nposition, the probability for NP!DT NN should go up to .66.\nThese dependencies could be captured if the probability of expanding an NPas\na pronoun (e.g., NP!PRP) versus a lexical NP(e.g., NP!DT NN ) were con-\nditioned on whether the NPwas a subject or an object. Section C.5 introduces the\ntechnique of parent annotation for adding this kind of conditioning.\nC.4.2 Lack of Sensitivity to Lexical Dependencies\nA second class of problems with PCFGs is their lack of sensitivity to the words in\nthe parse tree. Words do play a role in PCFGs since the parse probability includes\nthe probability of a word given a part-of-speech (e.g., from rules like V!sleep ,\nNN!book , etc.).\n1Distribution of subjects from 31,021 declarative sentences; distribution of objects from 7,489 sen-\ntences. This tendency is caused by the use of subject position to realize the topic or old information\nin a sentence (Giv \u00b4on, 1990). Pronouns are a way to talk about old information, while non-pronominal\n(\u201clexical\u201d) noun-phrases are often used to introduce new referents (Chapter 23).",
    "metadata": {
      "source": "C",
      "chunk_id": 11,
      "token_count": 743,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 APPENDIX C \u2022 S TATISTICAL CONSTITUENCY PARSING\nBut it turns out that lexical information is useful in other places in the grammar,\nsuch as in resolving prepositional phrase ( PP) attachment ambiguities. Since prepo-\nsitional phrases in English can modify a noun phrase or a verb phrase, when a parser\n\ufb01nds a prepositional phrase, it must decide where to attach it in the tree. Consider\nthe following example:\n(C.18) Workers dumped sacks into a bin.\nFigure C.5 shows two possible parse trees for this sentence; the one on the left is\nthe correct parse; Fig. C.6 shows another perspective on the preposition attachment\nproblem, demonstrating that resolving the ambiguity in Fig. C.5 is equivalent to\ndeciding whether to attach the prepositional phrase into the rest of the tree at the\nNPorVPnodes; we say that the correct parse requires VP attachment , and the VP attachment\nincorrect parse implies NP attachment . NP attachment\nS\nVP\nPP\nNP\nNN\nbinDT\naP\nintoNP\nNNS\nsacksVBD\ndumpedNP\nNNS\nworkersS\nVP\nNP\nPP\nNP\nNN\nbinDT\naP\nintoNP\nNNS\nsacksVBD\ndumpedNP\nNNS\nworkers\nFigure C.5 Two possible parse trees for a prepositional phrase attachment ambiguity . The left parse is the\nsensible one, in which \u201cinto a bin\u201d describes the resulting location of the sacks. In the right incorrect parse, the\nsacks to be dumped are the ones which are already \u201cinto a bin\u201d, whatever that might mean.\nWhy doesn\u2019t a PCFG already deal with PPattachment ambiguities? Note that\nthe two parse trees in Fig. C.5 have almost exactly the same rules; they differ only\nin that the left-hand parse has this rule:\nV P!V BD NP PP\nwhile the right-hand parse has these:\nV P!V BD NP\nNP!NP PP\nDepending on how these probabilities are set, a PCFG will always either prefer\nNPattachment or VPattachment. As it happens, NPattachment is slightly more\ncommon in English, so if we trained these rule probabilities on a corpus, we might\nalways prefer NPattachment, causing us to misparse this sentence.\nBut suppose we set the probabilities to prefer the VPattachment for this sen-\ntence. Now we would misparse the following, which requires NPattachment:",
    "metadata": {
      "source": "C",
      "chunk_id": 12,
      "token_count": 532,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\nC.5 \u2022 I MPROVING PCFG S BY SPLITTING NON-TERMINALS 11\nS\nVP\nNP\nNNS\nsacksVBD\ndumpedNP\nNNS\nworkersPP\nNP\nNN\nbinDT\naP\ninto\nFigure C.6 Another view of the preposition attachment problem. Should the PPon the right attach to the VP\norNPnodes of the partial parse tree on the left?\n(C.19) \ufb01shermen caught tons of herring\nWhat information in the input sentence lets us know that (C.19) requires NP\nattachment while (C.18) requires VPattachment? These preferences come from\nthe identities of the verbs, nouns, and prepositions. The af\ufb01nity between the verb\ndumped and the preposition intois greater than the af\ufb01nity between the noun sacks\nand the preposition into, thus leading to VPattachment. On the other hand, in (C.19)\nthe af\ufb01nity between tons andofis greater than that between caught andof, leading to\nNPattachment. Thus, to get the correct parse for these kinds of examples, we need\na model that somehow augments the PCFG probabilities to deal with these lexical\ndependency statistics for different verbs and prepositions.lexical\ndependency\nCoordination ambiguities are another case in which lexical dependencies are\nthe key to choosing the proper parse. Figure C.7 shows an example from Collins\n(1999) with two parses for the phrase dogs in houses and cats . Because dogs is\nsemantically a better conjunct for catsthan houses (and because most dogs can\u2019t \ufb01t\ninside cats), the parse [dogs in [ NPhouses and cats]] is intuitively unnatural and\nshould be dispreferred. The two parses in Fig. C.7, however, have exactly the same\nPCFG rules, and thus a PCFG will assign them the same probability.\nIn summary, we have shown in this section and the previous one that probabilistic\ncontext-free grammars are incapable of modeling important structural andlexical\ndependencies. In the next two sections we sketch current methods for augmenting\nPCFGs to deal with both these issues.\nC.5 Improving PCFGs by Splitting Non-Terminals\nLet\u2019s start with the \ufb01rst of the two problems with PCFGs mentioned above: their\ninability to model structural dependencies, like the fact that NPs in subject position\ntend to be pronouns, whereas NPs in object position tend to have full lexical (non-\npronominal) form. How could we augment a PCFG to correctly model this fact?\nOne idea would be to split theNPnon-terminal into two versions: one for sub- split\njects, one for objects. Having two nodes (e.g., NPsubject andNPobject ) would allow\nus to correctly model their different distributional properties, since we would have",
    "metadata": {
      "source": "C",
      "chunk_id": 13,
      "token_count": 623,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 APPENDIX C \u2022 S TATISTICAL CONSTITUENCY PARSING\nNP\nNP\nNoun\ncatsConj\nandNP\nPP\nNP\nNoun\nhousesPrep\ninNP\nNoun\ndogsNP\nPP\nNP\nNP\nNoun\ncatsConj\nandNP\nNoun\nhousesPrep\ninNP\nNoun\ndogs\nFigure C.7 An instance of coordination ambiguity. Although the left structure is intuitively\nthe correct one, a PCFG will assign them identical probabilities since both structures use\nexactly the same set of rules. After Collins (1999).\ndifferent probabilities for the rule NPsubject!PRP and the rule NPobject!PRP.\nOne way to implement this intuition of splits is to do parent annotation (John-parent\nannotation\nson, 1998), in which we annotate each node with its parent in the parse tree. Thus,\nanNPnode that is the subject of the sentence and hence has parent Swould be anno-\ntated NP\u02c6S, while a direct object NPwhose parent is VPwould be annotated NP\u02c6VP.\nFigure C.8 shows an example of a tree produced by a grammar that parent-annotates\nthe phrasal non-terminals (like NPandVP).\na) S\nVP\nNP\nNN\n\ufb02ightDT\naVBD\nneedNP\nPRP\nIb) S\nVP\u02c6S\nNP\u02c6VP\nNN\n\ufb02ightDT\naVBD\nneedNP\u02c6S\nPRP\nI\nFigure C.8 A standard PCFG parse tree (a) and one which has parent annotation on the\nnodes which aren\u2019t pre-terminal (b). All the non-terminal nodes (except the pre-terminal\npart-of-speech nodes) in parse (b) have been annotated with the identity of their parent.\nIn addition to splitting these phrasal nodes, we can also improve a PCFG by\nsplitting the pre-terminal part-of-speech nodes (Klein and Manning, 2003b). For ex-\nample, different kinds of adverbs (RB) tend to occur in different syntactic positions:\nthe most common adverbs with ADVP parents are also andnow, with VPparents\nn\u2019tandnot, and with NPparents only andjust. Thus, adding tags like RB\u02c6ADVP,\nRB\u02c6VP, and RB\u02c6NP can be useful in improving PCFG modeling.\nSimilarly, the Penn Treebank tag IN can mark a wide variety of parts-of-speech,\nincluding subordinating conjunctions ( while ,as,if), complementizers ( that,for),\nand prepositions ( of,in,from). Some of these differences can be captured by parent",
    "metadata": {
      "source": "C",
      "chunk_id": 14,
      "token_count": 581,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\nC.6 \u2022 P ROBABILISTIC LEXICALIZED CFG S13\nannotation (subordinating conjunctions occur under S, prepositions under PP), while\nothers require splitting the pre-terminal nodes. Figure C.9 shows an example from\nKlein and Manning (2003b) in which even a parent-annotated grammar incorrectly\nparses works as a noun in to see if advertising works . Splitting pre-terminals to allow\nifto prefer a sentential complement results in the correct verbal parse.\nNode-splitting is not without problems; it increases the size of the grammar and\nhence reduces the amount of training data available for each grammar rule, leading\nto over\ufb01tting. Thus, it is important to split to just the correct level of granularity for a\nparticular training set. While early models employed handwritten rules to try to \ufb01nd\nan optimal number of non-terminals (Klein and Manning, 2003b), modern models\nautomatically search for the optimal splits. The split and merge algorithm of Petrov split and merge\net al. (2006), for example, starts with a simple X-bar grammar, alternately splits the\nnon-terminals, and merges non-terminals, \ufb01nding the set of annotated nodes that\nmaximizes the likelihood of the training set treebank.\nC.6 Probabilistic Lexicalized CFGs\nThe previous section showed that a simple probabilistic CKY algorithm for pars-\ning raw PCFGs can achieve extremely high parsing accuracy if the grammar rule\nsymbols are redesigned by automatic splits and merges.\nIn this section, we discuss an alternative family of models in which instead of\nmodifying the grammar rules, we modify the probabilistic model of the parser to\nallow for lexicalized rules. The resulting family of lexicalized parsers includes the\nCollins parser (Collins, 1999) and the Charniak parser (Charniak, 1997).\nWe saw in Section ??that syntactic constituents could be associated with a lexi-\ncalhead , and we de\ufb01ned a lexicalized grammar in which each non-terminal in thelexicalized\ngrammar\ntree is annotated with its lexical head, where a rule like V P!V BD NP PP would\nbe extended as\nVP(dumped)!VBD(dumped) NP(sacks) PP(into) (C.20)\nVP\u02c6S\nVP\u02c6VP\nPP\u02c6VP\nNP\u02c6PP\nNNS\nworksNN\nadvertisingIN\nifVB\nseeTO\ntoVP\u02c6S\nVP\u02c6VP\nSBAR\u02c6VP\nS\u02c6SBAR\nVP\u02c6S\nVBZ\u02c6VP\nworksNP\u02c6S\nNN\u02c6NP\nadvertisingIN\u02c6SBAR\nifVB\u02c6VP\nseeTO\u02c6VP\nto\nFigure C.9 An incorrect parse even with a parent-annotated parse (left). The correct parse (right), was pro-\nduced by a grammar in which the pre-terminal nodes have been split, allowing the probabilistic grammar to\ncapture the fact that ifprefers sentential complements. Adapted from Klein and Manning (2003b).",
    "metadata": {
      "source": "C",
      "chunk_id": 15,
      "token_count": 676,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n14 APPENDIX C \u2022 S TATISTICAL CONSTITUENCY PARSING\nIn the standard type of lexicalized grammar, we actually make a further exten-\nsion, which is to associate the head tag , the part-of-speech tags of the headwords, head tag\nwith the non-terminal symbols as well. Each rule is thus lexicalized by both the\nheadword and the head tag of each constituent resulting in a format for lexicalized\nrules like\nVP(dumped,VBD) !VBD(dumped,VBD) NP(sacks,NNS) PP(into,P) (C.21)\nWe show a lexicalized parse tree with head tags in Fig. C.10, extended from Fig. ??.\nTOP\nS(dumped,VBD)\nVP(dumped,VBD)\nPP(into,P)\nNP(bin,NN)\nNN(bin,NN)\nbinDT(a,DT)\naP(into,P)\nintoNP(sacks,NNS)\nNNS(sacks,NNS)\nsacksVBD(dumped,VBD)\ndumpedNP(workers,NNS)\nNNS(workers,NNS)\nworkers\nInternal Rules Lexical Rules\nTOP !S(dumped,VBD) NNS(workers,NNS) !workers\nS(dumped,VBD) !NP(workers,NNS) VP(dumped,VBD) VBD(dumped,VBD) !dumped\nNP(workers,NNS) !NNS(workers,NNS) NNS(sacks,NNS) !sacks\nVP(dumped,VBD) !VBD(dumped, VBD) NP(sacks,NNS) PP(into,P) P(into,P) !into\nPP(into,P) !P(into,P) NP(bin,NN) DT(a,DT) !a\nNP(bin,NN) !DT(a,DT) NN(bin,NN) NN(bin,NN) !bin\nFigure C.10 A lexicalized tree, including head tags, for a WSJ sentence, adapted from Collins (1999). Below\nwe show the PCFG rules needed for this parse tree, internal rules on the left, and lexical rules on the right.\nTo generate such a lexicalized tree, each PCFG rule must be augmented to iden-\ntify one right-hand constituent to be the head daughter. The headword for a node is\nthen set to the headword of its head daughter, and the head tag to the part-of-speech\ntag of the headword. Recall that we gave in Fig. ??a set of handwritten rules for\nidentifying the heads of particular constituents.\nA natural way to think of a lexicalized grammar is as a parent annotation, that\nis, as a simple context-free grammar with many copies of each rule, one copy for\neach possible headword/head tag for each constituent. Thinking of a probabilistic\nlexicalized CFG in this way would lead to the set of simple PCFG rules shown below\nthe tree in Fig. C.10.\nNote that Fig. C.10 shows two kinds of rules: lexical rules , which express the lexical rules\nexpansion of a pre-terminal to a word, and internal rules , which express the other internal rules\nrule expansions. We need to distinguish these kinds of rules in a lexicalized grammar\nbecause they are associated with very different kinds of probabilities. The lexical\nrules are deterministic, that is, they have probability 1.0 since a lexicalized pre-",
    "metadata": {
      "source": "C",
      "chunk_id": 16,
      "token_count": 717,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15",
    "metadata": {
      "source": "C",
      "chunk_id": 17,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "C.6 \u2022 P ROBABILISTIC LEXICALIZED CFG S15\nterminal like NN(bin;NN)can only expand to the word bin. But for the internal\nrules, we need to estimate probabilities.\nSuppose we were to treat a probabilistic lexicalized CFG like a really big CFG\nthat just happened to have lots of very complex non-terminals and estimate the\nprobabilities for each rule from maximum likelihood estimates. Thus, according\nto Eq. C.17, the MLE estimate for the probability for the rule P(VP(dumped,VBD)\n!VBD(dumped, VBD) NP(sacks,NNS) PP(into,P)) would be\nCount(VP(dumped,VBD) !VBD(dumped, VBD) NP(sacks,NNS) PP(into,P))\nCount(VP(dumped,VBD))(C.22)\nBut there\u2019s no way we can get good estimates of counts like those in Eq. C.22 be-\ncause they are so speci\ufb01c: we\u2019re unlikely to see many (or even any) instances of a\nsentence with a verb phrase headed by dumped that has one NPargument headed by\nsacks and a PPargument headed by into. In other words, counts of fully lexicalized\nPCFG rules like this will be far too sparse, and most rule probabilities will come out\n0.\nThe idea of lexicalized parsing is to make some further independence assump-\ntions to break down each rule so that we would estimate the probability\nP(VP(dumped,VBD) !VBD(dumped, VBD) NP(sacks,NNS) PP(into,P))\nas the product of smaller independent probability estimates for which we could ac-\nquire reasonable counts. The next section summarizes one such method, the Collins\nparsing method.\nC.6.1 The Collins Parser\nStatistical parsers differ in exactly which independence assumptions they make.\nLet\u2019s look at the assumptions in a simpli\ufb01ed version of the Collins parser. The \ufb01rst\nintuition of the Collins parser is to think of the right-hand side of every (internal)\nCFG rule as consisting of a head non-terminal, together with the non-terminals to\nthe left of the head and the non-terminals to the right of the head. In the abstract, we\nthink about these rules as follows:\nLHS!LnLn\u00001:::L1H R 1:::Rn\u00001Rn (C.23)\nSince this is a lexicalized grammar, each of the symbols like L1orR3orHorLHS\nis actually a complex symbol representing the category and its head and head tag,\nlikeVP(dumped,VP) orNP(sacks,NNS) .\nNow, instead of computing a single MLE probability for this rule, we are going\nto break down this rule via a neat generative story, a slight simpli\ufb01cation of what is\ncalled Collins Model 1. This new generative story is that given the left-hand side,\nwe \ufb01rst generate the head of the rule and then generate the dependents of the head,\none by one, from the inside out. Each of these steps will have its own probability.\nWe also add a special STOP non-terminal at the left and right edges of the rule;\nthis non-terminal allows the model to know when to stop generating dependents on a\ngiven side. We generate dependents on the left side of the head until we\u2019ve generated\nSTOP on the left side of the head, at which point we move to the right side of the\nhead and start generating dependents there until we generate STOP . So it\u2019s as if we\nare generating a rule augmented as follows:",
    "metadata": {
      "source": "C",
      "chunk_id": 18,
      "token_count": 770,
      "chapter_title": ""
    }
  },
  {
    "content": "think about these rules as follows:\nLHS!LnLn\u00001:::L1H R 1:::Rn\u00001Rn (C.23)\nSince this is a lexicalized grammar, each of the symbols like L1orR3orHorLHS\nis actually a complex symbol representing the category and its head and head tag,\nlikeVP(dumped,VP) orNP(sacks,NNS) .\nNow, instead of computing a single MLE probability for this rule, we are going\nto break down this rule via a neat generative story, a slight simpli\ufb01cation of what is\ncalled Collins Model 1. This new generative story is that given the left-hand side,\nwe \ufb01rst generate the head of the rule and then generate the dependents of the head,\none by one, from the inside out. Each of these steps will have its own probability.\nWe also add a special STOP non-terminal at the left and right edges of the rule;\nthis non-terminal allows the model to know when to stop generating dependents on a\ngiven side. We generate dependents on the left side of the head until we\u2019ve generated\nSTOP on the left side of the head, at which point we move to the right side of the\nhead and start generating dependents there until we generate STOP . So it\u2019s as if we\nare generating a rule augmented as follows:\nP(VP(dumped,VBD) ! (C.24)\nSTOP VBD(dumped, VBD) NP(sacks,NNS) PP(into,P) STOP )",
    "metadata": {
      "source": "C",
      "chunk_id": 19,
      "token_count": 325,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\n16 APPENDIX C \u2022 S TATISTICAL CONSTITUENCY PARSING\nLet\u2019s see the generative story for this augmented rule. We make use of three kinds\nof probabilities: PHfor generating heads, PLfor generating dependents on the left,\nandPRfor generating dependents on the right.\n1. Generate the head VBD(dumped,VBD) with probability\nPH(HjLHS) = P H(VBD(dumped,VBD) jVP(dumped,VBD))VP(dumped,VBD)\nVBD(dumped,VBD)\n2. Generate the left dependent (which is STOP, since there isn\u2019t\none) with probability\nPL(STOPjVP(dumped,VBD) VBD(dumped,VBD))VP(dumped,VBD)\nVBD(dumped,VBD) STOP\n3. Generate right dependent NP(sacks,NNS) with probability\nPR(NP(sacks,NNS)jVP(dumped,VBD), VBD(dumped,VBD))VP(dumped,VBD)\nNP(sacks,NNS) VBD(dumped,VBD) STOP\n4. Generate the right dependent PP(into,P) with probability\nPR(PP(into,P)jVP(dumped,VBD), VBD(dumped,VBD))VP(dumped,VBD)\nPP(into,P) NP(sacks,NNS) VBD(dumped,VBD) STOP\n5) Generate the right dependent STOP with probability\nPR(STOPjVP(dumped,VBD), VBD(dumped,VBD))VP(dumped,VBD)\nSTOP PP(into,P) NP(sacks,NNS) VBD(dumped,VBD) STOP\nIn summary, the probability of this rule\nP(VP(dumped,VBD) ! (C.25)\nVBD(dumped, VBD) NP(sacks,NNS) PP(into,P) )\nis estimated (simplifying the notation a bit from the steps above):\nPH(VBDjVP , dumped)\u0002PL(STOPjVP;VBD;dumped ) (C.26)\n\u0002PR(NP(sacks,NNS)jVP;VBD;dumped )\n\u0002PR(PP(into,P)jVP;VBD;dumped )\n\u0002PR(STOPjVP;VBD;dumped )\nEach of these probabilities can be estimated from much smaller amounts of data than\nthe full probability in Eq. C.25. For example, the maximum likelihood estimate for\nthe component probability PR(NP(sacks,NNS)jVP;VBD;dumped )is\nCount (VP(dumped,VBD) with NNS(sacks) as a daughter somewhere on the right )\nCount (VP(dumped,VBD) )\n(C.27)\nThese counts are much less subject to sparsity problems than are complex counts\nlike those in Eq. C.25.",
    "metadata": {
      "source": "C",
      "chunk_id": 20,
      "token_count": 593,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\nC.7 \u2022 S UMMARY 17\nMore generally, if His a head with head word hwand head tag ht,lw=ltand\nrw=rtare the word/tag on the left and right respectively, and Pis the parent, then the\nprobability of an entire rule can be expressed as follows:\n1. Generate the head of the phrase H(hw;ht)with probability:\nPH(H(hw;ht)jP;hw;ht)\n2. Generate modi\ufb01ers to the left of the head with total probability\nn+1Y\ni=1PL(Li(lwi;lti)jP;H;hw;ht)\nsuch that Ln+1(lwn+1;ltn+1) = STOP , and we stop generating once we\u2019ve gen-\nerated a STOP token.\n3. Generate modi\ufb01ers to the right of the head with total probability:\nn+1Y\ni=1PR(Ri(rwi;rti)jP;H;hw;ht)\nsuch that Rn+1(rwn+1;rtn+1) = STOP , and we stop generating once we\u2019ve gen-\nerated a STOP token.\nThe parsing algorithm for the Collins model is an extension of probabilistic\nCKY . Extending the CKY algorithm to handle basic lexicalized probabilities is left\nas Exercises 14.5 and 14.6 for the reader.\nC.7 Summary\nThis chapter has sketched the basics of probabilistic parsing, concentrating on\nprobabilistic context-free grammars .\n\u2022 Probabilistic grammars assign a probability to a sentence or string of words\nwhile attempting to capture sophisticated grammatical information.\n\u2022 A probabilistic context-free grammar (PCFG ) is a context-free\ngrammar in which every rule is annotated with the probability of that rule\nbeing chosen. Each PCFG rule is treated as if it were conditionally inde-\npendent ; thus, the probability of a sentence is computed by multiplying the\nprobabilities of each rule in the parse of the sentence.\n\u2022 The probabilistic CKY ( Cocke-Kasami-Younger ) algorithm is a probabilistic\nversion of the CKY parsing algorithm.\n\u2022 PCFG probabilities can be learned by counting in a parsed corpus or by pars-\ning a corpus. The inside-outside algorithm is a way of dealing with the fact\nthat the sentences being parsed are ambiguous.\n\u2022 Raw PCFGs suffer from poor independence assumptions among rules and lack\nof sensitivity to lexical dependencies.\n\u2022 One way to deal with this problem is to split and merge non-terminals (auto-\nmatically or by hand).",
    "metadata": {
      "source": "C",
      "chunk_id": 21,
      "token_count": 547,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\n18 APPENDIX C \u2022 S TATISTICAL CONSTITUENCY PARSING\n\u2022Probabilistic lexicalized CFG s are another solution to this problem in which\nthe basic PCFG model is augmented with a lexical head for each rule. The\nprobability of a rule can then be conditioned on the lexical head or nearby\nheads.\n\u2022 Parsers for lexicalized PCFGs (like the Collins parser) are based on extensions\nto probabilistic CKY parsing.\nBibliographical and Historical Notes\nMany of the formal properties of probabilistic context-free grammars were \ufb01rst\nworked out by Booth (1969) and Salomaa (1969). Baker (1979) proposed the inside-\noutside algorithm for unsupervised training of PCFG probabilities, and used a CKY-\nstyle parsing algorithm to compute inside probabilities. Jelinek and Lafferty (1991)\nextended the CKY algorithm to compute probabilities for pre\ufb01xes. Stolcke (1995)\nadapted the Earley algorithm to use with PCFGs.\nA number of researchers starting in the early 1990s worked on adding lexical de-\npendencies to PCFGs and on making PCFG rule probabilities more sensitive to sur-\nrounding syntactic structure. For example, Schabes et al. (1988) and Schabes (1990)\npresented early work on the use of heads. Many papers on the use of lexical depen-\ndencies were \ufb01rst presented at the DARPA Speech and Natural Language Workshop\nin June 1990. A paper by Hindle and Rooth (1990) applied lexical dependencies\nto the problem of attaching prepositional phrases; in the question session to a later\npaper, Ken Church suggested applying this method to full parsing (Marcus, 1990).\nEarly work on such probabilistic CFG parsing augmented with probabilistic depen-\ndency information includes Magerman and Marcus (1991), Black et al. (1992), Bod\n(1993), and Jelinek et al. (1994), in addition to Collins (1996), Charniak (1997), and\nCollins (1999) discussed above. Other recent PCFG parsing models include Klein\nand Manning (2003a) and Petrov et al. (2006).\nThis early lexical probabilistic work led initially to work focused on solving\nspeci\ufb01c parsing problems like preposition-phrase attachment by using methods in-\ncluding transformation-based learning (TBL) (Brill and Resnik, 1994), maximum\nentropy (Ratnaparkhi et al., 1994), memory-based learning (Zavrel and Daelemans,\n1997), log-linear models (Franz, 1997), decision trees that used semantic distance\nbetween heads (computed from WordNet) (Stetina and Nagao, 1997), and boosting\n(Abney et al., 1999). Another direction extended the lexical probabilistic parsing\nwork to build probabilistic formulations of grammars other than PCFGs, such as\nprobabilistic TAG grammar (Resnik 1992, Schabes 1992), based on the TAG gram-\nmars discussed in Appendix D, probabilistic LR parsing (Briscoe and Carroll, 1993),\nand probabilistic link grammar (Lafferty et al., 1992). The supertagging approach\nwe saw for CCG was developed for TAG grammars (Bangalore and Joshi 1999,\nJoshi and Srinivas 1994), based on the lexicalized TAG grammars of Schabes et al.\n(1988).\nExercises\nC.1 Implement the CKY algorithm.",
    "metadata": {
      "source": "C",
      "chunk_id": 22,
      "token_count": 778,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\nEXERCISES 19\nC.2 Modify the algorithm for conversion to CNF from Chapter 18 to correctly\nhandle rule probabilities. Make sure that the resulting CNF assigns the same\ntotal probability to each parse tree.\nC.3 Recall that Exercise 13.3 asked you to update the CKY algorithm to han-\ndle unit productions directly rather than converting them to CNF. Extend this\nchange to probabilistic CKY .\nC.4 Fill out the rest of the probabilistic CKY chart in Fig. C.4.\nC.5 Sketch how the CKY algorithm would have to be augmented to handle lexi-\ncalized probabilities.\nC.6 Implement your lexicalized extension of the CKY algorithm.",
    "metadata": {
      "source": "C",
      "chunk_id": 23,
      "token_count": 153,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20",
    "metadata": {
      "source": "C",
      "chunk_id": 24,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "20 Appendix C \u2022 Statistical Constituency Parsing\nAbney, S. P., R. E. Schapire, and Y . Singer. 1999. Boosting\napplied to tagging and PP attachment. EMNLP/VLC .\nBaker, J. K. 1979. Trainable grammars for speech recogni-\ntion. Speech Communication Papers for the 97th Meeting\nof the Acoustical Society of America .\nBangalore, S. and A. K. Joshi. 1999. Supertagging: An\napproach to almost parsing. Computational Linguistics ,\n25(2):237\u2013265.\nBlack, E., F. Jelinek, J. D. Lafferty, D. M. Magerman, R. L.\nMercer, and S. Roukos. 1992. Towards history-based\ngrammars: Using richer models for probabilistic parsing.\nHLT.\nBod, R. 1993. Using an annotated corpus as a stochastic\ngrammar. EACL .\nBooth, T. L. 1969. Probabilistic representation of formal\nlanguages. IEEE Conference Record of the 1969 Tenth\nAnnual Symposium on Switching and Automata Theory .\nBooth, T. L. and R. A. Thompson. 1973. Applying proba-\nbility measures to abstract languages. IEEE Transactions\non Computers , C-22(5):442\u2013450.\nBresnan, J., ed. 1982. The Mental Representation of Gram-\nmatical Relations . MIT Press.\nBrill, E. and P. Resnik. 1994. A rule-based approach to\nprepositional phrase attachment disambiguation. COL-\nING.\nBriscoe, T. and J. Carroll. 1993. Generalized proba-\nbilistic LR parsing of natural language (corpora) with\nuni\ufb01cation-based grammars. Computational Linguistics ,\n19(1):25\u201359.\nCharniak, E. 1997. Statistical parsing with a context-free\ngrammar and word statistics. AAAI .\nChelba, C. and F. Jelinek. 2000. Structured language model-\ning. Computer Speech and Language , 14:283\u2013332.\nCollins, M. 1996. A new statistical parser based on bigram\nlexical dependencies. ACL.\nCollins, M. 1999. Head-Driven Statistical Models for Natu-\nral Language Parsing . Ph.D. thesis, University of Penn-\nsylvania, Philadelphia.\nFrancis, H. S., M. L. Gregory, and L. A. Michaelis. 1999. Are\nlexical subjects deviant? CLS-99 . University of Chicago.\nFranz, A. 1997. Independence assumptions considered\nharmful. ACL.\nGiv\u00b4on, T. 1990. Syntax: A Functional Typological Introduc-\ntion. John Benjamins.\nHindle, D. and M. Rooth. 1990. Structural ambiguity and\nlexical relations. Speech and Natural Language Work-\nshop .\nHindle, D. and M. Rooth. 1991. Structural ambiguity and\nlexical relations. ACL.\nJelinek, F. and J. D. Lafferty. 1991. Computation of the\nprobability of initial substring generation by stochas-\ntic context-free grammars. Computational Linguistics ,\n17(3):315\u2013323.\nJelinek, F., J. D. Lafferty, D. M. Magerman, R. L. Mercer,",
    "metadata": {
      "source": "C",
      "chunk_id": 25,
      "token_count": 757,
      "chapter_title": ""
    }
  },
  {
    "content": "Collins, M. 1996. A new statistical parser based on bigram\nlexical dependencies. ACL.\nCollins, M. 1999. Head-Driven Statistical Models for Natu-\nral Language Parsing . Ph.D. thesis, University of Penn-\nsylvania, Philadelphia.\nFrancis, H. S., M. L. Gregory, and L. A. Michaelis. 1999. Are\nlexical subjects deviant? CLS-99 . University of Chicago.\nFranz, A. 1997. Independence assumptions considered\nharmful. ACL.\nGiv\u00b4on, T. 1990. Syntax: A Functional Typological Introduc-\ntion. John Benjamins.\nHindle, D. and M. Rooth. 1990. Structural ambiguity and\nlexical relations. Speech and Natural Language Work-\nshop .\nHindle, D. and M. Rooth. 1991. Structural ambiguity and\nlexical relations. ACL.\nJelinek, F. and J. D. Lafferty. 1991. Computation of the\nprobability of initial substring generation by stochas-\ntic context-free grammars. Computational Linguistics ,\n17(3):315\u2013323.\nJelinek, F., J. D. Lafferty, D. M. Magerman, R. L. Mercer,\nA. Ratnaparkhi, and S. Roukos. 1994. Decision tree pars-\ning using a hidden derivation model. ARPA Human Lan-\nguage Technologies Workshop .Johnson, M. 1998. PCFG models of linguistic tree represen-\ntations. Computational Linguistics , 24(4):613\u2013632.\nJoshi, A. K. 1985. Tree adjoining grammars: How\nmuch context-sensitivity is required to provide reasonable\nstructural descriptions? In D. R. Dowty, L. Karttunen,\nand A. Zwicky, eds, Natural Language Parsing , 206\u2013250.\nCambridge University Press.\nJoshi, A. K. and B. Srinivas. 1994. Disambiguation of super\nparts of speech (or supertags): Almost parsing. COLING .\nKlein, D. and C. D. Manning. 2001. Parsing and hyper-\ngraphs. IWPT-01 .\nKlein, D. and C. D. Manning. 2003a. A* parsing: Fast exact\nViterbi parse selection. HLT-NAACL .\nKlein, D. and C. D. Manning. 2003b. Accurate unlexicalized\nparsing. HLT-NAACL .\nLafferty, J. D., D. Sleator, and D. Temperley. 1992. Gram-\nmatical trigrams: A probabilistic model of link gram-\nmar. AAAI Fall Symposium on Probabilistic Approaches\nto Natural Language .\nLari, K. and S. J. Young. 1990. The estimation of stochas-\ntic context-free grammars using the Inside-Outside algo-\nrithm. Computer Speech and Language , 4:35\u201356.\nMagerman, D. M. and M. P. Marcus. 1991. Pearl: A proba-\nbilistic chart parser. EACL .\nManning, C. D. and H. Sch \u00a8utze. 1999. Foundations of Sta-\ntistical Natural Language Processing . MIT Press.\nMarcus, M. P. 1990. Summary of session 9: Automatic ac-\nquisition of linguistic structure. Speech and Natural Lan-\nguage Workshop .",
    "metadata": {
      "source": "C",
      "chunk_id": 26,
      "token_count": 760,
      "chapter_title": ""
    }
  },
  {
    "content": "graphs. IWPT-01 .\nKlein, D. and C. D. Manning. 2003a. A* parsing: Fast exact\nViterbi parse selection. HLT-NAACL .\nKlein, D. and C. D. Manning. 2003b. Accurate unlexicalized\nparsing. HLT-NAACL .\nLafferty, J. D., D. Sleator, and D. Temperley. 1992. Gram-\nmatical trigrams: A probabilistic model of link gram-\nmar. AAAI Fall Symposium on Probabilistic Approaches\nto Natural Language .\nLari, K. and S. J. Young. 1990. The estimation of stochas-\ntic context-free grammars using the Inside-Outside algo-\nrithm. Computer Speech and Language , 4:35\u201356.\nMagerman, D. M. and M. P. Marcus. 1991. Pearl: A proba-\nbilistic chart parser. EACL .\nManning, C. D. and H. Sch \u00a8utze. 1999. Foundations of Sta-\ntistical Natural Language Processing . MIT Press.\nMarcus, M. P. 1990. Summary of session 9: Automatic ac-\nquisition of linguistic structure. Speech and Natural Lan-\nguage Workshop .\nNey, H. 1991. Dynamic programming parsing for context-\nfree grammars in continuous speech recognition. IEEE\nTransactions on Signal Processing , 39(2):336\u2013340.\nPetrov, S., L. Barrett, R. Thibaux, and D. Klein. 2006. Learn-\ning accurate, compact, and interpretable tree annotation.\nCOLING/ACL .\nPollard, C. and I. A. Sag. 1994. Head-Driven Phrase Struc-\nture Grammar . University of Chicago Press.\nRatnaparkhi, A., J. C. Reynar, and S. Roukos. 1994. A max-\nimum entropy model for prepositional phrase attachment.\nARPA Human Language Technologies Workshop .\nResnik, P. 1992. Probabilistic tree-adjoining grammar as\na framework for statistical natural language processing.\nCOLING .\nSalomaa, A. 1969. Probabilistic and weighted grammars.\nInformation and Control , 15:529\u2013544.\nSchabes, Y . 1990. Mathematical and Computational As-\npects of Lexicalized Grammars . Ph.D. thesis, University\nof Pennsylvania, Philadelphia, PA.\nSchabes, Y . 1992. Stochastic lexicalized tree-adjoining\ngrammars. COLING .\nSchabes, Y ., A. Abeill \u00b4e, and A. K. Joshi. 1988. Parsing\nstrategies with \u2018lexicalized\u2019 grammars: Applications to\nTree Adjoining Grammars. COLING .\nStetina, J. and M. Nagao. 1997. Corpus based PP attachment\nambiguity resolution with a semantic dictionary. Pro-\nceedings of the Fifth Workshop on Very Large Corpora .\nStolcke, A. 1995. An ef\ufb01cient probabilistic context-free\nparsing algorithm that computes pre\ufb01x probabilities.\nComputational Linguistics , 21(2):165\u2013202.",
    "metadata": {
      "source": "C",
      "chunk_id": 27,
      "token_count": 697,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21\n\nExercises 21\nZavrel, J. and W. Daelemans. 1997. Memory-based learning:\nUsing similarity for smoothing. ACL.",
    "metadata": {
      "source": "C",
      "chunk_id": 28,
      "token_count": 39,
      "chapter_title": ""
    }
  }
]