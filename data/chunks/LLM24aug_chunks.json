[
  {
    "content": "# LLM24aug\n\n## Page 1\n\nLarge Language ModelsIntroduction to Large Language Models\n\n## Page 2\n\nLanguage models\u2022Remember the simple n-gram language model\u2022Assigns probabilities to sequences of words\u2022Generate text by sampling possible next words\u2022Is trained on counts computed from lots of text\u2022Large language models are similar and different:\u2022Assigns probabilities to sequences of words\u2022Generate text by sampling possible next words\u2022Are trained by learning to guess the next word\n\n## Page 3\n\nLarge language models\u2022Even through pretrained only to predict words\u2022Learn a lot of useful language knowledge\u2022Since training on a lot of text\n\n## Page 4\n\nThree architectures for large language modelsDecoders   Encoders     Encoder-decodersGPT, Claude,  BERT family,  Flan-T5, WhisperLlama    HuBERTMixtralPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\n32Decoders\u2022Language models! What we\u2019ve seen so far.\u2022Nice to generate from; can\u2019t condition on future wordsEncoders\u2022Gets bidirectional context \u2013 can condition on future!\u2022How do we train them to build strong representations?Encoder-Decoders\u2022Good parts of decoders and encoders?\u2022What\u2019s the best way to pretrain them?Pretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\n32Decoders\u2022Language models! What we\u2019ve seen so far.\u2022Nice to generate from; can\u2019t condition on future wordsEncoders\u2022Gets bidirectional context \u2013 can condition on future!\u2022How do we train them to build strong representations?Encoder-Decoders\u2022Good parts of decoders and encoders?\u2022What\u2019s the best way to pretrain them?Pretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\n32Decoders\u2022Language models! What we\u2019ve seen so far.\u2022Nice to generate from; can\u2019t condition on future wordsEncoders\u2022Gets bidirectional context \u2013 can condition on future!\u2022How do we train them to build strong representations?Encoder-Decoders\u2022Good parts of decoders and encoders?\u2022What\u2019s the best way to pretrain them?\n\n## Page 5\n\nEncodersMany varieties!\u2022Popular: Masked Language Models (MLMs)\u2022BERT family\u2022Trained by predicting words from surrounding words on both sides\u2022Are usually finetuned (trained on supervised data) for classification tasks.Pretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\n32Decoders\u2022Language models! What we\u2019ve seen so far.\u2022Nice to generate from; can\u2019t condition on future wordsEncoders\u2022Gets bidirectional context \u2013 can condition on future!\u2022How do we train them to build strong representations?Encoder-Decoders\u2022Good parts of decoders and encoders?\u2022What\u2019s the best way to pretrain them?\n\n## Page 6\n\nEncoder-Decoders\u2022Trained to map from one sequence to another\u2022Very popular for:\u2022machine translation (map from one language to another)\u2022speech recognition (map from acoustics to words)Pretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\n32Decoders\u2022Language models! What we\u2019ve seen so far.\u2022Nice to generate from; can\u2019t condition on future wordsEncoders\u2022Gets bidirectional context \u2013 can condition on future!\u2022How do we train them to build strong representations?Encoder-Decoders\u2022Good parts of decoders and encoders?\u2022What\u2019s the best way to pretrain them?\n\n## Page 7\n\nLarge Language ModelsIntroduction to Large Language Models\n\n## Page 8\n\nLarge Language ModelsLarge Language Models: What tasks can they do?",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 0,
      "token_count": 781,
      "chapter_title": "LLM24aug"
    }
  },
  {
    "content": "## Page 6\n\nEncoder-Decoders\u2022Trained to map from one sequence to another\u2022Very popular for:\u2022machine translation (map from one language to another)\u2022speech recognition (map from acoustics to words)Pretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\n32Decoders\u2022Language models! What we\u2019ve seen so far.\u2022Nice to generate from; can\u2019t condition on future wordsEncoders\u2022Gets bidirectional context \u2013 can condition on future!\u2022How do we train them to build strong representations?Encoder-Decoders\u2022Good parts of decoders and encoders?\u2022What\u2019s the best way to pretrain them?\n\n## Page 7\n\nLarge Language ModelsIntroduction to Large Language Models\n\n## Page 8\n\nLarge Language ModelsLarge Language Models: What tasks can they do?\n\n## Page 9\n\nBig ideaMany tasks can be turned into tasks of predicting words!\n\n## Page 10\n\nThis lecture: decoder-only modelsAlso called:\u2022Causal LLMs\u2022Autoregressive LLMs\u2022Left-to-right LLMs\u2022Predict words left to rightPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\n32Decoders\u2022Language models! What we\u2019ve seen so far.\u2022Nice to generate from; can\u2019t condition on future wordsEncoders\u2022Gets bidirectional context \u2013 can condition on future!\u2022How do we train them to build strong representations?Encoder-Decoders\u2022Good parts of decoders and encoders?\u2022What\u2019s the best way to pretrain them?\n\n## Page 11\n\nConditional Generation: Generating text conditioned on previous text!\nPre\ufb01x TextCompletion Text\nEncoderTransformerBlocksSoftmax\nlongall\nandthanksforallthe\nthe\u2026UUUnencoder layerLanguage ModelingHeadlogits\nSo\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\u2026",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 1,
      "token_count": 399,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\nBig ideaMany tasks can be turned into tasks of predicting words!\n\n## Page 10\n\nThis lecture: decoder-only modelsAlso called:\u2022Causal LLMs\u2022Autoregressive LLMs\u2022Left-to-right LLMs\u2022Predict words left to rightPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\n32Decoders\u2022Language models! What we\u2019ve seen so far.\u2022Nice to generate from; can\u2019t condition on future wordsEncoders\u2022Gets bidirectional context \u2013 can condition on future!\u2022How do we train them to build strong representations?Encoder-Decoders\u2022Good parts of decoders and encoders?\u2022What\u2019s the best way to pretrain them?\n\n## Page 11\n\nConditional Generation: Generating text conditioned on previous text!\nPre\ufb01x TextCompletion Text\nEncoderTransformerBlocksSoftmax\nlongall\nandthanksforallthe\nthe\u2026UUUnencoder layerLanguage ModelingHeadlogits\nSo\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\u2026\n\n## Page 12\n\nMany practical NLP tasks can be cast as word prediction!Sentiment analysis: \u201cI like Jackie Chan\u201d1.We give the language model this string:The sentiment of the sentence \"I like Jackie Chan\" is: 2.And see what word it thinks comes next:10.1\u2022LARGELANGUAGEMODELS WITHTRANSFORMERS3\nPre\ufb01x TextCompletion Text\nEncoderTransformerBlocksSoftmax\nlongall\nandthanksforallthe\nthe\u2026UUUnencoder layerLanguage ModelingHeadlogits\nSo\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\u2026\nFigure 10.1Left-to-right (also called autoregressive) text completion with transformer-based large languagemodels. As each token is generated, it gets added onto the context as a pre\ufb01x for generating the next token.word \u201cnegative\u201d to see which is higher:P(positive|The sentiment of the sentence \u2018\u2018I like Jackie Chan\" is:)P(negative|The sentiment of the sentence \u2018\u2018I like Jackie Chan\" is:)If the word \u201cpositive\u201d is more probable, we say the sentiment of the sentence ispositive, otherwise we say the sentiment is negative.We can also cast more complex tasks as word prediction. Consider questionanswering, in which the system is given a question (for example a question witha simple factual answer) and must give a textual answer; we introduce this task indetail in Chapter 15. We can cast the task of question answering as word predictionby giving a language model a question and a token likeA:suggesting that an answershould come next:Q: Who wrote the book \u2018\u2018The Origin of Species\"? A:If we ask a language model to compute the probability distribution over possiblenext words given this pre\ufb01x:P(w|Q: Who wrote the book \u2018\u2018The Origin of Species\"? A:)and look at which wordswhave high probabilities, we might expect to see thatCharlesis very likely, and then if we chooseCharlesand continue and askP(w|Q: Who wrote the book \u2018\u2018The Origin of Species\"? A: Charles)we might now see thatDarwinis the most probable token, and select it.Conditional generation can even be used to accomplish tasks that must generatelonger responses. Consider the task oftext summarization, which is to take a longtextsummarizationtext, such as a full-length article, and produce an effective shorter summary of it. Wecan cast summarization as language modeling by giving a large language model atext, and follow the text by a token liketl;dr; this token is short for something like",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 2,
      "token_count": 786,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 3,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Framing lots of tasks as conditional generationQA: \u201cWho wrote The Origin of Species\u201d1.We give the language model this string:2.And see what word it thinks comes next:3.And iterate:20CHAPTER10\u2022TRANSFORMERS ANDLARGELANGUAGEMODELS\nPre\ufb01x TextCompletion Text\nInputEmbeddingsTransformerBlocksSample from Softmax\nSolongall\nandthanksforallthe\nthe\u2026linear layer\nFigure 10.15Autoregressive text completion with transformer-based large language models.word \u201cnegative\u201d to see which is higher:P(positive|The sentiment of the sentence \u201cI like Jackie Chan\u201d is:)P(negative|The sentiment of the sentence \u201cI like Jackie Chan\u201d is:)If the word \u201cpositive\u201d is more probable, we say the sentiment of the sentence ispositive, otherwise we say the sentiment is negative.We can also cast more complex tasks as word prediction. Consider the taskof answering simple questions, a task we return to in Chapter 14. In this task thesystem is given some question and must give a textual answer. We can cast the taskof question answering as word prediction by giving a language model a question anda token likeA:suggesting that an answer should come next:Q: Who wrote the book \u2018\u2018The Origin of Species\"? A:If we ask a language model to computeP(w|Q: Who wrote the book \u201cThe Origin of Species\u201d? A:)and look at which wordswhave high probabilities, we might expect to see thatCharlesis very likely, and then if we chooseCharlesand continue and askP(w|Q: Who wrote the book \u201cThe Origin of Species\u201d? A: Charles)we might now see thatDarwinis the most probable word, and select it.Conditional generation can even be used to accomplish tasks that must generatelonger responses. Consider the task oftext summarization, which is to take a longtextsummarizationtext, such as a full-length article, and produce an effective shorter summary of it.We can cast summarization as language modeling by giving a large language modela text, and follow the text by a token liketl;dr; this token is short for somethinglike \u2018too long; don\u2019t read\u2019 and in recent years people often use this token, especiallyin informal work emails, when they are going to give a short summary. We canthen do conditional generation: give the language model this pre\ufb01x, and then ask10.1\u2022LARGELANGUAGEMODELS WITHTRANSFORMERS3\nPre\ufb01x TextCompletion Text\nEncoderTransformerBlocksSoftmax\nlongall\nandthanksforallthe\nthe\u2026UUUnencoder layerLanguage ModelingHeadlogits\nSo\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\u2026",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 4,
      "token_count": 582,
      "chapter_title": ""
    }
  },
  {
    "content": "Pre\ufb01x TextCompletion Text\nEncoderTransformerBlocksSoftmax\nlongall\nandthanksforallthe\nthe\u2026UUUnencoder layerLanguage ModelingHeadlogits\nSo\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\u2026\nFigure 10.1Left-to-right (also called autoregressive) text completion with transformer-based large languagemodels. As each token is generated, it gets added onto the context as a pre\ufb01x for generating the next token.word \u201cnegative\u201d to see which is higher:P(positive|The sentiment of the sentence \u2018\u2018I like Jackie Chan\" is:)P(negative|The sentiment of the sentence \u2018\u2018I like Jackie Chan\" is:)If the word \u201cpositive\u201d is more probable, we say the sentiment of the sentence ispositive, otherwise we say the sentiment is negative.We can also cast more complex tasks as word prediction. Consider questionanswering, in which the system is given a question (for example a question witha simple factual answer) and must give a textual answer; we introduce this task indetail in Chapter 15. We can cast the task of question answering as word predictionby giving a language model a question and a token likeA:suggesting that an answershould come next:Q: Who wrote the book \u2018\u2018The Origin of Species\"? A:If we ask a language model to compute the probability distribution over possiblenext words given this pre\ufb01x:P(w|Q: Who wrote the book \u2018\u2018The Origin of Species\"? A:)and look at which wordswhave high probabilities, we might expect to see thatCharlesis very likely, and then if we chooseCharlesand continue and askP(w|Q: Who wrote the book \u2018\u2018The Origin of Species\"? A: Charles)we might now see thatDarwinis the most probable token, and select it.Conditional generation can even be used to accomplish tasks that must generatelonger responses. Consider the task oftext summarization, which is to take a longtextsummarizationtext, such as a full-length article, and produce an effective shorter summary of it. Wecan cast summarization as language modeling by giving a large language model atext, and follow the text by a token liketl;dr; this token is short for something like10.1\u2022LARGELANGUAGEMODELS WITHTRANSFORMERS3\nPre\ufb01x TextCompletion Text\nEncoderTransformerBlocksSoftmax\nlongall\nandthanksforallthe\nthe\u2026UUUnencoder layerLanguage ModelingHeadlogits\nSo\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\u2026",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 5,
      "token_count": 559,
      "chapter_title": ""
    }
  },
  {
    "content": "Pre\ufb01x TextCompletion Text\nEncoderTransformerBlocksSoftmax\nlongall\nandthanksforallthe\nthe\u2026UUUnencoder layerLanguage ModelingHeadlogits\nSo\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\u2026\nFigure 10.1Left-to-right (also called autoregressive) text completion with transformer-based large languagemodels. As each token is generated, it gets added onto the context as a pre\ufb01x for generating the next token.word \u201cnegative\u201d to see which is higher:P(positive|The sentiment of the sentence \u2018\u2018I like Jackie Chan\" is:)P(negative|The sentiment of the sentence \u2018\u2018I like Jackie Chan\" is:)If the word \u201cpositive\u201d is more probable, we say the sentiment of the sentence ispositive, otherwise we say the sentiment is negative.We can also cast more complex tasks as word prediction. Consider questionanswering, in which the system is given a question (for example a question witha simple factual answer) and must give a textual answer; we introduce this task indetail in Chapter 15. We can cast the task of question answering as word predictionby giving a language model a question and a token likeA:suggesting that an answershould come next:Q: Who wrote the book \u2018\u2018The Origin of Species\"? A:If we ask a language model to compute the probability distribution over possiblenext words given this pre\ufb01x:P(w|Q: Who wrote the book \u2018\u2018The Origin of Species\"? A:)and look at which wordswhave high probabilities, we might expect to see thatCharlesis very likely, and then if we chooseCharlesand continue and askP(w|Q: Who wrote the book \u2018\u2018The Origin of Species\"? A: Charles)we might now see thatDarwinis the most probable token, and select it.Conditional generation can even be used to accomplish tasks that must generatelonger responses. Consider the task oftext summarization, which is to take a longtextsummarizationtext, such as a full-length article, and produce an effective shorter summary of it. Wecan cast summarization as language modeling by giving a large language model atext, and follow the text by a token liketl;dr; this token is short for something like",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 6,
      "token_count": 482,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 7,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Summarization4CHAPTER10\u2022LARGELANGUAGEMODELS\u2018too long; didn\u2019t read\u2019 and in recent years people often use this token, especially ininformal work emails, when they are going to give a short summary. Since this tokenis suf\ufb01ciently frequent in language model training data, language models have seenmany texts in which the token occurs before a summary, and hence will interpret thetoken as instructions to generate a summary. We can then do conditional generation:give the language model this pre\ufb01x, and then have it generate the following words,one by one, and take the entire response as a summary. Fig.10.2shows an exampleof a text and a human-produced summary from a widely-used summarization corpusconsisting of CNN and Daily Mirror news articles.Original ArticleThe only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuffand offering it for sale online? People are actually buying it. For $89, self-styled entrepreneurKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box \u2013 enoughfor 10 to 15 snowballs, he says.But not if you live in New England or surrounding states. \u201cWe will not ship snow to any statesin the northeast!\u201d says Waring\u2019s website, ShipSnowYo.com. \u201cWe\u2019re in the business of expungingsnow!\u201dHis website and social media accounts claim to have \ufb01lled more than 133 orders for snow \u2013 morethan 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set arecord this winter for the snowiest month in its history. Most residents see the huge piles of snowchoking their yards and sidewalks as a nuisance, but Waring saw an opportunity.According to Boston.com, it all started a few weeks ago, when Waring and his wife were shov-eling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston. Hejoked about shipping the stuff to friends and family in warmer states, and an idea was born. [...]SummaryKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box \u2013 enoughfor 10 to 15 snowballs, he says. But not if you live in New England or surrounding states.Figure 10.2Excerpt from a sample article and its summary from the CNN/Daily Mail summarization corpus(Hermann et al.,2015), (Nallapati et al.,2016).If we take this full article and append the tokentl;dr, we can use this as the con-text to prime the generation process to produce a summary as illustrated in Fig.10.3.Again, what makes transformers able to succeed at this task (as compared, say, tothe primitive n-gram language model) is that attention can incorporate informationfrom the large context window, giving the model access to the original article as wellas to the newly generated text throughout the process.Which words do we generate at each step? One simple way to generate wordsis to always generate the most likely word given the context. Generating the mostlikely word given the context is calledgreedy decoding. A greedy algorithm is onegreedydecodingthat make a choice that is locally optimal, whether or not it will turn out to havebeen the best choice with hindsight. Thus in greedy decoding, at each time step ingeneration, the outputytis chosen by computing the probability for each possibleoutput (every word in the vocabulary) and then choosing the highest probabilityword (the argmax):\u02c6wt=argmaxw2VP(w|w<t)(10.1)In practice, however, we don\u2019t use greedy decoding with large language models.A major problem with greedy decoding is that because the words it chooses are",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 8,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "a sample article and its summary from the CNN/Daily Mail summarization corpus(Hermann et al.,2015), (Nallapati et al.,2016).If we take this full article and append the tokentl;dr, we can use this as the con-text to prime the generation process to produce a summary as illustrated in Fig.10.3.Again, what makes transformers able to succeed at this task (as compared, say, tothe primitive n-gram language model) is that attention can incorporate informationfrom the large context window, giving the model access to the original article as wellas to the newly generated text throughout the process.Which words do we generate at each step? One simple way to generate wordsis to always generate the most likely word given the context. Generating the mostlikely word given the context is calledgreedy decoding. A greedy algorithm is onegreedydecodingthat make a choice that is locally optimal, whether or not it will turn out to havebeen the best choice with hindsight. Thus in greedy decoding, at each time step ingeneration, the outputytis chosen by computing the probability for each possibleoutput (every word in the vocabulary) and then choosing the highest probabilityword (the argmax):\u02c6wt=argmaxw2VP(w|w<t)(10.1)In practice, however, we don\u2019t use greedy decoding with large language models.A major problem with greedy decoding is that because the words it chooses are (byde\ufb01nition) extremely predictable, the resulting text is generic and often quite repeti-tive. Indeed, greedy decoding is so predictable that it is deterministic; if the context4CHAPTER10\u2022LARGELANGUAGEMODELS\u2018too long; didn\u2019t read\u2019 and in recent years people often use this token, especially ininformal work emails, when they are going to give a short summary. Since this tokenis suf\ufb01ciently frequent in language model training data, language models have seenmany texts in which the token occurs before a summary, and hence will interpret thetoken as instructions to generate a summary. We can then do conditional generation:give the language model this pre\ufb01x, and then have it generate the following words,one by one, and take the entire response as a summary. Fig.10.2shows an exampleof a text and a human-produced summary from a widely-used summarization corpusconsisting of CNN and Daily Mirror news articles.Original ArticleThe only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuffand offering it for sale online? People are actually buying it. For $89, self-styled entrepreneurKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box \u2013 enoughfor 10 to 15 snowballs, he says.But not if you live in New England or surrounding states. \u201cWe will not ship snow to any statesin the northeast!\u201d says Waring\u2019s website, ShipSnowYo.com. \u201cWe\u2019re in the business of expungingsnow!\u201dHis website and social media accounts claim to have \ufb01lled more than 133 orders for snow \u2013 morethan 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set arecord this winter for the snowiest month in its history. Most residents see the huge piles of snowchoking their yards and sidewalks as a nuisance, but Waring saw an opportunity.According to Boston.com, it all started a few weeks ago, when Waring and his wife were shov-eling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston. Hejoked about shipping the stuff to friends and family in warmer states, and an idea was born. [...]SummaryKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box \u2013",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 9,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "Mirror news articles.Original ArticleThe only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuffand offering it for sale online? People are actually buying it. For $89, self-styled entrepreneurKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box \u2013 enoughfor 10 to 15 snowballs, he says.But not if you live in New England or surrounding states. \u201cWe will not ship snow to any statesin the northeast!\u201d says Waring\u2019s website, ShipSnowYo.com. \u201cWe\u2019re in the business of expungingsnow!\u201dHis website and social media accounts claim to have \ufb01lled more than 133 orders for snow \u2013 morethan 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set arecord this winter for the snowiest month in its history. Most residents see the huge piles of snowchoking their yards and sidewalks as a nuisance, but Waring saw an opportunity.According to Boston.com, it all started a few weeks ago, when Waring and his wife were shov-eling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston. Hejoked about shipping the stuff to friends and family in warmer states, and an idea was born. [...]SummaryKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box \u2013 enoughfor 10 to 15 snowballs, he says. But not if you live in New England or surrounding states.Figure 10.2Excerpt from a sample article and its summary from the CNN/Daily Mail summarization corpus(Hermann et al.,2015), (Nallapati et al.,2016).If we take this full article and append the tokentl;dr, we can use this as the con-text to prime the generation process to produce a summary as illustrated in Fig.10.3.Again, what makes transformers able to succeed at this task (as compared, say, tothe primitive n-gram language model) is that attention can incorporate informationfrom the large context window, giving the model access to the original article as wellas to the newly generated text throughout the process.Which words do we generate at each step? One simple way to generate wordsis to always generate the most likely word given the context. Generating the mostlikely word given the context is calledgreedy decoding. A greedy algorithm is onegreedydecodingthat make a choice that is locally optimal, whether or not it will turn out to havebeen the best choice with hindsight. Thus in greedy decoding, at each time step ingeneration, the outputytis chosen by computing the probability for each possibleoutput (every word in the vocabulary) and then choosing the highest probabilityword (the argmax):\u02c6wt=argmaxw2VP(w|w<t)(10.1)In practice, however, we don\u2019t use greedy decoding with large language models.A major problem with greedy decoding is that because the words it chooses are (byde\ufb01nition) extremely predictable, the resulting text is generic and often quite repeti-tive. Indeed, greedy decoding is so predictable that it is deterministic; if the contextOriginal",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 10,
      "token_count": 673,
      "chapter_title": ""
    }
  },
  {
    "content": "Summary",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 11,
      "token_count": 1,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\nLLMs for summarization (using  tl;dr)\nOriginal StoryGenerated Summary\n\u2026ideaKyle\nwasborn.KyleWaring\nWaringonlyThe\u2026will\nDelimiterwillUUU\ntl;drLM Head\nE\nE\nE\nE\nE\nE\nE\nE\u2026\n\n## Page 16\n\nLarge Language ModelsLarge Language Models: What tasks can they do?\n\n## Page 17\n\nLarge Language ModelsSampling for LLM Generation\n\n## Page 18\n\nDecoding and SamplingThis task of choosing a word to generate based on the model\u2019s probabilities is called decoding. The most common method for decoding in LLMs: sampling. Sampling from a model\u2019s distribution over words:\u2022choose random words according to their probability assigned by the model. After each token we\u2019ll sample words to generate according to their probability conditioned on our previous choices, \u2022A transformer language model will give the probability",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 12,
      "token_count": 189,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\nLarge Language ModelsLarge Language Models: What tasks can they do?\n\n## Page 17\n\nLarge Language ModelsSampling for LLM Generation\n\n## Page 18\n\nDecoding and SamplingThis task of choosing a word to generate based on the model\u2019s probabilities is called decoding. The most common method for decoding in LLMs: sampling. Sampling from a model\u2019s distribution over words:\u2022choose random words according to their probability assigned by the model. After each token we\u2019ll sample words to generate according to their probability conditioned on our previous choices, \u2022A transformer language model will give the probability\n\n## Page 19\n\nRandom sampling6CHAPTER10\u2022LARGELANGUAGEMODELSas de\ufb01ned by the model. Thus we are more likely to generate words that the modelthinks have a high probability in the context and less likely to generate words thatthe model thinks have a low probability.We saw back in Chapter 3 on page??how to generate text from a unigram lan-guage model , by repeatedly randomly sampling words according to their probabilityuntil we either reach a pre-determined length or select the end-of-sentence token. Togenerate text from a trained transformer language model we\u2019ll just generalize thismodel a bit: at each step we\u2019ll sample words according to their probabilitycondi-tioned on our previous choices, and we\u2019ll use a transformer language model as theprobability model that tells us this probability.We can formalize this algorithm for generating a sequence of wordsW=w1,w2,...,wNuntil we hit the end-of-sequence token, usingx\u21e0p(x)to mean \u2018choosexby sam-pling from the distributionp(x):i 1wi\u21e0p(w)whilewi!= EOSi i+1wi\u21e0p(wi|w<i)The algorithm above is calledrandom sampling, and it turns out random sam-randomsamplingpling doesn\u2019t work well enough. The problem is that even though random samplingis mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portionof the distribution that they get chosen often enough to result in generating weirdsentences. For this reason, instead of random sampling, we usually use samplingmethods that avoid generating the very unlikely words.The sampling methods we introduce below each have parameters that enabletrading off two important factors in generation:qualityanddiversity. Methodsthat emphasize the most probable words tend to produce generations that are ratedby people as more accurate, more coherent, and more factual, but also more boringand more repetitive. Methods that give a bit more weight to the middle-probabilitywords tend to be more creative and more diverse, but less factual and more likely tobe incoherent or otherwise low-quality.10.2.1 Top-ksamplingTop-k samplingis a simple generalization of greedy decoding. Instead of choosingtop-k samplingthe single most probable word to generate, we \ufb01rst truncate the distribution to thetopkmost likely words, renormalize to produce a legitimate probability distribution,and then randomly sample from within thesekwords according to their renormalizedprobabilities. More formally:1.Choose in advance a number of wordsk2.For each word in the vocabularyV, use the language model to compute thelikelihood of this word given the contextp(wt|w<t)3.Sort the words by their likelihood, and throw away any word that is not one ofthe topkmost probable words.4.Renormalize the scores of thekwords to be a legitimate probability distribu-tion.",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 13,
      "token_count": 767,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\nRandom sampling doesn't work very wellEven though random sampling mostly generate sensible, high-probable words, There are many odd, low- probability words in the tail of the distribution Each one is low- probability but added up they constitute a large portion of the distribution So they get picked enough to generate weird sentences\n\n## Page 21\n\nFactors in word sampling: quality and diversityEmphasize high-probability words  + quality: more  accurate, coherent, and factual, - diversity: boring, repetitive. Emphasize middle-probability words + diversity: more creative, diverse, - quality: less factual, incoherent\n\n## Page 22\n\nTop-k sampling:1. Choose # of words k 2. For each word in the vocabulary V , use the language model to compute the likelihood of this word given the context p(wt |w<t ) 3. Sort the words by likelihood, keep only the top k most probable words. 4. Renormalize the scores of the k words to be a legitimate probability distribution. 5. Randomly sample a word from within these remaining k most-probable words according to its probability.",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 14,
      "token_count": 241,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23\n\nTop-p sampling (= nucleus sampling)Problem with top-k:  k is fixed so may cover very different amounts of probability mass in different situationsIdea: Instead, keep the top p percent of the probability massGiven a distribution P(wt |w<t ), the top-p vocabulary V ( p) is the smallest set of words such that Holtzman et al., 2020 10.2\u2022SAMPLING FORLLM GENERATION75.Randomly sample a word from within these remainingkmost-probable wordsaccording to its probability.Whenk=1, top-ksampling is identical to greedy decoding. Settingkto a largernumber than 1 leads us to sometimes select a word which is not necessarily the mostprobable, but is still probable enough, and whose choice results in generating morediverse but still high-enough-quality text.10.2.2 Nucleus or top-psamplingOne problem with top-ksampling is thatkis \ufb01xed, but the shape of the probabilitydistribution over words differs in different contexts. If we setk=10, sometimesthe top 10 words will be very likely and include most of the probability mass, butother times the probability distribution will be \ufb02atter and the top 10 words will onlyinclude a small part of the probability mass.An alternative, calledtop-p samplingornucleus sampling(Holtzman et al.,top-p sampling2020), is to keep not the topkwords, but the topppercent of the probability mass.The goal is the same; to truncate the distribution to remove the very unlikely words.But by measuring probability rather than the number of words, the hope is that themeasure will be more robust in very different contexts, dynamically increasing anddecreasing the pool of word candidates.Given a distributionP(wt|w<t), the top-pvocabularyV(p)is the smallest set ofwords such thatXw2V(p)P(w|w<t)\u0000p.(10.2)10.2.3 Temperature samplingIntemperature sampling, we don\u2019t truncate the distribution, but instead reshapetemperaturesamplingit. The intuition for temperature sampling comes from thermodynamics, where asystem at a high temperature is very \ufb02exible and can explore many possible states,while a system at a lower temperature is likely to explore a subset of lower energy(better) states. In low-temperature sampling, we smoothly increase the probabilityof the most probable words and decrease the probability of the rare words.We implement this intuition by simply dividing the logit by a temperature param-etertbefore we normalize it by passing it through the softmax. In low-temperaturesampling,t2(0,1]. Thus instead of computing the probability distribution over thevocabulary directly from the logit as in the following (repeated from (??)):y=softmax(u)(10.3)we instead \ufb01rst divide the logits byt, computing the probability vectoryasy=softmax(u/t)(10.4)Why does this work? Whentis close to 1 the distribution doesn\u2019t change much.But the lowertis, the larger the scores being passed to the softmax (dividing by asmaller fractiont\uf8ff1 results in making each score larger). Recall that one of theuseful properties of a softmax is that it tends to push high values toward 1 and lowvalues toward 0. Thus when larger numbers are passed to a softmax the result isa distribution with increased probabilities of the most high-probability words anddecreased probabilities of the low probability words, making the distribution moregreedy. Astapproaches 0 the probability of the most likely word approaches 1.",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 15,
      "token_count": 759,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24\n\nTemperature samplingReshape the distribution instead of truncating itIntuition from thermodynamics, \u2022a system at high temperature is flexible and can explore many possible states,\u2022a system at lower temperature is likely to explore a subset of lower energy (better) states. In low-temperature sampling,  (\u03c4 \u2264 1) we smoothly\u2022increase the probability of the most probable words\u2022decrease the probability of the rare words.",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 16,
      "token_count": 91,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 25",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 17,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Temperature samplingDivide the logit by a temperature parameter \u03c4 before passing it through the softmax.Instead ofWe do  10.2\u2022SAMPLING FORLLM GENERATION75.Randomly sample a word from within these remainingkmost-probable wordsaccording to its probability.Whenk=1, top-ksampling is identical to greedy decoding. Settingkto a largernumber than 1 leads us to sometimes select a word which is not necessarily the mostprobable, but is still probable enough, and whose choice results in generating morediverse but still high-enough-quality text.10.2.2 Nucleus or top-psamplingOne problem with top-ksampling is thatkis \ufb01xed, but the shape of the probabilitydistribution over words differs in different contexts. If we setk=10, sometimesthe top 10 words will be very likely and include most of the probability mass, butother times the probability distribution will be \ufb02atter and the top 10 words will onlyinclude a small part of the probability mass.An alternative, calledtop-p samplingornucleus sampling(Holtzman et al.,top-p sampling2020), is to keep not the topkwords, but the topppercent of the probability mass.The goal is the same; to truncate the distribution to remove the very unlikely words.But by measuring probability rather than the number of words, the hope is that themeasure will be more robust in very different contexts, dynamically increasing anddecreasing the pool of word candidates.Given a distributionP(wt|w<t), the top-pvocabularyV(p)is the smallest set ofwords such thatXw2V(p)P(w|w<t)\u0000p.(10.2)10.2.3 Temperature samplingIntemperature sampling, we don\u2019t truncate the distribution, but instead reshapetemperaturesamplingit. The intuition for temperature sampling comes from thermodynamics, where asystem at a high temperature is very \ufb02exible and can explore many possible states,while a system at a lower temperature is likely to explore a subset of lower energy(better) states. In low-temperature sampling, we smoothly increase the probabilityof the most probable words and decrease the probability of the rare words.We implement this intuition by simply dividing the logit by a temperature param-etertbefore we normalize it by passing it through the softmax. In low-temperaturesampling,t2(0,1]. Thus instead of computing the probability distribution over thevocabulary directly from the logit as in the following (repeated from (??)):y=softmax(u)(10.3)we instead \ufb01rst divide the logits byt, computing the probability vectoryasy=softmax(u/t)(10.4)Why does this work? Whentis close to 1 the distribution doesn\u2019t change much.But the lowertis, the larger the scores being passed to the softmax (dividing by asmaller fractiont\uf8ff1 results in making each score larger). Recall that one of theuseful properties of a softmax is that it tends to push high values toward 1 and lowvalues toward 0. Thus when larger numbers are passed to a softmax the result isa distribution with increased probabilities of the most high-probability words anddecreased probabilities of the low probability words, making the distribution moregreedy. Astapproaches 0 the probability of the most likely word approaches 1.10.2\u2022SAMPLING FORLLM GENERATION75.Randomly sample a word from within these remainingkmost-probable wordsaccording to its probability.Whenk=1, top-ksampling is identical to greedy decoding. Settingkto a largernumber than 1 leads us to sometimes select a word which is not necessarily the mostprobable, but is still probable enough, and whose choice results in generating morediverse but still high-enough-quality",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 18,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "Thus instead of computing the probability distribution over thevocabulary directly from the logit as in the following (repeated from (??)):y=softmax(u)(10.3)we instead \ufb01rst divide the logits byt, computing the probability vectoryasy=softmax(u/t)(10.4)Why does this work? Whentis close to 1 the distribution doesn\u2019t change much.But the lowertis, the larger the scores being passed to the softmax (dividing by asmaller fractiont\uf8ff1 results in making each score larger). Recall that one of theuseful properties of a softmax is that it tends to push high values toward 1 and lowvalues toward 0. Thus when larger numbers are passed to a softmax the result isa distribution with increased probabilities of the most high-probability words anddecreased probabilities of the low probability words, making the distribution moregreedy. Astapproaches 0 the probability of the most likely word approaches 1.10.2\u2022SAMPLING FORLLM GENERATION75.Randomly sample a word from within these remainingkmost-probable wordsaccording to its probability.Whenk=1, top-ksampling is identical to greedy decoding. Settingkto a largernumber than 1 leads us to sometimes select a word which is not necessarily the mostprobable, but is still probable enough, and whose choice results in generating morediverse but still high-enough-quality text.10.2.2 Nucleus or top-psamplingOne problem with top-ksampling is thatkis \ufb01xed, but the shape of the probabilitydistribution over words differs in different contexts. If we setk=10, sometimesthe top 10 words will be very likely and include most of the probability mass, butother times the probability distribution will be \ufb02atter and the top 10 words will onlyinclude a small part of the probability mass.An alternative, calledtop-p samplingornucleus sampling(Holtzman et al.,top-p sampling2020), is to keep not the topkwords, but the topppercent of the probability mass.The goal is the same; to truncate the distribution to remove the very unlikely words.But by measuring probability rather than the number of words, the hope is that themeasure will be more robust in very different contexts, dynamically increasing anddecreasing the pool of word candidates.Given a distributionP(wt|w<t), the top-pvocabularyV(p)is the smallest set ofwords such thatXw2V(p)P(w|w<t)\u0000p.(10.2)10.2.3 Temperature samplingIntemperature sampling, we don\u2019t truncate the distribution, but instead reshapetemperaturesamplingit. The intuition for temperature sampling comes from thermodynamics, where asystem at a high temperature is very \ufb02exible and can explore many possible states,while a system at a lower temperature is likely to explore a subset of lower energy(better) states. In low-temperature sampling, we smoothly increase the probabilityof the most probable words and decrease the probability of the rare words.We implement this intuition by simply dividing the logit by a temperature param-etertbefore we normalize it by passing it through the softmax. In low-temperaturesampling,t2(0,1]. Thus instead of computing the probability distribution over thevocabulary directly from the logit as in the following (repeated from (??)):y=softmax(u)(10.3)we instead \ufb01rst divide the logits byt, computing the probability vectoryasy=softmax(u/t)(10.4)Why does this work? Whentis close to 1 the distribution doesn\u2019t change much.But the lowertis, the larger the scores being passed to the softmax (dividing by asmaller fractiont\uf8ff1 results in making each score larger). Recall that one of theuseful properties of",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 19,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "the smallest set ofwords such thatXw2V(p)P(w|w<t)\u0000p.(10.2)10.2.3 Temperature samplingIntemperature sampling, we don\u2019t truncate the distribution, but instead reshapetemperaturesamplingit. The intuition for temperature sampling comes from thermodynamics, where asystem at a high temperature is very \ufb02exible and can explore many possible states,while a system at a lower temperature is likely to explore a subset of lower energy(better) states. In low-temperature sampling, we smoothly increase the probabilityof the most probable words and decrease the probability of the rare words.We implement this intuition by simply dividing the logit by a temperature param-etertbefore we normalize it by passing it through the softmax. In low-temperaturesampling,t2(0,1]. Thus instead of computing the probability distribution over thevocabulary directly from the logit as in the following (repeated from (??)):y=softmax(u)(10.3)we instead \ufb01rst divide the logits byt, computing the probability vectoryasy=softmax(u/t)(10.4)Why does this work? Whentis close to 1 the distribution doesn\u2019t change much.But the lowertis, the larger the scores being passed to the softmax (dividing by asmaller fractiont\uf8ff1 results in making each score larger). Recall that one of theuseful properties of a softmax is that it tends to push high values toward 1 and lowvalues toward 0. Thus when larger numbers are passed to a softmax the result isa distribution with increased probabilities of the most high-probability words anddecreased probabilities of the low probability words, making the distribution moregreedy. Astapproaches 0 the probability of the most likely word approaches 1.",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 20,
      "token_count": 373,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 26\n\nTemperature samplingWhy does this work?\u2022When \u03c4 is close to 1 the distribution doesn\u2019t change much. \u2022The lower \u03c4 is, the larger the scores being passed to the softmax\u2022Softmax pushes high values toward 1 and low values toward 0. \u2022Large inputs pushes high-probability words higher and low probability word lower,  making the distribution more greedy. \u2022As \u03c4 approaches 0, the probability of most likely word approaches 1 10.2\u2022SAMPLING FORLLM GENERATION75.Randomly sample a word from within these remainingkmost-probable wordsaccording to its probability.Whenk=1, top-ksampling is identical to greedy decoding. Settingkto a largernumber than 1 leads us to sometimes select a word which is not necessarily the mostprobable, but is still probable enough, and whose choice results in generating morediverse but still high-enough-quality text.10.2.2 Nucleus or top-psamplingOne problem with top-ksampling is thatkis \ufb01xed, but the shape of the probabilitydistribution over words differs in different contexts. If we setk=10, sometimesthe top 10 words will be very likely and include most of the probability mass, butother times the probability distribution will be \ufb02atter and the top 10 words will onlyinclude a small part of the probability mass.An alternative, calledtop-p samplingornucleus sampling(Holtzman et al.,top-p sampling2020), is to keep not the topkwords, but the topppercent of the probability mass.The goal is the same; to truncate the distribution to remove the very unlikely words.But by measuring probability rather than the number of words, the hope is that themeasure will be more robust in very different contexts, dynamically increasing anddecreasing the pool of word candidates.Given a distributionP(wt|w<t), the top-pvocabularyV(p)is the smallest set ofwords such thatXw2V(p)P(w|w<t)\u0000p.(10.2)10.2.3 Temperature samplingIntemperature sampling, we don\u2019t truncate the distribution, but instead reshapetemperaturesamplingit. The intuition for temperature sampling comes from thermodynamics, where asystem at a high temperature is very \ufb02exible and can explore many possible states,while a system at a lower temperature is likely to explore a subset of lower energy(better) states. In low-temperature sampling, we smoothly increase the probabilityof the most probable words and decrease the probability of the rare words.We implement this intuition by simply dividing the logit by a temperature param-etertbefore we normalize it by passing it through the softmax. In low-temperaturesampling,t2(0,1]. Thus instead of computing the probability distribution over thevocabulary directly from the logit as in the following (repeated from (??)):y=softmax(u)(10.3)we instead \ufb01rst divide the logits byt, computing the probability vectoryasy=softmax(u/t)(10.4)Why does this work? Whentis close to 1 the distribution doesn\u2019t change much.But the lowertis, the larger the scores being passed to the softmax (dividing by asmaller fractiont\uf8ff1 results in making each score larger). Recall that one of theuseful properties of a softmax is that it tends to push high values toward 1 and lowvalues toward 0. Thus when larger numbers are passed to a softmax the result isa distribution with increased probabilities of the most high-probability words anddecreased probabilities of the low probability words, making the distribution moregreedy. Astapproaches 0 the probability of the most likely word approaches 1.0 \u2264 \u03c4 \u2264 1 \n\n## Page 27\n\nLarge Language ModelsSampling for LLM Generation",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 21,
      "token_count": 795,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 27\n\nLarge Language ModelsSampling for LLM Generation\n\n## Page 28\n\nLarge Language ModelsPretraining Large Language Models: Algorithm\n\n## Page 29\n\nPretrainingThe big idea that underlies all the amazing performance of language modelsFirst pretrain a transformer model on enormous amounts of textThen apply it to new tasks.\n\n## Page 30\n\nSelf-supervised training algorithmWe just train them to predict the next word!1.Take a corpus of text 2.At each time step t i.ask the model to predict the next word ii.train the model using gradient descent to minimize the error in this prediction\"Self-supervised\" because it just uses the next word as the label!\n\n## Page 31\n\nIntuition of language model training: loss\u2022Same loss function: cross-entropy loss\u2022We want the model to assign a high probability to true word w\u2022= want loss to be high if the model assigns too low a probability to w\u2022CE Loss: The negative log probability that the model assigns to the true next word w\u2022If the model assigns too low a probability to w\u2022We move the model weights in the direction that assigns a higher probability to w",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 22,
      "token_count": 238,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 32",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 23,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Cross-entropy loss for language modelingCE loss: difference between the correct probability distribution and the predicted distribution The correct distribution yt knows the next word, so is 1 for the actual next word and 0 for the others.So in this sum, all terms get multiplied by zero except one: the logp the model assigns to the correct next word, so: 8CHAPTER10\u2022LARGELANGUAGEMODELSNote, by the way, that there can be other situations where we may want to dosomething quite different and \ufb02atten the word probability distribution instead ofmaking it greedy. Temperature sampling can help with this situation too, in this casehigh-temperaturesampling, in which case we uset>1.10.3 Pretraining Large Language ModelsHow do we teach a transformer to be a language model? What is the algorithm andwhat data do we train on?10.3.1 Self-supervised training algorithmTo train a transformer as a language model, we use the sameself-supervision(orself-supervisionself-training) algorithm we saw in Section??: we take a corpus of text as trainingmaterial and at each time steptask the model to predict the next word. We callsuch a model self-supervised because we don\u2019t have to add any special gold labelsto the data; the natural sequence of words is its own supervision! We simply train themodel to minimize the error in predicting the true next word in the training sequence,using cross-entropy as the loss function.Recall that the cross-entropy loss measures the difference between a predictedprobability distribution and the correct distribution.LCE=\u0000Xw2Vyt[w]log\u02c6yt[w](10.5)In the case of language modeling, the correct distributionytcomes from knowing thenext word. This is represented as a one-hot vector corresponding to the vocabularywhere the entry for the actual next word is 1, and all the other entries are 0. Thus,the cross-entropy loss for language modeling is determined by the probability themodel assigns to the correct next word (all other words get multiplied by zero). Soat timetthe CE loss in (10.5) can be simpli\ufb01ed as the negative log probability themodel assigns to the next word in the training sequence.LCE(\u02c6yt,yt)=\u0000log\u02c6yt[wt+1](10.6)Thus at each word positiontof the input, the model takes as input the correct se-quence of tokensw1:t, and uses them to compute a probability distribution overpossible next words so as to compute the model\u2019s loss for the next tokenwt+1. Thenwe move to the next word, we ignore what the model predicted for the next wordand instead use the correct sequence of tokensw1:t+1to estimate the probability oftokenwt+2. This idea that we always give the model the correct history sequence topredict the next word (rather than feeding the model its best case from the previoustime step) is calledteacher forcing.teacher forcingFig.10.4illustrates the general training approach. At each step, given all thepreceding words, the \ufb01nal transformer layer produces an output distribution overthe entire vocabulary. During training, the probability assigned to the correct wordis used to calculate the cross-entropy loss for each item in the sequence. The lossfor a training sequence is the average cross-entropy loss over the entire sequence.The weights in the network are adjusted to minimize the average CE loss over thetraining sequence via gradient descent.8CHAPTER10\u2022LARGELANGUAGEMODELSNote, by the way, that there can be other situations where we may want to dosomething quite different and \ufb02atten the word probability distribution instead ofmaking it greedy. Temperature sampling can help with this situation too, in this casehigh-temperaturesampling, in which",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 24,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "the input, the model takes as input the correct se-quence of tokensw1:t, and uses them to compute a probability distribution overpossible next words so as to compute the model\u2019s loss for the next tokenwt+1. Thenwe move to the next word, we ignore what the model predicted for the next wordand instead use the correct sequence of tokensw1:t+1to estimate the probability oftokenwt+2. This idea that we always give the model the correct history sequence topredict the next word (rather than feeding the model its best case from the previoustime step) is calledteacher forcing.teacher forcingFig.10.4illustrates the general training approach. At each step, given all thepreceding words, the \ufb01nal transformer layer produces an output distribution overthe entire vocabulary. During training, the probability assigned to the correct wordis used to calculate the cross-entropy loss for each item in the sequence. The lossfor a training sequence is the average cross-entropy loss over the entire sequence.The weights in the network are adjusted to minimize the average CE loss over thetraining sequence via gradient descent.8CHAPTER10\u2022LARGELANGUAGEMODELSNote, by the way, that there can be other situations where we may want to dosomething quite different and \ufb02atten the word probability distribution instead ofmaking it greedy. Temperature sampling can help with this situation too, in this casehigh-temperaturesampling, in which case we uset>1.10.3 Pretraining Large Language ModelsHow do we teach a transformer to be a language model? What is the algorithm andwhat data do we train on?10.3.1 Self-supervised training algorithmTo train a transformer as a language model, we use the sameself-supervision(orself-supervisionself-training) algorithm we saw in Section??: we take a corpus of text as trainingmaterial and at each time steptask the model to predict the next word. We callsuch a model self-supervised because we don\u2019t have to add any special gold labelsto the data; the natural sequence of words is its own supervision! We simply train themodel to minimize the error in predicting the true next word in the training sequence,using cross-entropy as the loss function.Recall that the cross-entropy loss measures the difference between a predictedprobability distribution and the correct distribution.LCE=\u0000Xw2Vyt[w]log\u02c6yt[w](10.5)In the case of language modeling, the correct distributionytcomes from knowing thenext word. This is represented as a one-hot vector corresponding to the vocabularywhere the entry for the actual next word is 1, and all the other entries are 0. Thus,the cross-entropy loss for language modeling is determined by the probability themodel assigns to the correct next word (all other words get multiplied by zero). Soat timetthe CE loss in (10.5) can be simpli\ufb01ed as the negative log probability themodel assigns to the next word in the training sequence.LCE(\u02c6yt,yt)=\u0000log\u02c6yt[wt+1](10.6)Thus at each word positiontof the input, the model takes as input the correct se-quence of tokensw1:t, and uses them to compute a probability distribution overpossible next words so as to compute the model\u2019s loss for the next tokenwt+1. Thenwe move to the next word, we ignore what the model predicted for the next wordand instead use the correct sequence of tokensw1:t+1to estimate the probability oftokenwt+2. This idea that we always give the model the correct history sequence topredict the next word (rather than feeding the model its best case from the previoustime step) is calledteacher forcing.teacher forcingFig.10.4illustrates the general training approach. At each step,",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 25,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "the case of language modeling, the correct distributionytcomes from knowing thenext word. This is represented as a one-hot vector corresponding to the vocabularywhere the entry for the actual next word is 1, and all the other entries are 0. Thus,the cross-entropy loss for language modeling is determined by the probability themodel assigns to the correct next word (all other words get multiplied by zero). Soat timetthe CE loss in (10.5) can be simpli\ufb01ed as the negative log probability themodel assigns to the next word in the training sequence.LCE(\u02c6yt,yt)=\u0000log\u02c6yt[wt+1](10.6)Thus at each word positiontof the input, the model takes as input the correct se-quence of tokensw1:t, and uses them to compute a probability distribution overpossible next words so as to compute the model\u2019s loss for the next tokenwt+1. Thenwe move to the next word, we ignore what the model predicted for the next wordand instead use the correct sequence of tokensw1:t+1to estimate the probability oftokenwt+2. This idea that we always give the model the correct history sequence topredict the next word (rather than feeding the model its best case from the previoustime step) is calledteacher forcing.teacher forcingFig.10.4illustrates the general training approach. At each step, given all thepreceding words, the \ufb01nal transformer layer produces an output distribution overthe entire vocabulary. During training, the probability assigned to the correct wordis used to calculate the cross-entropy loss for each item in the sequence. The lossfor a training sequence is the average cross-entropy loss over the entire sequence.The weights in the network are adjusted to minimize the average CE loss over thetraining sequence via gradient descent.",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 26,
      "token_count": 377,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 33\n\nTeacher forcing\u2022At each token position t, model sees correct tokens w1:t, \u2022Computes  loss (\u2013log probability) for the next token wt+1 \u2022At next token position t+1 we ignore what model predicted for wt+1 \u2022Instead we take the correct word wt+1, add it to context, move on",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 27,
      "token_count": 74,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 34",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 28,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Training a transformer language model\nlongandthanksforNext tokenallLoss\u2026=\n<latexit sha1_base64=\"AovqpaL476UmJ1EU1xZPgDZ70tQ=\">AAAB9nicbVDLSsNAFL2pr1pfURcu3AwWwY0lEakui25cVrAPaEqYTCbt0EkmzEzEEvIrbkTcKPgZ/oJ/Y9Jm09YDA4dzznDvPV7MmdKW9WtU1tY3Nreq27Wd3b39A/PwqKtEIgntEMGF7HtYUc4i2tFMc9qPJcWhx2nPm9wXfu+ZSsVE9KSnMR2GeBSxgBGsc8k1Ty4dLkZo6qZOiPVYhimO/CyruWbdalgzoFVil6QOJdqu+eP4giQhjTThWKmBbcV6mGKpGeE0qzmJojEmEzyi6WztDJ3nko8CIfMXaTRTF3I4VGoaenmy2E0te4X4nzdIdHA7TFkUJ5pGZD4oSDjSAhUdIJ9JSjSf5gQTyfINERljiYnOmypOt5cPXSXdq4bdbDQfr+utu7KEKpzCGVyADTfQggdoQwcIZPAGn/BlvBivxrvxMY9WjPLPMSzA+P4DPEiSHA==</latexit>\u0000logyand\nStackedTransformerBlocksSolongandthanksfor\u2026\u2026\n\u2026\nU\nInput tokensx1x2LanguageModelingHeadx3x4x5InputEncoding\nE1+\nE2+\nE3+\nE4+\nE5+\u2026\u2026\u2026\u2026\u2026\nU\nU\nU\nU\u2026logitslogitslogitslogitslogits\u2026",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 29,
      "token_count": 455,
      "chapter_title": ""
    }
  },
  {
    "content": "StackedTransformerBlocksSolongandthanksfor\u2026\u2026\n\u2026\nU\nInput tokensx1x2LanguageModelingHeadx3x4x5InputEncoding\nE1+\nE2+\nE3+\nE4+\nE5+\u2026\u2026\u2026\u2026\u2026\nU\nU\nU\nU\u2026logitslogitslogitslogitslogits\u2026\n<latexit sha1_base64=\"q3ZgXDyG7qtkT7t8hT47RdlwYG4=\">AAAB+XicbVDLSsNAFJ3UV62vWHe6GVsEN5bERXUlBUVcVrAPaEqYTCft0MlMmJkIIQT8AT/CTRE3Cv6Ev+DfmLTdtPXAwOGcM9x7jxcyqrRl/RqFtfWNza3idmlnd2//wDwst5WIJCYtLJiQXQ8pwignLU01I91QEhR4jHS88W3ud56JVFTwJx2HpB+gIac+xUhnkmseXzhMDGHsJk6A9EgGiR4hPlZpWnLNqlWzpoCrxJ6TauP0tXw3qdw0XfPHGQgcBYRrzJBSPdsKdT9BUlPMSFpyIkVChMdoSJLp5ik8y6QB9IXMHtdwqi7kUKBUHHhZMl9PLXu5+J/Xi7R/3U8oDyNNOJ4N8iMGtYB5DXBAJcGaxRlBWNJsQ4hHSCKss7Ly0+3lQ1dJ+7Jm12v1x6yDezBDEZyACjgHNrgCDfAAmqAFMHgBE/AJvozEeDPejY9ZtGDM/xyBBRjff79pldo=</latexit>\u0000logythanks",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 30,
      "token_count": 447,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 35\n\nLarge Language ModelsPretraining Large Language Models: Algorithm\n\n## Page 36\n\nLarge Language ModelsPretraining data for LLMs\n\n## Page 37\n\nLLMs are mainly trained on the webCommon crawl, snapshots of the entire web produced by the non- profit Common Crawl with billions of pagesColossal Clean Crawled Corpus (C4; Raffel et al. 2020), 156 billion tokens of English,  filtered What's in it? Mostly patent text documents, Wikipedia, and news sites \n\n## Page 38\n\nThe Pile: a pretraining corpus\nFigure 1: Treemap of Pile components by effective size.troduce a new \ufb01ltered subset of Common Crawl,Pile-CC, with improved extraction quality.Through our analyses, we con\ufb01rm that the Pile issigni\ufb01cantly distinct from pure Common Crawldata. Additionally, our evaluations show that theexisting GPT-2 and GPT-3 models perform poorlyon many components of the Pile, and that modelstrained on the Pile signi\ufb01cantly outperform bothraw and \ufb01ltered Common Crawl models. To com-plement the performance evaluations, we also per-form an exploratory analysis of the text within thePile to provide a detailed picture of the data. Wehope that our extensive documentation of the con-struction and characteristics of the Pile will helpresearchers make informed decisions about poten-tial downstream applications.Finally, we make publicly available the preprocess-ing code for the constituent datasets of the Pile andthe code for constructing alternative versions2. Inthe interest of reproducibility, we also documentall processing performed on each dataset (and thePile as a whole) in as much detail as possible. Forfurther details about the processing of each dataset,see Section2and AppendixC.2https://github.com/EleutherAI/the-pile1.1 ContributionsThe core contributions of this paper are:1.The introduction of a825.18GiB english-language dataset for language modeling com-bining 22 diverse sources.2.The introduction of14new language model-ing datasets, which we expect to be of inde-pendent interest to researchers.3.Evaluations demonstrating signi\ufb01cant im-provements across many domains by GPT-2-sized models trained on this new dataset, com-pared to training on CC-100 and raw CommonCrawl.4.The investigation and documentation of thisdataset, which we hope will better inform re-searchers about how to use it as well as moti-vate them to undertake similar investigationsof their own data.2 The Pile DatasetsThe Pile is composed of 22 constituent sub-datasets,as shown in Table1. FollowingBrown et al.(2020),we increase the weights of higher quality compo-nents, with certain high-quality datasets such asWikipedia being seen up to 3 times (\u201cepochs\u201d) for2webacademicsbooks\ndialog\n\n## Page 39\n\nFiltering for quality and safetyQuality is subjective\u2022Many LLMs attempt to match Wikipedia, books, particular websites\u2022Need to remove boilerplate, adult content\u2022Deduplication at many levels (URLs, documents, even lines)Safety also subjective\u2022Toxicity detection is important, although that has mixed results\u2022Can mistakenly flag data written in dialects like African American English\n\n## Page 40\n\nWhat does a model learn from pretraining?\u2022There are canines everywhere! One dog in the front room, and two dogs\u2022It wasn't just big it was enormous\u2022The author of \"A Room of One's Own\" is Virginia Woolf\u2022The doctor told me that he\u2022The square root of 4 is 2",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 31,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 39\n\nFiltering for quality and safetyQuality is subjective\u2022Many LLMs attempt to match Wikipedia, books, particular websites\u2022Need to remove boilerplate, adult content\u2022Deduplication at many levels (URLs, documents, even lines)Safety also subjective\u2022Toxicity detection is important, although that has mixed results\u2022Can mistakenly flag data written in dialects like African American English\n\n## Page 40\n\nWhat does a model learn from pretraining?\u2022There are canines everywhere! One dog in the front room, and two dogs\u2022It wasn't just big it was enormous\u2022The author of \"A Room of One's Own\" is Virginia Woolf\u2022The doctor told me that he\u2022The square root of 4 is 2\n\n## Page 41\n\nBig ideaText contains enormous amounts of knowledgePretraining on lots of text with all that knowledge is what gives language models their ability to do so much\n\n## Page 42\n\nBut there are problems with scraping from the webCopyright: much of the text in these datasets is copyrighted\u2022Not clear if fair use doctrine in US allows for this use\u2022This remains an open legal questionData consent\u2022Website owners can indicate they don't want their site crawledPrivacy: \u2022Websites can contain private IP addresses and phone numbers\n\n## Page 43\n\nLarge Language ModelsPretraining data for LLMs\n\n## Page 44\n\nLarge Language ModelsFinetuning\n\n## Page 45\n\nFinetuning for daptation to new domainsWhat happens if we need our LLM to work well on a domain it didn't see in pretraining?Perhaps some specific medical or legal domain?Or maybe a multilingual LM needs to see more data on some language that was rare in pretraining?\n\n## Page 46\n\nFinetuningFine-tuning Data\nPretraining DataPretraining\n\u2026\n\u2026\n\u2026Fine-tuning\n\u2026\n\u2026\n\u2026Pretrained LMFine-tuned LM\n\n## Page 47\n\n\"Finetuning\" means 4 different thingsWe'll discuss 1 here, and 3 in later lecturesIn all four cases, finetuning means:taking a pretrained model and further adapting some or all of its parameters to some new data\n\n## Page 48\n\n1. Finetuning as \"continued pretraining\" on new data\u2022Further train all the parameters of model on new data\u2022using the same method (word prediction) and loss function (cross-entropy loss) as for pretraining.\u2022as if the new data were at the tail end of the pretraining data\u2022Hence sometimes called continued pretraining\n\n## Page 49\n\nLarge Language ModelsFinetuning\n\n## Page 50\n\nLarge Language ModelsEvaluating Large Language Models",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 32,
      "token_count": 557,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 51",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 33,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "PerplexityJust as for n-gram grammars, we use perplexity to measure how well the LM predicts unseen textThe perplexity of a model \u03b8 on an unseen test set is the inverse probability that \u03b8 assigns to the test set, normalized by the test set length. For a test set of n tokens w1:n the perplexity is :12CHAPTER10\u2022LARGELANGUAGEMODELSthe pretraining data, and so you\u2019ll sometimes see this method calledcontinued pre-training.continuedpretrainingRetraining all the parameters of the model is very slow and expensive when thelanguage model is huge. So instead we canfreezesome of the parameters (i.e., leavefreezethem unchanged from their pretrained value) and train only a subset of parameterson the new data. In Section10.5.3we\u2019ll describe this second variety of \ufb01netun-ing, calledparameter-ef\ufb01cient \ufb01netuning, orPEFT. because we ef\ufb01ciently selectspeci\ufb01c parameters to update when \ufb01netuning, and leave the rest in their pretrainedvalues.In Chapter 11 we\u2019ll introduce a third kind of \ufb01netuning, also parameter-ef\ufb01cient.In this version, the goal is to use a language model as a kind of classi\ufb01er or labelerfor a speci\ufb01c task. For example we might train the model to be a sentiment classi\ufb01er.We do this by adding extra neural circuitry (an extrahead) after the top layer of themodel. This classi\ufb01cation head takes as input some of the top layer embeddings ofthe transformer and produces as output a classi\ufb01cation. In this method, most com-monly used with masked language models like BERT, we freeze the entire pretrainedmodel and only train the classi\ufb01cation head on some new data, usually labeled withsome class that we want to predict.Finally, in Chapter 12 we\u2019ll introduce a fourth kind of \ufb01netuning, that is a cru-cial component of the largest language models:supervised \ufb01netuningorSFT. SFTis often used forinstruction \ufb01netuning, in which we want a pretrained languagemodel to learn to follow text instructions, for example to answer questions or followa command to write something. Here we create a dataset of prompts and desiredresponses (for example questions and their answers, or commands and their ful-\ufb01llments), and we train the language model using the normal cross-entropy loss topredict each token in the instruction prompt iteratively, essentially training it to pro-duce the desired response from the command in the prompt. It\u2019s called supervisedbecause unlike in pretraining, where we just take any data and predict the words init, we build the special \ufb01netuning dataset by hand, creating supervised responses toeach command.Often everything that happens after pretraining is lumped together aspost-training;we\u2019ll discuss the various parts of post-training in Chapter 12 and Chapter 13.10.4 Evaluating Large Language ModelsPerplexityAs we \ufb01rst saw in Chapter 3, one way to evaluate language models isto measure how well they predict unseen text. Intuitively, good models are those thatassign higher probabilities to unseen data (are less surprised when encountering thenew words).We instantiate this intuition by usingperplexityto measure the quality of aperplexitylanguage model. Recall from page??that the perplexity of a modelqon an unseentest set is the inverse probability thatqassigns to the test set, normalized by the testset length. For a test set ofntokensw1:n, the perplexity",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 34,
      "token_count": 769,
      "chapter_title": ""
    }
  },
  {
    "content": "learn to follow text instructions, for example to answer questions or followa command to write something. Here we create a dataset of prompts and desiredresponses (for example questions and their answers, or commands and their ful-\ufb01llments), and we train the language model using the normal cross-entropy loss topredict each token in the instruction prompt iteratively, essentially training it to pro-duce the desired response from the command in the prompt. It\u2019s called supervisedbecause unlike in pretraining, where we just take any data and predict the words init, we build the special \ufb01netuning dataset by hand, creating supervised responses toeach command.Often everything that happens after pretraining is lumped together aspost-training;we\u2019ll discuss the various parts of post-training in Chapter 12 and Chapter 13.10.4 Evaluating Large Language ModelsPerplexityAs we \ufb01rst saw in Chapter 3, one way to evaluate language models isto measure how well they predict unseen text. Intuitively, good models are those thatassign higher probabilities to unseen data (are less surprised when encountering thenew words).We instantiate this intuition by usingperplexityto measure the quality of aperplexitylanguage model. Recall from page??that the perplexity of a modelqon an unseentest set is the inverse probability thatqassigns to the test set, normalized by the testset length. For a test set ofntokensw1:n, the perplexity isPerplexityq(w1:n)=Pq(w1:n)\u00001n=ns1Pq(w1:n)(10.7)To visualize how perplexity can be computed as a function of the probabilities the",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 35,
      "token_count": 346,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 52\n\n\u2022Probability depends on size of test set\u2022Probability gets smaller the longer the text\u2022Better: a metric that is per-word, normalized by length\u2022Perplexity is the inverse probability of the test set, normalized by the number of words(The inverse comes from the original definition of perplexity from cross-entropy rate in information theory)Probability range is  [0,1], perplexity range is [1,\u221e]Why perplexity instead of raw probability of the test set?\n\n## Page 53\n\nPerplexity\u2022The higher the probability of the word sequence, the lower the perplexity.\u2022Thus the lower the perplexity of a model on the data, the better the model. \u2022Minimizing perplexity is the same as maximizing probabilityAlso: perplexity is sensitive to length/tokenization so best used when comparing LMs that use the same tokenizer.  \n\n## Page 54\n\nMany other factors that we evaluate, like:Size Big models take lots of GPUs and time to train, memory to storeEnergy usageCan measure kWh or kilograms of CO2 emitted FairnessBenchmarks measure gendered and racial stereotypes, or decreased performance for language from or about some groups. \n\n## Page 55\n\nLarge Language ModelsDealing with Scale\n\n## Page 56\n\nScaling LawsLLM performance depends on\u2022Model size: the number of parameters not counting embeddings\u2022Dataset size: the amount of training data\u2022Compute: Amount of compute (in FLOPS or etcCan improve a model by adding  parameters (more layers, wider contexts), more data, or training for more iterationsThe performance of a large language model (the loss) scales as a power-law with each of these three",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 36,
      "token_count": 347,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 57\n\nScaling LawsLoss L as a function of # parameters N, dataset size D, compute budget C (if other two are held constant)14CHAPTER10\u2022LARGELANGUAGEMODELS10.5 Dealing with ScaleLarge language models are large. For example theLlama 3.1 405B Instructmodelfrom Meta has 405 billion parameters (126 layers, a model dimensionality of 16,384,128 attention heads) and was trained on 15.6 terabytes of text tokens (Llama Team,2024), using a vocabulary of 128K tokens. So there is a lot of research on un-derstanding how LLMs scale, and especially how to implement them given limitedresources. In the next few sections we discuss how to think about scale (the conceptofscaling laws), and important techniques for getting language models to workef\ufb01ciently, such as theKV cacheand parameter-ef\ufb01cient \ufb01ne tuning.10.5.1 Scaling lawsThe performance of large language models has shown to be mainly determined by3 factors: model size (the number of parameters not counting embeddings), datasetsize (the amount of training data), and the amount of compute used for training. Thatis, we can improve a model by adding parameters (adding more layers or havingwider contexts or both), by training on more data, or by training for more iterations.The relationships between these factors and performance are known asscalinglaws. Roughly speaking, the performance of a large language model (the loss) scalesscaling lawsas a power-law with each of these three properties of model training.For example,Kaplan et al.(2020) found the following three relationships forlossLas a function of the number of non-embedding parametersN, the dataset sizeD, and the compute budgetC, for models training with limited parameters, dataset,or compute budget, if in each case the other two properties are held constant:L(N)=\u2713NcN\u25c6aN(10.9)L(D)=\u2713DcD\u25c6aD(10.10)L(C)=\u2713CcC\u25c6aC(10.11)The number of (non-embedding) parametersNcan be roughly computed as fol-lows (ignoring biases, and withdas the input and output dimensionality of themodel,dattnas the self-attention layer size, anddffthe size of the feedforward layer):N\u21e12dnlayer(2dattn+dff)\u21e112nlayerd2(10.12)(assumingdattn=dff/4=d)Thus GPT-3, withn=96 layers and dimensionalityd=12288, has 12\u21e596\u21e5122882\u21e1175 billion parameters.The values ofNc,Dc,Cc,aN,aD, andaCdepend on the exact transformerarchitecture, tokenization, and vocabulary size, so rather than all the precise values,scaling laws focus on the relationship with loss.2Scaling laws can be useful in deciding how to train a model to a particular per-formance, for example by looking at early in the training curve, or performance with2For the initial experiment inKaplan et al.(2020) the precise values wereaN= 0.076,Nc= 8.8\u21e51013(parameters),aD= 0.095,Dc= 5.4\u21e51013(tokens),aC= 0.050,Cc= 3.1\u21e5108(peta\ufb02op-days).Scaling laws can be used early in training to predict what the loss would be if we were to add more data or increase model size.",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 37,
      "token_count": 777,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 58\n\nNumber of non-embedding parameters N14CHAPTER10\u2022LARGELANGUAGEMODELS10.5 Dealing with ScaleLarge language models are large. For example theLlama 3.1 405B Instructmodelfrom Meta has 405 billion parameters (126 layers, a model dimensionality of 16,384,128 attention heads) and was trained on 15.6 terabytes of text tokens (Llama Team,2024), using a vocabulary of 128K tokens. So there is a lot of research on un-derstanding how LLMs scale, and especially how to implement them given limitedresources. In the next few sections we discuss how to think about scale (the conceptofscaling laws), and important techniques for getting language models to workef\ufb01ciently, such as theKV cacheand parameter-ef\ufb01cient \ufb01ne tuning.10.5.1 Scaling lawsThe performance of large language models has shown to be mainly determined by3 factors: model size (the number of parameters not counting embeddings), datasetsize (the amount of training data), and the amount of compute used for training. Thatis, we can improve a model by adding parameters (adding more layers or havingwider contexts or both), by training on more data, or by training for more iterations.The relationships between these factors and performance are known asscalinglaws. Roughly speaking, the performance of a large language model (the loss) scalesscaling lawsas a power-law with each of these three properties of model training.For example,Kaplan et al.(2020) found the following three relationships forlossLas a function of the number of non-embedding parametersN, the dataset sizeD, and the compute budgetC, for models training with limited parameters, dataset,or compute budget, if in each case the other two properties are held constant:L(N)=\u2713NcN\u25c6aN(10.9)L(D)=\u2713DcD\u25c6aD(10.10)L(C)=\u2713CcC\u25c6aC(10.11)The number of (non-embedding) parametersNcan be roughly computed as fol-lows (ignoring biases, and withdas the input and output dimensionality of themodel,dattnas the self-attention layer size, anddffthe size of the feedforward layer):N\u21e12dnlayer(2dattn+dff)\u21e112nlayerd2(10.12)(assumingdattn=dff/4=d)Thus GPT-3, withn=96 layers and dimensionalityd=12288, has 12\u21e596\u21e5122882\u21e1175 billion parameters.The values ofNc,Dc,Cc,aN,aD, andaCdepend on the exact transformerarchitecture, tokenization, and vocabulary size, so rather than all the precise values,scaling laws focus on the relationship with loss.2Scaling laws can be useful in deciding how to train a model to a particular per-formance, for example by looking at early in the training curve, or performance with2For the initial experiment inKaplan et al.(2020) the precise values wereaN= 0.076,Nc= 8.8\u21e51013(parameters),aD= 0.095,Dc= 5.4\u21e51013(tokens),aC= 0.050,Cc= 3.1\u21e5108(peta\ufb02op-days).Thus GPT-3, with n = 96 layers and dimensionality d = 12288, has 12 \u00d7 96 \u00d7 122882 \u2248 175 billion parameters.",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 38,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 59\n\nKV CacheIn training, we can compute attention very efficiently in parallel:But not at inference! We generate the next tokens one at a time!For a new token x, need to multiply by WQ, WK, and WV to get query, key, valuesBut don't want to recompute the key and value vectors for all the prior tokens x<iInstead, store key and value vectors in memory in the KV cache, and then we can just grab them from the cache 10.5\u2022DEALING WITHSCALE15smaller amounts of data, to predict what the loss would be if we were to add moredata or increase model size. Other aspects of scaling laws can also tell us how muchdata we need to add when scaling up a model.10.5.2 KV CacheWe saw in Fig.??and in Eq.??(repeated below) how the attention vector can bevery ef\ufb01ciently computed in parallel for training, via two matrix multiplications:A=softmax\u2713QK|pdk\u25c6V(10.13)Unfortunately we can\u2019t do quite the same ef\ufb01cient computation in inference asin training. That\u2019s because at inference time, we iteratively generate the next tokensone at a time. For a new token that we have just generated, call itxi, we need tocompute its query, key, and values by multiplying byWQ,WK, andWVrespec-tively. But it would be a waste of computation time to recompute the key and valuevectors for all thepriortokensx<i; at prior steps we already computed these keyand value vectors! So instead of recomputing these, whenever we compute the keyand value vectors we store them in memory in theKV cache, and then we can justKV cachegrab them from the cache when we need them. Fig.10.7modi\ufb01es Fig.??to showthe computation that takes place for a single new token, showing which values wecan take from the cache rather than recompute.\nq4k1k2k4QKTQKTv1v2v3v4V\nq4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4x==xa4A\n1 x dkdk x N1 x NN x dv1 x dv\nk3Figure 10.7Parts of the attention computation (extracted from Fig.??) showing, in black,the vectors that can be stored in the cache rather than recomputed when computing the atten-tion score for the 4th token.10.5.3 Parameter Ef\ufb01cient Fine TuningAs we mentioned above, it\u2019s very common to take a language model and give it moreinformation about a new domain by\ufb01netuningit (continuing to train it to predictupcoming words) on some additional data.Fine-tuning can be very dif\ufb01cult with very large language models, because thereare enormous numbers of parameters to train; each pass of batch gradient descenthas to backpropagate through many many huge layers. This makes \ufb01netuning hugelanguage models extremely expensive in processing power, in memory, and in time.For this reason, there are alternative methods that allow a model to be \ufb01netunedwithout changing all the parameters. Such methods are calledparameter-ef\ufb01cient\ufb01ne tuningor sometimesPEFT, because we ef\ufb01ciently select a subset of parametersparameter-ef\ufb01cient \ufb01netuningPEFTto update when \ufb01netuning. For example we freeze some of the parameters (don\u2019tchange them), and only update some particular subset of parameters.",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 39,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 60",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 40,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "KV Cache\nq4k1k2k4QKTQKTv1v2v3v4V\nq4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4x==xa4A\n1 x dkdk x N1 x NN x dv1 x dv\nk3q1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2\u2022k2q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k2q3\u2022k3\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221eq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3q1\u2022k2q2\u2022k3q1\u2022k3q3\u2022k4q2\u2022k4q1\u2022k4x=QKT maskedmask=q1\u2022k1q2\u2022k1q4\u2022k1q3\u2022k1q1\u2022k1q1\u2022k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dvq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2\u2022k2q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k2q3\u2022k3\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221eq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3q1\u2022k2q2\u2022k3q1\u2022k3q3\u2022k4q2\u2022k4q1\u2022k4x=QKT maskedmask=q1\u2022k1q2\u2022k1q4\u2022k1q3\u2022k1q1\u2022k1q1\u2022k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 41,
      "token_count": 740,
      "chapter_title": ""
    }
  },
  {
    "content": "N x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dvq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2\u2022k2q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k2q3\u2022k3\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221eq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3q1\u2022k2q2\u2022k3q1\u2022k3q3\u2022k4q2\u2022k4q1\u2022k4x=QKT maskedmask=q1\u2022k1q2\u2022k1q4\u2022k1q3\u2022k1q1\u2022k1q1\u2022k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X\nN x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dv",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 42,
      "token_count": 398,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 61\n\nParameter-Efficient FinetuningAdapting to a new domain by continued pretraining (finetuning) is a problem with huge LLMs.\u2022Enormous numbers of parameters to train \u2022Each pass of batch gradient descent has to backpropagate through many many huge layers. \u2022Expensive in processing power, in memory, and in time. Instead, parameter-efficient fine tuning (PEFT)\u2022Efficiently select a subset of parameters to update when finetuning.\u2022E.g., freeze some of the parameters (don\u2019t change them), \u2022And only update some a few parameters. \n\n## Page 62\n\nLoRA (Low-Rank Adaptation)\u2022Trransformers have many dense matrix multiply layers\u2022Like WQ, WK, WV, WO layers in attention\u2022Instead of updating these layers during finetuning, \u2022Freeze these layers \u2022Update a low-rank approximation with fewer parameters. \n\n## Page 63\n\nLoRA\u2022Consider a matrix W (shape [N \u00d7 d])  that needs to be updated during finetuning via gradient descent. \u2022Normally updates are \u2206W  (shape [N \u00d7 d])\u2022In LoRA, we freeze W and update instead a low-rank decomposition of W:\u2022A of shape [N\u00d7r], \u2022B of shape [r\u00d7d], r is very small  (like 1 or 2)\u2022That is, during  finetuning we update A and B instead of W. \u2022Replace W + \u2206W with W + BA. Forward pass: instead of     h = xW We do     h = xW + xAB \n\n## Page 64\n\nLoRAhPretrained WeightsWdkrkABrxd11k\nd\u00d7\n\n## Page 65\n\nLarge Language ModelsDealing with Scale\n\n## Page 66\n\nLarge Language ModelsHarms of Large Language Models\n\n## Page 67\n\nHallucination\n\n## Page 68\n\nCopyright\n\n## Page 69\n\nPrivacy\n\n## Page 70\n\nToxicity and Abuse\n\n## Page 71\n\nMisinformation\n\n## Page 72\n\nLarge Language ModelsHarms of Large Language Models",
    "metadata": {
      "source": "LLM24aug",
      "chunk_id": 43,
      "token_count": 451,
      "chapter_title": ""
    }
  }
]