[
  {
    "content": "# 4\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n4Naive Bayes, Text Classi\ufb01ca-\ntion, and Sentiment\nClassi\ufb01cation lies at the heart of both human and machine intelligence. Deciding\nwhat letter, word, or image has been presented to our senses, recognizing faces\nor voices, sorting mail, assigning grades to homeworks; these are all examples of\nassigning a category to an input. The potential challenges of this task are highlighted\nby the fabulist Jorge Luis Borges (1964), who imagined classifying animals into:\n(a) those that belong to the Emperor, (b) embalmed ones, (c) those that\nare trained, (d) suckling pigs, (e) mermaids, (f) fabulous ones, (g) stray\ndogs, (h) those that are included in this classi\ufb01cation, (i) those that\ntremble as if they were mad, (j) innumerable ones, (k) those drawn with\na very \ufb01ne camel\u2019s hair brush, (l) others, (m) those that have just broken\na \ufb02ower vase, (n) those that resemble \ufb02ies from a distance.\nMany language processing tasks involve classi\ufb01cation, although luckily our classes\nare much easier to de\ufb01ne than those of Borges. In this chapter we introduce the naive\nBayes algorithm and apply it to text categorization , the task of assigning a label ortext\ncategorization\ncategory to an entire text or document.\nWe focus on one common text categorization task, sentiment analysis , the ex-sentiment\nanalysis\ntraction of sentiment , the positive or negative orientation that a writer expresses\ntoward some object. A review of a movie, book, or product on the web expresses the\nauthor\u2019s sentiment toward the product, while an editorial or political text expresses\nsentiment toward a candidate or political action. Extracting consumer or public sen-\ntiment is thus relevant for \ufb01elds from marketing to politics.\nThe simplest version of sentiment analysis is a binary classi\ufb01cation task, and\nthe words of the review provide excellent cues. Consider, for example, the follow-\ning phrases extracted from positive and negative reviews of movies and restaurants.\nWords like great ,richly ,awesome , and pathetic , and awful andridiculously are very\ninformative cues:\n+...zany characters and richly applied satire, and some great plot twists\n\u0000It was pathetic. The worst part about it was the boxing scenes...\n+...awesome caramel sauce and sweet toasty almonds. I love this place!\n\u0000...awful pizza and ridiculously overpriced...\nSpam detection is another important commercial application, the binary clas- spam detection\nsi\ufb01cation task of assigning an email to one of the two classes spam ornot-spam .\nMany lexical and other features can be used to perform this classi\ufb01cation. For ex-\nample you might quite reasonably be suspicious of an email containing phrases like\n\u201conline pharmaceutical\u201d or \u201cWITHOUT ANY COST\u201d or \u201cDear Winner\u201d.\nAnother thing we might want to know about a text is the language it\u2019s written\nin. Texts on social media, for example, can be in any number of languages and\nwe\u2019ll need to apply different processing. The task of language id is thus the \ufb01rst language id\nstep in most language processing pipelines. Related text classi\ufb01cation tasks like au-\nthorship attribution \u2014 determining a text\u2019s author\u2014 are also relevant to the digitalauthorship\nattribution\nhumanities, social sciences, and forensic linguistics.",
    "metadata": {
      "source": "4",
      "chunk_id": 0,
      "token_count": 799,
      "chapter_title": "4"
    }
  },
  {
    "content": "## Page 2",
    "metadata": {
      "source": "4",
      "chunk_id": 1,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "2CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\nFinally, one of the oldest tasks in text classi\ufb01cation is assigning a library sub-\nject category or topic label to a text. Deciding whether a research paper concerns\nepidemiology or instead, perhaps, embryology, is an important component of infor-\nmation retrieval. Various sets of subject categories exist, such as the MeSH (Medical\nSubject Headings) thesaurus. In fact, as we will see, subject category classi\ufb01cation\nis the task for which the naive Bayes algorithm was invented in 1961 (Maron, 1961).\nClassi\ufb01cation is essential for tasks below the level of the document as well.\nWe\u2019ve already seen period disambiguation (deciding if a period is the end of a sen-\ntence or part of a word), and word tokenization (deciding if a character should be\na word boundary). Even language modeling can be viewed as classi\ufb01cation: each\nword can be thought of as a class, and so predicting the next word is classifying the\ncontext-so-far into a class for each next word. A part-of-speech tagger (Chapter 17)\nclassi\ufb01es each occurrence of a word in a sentence as, e.g., a noun or a verb.\nThe goal of classi\ufb01cation is to take a single observation, extract some useful\nfeatures, and thereby classify the observation into one of a set of discrete classes.\nOne method for classifying text is to use rules handwritten by humans. Handwrit-\nten rule-based classi\ufb01ers can be components of state-of-the-art systems in language\nprocessing. But rules can be fragile, as situations or data change over time, and for\nsome tasks humans aren\u2019t necessarily good at coming up with the rules.\nThe most common way of doing text classi\ufb01cation in language processing is\ninstead via supervised machine learning , the subject of this chapter. In supervisedsupervised\nmachine\nlearninglearning, we have a data set of input observations, each associated with some correct\noutput (a \u2018supervision signal\u2019). The goal of the algorithm is to learn how to map\nfrom a new observation to a correct output.\nFormally, the task of supervised classi\ufb01cation is to take an input xand a \ufb01xed\nset of output classes Y=fy1;y2;:::;yMgand return a predicted class y2Y. For\ntext classi\ufb01cation, we\u2019ll sometimes talk about c(for \u201cclass\u201d) instead of yas our\noutput variable, and d(for \u201cdocument\u201d) instead of xas our input variable. In the\nsupervised situation we have a training set of Ndocuments that have each been hand-\nlabeled with a class: f(d1;c1);::::; (dN;cN)g. Our goal is to learn a classi\ufb01er that is\ncapable of mapping from a new document dto its correct class c2C, where Cis\nsome set of useful document classes. A probabilistic classi\ufb01er additionally will tell\nus the probability of the observation being in the class. This full distribution over\nthe classes can be useful information for downstream decisions; avoiding making\ndiscrete decisions early on can be useful when combining systems.\nMany kinds of machine learning algorithms are used to build classi\ufb01ers. This\nchapter introduces naive Bayes; the following one introduces logistic regression.\nThese exemplify two ways of doing classi\ufb01cation. Generative classi\ufb01ers like naive\nBayes build a model of how a class could generate some input data. Given an ob-",
    "metadata": {
      "source": "4",
      "chunk_id": 2,
      "token_count": 784,
      "chapter_title": ""
    }
  },
  {
    "content": "set of output classes Y=fy1;y2;:::;yMgand return a predicted class y2Y. For\ntext classi\ufb01cation, we\u2019ll sometimes talk about c(for \u201cclass\u201d) instead of yas our\noutput variable, and d(for \u201cdocument\u201d) instead of xas our input variable. In the\nsupervised situation we have a training set of Ndocuments that have each been hand-\nlabeled with a class: f(d1;c1);::::; (dN;cN)g. Our goal is to learn a classi\ufb01er that is\ncapable of mapping from a new document dto its correct class c2C, where Cis\nsome set of useful document classes. A probabilistic classi\ufb01er additionally will tell\nus the probability of the observation being in the class. This full distribution over\nthe classes can be useful information for downstream decisions; avoiding making\ndiscrete decisions early on can be useful when combining systems.\nMany kinds of machine learning algorithms are used to build classi\ufb01ers. This\nchapter introduces naive Bayes; the following one introduces logistic regression.\nThese exemplify two ways of doing classi\ufb01cation. Generative classi\ufb01ers like naive\nBayes build a model of how a class could generate some input data. Given an ob-\nservation, they return the class most likely to have generated the observation. Dis-\ncriminative classi\ufb01ers like logistic regression instead learn what features from the\ninput are most useful to discriminate between the different possible classes. While\ndiscriminative systems are often more accurate and hence more commonly used,\ngenerative classi\ufb01ers still have a role.\n4.1 Naive Bayes Classi\ufb01ers\nIn this section we introduce the multinomial naive Bayes classi\ufb01er , so called be-naive Bayes\nclassi\ufb01er\ncause it is a Bayesian classi\ufb01er that makes a simplifying (naive) assumption about",
    "metadata": {
      "source": "4",
      "chunk_id": 3,
      "token_count": 418,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\n4.1 \u2022 N AIVE BAYES CLASSIFIERS 3\nhow the features interact.\nThe intuition of the classi\ufb01er is shown in Fig. 4.1. We represent a text document\nas if it were a bag of words , that is, an unordered set of words with their position bag of words\nignored, keeping only their frequency in the document. In the example in the \ufb01gure,\ninstead of representing the word order in all the phrases like \u201cI love this movie\u201d and\n\u201cI would recommend it\u201d, we simply note that the word Ioccurred 5 times in the\nentire excerpt, the word it6 times, the words love,recommend , and movie once, and\nso on.\nititititititIIII\nIloverecommendmoviethethethetheto\ntotoand\nandandseenseenyetwouldwithwhowhimsical\nwhilewhenevertimessweetseveralscenessatiricalromanticofmanageshumorhavehappyfunfriendfairydialoguebutconventionsareanyoneadventurealwaysagainaboutI love this movie! It's sweet, but with satirical humor. The dialogue is great and the adventure scenes are fun... It manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre. I would recommend it to just about anyone. I've seen it several times, and I'm always happy to see it again whenever I have a friend who hasn't seen it yet!it Ithetoandseenyetwouldwhimsicaltimessweetsatiricaladventuregenrefairyhumorhavegreat\u20266 54332111111111111\u2026\nFigure 4.1 Intuition of the multinomial naive Bayes classi\ufb01er applied to a movie review. The position of the\nwords is ignored (the bag-of-words assumption) and we make use of the frequency of each word.\nNaive Bayes is a probabilistic classi\ufb01er, meaning that for a document d, out of\nall classes c2Cthe classi\ufb01er returns the class \u02c6 cwhich has the maximum posterior\nprobability given the document. In Eq. 4.1 we use the hat notation \u02c6to mean \u201cour \u02c6\nestimate of the correct class\u201d, and we use argmax to mean an operation that selects argmax\nthe argument (in this case the class c) that maximizes a function (in this case the\nprobability P(cjd).\n\u02c6c=argmax\nc2CP(cjd) (4.1)\nThis idea of Bayesian inference has been known since the work of Bayes (1763),Bayesian\ninference\nand was \ufb01rst applied to text classi\ufb01cation by Mosteller and Wallace (1964). The\nintuition of Bayesian classi\ufb01cation is to use Bayes\u2019 rule to transform Eq. 4.1 into\nother probabilities that have some useful properties. Bayes\u2019 rule is presented in\nEq. 4.2; it gives us a way to break down any conditional probability P(xjy)into\nthree other probabilities:\nP(xjy) =P(yjx)P(x)\nP(y)(4.2)",
    "metadata": {
      "source": "4",
      "chunk_id": 4,
      "token_count": 688,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\nWe can then substitute Eq. 4.2 into Eq. 4.1 to get Eq. 4.3:\n\u02c6c=argmax\nc2CP(cjd) =argmax\nc2CP(djc)P(c)\nP(d)(4.3)\nWe can conveniently simplify Eq. 4.3 by dropping the denominator P(d). This\nis possible because we will be computingP(djc)P(c)\nP(d)for each possible class. But P(d)\ndoesn\u2019t change for each class; we are always asking about the most likely class for\nthe same document d, which must have the same probability P(d). Thus, we can\nchoose the class that maximizes this simpler formula:\n\u02c6c=argmax\nc2CP(cjd) =argmax\nc2CP(djc)P(c) (4.4)\nWe call Naive Bayes a generative model because we can read Eq. 4.4 as stating\na kind of implicit assumption about how a document is generated: \ufb01rst a class is\nsampled from P(c), and then the words are generated by sampling from P(djc). (In\nfact we could imagine generating arti\ufb01cial documents, or at least their word counts,\nby following this process). We\u2019ll say more about this intuition of generative models\nin Chapter 5.\nTo return to classi\ufb01cation: we compute the most probable class \u02c6 cgiven some\ndocument dby choosing the class which has the highest product of two probabilities:\ntheprior probability of the class P(c)and the likelihood of the document P(djc):prior\nprobability\nlikelihood\n\u02c6c=argmax\nc2Clikelihoodz}|{\nP(djc)priorz}|{\nP(c) (4.5)\nWithout loss of generality, we can represent a document das a set of features\nf1;f2;:::;fn:\n\u02c6c=argmax\nc2Clikelihoodz}|{\nP(f1;f2;::::; fnjc)priorz}|{\nP(c) (4.6)\nUnfortunately, Eq. 4.6 is still too hard to compute directly: without some sim-\nplifying assumptions, estimating the probability of every possible combination of\nfeatures (for example, every possible set of words and positions) would require huge\nnumbers of parameters and impossibly large training sets. Naive Bayes classi\ufb01ers\ntherefore make two simplifying assumptions.\nThe \ufb01rst is the bag-of-words assumption discussed intuitively above: we assume\nposition doesn\u2019t matter, and that the word \u201clove\u201d has the same effect on classi\ufb01cation\nwhether it occurs as the 1st, 20th, or last word in the document. Thus we assume\nthat the features f1;f2;:::;fnonly encode word identity and not position.\nThe second is commonly called the naive Bayes assumption : this is the condi-naive Bayes\nassumption\ntional independence assumption that the probabilities P(fijc)are independent given\nthe class cand hence can be \u2018naively\u2019 multiplied as follows:\nP(f1;f2;::::; fnjc) = P(f1jc)\u0001P(f2jc)\u0001:::\u0001P(fnjc) (4.7)\nThe \ufb01nal equation for the class chosen by a naive Bayes classi\ufb01er is thus:\ncNB=argmax\nc2CP(c)Y\nf2FP(fjc) (4.8)",
    "metadata": {
      "source": "4",
      "chunk_id": 5,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\n4.2 \u2022 T RAINING THE NAIVE BAYES CLASSIFIER 5\nTo apply the naive Bayes classi\ufb01er to text, we will use each word in the documents\nas a feature, as suggested above, and we consider each of the words in the document\nby walking an index through every word position in the document:\npositions all word positions in test document\ncNB =argmax\nc2CP(c)Y\ni2positionsP(wijc) (4.9)\nNaive Bayes calculations, like calculations for language modeling, are done in log\nspace, to avoid under\ufb02ow and increase speed. Thus Eq. 4.9 is generally instead\nexpressed1as\ncNB=argmax\nc2ClogP(c) +X\ni2positionslogP(wijc) (4.10)\nBy considering features in log space, Eq. 4.10 computes the predicted class as a lin-\near function of input features. Classi\ufb01ers that use a linear combination of the inputs\nto make a classi\ufb01cation decision \u2014like naive Bayes and also logistic regression\u2014\nare called linear classi\ufb01ers .linear\nclassi\ufb01ers\n4.2 Training the Naive Bayes Classi\ufb01er\nHow can we learn the probabilities P(c)andP(fijc)? Let\u2019s \ufb01rst consider the maxi-\nmum likelihood estimate. We\u2019ll simply use the frequencies in the data. For the class\nprior P(c)we ask what percentage of the documents in our training set are in each\nclass c. Let Ncbe the number of documents in our training data with class cand\nNdocbe the total number of documents. Then:\n\u02c6P(c) =Nc\nNdoc(4.11)\nTo learn the probability P(fijc), we\u2019ll assume a feature is just the existence of a word\nin the document\u2019s bag of words, and so we\u2019ll want P(wijc), which we compute as\nthe fraction of times the word wiappears among all words in all documents of topic\nc. We \ufb01rst concatenate all documents with category cinto one big \u201ccategory c\u201d text.\nThen we use the frequency of wiin this concatenated document to give a maximum\nlikelihood estimate of the probability:\n\u02c6P(wijc) =count (wi;c)P\nw2Vcount (w;c)(4.12)\nHere the vocabulary V consists of the union of all the word types in all classes, not\njust the words in one class c.\nThere is a problem, however, with maximum likelihood training. Imagine we\nare trying to estimate the likelihood of the word \u201cfantastic\u201d given class positive , but\nsuppose there are no training documents that both contain the word \u201cfantastic\u201d and\nare classi\ufb01ed as positive . Perhaps the word \u201cfantastic\u201d happens to occur (sarcasti-\ncally?) in the class negative . In such a case the probability for this feature will be\nzero:\n\u02c6P(\u201cfantastic\u201djpositive ) =count (\u201cfantastic\u201d ;positive )P\nw2Vcount (w;positive )=0 (4.13)\n1In practice throughout this book, we\u2019ll use log to mean natural log (ln) when the base is not speci\ufb01ed.",
    "metadata": {
      "source": "4",
      "chunk_id": 6,
      "token_count": 714,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\nBut since naive Bayes naively multiplies all the feature likelihoods together, zero\nprobabilities in the likelihood term for any class will cause the probability of the\nclass to be zero, no matter the other evidence!\nThe simplest solution is the add-one (Laplace) smoothing introduced in Chap-\nter 3. While Laplace smoothing is usually replaced by more sophisticated smoothing\nalgorithms in language modeling, it is commonly used in naive Bayes text catego-\nrization:\n\u02c6P(wijc) =count (wi;c) +1P\nw2V(count (w;c) +1)=count (wi;c) +1\u0000P\nw2Vcount (w;c)\u0001\n+jVj(4.14)\nNote once again that it is crucial that the vocabulary V consists of the union of all the\nword types in all classes, not just the words in one class c(try to convince yourself\nwhy this must be true; see the exercise at the end of the chapter).\nWhat do we do about words that occur in our test data but are not in our vocab-\nulary at all because they did not occur in any training document in any class? The\nsolution for such unknown words is to ignore them\u2014remove them from the test unknown word\ndocument and not include any probability for them at all.\nFinally, some systems choose to completely ignore another class of words: stop\nwords , very frequent words like theanda. This can be done by sorting the vocabu- stop words\nlary by frequency in the training set, and de\ufb01ning the top 10\u2013100 vocabulary entries\nas stop words, or alternatively by using one of the many prede\ufb01ned stop word lists\navailable online. Then each instance of these stop words is simply removed from\nboth training and test documents as if it had never occurred. In most text classi\ufb01ca-\ntion applications, however, using a stop word list doesn\u2019t improve performance, and\nso it is more common to make use of the entire vocabulary and not use a stop word\nlist.\nFig. 4.2 shows the \ufb01nal algorithm.\n4.3 Worked example\nLet\u2019s walk through an example of training and testing naive Bayes with add-one\nsmoothing. We\u2019ll use a sentiment analysis domain with the two classes positive\n(+) and negative (-), and take the following miniature training and test documents\nsimpli\ufb01ed from actual movie reviews.\nCat Documents\nTraining - just plain boring\n- entirely predictable and lacks energy\n- no surprises and very few laughs\n+ very powerful\n+ the most fun \ufb01lm of the summer\nTest ? predictable with no fun\nThe prior P(c)for the two classes is computed via Eq. 4.11 asNc\nNdoc:\nP(\u0000) =3\n5P(+) =2\n5\nThe word with doesn\u2019t occur in the training set, so we drop it completely (as\nmentioned above, we don\u2019t use unknown word models for naive Bayes). The like-\nlihoods from the training set for the remaining three words \u201cpredictable\u201d, \u201cno\u201d, and",
    "metadata": {
      "source": "4",
      "chunk_id": 7,
      "token_count": 680,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\n4.4 \u2022 O PTIMIZING FOR SENTIMENT ANALYSIS 7\nfunction TRAIN NAIVE BAYES (D, C) returns V;logP(c), log P(wjc)\nfor each class c2C # Calculate P(c)terms\nNdoc= number of documents in D\nNc= number of documents from D in class c\nlogprior [c] logNc\nNdoc\nV vocabulary of D\nbigdoc [c] append (d)ford2Dwith class c\nfor each word win V # Calculate P(wjc)terms\ncount(w,c) # of occurrences of winbigdoc [c]\nloglikelihood [w,c] logcount (w;c) + 1P\nw0in V(count (w0;c) + 1)\nreturn logprior ,loglikelihood ,V\nfunction TESTNAIVE BAYES (testdoc ,logprior ,loglikelihood , C, V) returns bestc\nfor each class c2C\nsum[c] logprior [c]\nfor each position iintestdoc\nword testdoc[i]\nifword2V\nsum[c] sum[c]+loglikelihood [word ,c]\nreturn argmaxcsum[c]\nFigure 4.2 The naive Bayes algorithm, using add-1 smoothing. To use add- asmoothing\ninstead, change the +1 to +afor loglikelihood counts in training.\n\u201cfun\u201d, are as follows, from Eq. 4.14 (computing the probabilities for the remainder\nof the words in the training set is left as an exercise for the reader):\nP(\u201cpredictable\u201dj\u0000) =1+1\n14+20P(\u201cpredictable\u201dj+) =0+1\n9+20\nP(\u201cno\u201dj\u0000) =1+1\n14+20P(\u201cno\u201dj+) =0+1\n9+20\nP(\u201cfun\u201dj\u0000) =0+1\n14+20P(\u201cfun\u201dj+) =1+1\n9+20\nFor the test sentence S = \u201cpredictable with no fun\u201d, after removing the word \u2018with\u2019,\nthe chosen class, via Eq. 4.9, is therefore computed as follows:\nP(\u0000)P(Sj\u0000) =3\n5\u00022\u00022\u00021\n343=6:1\u000210\u00005\nP(+)P(Sj+) =2\n5\u00021\u00021\u00022\n293=3:2\u000210\u00005\nThe model thus predicts the class negative for the test sentence.\n4.4 Optimizing for Sentiment Analysis\nWhile standard naive Bayes text classi\ufb01cation can work well for sentiment analysis,\nsome small changes are generally employed that improve performance.",
    "metadata": {
      "source": "4",
      "chunk_id": 8,
      "token_count": 582,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8",
    "metadata": {
      "source": "4",
      "chunk_id": 9,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "8CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\nFirst, for sentiment classi\ufb01cation and a number of other text classi\ufb01cation tasks,\nwhether a word occurs or not seems to matter more than its frequency. Thus it often\nimproves performance to clip the word counts in each document at 1 (see the end\nof the chapter for pointers to these results). This variant is called binary multino-\nmial naive Bayes orbinary naive Bayes . The variant uses the same algorithm asbinary naive\nBayes\nin Fig. 4.2 except that for each document we remove all duplicate words before con-\ncatenating them into the single big document during training and we also remove\nduplicate words from test documents. Fig. 4.3 shows an example in which a set\nof four documents (shortened and text-normalized for this example) are remapped\nto binary, with the modi\ufb01ed counts shown in the table on the right. The example\nis worked without add-1 smoothing to make the differences clearer. Note that the\nresults counts need not be 1; the word great has a count of 2 even for binary naive\nBayes, because it appears in multiple documents.\nFour original documents:\n\u0000it was pathetic the worst part was the\nboxing scenes\n\u0000no plot twists or great scenes\n+and satire and great plot twists\n+great scenes great \ufb01lm\nAfter per-document binarization:\n\u0000it was pathetic the worst part boxing\nscenes\n\u0000no plot twists or great scenes\n+and satire great plot twists\n+great scenes \ufb01lmNB Binary\nCounts Counts\n+\u0000 +\u0000\nand 2 0 1 0\nboxing 0 1 0 1\n\ufb01lm 1 0 1 0\ngreat 3 1 2 1\nit 0 1 0 1\nno 0 1 0 1\nor 0 1 0 1\npart 0 1 0 1\npathetic 0 1 0 1\nplot 1 1 1 1\nsatire 1 0 1 0\nscenes 1 2 1 2\nthe 0 2 0 1\ntwists 1 1 1 1\nwas 0 2 0 1\nworst 0 1 0 1\nFigure 4.3 An example of binarization for the binary naive Bayes algorithm.\nA second important addition commonly made when doing text classi\ufb01cation for\nsentiment is to deal with negation. Consider the difference between I really like this\nmovie (positive) and I didn\u2019t like this movie (negative). The negation expressed by\ndidn\u2019t completely alters the inferences we draw from the predicate like. Similarly,\nnegation can modify a negative word to produce a positive review ( don\u2019t dismiss this\n\ufb01lm,doesn\u2019t let us get bored ).\nA very simple baseline that is commonly used in sentiment analysis to deal with\nnegation is the following: during text normalization, prepend the pre\ufb01x NOT to\nevery word after a token of logical negation ( n\u2019t, not, no, never ) until the next punc-\ntuation mark. Thus the phrase\ndidn't like this movie , but I\nbecomes\ndidn't NOT_like NOT_this NOT_movie , but I\nNewly formed \u2018words\u2019 like NOT like,NOT recommend will thus occur more\noften in negative document and act as cues for negative sentiment, while words\nlikeNOT bored ,NOT dismiss will acquire positive associations. Syntactic parsing\n(Chapter 18) can be used deal more accurately with the scope relationship between",
    "metadata": {
      "source": "4",
      "chunk_id": 10,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9",
    "metadata": {
      "source": "4",
      "chunk_id": 11,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "4.5 \u2022 N AIVE BAYES FOR OTHER TEXT CLASSIFICATION TASKS 9\nthese negation words and the predicates they modify, but this simple baseline works\nquite well in practice.\nFinally, in some situations we might have insuf\ufb01cient labeled training data to\ntrain accurate naive Bayes classi\ufb01ers using all words in the training set to estimate\npositive and negative sentiment. In such cases we can instead derive the positive\nand negative word features from sentiment lexicons , lists of words that are pre-sentiment\nlexicons\nannotated with positive or negative sentiment. Four popular lexicons are the General\nInquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexiconGeneral\nInquirer\nLIWC of Hu and Liu (2004) and the MPQA Subjectivity Lexicon (Wilson et al., 2005).\nFor example the MPQA subjectivity lexicon has 6885 words each marked for\nwhether it is strongly or weakly biased positive or negative. Some examples:\n+:admirable, beautiful, con\ufb01dent, dazzling, ecstatic, favor, glee, great\n\u0000:awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate\nA common way to use lexicons in a naive Bayes classi\ufb01er is to add a feature\nthat is counted whenever a word from that lexicon occurs. Thus we might add a\nfeature called \u2018this word occurs in the positive lexicon\u2019, and treat all instances of\nwords in the lexicon as counts for that one feature, instead of counting each word\nseparately. Similarly, we might add as a second feature \u2018this word occurs in the\nnegative lexicon\u2019 of words in the negative lexicon. If we have lots of training data,\nand if the test data matches the training data, using just two features won\u2019t work as\nwell as using all the words. But when training data is sparse or not representative of\nthe test set, using dense lexicon features instead of sparse individual-word features\nmay generalize better.\nWe\u2019ll return to this use of lexicons in Chapter 22, showing how these lexicons\ncan be learned automatically, and how they can be applied to many other tasks be-\nyond sentiment classi\ufb01cation.\n4.5 Naive Bayes for other text classi\ufb01cation tasks\nIn the previous section we pointed out that naive Bayes doesn\u2019t require that our\nclassi\ufb01er use all the words in the training data as features. In fact features in naive\nBayes can express any property of the input text we want.\nConsider the task of spam detection , deciding if a particular piece of email is spam detection\nan example of spam (unsolicited bulk email)\u2014one of the \ufb01rst applications of naive\nBayes to text classi\ufb01cation (Sahami et al., 1998).\nA common solution here, rather than using all the words as individual features,\nis to prede\ufb01ne likely sets of words or phrases as features, combined with features\nthat are not purely linguistic. For example the open-source SpamAssassin tool2\nprede\ufb01nes features like the phrase \u201cone hundred percent guaranteed\u201d, or the feature\nmentions millions of dollars , which is a regular expression that matches suspiciously\nlarge sums of money. But it also includes features like HTML has a low ratio of text\nto image area , that aren\u2019t purely linguistic and might require some sophisticated\ncomputation, or totally non-linguistic features about, say, the path that the email\ntook to arrive. More sample SpamAssassin features:\n\u2022 Email subject line is all capital letters",
    "metadata": {
      "source": "4",
      "chunk_id": 12,
      "token_count": 780,
      "chapter_title": ""
    }
  },
  {
    "content": "4.5 Naive Bayes for other text classi\ufb01cation tasks\nIn the previous section we pointed out that naive Bayes doesn\u2019t require that our\nclassi\ufb01er use all the words in the training data as features. In fact features in naive\nBayes can express any property of the input text we want.\nConsider the task of spam detection , deciding if a particular piece of email is spam detection\nan example of spam (unsolicited bulk email)\u2014one of the \ufb01rst applications of naive\nBayes to text classi\ufb01cation (Sahami et al., 1998).\nA common solution here, rather than using all the words as individual features,\nis to prede\ufb01ne likely sets of words or phrases as features, combined with features\nthat are not purely linguistic. For example the open-source SpamAssassin tool2\nprede\ufb01nes features like the phrase \u201cone hundred percent guaranteed\u201d, or the feature\nmentions millions of dollars , which is a regular expression that matches suspiciously\nlarge sums of money. But it also includes features like HTML has a low ratio of text\nto image area , that aren\u2019t purely linguistic and might require some sophisticated\ncomputation, or totally non-linguistic features about, say, the path that the email\ntook to arrive. More sample SpamAssassin features:\n\u2022 Email subject line is all capital letters\n\u2022 Contains phrases of urgency like \u201curgent reply\u201d\n2https://spamassassin.apache.org",
    "metadata": {
      "source": "4",
      "chunk_id": 13,
      "token_count": 306,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\n\u2022 Email subject line contains \u201conline pharmaceutical\u201d\n\u2022 HTML has unbalanced \u201chead\u201d tags\n\u2022 Claims you can be removed from the list\nFor other tasks, like language id \u2014determining what language a given piece language id\nof text is written in\u2014the most effective naive Bayes features are not words at all,\nbutcharacter n-grams , 2-grams (\u2018zw\u2019) 3-grams (\u2018nya\u2019, \u2018 V o\u2019), or 4-grams (\u2018ie z\u2019,\n\u2018thei\u2019), or, even simpler byte n-grams , where instead of using the multibyte Unicode\ncharacter representations called codepoints, we just pretend everything is a string of\nraw bytes. Because spaces count as a byte, byte n-grams can model statistics about\nthe beginning or ending of words. A widely used naive Bayes system, langid.py\n(Lui and Baldwin, 2012) begins with all possible n-grams of lengths 1-4, using fea-\nture selection to winnow down to the most informative 7000 \ufb01nal features.\nLanguage ID systems are trained on multilingual text, such as Wikipedia (Wiki-\npedia text in 68 different languages was used in (Lui and Baldwin, 2011)), or newswire.\nTo make sure that this multilingual text correctly re\ufb02ects different regions, dialects,\nand socioeconomic classes, systems also add Twitter text in many languages geo-\ntagged to many regions (important for getting world English dialects from countries\nwith large Anglophone populations like Nigeria or India), Bible and Quran transla-\ntions, slang websites like Urban Dictionary, corpora of African American Vernacular\nEnglish (Blodgett et al., 2016), and so on (Jurgens et al., 2017).\n4.6 Naive Bayes as a Language Model\nAs we saw in the previous section, naive Bayes classi\ufb01ers can use any sort of feature:\ndictionaries, URLs, email addresses, network features, phrases, and so on. But if,\nas in Section 4.3, we use only individual word features, and we use all of the words\nin the text (not a subset), then naive Bayes has an important similarity to language\nmodeling. Speci\ufb01cally, a naive Bayes model can be viewed as a set of class-speci\ufb01c\nunigram language models, in which the model for each class instantiates a unigram\nlanguage model.\nSince the likelihood features from the naive Bayes model assign a probability to\neach word P(wordjc), the model also assigns a probability to each sentence:\nP(sjc) =Y\ni2positionsP(wijc) (4.15)\nThus consider a naive Bayes model with the classes positive (+) and negative (-)\nand the following model parameters:\nw P(wj+)P(wj-)\nI 0.1 0.2\nlove 0.1 0.001\nthis 0.01 0.01\nfun 0.05 0.005\n\ufb01lm 0.1 0.1\n... ... ...\nEach of the two columns above instantiates a language model that can assign a\nprobability to the sentence \u201cI love this fun \ufb01lm\u201d:",
    "metadata": {
      "source": "4",
      "chunk_id": 14,
      "token_count": 716,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11",
    "metadata": {
      "source": "4",
      "chunk_id": 15,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "4.7 \u2022 E VALUATION : PRECISION , RECALL , F- MEASURE 11\nP(\u201cI love this fun \ufb01lm\u201d j+) = 0:1\u00020:1\u00020:01\u00020:05\u00020:1=5\u000210\u00007\nP(\u201cI love this fun \ufb01lm\u201d j\u0000) = 0:2\u00020:001\u00020:01\u00020:005\u00020:1=1:0\u000210\u00009\nAs it happens, the positive model assigns a higher probability to the sentence:\nP(sjpos)>P(sjneg). Note that this is just the likelihood part of the naive Bayes\nmodel; once we multiply in the prior a full naive Bayes model might well make a\ndifferent classi\ufb01cation decision.\n4.7 Evaluation: Precision, Recall, F-measure\nTo introduce the methods for evaluating text classi\ufb01cation, let\u2019s \ufb01rst consider some\nsimple binary detection tasks. For example, in spam detection, our goal is to label\nevery text as being in the spam category (\u201cpositive\u201d) or not in the spam category\n(\u201cnegative\u201d). For each item (email document) we therefore need to know whether\nour system called it spam or not. We also need to know whether the email is actually\nspam or not, i.e. the human-de\ufb01ned labels for each document that we are trying to\nmatch. We will refer to these human labels as the gold labels . gold labels\nOr imagine you\u2019re the CEO of the Delicious Pie Company and you need to know\nwhat people are saying about your pies on social media, so you build a system that\ndetects tweets concerning Delicious Pie. Here the positive class is tweets about\nDelicious Pie and the negative class is all other tweets.\nIn both cases, we need a metric for knowing how well our spam detector (or\npie-tweet-detector) is doing. To evaluate any system for detecting things, we start\nby building a confusion matrix like the one shown in Fig. 4.4. A confusion matrixconfusion\nmatrix\nis a table for visualizing how an algorithm performs with respect to the human gold\nlabels, using two dimensions (system output and gold labels), and each cell labeling\na set of possible outcomes. In the spam detection case, for example, true positives\nare documents that are indeed spam (indicated by human-created gold labels) that\nour system correctly said were spam. False negatives are documents that are indeed\nspam but our system incorrectly labeled as non-spam.\nTo the bottom right of the table is the equation for accuracy , which asks what\npercentage of all the observations (for the spam or pie examples that means all emails\nor tweets) our system labeled correctly. Although accuracy might seem a natural\nmetric, we generally don\u2019t use it for text classi\ufb01cation tasks. That\u2019s because accuracy\ndoesn\u2019t work well when the classes are unbalanced (as indeed they are with spam,\nwhich is a large majority of email, or with tweets, which are mainly not about pie).\nTo make this more explicit, imagine that we looked at a million tweets, and\nlet\u2019s say that only 100 of them are discussing their love (or hatred) for our pie,\nwhile the other 999,900 are tweets about something completely unrelated. Imagine a\nsimple classi\ufb01er that stupidly classi\ufb01ed every tweet as \u201cnot about pie\u201d. This classi\ufb01er\nwould have 999,900 true negatives and only 100 false negatives for an accuracy of\n999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we should",
    "metadata": {
      "source": "4",
      "chunk_id": 16,
      "token_count": 775,
      "chapter_title": ""
    }
  },
  {
    "content": "are documents that are indeed spam (indicated by human-created gold labels) that\nour system correctly said were spam. False negatives are documents that are indeed\nspam but our system incorrectly labeled as non-spam.\nTo the bottom right of the table is the equation for accuracy , which asks what\npercentage of all the observations (for the spam or pie examples that means all emails\nor tweets) our system labeled correctly. Although accuracy might seem a natural\nmetric, we generally don\u2019t use it for text classi\ufb01cation tasks. That\u2019s because accuracy\ndoesn\u2019t work well when the classes are unbalanced (as indeed they are with spam,\nwhich is a large majority of email, or with tweets, which are mainly not about pie).\nTo make this more explicit, imagine that we looked at a million tweets, and\nlet\u2019s say that only 100 of them are discussing their love (or hatred) for our pie,\nwhile the other 999,900 are tweets about something completely unrelated. Imagine a\nsimple classi\ufb01er that stupidly classi\ufb01ed every tweet as \u201cnot about pie\u201d. This classi\ufb01er\nwould have 999,900 true negatives and only 100 false negatives for an accuracy of\n999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we should\nbe happy with this classi\ufb01er? But of course this fabulous \u2018no pie\u2019 classi\ufb01er would\nbe completely useless, since it wouldn\u2019t \ufb01nd a single one of the customer comments\nwe are looking for. In other words, accuracy is not a good metric when the goal is\nto discover something that is rare, or at least not completely balanced in frequency,\nwhich is a very common situation in the world.",
    "metadata": {
      "source": "4",
      "chunk_id": 17,
      "token_count": 370,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\ntrue positivefalse negativefalse positivetrue negativegold positivegold negativesystempositivesystemnegativegold standard labelssystemoutputlabelsrecall = tptp+fnprecision = tptp+fpaccuracy = tp+tntp+fp+tn+fn\nFigure 4.4 A confusion matrix for visualizing how well a binary classi\ufb01cation system per-\nforms against gold standard labels.\nThat\u2019s why instead of accuracy we generally turn to two other metrics shown in\nFig. 4.4: precision andrecall .Precision measures the percentage of the items that precision\nthe system detected (i.e., the system labeled as positive) that are in fact positive (i.e.,\nare positive according to the human gold labels). Precision is de\ufb01ned as\nPrecision =true positives\ntrue positives + false positives\nRecall measures the percentage of items actually present in the input that were recall\ncorrectly identi\ufb01ed by the system. Recall is de\ufb01ned as\nRecall =true positives\ntrue positives + false negatives\nPrecision and recall will help solve the problem with the useless \u201cnothing is\npie\u201d classi\ufb01er. This classi\ufb01er, despite having a fabulous accuracy of 99.99%, has\na terrible recall of 0 (since there are no true positives, and 100 false negatives, the\nrecall is 0/100). You should convince yourself that the precision at \ufb01nding relevant\ntweets is equally problematic. Thus precision and recall, unlike accuracy, emphasize\ntrue positives: \ufb01nding the things that we are supposed to be looking for.\nThere are many ways to de\ufb01ne a single metric that incorporates aspects of both\nprecision and recall. The simplest of these combinations is the F-measure (van F-measure\nRijsbergen, 1975) , de\ufb01ned as:\nFb=(b2+1)PR\nb2P+R\nThebparameter differentially weights the importance of recall and precision,\nbased perhaps on the needs of an application. Values of b>1 favor recall, while\nvalues of b<1 favor precision. When b=1, precision and recall are equally bal-\nanced; this is the most frequently used metric, and is called F b=1or just F 1: F1\nF1=2PR\nP+R(4.16)\nF-measure comes from a weighted harmonic mean of precision and recall. The\nharmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-\nrocals:\nHarmonicMean (a1;a2;a3;a4;:::;an) =n\n1\na1+1\na2+1\na3+:::+1\nan(4.17)",
    "metadata": {
      "source": "4",
      "chunk_id": 18,
      "token_count": 605,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\n4.7 \u2022 E VALUATION : PRECISION , RECALL , F- MEASURE 13\nand hence F-measure is\nF=1\na1\nP+ (1\u0000a)1\nRor\u0012\nwithb2=1\u0000a\na\u0013\nF=(b2+1)PR\nb2P+R(4.18)\nHarmonic mean is used because the harmonic mean of two values is closer to the\nminimum of the two values than the arithmetic mean is. Thus it weighs the lower of\nthe two numbers more heavily, which is more conservative in this situation.\n4.7.1 Evaluating with more than two classes\nUp to now we have been describing text classi\ufb01cation tasks with only two classes.\nBut lots of classi\ufb01cation tasks in language processing have more than two classes.\nFor sentiment analysis we generally have 3 classes (positive, negative, neutral) and\neven more classes are common for tasks like part-of-speech tagging, word sense\ndisambiguation, semantic role labeling, emotion detection, and so on. Luckily the\nnaive Bayes algorithm is already a multi-class classi\ufb01cation algorithm.\n851060urgentnormalgold labelssystemoutputrecallu = 88+5+3precisionu= 88+10+115030200spamurgentnormalspam3recalln = recalls = precisionn= 605+60+50precisions= 2003+30+2006010+60+302001+50+200\nFigure 4.5 Confusion matrix for a three-class categorization task, showing for each pair of\nclasses (c1;c2), how many documents from c1were (in)correctly assigned to c2.\nBut we\u2019ll need to slightly modify our de\ufb01nitions of precision and recall. Con-\nsider the sample confusion matrix for a hypothetical 3-way one-of email catego-\nrization decision (urgent, normal, spam) shown in Fig. 4.5. The matrix shows, for\nexample, that the system mistakenly labeled one spam document as urgent, and we\nhave shown how to compute a distinct precision and recall value for each class. In\norder to derive a single metric that tells us how well the system is doing, we can com-\nbine these values in two ways. In macroaveraging , we compute the performance macroaveraging\nfor each class, and then average over classes. In microaveraging , we collect the de- microaveraging\ncisions for all classes into a single confusion matrix, and then compute precision and\nrecall from that table. Fig. 4.6 shows the confusion matrix for each class separately,\nand shows the computation of microaveraged and macroaveraged precision.\nAs the \ufb01gure shows, a microaverage is dominated by the more frequent class (in\nthis case spam), since the counts are pooled. The macroaverage better re\ufb02ects the\nstatistics of the smaller classes, and so is more appropriate when performance on all\nthe classes is equally important.",
    "metadata": {
      "source": "4",
      "chunk_id": 19,
      "token_count": 648,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n14 CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\n8811340trueurgenttruenotsystemurgentsystemnot604055212truenormaltruenotsystemnormalsystemnot200513383truespamtruenotsystemspamsystemnot2689999635trueyestruenosystemyessystemnoprecision =8+118= .42precision =200+33200= .86precision =60+5560= .52microaverageprecision268+99268= .73=macroaverageprecision3.42+.52+.86= .60=PooledClass 3: SpamClass 2: NormalClass 1: Urgent\nFigure 4.6 Separate confusion matrices for the 3 classes from the previous \ufb01gure, showing the pooled confu-\nsion matrix and the microaveraged and macroaveraged precision.\n4.8 Test sets and Cross-validation\nThe training and testing procedure for text classi\ufb01cation follows what we saw with\nlanguage modeling (Section ??): we use the training set to train the model, then use\nthedevelopment test set (also called a devset ) to perhaps tune some parameters,development\ntest set\ndevset and in general decide what the best model is. Once we come up with what we think\nis the best model, we run it on the (hitherto unseen) test set to report its performance.\nWhile the use of a devset avoids over\ufb01tting the test set, having a \ufb01xed train-\ning set, devset, and test set creates another problem: in order to save lots of data\nfor training, the test set (or devset) might not be large enough to be representative.\nWouldn\u2019t it be better if we could somehow use all our data for training and still use\nall our data for test? We can do this by cross-validation . cross-validation\nIn cross-validation, we choose a number k, and partition our data into kdisjoint\nsubsets called folds . Now we choose one of those kfolds as a test set, train our folds\nclassi\ufb01er on the remaining k\u00001 folds, and then compute the error rate on the test\nset. Then we repeat with another fold as the test set, again training on the other k\u00001\nfolds. We do this sampling process ktimes and average the test set error rate from\nthese kruns to get an average error rate. If we choose k=10, we would train 10\ndifferent models (each on 90% of our data), test the model 10 times, and average\nthese 10 values. This is called 10-fold cross-validation .10-fold\ncross-validation\nThe only problem with cross-validation is that because all the data is used for\ntesting, we need the whole corpus to be blind; we can\u2019t examine any of the data\nto suggest possible features and in general see what\u2019s going on, because we\u2019d be\npeeking at the test set, and such cheating would cause us to overestimate the perfor-\nmance of our system. However, looking at the corpus to understand what\u2019s going\non is important in designing NLP systems! What to do? For this reason, it is com-\nmon to create a \ufb01xed training set and test set, then do 10-fold cross-validation inside\nthe training set, but compute error rate the normal way in the test set, as shown in\nFig. 4.7.",
    "metadata": {
      "source": "4",
      "chunk_id": 20,
      "token_count": 737,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\n4.9 \u2022 S TATISTICAL SIGNIFICANCE TESTING 15\nTraining Iterations13452678910DevDevDevDevDevDevDevDevDevDevTrainingTrainingTrainingTrainingTrainingTrainingTrainingTrainingTrainingTrainingTrainingTest SetTesting\nFigure 4.7 10-fold cross-validation\n4.9 Statistical Signi\ufb01cance Testing\nIn building systems we often need to compare the performance of two systems. How\ncan we know if the new system we just built is better than our old one? Or better\nthan some other system described in the literature? This is the domain of statistical\nhypothesis testing, and in this section we introduce tests for statistical signi\ufb01cance\nfor NLP classi\ufb01ers, drawing especially on the work of Dror et al. (2020) and Berg-\nKirkpatrick et al. (2012).\nSuppose we\u2019re comparing the performance of classi\ufb01ers AandBon a metric M\nsuch as F 1, or accuracy. Perhaps we want to know if our logistic regression senti-\nment classi\ufb01er A(Chapter 5) gets a higher F 1score than our naive Bayes sentiment\nclassi\ufb01er Bon a particular test set x. Let\u2019s call M(A;x)the score that system Agets\non test set x, and d(x)the performance difference between AandBonx:\nd(x) =M(A;x)\u0000M(B;x) (4.19)\nWe would like to know if d(x)>0, meaning that our logistic regression classi\ufb01er\nhas a higher F 1than our naive Bayes classi\ufb01er on x.d(x)is called the effect size ; a effect size\nbigger dmeans that Aseems to be way better than B; a small dmeans Aseems to\nbe only a little better.\nWhy don\u2019t we just check if d(x)is positive? Suppose we do, and we \ufb01nd that\nthe F 1score of Ais higher than B\u2019s by .04. Can we be certain that Ais better? We\ncannot! That\u2019s because Amight just be accidentally better than Bon this particular x.\nWe need something more: we want to know if A\u2019s superiority over Bis likely to hold\nagain if we checked another test set x0, or under some other set of circumstances.\nIn the paradigm of statistical hypothesis testing, we test this by formalizing two\nhypotheses.\nH0:d(x)\u00140\nH1:d(x)>0 (4.20)\nThe hypothesis H0, called the null hypothesis , supposes that d(x)is actually nega- null hypothesis\ntive or zero, meaning that Ais not better than B. We would like to know if we can\ncon\ufb01dently rule out this hypothesis, and instead support H1, that Ais better.\nWe do this by creating a random variable Xranging over all test sets. Now we\nask how likely is it, if the null hypothesis H0was correct, that among these test sets",
    "metadata": {
      "source": "4",
      "chunk_id": 21,
      "token_count": 649,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16",
    "metadata": {
      "source": "4",
      "chunk_id": 22,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "16 CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\nwe would encounter the value of d(x)that we found, if we repeated the experiment\na great many times. We formalize this likelihood as the p-value : the probability, p-value\nassuming the null hypothesis H0is true, of seeing the d(x)that we saw or one even\ngreater\nP(d(X)\u0015d(x)jH0is true ) (4.21)\nSo in our example, this p-value is the probability that we would see d(x)assuming\nAisnotbetter than B. Ifd(x)is huge (let\u2019s say Ahas a very respectable F 1of .9\nandBhas a terrible F 1of only .2 on x), we might be surprised, since that would be\nextremely unlikely to occur if H0were in fact true, and so the p-value would be low\n(unlikely to have such a large difAis in fact not better than B). But if d(x)is very\nsmall, it might be less surprising to us even if H0were true and Ais not really better\nthan B, and so the p-value would be higher.\nA very small p-value means that the difference we observed is very unlikely\nunder the null hypothesis, and we can reject the null hypothesis. What counts as very\nsmall? It is common to use values like .05 or .01 as the thresholds. A value of .01\nmeans that if the p-value (the probability of observing the dwe saw assuming H0is\ntrue) is less than .01, we reject the null hypothesis and assume that Ais indeed better\nthan B. We say that a result (e.g., \u201c Ais better than B\u201d) is statistically signi\ufb01cant ifstatistically\nsigni\ufb01cant\nthedwe saw has a probability that is below the threshold and we therefore reject\nthis null hypothesis.\nHow do we compute this probability we need for the p-value? In NLP we gen-\nerally don\u2019t use simple parametric tests like t-tests or ANOV As that you might be\nfamiliar with. Parametric tests make assumptions about the distributions of the test\nstatistic (such as normality) that don\u2019t generally hold in our cases. So in NLP we\nusually use non-parametric tests based on sampling: we arti\ufb01cially create many ver-\nsions of the experimental setup. For example, if we had lots of different test sets x0\nwe could just measure all the d(x0)for all the x0. That gives us a distribution. Now\nwe set a threshold (like .01) and if we see in this distribution that 99% or more of\nthose deltas are smaller than the delta we observed, i.e., that p-value( x)\u2014the proba-\nbility of seeing a d(x)as big as the one we saw\u2014is less than .01, then we can reject\nthe null hypothesis and agree that d(x)was a suf\ufb01ciently surprising difference and\nAis really a better algorithm than B.\nThere are two common non-parametric tests used in NLP: approximate ran-\ndomization (Noreen, 1989) and the bootstrap test . We will describe bootstrapapproximate\nrandomization\nbelow, showing the paired version of the test, which again is most common in NLP.\nPaired tests are those in which we compare two sets of observations that are aligned: paired\neach observation in one set can be paired with an observation in another. This hap-\npens naturally when we are comparing the performance of two systems on the same\ntest set; we can pair the performance of system Aon an individual observation xi",
    "metadata": {
      "source": "4",
      "chunk_id": 23,
      "token_count": 788,
      "chapter_title": ""
    }
  },
  {
    "content": "sions of the experimental setup. For example, if we had lots of different test sets x0\nwe could just measure all the d(x0)for all the x0. That gives us a distribution. Now\nwe set a threshold (like .01) and if we see in this distribution that 99% or more of\nthose deltas are smaller than the delta we observed, i.e., that p-value( x)\u2014the proba-\nbility of seeing a d(x)as big as the one we saw\u2014is less than .01, then we can reject\nthe null hypothesis and agree that d(x)was a suf\ufb01ciently surprising difference and\nAis really a better algorithm than B.\nThere are two common non-parametric tests used in NLP: approximate ran-\ndomization (Noreen, 1989) and the bootstrap test . We will describe bootstrapapproximate\nrandomization\nbelow, showing the paired version of the test, which again is most common in NLP.\nPaired tests are those in which we compare two sets of observations that are aligned: paired\neach observation in one set can be paired with an observation in another. This hap-\npens naturally when we are comparing the performance of two systems on the same\ntest set; we can pair the performance of system Aon an individual observation xi\nwith the performance of system Bon the same xi.\n4.9.1 The Paired Bootstrap Test\nThebootstrap test (Efron and Tibshirani, 1993) can apply to any metric; from pre- bootstrap test\ncision, recall, or F1 to the BLEU metric used in machine translation. The word\nbootstrapping refers to repeatedly drawing large numbers of samples with replace- bootstrapping\nment (called bootstrap samples ) from an original set. The intuition of the bootstrap\ntest is that we can create many virtual test sets from an observed test set by repeat-\nedly sampling from it. The method only makes the assumption that the sample is\nrepresentative of the population.",
    "metadata": {
      "source": "4",
      "chunk_id": 24,
      "token_count": 422,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17",
    "metadata": {
      "source": "4",
      "chunk_id": 25,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "4.9 \u2022 S TATISTICAL SIGNIFICANCE TESTING 17\nConsider a tiny text classi\ufb01cation example with a test set xof 10 documents. The\n\ufb01rst row of Fig. 4.8 shows the results of two classi\ufb01ers (A and B) on this test set.\nEach document is labeled by one of the four possibilities (A and B both right, both\nwrong, A right and B wrong, A wrong and B right). A slash through a letter ( \u0013B)\nmeans that that classi\ufb01er got the answer wrong. On the \ufb01rst document both A and\nB get the correct class (AB), while on the second document A got it right but B got\nit wrong (A \u0013B). If we assume for simplicity that our metric is accuracy, A has an\naccuracy of .70 and B of .50, so d(x)is .20.\nNow we create a large number b(perhaps 105) of virtual test sets x(i), each of size\nn=10. Fig. 4.8 shows a couple of examples. To create each virtual test set x(i), we\nrepeatedly ( n=10 times) select a cell from row xwith replacement. For example, to\ncreate the \ufb01rst cell of the \ufb01rst virtual test set x(1), if we happened to randomly select\nthe second cell of the xrow; we would copy the value A \u0013B into our new cell, and\nmove on to create the second cell of x(1), each time sampling (randomly choosing)\nfrom the original xwith replacement.\n1 2 3 4 5 6 7 8 910A% B% d()\nx AB A\u0013\u0013BAB\u0000\u0000AB A\u0013\u0013B\u0000\u0000AB A\u0013\u0013BAB\u0000\u0000A\u0013\u0013BA\u0013\u0013B.70 .50 .20\nx(1)A\u0013\u0013BAB A\u0013\u0013B\u0000\u0000AB\u0000\u0000AB A\u0013\u0013B\u0000\u0000AB AB\u0000\u0000A\u0013\u0013BAB .60 .60 .00\nx(2)A\u0013\u0013BAB\u0000\u0000A\u0013\u0013B\u0000\u0000AB\u0000\u0000AB AB\u0000\u0000AB A\u0013\u0013BAB AB .60 .70-.10\n...\nx(b)\nFigure 4.8 The paired bootstrap test: Examples of bpseudo test sets x(i)being created\nfrom an initial true test set x. Each pseudo test set is created by sampling n=10 times with\nreplacement; thus an individual sample is a single cell, a document with its gold label and\nthe correct or incorrect performance of classi\ufb01ers A and B. Of course real test sets don\u2019t have\nonly 10 examples, and bneeds to be large as well.\nNow that we have the btest sets, providing a sampling distribution, we can do\nstatistics on how often Ahas an accidental advantage. There are various ways to\ncompute this advantage; here we follow the version laid out in Berg-Kirkpatrick\net al. (2012). Assuming H0(Aisn\u2019t better than B), we would expect that d(X),\nestimated over many test sets, would be zero or negative; a much higher value would\nbe surprising, since H0speci\ufb01cally assumes Aisn\u2019t better than B. To measure exactly\nhow surprising our observed d(x)is, we would in other circumstances compute the\np-value by counting over many test sets how often d(x(i))exceeds the expected zero\nvalue by d(x)or more:\np-value (x) =1\nbbX\ni=11\u0010\nd(x(i))\u0000d(x)\u00150\u0011",
    "metadata": {
      "source": "4",
      "chunk_id": 26,
      "token_count": 777,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 4.8 The paired bootstrap test: Examples of bpseudo test sets x(i)being created\nfrom an initial true test set x. Each pseudo test set is created by sampling n=10 times with\nreplacement; thus an individual sample is a single cell, a document with its gold label and\nthe correct or incorrect performance of classi\ufb01ers A and B. Of course real test sets don\u2019t have\nonly 10 examples, and bneeds to be large as well.\nNow that we have the btest sets, providing a sampling distribution, we can do\nstatistics on how often Ahas an accidental advantage. There are various ways to\ncompute this advantage; here we follow the version laid out in Berg-Kirkpatrick\net al. (2012). Assuming H0(Aisn\u2019t better than B), we would expect that d(X),\nestimated over many test sets, would be zero or negative; a much higher value would\nbe surprising, since H0speci\ufb01cally assumes Aisn\u2019t better than B. To measure exactly\nhow surprising our observed d(x)is, we would in other circumstances compute the\np-value by counting over many test sets how often d(x(i))exceeds the expected zero\nvalue by d(x)or more:\np-value (x) =1\nbbX\ni=11\u0010\nd(x(i))\u0000d(x)\u00150\u0011\n(We use the notation 1(x)to mean \u201c1 if xis true, and 0 otherwise\u201d.) However,\nalthough it\u2019s generally true that the expected value of d(X)over many test sets,\n(again assuming Aisn\u2019t better than B) is 0, this isn\u2019t true for the bootstrapped test\nsets we created. That\u2019s because we didn\u2019t draw these samples from a distribution\nwith 0 mean; we happened to create them from the original test set x, which happens\nto be biased (by .20) in favor of A. So to measure how surprising is our observed\nd(x), we actually compute the p-value by counting over many test sets how often",
    "metadata": {
      "source": "4",
      "chunk_id": 27,
      "token_count": 440,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\n18 CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\nd(x(i))exceeds the expected value of d(x)byd(x)or more:\np-value (x) =1\nbbX\ni=11\u0010\nd(x(i))\u0000d(x)\u0015d(x)\u0011\n=1\nbbX\ni=11\u0010\nd(x(i))\u00152d(x)\u0011\n(4.22)\nSo if for example we have 10,000 test sets x(i)and a threshold of .01, and in only 47\nof the test sets do we \ufb01nd that A is accidentally better d(x(i))\u00152d(x), the resulting\np-value of .0047 is smaller than .01, indicating that the delta we found, d(x)is indeed\nsuf\ufb01ciently surprising and unlikely to have happened by accident, and we can reject\nthe null hypothesis and conclude Ais better than B.\nfunction BOOTSTRAP (test set x,num of samples b)returns p-value (x)\nCalculate d(x)# how much better does algorithm A do than B on x\ns= 0\nfori= 1tobdo\nforj= 1tondo # Draw a bootstrap sample x(i)of size n\nSelect a member of xat random and add it to x(i)\nCalculate d(x(i))# how much better does algorithm A do than B on x(i)\ns s+ 1ifd(x(i))\u00152d(x)\np-value( x)\u0019s\nb# on what % of the b samples did algorithm A beat expectations?\nreturn p-value( x) # if very few did, our observed dis probably not accidental\nFigure 4.9 A version of the paired bootstrap algorithm after Berg-Kirkpatrick et al. (2012).\nThe full algorithm for the bootstrap is shown in Fig. 4.9. It is given a test set x, a\nnumber of samples b, and counts the percentage of the bbootstrap test sets in which\nd(x\u0003(i))>2d(x). This percentage then acts as a one-sided empirical p-value.\n4.10 Avoiding Harms in Classi\ufb01cation\nIt is important to avoid harms that may result from classi\ufb01ers, harms that exist both\nfor naive Bayes classi\ufb01ers and for the other classi\ufb01cation algorithms we introduce\nin later chapters.\nOne class of harms is representational harms (Crawford 2017, Blodgett et al.representational\nharms\n2020), harms caused by a system that demeans a social group, for example by per-\npetuating negative stereotypes about them. For example Kiritchenko and Moham-\nmad (2018) examined the performance of 200 sentiment analysis systems on pairs of\nsentences that were identical except for containing either a common African Amer-\nican \ufb01rst name (like Shaniqua ) or a common European American \ufb01rst name (like\nStephanie ), chosen from the Caliskan et al. (2017) study discussed in Chapter 6.\nThey found that most systems assigned lower sentiment and more negative emotion\nto sentences with African American names, re\ufb02ecting and perpetuating stereotypes\nthat associate African Americans with negative emotions (Popp et al., 2003).\nIn other tasks classi\ufb01ers may lead to both representational harms and other\nharms, such as silencing. For example the important text classi\ufb01cation task of tox-",
    "metadata": {
      "source": "4",
      "chunk_id": 28,
      "token_count": 755,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\n4.11 \u2022 S UMMARY 19\nicity detection is the task of detecting hate speech, abuse, harassment, or othertoxicity\ndetection\nkinds of toxic language. While the goal of such classi\ufb01ers is to help reduce soci-\netal harm, toxicity classi\ufb01ers can themselves cause harms. For example, researchers\nhave shown that some widely used toxicity classi\ufb01ers incorrectly \ufb02ag as being toxic\nsentences that are non-toxic but simply mention identities like women (Park et al.,\n2018), blind people (Hutchinson et al., 2020) or gay people (Dixon et al., 2018;\nDias Oliva et al., 2021), or simply use linguistic features characteristic of varieties\nlike African-American Vernacular English (Sap et al. 2019, Davidson et al. 2019).\nSuch false positive errors could lead to the silencing of discourse by or about these\ngroups.\nThese model problems can be caused by biases or other problems in the training\ndata; in general, machine learning systems replicate and even amplify the biases\nin their training data. But these problems can also be caused by the labels (for\nexample due to biases in the human labelers), by the resources used (like lexicons,\nor model components like pretrained embeddings), or even by model architecture\n(like what the model is trained to optimize). While the mitigation of these biases\n(for example by carefully considering the training data sources) is an important area\nof research, we currently don\u2019t have general solutions. For this reason it\u2019s important,\nwhen introducing any NLP model, to study these kinds of factors and make them\nclear. One way to do this is by releasing a model card (Mitchell et al., 2019) for model card\neach version of a model. A model card documents a machine learning model with\ninformation like:\n\u2022 training algorithms and parameters\n\u2022 training data sources, motivation, and preprocessing\n\u2022 evaluation data sources, motivation, and preprocessing\n\u2022 intended use and users\n\u2022 model performance across different demographic or other groups and envi-\nronmental situations\n4.11 Summary\nThis chapter introduced the naive Bayes model for classi\ufb01cation and applied it to\nthetext categorization task of sentiment analysis .\n\u2022 Many language processing tasks can be viewed as tasks of classi\ufb01cation .\n\u2022 Text categorization, in which an entire text is assigned a class from a \ufb01nite set,\nincludes such tasks as sentiment analysis ,spam detection , language identi-\n\ufb01cation, and authorship attribution.\n\u2022 Sentiment analysis classi\ufb01es a text as re\ufb02ecting the positive or negative orien-\ntation ( sentiment ) that a writer expresses toward some object.\n\u2022 Naive Bayes is a generative model that makes the bag-of-words assumption\n(position doesn\u2019t matter) and the conditional independence assumption (words\nare conditionally independent of each other given the class)\n\u2022 Naive Bayes with binarized features seems to work better for many text clas-\nsi\ufb01cation tasks.\n\u2022 Classi\ufb01ers are evaluated based on precision andrecall .\n\u2022 Classi\ufb01ers are trained using distinct training, dev, and test sets, including the\nuse of cross-validation in the training set.",
    "metadata": {
      "source": "4",
      "chunk_id": 29,
      "token_count": 698,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20",
    "metadata": {
      "source": "4",
      "chunk_id": 30,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "20 CHAPTER 4 \u2022 N AIVE BAYES , TEXT CLASSIFICATION ,AND SENTIMENT\n\u2022 Statistical signi\ufb01cance tests should be used to determine whether we can be\ncon\ufb01dent that one version of a classi\ufb01er is better than another.\n\u2022 Designers of classi\ufb01ers should carefully consider harms that may be caused\nby the model, including its training data and other components, and report\nmodel characteristics in a model card .\nBibliographical and Historical Notes\nMultinomial naive Bayes text classi\ufb01cation was proposed by Maron (1961) at the\nRAND Corporation for the task of assigning subject categories to journal abstracts.\nHis model introduced most of the features of the modern form presented here, ap-\nproximating the classi\ufb01cation task with one-of categorization, and implementing\nadd-dsmoothing and information-based feature selection.\nThe conditional independence assumptions of naive Bayes and the idea of Bayes-\nian analysis of text seems to have arisen multiple times. The same year as Maron\u2019s\npaper, Minsky (1961) proposed a naive Bayes classi\ufb01er for vision and other arti-\n\ufb01cial intelligence problems, and Bayesian techniques were also applied to the text\nclassi\ufb01cation task of authorship attribution by Mosteller and Wallace (1963). It had\nlong been known that Alexander Hamilton, John Jay, and James Madison wrote\nthe anonymously-published Federalist papers in 1787\u20131788 to persuade New York\nto ratify the United States Constitution. Yet although some of the 85 essays were\nclearly attributable to one author or another, the authorship of 12 were in dispute\nbetween Hamilton and Madison. Mosteller and Wallace (1963) trained a Bayesian\nprobabilistic model of the writing of Hamilton and another model on the writings\nof Madison, then computed the maximum-likelihood author for each of the disputed\nessays. Naive Bayes was \ufb01rst applied to spam detection in Heckerman et al. (1998).\nMetsis et al. (2006), Pang et al. (2002), and Wang and Manning (2012) show\nthat using boolean attributes with multinomial naive Bayes works better than full\ncounts. Binary multinomial naive Bayes is sometimes confused with another variant\nof naive Bayes that also uses a binary representation of whether a term occurs in\na document: Multivariate Bernoulli naive Bayes . The Bernoulli variant instead\nestimates P(wjc)as the fraction of documents that contain a term, and includes a\nprobability for whether a term is notin a document. McCallum and Nigam (1998)\nand Wang and Manning (2012) show that the multivariate Bernoulli variant of naive\nBayes doesn\u2019t work as well as the multinomial algorithm for sentiment or other text\ntasks.\nThere are a variety of sources covering the many kinds of text classi\ufb01cation\ntasks. For sentiment analysis see Pang and Lee (2008), and Liu and Zhang (2012).\nStamatatos (2009) surveys authorship attribute algorithms. On language identi\ufb01ca-\ntion see Jauhiainen et al. (2019); Jaech et al. (2016) is an important early neural\nsystem. The task of newswire indexing was often used as a test case for text classi-\n\ufb01cation algorithms, based on the Reuters-21578 collection of newswire articles.\nSee Manning et al. (2008) and Aggarwal and Zhai (2012) on text classi\ufb01cation;\nclassi\ufb01cation in general is covered in machine learning textbooks (Hastie et al. 2001,",
    "metadata": {
      "source": "4",
      "chunk_id": 31,
      "token_count": 783,
      "chapter_title": ""
    }
  },
  {
    "content": "a document: Multivariate Bernoulli naive Bayes . The Bernoulli variant instead\nestimates P(wjc)as the fraction of documents that contain a term, and includes a\nprobability for whether a term is notin a document. McCallum and Nigam (1998)\nand Wang and Manning (2012) show that the multivariate Bernoulli variant of naive\nBayes doesn\u2019t work as well as the multinomial algorithm for sentiment or other text\ntasks.\nThere are a variety of sources covering the many kinds of text classi\ufb01cation\ntasks. For sentiment analysis see Pang and Lee (2008), and Liu and Zhang (2012).\nStamatatos (2009) surveys authorship attribute algorithms. On language identi\ufb01ca-\ntion see Jauhiainen et al. (2019); Jaech et al. (2016) is an important early neural\nsystem. The task of newswire indexing was often used as a test case for text classi-\n\ufb01cation algorithms, based on the Reuters-21578 collection of newswire articles.\nSee Manning et al. (2008) and Aggarwal and Zhai (2012) on text classi\ufb01cation;\nclassi\ufb01cation in general is covered in machine learning textbooks (Hastie et al. 2001,\nWitten and Frank 2005, Bishop 2006, Murphy 2012).\nNon-parametric methods for computing statistical signi\ufb01cance were used \ufb01rst in\nNLP in the MUC competition (Chinchor et al., 1993), and even earlier in speech\nrecognition (Gillick and Cox 1989, Bisani and Ney 2004). Our description of the\nbootstrap draws on the description in Berg-Kirkpatrick et al. (2012). Recent work\nhas focused on issues including multiple test sets and multiple metrics (S\u00f8gaard et al.",
    "metadata": {
      "source": "4",
      "chunk_id": 32,
      "token_count": 403,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21\n\nEXERCISES 21\n2014, Dror et al. 2017).\nFeature selection is a method of removing features that are unlikely to generalize\nwell. Features are generally ranked by how informative they are about the classi\ufb01ca-\ntion decision. A very common metric, information gain , tells us how many bits ofinformation\ngain\ninformation the presence of the word gives us for guessing the class. Other feature\nselection metrics include c2, pointwise mutual information, and GINI index; see\nYang and Pedersen (1997) for a comparison and Guyon and Elisseeff (2003) for an\nintroduction to feature selection.\nExercises\n4.1 Assume the following likelihoods for each word being part of a positive or\nnegative movie review, and equal prior probabilities for each class.\npos neg\nI 0.09 0.16\nalways 0.07 0.06\nlike 0.29 0.06\nforeign 0.04 0.15\n\ufb01lms 0.08 0.11\nWhat class will Naive bayes assign to the sentence \u201cI always like foreign\n\ufb01lms.\u201d?\n4.2 Given the following short movie reviews, each labeled with a genre, either\ncomedy or action:\n1. fun, couple, love, love comedy\n2. fast, furious, shoot action\n3. couple, \ufb02y, fast, fun, fun comedy\n4. furious, shoot, shoot, fun action\n5. \ufb02y, fast, shoot, love action\nand a new document D:\nfast, couple, shoot, \ufb02y\ncompute the most likely class for D. Assume a naive Bayes classi\ufb01er and use\nadd-1 smoothing for the likelihoods.\n4.3 Train two models, multinomial naive Bayes and binarized naive Bayes, both\nwith add-1 smoothing, on the following document counts for key sentiment\nwords, with positive or negative class assigned as noted.\ndoc \u201cgood\u201d \u201cpoor\u201d \u201cgreat\u201d (class)\nd1. 3 0 3 pos\nd2. 0 1 2 pos\nd3. 1 3 0 neg\nd4. 1 5 2 neg\nd5. 0 2 0 neg\nUse both naive Bayes models to assign a class (pos or neg) to this sentence:\nA good, good plot and great characters, but poor acting.\nRecall from page 6 that with naive Bayes text classi\ufb01cation, we simply ignore\n(throw out) any word that never occurred in the training document. (We don\u2019t\nthrow out words that appear in some classes but not others; that\u2019s what add-\none smoothing is for.) Do the two models agree or disagree?",
    "metadata": {
      "source": "4",
      "chunk_id": 33,
      "token_count": 606,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22",
    "metadata": {
      "source": "4",
      "chunk_id": 34,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "22 Chapter 4 \u2022 Naive Bayes, Text Classi\ufb01cation, and Sentiment\nAggarwal, C. C. and C. Zhai. 2012. A survey of text classi-\n\ufb01cation algorithms. In C. C. Aggarwal and C. Zhai, eds,\nMining text data , 163\u2013222. Springer.\nBayes, T. 1763. An Essay Toward Solving a Problem in the\nDoctrine of Chances , volume 53. Reprinted in Facsimiles\nof Two Papers by Bayes , Hafner Publishing, 1963.\nBerg-Kirkpatrick, T., D. Burkett, and D. Klein. 2012. An\nempirical investigation of statistical signi\ufb01cance in NLP.\nEMNLP .\nBisani, M. and H. Ney. 2004. Bootstrap estimates for con\ufb01-\ndence intervals in ASR performance evaluation. ICASSP .\nBishop, C. M. 2006. Pattern recognition and machine learn-\ning. Springer.\nBlodgett, S. L., S. Barocas, H. Daum \u00b4e III, and H. Wallach.\n2020. Language (technology) is power: A critical survey\nof \u201cbias\u201d in NLP. ACL.\nBlodgett, S. L., L. Green, and B. O\u2019Connor. 2016. Demo-\ngraphic dialectal variation in social media: A case study\nof African-American English. EMNLP .\nBorges, J. L. 1964. The analytical language of john wilkins.\nInOther inquisitions 1937\u20131952 . University of Texas\nPress. Trans. Ruth L. C. Simms.\nCaliskan, A., J. J. Bryson, and A. Narayanan. 2017. Seman-\ntics derived automatically from language corpora contain\nhuman-like biases. Science , 356(6334):183\u2013186.\nChinchor, N., L. Hirschman, and D. L. Lewis. 1993. Eval-\nuating Message Understanding systems: An analysis of\nthe third Message Understanding Conference. Computa-\ntional Linguistics , 19(3):409\u2013449.\nCrawford, K. 2017. The trouble with bias. Keynote at\nNeurIPS.\nDavidson, T., D. Bhattacharya, and I. Weber. 2019. Racial\nbias in hate speech and abusive language detection\ndatasets. Third Workshop on Abusive Language Online .\nDias Oliva, T., D. Antonialli, and A. Gomes. 2021. Fighting\nhate speech, silencing drag queens? arti\ufb01cial intelligence\nin content moderation and risks to lgbtq voices online.\nSexuality & Culture , 25:700\u2013732.\nDixon, L., J. Li, J. Sorensen, N. Thain, and L. Vasserman.\n2018. Measuring and mitigating unintended bias in text\nclassi\ufb01cation. 2018 AAAI/ACM Conference on AI, Ethics,\nand Society .\nDror, R., G. Baumer, M. Bogomolov, and R. Reichart. 2017.\nReplicability analysis for natural language processing:\nTesting signi\ufb01cance with multiple datasets. TACL , 5:471\u2013\n\u2013486.\nDror, R., L. Peled-Cohen, S. Shlomov, and R. Reichart.",
    "metadata": {
      "source": "4",
      "chunk_id": 35,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "Crawford, K. 2017. The trouble with bias. Keynote at\nNeurIPS.\nDavidson, T., D. Bhattacharya, and I. Weber. 2019. Racial\nbias in hate speech and abusive language detection\ndatasets. Third Workshop on Abusive Language Online .\nDias Oliva, T., D. Antonialli, and A. Gomes. 2021. Fighting\nhate speech, silencing drag queens? arti\ufb01cial intelligence\nin content moderation and risks to lgbtq voices online.\nSexuality & Culture , 25:700\u2013732.\nDixon, L., J. Li, J. Sorensen, N. Thain, and L. Vasserman.\n2018. Measuring and mitigating unintended bias in text\nclassi\ufb01cation. 2018 AAAI/ACM Conference on AI, Ethics,\nand Society .\nDror, R., G. Baumer, M. Bogomolov, and R. Reichart. 2017.\nReplicability analysis for natural language processing:\nTesting signi\ufb01cance with multiple datasets. TACL , 5:471\u2013\n\u2013486.\nDror, R., L. Peled-Cohen, S. Shlomov, and R. Reichart.\n2020. Statistical Signi\ufb01cance Testing for Natural Lan-\nguage Processing , volume 45 of Synthesis Lectures on\nHuman Language Technologies . Morgan & Claypool.\nEfron, B. and R. J. Tibshirani. 1993. An introduction to the\nbootstrap . CRC press.\nGillick, L. and S. J. Cox. 1989. Some statistical issues in the\ncomparison of speech recognition algorithms. ICASSP .\nGuyon, I. and A. Elisseeff. 2003. An introduction to variable\nand feature selection. JMLR , 3:1157\u20131182.\nHastie, T., R. J. Tibshirani, and J. H. Friedman. 2001. The\nElements of Statistical Learning . Springer.Heckerman, D., E. Horvitz, M. Sahami, and S. T. Dumais.\n1998. A bayesian approach to \ufb01ltering junk e-mail. AAAI-\n98 Workshop on Learning for Text Categorization .\nHu, M. and B. Liu. 2004. Mining and summarizing customer\nreviews. KDD .\nHutchinson, B., V . Prabhakaran, E. Denton, K. Webster,\nY . Zhong, and S. Denuyl. 2020. Social biases in NLP\nmodels as barriers for persons with disabilities. ACL.\nJaech, A., G. Mulcaire, S. Hathi, M. Ostendorf, and N. A.\nSmith. 2016. Hierarchical character-word models for lan-\nguage identi\ufb01cation. ACL Workshop on NLP for Social\nMedia .\nJauhiainen, T., M. Lui, M. Zampieri, T. Baldwin, and\nK. Lind \u00b4en. 2019. Automatic language identi\ufb01cation in\ntexts: A survey. JAIR , 65(1):675\u2013682.\nJurgens, D., Y . Tsvetkov, and D. Jurafsky. 2017. Incorpo-\nrating dialectal variability for socially equitable language\nidenti\ufb01cation. ACL.\nKiritchenko, S. and S. M. Mohammad. 2018. Examining",
    "metadata": {
      "source": "4",
      "chunk_id": 36,
      "token_count": 767,
      "chapter_title": ""
    }
  },
  {
    "content": "98 Workshop on Learning for Text Categorization .\nHu, M. and B. Liu. 2004. Mining and summarizing customer\nreviews. KDD .\nHutchinson, B., V . Prabhakaran, E. Denton, K. Webster,\nY . Zhong, and S. Denuyl. 2020. Social biases in NLP\nmodels as barriers for persons with disabilities. ACL.\nJaech, A., G. Mulcaire, S. Hathi, M. Ostendorf, and N. A.\nSmith. 2016. Hierarchical character-word models for lan-\nguage identi\ufb01cation. ACL Workshop on NLP for Social\nMedia .\nJauhiainen, T., M. Lui, M. Zampieri, T. Baldwin, and\nK. Lind \u00b4en. 2019. Automatic language identi\ufb01cation in\ntexts: A survey. JAIR , 65(1):675\u2013682.\nJurgens, D., Y . Tsvetkov, and D. Jurafsky. 2017. Incorpo-\nrating dialectal variability for socially equitable language\nidenti\ufb01cation. ACL.\nKiritchenko, S. and S. M. Mohammad. 2018. Examining\ngender and race bias in two hundred sentiment analysis\nsystems. *SEM .\nLiu, B. and L. Zhang. 2012. A survey of opinion mining and\nsentiment analysis. In C. C. Aggarwal and C. Zhai, eds,\nMining text data , 415\u2013464. Springer.\nLui, M. and T. Baldwin. 2011. Cross-domain feature selec-\ntion for language identi\ufb01cation. IJCNLP .\nLui, M. and T. Baldwin. 2012. langid.py : An off-the-shelf\nlanguage identi\ufb01cation tool. ACL.\nManning, C. D., P. Raghavan, and H. Sch \u00a8utze. 2008. Intro-\nduction to Information Retrieval . Cambridge.\nMaron, M. E. 1961. Automatic indexing: an experimental\ninquiry. Journal of the ACM , 8(3):404\u2013417.\nMcCallum, A. and K. Nigam. 1998. A comparison of event\nmodels for naive bayes text classi\ufb01cation. AAAI/ICML-98\nWorkshop on Learning for Text Categorization .\nMetsis, V ., I. Androutsopoulos, and G. Paliouras. 2006.\nSpam \ufb01ltering with naive bayes-which naive bayes?\nCEAS .\nMinsky, M. 1961. Steps toward arti\ufb01cial intelligence. Pro-\nceedings of the IRE , 49(1):8\u201330.\nMitchell, M., S. Wu, A. Zaldivar, P. Barnes, L. Vasserman,\nB. Hutchinson, E. Spitzer, I. D. Raji, and T. Gebru. 2019.\nModel cards for model reporting. ACM FAccT .\nMosteller, F. and D. L. Wallace. 1963. Inference in an au-\nthorship problem: A comparative study of discrimination\nmethods applied to the authorship of the disputed feder-\nalist papers. Journal of the American Statistical Associa-\ntion, 58(302):275\u2013309.\nMosteller, F. and D. L. Wallace. 1964. Inference and Dis-",
    "metadata": {
      "source": "4",
      "chunk_id": 37,
      "token_count": 760,
      "chapter_title": ""
    }
  },
  {
    "content": "McCallum, A. and K. Nigam. 1998. A comparison of event\nmodels for naive bayes text classi\ufb01cation. AAAI/ICML-98\nWorkshop on Learning for Text Categorization .\nMetsis, V ., I. Androutsopoulos, and G. Paliouras. 2006.\nSpam \ufb01ltering with naive bayes-which naive bayes?\nCEAS .\nMinsky, M. 1961. Steps toward arti\ufb01cial intelligence. Pro-\nceedings of the IRE , 49(1):8\u201330.\nMitchell, M., S. Wu, A. Zaldivar, P. Barnes, L. Vasserman,\nB. Hutchinson, E. Spitzer, I. D. Raji, and T. Gebru. 2019.\nModel cards for model reporting. ACM FAccT .\nMosteller, F. and D. L. Wallace. 1963. Inference in an au-\nthorship problem: A comparative study of discrimination\nmethods applied to the authorship of the disputed feder-\nalist papers. Journal of the American Statistical Associa-\ntion, 58(302):275\u2013309.\nMosteller, F. and D. L. Wallace. 1964. Inference and Dis-\nputed Authorship: The Federalist . Springer-Verlag. 1984\n2nd edition: Applied Bayesian and Classical Inference .\nMurphy, K. P. 2012. Machine learning: A probabilistic per-\nspective . MIT Press.\nNoreen, E. W. 1989. Computer Intensive Methods for Testing\nHypothesis . Wiley.\nPang, B. and L. Lee. 2008. Opinion mining and sentiment\nanalysis. Foundations and trends in information retrieval ,\n2(1-2):1\u2013135.",
    "metadata": {
      "source": "4",
      "chunk_id": 38,
      "token_count": 398,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23\n\nExercises 23\nPang, B., L. Lee, and S. Vaithyanathan. 2002. Thumbs\nup? Sentiment classi\ufb01cation using machine learning tech-\nniques. EMNLP .\nPark, J. H., J. Shin, and P. Fung. 2018. Reducing gender bias\nin abusive language detection. EMNLP .\nPennebaker, J. W., R. J. Booth, and M. E. Francis. 2007.\nLinguistic Inquiry and Word Count: LIWC 2007 . Austin,\nTX.\nPopp, D., R. A. Donovan, M. Crawford, K. L. Marsh, and\nM. Peele. 2003. Gender, race, and speech style stereo-\ntypes. Sex Roles , 48(7-8):317\u2013325.\nSahami, M., S. T. Dumais, D. Heckerman, and E. Horvitz.\n1998. A Bayesian approach to \ufb01ltering junk e-mail. AAAI\nWorkshop on Learning for Text Categorization .\nSap, M., D. Card, S. Gabriel, Y . Choi, and N. A. Smith. 2019.\nThe risk of racial bias in hate speech detection. ACL.\nS\u00f8gaard, A., A. Johannsen, B. Plank, D. Hovy, and H. M.\nAlonso. 2014. What\u2019s in a p-value in NLP? CoNLL .\nStamatatos, E. 2009. A survey of modern authorship attribu-\ntion methods. JASIST , 60(3):538\u2013556.\nStone, P., D. Dunphry, M. Smith, and D. Ogilvie. 1966.\nThe General Inquirer: A Computer Approach to Content\nAnalysis . MIT Press.\nvan Rijsbergen, C. J. 1975. Information Retrieval . Butter-\nworths.\nWang, S. and C. D. Manning. 2012. Baselines and bigrams:\nSimple, good sentiment and topic classi\ufb01cation. ACL.\nWilson, T., J. Wiebe, and P. Hoffmann. 2005. Recogniz-\ning contextual polarity in phrase-level sentiment analysis.\nEMNLP .\nWitten, I. H. and E. Frank. 2005. Data Mining: Practi-\ncal Machine Learning Tools and Techniques , 2nd edition.\nMorgan Kaufmann.\nYang, Y . and J. Pedersen. 1997. A comparative study on\nfeature selection in text categorization. ICML .",
    "metadata": {
      "source": "4",
      "chunk_id": 39,
      "token_count": 568,
      "chapter_title": ""
    }
  }
]