[
  {
    "content": "# 11\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n11Masked Language Models\nLarvatus prodeo [Masked, I go forward]\nDescartes\nIn the previous two chapters we introduced the transformer and saw how to pre-\ntrain a transformer language model as a causal or left-to-right language model. In\nthis chapter we\u2019ll introduce a second paradigm for pretrained language models, the\nbidirectional transformer encoder, and the most widely-used version, the BERT BERT\nmodel (Devlin et al., 2019). This model is trained via masked language modeling ,masked\nlanguage\nmodelingwhere instead of predicting the following word, we mask a word in the middle and\nask the model to guess the word given the words on both sides. This method thus\nallows the model to see both the right and left context.\nWe also introduced \ufb01netuning in the prior chapter. Here we describe a new \ufb01netuning\nkind of \ufb01netuning, in which we take the transformer network learned by these pre-\ntrained models, add a neural net classi\ufb01er after the top layer of the network, and train\nit on some additional labeled data to perform some downstream task like named\nentity tagging or natural language inference. As before, the intuition is that the\npretraining phase learns a language model that instantiates rich representations of\nword meaning, that thus enables the model to more easily learn (\u2018be \ufb01netuned to\u2019)\nthe requirements of a downstream language understanding task. This aspect of the\npretrain-\ufb01netune paradigm is an instance of what is called transfer learning in ma-transfer\nlearning\nchine learning: the method of acquiring knowledge from one task or domain, and\nthen applying it (transferring it) to solve a new task.\nThe second idea that we introduce in this chapter is the idea of contextual em-\nbeddings : representations for words in context. The methods of Chapter 6 like\nword2vec or GloVe learned a single vector embedding for each unique word win\nthe vocabulary. By contrast, with contextual embeddings, such as those learned by\nmasked language models like BERT, each word wwill be represented by a different\nvector each time it appears in a different context. While the causal language models\nof Chapter 9 also use contextual embeddings, the embeddings created by masked\nlanguage models seem to function particularly well as representations.\n11.1 Bidirectional Transformer Encoders\nLet\u2019s begin by introducing the bidirectional transformer encoder that underlies mod-\nels like BERT and its descendants like RoBERTa (Liu et al., 2019) or SpanBERT\n(Joshi et al., 2020). In Chapter 9 we introduced causal (left-to-right) transformers\nand in Chapter 10 saw how they can serve as the basis for language models that can\nbe applied to autoregressive contextual generation problems like question answering\nor summarization. But this left-to-right nature of these models is also a limitation,\nbecause there are tasks for which it would be useful, when processing a token, to\nbe able to peak at future tokens. This is especially true for sequence labeling tasks",
    "metadata": {
      "source": "11",
      "chunk_id": 0,
      "token_count": 686,
      "chapter_title": "11"
    }
  },
  {
    "content": "## Page 2\n\n2CHAPTER 11 \u2022 M ASKED LANGUAGE MODELS\nin which we want to tag each token with a label, such as the named entity tagging\ntask we\u2019ll introduce in Section 11.5, or tasks like part-of-speech tagging or parsing\nthat come up in later chapters.\nThebidirectional encoders that we introduce here are a different kind of beast\nthan causal models. The causal models of Chapter 9 are generative models, de-\nsigned to easily generate the next token in a sequence. But the focus of bidirec-\ntional encoders is instead on computing contextualized representations of the input\ntokens. Bidirectional encoders use self-attention to map sequences of input embed-\ndings (x1; :::;xn)to sequences of output embeddings of the same length (h1; :::;hn),\nwhere the output vectors have been contextualized using information from the en-\ntire input sequence. These output embeddings are contextualized representations of\neach input token that are useful across a range of applications where we need to do\na classi\ufb01cation or a decision based on the token in context.\nRemember that we said the models of Chapter 9 are sometimes called decoder-\nonly, because they correspond to the decoder part of the encoder-decoder model we\nwill introduce in Chapter 13. By contrast, the masked language models of this chap-\nter are sometimes called encoder-only , because they produce an encoding for each\ninput token but generally aren\u2019t used to produce running text by decoding/sampling.\nThat\u2019s an important point: masked language models are not used for generation.\nThey are generally instead used for interpretative tasks.\n11.1.1 The architecture for bidirectional masked models\nLet\u2019s \ufb01rst discuss the overall architecture. Bidirectional transformer-based language\nmodels differ in two ways from the causal transformers in the previous chapters. The\n\ufb01rst is that the attention function isn\u2019t causal; the attention for a token ican look at\nfollowing tokens i+1 and so on. The second is that the training is slightly different\nsince we are predicting something in the middle of our text rather than at the end.\nWe\u2019ll discuss the \ufb01rst here and the second in the following section.\nFig. 11.1a, reproduced here from Chapter 9, shows the information \ufb02ow in the\nleft-to-right approach of Chapter 9. The attention computation at each token is based\non the preceding (and current) input tokens, ignoring potentially useful information\nlocated to the right of the token under consideration. Bidirectional encoders over-\ncome this limitation by allowing the attention mechanism to range over the entire\ninput, as shown in Fig. 11.1b.\na) A causal self-attention layerb) A bidirectional self-attention layer\nattentionattentionattentionattentionattentiona1a2a3a4a5x3x4x5x1x2\nattentionattentionattentionattentionattentiona1a2a3a4a5x3x4x5x1x2\nFigure 11.1 (a) The causal transformer from Chapter 9, highlighting the attention computation at token 3. The\nattention value at each token is computed using only information seen earlier in the context. (b) Information\n\ufb02ow in a bidirectional attention model. In processing each token, the model attends to all inputs, both before\nand after the current one. So attention for token 3 can draw on information from following tokens.\nThe implementation is very simple! We simply remove the attention masking\nstep that we introduced in Eq. ??. Recall from Chapter 9 that we had to mask the\nQK|matrix for causal transformers so that attention couldn\u2019t look at future tokens",
    "metadata": {
      "source": "11",
      "chunk_id": 1,
      "token_count": 778,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\n11.1 \u2022 B IDIRECTIONAL TRANSFORMER ENCODERS 3\n(repeated from Eq. ??for a single attention head):\nhead =softmax\u0012\nmask\u0012QK|\npdk\u0013\u0013\nV\n(11.1)\nq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NN\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\u2212\u221e\nq1\u2022k1q2\u2022k1q2\u2022k2q4\u2022k1q4\u2022k2q4\u2022k3q4\u2022k4q3\u2022k1q3\u2022k2q3\u2022k3NNq1\u2022k2q1\u2022k3q1\u2022k4q2\u2022k3q2\u2022k4q3\u2022k4\n(a) (b)\nFigure 11.2 The N\u0002NQK|matrix showing the qi\u0001kjvalues, with the upper-triangle\nportion of the comparisons matrix zeroed out (set to \u0000\u00a5, which the softmax will turn to\nzero).\nFig. 11.2 shows the masked version of QK|and the unmasked version. For bidi-\nrectional attention, we use the unmasked version of Fig. 11.2b. Thus the attention\ncomputation for bidirectional attention is exactly the same as Eq. 11.1 but with the\nmask removed:\nhead =softmax\u0012QK|\npdk\u0013\nV (11.2)\nOtherwise, the attention computation is identical to what we saw in Chapter 9, as is\nthe transformer block architecture (the feedforward layer, layer norm, and so on). As\nin Chapter 9, the input is also a series of subword tokens, usually computed by one of\nthe 3 popular tokenization algorithms (including the BPE algorithm that we already\nsaw in Chapter 2 and two others, the WordPiece algorithm and the SentencePiece\nUnigram LM algorithm). That means every input sentence \ufb01rst has to be tokenized,\nand all further processing takes place on subword tokens rather than words. This will\nrequire, as we\u2019ll see in the third part of the textbook, that for some NLP tasks that\nrequire notions of words (like parsing) we will occasionally need to map subwords\nback to words.\nTo make this more concrete, the original English-only bidirectional transformer\nencoder model, BERT (Devlin et al., 2019), consisted of the following:\n\u2022 An English-only subword vocabulary consisting of 30,000 tokens generated\nusing the WordPiece algorithm (Schuster and Nakajima, 2012).\n\u2022 Input context window N=512 tokens, and model dimensionality d=768\n\u2022 So X, the input to the model, is of shape [N\u0002d]=[512\u0002768].\n\u2022L=12 layers of transformer blocks, each with A=12 (bidirectional) multihead\nattention layers.\n\u2022 The resulting model has about 100M parameters.\nThe larger multilingual XLM-RoBERTa model, trained on 100 languages, has\n\u2022 A multilingual subword vocabulary with 250,000 tokens generated using the\nSentencePiece Unigram LM algorithm (Kudo and Richardson, 2018).",
    "metadata": {
      "source": "11",
      "chunk_id": 2,
      "token_count": 719,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4CHAPTER 11 \u2022 M ASKED LANGUAGE MODELS\n\u2022 Input context window N=512 tokens, and model dimensionality d=1024, hence\nX, the input to the model, is of shape [N\u0002d]=[512\u00021024].\n\u2022L=24 layers of transformer blocks, with A=16 multihead attention layers each\n\u2022 The resulting model has about 550M parameters.\nNote that 550M parameters is relatively small as large language models go\n(Llama 3 has 405B parameters, so is 3 orders of magnitude bigger). Indeed, masked\nlanguage models tend to be much smaller than causal language models.\n11.2 Training Bidirectional Encoders\nWe trained causal transformer language models in Chapter 9 by making them it-\neratively predict the next word in a text. But eliminating the causal mask in at-\ntention makes the guess-the-next-word language modeling task trivial\u2014the answer\nis directly available from the context\u2014so we\u2019re in need of a new training scheme.\nInstead of trying to predict the next word, the model learns to perform a \ufb01ll-in-the-\nblank task, technically called the cloze task (Taylor, 1953). To see this, let\u2019s return cloze task\nto the motivating example from Chapter 3. Instead of predicting which words are\nlikely to come next in this example:\nThe water of Walden Pond is so beautifully\nwe\u2019re asked to predict a missing item given the rest of the sentence.\nThe of Walden Pond is so beautifully ...\nThat is, given an input sequence with one or more elements missing, the learning\ntask is to predict the missing elements. More precisely, during training the model is\ndeprived of one or more tokens of an input sequence and must generate a probability\ndistribution over the vocabulary for each of the missing items. We then use the cross-\nentropy loss from each of the model\u2019s predictions to drive the learning process.\nThis approach can be generalized to any of a variety of methods that corrupt the\ntraining input and then asks the model to recover the original input. Examples of the\nkinds of manipulations that have been used include masks, substitutions, reorder-\nings, deletions, and extraneous insertions into the training text. The general name\nfor this kind of training is called denoising : we corrupt (add noise to) the input in denoising\nsome way (by masking a word, or putting in an incorrect word) and the goal of the\nsystem is to remove the noise.\n11.2.1 Masking Words\nLet\u2019s describe the Masked Language Modeling (MLM) approach to training bidi-Masked\nLanguage\nModelingrectional encoders (Devlin et al., 2019). As with the language model training meth-\nods we\u2019ve already seen, MLM uses unannotated text from a large corpus. In MLM\ntraining, the model is presented with a series of sentences from the training corpus\nin which a percentage of tokens (15% in the BERT model) have been randomly cho-\nsen to be manipulated by the masking procedure. Given an input sentence lunch\nwas delicious and assume we randomly chose the 3rd token delicious to be\nmanipulated,\n\u2022 80% of the time: The token is replaced with the special vocabulary token\nnamed [MASK] , e.g. lunch was delicious !lunch was [MASK] .",
    "metadata": {
      "source": "11",
      "chunk_id": 3,
      "token_count": 708,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\n11.2 \u2022 T RAINING BIDIRECTIONAL ENCODERS 5\n\u2022 10% of the time: The token is replaced with another token, randomly sampled\nfrom the vocabulary based on token unigram probabilities. e.g. lunch was\ndelicious!lunch was gasp .\n\u2022 10% of the time: the token is left unchanged. e.g. lunch was delicious\n!lunch was delicious .\nWe then train the model to guess the correct token for the manipulated tokens. Why\nthe three possible manipulations? Adding the [MASK] token creates a mismatch be-\ntween pretraining and downstream \ufb01ne-tuning or inference, since when we employ\nthe MLM model to perform a downstream task, we don\u2019t use any [MASK] tokens. If\nwe just replaced tokens with the [MASK] , the model might only predict tokens when\nit sees a [MASK] , but we want the model to try to always predict the input token.\nTo train the model to make the prediction, the original input sequence is to-\nkenized using a subword model and tokens are sampled to be manipulated. Word\nembeddings for all of the tokens in the input are retrieved from the Eembedding ma-\ntrix and combined with positional embeddings to form the input to the transformer,\npassed through the stack of bidirectional transformer blocks, and then the language\nmodeling head. The MLM training objective is to predict the original inputs for\neach of the masked tokens and the cross-entropy loss from these predictions drives\nthe training process for all the parameters in the model. That is, all of the input\ntokens play a role in the self-attention process, but only the sampled tokens are used\nfor learning.\nLM Head with Softmax over Vocabulary\nSo[mask]and[mask]for longthanksCE Loss\nall apricot \ufb01shthe\nToken +Positional EmbeddingsSolongandthanksfor all \ufb01shthe\nBidirectional Transformer Encoder+p1+++++++p2p3p4p5p6p7p8z1z2z3z4z5z6z7z8\nFigure 11.3 Masked language model training. In this example, three of the input tokens are selected, two of\nwhich are masked and the third is replaced with an unrelated word. The probabilities assigned by the model to\nthese three items are used as the training loss. The other 5 tokens don\u2019t play a role in training loss.\nFig. 11.3 illustrates this approach with a simple example. Here, long,thanks and\nthehave been sampled from the training sequence, with the \ufb01rst two masked and the\nreplaced with the randomly sampled token apricot . The resulting embeddings are\npassed through a stack of bidirectional transformer blocks. Recall from Section ??\nin Chapter 9 that to produce a probability distribution over the vocabulary for each\nof the masked tokens, the language modeling head takes the output vector hL\nifrom\nthe \ufb01nal transformer layer Lfor each masked token i, multiplies it by the unembed-\nding layer ETto produce the logits u, and then uses softmax to turn the logits into",
    "metadata": {
      "source": "11",
      "chunk_id": 4,
      "token_count": 660,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6CHAPTER 11 \u2022 M ASKED LANGUAGE MODELS\nprobabilities yover the vocabulary:\nui=hL\niET(11.3)\nyi=softmax (ui) (11.4)\nWith a predicted probability distribution for each masked item, we can use cross-\nentropy to compute the loss for each masked item\u2014the negative log probability\nassigned to the actual masked word, as shown in Fig. 11.3. More formally, for a\ngiven vector of input tokens in a sentence or batch be x, let the set of tokens that are\nmasked be M, the version of that sentence with some tokens replaced by masks be\nxmask, and the sequence of output vectors be h. For a given input token xi, such as\nthe word long in Fig. 11.3, the loss is the probability of the correct word long, given\nxmask(as summarized in the single output vector hL\ni):\nLMLM(xi) =\u0000logP(xijhL\ni)\nThe gradients that form the basis for the weight updates are based on the average\nloss over the sampled learning items from a single training sequence (or batch of\nsequences).\nLMLM=\u00001\njMjX\ni2MlogP(xijhL\ni)\nNote that only the tokens in Mplay a role in learning; the other words play no role\nin the loss function, so in that sense BERT and its descendents are inef\ufb01cient; only\n15% of the input samples in the training data are actually used for training weights.1\n11.2.2 Next Sentence Prediction\nThe focus of mask-based learning is on predicting words from surrounding contexts\nwith the goal of producing effective word-level representations. However, an im-\nportant class of applications involves determining the relationship between pairs of\nsentences. These include tasks like paraphrase detection (detecting if two sentences\nhave similar meanings), entailment (detecting if the meanings of two sentences en-\ntail or contradict each other) or discourse coherence (deciding if two neighboring\nsentences form a coherent discourse).\nTo capture the kind of knowledge required for applications such as these, some\nmodels in the BERT family include a second learning objective called Next Sen-\ntence Prediction (NSP). In this task, the model is presented with pairs of sentencesNext Sentence\nPrediction\nand is asked to predict whether each pair consists of an actual pair of adjacent sen-\ntences from the training corpus or a pair of unrelated sentences. In BERT, 50% of\nthe training pairs consisted of positive pairs, and in the other 50% the second sen-\ntence of a pair was randomly selected from elsewhere in the corpus. The NSP loss\nis based on how well the model can distinguish true pairs from random pairs.\nTo facilitate NSP training, BERT introduces two special tokens to the input rep-\nresentation (tokens that will prove useful for \ufb01netuning as well). After tokenizing\nthe input with the subword model, the token [CLS] is prepended to the input sen-\ntence pair, and the token [SEP] is placed between the sentences and after the \ufb01nal\ntoken of the second sentence. There are actually two more special tokens, a \u2018First\nSegment\u2019 token, and a \u2018Second Segment\u2019 token. These tokens are added in the in-\nput stage to the word and positional embeddings. That is, each token of the input\n1ELECTRA, another BERT family member, does use all examples for training (Clark et al., 2020).",
    "metadata": {
      "source": "11",
      "chunk_id": 5,
      "token_count": 749,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\n11.2 \u2022 T RAINING BIDIRECTIONAL ENCODERS 7\nXis actually formed by summing 3 embeddings: word, position, and \ufb01rst/second\nsegment embeddings.\nDuring training, the output vector hL\nCLSfrom the \ufb01nal layer associated with the\n[CLS] token represents the next sentence prediction. As with the MLM objective,\nwe add a special head, in this case an NSP head, which consists of a learned set of\nclassi\ufb01cation weights WNSP2Rd\u00022that produces a two-class prediction from the\nraw[CLS] vector hL\nCLS:\nyi=softmax (hL\nCLSWNSP)\nCross entropy is used to compute the NSP loss for each sentence pair presented\nto the model. Fig. 11.4 illustrates the overall NSP training setup. In BERT, the NSP\nloss was used in conjunction with the MLM training objective to form \ufb01nal loss.\nCancelmy\ufb02ight[SEP] 1CE Loss\nAnd the \nBidirectional Transformer Encoder\np1p2p3p4p5p6p7p8[CLS]++s1NSPHeadToken +Segment +PositionalEmbeddings hotel p9[SEP] ++s1s1s1s1s2s2s2s2++++++++++++++hCLS\nFigure 11.4 An example of the NSP loss calculation.\n11.2.3 Training Regimes\nBERT and other early transformer-based language models were trained on about\n3.3 billion words (a combination of English Wikipedia and a corpus of book texts\ncalled BooksCorpus (Zhu et al., 2015) that is no longer used for intellectual property\nreasons). Modern masked language models are now trained on much larger datasets\nof web text, \ufb01ltered a bit, and augmented by higher-quality data like Wikipedia,\nthe same as those we discussed for the causal large language models of Chapter 9.\nMultilingual models similarly use webtext and multilingual Wikipedia. For example\nthe XLM-R model was trained on about 300 billion tokens in 100 languages, taken\nfrom the web via Common Crawl ( https://commoncrawl.org/ ).\nTo train the original BERT models, pairs of text segments were selected from the\ntraining corpus according to the next sentence prediction 50/50 scheme. Pairs were\nsampled so that their combined length was less than the 512 token input. Tokens\nwithin these sentence pairs were then masked using the MLM approach with the\ncombined loss from the MLM and NSP objectives used for a \ufb01nal loss. Because this\n\ufb01nal loss is backpropagated through the entire transformer, the embeddings at each\ntransformer layer will learn representations that are useful for predicting words from\ntheir neighbors. Since the [CLS] tokens are the direct input to the NSP classi\ufb01er,\ntheir learned representations will tend to contain information about the sequence as",
    "metadata": {
      "source": "11",
      "chunk_id": 6,
      "token_count": 628,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8CHAPTER 11 \u2022 M ASKED LANGUAGE MODELS\na whole. Approximately 40 passes (epochs) over the training data was required for\nthe model to converge.\nSome models, like the RoBERTa model, drop the next sentence prediction ob-\njective, and therefore change the training regime a bit. Instead of sampling pairs of\nsentence, the input is simply a series of contiguous sentences, still beginning with\nthe special [CLS] token. If the document runs out before 512 tokens are reached, an\nextra separator token is added, and sentences from the next document are packed in,\nuntil we reach a total of 512 tokens. Usually large batch sizes are used, between 8K\nand 32K tokens.\nMultilingual models have an additional decision to make: what data to use to\nbuild the vocabulary? Recall that all language models use subword tokenization\n(BPE or SentencePiece Unigram LM are the two most common algorithms). What\ntext should be used to learn this multilingual tokenization, given that it\u2019s easier to get\nmuch more text in some languages than others? One option would be to create this\nvocabulary-learning dataset by sampling sentences from our training data (perhaps\nweb text from Common Crawl), randomly. In that case we will choose a lot of sen-\ntences from languages like languages with lots of web representation like English,\nand the tokens will be biased toward rare English tokens instead of creating frequent\ntokens from languages with less data. Instead, it is common to divide the training\ndata into subcorpora of Ndifferent languages, compute the number of sentences ni\nof each language i, and readjust these probabilities so as to upweight the probability\nof less-represented languages (Lample and Conneau, 2019). The new probability of\nselecting a sentence from each of the Nlanguages (whose prior frequency is ni) is\nfqigi=1:::N, where:\nqi=pa\niPN\nj=1pa\njwith pi=niPN\nk=1nk(11.5)\nRecall from Eq. ??in Chapter 6 that an avalue between 0 and 1 will give higher\nweight to lower probability samples. Conneau et al. (2020) show that a=0:3 works\nwell to give rare languages more inclusion in the tokenization, resulting in better\nmultilingual performance overall.\nThe result of this pretraining process consists of both learned word embeddings,\nas well as all the parameters of the bidirectional encoder that are used to produce\ncontextual embeddings for novel inputs.\nFor many purposes, a pretrained multilingual model is more practical than a\nmonolingual model, since it avoids the need to build many (a hundred!) separate\nmonolingual models. And multilingual models can improve performance on low-\nresourced languages by leveraging linguistic information from a similar language in\nthe training data that happens to have more resources. Nonetheless, when the num-\nber of languages grows very large, multilingual models exhibit what has been called\nthecurse of multilinguality (Conneau et al., 2020): the performance on each lan-\nguage degrades compared to a model training on fewer languages. Another problem\nwith multilingual models is that they \u2018have an accent\u2019: grammatical structures in\nhigher-resource languages (often English) bleed into lower-resource languages; the\nvast amount of English language in training makes the model\u2019s representations for\nlow-resource languages slightly more English-like (Papadimitriou et al., 2023).",
    "metadata": {
      "source": "11",
      "chunk_id": 7,
      "token_count": 743,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\n11.3 \u2022 C ONTEXTUAL EMBEDDINGS 9\n11.3 Contextual Embeddings\nGiven a pretrained language model and a novel input sentence, we can think of the\nsequence of model outputs as constituting contextual embeddings for each token incontextual\nembeddings\nthe input. These contextual embeddings are vectors representing some aspect of the\nmeaning of a token in context, and can be used for any task requiring the meaning\nof tokens or words. More formally, given a sequence of input tokens x1; :::;xn, we\ncan use the output vector hLifrom the \ufb01nal layer Lof the model as a representation\nof the meaning of token xiin the context of sentence x1; :::;xn. Or instead of just\nusing the vector hLifrom the \ufb01nal layer of the model, it\u2019s common to compute a\nrepresentation for xiby averaging the output tokens hifrom each of the last four\nlayers of the model, i.e., hLi,hL\u00001i,hL\u00002i, and hL\u00003i.\n[CLS]SolongandthanksforallhL1hLCLShL2hL3hL4hL5hL6\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nFigure 11.5 The output of a BERT-style model is a contextual embedding vector hL\nifor\neach input token xi.\nJust as we used static embeddings like word2vec in Chapter 6 to represent the\nmeaning of words, we can use contextual embeddings as representations of word\nmeanings in context for any task that might require a model of word meaning. Where\nstatic embeddings represent the meaning of word types (vocabulary entries), contex-\ntual embeddings represent the meaning of word instances : instances of a particular\nword type in a particular context. Thus where word2vec had a single vector for each\nword type, contextual embeddings provide a single vector for each instance of that\nword type in its sentential context. Contextual embeddings can thus be used for\ntasks like measuring the semantic similarity of two words in context, and are useful\nin linguistic tasks that require models of word meaning.\n11.3.1 Contextual Embeddings and Word Sense\nWords are ambiguous : the same word can be used to mean different things. In ambiguous\nChapter 6 we saw that the word \u201cmouse\u201d can mean (1) a small rodent, or (2) a hand-\noperated device to control a cursor. The word \u201cbank\u201d can mean: (1) a \ufb01nancial\ninstitution or (2) a sloping mound. We say that the words \u2018mouse\u2019 or \u2018bank\u2019 are",
    "metadata": {
      "source": "11",
      "chunk_id": 8,
      "token_count": 578,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10",
    "metadata": {
      "source": "11",
      "chunk_id": 9,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "10 CHAPTER 11 \u2022 M ASKED LANGUAGE MODELS\npolysemous (from Greek \u2018many senses\u2019, poly- \u2018many\u2019 + sema , \u2018sign, mark\u2019).2\nAsense (orword sense ) is a discrete representation of one aspect of the meaning word sense\nof a word. We can represent each sense with a superscript: bank1andbank2,\nmouse1andmouse2. These senses can be found listed in online thesauruses (or\nthesauri) like WordNet (Fellbaum, 1998), which has datasets in many languages WordNet\nlisting the senses of many words. In context, it\u2019s easy to see the different meanings:\nmouse1: .... a mouse controlling a computer system in 1968.\nmouse2: .... a quiet animal like a mouse\nbank1: ...a bank can hold the investments in a custodial account ...\nbank2: ...as agriculture burgeons on the east bank , the river ...\nThis fact that context disambiguates the senses of mouse andbank above can\nalso be visualized geometrically. Fig. 11.6 shows a two-dimensional projection of\nmany instances of the BERT embeddings of the word diein English and German.\nEach point in the graph represents the use of diein one input sentence. We can\nclearly see at least two different English senses of die(the singular of dice and the\nverb to die , as well as the German article, in the BERT embedding space.\nFigure 4: Embeddings for the word \"die\" in different contexts, visualized with UMAP. Sample points\nare annotated with corresponding sentences. Overall annotations (blue text) are added as a guide.\n4.1 Visualization of word senses\nOur \ufb01rst experiment is an exploratory visualization of how word sense affects context embeddings.\nFor data on different word senses, we collected all sentences used in the introductions to English-\nlanguage Wikipedia articles. (Text outside of introductions was frequently fragmentary.) We created\nan interactive application, which we plan to make public. A user enters a word, and the system\nretrieves 1,000 sentences containing that word. It sends these sentences to BERT-base as input, and\nfor each one it retrieves the context embedding for the word from a layer of the user\u2019s choosing.\nThe system visualizes these 1,000 context embeddings using UMAP [ 15], generally showing clear\nclusters relating to word senses. Different senses of a word are typically spatially separated, and\nwithin the clusters there is often further structure related to \ufb01ne shades of meaning. In Figure 4, for\nexample, we not only see crisp, well-separated clusters for three meanings of the word \u201cdie,\u201d but\nwithin one of these clusters there is a kind of quantitative scale, related to the number of people\ndying. See Appendix 6.4 for further examples. The apparent detail in the clusters we visualized raises\ntwo immediate questions. First, is it possible to \ufb01nd quantitative corroboration that word senses are\nwell-represented? Second, how can we resolve a seeming contradiction: in the previous section, we\nsaw how position represented syntax; yet here we see position representing semantics.\n4.2 Measurement of word sense disambiguation capability\nThe crisp clusters seen in visualizations such as Figure 4 suggest that BERT may create simple,\neffective internal representations of word senses, putting different meanings in different locations. To\ntest this hypothesis quantitatively, we test whether a simple classi\ufb01er on these internal representations\ncan perform well at word-sense disambiguation (WSD).\nWe follow the procedure described in [ 20], which performed a similar experiment with the ELMo",
    "metadata": {
      "source": "11",
      "chunk_id": 10,
      "token_count": 778,
      "chapter_title": ""
    }
  },
  {
    "content": "The system visualizes these 1,000 context embeddings using UMAP [ 15], generally showing clear\nclusters relating to word senses. Different senses of a word are typically spatially separated, and\nwithin the clusters there is often further structure related to \ufb01ne shades of meaning. In Figure 4, for\nexample, we not only see crisp, well-separated clusters for three meanings of the word \u201cdie,\u201d but\nwithin one of these clusters there is a kind of quantitative scale, related to the number of people\ndying. See Appendix 6.4 for further examples. The apparent detail in the clusters we visualized raises\ntwo immediate questions. First, is it possible to \ufb01nd quantitative corroboration that word senses are\nwell-represented? Second, how can we resolve a seeming contradiction: in the previous section, we\nsaw how position represented syntax; yet here we see position representing semantics.\n4.2 Measurement of word sense disambiguation capability\nThe crisp clusters seen in visualizations such as Figure 4 suggest that BERT may create simple,\neffective internal representations of word senses, putting different meanings in different locations. To\ntest this hypothesis quantitatively, we test whether a simple classi\ufb01er on these internal representations\ncan perform well at word-sense disambiguation (WSD).\nWe follow the procedure described in [ 20], which performed a similar experiment with the ELMo\nmodel. For a given word with nsenses, we make a nearest-neighbor classi\ufb01er where each neighbor is\nthe centroid of a given word sense\u2019s BERT-base embeddings in the training data. To classify a new\nword we \ufb01nd the closest of these centroids, defaulting to the most commonly used sense if the word\nwas not present in the training data. We used the data and evaluation from [ 21]: the training data was\nSemCor [17] (33,362 senses), and the testing data was the suite described in [21] (3,669 senses).\nThe simple nearest-neighbor classi\ufb01er achieves an F1 score of 71.1, higher than the current state of\nthe art (Table 1), with the accuracy monotonically increasing through the layers. This is a strong\nsignal that context embeddings are representing word-sense information. Additionally, an even higher\nscore of 71.5 was obtained using the technique described in the following section.\n6\nFigure 11.6 Each blue dot shows a BERT contextual embedding for the word diefrom different sentences\nin English and German, projected into two dimensions with the UMAP algorithm. The German and English\nmeanings and the different English senses fall into different clusters. Some sample points are shown with the\ncontextual sentence they came from. Figure from Coenen et al. (2019).\nThus while thesauruses like WordNet give discrete lists of senses, embeddings\n(whether static or contextual) offer a continuous high-dimensional model of meaning\nthat, although it can be clustered, doesn\u2019t divide up into fully discrete senses.\nWord Sense Disambiguation\nThe task of selecting the correct sense for a word is called word sense disambigua-\ntion, orWSD . WSD algorithms take as input a word in context and a \ufb01xed inventoryword sense\ndisambiguation\nWSD of potential word senses (like the ones in WordNet) and outputs the correct word\nsense in context. Fig. 11.7 sketches out the task.\n2The word polysemy itself is ambiguous; you may see it used in a different way, to refer only to cases\nwhere a word\u2019s senses are related in some structured way, reserving the word homonymy to mean sense",
    "metadata": {
      "source": "11",
      "chunk_id": 11,
      "token_count": 775,
      "chapter_title": ""
    }
  },
  {
    "content": "score of 71.5 was obtained using the technique described in the following section.\n6\nFigure 11.6 Each blue dot shows a BERT contextual embedding for the word diefrom different sentences\nin English and German, projected into two dimensions with the UMAP algorithm. The German and English\nmeanings and the different English senses fall into different clusters. Some sample points are shown with the\ncontextual sentence they came from. Figure from Coenen et al. (2019).\nThus while thesauruses like WordNet give discrete lists of senses, embeddings\n(whether static or contextual) offer a continuous high-dimensional model of meaning\nthat, although it can be clustered, doesn\u2019t divide up into fully discrete senses.\nWord Sense Disambiguation\nThe task of selecting the correct sense for a word is called word sense disambigua-\ntion, orWSD . WSD algorithms take as input a word in context and a \ufb01xed inventoryword sense\ndisambiguation\nWSD of potential word senses (like the ones in WordNet) and outputs the correct word\nsense in context. Fig. 11.7 sketches out the task.\n2The word polysemy itself is ambiguous; you may see it used in a different way, to refer only to cases\nwhere a word\u2019s senses are related in some structured way, reserving the word homonymy to mean sense\nambiguities with no relation between the senses (Haber and Poesio, 2020). Here we will use \u2018polysemy\u2019\nto mean any kind of sense ambiguity, and \u2018structured polysemy\u2019 for polysemy with sense relations.",
    "metadata": {
      "source": "11",
      "chunk_id": 12,
      "token_count": 341,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\n11.3 \u2022 C ONTEXTUAL EMBEDDINGS 11\nanelectricguitarandbassplayerstando\ufb00toonesideelectric1: using electricityelectric2:  tenseelectric3: thrillingguitar1 bass1: low range\u2026bass4: sea \ufb01sh\u2026 bass7: instrument\u2026player1: in gameplayer2: musician player3: actor\u2026stand1: upright\u2026stand5: bear\u2026 stand10: put upright\u2026side1: relative region\u2026side3: of body\u2026 side11: slope\u2026x1y1x2y2x3y3y4y5y6\nx4x5x6\nFigure 11.7 The all-words WSD task, mapping from input words ( x) to WordNet senses\n(y). Figure inspired by Chaplot and Salakhutdinov (2018).\nWSD can be a useful analytic tool for text analysis in the humanities and social\nsciences, and word senses can play a role in model interpretability for word repre-\nsentations. Word senses also have interesting distributional properties. For example\na word often is used in roughly the same sense through a discourse, an observation\ncalled the one sense per discourse rule (Gale et al., 1992).one sense per\ndiscourse\nThe best performing WSD algorithm is a simple 1-nearest-neighbor algorithm\nusing contextual word embeddings, due to Melamud et al. (2016) and Peters et al.\n(2018). At training time we pass each sentence in some sense-labeled dataset (like\nthe SemCore or SenseEval datasets in various languages) through any contextual\nembedding (e.g., BERT) resulting in a contextual embedding for each labeled token.\n(There are various ways to compute this contextual embedding vifor a token i; for\nBERT it is common to pool multiple layers by summing the vector representations\nofifrom the last four BERT layers). Then for each sense sof any word in the corpus,\nfor each of the ntokens of that sense, we average their ncontextual representations\nvito produce a contextual sense embedding v sfors:\nvs=1\nnX\nivi8vi2tokens (s) (11.6)\nAt test time, given a token of a target word tin context, we compute its contextual\nembedding tand choose its nearest neighbor sense from the training set, i.e., the\nsense whose sense embedding has the highest cosine with t:\nsense(t) =argmax\ns2senses (t)cosine (t;vs) (11.7)\nFig. 11.8 illustrates the model.\n11.3.2 Contextual Embeddings and Word Similarity\nIn Chapter 6 we introduced the idea that we could measure the similarity of two\nwords by considering how close they are geometrically, by using the cosine as a\nsimilarity function. The idea of meaning similarity is also clear geometrically in the\nmeaning clusters in Fig. 11.6; the representation of a word which has a particular\nsense in a context is closer to other instances of the same sense of the word. Thus we",
    "metadata": {
      "source": "11",
      "chunk_id": 13,
      "token_count": 663,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 CHAPTER 11 \u2022 M ASKED LANGUAGE MODELS\nI  found  the  jar  emptycIcfoundfind1vcthecjarcemptyfind9vfind5vfind4vENCODER\nFigure 11.8 The nearest-neighbor algorithm for WSD. In green are the contextual embed-\ndings precomputed for each sense of each word; here we just show a few of the senses for\n\ufb01nd. A contextual embedding is computed for the target word found , and then the nearest\nneighbor sense (in this case \ufb01nd9v) is chosen. Figure inspired by Loureiro and Jorge (2019).\noften measure the similarity between two instances of two words in context (or two\ninstances of the same word in two different contexts) by using the cosine between\ntheir contextual embeddings.\nUsually some transformations to the embeddings are required before computing\ncosine. This is because contextual embeddings (whether from masked language\nmodels or from autoregressive ones) have the property that the vectors for all words\nare extremely similar. If we look at the embeddings from the \ufb01nal layer of BERT\nor other models, embeddings for instances of any two randomly chosen words will\nhave extremely high cosines that can be quite close to 1, meaning all word vectors\ntend to point in the same direction. The property of vectors in a system all tending\nto point in the same direction is known as anisotropy . Ethayarajh (2019) de\ufb01nes\ntheanisotropy of a model as the expected cosine similarity of any pair of words in anisotropy\na corpus. The word \u2018isotropy\u2019 means uniformity in all directions, so in an isotropic\nmodel, the collection of vectors should point in all directions and the expected cosine\nbetween a pair of random embeddings would be zero. Timkey and van Schijndel\n(2021) show that one cause of anisotropy is that cosine measures are dominated by\na small number of dimensions of the contextual embedding whose values are very\ndifferent than the others: these rogue dimensions have very large magnitudes and\nvery high variance.\nTimkey and van Schijndel (2021) shows that we can make the embeddings more\nisotropic by standardizing (z-scoring) the vectors, i.e., subtracting the mean and\ndividing by the variance. Given a set Cof all the embeddings in some corpus, each\nwith dimensionality d(i.e., x2Rd), the mean vector m2Rdis:\nm=1\njCjX\nx2Cx (11.8)\nThe standard deviation in each dimension s2Rdis:\ns=s\n1\njCjX\nx2C(x\u0000m)2(11.9)\nThen each word vector xis replaced by a standardized version z:\nz=x\u0000m\ns(11.10)",
    "metadata": {
      "source": "11",
      "chunk_id": 14,
      "token_count": 615,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\n11.4 \u2022 F INE-TUNING FOR CLASSIFICATION 13\nOne problem with cosine that is not solved by standardization is that cosine tends\nto underestimate human judgments on similarity of word meaning for very frequent\nwords (Zhou et al., 2022).\nIn the next section we\u2019ll see the most common use of contextual representations:\nas representations of words or even entire sentences that can be the inputs to classi-\n\ufb01ers in the \ufb01netuning process for downstream NLP applications.\n11.4 Fine-Tuning for Classi\ufb01cation\nThe power of pretrained language models lies in their ability to extract generaliza-\ntions from large amounts of text\u2014generalizations that are useful for myriad down-\nstream applications. There are two ways to make practical use of the generalizations\nto solve downstream tasks. The most common way is to use natural language to\nprompt the model, putting it in a state where it contextually generates what we\nwant. We\u2019ll introduce prompting in Chapter 12.\nIn this section we explore an alternative way to use pretrained language models\nfor downstream applications: a version of the \ufb01netuning paradigm from Chapter 10. \ufb01netuning\nIn the kind of \ufb01netuning used for masked language models, we add application-\nspeci\ufb01c circuitry (often called a special head ) on top of pretrained models, taking\ntheir output as its input. The \ufb01netuning process consists of using labeled data about\nthe application to train these additional application-speci\ufb01c parameters. Typically,\nthis training will either freeze or make only minimal adjustments to the pretrained\nlanguage model parameters.\nThe following sections introduce \ufb01netuning methods for the most common kinds\nof applications: sequence classi\ufb01cation, sentence-pair classi\ufb01cation, and sequence\nlabeling.\n11.4.1 Sequence Classi\ufb01cation\nThe task of sequence classi\ufb01cation is to classify an entire sequence of text with a\nsingle label. This set of tasks is commonly called text classi\ufb01cation , like sentiment\nanalysis or spam detection (Chapter 4) in which we classify a text into two or three\nclasses (like positive or negative), as well as classi\ufb01cation tasks with a large number\nof categories, like document-level topic classi\ufb01cation.\nFor sequence classi\ufb01cation we represent the entire input to be classi\ufb01ed by a\nsingle vector. We can represent a sequence in various ways. One way is to take\nthe sum or the mean of the last output vector from each token in the sequence.\nFor BERT, we instead add a new unique token to the vocabulary called [CLS] , and\nprepended it to the start of all input sequences, both during pretraining and encoding.\nThe output vector in the \ufb01nal layer of the model for the [CLS] input represents\nthe entire input sequence and serves as the input to a classi\ufb01er head , a logistic classi\ufb01er head\nregression or neural network classi\ufb01er that makes the relevant decision.\nAs an example, let\u2019s return to the problem of sentiment classi\ufb01cation. to \ufb01netun-\ning a classi\ufb01er for this application involves learning a set of weights, WC, to map the\noutput vector for the [CLS] token\u2014 hL\nCLS\u2014to a set of scores over the possible senti-\nment classes. Assuming a three-way sentiment classi\ufb01cation task (positive, negative,\nneutral) and dimensionality das the model dimension, WCwill be of size [d\u00023]. To\nclassify a document, we pass the input text through the pretrained language model to",
    "metadata": {
      "source": "11",
      "chunk_id": 15,
      "token_count": 776,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n14 CHAPTER 11 \u2022 M ASKED LANGUAGE MODELS\ngenerate hL\nCLS, multiply it by WC, and pass the resulting vector through a softmax.\ny=softmax (hL\nCLSWC) (11.11)\nFinetuning the values in WCrequires supervised training data consisting of input\nsequences labeled with the appropriate sentiment class. Training proceeds in the\nusual way; cross-entropy loss between the softmax output and the correct answer is\nused to drive the learning that produces WC.\nA key difference from what we\u2019ve seen earlier with neural classi\ufb01ers is that this\nloss can be used to not only learn the weights of the classi\ufb01er, but also to update the\nweights for the pretrained language model itself. In practice, reasonable classi\ufb01ca-\ntion performance is typically achieved with only minimal changes to the language\nmodel parameters, often limited to updates over the \ufb01nal few layers of the trans-\nformer. Fig. 11.9 illustrates this overall approach to sequence classi\ufb01cation.\n[CLS]entirelypredictableandlacksenergy\nBidirectional Transformer EncoderhCLS\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+sentimentclassi\ufb01cation headWCy\nFigure 11.9 Sequence classi\ufb01cation with a bidirectional transformer encoder. The output vector for the\n[CLS] token serves as input to a simple classi\ufb01er.\n11.4.2 Sequence-Pair Classi\ufb01cation\nAs mentioned in Section 11.2.2, an important type of problem involves the classi\ufb01ca-\ntion of pairs of input sequences. Practical applications that fall into this class include\nparaphrase detection (are the two sentences paraphrases of each other?), logical en-\ntailment (does sentence A logically entail sentence B?), and discourse coherence\n(how coherent is sentence B as a follow-on to sentence A?).\nFine-tuning an application for one of these tasks proceeds just as with pretrain-\ning using the NSP objective. During \ufb01netuning, pairs of labeled sentences from a\nsupervised \ufb01netuning set are presented to the model, and run through all the layers\nof the model to produce the houtputs for each input token. As with sequence classi-\n\ufb01cation, the output vector associated with the prepended [CLS] token represents the\nmodel\u2019s view of the input pair. And as with NSP training, the two inputs are sepa-\nrated by the [SEP] token. To perform classi\ufb01cation, the [CLS] vector is multiplied\nby a set of learning classi\ufb01cation weights and passed through a softmax to generate\nlabel predictions, which are then used to update the weights.",
    "metadata": {
      "source": "11",
      "chunk_id": 16,
      "token_count": 585,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\n11.5 \u2022 F INE-TUNING FOR SEQUENCE LABELLING : NAMED ENTITY RECOGNITION 15\nAs an example, let\u2019s consider an entailment classi\ufb01cation task with the Multi-\nGenre Natural Language Inference (MultiNLI) dataset (Williams et al., 2018). In\nthe task of natural language inference orNLI, also called recognizing textualnatural\nlanguage\ninferenceentailment , a model is presented with a pair of sentences and must classify the re-\nlationship between their meanings. For example in the MultiNLI corpus, pairs of\nsentences are given one of 3 labels: entails ,contradicts andneutral . These labels\ndescribe a relationship between the meaning of the \ufb01rst sentence (the premise) and\nthe meaning of the second sentence (the hypothesis). Here are representative exam-\nples of each class from the corpus:\n\u2022Neutral\na: Jon walked back to the town to the smithy.\nb: Jon traveled back to his hometown.\n\u2022Contradicts\na: Tourist Information of\ufb01ces can be very helpful.\nb: Tourist Information of\ufb01ces are never of any help.\n\u2022Entails\na: I\u2019m confused.\nb: Not all of it is very clear to me.\nA relationship of contradicts means that the premise contradicts the hypothesis; en-\ntails means that the premise entails the hypothesis; neutral means that neither is\nnecessarily true. The meaning of these labels is looser than strict logical entailment\nor contradiction indicating that a typical human reading the sentences would most\nlikely interpret the meanings in this way.\nTo \ufb01netune a classi\ufb01er for the MultiNLI task, we pass the premise/hypothesis\npairs through a bidirectional encoder as described above and use the output vector\nfor the [CLS] token as the input to the classi\ufb01cation head. As with ordinary sequence\nclassi\ufb01cation, this head provides the input to a three-way classi\ufb01er that can be trained\non the MultiNLI training corpus.\n11.5 Fine-Tuning for Sequence Labelling: Named En-\ntity Recognition\nIn sequence labeling, the network\u2019s task is to assign a label chosen from a small\n\ufb01xed set of labels to each token in the sequence. One of the most common sequence\nlabeling task is named entity recognition .\n11.5.1 Named Entities\nAnamed entity is, roughly speaking, anything that can be referred to with a proper named entity\nname: a person, a location, an organization. The task of named entity recognitionnamed entity\nrecognition\n(NER ) is to \ufb01nd spans of text that constitute proper names and tag the type of the NER\nentity. Four entity tags are most common: PER (person), LOC (location), ORG\n(organization), or GPE (geo-political entity). However, the term named entity is\ncommonly extended to include things that aren\u2019t entities per se, including temporal\nexpressions like dates and times, and even numerical expressions like prices. Here\u2019s\nan example of the output of an NER tagger:",
    "metadata": {
      "source": "11",
      "chunk_id": 17,
      "token_count": 665,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16",
    "metadata": {
      "source": "11",
      "chunk_id": 18,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "16 CHAPTER 11 \u2022 M ASKED LANGUAGE MODELS\nCiting high fuel prices, [ ORG United Airlines ] said [ TIME Friday ] it\nhas increased fares by [ MONEY $6] per round trip on \ufb02ights to some\ncities also served by lower-cost carriers. [ ORG American Airlines ], a\nunit of [ ORG AMR Corp.] , immediately matched the move, spokesman\n[PER Tim Wagner ] said. [ ORG United] , a unit of [ ORG UAL Corp.] ,\nsaid the increase took effect [ TIME Thursday] and applies to most\nroutes where it competes against discount carriers, such as [ LOC Chicago]\nto [LOC Dallas] and [ LOC Denver] to [LOC San Francisco] .\nThe text contains 13 mentions of named entities including 5 organizations, 4 loca-\ntions, 2 times, 1 person, and 1 mention of money. Figure 11.10 shows typical generic\nnamed entity types. Many applications will also need to use speci\ufb01c entity types like\nproteins, genes, commercial products, or works of art.\nType Tag Sample Categories Example sentences\nPeople PER people, characters Turing is a giant of computer science.\nOrganization ORG companies, sports teams The IPCC warned about the cyclone.\nLocation LOC regions, mountains, seas Mt. Sanitas is in Sunshine Canyon .\nGeo-Political Entity GPE countries, states Palo Alto is raising the fees for parking.\nFigure 11.10 A list of generic named entity types with the kinds of entities they refer to.\nNamed entity recognition is a useful step in various natural language processing\ntasks, including linking text to information in structured knowledge sources like\nWikipedia, measuring sentiment or attitudes toward a particular entity in text, or\neven as part of anonymizing text for privacy. The NER task is is dif\ufb01cult because\nof the ambiguity of segmenting NER spans, \ufb01guring out which tokens are entities\nand which aren\u2019t, since most words in a text will not be named entities. Another\ndif\ufb01culty is caused by type ambiguity. The mention Washington can refer to a\nperson, a sports team, a city, or the US government, as we see in Fig. 11.11.\n[PER Washington] was born into slavery on the farm of James Burroughs.\n[ORG Washington] went up 2 games to 1 in the four-game series.\nBlair arrived in [ LOC Washington] for what may well be his last state visit.\nIn June, [ GPE Washington] passed a primary seatbelt law.\nFigure 11.11 Examples of type ambiguities in the use of the name Washington .\n11.5.2 BIO Tagging\nOne standard approach to sequence labeling for a span-recognition problem like\nNER is BIO tagging (Ramshaw and Marcus, 1995). This is a method that allows us BIO tagging\nto treat NER like a word-by-word sequence labeling task, via tags that capture both\nthe boundary and the named entity type. Consider the following sentence:\n[PER Jane Villanueva ] of [ ORG United ] , a unit of [ ORG United Airlines\nHolding ] , said the fare applies to the [ LOC Chicago ] route.\nFigure 11.12 shows the same excerpt represented with BIO tagging, as well as BIO\nvariants called IOtagging and BIOES tagging. In BIO tagging we label any token\nthatbegins a span of interest with the label B, tokens that occur inside a span are\ntagged with an I, and any tokens outside of any span of interest are labeled O. While\nthere is only one Otag, we\u2019ll have distinct Band Itags for each named entity class.",
    "metadata": {
      "source": "11",
      "chunk_id": 19,
      "token_count": 775,
      "chapter_title": ""
    }
  },
  {
    "content": "[ORG Washington] went up 2 games to 1 in the four-game series.\nBlair arrived in [ LOC Washington] for what may well be his last state visit.\nIn June, [ GPE Washington] passed a primary seatbelt law.\nFigure 11.11 Examples of type ambiguities in the use of the name Washington .\n11.5.2 BIO Tagging\nOne standard approach to sequence labeling for a span-recognition problem like\nNER is BIO tagging (Ramshaw and Marcus, 1995). This is a method that allows us BIO tagging\nto treat NER like a word-by-word sequence labeling task, via tags that capture both\nthe boundary and the named entity type. Consider the following sentence:\n[PER Jane Villanueva ] of [ ORG United ] , a unit of [ ORG United Airlines\nHolding ] , said the fare applies to the [ LOC Chicago ] route.\nFigure 11.12 shows the same excerpt represented with BIO tagging, as well as BIO\nvariants called IOtagging and BIOES tagging. In BIO tagging we label any token\nthatbegins a span of interest with the label B, tokens that occur inside a span are\ntagged with an I, and any tokens outside of any span of interest are labeled O. While\nthere is only one Otag, we\u2019ll have distinct Band Itags for each named entity class.\nThe number of tags is thus 2 n+1 tags, where nis the number of entity types. BIO",
    "metadata": {
      "source": "11",
      "chunk_id": 20,
      "token_count": 308,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\n11.5 \u2022 F INE-TUNING FOR SEQUENCE LABELLING : NAMED ENTITY RECOGNITION 17\ntagging can represent exactly the same information as the bracketed notation, but has\nthe advantage that we can represent the task in the same simple sequence modeling\nway as part-of-speech tagging: assigning a single label yito each input word xi:\nWords IO Label BIO Label BIOES Label\nJane I-PER B-PER B-PER\nVillanueva I-PER I-PER E-PER\nof O O O\nUnited I-ORG B-ORG B-ORG\nAirlines I-ORG I-ORG I-ORG\nHolding I-ORG I-ORG E-ORG\ndiscussed O O O\nthe O O O\nChicago I-LOC B-LOC S-LOC\nroute O O O\n. O O O\nFigure 11.12 NER as a sequence model, showing IO, BIO, and BIOES taggings.\nWe\u2019ve also shown two variant tagging schemes: IO tagging, which loses some\ninformation by eliminating the B tag, and BIOES tagging, which adds an end tag E\nfor the end of a span, and a span tag Sfor a span consisting of only one word.\n11.5.3 Sequence Labeling\nIn sequence labeling, we pass the \ufb01nal output vector corresponding to each input\ntoken to a classi\ufb01er that produces a softmax distribution over the possible set of\ntags. For a single feedforward layer classi\ufb01er, the set of weights to be learned is\nWKof size [d\u0002k], where kis the number of possible tags for the task. A greedy\napproach, where the argmax tag for each token is taken as a likely answer, can be\nused to generate the \ufb01nal output tag sequence. Fig. 11.13 illustrates an example of\nthis approach, where yiis a vector of probabilities over tags, and kindexes the tags.\nyi=softmax (hL\niWK) (11.12)\nti=argmaxk(yi) (11.13)\nAlternatively, the distribution over labels provided by the softmax for each input\ntoken can be passed to a conditional random \ufb01eld (CRF) layer which can take global\ntag-level transitions into account (see Chapter 17 on CRFs).\nTokenization and NER\nNote that supervised training data for NER is typically in the form of BIO tags as-\nsociated with text segmented at the word level. For example the following sentence\ncontaining two named entities:\n[LOC Mt. Sanitas ] is in [ LOC Sunshine Canyon ] .\nwould have the following set of per-word BIO tags.\n(11.14) Mt.\nB-LOCSanitas\nI-LOCis\nOin\nOSunshine\nB-LOCCanyon\nI-LOC.\nO\nUnfortunately, the sequence of WordPiece tokens for this sentence doesn\u2019t align\ndirectly with BIO tags in the annotation:",
    "metadata": {
      "source": "11",
      "chunk_id": 21,
      "token_count": 633,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\n18 CHAPTER 11 \u2022 M ASKED LANGUAGE MODELS\n[CLS]JaneVillanuevaofUnitedAirlines\nBidirectional Transformer EncoderB-PERI-PEROB-ORGI-ORG\nHoldingdiscussedI-ORGOWKNER headhiargmax\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+\nEi+WKWKWKWKWKWKyi\nFigure 11.13 Sequence labeling for named entity recognition with a bidirectional transformer encoder. The\noutput vector for each input token is passed to a simple k-way classi\ufb01er.\n'Mt', '.', 'San', '##itas', 'is', 'in', 'Sunshine', 'Canyon' '.'\nTo deal with this misalignment, we need a way to assign BIO tags to subword\ntokens during training and a corresponding way to recover word-level tags from\nsubwords during decoding. For training, we can just assign the gold-standard tag\nassociated with each word to all of the subword tokens derived from it.\nFor decoding, the simplest approach is to use the argmax BIO tag associated with\nthe \ufb01rst subword token of a word. Thus, in our example, the BIO tag assigned to\n\u201cMt\u201d would be assigned to \u201cMt.\u201d and the tag assigned to \u201cSan\u201d would be assigned\nto \u201cSanitas\u201d, effectively ignoring the information in the tags assigned to \u201c.\u201d and\n\u201c##itas\u201d. More complex approaches combine the distribution of tag probabilities\nacross the subwords in an attempt to \ufb01nd an optimal word-level tag.\n11.5.4 Evaluating Named Entity Recognition\nNamed entity recognizers are evaluated by recall ,precision , and F1measure . Re-\ncall that recall is the ratio of the number of correctly labeled responses to the total\nthat should have been labeled; precision is the ratio of the number of correctly la-\nbeled responses to the total labeled; and F-measure is the harmonic mean of the\ntwo.\nTo know if the difference between the F 1scores of two NER systems is a signif-\nicant difference, we use the paired bootstrap test, or the similar randomization test\n(Section ??).\nFor named entity tagging, the entity rather than the word is the unit of response.\nThus in the example in Fig. 11.12, the two entities Jane Villanueva andUnited Air-\nlines Holding and the non-entity discussed would each count as a single response.\nThe fact that named entity tagging has a segmentation component which is not\npresent in tasks like text categorization or part-of-speech tagging causes some prob-\nlems with evaluation. For example, a system that labeled Jane but not Jane Vil-\nlanueva as a person would cause two errors, a false positive for O and a false nega-",
    "metadata": {
      "source": "11",
      "chunk_id": 22,
      "token_count": 590,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\n11.6 \u2022 S UMMARY 19\ntive for I-PER. In addition, using entities as the unit of response but words as the unit\nof training means that there is a mismatch between the training and test conditions.\n11.6 Summary\nThis chapter has introduced the bidirectional encoder and the masked language\nmodel . Here\u2019s a summary of the main points that we covered:\n\u2022 Bidirectional encoders can be used to generate contextualized representations\nof input embeddings using the entire input context.\n\u2022 Pretrained language models based on bidirectional encoders can be learned\nusing a masked language model objective where a model is trained to guess\nthe missing information from an input.\n\u2022 The vector output of each transformer block or component in a particular to-\nken column is a contextual embedding that represents some aspect of the\nmeaning of a token in context.\n\u2022 A word sense is a discrete representation of one aspect of the meaning of a\nword. Contextual embeddings offer a continuous high-dimensional model of\nmeaning that is richer than fully discrete senses.\n\u2022 The cosine between contextual embeddings can be used as one way to model\nthe similarity between two words in context, although some transformations\nto the embeddings are required \ufb01rst.\n\u2022 Pretrained language models can be \ufb01netuned for speci\ufb01c applications by adding\nlightweight classi\ufb01er layers on top of the outputs of the pretrained model.\n\u2022 These applications can include sequence classi\ufb01cation tasks like sentiment\nanalysis, sequence-pair classi\ufb01cation tasks like natural language inference,\norsequence labeling tasks like named entity recognition .\nBibliographical and Historical Notes\nHistory TBD.",
    "metadata": {
      "source": "11",
      "chunk_id": 23,
      "token_count": 344,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20",
    "metadata": {
      "source": "11",
      "chunk_id": 24,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "20 Chapter 11 \u2022 Masked Language Models\nChaplot, D. S. and R. Salakhutdinov. 2018. Knowledge-\nbased word sense disambiguation using topic models.\nAAAI .\nClark, K., M.-T. Luong, Q. V . Le, and C. D. Manning.\n2020. Electra: Pre-training text encoders as discrimina-\ntors rather than generators. ICLR .\nCoenen, A., E. Reif, A. Yuan, B. Kim, A. Pearce, F. Vi \u00b4egas,\nand M. Wattenberg. 2019. Visualizing and measuring the\ngeometry of bert. NeurIPS .\nConneau, A., K. Khandelwal, N. Goyal, V . Chaudhary,\nG. Wenzek, F. Guzm \u00b4an, E. Grave, M. Ott, L. Zettlemoyer,\nand V . Stoyanov. 2020. Unsupervised cross-lingual rep-\nresentation learning at scale. ACL.\nDevlin, J., M.-W. Chang, K. Lee, and K. Toutanova. 2019.\nBERT: Pre-training of deep bidirectional transformers for\nlanguage understanding. NAACL HLT .\nEthayarajh, K. 2019. How contextual are contextual-\nized word representations? Comparing the geometry of\nBERT, ELMo, and GPT-2 embeddings. EMNLP .\nFellbaum, C., ed. 1998. WordNet: An Electronic Lexical\nDatabase . MIT Press.\nGale, W. A., K. W. Church, and D. Yarowsky. 1992. One\nsense per discourse. HLT.\nHaber, J. and M. Poesio. 2020. Assessing polyseme sense\nsimilarity through co-predication acceptability and con-\ntextualised embedding distance. *SEM .\nJoshi, M., D. Chen, Y . Liu, D. S. Weld, L. Zettlemoyer, and\nO. Levy. 2020. SpanBERT: Improving pre-training by\nrepresenting and predicting spans. TACL , 8:64\u201377.\nKudo, T. and J. Richardson. 2018. SentencePiece: A simple\nand language independent subword tokenizer and detok-\nenizer for neural text processing. EMNLP .\nLample, G. and A. Conneau. 2019. Cross-lingual language\nmodel pretraining. NeurIPS , volume 32.\nLiu, Y ., M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,\nO. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov.\n2019. RoBERTa: A robustly optimized BERT pretraining\napproach. ArXiv preprint arXiv:1907.11692.\nLoureiro, D. and A. Jorge. 2019. Language modelling makes\nsense: Propagating representations through WordNet for\nfull-coverage word sense disambiguation. ACL.\nMelamud, O., J. Goldberger, and I. Dagan. 2016. con-\ntext2vec: Learning generic context embedding with bidi-\nrectional LSTM. CoNLL .\nPapadimitriou, I., K. Lopez, and D. Jurafsky. 2023. Multilin-",
    "metadata": {
      "source": "11",
      "chunk_id": 25,
      "token_count": 751,
      "chapter_title": ""
    }
  },
  {
    "content": "representing and predicting spans. TACL , 8:64\u201377.\nKudo, T. and J. Richardson. 2018. SentencePiece: A simple\nand language independent subword tokenizer and detok-\nenizer for neural text processing. EMNLP .\nLample, G. and A. Conneau. 2019. Cross-lingual language\nmodel pretraining. NeurIPS , volume 32.\nLiu, Y ., M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,\nO. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov.\n2019. RoBERTa: A robustly optimized BERT pretraining\napproach. ArXiv preprint arXiv:1907.11692.\nLoureiro, D. and A. Jorge. 2019. Language modelling makes\nsense: Propagating representations through WordNet for\nfull-coverage word sense disambiguation. ACL.\nMelamud, O., J. Goldberger, and I. Dagan. 2016. con-\ntext2vec: Learning generic context embedding with bidi-\nrectional LSTM. CoNLL .\nPapadimitriou, I., K. Lopez, and D. Jurafsky. 2023. Multilin-\ngual BERT has an accent: Evaluating English in\ufb02uences\non \ufb02uency in multilingual models. EACL Findings .\nPeters, M., M. Neumann, M. Iyyer, M. Gardner, C. Clark,\nK. Lee, and L. Zettlemoyer. 2018. Deep contextualized\nword representations. NAACL HLT .\nRamshaw, L. A. and M. P. Marcus. 1995. Text chunking\nusing transformation-based learning. Proceedings of the\n3rd Annual Workshop on Very Large Corpora .\nSchuster, M. and K. Nakajima. 2012. Japanese and Korean\nvoice search. ICASSP .\nTaylor, W. L. 1953. Cloze procedure: A new tool for mea-\nsuring readability. Journalism Quarterly , 30:415\u2013433.Timkey, W. and M. van Schijndel. 2021. All bark and no\nbite: Rogue dimensions in transformer language models\nobscure representational quality. EMNLP .\nWilliams, A., N. Nangia, and S. Bowman. 2018. A broad-\ncoverage challenge corpus for sentence understanding\nthrough inference. NAACL HLT .\nZhou, K., K. Ethayarajh, D. Card, and D. Jurafsky. 2022.\nProblems with cosine as a measure of embedding simi-\nlarity for high frequency words. ACL.\nZhu, Y ., R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun,\nA. Torralba, and S. Fidler. 2015. Aligning books and\nmovies: Towards story-like visual explanations by watch-\ning movies and reading books. IEEE International Con-\nference on Computer Vision .",
    "metadata": {
      "source": "11",
      "chunk_id": 26,
      "token_count": 683,
      "chapter_title": ""
    }
  }
]