[
  {
    "content": "# 8\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n8RNNs and LSTMs\nTime will explain.\nJane Austen, Persuasion\nLanguage is an inherently temporal phenomenon. Spoken language is a sequence of\nacoustic events over time, and we comprehend and produce both spoken and written\nlanguage as a sequential input stream. The temporal nature of language is re\ufb02ected\nin the metaphors we use; we talk of the \ufb02ow of conversations ,news feeds , and twitter\nstreams , all of which emphasize that language is a sequence that unfolds in time.\nYet most of the machine learning approaches we\u2019ve studied so far, like those\nfor sentiment analysis and other text classi\ufb01cation tasks don\u2019t have this temporal\nnature \u2013 they assume simultaneous access to all aspects of their input. The feedfor-\nward networks of Chapter 7 also assumed simultaneous access, although they also\nhad a simple model for time. Recall that we applied feedforward networks to lan-\nguage modeling by having them look only at a \ufb01xed-size window of words, and then\nsliding this window over the input, making independent predictions along the way.\nThis sliding-window approach is also used in the transformer architecture we will\nintroduce in Chapter 9.\nThis chapter introduces a deep learning architecture that offers an alternative\nway of representing time: recurrent neural networks (RNNs), and their variants like\nLSTMs. RNNs have a mechanism that deals directly with the sequential nature of\nlanguage, allowing them to handle the temporal nature of language without the use of\narbitrary \ufb01xed-sized windows. The recurrent network offers a new way to represent\nthe prior context, in its recurrent connections , allowing the model\u2019s decision to\ndepend on information from hundreds of words in the past. We\u2019ll see how to apply\nthe model to the task of language modeling, to text classi\ufb01cation tasks like sentiment\nanalysis, and to sequence modeling tasks like part-of-speech tagging (a task we\u2019ll\nreturn to in detail in Chapter 17).\n8.1 Recurrent Neural Networks\nA recurrent neural network (RNN) is any network that contains a cycle within its\nnetwork connections, meaning that the value of some unit is directly, or indirectly,\ndependent on its own earlier outputs as an input. While powerful, such networks\nare dif\ufb01cult to reason about and to train. However, within the general class of recur-\nrent networks there are constrained architectures that have proven to be extremely\neffective when applied to language. In this section, we consider a class of recurrent\nnetworks referred to as Elman Networks (Elman, 1990) or simple recurrent net-Elman\nNetworks\nworks . These networks are useful in their own right and serve as the basis for more\ncomplex approaches like the Long Short-Term Memory (LSTM) networks discussed",
    "metadata": {
      "source": "8",
      "chunk_id": 0,
      "token_count": 631,
      "chapter_title": "8"
    }
  },
  {
    "content": "## Page 2\n\n2CHAPTER 8 \u2022 RNN S AND LSTM S\nlater in this chapter. In this chapter when we use the term RNN we\u2019ll be referring to\nthese simpler more constrained networks (although you will often see the term RNN\nto mean any net with recurrent properties including LSTMs).\nxthtyt\nFigure 8.1 Simple recurrent neural network after Elman (1990). The hidden layer includes\na recurrent connection as part of its input. That is, the activation value of the hidden layer\ndepends on the current input as well as the activation value of the hidden layer from the\nprevious time step.\nFig. 8.1 illustrates the structure of an RNN. As with ordinary feedforward net-\nworks, an input vector representing the current input, xt, is multiplied by a weight\nmatrix and then passed through a non-linear activation function to compute the val-\nues for a layer of hidden units. This hidden layer is then used to calculate a cor-\nresponding output, yt. In a departure from our earlier window-based approach, se-\nquences are processed by presenting one item at a time to the network. We\u2019ll use\nsubscripts to represent time, thus xtwill mean the input vector xat time t. The key\ndifference from a feedforward network lies in the recurrent link shown in the \ufb01gure\nwith the dashed line. This link augments the input to the computation at the hidden\nlayer with the value of the hidden layer from the preceding point in time .\nThe hidden layer from the previous time step provides a form of memory, or\ncontext, that encodes earlier processing and informs the decisions to be made at\nlater points in time. Critically, this approach does not impose a \ufb01xed-length limit\non this prior context; the context embodied in the previous hidden layer can include\ninformation extending back to the beginning of the sequence.\nAdding this temporal dimension makes RNNs appear to be more complex than\nnon-recurrent architectures. But in reality, they\u2019re not all that different. Given an\ninput vector and the values for the hidden layer from the previous time step, we\u2019re\nstill performing the standard feedforward calculation introduced in Chapter 7. To\nsee this, consider Fig. 8.2 which clari\ufb01es the nature of the recurrence and how it\nfactors into the computation at the hidden layer. The most signi\ufb01cant change lies in\nthe new set of weights, U, that connect the hidden layer from the previous time step\nto the current hidden layer. These weights determine how the network makes use of\npast context in calculating the output for the current input. As with the other weights\nin the network, these connections are trained via backpropagation.\n8.1.1 Inference in RNNs\nForward inference (mapping a sequence of inputs to a sequence of outputs) in an\nRNN is nearly identical to what we\u2019ve already seen with feedforward networks. To\ncompute an output ytfor an input xt, we need the activation value for the hidden\nlayer ht. To calculate this, we multiply the input xtwith the weight matrix W, and\nthe hidden layer from the previous time step ht\u00001with the weight matrix U. We\nadd these values together and pass them through a suitable activation function, g,\nto arrive at the activation value for the current hidden layer, ht. Once we have the\nvalues for the hidden layer, we proceed with the usual computation to generate the",
    "metadata": {
      "source": "8",
      "chunk_id": 1,
      "token_count": 725,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\n8.1 \u2022 R ECURRENT NEURAL NETWORKS 3\n+UVWyt\nxththt-1\nFigure 8.2 Simple recurrent neural network illustrated as a feedforward network. The hid-\nden layer ht\u00001from the prior time step is multiplied by weight matrix Uand then added to\nthe feedforward component from the current time step.\noutput vector.\nht=g(Uht\u00001+Wx t) (8.1)\nyt=f(Vht) (8.2)\nLet\u2019s refer to the input, hidden and output layer dimensions as din,dh, and dout\nrespectively. Given this, our three parameter matrices are: W2Rdh\u0002din,U2Rdh\u0002dh,\nandV2Rdout\u0002dh.\nWe compute ytvia a softmax computation that gives a probability distribution\nover the possible output classes.\nyt=softmax (Vht) (8.3)\nThe fact that the computation at time trequires the value of the hidden layer from\ntime t\u00001 mandates an incremental inference algorithm that proceeds from the start\nof the sequence to the end as illustrated in Fig. 8.3. The sequential nature of simple\nrecurrent networks can also be seen by unrolling the network in time as is shown in\nFig. 8.4. In this \ufb01gure, the various layers of units are copied for each time step to\nillustrate that they will have differing values over time. However, the various weight\nmatrices are shared across time.\nfunction FORWARD RNN( x,network )returns output sequence y\nh0 0\nfori 1toLENGTH (x)do\nhi g(Uhi\u00001+Wx i)\nyi f(Vhi)\nreturn y\nFigure 8.3 Forward inference in a simple recurrent network. The matrices U,VandWare\nshared across time, while new values for handyare calculated with each time step.\n8.1.2 Training\nAs with feedforward networks, we\u2019ll use a training set, a loss function, and back-\npropagation to obtain the gradients needed to adjust the weights in these recurrent\nnetworks. As shown in Fig. 8.2, we now have 3 sets of weights to update: W, the",
    "metadata": {
      "source": "8",
      "chunk_id": 2,
      "token_count": 471,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4CHAPTER 8 \u2022 RNN S AND LSTM S\nUVWUVWUVW\nx1x2x3y1y2y3\nh1h3h2\nh0\nFigure 8.4 A simple recurrent neural network shown unrolled in time. Network layers are recalculated for\neach time step, while the weights U,VandWare shared across all time steps.\nweights from the input layer to the hidden layer, U, the weights from the previous\nhidden layer to the current hidden layer, and \ufb01nally V, the weights from the hidden\nlayer to the output layer.\nFig. 8.4 highlights two considerations that we didn\u2019t have to worry about with\nbackpropagation in feedforward networks. First, to compute the loss function for\nthe output at time twe need the hidden layer from time t\u00001. Second, the hidden\nlayer at time tin\ufb02uences both the output at time tand the hidden layer at time t+1\n(and hence the output and loss at t+1). It follows from this that to assess the error\naccruing to ht, we\u2019ll need to know its in\ufb02uence on both the current output as well as\nthe ones that follow .\nTailoring the backpropagation algorithm to this situation leads to a two-pass al-\ngorithm for training the weights in RNNs. In the \ufb01rst pass, we perform forward\ninference, computing ht,yt, accumulating the loss at each step in time, saving the\nvalue of the hidden layer at each step for use at the next time step. In the second\nphase, we process the sequence in reverse, computing the required gradients as we\ngo, computing and saving the error term for use in the hidden layer for each step\nbackward in time. This general approach is commonly referred to as backpropaga-\ntion through time (Werbos 1974, Rumelhart et al. 1986, Werbos 1990).backpropaga-\ntion through\ntimeFortunately, with modern computational frameworks and adequate computing\nresources, there is no need for a specialized approach to training RNNs. As illus-\ntrated in Fig. 8.4, explicitly unrolling a recurrent network into a feedforward com-\nputational graph eliminates any explicit recurrences, allowing the network weights\nto be trained directly. In such an approach, we provide a template that speci\ufb01es the\nbasic structure of the network, including all the necessary parameters for the input,\noutput, and hidden layers, the weight matrices, as well as the activation and output\nfunctions to be used. Then, when presented with a speci\ufb01c input sequence, we can\ngenerate an unrolled feedforward network speci\ufb01c to that input, and use that graph\nto perform forward inference or training via ordinary backpropagation.",
    "metadata": {
      "source": "8",
      "chunk_id": 3,
      "token_count": 604,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\n8.2 \u2022 RNN S AS LANGUAGE MODELS 5\nFor applications that involve much longer input sequences, such as speech recog-\nnition, character-level processing, or streaming continuous inputs, unrolling an en-\ntire input sequence may not be feasible. In these cases, we can unroll the input into\nmanageable \ufb01xed-length segments and treat each segment as a distinct training item.\n8.2 RNNs as Language Models\nLet\u2019s see how to apply RNNs to the language modeling task. Recall from Chapter 3\nthat language models predict the next word in a sequence given some preceding\ncontext. For example, if the preceding context is \u201cThanks for all the\u201d and we want\nto know how likely the next word is \u201c\ufb01sh\u201d we would compute:\nP(\ufb01shjThanks for all the )\nLanguage models give us the ability to assign such a conditional probability to every\npossible next word, giving us a distribution over the entire vocabulary. We can also\nassign probabilities to entire sequences by combining these conditional probabilities\nwith the chain rule:\nP(w1:n) =nY\ni=1P(wijw<i)\nThe n-gram language models of Chapter 3 compute the probability of a word given\ncounts of its occurrence with the n\u00001 prior words. The context is thus of size n\u00001.\nFor the feedforward language models of Chapter 7, the context is the window size.\nRNN language models (Mikolov et al., 2010) process the input sequence one\nword at a time, attempting to predict the next word from the current word and the\nprevious hidden state. RNNs thus don\u2019t have the limited context problem that n-gram\nmodels have, or the \ufb01xed context that feedforward language models have, since the\nhidden state can in principle represent information about all of the preceding words\nall the way back to the beginning of the sequence. Fig. 8.5 sketches this difference\nbetween a FFN language model and an RNN language model, showing that the\nRNN language model uses ht\u00001, the hidden state from the previous time step, as a\nrepresentation of the past context.\n8.2.1 Forward Inference in an RNN language model\nForward inference in a recurrent language model proceeds exactly as described in\nSection 8.1.1. The input sequence X= [x1;:::;xt;:::;xN]consists of a series of words\neach represented as a one-hot vector of size jVj\u00021, and the output prediction, y, is a\nvector representing a probability distribution over the vocabulary. At each step, the\nmodel uses the word embedding matrix Eto retrieve the embedding for the current\nword, multiples it by the weight matrix W, and then adds it to the hidden layer from\nthe previous step (weighted by weight matrix U) to compute a new hidden layer.\nThis hidden layer is then used to generate an output layer which is passed through a\nsoftmax layer to generate a probability distribution over the entire vocabulary. That\nis, at time t:\net=Ext (8.4)\nht=g(Uht\u00001+We t) (8.5)\n^ yt=softmax (Vht) (8.6)",
    "metadata": {
      "source": "8",
      "chunk_id": 4,
      "token_count": 693,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6CHAPTER 8 \u2022 RNN S AND LSTM S\nVWethtUht-1ethtet-1et-2U       Wa)b)^yt\net-1^ytht-2WWet-2U\nFigure 8.5 Simpli\ufb01ed sketch of two LM architectures moving through a text, showing a\nschematic context of three tokens: (a) a feedforward neural language model which has a \ufb01xed\ncontext input to the weight matrix W, (b) an RNN language model, in which the hidden state\nht\u00001summarizes the prior context.\nWhen we do language modeling with RNNs (and we\u2019ll see this again in Chapter 9\nwith transformers), it\u2019s convenient to make the assumption that the embedding di-\nmension deand the hidden dimension dhare the same. So we\u2019ll just call both of\nthese the model dimension d. So the embedding matrix Eis of shape [d\u0002jVj], and\nxtis a one-hot vector of shape [jVj\u00021]. The product etis thus of shape [d\u00021].W\nandUare of shape [d\u0002d], sohtis also of shape [d\u00021].Vis of shape [jVj\u0002d],\nso the result of Vhis a vector of shape [jVj\u00021]. This vector can be thought of as\na set of scores over the vocabulary given the evidence provided in h. Passing these\nscores through the softmax normalizes the scores into a probability distribution. The\nprobability that a particular word kin the vocabulary is the next word is represented\nby^ yt[k], the kth component of ^ yt:\nP(wt+1=kjw1;:::; wt) = ^ yt[k] (8.7)\nThe probability of an entire sequence is just the product of the probabilities of each\nitem in the sequence, where we\u2019ll use ^ yi[wi]to mean the probability of the true word\nwiat time step i.\nP(w1:n) =nY\ni=1P(wijw1:i\u00001) (8.8)\n=nY\ni=1^ yi[wi] (8.9)\n8.2.2 Training an RNN language model\nTo train an RNN as a language model, we use the same self-supervision (orself- self-supervision\ntraining ) algorithm we saw in Section ??: we take a corpus of text as training\nmaterial and at each time step task the model to predict the next word. We call\nsuch a model self-supervised because we don\u2019t have to add any special gold labels\nto the data; the natural sequence of words is its own supervision! We simply train\nthe model to minimize the error in predicting the true next word in the training\nsequence, using cross-entropy as the loss function. Recall that the cross-entropy\nloss measures the difference between a predicted probability distribution and the",
    "metadata": {
      "source": "8",
      "chunk_id": 5,
      "token_count": 624,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7",
    "metadata": {
      "source": "8",
      "chunk_id": 6,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "8.2 \u2022 RNN S AS LANGUAGE MODELS 7\nInputEmbeddingsSoftmax overVocabulary\nSolongandthanksforlongandthanksforNext wordall\u2026Loss\u2026\u2026RNNhyVh",
    "metadata": {
      "source": "8",
      "chunk_id": 7,
      "token_count": 43,
      "chapter_title": ""
    }
  },
  {
    "content": "<latexit sha1_base64=\"9tru+5ysH1zS9iUXRg/IsnxmpMA=\">AAAB/XicbVDLSsNAFL3xWesr6lKQwSK4sSQi1WXRjcsK9gFNCZPpJB06yYSZiRBCcOOvuBFxo+Av+Av+jUnbTVsPDBzOOcO993gxZ0pb1q+xsrq2vrFZ2apu7+zu7ZsHhx0lEklomwguZM/DinIW0bZmmtNeLCkOPU673viu9LtPVComokedxnQQ4iBiPiNYF5Jrnlw4XATIGWGdpbmbOSHWIxlmXERBnldds2bVrQnQMrFnpAYztFzzxxkKkoQ00oRjpfq2FetBhqVmhNO86iSKxpiMcUCzyfY5OiukIfKFLF6k0USdy+FQqTT0imS5nFr0SvE/r59o/2aQsShONI3IdJCfcKQFKqtAQyYp0TwtCCaSFRsiMsISE10UVp5uLx66TDqXdbtRbzxc1Zq3sxIqcAyncA42XEMT7qEFbSDwAm/wCV/Gs/FqvBsf0+iKMftzBHMwvv8ADJKVcA==</latexit>\u0000log \u02c6ylong<latexit sha1_base64=\"tuzkS/BeX/Xmg79qpWZlpeYDhtE=\">AAAB/HicbVDLSsNAFL3xWesr6lKEwSK4sSQi1WXRjcsK9gFNCZPJpB06mYSZiRBC3PgrbkTcKPgN/oJ/Y9J209YDA4dzznDvPV7MmdKW9WusrK6tb2xWtqrbO7t7++bBYUdFiSS0TSIeyZ6HFeVM0LZmmtNeLCkOPU673viu9LtPVCoWiUedxnQQ4qFgASNYF5Jrnlw4PBoiZ4R1luZu5oRYj2SYYeHnedU1a1bdmgAtE3tGajBDyzV/HD8iSUiFJhwr1betWA8yLDUjnOZVJ1E0xmSMhzSbLJ+js0LyURDJ4gmNJupcDodKpaFXJMvd1KJXiv95/UQHN4OMiTjRVJDpoCDhSEeobAL5TFKieVoQTCQrNkRkhCUmuuirPN1ePHSZdC7rdqPeeLiqNW9nJVTgGE7hHGy4hibcQwvaQOAF3uATvoxn49V4Nz6m0RVj9ucI5mB8/wEiupTp</latexit>\u0000log \u02c6yand<latexit",
    "metadata": {
      "source": "8",
      "chunk_id": 8,
      "token_count": 721,
      "chapter_title": ""
    }
  },
  {
    "content": "\u02c6yand<latexit sha1_base64=\"0zdsmbBovZ+hafWZN7Hvufo85tU=\">AAAB/3icbVDLSsNAFJ3UV62vqEs3g0VwY0lEqsuiG5cV7AOaEibTSTN0kgkzN0IIWbjxV9yIuFHwD/wF/8ak7aatBwYO55zh3nu8WHANlvVrVNbWNza3qtu1nd29/QPz8KirZaIo61AppOp7RDPBI9YBDoL1Y8VI6AnW8yZ3pd97YkpzGT1CGrNhSMYR9zklUEiuiS8cIcfYCQhkae5mTkggUGEGAYkmOs9rrlm3GtYUeJXYc1JHc7Rd88cZSZqELAIqiNYD24phmBEFnAqW15xEs5jQCRmzbLp/js8KaYR9qYoXAZ6qCzkSap2GXpEs19PLXin+5w0S8G+GGY/iBFhEZ4P8RGCQuCwDj7hiFERaEEIVLzbENCCKUCgqK0+3lw9dJd3Lht1sNB+u6q3beQlVdIJO0Tmy0TVqoXvURh1E0Qt6Q5/oy3g2Xo1342MWrRjzP8doAcb3H7Aall0=</latexit>\u0000log \u02c6ythanks<latexit sha1_base64=\"D3c31Jvxp3QWPr2h4tzQWmeenDs=\">AAAB/HicbVDLSsNAFL3xWesr6lKEwSK4sSQi1WXRjcsK9gFNCZPppB06yYSZiRBC3PgrbkTcKPgN/oJ/Y9Jm09YDA4dzznDvPV7EmdKW9WusrK6tb2xWtqrbO7t7++bBYUeJWBLaJoIL2fOwopyFtK2Z5rQXSYoDj9OuN7kr/O4TlYqJ8FEnER0EeBQynxGsc8k1Ty4cLkbIGWOdJpmbOgHWYxmkvpBZVnXNmlW3pkDLxC5JDUq0XPPHGQoSBzTUhGOl+rYV6UGKpWaE06zqxIpGmEzwiKbT5TN0lktDlM/LX6jRVJ3L4UCpJPDyZLGbWvQK8T+vH2v/ZpCyMIo1DclskB9zpAUqmkBDJinRPMkJJpLlGyIyxhITnfdVnG4vHrpMOpd1u1FvPFzVmrdlCRU4hlM4BxuuoQn30II2EHiBN/iEL+PZeDXejY9ZdMUo/xzBHIzvP0CJlP0=</latexit>\u0000log \u02c6yfor<latexit",
    "metadata": {
      "source": "8",
      "chunk_id": 9,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "\u02c6yfor<latexit sha1_base64=\"PI3y1fb9LhumoVCQRh2+Y84dRkc=\">AAAB/HicbVDLSsNAFL3xWesr6lKEwSK4sSQi1WXRjcsK9gFNCZPppB06yYSZiRBC3PgrbkTcKPgN/oJ/Y9Jm09YDA4dzznDvPV7EmdKW9WusrK6tb2xWtqrbO7t7++bBYUeJWBLaJoIL2fOwopyFtK2Z5rQXSYoDj9OuN7kr/O4TlYqJ8FEnER0EeBQynxGsc8k1Ty4cLkbIGWOdJpmbOgHWYxmkmPMsq7pmzapbU6BlYpekBiVarvnjDAWJAxpqwrFSfduK9CDFUjPCaVZ1YkUjTCZ4RNPp8hk6y6Uh8oXMX6jRVJ3L4UCpJPDyZLGbWvQK8T+vH2v/ZpCyMIo1DclskB9zpAUqmkBDJinRPMkJJpLlGyIyxhITnfdVnG4vHrpMOpd1u1FvPFzVmrdlCRU4hlM4BxuuoQn30II2EHiBN/iEL+PZeDXejY9ZdMUo/xzBHIzvPyumlO8=</latexit>\u0000log \u02c6yall",
    "metadata": {
      "source": "8",
      "chunk_id": 10,
      "token_count": 382,
      "chapter_title": ""
    }
  },
  {
    "content": "e\nFigure 8.6 Training RNNs as language models.\ncorrect distribution.\nLCE=\u0000X\nw2Vyt[w]log\u02c6yt[w] (8.10)\nIn the case of language modeling, the correct distribution ytcomes from knowing the\nnext word. This is represented as a one-hot vector corresponding to the vocabulary\nwhere the entry for the actual next word is 1, and all the other entries are 0. Thus,\nthe cross-entropy loss for language modeling is determined by the probability the\nmodel assigns to the correct next word. So at time tthe CE loss is the negative log\nprobability the model assigns to the next word in the training sequence.\nLCE(\u02c6yt;yt) =\u0000log\u02c6yt[wt+1] (8.11)\nThus at each word position tof the input, the model takes as input the correct word wt\ntogether with ht\u00001, encoding information from the preceding w1:t\u00001, and uses them\nto compute a probability distribution over possible next words so as to compute the\nmodel\u2019s loss for the next token wt+1. Then we move to the next word, we ignore\nwhat the model predicted for the next word and instead use the correct word wt+1\nalong with the prior history encoded to estimate the probability of token wt+2. This\nidea that we always give the model the correct history sequence to predict the next\nword (rather than feeding the model its best case from the previous time step) is\ncalled teacher forcing . teacher forcing\nThe weights in the network are adjusted to minimize the average CE loss over\nthe training sequence via gradient descent. Fig. 8.6 illustrates this training regimen.\n8.2.3 Weight Tying\nCareful readers may have noticed that the input embedding matrix Eand the \ufb01nal\nlayer matrix V, which feeds the output softmax, are quite similar.\nThe columns of Erepresent the word embeddings for each word in the vocab-\nulary learned during the training process with the goal that words that have similar\nmeaning and function will have similar embeddings. And, since when we use RNNs\nfor language modeling we make the assumption that the embedding dimension and",
    "metadata": {
      "source": "8",
      "chunk_id": 11,
      "token_count": 459,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8CHAPTER 8 \u2022 RNN S AND LSTM S\nthe hidden dimension are the same (= the model dimension d), the embedding ma-\ntrixEhas shape [d\u0002jVj]. And the \ufb01nal layer matrix Vprovides a way to score\nthe likelihood of each word in the vocabulary given the evidence present in the \ufb01nal\nhidden layer of the network through the calculation of Vh.Vis of shape [jVj\u0002d].\nThat is, is, the rows of Vare shaped like a transpose of E, meaning that Vprovides\nasecond set of learned word embeddings.\nInstead of having two sets of embedding matrices, language models use a single\nembedding matrix, which appears at both the input and softmax layers. That is,\nwe dispense with Vand use Eat the start of the computation and E|(because the\nshape of Vis the transpose of Eat the end. Using the same matrix (transposed) in\ntwo places is called weight tying .1The weight-tied equations for an RNN language weight tying\nmodel then become:\net=Ext (8.12)\nht=g(Uht\u00001+We t) (8.13)\n^ yt=softmax (E|ht) (8.14)\nIn addition to providing improved model perplexity, this approach signi\ufb01cantly re-\nduces the number of parameters required for the model.\n8.3 RNNs for other NLP tasks\nNow that we\u2019ve seen the basic RNN architecture, let\u2019s consider how to apply it to\nthree types of NLP tasks: sequence classi\ufb01cation tasks like sentiment analysis and\ntopic classi\ufb01cation, sequence labeling tasks like part-of-speech tagging, and text\ngeneration tasks, including with a new architecture called the encoder-decoder .\n8.3.1 Sequence Labeling\nIn sequence labeling, the network\u2019s task is to assign a label chosen from a small\n\ufb01xed set of labels to each element of a sequence. One classic sequence labeling\ntasks is part-of-speech (POS) tagging (assigning grammatical tags like NOUN and\nVERB to each word in a sentence). We\u2019ll discuss part-of-speech tagging in detail\nin Chapter 17, but let\u2019s give a motivating example here. In an RNN approach to\nsequence labeling, inputs are word embeddings and the outputs are tag probabilities\ngenerated by a softmax layer over the given tagset, as illustrated in Fig. 8.7.\nIn this \ufb01gure, the inputs at each time step are pretrained word embeddings cor-\nresponding to the input tokens. The RNN block is an abstraction that represents\nan unrolled simple recurrent network consisting of an input layer, hidden layer, and\noutput layer at each time step, as well as the shared U,VandWweight matrices\nthat comprise the network. The outputs of the network at each time step represent\nthe distribution over the POS tagset generated by a softmax layer.\nTo generate a sequence of tags for a given input, we run forward inference over\nthe input sequence and select the most likely tag from the softmax at each step. Since\nwe\u2019re using a softmax layer to generate the probability distribution over the output\ntagset at each time step, we will again employ the cross-entropy loss during training.\n1We also do this for transformers (Chapter 9) where it\u2019s common to call E|theunembedding matrix .",
    "metadata": {
      "source": "8",
      "chunk_id": 12,
      "token_count": 714,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\n8.3 \u2022 RNN S FOR OTHER NLP TASKS 9\nJanetwillbackthebillNNDTVBMDNNPArgmax\nEmbeddingsWordsehVhyRNNLayer(s)Softmax overtags\nFigure 8.7 Part-of-speech tagging as sequence labeling with a simple RNN. The goal of\npart-of-speech (POS) tagging is to assign a grammatical label to each word in a sentence,\ndrawn from a prede\ufb01ned set of tags. (The tags for this sentence include NNP (proper noun),\nMD (modal verb) and others; we\u2019ll give a complete description of the task of part-of-speech\ntagging in Chapter 17.) Pre-trained word embeddings serve as inputs and a softmax layer\nprovides a probability distribution over the part-of-speech tags as output at each time step.\n8.3.2 RNNs for Sequence Classi\ufb01cation\nAnother use of RNNs is to classify entire sequences rather than the tokens within\nthem. This is the set of tasks commonly called text classi\ufb01cation , like sentiment\nanalysis or spam detection, in which we classify a text into two or three classes\n(like positive or negative), as well as classi\ufb01cation tasks with a large number of\ncategories, like document-level topic classi\ufb01cation, or message routing for customer\nservice applications.\nTo apply RNNs in this setting, we pass the text to be classi\ufb01ed through the RNN\na word at a time generating a new hidden layer representation at each time step.\nWe can then take the hidden layer for the last token of the text, hn, to constitute a\ncompressed representation of the entire sequence. We can pass this representation\nhnto a feedforward network that chooses a class via a softmax over the possible\nclasses. Fig. 8.8 illustrates this approach.\nNote that in this approach we don\u2019t need intermediate outputs for the words in\nthe sequence preceding the last element. Therefore, there are no loss terms associ-\nated with those elements. Instead, the loss function used to train the weights in the\nnetwork is based entirely on the \ufb01nal text classi\ufb01cation task. The output from the\nsoftmax output from the feedforward classi\ufb01er together with a cross-entropy loss\ndrives the training. The error signal from the classi\ufb01cation is backpropagated all the\nway through the weights in the feedforward classi\ufb01er through, to its input, and then\nthrough to the three sets of weights in the RNN as described earlier in Section 8.1.2.\nThe training regimen that uses the loss from a downstream application to adjust the\nweights all the way through the network is referred to as end-to-end training .end-to-end\ntraining\nAnother option, instead of using just hidden state of the last token hnto represent\nthe whole sequence, is to use some sort of pooling function of all the hidden states pooling\nhifor each word iin the sequence. For example, we can create a representation that",
    "metadata": {
      "source": "8",
      "chunk_id": 13,
      "token_count": 655,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 CHAPTER 8 \u2022 RNN S AND LSTM S\nx1\nRNNhnx2x3xnSoftmaxFFN\nFigure 8.8 Sequence classi\ufb01cation using a simple RNN combined with a feedforward net-\nwork. The \ufb01nal hidden state from the RNN is used as the input to a feedforward network that\nperforms the classi\ufb01cation.\npools all the nhidden states by taking their element-wise mean:\nhmean=1\nnnX\ni=1hi (8.15)\nOr we can take the element-wise max; the element-wise max of a set of nvectors is\na new vector whose kth element is the max of the kth elements of all the nvectors.\nThe long contexts of RNNs makes it quite dif\ufb01cult to successfully backpropagate\nerror all the way through the entire input; we\u2019ll talk about this problem, and some\nstandard solutions, in Section 8.5.\n8.3.3 Generation with RNN-Based Language Models\nRNN-based language models can also be used to generate text. Text generation is\nof enormous practical importance, part of tasks like question answering, machine\ntranslation, text summarization, grammar correction, story generation, and conver-\nsational dialogue; any task where a system needs to produce text, conditioned on\nsome other text. This use of a language model to generate text is one of the areas\nin which the impact of neural language models on NLP has been the largest. Text\ngeneration, along with image generation and code generation, constitute a new area\nof AI that is often called generative AI .\nRecall back in Chapter 3 we saw how to generate text from an n-gram language\nmodel by adapting a sampling technique suggested at about the same time by Claude\nShannon (Shannon, 1951) and the psychologists George Miller and Jennifer Self-\nridge (Miller and Selfridge, 1950). We \ufb01rst randomly sample a word to begin a\nsequence based on its suitability as the start of a sequence. We then continue to\nsample words conditioned on our previous choices until we reach a pre-determined\nlength, or an end of sequence token is generated.\nToday, this approach of using a language model to incrementally generate words\nby repeatedly sampling the next word conditioned on our previous choices is called\nautoregressive generation orcausal LM generation . The procedure is basicallyautoregressive\ngeneration\nthe same as that described on page ??, but adapted to a neural context:\n\u2022 Sample a word in the output from the softmax distribution that results from\nusing the beginning of sentence marker, <s>, as the \ufb01rst input.",
    "metadata": {
      "source": "8",
      "chunk_id": 14,
      "token_count": 566,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\n8.4 \u2022 S TACKED AND BIDIRECTIONAL RNN ARCHITECTURES 11\n\u2022 Use the word embedding for that \ufb01rst word as the input to the network at the\nnext time step, and then sample the next word in the same fashion.\n\u2022 Continue generating until the end of sentence marker, </s> , is sampled or a\n\ufb01xed length limit is reached.\nTechnically an autoregressive model is a model that predicts a value at time tbased\non a linear function of the previous values at times t\u00001,t\u00002, and so on. Although\nlanguage models are not linear (since they have many layers of non-linearities), we\nloosely refer to this generation technique as autoregressive generation since the\nword generated at each time step is conditioned on the word selected by the network\nfrom the previous step. Fig. 8.9 illustrates this approach. In this \ufb01gure, the details of\nthe RNN\u2019s hidden layers and recurrent connections are hidden within the blue block.\nThis simple architecture underlies state-of-the-art approaches to applications\nsuch as machine translation, summarization, and question answering. The key to\nthese approaches is to prime the generation component with an appropriate context.\nThat is, instead of simply using <s> to get things started we can provide a richer\ntask-appropriate context; for translation the context is the sentence in the source\nlanguage; for summarization it\u2019s the long text we want to summarize.\nSolong\n<s>and\nSolongand?Sampled WordSoftmaxEmbeddingInput WordRNN\nFigure 8.9 Autoregressive generation with an RNN-based neural language model.\n8.4 Stacked and Bidirectional RNN architectures\nRecurrent networks are quite \ufb02exible. By combining the feedforward nature of un-\nrolled computational graphs with vectors as common inputs and outputs, complex\nnetworks can be treated as modules that can be combined in creative ways. This\nsection introduces two of the more common network architectures used in language\nprocessing with RNNs.\n8.4.1 Stacked RNNs\nIn our examples thus far, the inputs to our RNNs have consisted of sequences of\nword or character embeddings (vectors) and the outputs have been vectors useful for\npredicting words, tags or sequence labels. However, nothing prevents us from using",
    "metadata": {
      "source": "8",
      "chunk_id": 15,
      "token_count": 501,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 CHAPTER 8 \u2022 RNN S AND LSTM S\nthe entire sequence of outputs from one RNN as an input sequence to another one.\nStacked RNNs consist of multiple networks where the output of one layer serves as Stacked RNNs\nthe input to a subsequent layer, as shown in Fig. 8.10.\ny1y2y3yn\nx1x2x3xn\nRNN 1\nRNN 2\n RNN 3\nFigure 8.10 Stacked recurrent networks. The output of a lower level serves as the input to\nhigher levels with the output of the last network serving as the \ufb01nal output.\nStacked RNNs generally outperform single-layer networks. One reason for this\nsuccess seems to be that the network induces representations at differing levels of\nabstraction across layers. Just as the early stages of the human visual system detect\nedges that are then used for \ufb01nding larger regions and shapes, the initial layers of\nstacked networks can induce representations that serve as useful abstractions for\nfurther layers\u2014representations that might prove dif\ufb01cult to induce in a single RNN.\nThe optimal number of stacked RNNs is speci\ufb01c to each application and to each\ntraining set. However, as the number of stacks is increased the training costs rise\nquickly.\n8.4.2 Bidirectional RNNs\nThe RNN uses information from the left (prior) context to make its predictions at\ntime t. But in many applications we have access to the entire input sequence; in\nthose cases we would like to use words from the context to the right of t. One way\nto do this is to run two separate RNNs, one left-to-right, and one right-to-left, and\nconcatenate their representations.\nIn the left-to-right RNNs we\u2019ve discussed so far, the hidden state at a given time\ntrepresents everything the network knows about the sequence up to that point. The\nstate is a function of the inputs x1;:::;xtand represents the context of the network to\nthe left of the current time.\nhf\nt=RNN forward (x1;:::;xt) (8.16)\nThis new notation hf\ntsimply corresponds to the normal hidden state at time t, repre-\nsenting everything the network has gleaned from the sequence so far.\nTo take advantage of context to the right of the current input, we can train an\nRNN on a reversed input sequence. With this approach, the hidden state at time t\nrepresents information about the sequence to the right of the current input:\nhb\nt=RNN backward (xt;:::xn) (8.17)",
    "metadata": {
      "source": "8",
      "chunk_id": 16,
      "token_count": 578,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\n8.4 \u2022 S TACKED AND BIDIRECTIONAL RNN ARCHITECTURES 13\nHere, the hidden state hb\ntrepresents all the information we have discerned about the\nsequence from tto the end of the sequence.\nAbidirectional RNN (Schuster and Paliwal, 1997) combines two independentbidirectional\nRNN\nRNNs, one where the input is processed from the start to the end, and the other from\nthe end to the start. We then concatenate the two representations computed by the\nnetworks into a single vector that captures both the left and right contexts of an input\nat each point in time. Here we use either the semicolon \u201d;\u201d or the equivalent symbol\n\bto mean vector concatenation:\nht= [hf\nt;hb\nt]\n=hf\nt\bhb\nt (8.18)\nFig. 8.11 illustrates such a bidirectional network that concatenates the outputs of\nthe forward and backward pass. Other simple ways to combine the forward and\nbackward contexts include element-wise addition or multiplication. The output at\neach step in time thus captures information to the left and to the right of the current\ninput. In sequence labeling applications, these concatenated outputs can serve as the\nbasis for a local labeling decision.\nRNN 2 \nRNN 1x1y2y1y3ynconcatenatedoutputs\nx2x3xn\nFigure 8.11 A bidirectional RNN. Separate models are trained in the forward and backward\ndirections, with the output of each model at each time point concatenated to represent the\nbidirectional state at that time point.\nBidirectional RNNs have also proven to be quite effective for sequence classi\ufb01-\ncation. Recall from Fig. 8.8 that for sequence classi\ufb01cation we used the \ufb01nal hidden\nstate of the RNN as the input to a subsequent feedforward classi\ufb01er. A dif\ufb01culty\nwith this approach is that the \ufb01nal state naturally re\ufb02ects more information about\nthe end of the sentence than its beginning. Bidirectional RNNs provide a simple\nsolution to this problem; as shown in Fig. 8.12, we simply combine the \ufb01nal hidden\nstates from the forward and backward passes (for example by concatenation) and\nuse that as input for follow-on processing.",
    "metadata": {
      "source": "8",
      "chunk_id": 17,
      "token_count": 513,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n14 CHAPTER 8 \u2022 RNN S AND LSTM S\nRNN 2 \nRNN 1x1x2x3xnhn\u2192h1\u2190hn\u2192SoftmaxFFNh1\u2190\nFigure 8.12 A bidirectional RNN for sequence classi\ufb01cation. The \ufb01nal hidden units from\nthe forward and backward passes are combined to represent the entire sequence. This com-\nbined representation serves as input to the subsequent classi\ufb01er.\n8.5 The LSTM\nIn practice, it is quite dif\ufb01cult to train RNNs for tasks that require a network to make\nuse of information distant from the current point of processing. Despite having ac-\ncess to the entire preceding sequence, the information encoded in hidden states tends\nto be fairly local, more relevant to the most recent parts of the input sequence and\nrecent decisions. Yet distant information is critical to many language applications.\nConsider the following example in the context of language modeling.\n(8.19) The \ufb02ights the airline was canceling were full.\nAssigning a high probability to wasfollowing airline is straightforward since airline\nprovides a strong local context for the singular agreement. However, assigning an\nappropriate probability to were is quite dif\ufb01cult, not only because the plural \ufb02ights\nis quite distant, but also because the singular noun airline is closer in the intervening\ncontext. Ideally, a network should be able to retain the distant information about\nplural \ufb02ights until it is needed, while still processing the intermediate parts of the\nsequence correctly.\nOne reason for the inability of RNNs to carry forward critical information is that\nthe hidden layers, and, by extension, the weights that determine the values in the hid-\nden layer, are being asked to perform two tasks simultaneously: provide information\nuseful for the current decision, and updating and carrying forward information re-\nquired for future decisions.\nA second dif\ufb01culty with training RNNs arises from the need to backpropagate\nthe error signal back through time. Recall from Section 8.1.2 that the hidden layer at\ntime tcontributes to the loss at the next time step since it takes part in that calcula-\ntion. As a result, during the backward pass of training, the hidden layers are subject\nto repeated multiplications, as determined by the length of the sequence. A frequent\nresult of this process is that the gradients are eventually driven to zero, a situation",
    "metadata": {
      "source": "8",
      "chunk_id": 18,
      "token_count": 518,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\n8.5 \u2022 T HELSTM 15\ncalled the vanishing gradients problem.vanishing\ngradients\nTo address these issues, more complex network architectures have been designed\nto explicitly manage the task of maintaining relevant context over time, by enabling\nthe network to learn to forget information that is no longer needed and to remember\ninformation required for decisions still to come.\nThe most commonly used such extension to RNNs is the long short-term mem-\nory(LSTM) network (Hochreiter and Schmidhuber, 1997). LSTMs divide the con-long short-term\nmemory\ntext management problem into two subproblems: removing information no longer\nneeded from the context, and adding information likely to be needed for later de-\ncision making. The key to solving both problems is to learn how to manage this\ncontext rather than hard-coding a strategy into the architecture. LSTMs accomplish\nthis by \ufb01rst adding an explicit context layer to the architecture (in addition to the\nusual recurrent hidden layer), and through the use of specialized neural units that\nmake use of gates to control the \ufb02ow of information into and out of the units that\ncomprise the network layers. These gates are implemented through the use of addi-\ntional weights that operate sequentially on the input, and previous hidden layer, and\nprevious context layers.\nThe gates in an LSTM share a common design pattern; each consists of a feed-\nforward layer, followed by a sigmoid activation function, followed by a pointwise\nmultiplication with the layer being gated. The choice of the sigmoid as the activation\nfunction arises from its tendency to push its outputs to either 0 or 1. Combining this\nwith a pointwise multiplication has an effect similar to that of a binary mask. Values\nin the layer being gated that align with values near 1 in the mask are passed through\nnearly unchanged; values corresponding to lower values are essentially erased.\nThe \ufb01rst gate we\u2019ll consider is the forget gate . The purpose of this gate is forget gate\nto delete information from the context that is no longer needed. The forget gate\ncomputes a weighted sum of the previous state\u2019s hidden layer and the current in-\nput and passes that through a sigmoid. This mask is then multiplied element-wise\nby the context vector to remove the information from context that is no longer re-\nquired. Element-wise multiplication of two vectors (represented by the operator \f,\nand sometimes called the Hadamard product ) is the vector of the same dimension\nas the two input vectors, where each element iis the product of element iin the two\ninput vectors:\nft=s(Ufht\u00001+Wfxt) (8.20)\nkt=ct\u00001\fft (8.21)\nThe next task is to compute the actual information we need to extract from the previ-\nous hidden state and current inputs\u2014the same basic computation we\u2019ve been using\nfor all our recurrent networks.\ngt=tanh(Ught\u00001+Wgxt) (8.22)\nNext, we generate the mask for the add gate to select the information to add to the add gate\ncurrent context.\nit=s(Uiht\u00001+Wixt) (8.23)\njt=gt\fit (8.24)\nNext, we add this to the modi\ufb01ed context vector to get our new context vector.\nct=jt+kt (8.25)",
    "metadata": {
      "source": "8",
      "chunk_id": 19,
      "token_count": 713,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\n16 CHAPTER 8 \u2022 RNN S AND LSTM S\n+\nxtht-1cthtcthtct-1ht-1xttanh\n+\u03c3tanh\u03c3\u03c3+++igf\no\u3f4b\u3f4b\u3f4bLSTMct-1\nFigure 8.13 A single LSTM unit displayed as a computation graph. The inputs to each unit consists of the\ncurrent input, x, the previous hidden state, ht\u00001, and the previous context, ct\u00001. The outputs are a new hidden\nstate, htand an updated context, ct.\nThe \ufb01nal gate we\u2019ll use is the output gate which is used to decide what informa- output gate\ntion is required for the current hidden state (as opposed to what information needs\nto be preserved for future decisions).\not=s(Uoht\u00001+Woxt) (8.26)\nht=ot\ftanh(ct) (8.27)\nFig. 8.13 illustrates the complete computation for a single LSTM unit. Given the\nappropriate weights for the various gates, an LSTM accepts as input the context\nlayer, and hidden layer from the previous time step, along with the current input\nvector. It then generates updated context and hidden vectors as output.\nIt is the hidden state, ht, that provides the output for the LSTM at each time step.\nThis output can be used as the input to subsequent layers in a stacked RNN, or at the\n\ufb01nal layer of a network htcan be used to provide the \ufb01nal output of the LSTM.\n8.5.1 Gated Units, Layers and Networks\nThe neural units used in LSTMs are obviously much more complex than those used\nin basic feedforward networks. Fortunately, this complexity is encapsulated within\nthe basic processing units, allowing us to maintain modularity and to easily exper-\niment with different architectures. To see this, consider Fig. 8.14 which illustrates\nthe inputs and outputs associated with each kind of unit.\nAt the far left, (a) is the basic feedforward unit where a single set of weights and\na single activation function determine its output, and when arranged in a layer there\nare no connections among the units in the layer. Next, (b) represents the unit in a\nsimple recurrent network. Now there are two inputs and an additional set of weights\nto go with it. However, there is still a single activation function and output.\nThe increased complexity of the LSTM units is encapsulated within the unit\nitself. The only additional external complexity for the LSTM over the basic recurrent\nunit (b) is the presence of the additional context vector as an input and output.\nThis modularity is key to the power and widespread applicability of LSTM units.\nLSTM units (or other varieties, like GRUs) can be substituted into any of the network\narchitectures described in Section 8.4. And, as with simple RNNs, multi-layered\nnetworks making use of gated units can be unrolled into deep feedforward networks",
    "metadata": {
      "source": "8",
      "chunk_id": 20,
      "token_count": 638,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\n8.6 \u2022 S UMMARY : COMMON RNN NLP A RCHITECTURES 17\nh\nxxtxtht-1htht\nct-1ct\nht-1(b)(a)(c)\u2303gza\u2303gzLSTMUnita\nFigure 8.14 Basic neural units used in feedforward, simple recurrent networks (SRN), and\nlong short-term memory (LSTM).\nand trained in the usual fashion with backpropagation. In practice, therefore, LSTMs\nrather than RNNs have become the standard unit for any modern system that makes\nuse of recurrent networks.\n8.6 Summary: Common RNN NLP Architectures\nWe\u2019ve now introduced the RNN, seen advanced components like stacking multiple\nlayers and using the LSTM version, and seen how the RNN can be applied to various\ntasks. Let\u2019s take a moment to summarize the architectures for these applications.\nFig. 8.15 shows the three architectures we\u2019ve discussed so far: sequence la-\nbeling, sequence classi\ufb01cation, and language modeling. In sequence labeling (for\nexample for part of speech tagging), we train a model to produce a label for each\ninput word or token. In sequence classi\ufb01cation, for example for sentiment analysis,\nwe ignore the output for each token, and only take the value from the end of the\nsequence (and similarly the model\u2019s training signal comes from backpropagation\nfrom that last token). In language modeling, we train the model to predict the next\nword at each token step. In the next section we\u2019ll introduce a fourth architecture, the\nencoder-decoder .\n8.7 The Encoder-Decoder Model with RNNs\nIn this section we introduce a new model, the encoder-decoder model, which is used\nwhen we are taking an input sequence and translating it to an output sequence that is\nof a different length than the input, and doesn\u2019t align with it in a word-to-word way.\nRecall that in the sequence labeling task, we have two sequences, but they are the\nsame length (for example in part-of-speech tagging each token gets an associated\ntag), each input is associated with a speci\ufb01c output, and the labeling for that output\ntakes mostly local information. Thus deciding whether a word is a verb or a noun,\nwe look mostly at the word and the neighboring words.\nBy contrast, encoder-decoder models are used especially for tasks like machine\ntranslation, where the input sequence and output sequence can have different lengths",
    "metadata": {
      "source": "8",
      "chunk_id": 21,
      "token_count": 538,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\n18 CHAPTER 8 \u2022 RNN S AND LSTM S\n\u2026Encoder RNNDecoder RNNContext\u2026\nx1x2xny1y2ym\u2026RNNx1x2xn\u2026y1y2yn\u2026RNNx1x2xny\n\u2026RNNx1x2xt-1\u2026x2x3xta) sequence labeling b) sequence classification \nc) language modelingd) encoder-decoder\nFigure 8.15 Four architectures for NLP tasks. In sequence labeling (POS or named entity tagging) we map\neach input token xito an output token yi. In sequence classi\ufb01cation we map the entire input sequence to a single\nclass. In language modeling we output the next token conditioned on previous tokens. In the encoder model we\nhave two separate RNN models, one of which maps from an input sequence xto an intermediate representation\nwe call the context , and a second of which maps from the context to an output sequence y.\nand the mapping between a token in the input and a token in the output can be very\nindirect (in some languages the verb appears at the beginning of the sentence; in\nother languages at the end). We\u2019ll introduce machine translation in detail in Chap-\nter 13, but for now we\u2019ll just point out that the mapping for a sentence in English to\na sentence in Tagalog or Yoruba can have very different numbers of words, and the\nwords can be in a very different order.\nEncoder-decoder networks, sometimes called sequence-to-sequence networks,encoder-\ndecoder\nare models capable of generating contextually appropriate, arbitrary length, output\nsequences given an input sequence. Encoder-decoder networks have been applied\nto a very wide range of applications including summarization, question answering,\nand dialogue, but they are particularly popular for machine translation.\nThe key idea underlying these networks is the use of an encoder network that\ntakes an input sequence and creates a contextualized representation of it, often called\nthecontext . This representation is then passed to a decoder which generates a task-\nspeci\ufb01c output sequence. Fig. 8.16 illustrates the architecture.\nEncoder-decoder networks consist of three conceptual components:\n1. An encoder that accepts an input sequence, x1:n, and generates a correspond-\ning sequence of contextualized representations, h1:n. LSTMs, convolutional\nnetworks, and transformers can all be employed as encoders.\n2. A context vector ,c, which is a function of h1:n, and conveys the essence of\nthe input to the decoder.\n3. A decoder , which accepts cas input and generates an arbitrary length se-\nquence of hidden states h1:m, from which a corresponding sequence of output\nstates y1:m, can be obtained. Just as with encoders, decoders can be realized",
    "metadata": {
      "source": "8",
      "chunk_id": 22,
      "token_count": 591,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\n8.7 \u2022 T HEENCODER -DECODER MODEL WITH RNN S19\n\u2026EncoderDecoderContext\u2026\nx1x2xny1y2ym\nFigure 8.16 The encoder-decoder architecture. The context is a function of the hidden\nrepresentations of the input, and may be used by the decoder in a variety of ways.\nby any kind of sequence architecture.\nIn this section we\u2019ll describe an encoder-decoder network based on a pair of\nRNNs, but we\u2019ll see in Chapter 13 how to apply them to transformers as well. We\u2019ll\nbuild up the equations for encoder-decoder models by starting with the conditional\nRNN language model p(y), the probability of a sequence y.\nRecall that in any language model, we can break down the probability as follows:\np(y) = p(y1)p(y2jy1)p(y3jy1;y2):::p(ymjy1;:::;ym\u00001) (8.28)\nIn RNN language modeling, at a particular time t, we pass the pre\ufb01x of t\u00001\ntokens through the language model, using forward inference to produce a sequence\nof hidden states, ending with the hidden state corresponding to the last word of\nthe pre\ufb01x. We then use the \ufb01nal hidden state of the pre\ufb01x as our starting point to\ngenerate the next token.\nMore formally, if gis an activation function like tanh or ReLU, a function of\nthe input at time tand the hidden state at time t\u00001, and the softmax is over the\nset of possible vocabulary items, then at time tthe output ytand hidden state htare\ncomputed as:\nht=g(ht\u00001;xt) (8.29)\n^ yt=softmax (ht) (8.30)\nWe only have to make one slight change to turn this language model with au-\ntoregressive generation into an encoder-decoder model that is a translation model\nthat can translate from a source text in one language to a target text in a second:\nadd a sentence separation marker at the end of the source text, and then simplysentence\nseparation\nconcatenate the target text.\nLet\u2019s use <s>for our sentence separator token, and let\u2019s think about translating\nan English source text (\u201cthe green witch arrived\u201d), to a Spanish sentence (\u201c lleg\u00b4o\nla bruja verde \u201d (which can be glossed word-by-word as \u2018arrived the witch green\u2019).\nWe could also illustrate encoder-decoder models with a question-answer pair, or a\ntext-summarization pair.\nLet\u2019s use xto refer to the source text (in this case in English) plus the separator\ntoken <s>, and yto refer to the target text y(in this case in Spanish). Then an\nencoder-decoder model computes the probability p(yjx)as follows:\np(yjx) = p(y1jx)p(y2jy1;x)p(y3jy1;y2;x):::p(ymjy1;:::;ym\u00001;x) (8.31)\nFig. 8.17 shows the setup for a simpli\ufb01ed version of the encoder-decoder model\n(we\u2019ll see the full model, which requires the new concept of attention , in the next\nsection).",
    "metadata": {
      "source": "8",
      "chunk_id": 23,
      "token_count": 704,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\n20 CHAPTER 8 \u2022 RNN S AND LSTM S\nSource TextTarget Text\nhnembeddinglayerhiddenlayer(s)softmaxthegreenlleg\u00f3\nwitcharrived<s>lleg\u00f3la\nlabruja\nbrujaverde\nverde</s>(output of source is ignored)\nSeparator\nFigure 8.17 Translating a single sentence (inference time) in the basic RNN version of encoder-decoder ap-\nproach to machine translation. Source and target sentences are concatenated with a separator token in between,\nand the decoder uses context information from the encoder\u2019s last hidden state.\nFig. 8.17 shows an English source text (\u201cthe green witch arrived\u201d), a sentence\nseparator token ( <s>, and a Spanish target text (\u201c lleg\u00b4o la bruja verde \u201d). To trans-\nlate a source text, we run it through the network performing forward inference to\ngenerate hidden states until we get to the end of the source. Then we begin autore-\ngressive generation, asking for a word in the context of the hidden layer from the\nend of the source input as well as the end-of-sentence marker. Subsequent words\nare conditioned on the previous hidden state and the embedding for the last word\ngenerated.\nLet\u2019s formalize and generalize this model a bit in Fig. 8.18. (To help keep things\nstraight, we\u2019ll use the superscripts eanddwhere needed to distinguish the hidden\nstates of the encoder and the decoder.) The elements of the network on the left\nprocess the input sequence xand comprise the encoder . While our simpli\ufb01ed \ufb01gure\nshows only a single network layer for the encoder, stacked architectures are the\nnorm, where the output states from the top layer of the stack are taken as the \ufb01nal\nrepresentation, and the encoder consists of stacked biLSTMs where the hidden states\nfrom top layers from the forward and backward passes are concatenated to provide\nthe contextualized representations for each time step.\nThe entire purpose of the encoder is to generate a contextualized representation\nof the input. This representation is embodied in the \ufb01nal hidden state of the encoder,\nhe\nn. This representation, also called cforcontext , is then passed to the decoder.\nThe simplest version of the decoder network would take this state and use it\njust to initialize the \ufb01rst hidden state of the decoder; the \ufb01rst decoder RNN cell\nwould use cas its prior hidden state hd\n0. The decoder would then autoregressively\ngenerates a sequence of outputs, an element at a time, until an end-of-sequence\nmarker is generated. Each hidden state is conditioned on the previous hidden state\nand the output generated in the previous state.\nAs Fig. 8.18 shows, we do something more complex: we make the context vector\ncavailable to more than just the \ufb01rst decoder hidden state, to ensure that the in\ufb02uence\nof the context vector, c, doesn\u2019t wane as the output sequence is generated. We do\nthis by adding cas a parameter to the computation of the current hidden state. using\nthe following equation:\nhd\nt=g(\u02c6yt\u00001;hd\nt\u00001;c) (8.32)",
    "metadata": {
      "source": "8",
      "chunk_id": 24,
      "token_count": 684,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21\n\n8.7 \u2022 T HEENCODER -DECODER MODEL WITH RNN S21\nEncoderDecoder\nhn hd1he3he2he1hd2hd3hd4embeddinglayerhiddenlayer(s)softmaxx1x2y1hdmx3xn<s>y1y2\ny2y3\ny3y4\nym</s>hen = c = hd0(output is ignored during encoding)\nFigure 8.18 A more formal version of translating a sentence at inference time in the basic RNN-based\nencoder-decoder architecture. The \ufb01nal hidden state of the encoder RNN, hen, serves as the context for the\ndecoder in its role as hd\n0in the decoder RNN, and is also made available to each decoder hidden state.\nNow we\u2019re ready to see the full equations for this version of the decoder in the basic\nencoder-decoder model, with context available at each decoding timestep. Recall\nthatgis a stand-in for some \ufb02avor of RNN and \u02c6 yt\u00001is the embedding for the output\nsampled from the softmax at the previous step:\nc=he\nn\nhd\n0=c\nhd\nt=g(\u02c6yt\u00001;hd\nt\u00001;c)\n^ yt=softmax (hd\nt) (8.33)\nThus ^ ytis a vector of probabilities over the vocabulary, representing the probability\nof each word occurring at time t. To generate text, we sample from this distribution\n^ yt. For example, the greedy choice is simply to choose the most probable word to\ngenerate at each timestep. We\u2019ll introduce more sophisticated sampling methods in\nSection ??.\n8.7.1 Training the Encoder-Decoder Model\nEncoder-decoder architectures are trained end-to-end. Each training example is a\ntuple of paired strings, a source and a target. Concatenated with a separator token,\nthese source-target pairs can now serve as training data.\nFor MT, the training data typically consists of sets of sentences and their transla-\ntions. These can be drawn from standard datasets of aligned sentence pairs, as we\u2019ll\ndiscuss in Section ??. Once we have a training set, the training itself proceeds as\nwith any RNN-based language model. The network is given the source text and then\nstarting with the separator token is trained autoregressively to predict the next word,\nas shown in Fig. 8.19.\nNote the differences between training (Fig. 8.19) and inference (Fig. 8.17) with\nrespect to the outputs at each time step. The decoder during inference uses its own\nestimated output \u02c6 ytas the input for the next time step xt+1. Thus the decoder will\ntend to deviate more and more from the gold target sentence as it keeps generating\nmore tokens. In training, therefore, it is more common to use teacher forcing in the teacher forcing\ndecoder. Teacher forcing means that we force the system to use the gold target token",
    "metadata": {
      "source": "8",
      "chunk_id": 25,
      "token_count": 630,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22\n\n22 CHAPTER 8 \u2022 RNN S AND LSTM S\nEncoderDecoder\nembeddinglayerhiddenlayer(s)softmaxthegreenlleg\u00f3\nwitcharrived<s>lleg\u00f3la\nlabruja\nbrujaverde\nverde</s>goldanswersL1 =-log P(y1)\nx1x2x3x4L2 =-log P(y2)L3 =-log P(y3)L4 =-log P(y4)L5 =-log P(y5)per-wordlossy1y2y3y4y5Total loss is the average cross-entropy loss per target word:\nFigure 8.19 Training the basic RNN encoder-decoder approach to machine translation. Note that in the\ndecoder we usually don\u2019t propagate the model\u2019s softmax outputs \u02c6 yt, but use teacher forcing to force each input\nto the correct gold value for training. We compute the softmax output distribution over \u02c6 yin the decoder in order\nto compute the loss at each token, which can then be averaged to compute a loss for the sentence. This loss is\nthen propagated through the decoder parameters and the encoder parameters.\nfrom training as the next input xt+1, rather than allowing it to rely on the (possibly\nerroneous) decoder output \u02c6 yt. This speeds up training.\n8.8 Attention\nThe simplicity of the encoder-decoder model is its clean separation of the encoder\u2014\nwhich builds a representation of the source text\u2014from the decoder, which uses this\ncontext to generate a target text. In the model as we\u2019ve described it so far, this\ncontext vector is hn, the hidden state of the last ( nth) time step of the source text.\nThis \ufb01nal hidden state is thus acting as a bottleneck : it must represent absolutely\neverything about the meaning of the source text, since the only thing the decoder\nknows about the source text is what\u2019s in this context vector (Fig. 8.20). Information\nat the beginning of the sentence, especially for long sentences, may not be equally\nwell represented in the context vector.\nEncoderDecoderbottleneckbottleneck\nFigure 8.20 Requiring the context cto be only the encoder\u2019s \ufb01nal hidden state forces all the\ninformation from the entire source sentence to pass through this representational bottleneck.\nThe attention mechanism is a solution to the bottleneck problem, a way ofattention\nmechanism\nallowing the decoder to get information from allthe hidden states of the encoder,\nnot just the last hidden state.",
    "metadata": {
      "source": "8",
      "chunk_id": 26,
      "token_count": 533,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23\n\n8.8 \u2022 A TTENTION 23\nIn the attention mechanism, as in the vanilla encoder-decoder model, the context\nvector cis a single vector that is a function of the hidden states of the encoder. But\ninstead of being taken from the last hidden state, it\u2019s a weighted average of allthe\nhidden states of the decoder. And this weighted average is also informed by part of\nthe decoder state as well, the state of the decoder right before the current token i.\nThat is, c=f(he\n1:::he\nn;hd\ni\u00001). The weights focus on (\u2018attend to\u2019) a particular part of\nthe source text that is relevant for the token ithat the decoder is currently producing.\nAttention thus replaces the static context vector with one that is dynamically derived\nfrom the encoder hidden states, but also informed by and hence different for each\ntoken in decoding.\nThis context vector, ci, is generated anew with each decoding step iand takes\nall of the encoder hidden states into account in its derivation. We then make this\ncontext available during decoding by conditioning the computation of the current\ndecoder hidden state on it (along with the prior hidden state and the previous output\ngenerated by the decoder), as we see in this equation (and Fig. 8.21):\nhd\ni=g(\u02c6yi\u00001;hd\ni\u00001;ci) (8.34)\nhd1hd2hdiy1y2yic1c2ci\u2026\u2026\nFigure 8.21 The attention mechanism allows each hidden state of the decoder to see a\ndifferent, dynamic, context, which is a function of all the encoder hidden states.\nThe \ufb01rst step in computing ciis to compute how much to focus on each encoder\nstate, how relevant each encoder state is to the decoder state captured in hd\ni\u00001. We\ncapture relevance by computing\u2014 at each state iduring decoding\u2014a score(hd\ni\u00001;he\nj)\nfor each encoder state j.\nThe simplest such score, called dot-product attention , implements relevance asdot-product\nattention\nsimilarity: measuring how similar the decoder hidden state is to an encoder hidden\nstate, by computing the dot product between them:\nscore(hd\ni\u00001;he\nj) = hd\ni\u00001\u0001he\nj (8.35)\nThe score that results from this dot product is a scalar that re\ufb02ects the degree of\nsimilarity between the two vectors. The vector of these scores across all the encoder\nhidden states gives us the relevance of each encoder state to the current step of the\ndecoder.\nTo make use of these scores, we\u2019ll normalize them with a softmax to create a\nvector of weights, ai j, that tells us the proportional relevance of each encoder hidden\nstate jto the prior hidden decoder state, hd\ni\u00001.\nai j=softmax (score(hd\ni\u00001;he\nj))\n=exp(score(hd\ni\u00001;he\nj)\nP\nkexp(score(hd\ni\u00001;he\nk))(8.36)\nFinally, given the distribution in a, we can compute a \ufb01xed-length context vector for\nthe current decoder state by taking a weighted average over all the encoder hidden",
    "metadata": {
      "source": "8",
      "chunk_id": 27,
      "token_count": 688,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24",
    "metadata": {
      "source": "8",
      "chunk_id": 28,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "24 CHAPTER 8 \u2022 RNN S AND LSTM S\nstates.\nci=X\njai jhe\nj (8.37)\nWith this, we \ufb01nally have a \ufb01xed-length context vector that takes into account\ninformation from the entire encoder state that is dynamically updated to re\ufb02ect the\nneeds of the decoder at each step of decoding. Fig. 8.22 illustrates an encoder-\ndecoder network with attention, focusing on the computation of one context vector\nci.\nEncoderDecoder\nhdi-1he3he2he1hdihiddenlayer(s)x1x2yi-1x3xnyi-2yi-1yihen ci.2.1.3.4attentionweightsci-1ci",
    "metadata": {
      "source": "8",
      "chunk_id": 29,
      "token_count": 156,
      "chapter_title": ""
    }
  },
  {
    "content": "<latexit sha1_base64=\"TNdNmv/RIlrhPa6LgQyjjQLqyBA=\">AAACAnicdVDLSsNAFJ3UV62vqCtxM1gEVyHpI9Vd0Y3LCvYBTQyT6bSddvJgZiKUUNz4K25cKOLWr3Dn3zhpK6jogQuHc+7l3nv8mFEhTfNDyy0tr6yu5dcLG5tb2zv67l5LRAnHpIkjFvGOjwRhNCRNSSUjnZgTFPiMtP3xRea3bwkXNAqv5SQmboAGIe1TjKSSPP3AEUngjVIHsXiIvJSOpnB4Q7zR1NOLpmGaVbtqQdOwLbtk24qY5Yp9VoOWsjIUwQINT393ehFOAhJKzJAQXcuMpZsiLilmZFpwEkFihMdoQLqKhiggwk1nL0zhsVJ6sB9xVaGEM/X7RIoCISaBrzoDJIfit5eJf3ndRPZP3ZSGcSJJiOeL+gmDMoJZHrBHOcGSTRRBmFN1K8RDxBGWKrWCCuHrU/g/aZUMyzbKV5Vi/XwRRx4cgiNwAixQA3VwCRqgCTC4Aw/gCTxr99qj9qK9zltz2mJmH/yA9vYJSymYCA==</latexit>Xj\u21b5ijhej\u21b5ij<latexit sha1_base64=\"y8s4mGdpwrGrBnuSR+p1gJJXYdo=\">AAAB/nicdVDJSgNBEO2JW4zbqHjy0hgEL4YeJyQBL0EvHiOYBbIMPT09mTY9C909QhgC/ooXD4p49Tu8+Td2FkFFHxQ83quiqp6bcCYVQh9Gbml5ZXUtv17Y2Nza3jF391oyTgWhTRLzWHRcLClnEW0qpjjtJILi0OW07Y4up377jgrJ4uhGjRPaD/EwYj4jWGnJMQ+Cgedk7NSa9IgXq955MKDOrWMWUQnNAFGpYtfsakUTZNtWGUFrYRXBAg3HfO95MUlDGinCsZRdCyWqn2GhGOF0UuilkiaYjPCQdjWNcEhlP5udP4HHWvGgHwtdkYIz9ftEhkMpx6GrO0OsAvnbm4p/ed1U+bV+xqIkVTQi80V+yqGK4TQL6DFBieJjTTARTN8KSYAFJkonVtAhfH0K/yets5JVKdnX5WL9YhFHHhyCI3ACLFAFdXAFGqAJCMjAA3gCz8a98Wi8GK/z1pyxmNkHP2C8fQICDpWK</latexit>hdi\u00001\u00b7hej\u2026\u2026\nFigure 8.22 A sketch of the encoder-decoder network with attention, focusing on the computation of ci. The",
    "metadata": {
      "source": "8",
      "chunk_id": 30,
      "token_count": 795,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 8.22 A sketch of the encoder-decoder network with attention, focusing on the computation of ci. The\ncontext value ciis one of the inputs to the computation of hd\ni. It is computed by taking the weighted sum of all\nthe encoder hidden states, each weighted by their dot product with the prior decoder hidden state hd\ni\u00001.\nIt\u2019s also possible to create more sophisticated scoring functions for attention\nmodels. Instead of simple dot product attention, we can get a more powerful function\nthat computes the relevance of each encoder hidden state to the decoder hidden state\nby parameterizing the score with its own set of weights, Ws.\nscore(hd\ni\u00001;he\nj) = hd\nt\u00001Wshe\nj\nThe weights Ws, which are then trained during normal end-to-end training, give the\nnetwork the ability to learn which aspects of similarity between the decoder and\nencoder states are important to the current application. This bilinear model also\nallows the encoder and decoder to use different dimensional vectors, whereas the\nsimple dot-product attention requires that the encoder and decoder hidden states\nhave the same dimensionality.\nWe\u2019ll return to the concept of attention when we de\ufb01ne the transformer archi-\ntecture in Chapter 9, which is based on a slight modi\ufb01cation of attention called\nself-attention .\n8.9 Summary\nThis chapter has introduced the concepts of recurrent neural networks and how they\ncan be applied to language problems. Here\u2019s a summary of the main points that we",
    "metadata": {
      "source": "8",
      "chunk_id": 31,
      "token_count": 320,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 25\n\nBIBLIOGRAPHICAL AND HISTORICAL NOTES 25\ncovered:\n\u2022 In simple Recurrent Neural Networks sequences are processed one element at\na time, with the output of each neural unit at time tbased both on the current\ninput at tand the hidden layer from time t\u00001.\n\u2022 RNNs can be trained with a straightforward extension of the backpropagation\nalgorithm, known as backpropagation through time (BPTT).\n\u2022 Simple recurrent networks fail on long inputs because of problems like van-\nishing gradients ; instead modern systems use more complex gated architec-\ntures such as LSTMs that explicitly decide what to remember and forget in\ntheir hidden and context layers.\n\u2022 Common language-based applications for RNNs include:\n\u2013Probabilistic language modeling: assigning a probability to a sequence,\nor to the next element of a sequence given the preceding words.\n\u2013Auto-regressive generation using a trained language model.\n\u2013Sequence labeling like part-of-speech tagging, where each element of a\nsequence is assigned a label.\n\u2013Sequence classi\ufb01cation, where an entire text is assigned to a category, as\nin spam detection, sentiment analysis or topic classi\ufb01cation.\n\u2013Encoder-decoder architectures, where an input is mapped to an output\nof different length and alignment.\nBibliographical and Historical Notes\nIn\ufb02uential investigations of RNNs were conducted in the context of the Parallel Dis-\ntributed Processing (PDP) group at UC San Diego in the 1980\u2019s. Much of this work\nwas directed at human cognitive modeling rather than practical NLP applications\n(Rumelhart and McClelland 1986, McClelland and Rumelhart 1986). Models using\nrecurrence at the hidden layer in a feedforward network (Elman networks) were in-\ntroduced by Elman (1990). Similar architectures were investigated by Jordan (1986)\nwith a recurrence from the output layer, and Mathis and Mozer (1995) with the\naddition of a recurrent context layer prior to the hidden layer. The possibility of\nunrolling a recurrent network into an equivalent feedforward network is discussed\nin (Rumelhart and McClelland, 1986).\nIn parallel with work in cognitive modeling, RNNs were investigated extensively\nin the continuous domain in the signal processing and speech communities (Giles\net al. 1994, Robinson et al. 1996). Schuster and Paliwal (1997) introduced bidirec-\ntional RNNs and described results on the TIMIT phoneme transcription task.\nWhile theoretically interesting, the dif\ufb01culty with training RNNs and manag-\ning context over long sequences impeded progress on practical applications. This\nsituation changed with the introduction of LSTMs in Hochreiter and Schmidhuber\n(1997) and Gers et al. (2000). Impressive performance gains were demonstrated\non tasks at the boundary of signal processing and language processing including\nphoneme recognition (Graves and Schmidhuber, 2005), handwriting recognition\n(Graves et al., 2007) and most signi\ufb01cantly speech recognition (Graves et al., 2013).\nInterest in applying neural networks to practical NLP problems surged with the\nwork of Collobert and Weston (2008) and Collobert et al. (2011). These efforts made\nuse of learned word embeddings, convolutional networks, and end-to-end training.",
    "metadata": {
      "source": "8",
      "chunk_id": 32,
      "token_count": 739,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 26\n\n26 CHAPTER 8 \u2022 RNN S AND LSTM S\nThey demonstrated near state-of-the-art performance on a number of standard shared\ntasks including part-of-speech tagging, chunking, named entity recognition and se-\nmantic role labeling without the use of hand-engineered features.\nApproaches that married LSTMs with pretrained collections of word-embeddings\nbased on word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014)\nquickly came to dominate many common tasks: part-of-speech tagging (Ling et al.,\n2015), syntactic chunking (S\u00f8gaard and Goldberg, 2016), named entity recognition\n(Chiu and Nichols, 2016; Ma and Hovy, 2016), opinion mining (Irsoy and Cardie,\n2014), semantic role labeling (Zhou and Xu, 2015) and AMR parsing (Foland and\nMartin, 2016). As with the earlier surge of progress involving statistical machine\nlearning, these advances were made possible by the availability of training data pro-\nvided by CONLL, SemEval, and other shared tasks, as well as shared resources such\nas Ontonotes (Pradhan et al., 2007), and PropBank (Palmer et al., 2005).\nThe modern neural encoder-decoder approach was pioneered by Kalchbrenner\nand Blunsom (2013), who used a CNN encoder and an RNN decoder. Cho et al.\n(2014) (who coined the name \u201cencoder-decoder\u201d) and Sutskever et al. (2014) then\nshowed how to use extended RNNs for both encoder and decoder. The idea that a\ngenerative decoder should take as input a soft weighting of the inputs, the central\nidea of attention, was \ufb01rst developed by Graves (2013) in the context of handwriting\nrecognition. Bahdanau et al. (2015) extended the idea, named it \u201cattention\u201d and\napplied it to MT.",
    "metadata": {
      "source": "8",
      "chunk_id": 33,
      "token_count": 440,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 27",
    "metadata": {
      "source": "8",
      "chunk_id": 34,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Bibliographical and Historical Notes 27\nBahdanau, D., K. H. Cho, and Y . Bengio. 2015. Neural ma-\nchine translation by jointly learning to align and translate.\nICLR 2015 .\nChiu, J. P. C. and E. Nichols. 2016. Named entity recognition\nwith bidirectional LSTM-CNNs. TACL , 4:357\u2013370.\nCho, K., B. van Merri \u00a8enboer, C. Gulcehre, D. Bahdanau,\nF. Bougares, H. Schwenk, and Y . Bengio. 2014. Learn-\ning phrase representations using RNN encoder\u2013decoder\nfor statistical machine translation. EMNLP .\nCollobert, R. and J. Weston. 2008. A uni\ufb01ed architecture for\nnatural language processing: Deep neural networks with\nmultitask learning. ICML .\nCollobert, R., J. Weston, L. Bottou, M. Karlen,\nK. Kavukcuoglu, and P. Kuksa. 2011. Natural language\nprocessing (almost) from scratch. JMLR , 12:2493\u20132537.\nElman, J. L. 1990. Finding structure in time. Cognitive sci-\nence, 14(2):179\u2013211.\nFoland, W. and J. H. Martin. 2016. CU-NLP at SemEval-\n2016 task 8: AMR parsing using LSTM-based recurrent\nneural networks. SemEval-2016 .\nGers, F. A., J. Schmidhuber, and F. Cummins. 2000. Learn-\ning to forget: Continual prediction with lstm. Neural\ncomputation , 12(10):2451\u20132471.\nGiles, C. L., G. M. Kuhn, and R. J. Williams. 1994. Dynamic\nrecurrent neural networks: Theory and applications.\nIEEE Trans. Neural Netw. Learning Syst. , 5(2):153\u2013156.\nGraves, A. 2013. Generating sequences with recurrent neural\nnetworks. ArXiv.\nGraves, A., S. Fern \u00b4andez, M. Liwicki, H. Bunke, and\nJ. Schmidhuber. 2007. Unconstrained on-line handwrit-\ning recognition with recurrent neural networks. NeurIPS .\nGraves, A., A.-r. Mohamed, and G. Hinton. 2013.\nSpeech recognition with deep recurrent neural networks.\nICASSP .\nGraves, A. and J. Schmidhuber. 2005. Framewise phoneme\nclassi\ufb01cation with bidirectional LSTM and other neural\nnetwork architectures. Neural Networks , 18(5-6):602\u2013\n610.\nHochreiter, S. and J. Schmidhuber. 1997. Long short-term\nmemory. Neural Computation , 9(8):1735\u20131780.\nIrsoy, O. and C. Cardie. 2014. Opinion mining with deep\nrecurrent neural networks. EMNLP .\nJordan, M. 1986. Serial order: A parallel distributed process-\ning approach. Technical Report ICS Report 8604, Univer-\nsity of California, San Diego.\nKalchbrenner, N. and P. Blunsom. 2013. Recurrent continu-\nous translation models. EMNLP .",
    "metadata": {
      "source": "8",
      "chunk_id": 35,
      "token_count": 754,
      "chapter_title": ""
    }
  },
  {
    "content": "Graves, A., S. Fern \u00b4andez, M. Liwicki, H. Bunke, and\nJ. Schmidhuber. 2007. Unconstrained on-line handwrit-\ning recognition with recurrent neural networks. NeurIPS .\nGraves, A., A.-r. Mohamed, and G. Hinton. 2013.\nSpeech recognition with deep recurrent neural networks.\nICASSP .\nGraves, A. and J. Schmidhuber. 2005. Framewise phoneme\nclassi\ufb01cation with bidirectional LSTM and other neural\nnetwork architectures. Neural Networks , 18(5-6):602\u2013\n610.\nHochreiter, S. and J. Schmidhuber. 1997. Long short-term\nmemory. Neural Computation , 9(8):1735\u20131780.\nIrsoy, O. and C. Cardie. 2014. Opinion mining with deep\nrecurrent neural networks. EMNLP .\nJordan, M. 1986. Serial order: A parallel distributed process-\ning approach. Technical Report ICS Report 8604, Univer-\nsity of California, San Diego.\nKalchbrenner, N. and P. Blunsom. 2013. Recurrent continu-\nous translation models. EMNLP .\nLing, W., C. Dyer, A. W. Black, I. Trancoso, R. Fermandez,\nS. Amir, L. Marujo, and T. Lu \u00b4\u0131s. 2015. Finding function\nin form: Compositional character models for open vocab-\nulary word representation. EMNLP .\nMa, X. and E. H. Hovy. 2016. End-to-end sequence labeling\nvia bi-directional LSTM-CNNs-CRF. ACL.\nMathis, D. A. and M. C. Mozer. 1995. On the computational\nutility of consciousness. NeurIPS . MIT Press.\nMcClelland, J. L. and D. E. Rumelhart, eds. 1986. Parallel\nDistributed Processing: Explorations in the Microstruc-\nture of Cognition , volume 2: Psychological and Biologi-\ncal Models . MIT Press.Mikolov, T., K. Chen, G. S. Corrado, and J. Dean. 2013. Ef-\n\ufb01cient estimation of word representations in vector space.\nICLR 2013 .\nMikolov, T., M. Kara\ufb01 \u00b4at, L. Burget, J. \u02c7Cernock `y, and\nS. Khudanpur. 2010. Recurrent neural network based lan-\nguage model. INTERSPEECH .\nMiller, G. A. and J. A. Selfridge. 1950. Verbal context and\nthe recall of meaningful material. American Journal of\nPsychology , 63:176\u2013185.\nPalmer, M., P. Kingsbury, and D. Gildea. 2005. The proposi-\ntion bank: An annotated corpus of semantic roles. Com-\nputational Linguistics , 31(1):71\u2013106.\nPennington, J., R. Socher, and C. D. Manning. 2014. GloVe:\nGlobal vectors for word representation. EMNLP .\nPradhan, S., E. H. Hovy, M. P. Marcus, M. Palmer, L. A.\nRamshaw, and R. M. Weischedel. 2007. Ontonotes: a",
    "metadata": {
      "source": "8",
      "chunk_id": 36,
      "token_count": 765,
      "chapter_title": ""
    }
  },
  {
    "content": "\ufb01cient estimation of word representations in vector space.\nICLR 2013 .\nMikolov, T., M. Kara\ufb01 \u00b4at, L. Burget, J. \u02c7Cernock `y, and\nS. Khudanpur. 2010. Recurrent neural network based lan-\nguage model. INTERSPEECH .\nMiller, G. A. and J. A. Selfridge. 1950. Verbal context and\nthe recall of meaningful material. American Journal of\nPsychology , 63:176\u2013185.\nPalmer, M., P. Kingsbury, and D. Gildea. 2005. The proposi-\ntion bank: An annotated corpus of semantic roles. Com-\nputational Linguistics , 31(1):71\u2013106.\nPennington, J., R. Socher, and C. D. Manning. 2014. GloVe:\nGlobal vectors for word representation. EMNLP .\nPradhan, S., E. H. Hovy, M. P. Marcus, M. Palmer, L. A.\nRamshaw, and R. M. Weischedel. 2007. Ontonotes: a\nuni\ufb01ed relational semantic representation. Int. J. Seman-\ntic Computing , 1(4):405\u2013419.\nRobinson, T., M. Hochberg, and S. Renals. 1996. The use\nof recurrent neural networks in continuous speech recog-\nnition. In C.-H. Lee, F. K. Soong, and K. K. Paliwal,\neds,Automatic speech and speaker recognition , 233\u2013258.\nSpringer.\nRumelhart, D. E., G. E. Hinton, and R. J. Williams. 1986.\nLearning internal representations by error propagation. In\nD. E. Rumelhart and J. L. McClelland, eds, Parallel Dis-\ntributed Processing , volume 2, 318\u2013362. MIT Press.\nRumelhart, D. E. and J. L. McClelland, eds. 1986. Parallel\nDistributed Processing: Explorations in the Microstruc-\nture of Cognition , volume 1: Foundations . MIT Press.\nSchuster, M. and K. K. Paliwal. 1997. Bidirectional recurrent\nneural networks. IEEE Transactions on Signal Process-\ning, 45:2673\u20132681.\nShannon, C. E. 1951. Prediction and entropy of printed En-\nglish. Bell System Technical Journal , 30:50\u201364.\nS\u00f8gaard, A. and Y . Goldberg. 2016. Deep multi-task learning\nwith low level tasks supervised at lower layers. ACL.\nSutskever, I., O. Vinyals, and Q. V . Le. 2014. Sequence to\nsequence learning with neural networks. NeurIPS .\nWerbos, P. 1974. Beyond regression: new tools for predic-\ntion and analysis in the behavioral sciences . Ph.D. thesis,\nHarvard University.\nWerbos, P. J. 1990. Backpropagation through time: what\nit does and how to do it. Proceedings of the IEEE ,\n78(10):1550\u20131560.\nZhou, J. and W. Xu. 2015. End-to-end learning of semantic\nrole labeling using recurrent neural networks. ACL.",
    "metadata": {
      "source": "8",
      "chunk_id": 37,
      "token_count": 737,
      "chapter_title": ""
    }
  }
]