[
  {
    "content": "# H\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\nHPhonetics\nThe characters that make up the texts we\u2019ve been discussing in this book aren\u2019t just\nrandom symbols. They are also an amazing scienti\ufb01c invention: a theoretical model\nof the elements that make up human speech.\nThe earliest writing systems we know of (Sumerian, Chinese, Mayan) were\nmainly logographic : one symbol representing a whole word. But from the ear-\nliest stages we can \ufb01nd, some symbols were also used to represent the sounds\nthat made up words. The cuneiform sign to the right pro-\nnounced baand meaning \u201cration\u201d in Sumerian could also\nfunction purely as the sound /ba/. The earliest Chinese char-\nacters we have, carved into bones for divination, similarly\ncontain phonetic elements. Purely sound-based writing systems, whether syllabic\n(like Japanese hiragana ), alphabetic (like the Roman alphabet), or consonantal (like\nSemitic writing systems), trace back to these early logo-syllabic systems, often as\ntwo cultures came together. Thus, the Arabic, Aramaic, Hebrew, Greek, and Roman\nsystems all derive from a West Semitic script that is presumed to have been modi\ufb01ed\nby Western Semitic mercenaries from a cursive form of Egyptian hieroglyphs. The\nJapanese syllabaries were modi\ufb01ed from a cursive form of Chinese phonetic charac-\nters, which themselves were used in Chinese to phonetically represent the Sanskrit\nin the Buddhist scriptures that came to China in the Tang dynasty.\nThis implicit idea that the spoken word is composed of smaller units of speech\nunderlies algorithms for both speech recognition (transcribing waveforms into text)\nandtext-to-speech (converting text into waveforms). In this chapter we give a com-\nputational perspective on phonetics , the study of the speech sounds used in the phonetics\nlanguages of the world, how they are produced in the human vocal tract, how they\nare realized acoustically, and how they can be digitized and processed.\nH.1 Speech Sounds and Phonetic Transcription\nA letter like \u2018p\u2019 or \u2018a\u2019 is already a useful model of the sounds of human speech,\nand indeed we\u2019ll see in Chapter 16 how to map between letters and waveforms.\nNonetheless, it is helpful to represent sounds slightly more abstractly. We\u2019ll repre-\nsent the pronunciation of a word as a string of phones , which are speech sounds, phone\neach represented with symbols adapted from the Roman alphabet.\nThe standard phonetic representation for transcribing the world\u2019s languages is\ntheInternational Phonetic Alphabet (IPA), an evolving standard \ufb01rst developed in IPA\n1888, But in this chapter we\u2019ll instead represent phones with the ARPAbet (Shoup,\n1980), a simple phonetic alphabet (Fig. H.1) that conveniently uses ASCII symbols\nto represent an American-English subset of the IPA.\nMany of the IPA and ARPAbet symbols are equivalent to familiar Roman let-\nters. So, for example, the ARPAbet phone [p]represents the consonant sound at the",
    "metadata": {
      "source": "H",
      "chunk_id": 0,
      "token_count": 701,
      "chapter_title": "H"
    }
  },
  {
    "content": "## Page 2",
    "metadata": {
      "source": "H",
      "chunk_id": 1,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "2APPENDIX H \u2022 P HONETICS\nARPAbet IPA ARPAbet ARPAbet IPA ARPAbet\nSymbol Symbol Word Transcription Symbol Symbol Word Transcription\n[p] [p] p arsley [p aa r s l iy] [iy] [i] lily [l ih l iy]\n[t] [t] t ea [t iy] [ih] [I] lily [l ih l iy]\n[k] [k] c ook [k uh k] [ey] [eI] daisy [d ey z iy]\n[b] [b] b ay [b ey] [eh] [E] pen [p eh n]\n[d] [d] d ill [d ih l] [ae] [\u00e6] aster [ae s t axr]\n[g] [g] g arlic [g aa r l ix k] [aa] [A] poppy [p aa p iy]\n[m] [m] m int [m ih n t] [ao] [O] orchid [ao r k ix d]\n[n] [n] n utmeg [n ah t m eh g] [uh] [U] wood [w uh d]\n[ng] [ N] baking [b ey k ix ng] [ow] [oU] lotus [l ow dx ax s]\n[f] [f] f lour [f l aw axr] [uw] [u] tulip [t uw l ix p]\n[v] [v] clov e [k l ow v] [ah] [2] butter [b ah dx axr]\n[th] [ T] th ick [th ih k] [er] [\u00c7] bird [b er d]\n[dh] [ D] th ose [dh ow z] [ay] [aI] iris [ay r ix s]\n[s] [s] s oup [s uw p] [aw] [aU] \ufb02ower [f l aw axr]\n[z] [z] eggs [eh g z] [oy] [oI] soil [s oy l]\n[sh] [ S] squash [s k w aa sh] [ax] [@] pita [p iy t ax]\n[zh] [ Z] ambros ia [ae m b r ow zh ax]\n[ch] [t S] ch erry [ch eh r iy]\n[jh] [d Z] j ar [jh aa r]\n[l] [l] l icorice [l ih k axr ix sh]\n[w] [w] kiw i [k iy w iy]\n[r] [r] r ice [r ay s]\n[y] [j] y ellow [y eh l ow]\n[h] [h] h oney [h ah n iy]\nFigure H.1 ARPAbet and IPA symbols for English consonants (left) and vowels (right).\nbeginning of platypus ,puma , and plantain , the middle of leopard , or the end of an-\ntelope . In general, however, the mapping between the letters of English orthography\nand phones is relatively opaque ; a single letter can represent very different sounds\nin different contexts. The English letter ccorresponds to phone [k] in cougar [k uw\ng axr], but phone [s] in cell[s eh l]. Besides appearing as candk, the phone [k] can\nappear as part of x(fox[f aa k s]), as ck(jackal [jh ae k el]) and as cc(raccoon [r ae",
    "metadata": {
      "source": "H",
      "chunk_id": 2,
      "token_count": 761,
      "chapter_title": ""
    }
  },
  {
    "content": "[zh] [ Z] ambros ia [ae m b r ow zh ax]\n[ch] [t S] ch erry [ch eh r iy]\n[jh] [d Z] j ar [jh aa r]\n[l] [l] l icorice [l ih k axr ix sh]\n[w] [w] kiw i [k iy w iy]\n[r] [r] r ice [r ay s]\n[y] [j] y ellow [y eh l ow]\n[h] [h] h oney [h ah n iy]\nFigure H.1 ARPAbet and IPA symbols for English consonants (left) and vowels (right).\nbeginning of platypus ,puma , and plantain , the middle of leopard , or the end of an-\ntelope . In general, however, the mapping between the letters of English orthography\nand phones is relatively opaque ; a single letter can represent very different sounds\nin different contexts. The English letter ccorresponds to phone [k] in cougar [k uw\ng axr], but phone [s] in cell[s eh l]. Besides appearing as candk, the phone [k] can\nappear as part of x(fox[f aa k s]), as ck(jackal [jh ae k el]) and as cc(raccoon [r ae\nk uw n]). Many other languages, for example, Spanish, are much more transparent\nin their sound-orthography mapping than English.\nH.2 Articulatory Phonetics\nArticulatory phonetics is the study of how these phones are produced as the variousarticulatory\nphonetics\norgans in the mouth, throat, and nose modify the air\ufb02ow from the lungs.\nThe Vocal Organs\nFigure H.2 shows the organs of speech. Sound is produced by the rapid movement\nof air. Humans produce most sounds in spoken languages by expelling air from the\nlungs through the windpipe (technically, the trachea ) and then out the mouth or\nnose. As it passes through the trachea, the air passes through the larynx , commonly\nknown as the Adam\u2019s apple or voice box. The larynx contains two small folds of",
    "metadata": {
      "source": "H",
      "chunk_id": 3,
      "token_count": 467,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\nH.2 \u2022 A RTICULATORY PHONETICS 3\nFigure H.2 The vocal organs, shown in side view. (Figure from OpenStax University\nPhysics, CC BY 4.0)\nmuscle, the vocal folds (often referred to non-technically as the vocal cords ), which\ncan be moved together or apart. The space between these two folds is called the\nglottis . If the folds are close together (but not tightly closed), they will vibrate as air glottis\npasses through them; if they are far apart, they won\u2019t vibrate. Sounds made with the\nvocal folds together and vibrating are called voiced ; sounds made without this vocal voiced sound\ncord vibration are called unvoiced orvoiceless . V oiced sounds include [b], [d], [g], unvoiced sound\n[v], [z], and all the English vowels, among others. Unvoiced sounds include [p], [t],\n[k], [f], [s], and others.\nThe area above the trachea is called the vocal tract ; it consists of the oral tract\nand the nasal tract . After the air leaves the trachea, it can exit the body through the\nmouth or the nose. Most sounds are made by air passing through the mouth. Sounds\nmade by air passing through the nose are called nasal sounds ; nasal sounds (like nasal\nEnglish [m], [n], and [ng]) use both the oral and nasal tracts as resonating cavities.\nPhones are divided into two main classes: consonants andvowels . Both kinds consonant\nvowel of sounds are formed by the motion of air through the mouth, throat or nose. Con-\nsonants are made by restriction or blocking of the air\ufb02ow in some way, and can be\nvoiced or unvoiced. V owels have less obstruction, are usually voiced, and are gen-\nerally louder and longer-lasting than consonants. The technical use of these terms is\nmuch like the common usage; [p], [b], [t], [d], [k], [g], [f], [v], [s], [z], [r], [l], etc.,\nare consonants; [aa], [ae], [ao], [ih], [aw], [ow], [uw], etc., are vowels. Semivow-\nels(such as [y] and [w]) have some of the properties of both; they are voiced like\nvowels, but they are short and less syllabic like consonants.",
    "metadata": {
      "source": "H",
      "chunk_id": 4,
      "token_count": 545,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4APPENDIX H \u2022 P HONETICS\nConsonants: Place of Articulation\nBecause consonants are made by restricting air\ufb02ow, we can group them into classes\nby their point of maximum restriction, their place of articulation (Fig. H.3).place of\narticulation\n(nasal tract)dentalbilabialglottalpalatalvelaralveolar\nFigure H.3 Major English places of articulation.\nLabial: Consonants whose main restriction is formed by the two lips coming to- labial\ngether have a bilabial place of articulation. In English these include [p] as\ninpossum , [b] as in bear, and [m] as in marmot . The English labiodental\nconsonants [v] and [f] are made by pressing the bottom lip against the upper\nrow of teeth and letting the air \ufb02ow through the space in the upper teeth.\nDental: Sounds that are made by placing the tongue against the teeth are dentals. dental\nThe main dentals in English are the [th] of thingand the [dh] of though , which\nare made by placing the tongue behind the teeth with the tip slightly between\nthe teeth.\nAlveolar: The alveolar ridge is the portion of the roof of the mouth just behind the alveolar\nupper teeth. Most speakers of American English make the phones [s], [z], [t],\nand [d] by placing the tip of the tongue against the alveolar ridge. The word\ncoronal is often used to refer to both dental and alveolar.\nPalatal: The roof of the mouth (the palate ) rises sharply from the back of the palatal\npalate alveolar ridge. The palato-alveolar sounds [sh] ( shrimp ), [ch] ( china), [zh]\n(Asian ), and [jh] ( jar) are made with the blade of the tongue against the rising\nback of the alveolar ridge. The palatal sound [y] of yakis made by placing the\nfront of the tongue up close to the palate.\nVelar: Thevelum , or soft palate, is a movable muscular \ufb02ap at the very back of the velar\nroof of the mouth. The sounds [k] ( cuckoo), [g] ( goose), and [N](king\ufb01sher )\nare made by pressing the back of the tongue up against the velum.\nGlottal: The glottal stop [q] is made by closing the glottis (by bringing the vocal glottal\nfolds together).\nConsonants: Manner of Articulation\nConsonants are also distinguished by how the restriction in air\ufb02ow is made, for ex-\nample, by a complete stoppage of air or by a partial blockage. This feature is called\nthemanner of articulation of a consonant. The combination of place and mannermanner of\narticulation\nof articulation is usually suf\ufb01cient to uniquely identify a consonant. Following are\nthe major manners of articulation for English consonants:\nAstop is a consonant in which air\ufb02ow is completely blocked for a short time. stop\nThis blockage is followed by an explosive sound as the air is released. The period\nof blockage is called the closure , and the explosion is called the release . English",
    "metadata": {
      "source": "H",
      "chunk_id": 5,
      "token_count": 729,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5",
    "metadata": {
      "source": "H",
      "chunk_id": 6,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "H.2 \u2022 A RTICULATORY PHONETICS 5\nhas voiced stops like [b], [d], and [g] as well as unvoiced stops like [p], [t], and [k].\nStops are also called plosives .\nThenasal sounds [n], [m], and [ng] are made by lowering the velum and allow- nasal\ning air to pass into the nasal cavity.\nInfricatives , air\ufb02ow is constricted but not cut off completely. The turbulent fricatives\nair\ufb02ow that results from the constriction produces a characteristic \u201chissing\u201d sound.\nThe English labiodental fricatives [f] and [v] are produced by pressing the lower\nlip against the upper teeth, allowing a restricted air\ufb02ow between the upper teeth.\nThe dental fricatives [th] and [dh] allow air to \ufb02ow around the tongue between the\nteeth. The alveolar fricatives [s] and [z] are produced with the tongue against the\nalveolar ridge, forcing air over the edge of the teeth. In the palato-alveolar fricatives\n[sh] and [zh], the tongue is at the back of the alveolar ridge, forcing air through a\ngroove formed in the tongue. The higher-pitched fricatives (in English [s], [z], [sh]\nand [zh]) are called sibilants . Stops that are followed immediately by fricatives are sibilants\ncalled affricates ; these include English [ch] ( chicken ) and [jh] ( giraffe ).\nInapproximants , the two articulators are close together but not close enough to approximant\ncause turbulent air\ufb02ow. In English [y] ( yellow ), the tongue moves close to the roof\nof the mouth but not close enough to cause the turbulence that would characterize a\nfricative. In English [w] ( wood), the back of the tongue comes close to the velum.\nAmerican [r] can be formed in at least two ways; with just the tip of the tongue\nextended and close to the palate or with the whole tongue bunched up near the palate.\n[l] is formed with the tip of the tongue up against the alveolar ridge or the teeth, with\none or both sides of the tongue lowered to allow air to \ufb02ow over it. [l] is called a\nlateral sound because of the drop in the sides of the tongue.\nAtapor\ufb02ap[dx] is a quick motion of the tongue against the alveolar ridge. The tap\nconsonant in the middle of the word lotus ([l ow dx ax s]) is a tap in most dialects of\nAmerican English; speakers of many U.K. dialects would use a [t] instead.\nVowels\nLike consonants, vowels can be characterized by the position of the articulators as\nthey are made. The three most relevant parameters for vowels are what is called\nvowel height , which correlates roughly with the height of the highest part of the\ntongue, vowel frontness orbackness , indicating whether this high point is toward\nthe front or back of the oral tract and whether the shape of the lips is rounded or\nnot. Figure H.4 shows the position of the tongue for different vowels.\nboot [uw]closedvelumbat [ae]palatebeet [iy]tongue\nFigure H.4 Tongue positions for English high front [iy], low front [ae]and high back [uw].\nIn the vowel [iy], for example, the highest point of the tongue is toward the",
    "metadata": {
      "source": "H",
      "chunk_id": 7,
      "token_count": 782,
      "chapter_title": ""
    }
  },
  {
    "content": "one or both sides of the tongue lowered to allow air to \ufb02ow over it. [l] is called a\nlateral sound because of the drop in the sides of the tongue.\nAtapor\ufb02ap[dx] is a quick motion of the tongue against the alveolar ridge. The tap\nconsonant in the middle of the word lotus ([l ow dx ax s]) is a tap in most dialects of\nAmerican English; speakers of many U.K. dialects would use a [t] instead.\nVowels\nLike consonants, vowels can be characterized by the position of the articulators as\nthey are made. The three most relevant parameters for vowels are what is called\nvowel height , which correlates roughly with the height of the highest part of the\ntongue, vowel frontness orbackness , indicating whether this high point is toward\nthe front or back of the oral tract and whether the shape of the lips is rounded or\nnot. Figure H.4 shows the position of the tongue for different vowels.\nboot [uw]closedvelumbat [ae]palatebeet [iy]tongue\nFigure H.4 Tongue positions for English high front [iy], low front [ae]and high back [uw].\nIn the vowel [iy], for example, the highest point of the tongue is toward the\nfront of the mouth. In the vowel [uw], by contrast, the high-point of the tongue is\nlocated toward the back of the mouth. V owels in which the tongue is raised toward\nthe front are called front vowels ; those in which the tongue is raised toward the Front vowel",
    "metadata": {
      "source": "H",
      "chunk_id": 8,
      "token_count": 345,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6APPENDIX H \u2022 P HONETICS\nback are called back vowels . Note that while both [ih] and [eh] are front vowels, back vowel\nthe tongue is higher for [ih] than for [eh]. V owels in which the highest point of the\ntongue is comparatively high are called high vowels ; vowels with mid or low values high vowel\nof maximum tongue height are called mid vowels orlow vowels , respectively.\nfront back\nlowhigh\niy\nih\neh\naeuw\nuh\nax\nahao\naay uw\neyowoy\nayaw\nFigure H.5 The schematic \u201cvowel space\u201d for English vowels.\nFigure H.5 shows a schematic characterization of the height of different vowels.\nIt is schematic because the abstract property height correlates only roughly with ac-\ntual tongue positions; it is, in fact, a more accurate re\ufb02ection of acoustic facts. Note\nthat the chart has two kinds of vowels: those in which tongue height is represented\nas a point and those in which it is represented as a path. A vowel in which the tongue\nposition changes markedly during the production of the vowel is a diphthong . En- diphthong\nglish is particularly rich in diphthongs.\nThe second important articulatory dimension for vowels is the shape of the lips.\nCertain vowels are pronounced with the lips rounded (the same lip shape used for\nwhistling). These rounded vowels include [uw], [ao], and [ow]. rounded vowel\nSyllables\nConsonants and vowels combine to make a syllable . A syllable is a vowel-like (or syllable\nsonorant ) sound together with some of the surrounding consonants that are most\nclosely associated with it. The word doghas one syllable, [d aa g] (in our dialect);\nthe word catnip has two syllables, [k ae t] and [n ih p]. We call the vowel at the\ncore of a syllable the nucleus . Initial consonants, if any, are called the onset . Onsets nucleus\nonset with more than one consonant (as in strike [s t r ay k]), are called complex onsets .\nThecoda is the optional consonant or sequence of consonants following the nucleus. coda\nThus [d] is the onset of dog, and [g] is the coda. The rime , orrhyme , is the nucleus rime\nplus coda. Figure H.6 shows some sample syllable structures.\nThe task of automatically breaking up a word into syllables is called syllabi\ufb01ca-\ntion. Syllable structure is also closely related to the phonotactics of a language. The syllabi\ufb01cation\nterm phonotactics means the constraints on which phones can follow each other in phonotactics\na language. For example, English has strong constraints on what kinds of conso-\nnants can appear together in an onset; the sequence [zdr], for example, cannot be a\nlegal English syllable onset. Phonotactics can be represented by a language model\nor \ufb01nite-state model of phone sequences.",
    "metadata": {
      "source": "H",
      "chunk_id": 9,
      "token_count": 668,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\nH.3 \u2022 P ROSODY 7\ns\nRime\nCoda\nmNucleus\naeOnset\nhs\nRime\nCoda\nnNucleus\niyOnset\nrgs\nRime\nCoda\nzgNucleus\neh\nFigure H.6 Syllable structure of ham,green ,eggs.s=syllable.\nH.3 Prosody\nProsody is the study of the intonational and rhythmic aspects of language, and in prosody\nparticular the use of F0,energy , and duration to convey pragmatic, affective, or\nconversation-interactional meanings.1We\u2019ll introduce these acoustic quantities in\ndetail in the next section when we turn to acoustic phonetics, but brie\ufb02y we can\nthink of energy as the acoustic quality that we perceive as loudness, and F0 as the\nfrequency of the sound that is produced, the acoustic quality that we hear as the\npitch of an utterance. Prosody can be used to mark discourse structure , like the\ndifference between statements and questions, or the way that a conversation is struc-\ntured. Prosody is used to mark the saliency of a particular word or phrase. Prosody\nis heavily used for paralinguistic functions like conveying affective meanings like\nhappiness, surprise, or anger. And prosody plays an important role in managing\nturn-taking in conversation.\nH.3.1 Prosodic Prominence: Accent, Stress and Schwa\nIn a natural utterance of American English, some words sound more prominent than prominence\nothers, and certain syllables in these words are also more prominent than others.\nWhat we mean by prominence is that these words or syllables are perceptually more\nsalient to the listener. Speakers make a word or syllable more salient in English\nby saying it louder, saying it slower (so it has a longer duration), or by varying F0\nduring the word, making it higher or more variable.\nAccent We represent prominence via a linguistic marker called pitch accent . Words pitch accent\nor syllables that are prominent are said to bear (be associated with) a pitch accent.\nThus this utterance might be pronounced by accenting the underlined words:\n(H.1) I\u2019m a little surprised to hear it characterized as happy .\nLexical Stress The syllables that bear pitch accent are called accented syllables.\nNot every syllable of a word can be accented: pitch accent has to be realized on the\nsyllable that has lexical stress . Lexical stress is a property of the word\u2019s pronuncia- lexical stress\ntion in dictionaries; the syllable that has lexical stress is the one that will be louder\nor longer if the word is accented. For example, the word surprised is stressed on its\nsecond syllable, not its \ufb01rst. (Try stressing the other syllable by saying SURprised;\nhopefully that sounds wrong to you). Thus, if the word surprised receives a pitch\naccent in a sentence, it is the second syllable that will be stronger. The following ex-\n1The word is used in a different but related way in poetry, to mean the study of verse metrical structure.",
    "metadata": {
      "source": "H",
      "chunk_id": 10,
      "token_count": 678,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8APPENDIX H \u2022 P HONETICS\nample shows underlined accented words with the stressed syllable bearing the accent\n(the louder, longer syllable) in boldface:\n(H.2) I\u2019m a little sur prised to hear it char acterized ashappy.\nStress is marked in dictionaries. The CMU dictionary (CMU, 1993), for ex-\nample, marks vowels with 0 (unstressed) or 1 (stressed) as in entries for counter :\n[K AW1 N T ER0], or table : [T EY1 B AH0 L]. Difference in lexical stress can\naffect word meaning; the noun content is pronounced [K AA1 N T EH0 N T], while\nthe adjective is pronounced [K AA0 N T EH1 N T].\nReduced Vowels and Schwa Unstressed vowels can be weakened even further to\nreduced vowels , the most common of which is schwa ([ax]), as in the second vowel reduced vowel\nschwa ofparakeet : [p ae r ax k iy t]. In a reduced vowel the articulatory gesture isn\u2019t as\ncomplete as for a full vowel. Not all unstressed vowels are reduced; any vowel, and\ndiphthongs in particular, can retain its full quality even in unstressed position. For\nexample, the vowel [iy] can appear in stressed position as in the word eat[iy t] or in\nunstressed position as in the word carry [k ae r iy].\nIn summary, there is a continuum of prosodic prominence , for which it is often prominence\nuseful to represent levels like accented, stressed, full vowel, and reduced vowel.\nH.3.2 Prosodic Structure\nSpoken sentences have prosodic structure: some words seem to group naturally to-\ngether, while some words seem to have a noticeable break or disjuncture between\nthem. Prosodic structure is often described in terms of prosodic phrasing , mean-prosodic\nphrasing\ning that an utterance has a prosodic phrase structure in a similar way to it having\na syntactic phrase structure. For example, the sentence I wanted to go to London,\nbut could only get tickets for France seems to have two main intonation phrases ,intonation\nphrase\ntheir boundary occurring at the comma. Furthermore, in the \ufb01rst phrase, there seems\nto be another set of lesser prosodic phrase boundaries (often called intermediate\nphrase s) that split up the words as I wantedjto gojto London . These kinds ofintermediate\nphrase\nintonation phrases are often correlated with syntactic structure constituents (Price\net al. 1991, Bennett and Elfner 2019).\nAutomatically predicting prosodic boundaries can be important for tasks like\nTTS. Modern approaches use sequence models that take either raw text or text an-\nnotated with features like parse trees as input, and make a break/no-break decision\nat each word boundary. They can be trained on data labeled for prosodic structure\nlike the Boston University Radio News Corpus (Ostendorf et al., 1995).\nH.3.3 Tune\nTwo utterances with the same prominence and phrasing patterns can still differ\nprosodically by having different tunes . The tune of an utterance is the rise and tune\nfall of its F0 over time. A very obvious example of tune is the difference between\nstatements and yes-no questions in English. The same words can be said with a \ufb01nal\nF0 rise to indicate a yes-no question (called a question rise ): question rise\nYou    know    what    Imean ?\nor a \ufb01nal drop in F0 (called a \ufb01nal fall ) to indicate a declarative intonation: \ufb01nal fall",
    "metadata": {
      "source": "H",
      "chunk_id": 11,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\nH.4 \u2022 A COUSTIC PHONETICS AND SIGNALS 9\nYou    know        what         Imean .\nLanguages make wide use of tune to express meaning (Xu, 2005). In English,\nfor example, besides this well-known rise for yes-no questions, a phrase containing\na list of nouns separated by commas often has a short rise called a continuation\nriseafter each noun. Other examples include the characteristic English contours forcontinuation\nrise\nexpressing contradiction and expressing surprise .\nLinking Prominence and Tune\nPitch accents come in different varieties that are related to tune; high pitched accents,\nfor example, have different functions than low pitched accents. There are many\ntypologies of accent classes in different languages. One such typology is part of the\nToBI (Tone and Break Indices) theory of intonation (Silverman et al. 1992). Each ToBI\nword in ToBI can be associated with one of \ufb01ve types of pitch accents shown in\nin Fig. H.7. Each utterance in ToBI consists of a sequence of intonational phrases,\neach of which ends in one of four boundary tones shown in Fig. H.7, representing boundary tone\nthe utterance \ufb01nal aspects of tune. There are version of ToBI for many languages.\nPitch Accents Boundary Tones\nH* peak accent L-L% \u201c\ufb01nal fall\u201d: \u201cdeclarative contour\u201d of American\nEnglish\nL* low accent L-H% continuation rise\nL*+H scooped accent H-H% \u201cquestion rise\u201d: cantonical yes-no question\ncontour\nL+H* rising peak accent H-L% \ufb01nal level plateau\nH+!H* step down\nFigure H.7 The accent and boundary tones labels from the ToBI transcription system for\nAmerican English intonation (Beckman and Ayers 1997, Beckman and Hirschberg 1994).\nH.4 Acoustic Phonetics and Signals\nWe begin with a very brief introduction to the acoustic waveform and its digitization\nand frequency analysis; the interested reader is encouraged to consult the references\nat the end of the chapter.\nH.4.1 Waves\nAcoustic analysis is based on the sine and cosine functions. Figure H.8 shows a plot\nof a sine wave, in particular the function\ny=A\u0003sin(2pft) (H.3)\nwhere we have set the amplitude A to 1 and the frequency fto 10 cycles per second.\nRecall from basic mathematics that two important characteristics of a wave are\nitsfrequency andamplitude . The frequency is the number of times a second that a frequency\namplitude wave repeats itself, that is, the number of cycles . We usually measure frequency in\ncycles per second . The signal in Fig. H.8 repeats itself 5 times in .5 seconds, hence\n10 cycles per second. Cycles per second are usually called hertz (shortened to Hz), Hertz",
    "metadata": {
      "source": "H",
      "chunk_id": 12,
      "token_count": 627,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 APPENDIX H \u2022 P HONETICS\nTime (s)0 0.5\u20131.01.0\n0\n0 0.1 0.2 0.3 0.4 0.5\nFigure H.8 A sine wave with a frequency of 10 Hz and an amplitude of 1.\nso the frequency in Fig. H.8 would be described as 10 Hz. The amplitude Aof a\nsine wave is the maximum value on the Y axis. The period Tof the wave is the time period\nit takes for one cycle to complete, de\ufb01ned as\nT=1\nf(H.4)\nEach cycle in Fig. H.8 lasts a tenth of a second; hence T=:1 seconds.\nH.4.2 Speech Sound Waves\nLet\u2019s turn from hypothetical waves to sound waves. The input to a speech recog-\nnizer, like the input to the human ear, is a complex series of changes in air pressure.\nThese changes in air pressure obviously originate with the speaker and are caused\nby the speci\ufb01c way that air passes through the glottis and out the oral or nasal cav-\nities. We represent sound waves by plotting the change in air pressure over time.\nOne metaphor which sometimes helps in understanding these graphs is that of a ver-\ntical plate blocking the air pressure waves (perhaps in a microphone in front of a\nspeaker\u2019s mouth, or the eardrum in a hearer\u2019s ear). The graph measures the amount\nofcompression orrarefaction (uncompression) of the air molecules at this plate.\nFigure H.9 shows a short segment of a waveform taken from the Switchboard corpus\nof telephone speech of the vowel [iy] from someone saying \u201cshe just had a baby\u201d.\nTime (s)0 0.03875\u20130.016970.02283\n0\nFigure H.9 A waveform of the vowel [iy] from an utterance shown later in Fig. H.13 on page 14. The y-axis\nshows the level of air pressure above and below normal atmospheric pressure. The x-axis shows time. Notice\nthat the wave repeats regularly.\nThe \ufb01rst step in digitizing a sound wave like Fig. H.9 is to convert the analog\nrepresentations (\ufb01rst air pressure and then analog electric signals in a microphone)\ninto a digital signal. This analog-to-digital conversion has two steps: sampling and sampling\nquantization . To sample a signal, we measure its amplitude at a particular time; the\nsampling rate is the number of samples taken per second. To accurately measure a\nwave, we must have at least two samples in each cycle: one measuring the positive",
    "metadata": {
      "source": "H",
      "chunk_id": 13,
      "token_count": 572,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\nH.4 \u2022 A COUSTIC PHONETICS AND SIGNALS 11\npart of the wave and one measuring the negative part. More than two samples per\ncycle increases the amplitude accuracy, but fewer than two samples causes the fre-\nquency of the wave to be completely missed. Thus, the maximum frequency wave\nthat can be measured is one whose frequency is half the sample rate (since every\ncycle needs two samples). This maximum frequency for a given sampling rate is\ncalled the Nyquist frequency . Most information in human speech is in frequenciesNyquist\nfrequency\nbelow 10,000 Hz; thus, a 20,000 Hz sampling rate would be necessary for com-\nplete accuracy. But telephone speech is \ufb01ltered by the switching network, and only\nfrequencies less than 4,000 Hz are transmitted by telephones. Thus, an 8,000 Hz\nsampling rate is suf\ufb01cient for telephone-bandwidth speech like the Switchboard\ncorpus, while 16,000 Hz sampling is often used for microphone speech.\nEven an 8,000 Hz sampling rate requires 8000 amplitude measurements for each\nsecond of speech, so it is important to store amplitude measurements ef\ufb01ciently.\nThey are usually stored as integers, either 8 bit (values from -128\u2013127) or 16 bit\n(values from -32768\u201332767). This process of representing real-valued numbers as\nintegers is called quantization because the difference between two integers acts as quantization\na minimum granularity (a quantum size) and all values that are closer together than\nthis quantum size are represented identically.\nOnce data is quantized, it is stored in various formats. One parameter of these\nformats is the sample rate and sample size discussed above; telephone speech is\noften sampled at 8 kHz and stored as 8-bit samples, and microphone data is often\nsampled at 16 kHz and stored as 16-bit samples. Another parameter is the number of\nchannels . For stereo data or for two-party conversations, we can store both channels channel\nin the same \ufb01le or we can store them in separate \ufb01les. A \ufb01nal parameter is individual\nsample storage\u2014linearly or compressed. One common compression format used for\ntelephone speech is m-law (often written u-law but still pronounced mu-law). The\nintuition of log compression algorithms like m-law is that human hearing is more\nsensitive at small intensities than large ones; the log represents small values with\nmore faithfulness at the expense of more error on large values. The linear (unlogged)\nvalues are generally referred to as linear PCM values (PCM stands for pulse code PCM\nmodulation, but never mind that). Here\u2019s the equation for compressing a linear PCM\nsample value xto 8-bit m-law, (where m=255 for 8 bits):\nF(x) =sgn(x)log(1+mjxj)\nlog(1+m)\u00001\u0014x\u00141 (H.5)\nThere are a number of standard \ufb01le formats for storing the resulting digitized wave-\n\ufb01le, such as Microsoft\u2019s .wav and Apple\u2019s AIFF all of which have special headers;\nsimple headerless \u201craw\u201d \ufb01les are also used. For example, the .wav format is a subset\nof Microsoft\u2019s RIFF format for multimedia \ufb01les; RIFF is a general format that can\nrepresent a series of nested chunks of data and control information. Figure H.10\nshows a simple .wav \ufb01le with a single data chunk together with its format chunk.\nFigure H.10 Microsoft wave\ufb01le header format, assuming simple \ufb01le with one chunk. Fol-\nlowing this 44-byte header would be the data chunk.",
    "metadata": {
      "source": "H",
      "chunk_id": 14,
      "token_count": 791,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 APPENDIX H \u2022 P HONETICS\nH.4.3 Frequency and Amplitude; Pitch and Loudness\nSound waves, like all waves, can be described in terms of frequency, amplitude, and\nthe other characteristics that we introduced earlier for pure sine waves. In sound\nwaves, these are not quite as simple to measure as they were for sine waves. Let\u2019s\nconsider frequency. Note in Fig. H.9 that although not exactly a sine, the wave is\nnonetheless periodic, repeating 10 times in the 38.75 milliseconds (.03875 seconds)\ncaptured in the \ufb01gure. Thus, the frequency of this segment of the wave is 10/.03875\nor 258 Hz.\nWhere does this periodic 258 Hz wave come from? It comes from the speed of\nvibration of the vocal folds; since the waveform in Fig. H.9 is from the vowel [iy], it\nis voiced. Recall that voicing is caused by regular openings and closing of the vocal\nfolds. When the vocal folds are open, air is pushing up through the lungs, creating\na region of high pressure. When the folds are closed, there is no pressure from the\nlungs. Thus, when the vocal folds are vibrating, we expect to see regular peaks in\namplitude of the kind we see in Fig. H.9, each major peak corresponding to an open-\ning of the vocal folds. The frequency of the vocal fold vibration, or the frequency\nof the complex wave, is called the fundamental frequency of the waveform, oftenfundamental\nfrequency\nabbreviated F0. We can plot F0 over time in a pitch track . Figure H.11 shows the F0\npitch track pitch track of a short question, \u201cThree o\u2019clock?\u201d represented below the waveform.\nNote the rise in F0 at the end of the question.\nthree o\u2019clock\nTime (s)0 0.5443750 Hz500 Hz\nFigure H.11 Pitch track of the question \u201cThree o\u2019clock?\u201d, shown below the wave\ufb01le. Note\nthe rise in F0 at the end of the question. Note the lack of pitch trace during the very quiet part\n(the \u201co\u2019\u201d of \u201co\u2019clock\u201d; automatic pitch tracking is based on counting the pulses in the voiced\nregions, and doesn\u2019t work if there is no voicing (or insuf\ufb01cient sound).\nThe vertical axis in Fig. H.9 measures the amount of air pressure variation; pres-\nsure is force per unit area, measured in Pascals (Pa). A high value on the vertical\naxis (a high amplitude) indicates that there is more air pressure at that point in time,\na zero value means there is normal (atmospheric) air pressure, and a negative value\nmeans there is lower than normal air pressure (rarefaction).\nIn addition to this value of the amplitude at any point in time, we also often\nneed to know the average amplitude over some time range, to give us some idea\nof how great the average displacement of air pressure is. But we can\u2019t just take\nthe average of the amplitude values over a range; the positive and negative values\nwould (mostly) cancel out, leaving us with a number close to zero. Instead, we\ngenerally use the RMS (root-mean-square) amplitude, which squares each number",
    "metadata": {
      "source": "H",
      "chunk_id": 15,
      "token_count": 710,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\nH.4 \u2022 A COUSTIC PHONETICS AND SIGNALS 13\nbefore averaging (making it positive), and then takes the square root at the end.\nRMS amplitudeN\ni=1=vuut1\nNNX\ni=1x2\ni (H.6)\nThepower of the signal is related to the square of the amplitude. If the number power\nof samples of a sound is N, the power is\nPower =1\nNNX\ni=1x2\ni (H.7)\nRather than power, we more often refer to the intensity of the sound, which intensity\nnormalizes the power to the human auditory threshold and is measured in dB. If P0\nis the auditory threshold pressure = 2 \u000210\u00005Pa, then intensity is de\ufb01ned as follows:\nIntensity =10log101\nNP0NX\ni=1x2\ni (H.8)\nFigure H.12 shows an intensity plot for the sentence \u201cIs it a long movie?\u201d from\nthe CallHome corpus, again shown below the waveform plot.\nis it a long movie?\nTime (s)0 1.1675\nFigure H.12 Intensity plot for the sentence \u201cIs it a long movie?\u201d. Note the intensity peaks\nat each vowel and the especially high peak for the word long.\nTwo important perceptual properties, pitch andloudness , are related to fre-\nquency and intensity. The pitch of a sound is the mental sensation, or perceptual pitch\ncorrelate, of fundamental frequency; in general, if a sound has a higher fundamen-\ntal frequency we perceive it as having a higher pitch. We say \u201cin general\u201d because\nthe relationship is not linear, since human hearing has different acuities for different\nfrequencies. Roughly speaking, human pitch perception is most accurate between\n100 Hz and 1000 Hz and in this range pitch correlates linearly with frequency. Hu-\nman hearing represents frequencies above 1000 Hz less accurately, and above this\nrange, pitch correlates logarithmically with frequency. Logarithmic representation\nmeans that the differences between high frequencies are compressed and hence not\nas accurately perceived. There are various psychoacoustic models of pitch percep-\ntion scales. One common model is the melscale (Stevens et al. 1937, Stevens and Mel",
    "metadata": {
      "source": "H",
      "chunk_id": 16,
      "token_count": 499,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n14 APPENDIX H \u2022 P HONETICS\nV olkmann 1940). A mel is a unit of pitch de\ufb01ned such that pairs of sounds which\nare perceptually equidistant in pitch are separated by an equal number of mels. The\nmel frequency mcan be computed from the raw acoustic frequency as follows:\nm=1127ln (1+f\n700) (H.9)\nAs we\u2019ll see in Chapter 16, the mel scale plays an important role in speech\nrecognition.\nTheloudness of a sound is the perceptual correlate of the power . So sounds with\nhigher amplitudes are perceived as louder, but again the relationship is not linear.\nFirst of all, as we mentioned above when we de\ufb01ned m-law compression, humans\nhave greater resolution in the low-power range; the ear is more sensitive to small\npower differences. Second, it turns out that there is a complex relationship between\npower, frequency, and perceived loudness; sounds in certain frequency ranges are\nperceived as being louder than those in other frequency ranges.\nVarious algorithms exist for automatically extracting F0. In a slight abuse of ter-\nminology, these are called pitch extraction algorithms. The autocorrelation method pitch extraction\nof pitch extraction, for example, correlates the signal with itself at various offsets.\nThe offset that gives the highest correlation gives the period of the signal. There\nare various publicly available pitch extraction toolkits; for example, an augmented\nautocorrelation pitch tracker is provided with Praat (Boersma and Weenink, 2005).\nH.4.4 Interpretation of Phones from a Waveform\nMuch can be learned from a visual inspection of a waveform. For example, vowels\nare pretty easy to spot. Recall that vowels are voiced; another property of vowels is\nthat they tend to be long and are relatively loud (as we can see in the intensity plot in\nFig. H.12). Length in time manifests itself directly on the x-axis, and loudness is re-\nlated to (the square of) amplitude on the y-axis. We saw in the previous section that\nvoicing is realized by regular peaks in amplitude of the kind we saw in Fig. H.9, each\nmajor peak corresponding to an opening of the vocal folds. Figure H.13 shows the\nwaveform of the short sentence \u201cshe just had a baby\u201d. We have labeled this wave-\nform with word and phone labels. Notice that each of the six vowels in Fig. H.13,\n[iy], [ax], [ae], [ax], [ey], [iy], all have regular amplitude peaks indicating voicing.\nshe just had a baby\nsh iy j ax s h ae dx ax b ey b iy\nTime (s)0 1.059\nFigure H.13 A waveform of the sentence \u201cShe just had a baby\u201d from the Switchboard corpus (conversation\n4325). The speaker is female, was 20 years old in 1991, which is approximately when the recording was made,\nand speaks the South Midlands dialect of American English.\nFor a stop consonant, which consists of a closure followed by a release, we can\noften see a period of silence or near silence followed by a slight burst of amplitude.\nWe can see this for both of the [b]\u2019s in baby in Fig. H.13.",
    "metadata": {
      "source": "H",
      "chunk_id": 17,
      "token_count": 709,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\nH.4 \u2022 A COUSTIC PHONETICS AND SIGNALS 15\nAnother phone that is often quite recognizable in a waveform is a fricative. Re-\ncall that fricatives, especially very strident fricatives like [sh], are made when a\nnarrow channel for air\ufb02ow causes noisy, turbulent air. The resulting hissy sounds\nhave a noisy, irregular waveform. This can be seen somewhat in Fig. H.13; it\u2019s even\nclearer in Fig. H.14, where we\u2019ve magni\ufb01ed just the \ufb01rst word she.\nshe\nsh iy\nTime (s)0 0.257\nFigure H.14 A more detailed view of the \ufb01rst word \u201cshe\u201d extracted from the wave\ufb01le in Fig. H.13. Notice\nthe difference between the random noise of the fricative [sh] and the regular voicing of the vowel [iy].\nH.4.5 Spectra and the Frequency Domain\nWhile some broad phonetic features (such as energy, pitch, and the presence of voic-\ning, stop closures, or fricatives) can be interpreted directly from the waveform, most\ncomputational applications such as speech recognition (as well as human auditory\nprocessing) are based on a different representation of the sound in terms of its com-\nponent frequencies. The insight of Fourier analysis is that every complex wave can\nbe represented as a sum of many sine waves of different frequencies. Consider the\nwaveform in Fig. H.15. This waveform was created (in Praat) by summing two sine\nwaveforms, one of frequency 10 Hz and one of frequency 100 Hz.\nTime (s)0 0.5\u201311\n0\nFigure H.15 A waveform that is the sum of two sine waveforms, one of frequency 10\nHz (note \ufb01ve repetitions in the half-second window) and one of frequency 100 Hz, both of\namplitude 1.\nWe can represent these two component frequencies with a spectrum . The spec- spectrum\ntrum of a signal is a representation of each of its frequency components and their\namplitudes. Figure H.16 shows the spectrum of Fig. H.15. Frequency in Hz is on\nthe x-axis and amplitude on the y-axis. Note the two spikes in the \ufb01gure, one at\n10 Hz and one at 100 Hz. Thus, the spectrum is an alternative representation of\nthe original waveform, and we use the spectrum as a tool to study the component\nfrequencies of a sound wave at a particular time point.",
    "metadata": {
      "source": "H",
      "chunk_id": 18,
      "token_count": 554,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\n16 APPENDIX H \u2022 P HONETICS\nFrequency (Hz)1 10 100 2 20 200 5 50Sound pressure level (dB /Hz)\n406080\nFigure H.16 The spectrum of the waveform in Fig. H.15.\nLet\u2019s look now at the frequency components of a speech waveform. Figure H.17\nshows part of the waveform for the vowel [ae] of the word had, cut out from the\nsentence shown in Fig. H.13.\nTime (s)0 0.04275\u20130.055540.04968\n0\nFigure H.17 The waveform of part of the vowel [ae] from the word hadcut out from the\nwaveform shown in Fig. H.13.\nNote that there is a complex wave that repeats about ten times in the \ufb01gure; but\nthere is also a smaller repeated wave that repeats four times for every larger pattern\n(notice the four small peaks inside each repeated wave). The complex wave has a\nfrequency of about 234 Hz (we can \ufb01gure this out since it repeats roughly 10 times\nin .0427 seconds, and 10 cycles/.0427 seconds = 234 Hz).\nThe smaller wave then should have a frequency of roughly four times the fre-\nquency of the larger wave, or roughly 936 Hz. Then, if you look carefully, you can\nsee two little waves on the peak of many of the 936 Hz waves. The frequency of this\ntiniest wave must be roughly twice that of the 936 Hz wave, hence 1872 Hz.\nFigure H.18 shows a smoothed spectrum for the waveform in Fig. H.17, com-\nputed with a discrete Fourier transform (DFT).\nFrequency (Hz)0 4000Sound pressure level (dB /Hz)\n\u201320020\n0 2000 4000 0 1000 2000 3000 4000\nFigure H.18 A spectrum for the vowel [ae] from the word hadin the waveform of She just\nhad a baby in Fig. H.13.",
    "metadata": {
      "source": "H",
      "chunk_id": 19,
      "token_count": 447,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\nH.4 \u2022 A COUSTIC PHONETICS AND SIGNALS 17\nThe x-axis of a spectrum shows frequency, and the y-axis shows some mea-\nsure of the magnitude of each frequency component (in decibels (dB), a logarithmic\nmeasure of amplitude that we saw earlier). Thus, Fig. H.18 shows signi\ufb01cant fre-\nquency components at around 930 Hz, 1860 Hz, and 3020 Hz, along with many\nother lower-magnitude frequency components. These \ufb01rst two components are just\nwhat we noticed in the time domain by looking at the wave in Fig. H.17!\nWhy is a spectrum useful? It turns out that these spectral peaks that are easily\nvisible in a spectrum are characteristic of different phones; phones have characteris-\ntic spectral \u201csignatures\u201d. Just as chemical elements give off different wavelengths of\nlight when they burn, allowing us to detect elements in stars by looking at the spec-\ntrum of the light, we can detect the characteristic signature of the different phones\nby looking at the spectrum of a waveform. This use of spectral information is essen-\ntial to both human and machine speech recognition. In human audition, the function\nof the cochlea , orinner ear , is to compute a spectrum of the incoming waveform. cochlea\nSimilarly, the acoustic features used in speech recognition are spectral representa-\ntions.\nLet\u2019s look at the spectrum of different vowels. Since some vowels change over\ntime, we\u2019ll use a different kind of plot called a spectrogram . While a spectrum\nshows the frequency components of a wave at one point in time, a spectrogram is a spectrogram\nway of envisioning how the different frequencies that make up a waveform change\nover time. The x-axis shows time, as it did for the waveform, but the y-axis now\nshows frequencies in hertz. The darkness of a point on a spectrogram corresponds\nto the amplitude of the frequency component. Very dark points have high amplitude,\nlight points have low amplitude. Thus, the spectrogram is a useful way of visualizing\nthe three dimensions (time x frequency x amplitude).\nFigure H.19 shows spectrograms of three American English vowels, [ih], [ae],\nand [ah]. Note that each vowel has a set of dark bars at various frequency bands,\nslightly different bands for each vowel. Each of these represents the same kind of\nspectral peak that we saw in Fig. H.17.\nTime (s)0 2.8139705000Frequency (Hz)\nFigure H.19 Spectrograms for three American English vowels, [ih], [ae], and [uh]\nEach dark bar (or spectral peak) is called a formant . As we discuss below, a formant\nformant is a frequency band that is particularly ampli\ufb01ed by the vocal tract. Since\ndifferent vowels are produced with the vocal tract in different positions, they will\nproduce different kinds of ampli\ufb01cations or resonances. Let\u2019s look at the \ufb01rst two\nformants, called F1 and F2. Note that F1, the dark bar closest to the bottom, is in a\ndifferent position for the three vowels; it\u2019s low for [ih] (centered at about 470 Hz)\nand somewhat higher for [ae] and [ah] (somewhere around 800 Hz). By contrast,\nF2, the second dark bar from the bottom, is highest for [ih], in the middle for [ae],\nand lowest for [ah].\nWe can see the same formants in running speech, although the reduction and",
    "metadata": {
      "source": "H",
      "chunk_id": 20,
      "token_count": 759,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\n18 APPENDIX H \u2022 P HONETICS\ncoarticulation processes make them somewhat harder to see. Figure H.20 shows the\nspectrogram of \u201cshe just had a baby\u201d, whose waveform was shown in Fig. H.13. F1\nand F2 (and also F3) are pretty clear for the [ax] of just, the [ae] of had, and the [ey]\nofbaby .\nshe just had a baby\nsh iy j ax s h ae dxax b ey b iy\nTime (s)0 1.059\nFigure H.20 A spectrogram of the sentence \u201cshe just had a baby\u201d whose waveform was shown in Fig. H.13.\nWe can think of a spectrogram as a collection of spectra (time slices), like Fig. H.18 placed end to end.\nWhat speci\ufb01c clues can spectral representations give for phone identi\ufb01cation?\nFirst, since different vowels have their formants at characteristic places, the spectrum\ncan distinguish vowels from each other. We\u2019ve seen that [ae] in the sample waveform\nhad formants at 930 Hz, 1860 Hz, and 3020 Hz. Consider the vowel [iy] at the\nbeginning of the utterance in Fig. H.13. The spectrum for this vowel is shown in\nFig. H.21. The \ufb01rst formant of [iy] is 540 Hz, much lower than the \ufb01rst formant for\n[ae], and the second formant (2581 Hz) is much higher than the second formant for\n[ae]. If you look carefully, you can see these formants as dark bars in Fig. H.20 just\naround 0.5 seconds.\n\u22121001020304050607080\n0 1000 2000 3000\nFigure H.21 A smoothed (LPC) spectrum for the vowel [iy] at the start of She just had a\nbaby . Note that the \ufb01rst formant (540 Hz) is much lower than the \ufb01rst formant for [ae] shown\nin Fig. H.18, and the second formant (2581 Hz) is much higher than the second formant for\n[ae].\nThe location of the \ufb01rst two formants (called F1 and F2) plays a large role in de-\ntermining vowel identity, although the formants still differ from speaker to speaker.\nHigher formants tend to be caused more by general characteristics of a speaker\u2019s\nvocal tract rather than by individual vowels. Formants also can be used to identify\nthe nasal phones [n], [m], and [ng] and the liquids [l] and [r].",
    "metadata": {
      "source": "H",
      "chunk_id": 21,
      "token_count": 574,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\nH.4 \u2022 A COUSTIC PHONETICS AND SIGNALS 19\nH.4.6 The Source-Filter Model\nWhy do different vowels have different spectral signatures? As we brie\ufb02y mentioned\nabove, the formants are caused by the resonant cavities of the mouth. The source-\n\ufb01lter model is a way of explaining the acoustics of a sound by modeling how thesource-\ufb01lter\nmodel\npulses produced by the glottis (the source ) are shaped by the vocal tract (the \ufb01lter ).\nLet\u2019s see how this works. Whenever we have a wave such as the vibration in air\ncaused by the glottal pulse, the wave also has harmonics . A harmonic is another harmonic\nwave whose frequency is a multiple of the fundamental wave. Thus, for example, a\n115 Hz glottal fold vibration leads to harmonics (other waves) of 230 Hz, 345 Hz,\n460 Hz, and so on on. In general, each of these waves will be weaker, that is, will\nhave much less amplitude than the wave at the fundamental frequency.\nIt turns out, however, that the vocal tract acts as a kind of \ufb01lter or ampli\ufb01er;\nindeed any cavity, such as a tube, causes waves of certain frequencies to be ampli\ufb01ed\nand others to be damped. This ampli\ufb01cation process is caused by the shape of the\ncavity; a given shape will cause sounds of a certain frequency to resonate and hence\nbe ampli\ufb01ed. Thus, by changing the shape of the cavity, we can cause different\nfrequencies to be ampli\ufb01ed.\nWhen we produce particular vowels, we are essentially changing the shape of\nthe vocal tract cavity by placing the tongue and the other articulators in particular\npositions. The result is that different vowels cause different harmonics to be ampli-\n\ufb01ed. So a wave of the same fundamental frequency passed through different vocal\ntract positions will result in different harmonics being ampli\ufb01ed.\nWe can see the result of this ampli\ufb01cation by looking at the relationship between\nthe shape of the vocal tract and the corresponding spectrum. Figure H.22 shows\nthe vocal tract position for three vowels and a typical resulting spectrum. The for-\nmants are places in the spectrum where the vocal tract happens to amplify particular\nharmonic frequencies.\nFrequency (Hz)04000Sound pressure level (dB/Hz)\n020\n2682416F1F2[iy]  (tea)\nFrequency (Hz)04000Sound pressure level (dB/Hz)\n020\n9031695F1 F2[ae] (cat) \nFrequency (Hz)04000Sound pressure level (dB/Hz)\n\u2013200\n295817F1 F2[uw]  (moo)\n[ae] (cat) [uw]  (moo)[iy]  (tea)\nFigure H.22 Visualizing the vocal tract position as a \ufb01lter: the tongue positions for three English vowels and\nthe resulting smoothed spectra showing F1 and F2.",
    "metadata": {
      "source": "H",
      "chunk_id": 22,
      "token_count": 667,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20",
    "metadata": {
      "source": "H",
      "chunk_id": 23,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "20 APPENDIX H \u2022 P HONETICS\nH.5 Phonetic Resources\nA wide variety of phonetic resources can be drawn on for computational work. On-\nlinepronunciation dictionaries give phonetic transcriptions for words. The LDCpronunciation\ndictionary\ndistributes pronunciation lexicons for Egyptian Arabic, Dutch, English, German,\nJapanese, Korean, Mandarin, and Spanish. For English, the CELEX dictionary\n(Baayen et al., 1995) has pronunciations for 160,595 wordforms, with syllabi\ufb01ca-\ntion, stress, and morphological and part-of-speech information. The open-source\nCMU Pronouncing Dictionary (CMU, 1993) has pronunciations for about 134,000\nwordforms, while the \ufb01ne-grained 110,000 word UNISYN dictionary (Fitt, 2002),\nfreely available for research purposes, gives syllabi\ufb01cations, stress, and also pronun-\nciations for dozens of dialects of English.\nAnother useful resource is a phonetically annotated corpus , in which a col-\nlection of waveforms is hand-labeled with the corresponding string of phones. The\nTIMIT corpus (NIST, 1990), originally a joint project between Texas Instruments\n(TI), MIT, and SRI, is a corpus of 6300 read sentences, with 10 sentences each from\n630 speakers. The 6300 sentences were drawn from a set of 2342 sentences, some\nselected to have particular dialect shibboleths, others to maximize phonetic diphone\ncoverage. Each sentence in the corpus was phonetically hand-labeled, the sequence\nof phones was automatically aligned with the sentence wave\ufb01le, and then the au-\ntomatic phone boundaries were manually hand-corrected (Seneff and Zue, 1988).\nThe result is a time-aligned transcription : a transcription in which each phone istime-aligned\ntranscription\nassociated with a start and end time in the waveform, like the example in Fig. H.23.\nshe had your dark suit ingreasy wash water all year\nsh iy hv ae dcl jh axr dcl d aa r kcl s ux q engcl g r iy s ix w aa sh q w aa dx axr q aa l y ix axr\nFigure H.23 Phonetic transcription from the TIMIT corpus, using special ARPAbet features for narrow tran-\nscription, such as the palatalization of [d] in had, unreleased \ufb01nal stop in dark, glottalization of \ufb01nal [t] in suit\nto [q], and \ufb02ap of [t] in water . The TIMIT corpus also includes time-alignments (not shown).\nThe Switchboard Transcription Project phonetically annotated corpus consists\nof 3.5 hours of sentences extracted from the Switchboard corpus (Greenberg et al.,\n1996), together with transcriptions time-aligned at the syllable level. Figure H.24\nshows an example .\n0.470 0.640 0.720 0.900 0.953 1.279 1.410 1.630\ndh er k aa n ax v ih m b ix t w iy n r ay n aw\nFigure H.24 Phonetic transcription of the Switchboard phrase they\u2019re kind of in between\nright now . Note vowel reduction in they\u2019re andof, coda deletion in kind andright , and re-\nsyllabi\ufb01cation (the [v] of ofattaches as the onset of in). Time is given in number of seconds\nfrom the beginning of sentence to the start of each syllable.",
    "metadata": {
      "source": "H",
      "chunk_id": 24,
      "token_count": 771,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure H.23 Phonetic transcription from the TIMIT corpus, using special ARPAbet features for narrow tran-\nscription, such as the palatalization of [d] in had, unreleased \ufb01nal stop in dark, glottalization of \ufb01nal [t] in suit\nto [q], and \ufb02ap of [t] in water . The TIMIT corpus also includes time-alignments (not shown).\nThe Switchboard Transcription Project phonetically annotated corpus consists\nof 3.5 hours of sentences extracted from the Switchboard corpus (Greenberg et al.,\n1996), together with transcriptions time-aligned at the syllable level. Figure H.24\nshows an example .\n0.470 0.640 0.720 0.900 0.953 1.279 1.410 1.630\ndh er k aa n ax v ih m b ix t w iy n r ay n aw\nFigure H.24 Phonetic transcription of the Switchboard phrase they\u2019re kind of in between\nright now . Note vowel reduction in they\u2019re andof, coda deletion in kind andright , and re-\nsyllabi\ufb01cation (the [v] of ofattaches as the onset of in). Time is given in number of seconds\nfrom the beginning of sentence to the start of each syllable.\nThe Buckeye corpus (Pitt et al. 2007, Pitt et al. 2005) is a phonetically tran-\nscribed corpus of spontaneous American speech, containing about 300,000 words\nfrom 40 talkers. Phonetically transcribed corpora are also available for other lan-\nguages, including the Kiel corpus of German and Mandarin corpora transcribed by\nthe Chinese Academy of Social Sciences (Li et al., 2000).\nIn addition to resources like dictionaries and corpora, there are many useful pho-\nnetic software tools. Many of the \ufb01gures in this book were generated by the Praat\npackage (Boersma and Weenink, 2005), which includes pitch, spectral, and formant\nanalysis, as well as a scripting language.",
    "metadata": {
      "source": "H",
      "chunk_id": 25,
      "token_count": 447,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21\n\nH.6 \u2022 S UMMARY 21\nH.6 Summary\nThis chapter has introduced many of the important concepts of phonetics and com-\nputational phonetics.\n\u2022 We can represent the pronunciation of words in terms of units called phones .\nThe standard system for representing phones is the International Phonetic\nAlphabet orIPA. The most common computational system for transcription\nof English is the ARPAbet , which conveniently uses ASCII symbols.\n\u2022 Phones can be described by how they are produced articulatorily by the vocal\norgans; consonants are de\ufb01ned in terms of their place andmanner of articu-\nlation and voicing ; vowels by their height ,backness , and roundness .\n\u2022 Speech sounds can also be described acoustically . Sound waves can be de-\nscribed in terms of frequency ,amplitude , or their perceptual correlates, pitch\nandloudness .\n\u2022 The spectrum of a sound describes its different frequency components. While\nsome phonetic properties are recognizable from the waveform, both humans\nand machines rely on spectral analysis for phone detection.\n\u2022 A spectrogram is a plot of a spectrum over time. V owels are described by\ncharacteristic harmonics called formants .\nBibliographical and Historical Notes\nThe major insights of articulatory phonetics date to the linguists of 800\u2013150 B.C.\nIndia. They invented the concepts of place and manner of articulation, worked out\nthe glottal mechanism of voicing, and understood the concept of assimilation. Eu-\nropean science did not catch up with the Indian phoneticians until over 2000 years\nlater, in the late 19th century. The Greeks did have some rudimentary phonetic\nknowledge; by the time of Plato\u2019s Theaetetus andCratylus , for example, they distin-\nguished vowels from consonants, and stop consonants from continuants. The Stoics\ndeveloped the idea of the syllable and were aware of phonotactic constraints on pos-\nsible words. An unknown Icelandic scholar of the 12th century exploited the concept\nof the phoneme and proposed a phonemic writing system for Icelandic, including\ndiacritics for length and nasality. But his text remained unpublished until 1818 and\neven then was largely unknown outside Scandinavia (Robins, 1967). The modern\nera of phonetics is usually said to have begun with Sweet, who proposed what is\nessentially the phoneme in his Handbook of Phonetics 1877. He also devised an al-\nphabet for transcription and distinguished between broad andnarrow transcription,\nproposing many ideas that were eventually incorporated into the IPA. Sweet was\nconsidered the best practicing phonetician of his time; he made the \ufb01rst scienti\ufb01c\nrecordings of languages for phonetic purposes and advanced the state of the art of\narticulatory description. He was also infamously dif\ufb01cult to get along with, a trait\nthat is well captured in Henry Higgins, the stage character that George Bernard Shaw\nmodeled after him. The phoneme was \ufb01rst named by the Polish scholar Baudouin\nde Courtenay, who published his theories in 1894.\nIntroductory phonetics textbooks include Ladefoged (1993) and Clark and Yal-\nlop (1995). Wells (1982) is the de\ufb01nitive three-volume source on dialects of English.\nMany of the classic insights in acoustic phonetics had been developed by the\nlate 1950s or early 1960s; just a few highlights include techniques like the sound",
    "metadata": {
      "source": "H",
      "chunk_id": 26,
      "token_count": 758,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22\n\n22 APPENDIX H \u2022 P HONETICS\nspectrograph (Koenig et al., 1946), theoretical insights like the working out of the\nsource-\ufb01lter theory and other issues in the mapping between articulation and acous-\ntics ((Fant, 1960), Stevens et al. 1953, Stevens and House 1955, Heinz and Stevens\n1961, Stevens and House 1961) the F1xF2 space of vowel formants (Peterson and\nBarney, 1952), the understanding of the phonetic nature of stress and the use of\nduration and intensity as cues (Fry, 1955), and a basic understanding of issues in\nphone perception (Miller and Nicely 1955,Liberman et al. 1952). Lehiste (1967) is\na collection of classic papers on acoustic phonetics. Many of the seminal papers of\nGunnar Fant have been collected in Fant (2004).\nExcellent textbooks on acoustic phonetics include Johnson (2003) and Lade-\nfoged (1996). Coleman (2005) includes an introduction to computational process-\ning of acoustics and speech from a linguistic perspective. Stevens (1998) lays out\nan in\ufb02uential theory of speech sound production. There are a number of software\npackages for acoustic phonetic analysis. Probably the most widely used one is Praat\n(Boersma and Weenink, 2005).\nExercises\nH.1 Find the mistakes in the ARPAbet transcriptions of the following words:\na.\u201cthree\u201d [dh r i] d.\u201cstudy\u201d [s t uh d i] g.\u201cslight\u201d [s l iy t]\nb.\u201csing\u201d [s ih n g] e.\u201cthough\u201d [th ow]\nc.\u201ceyes\u201d [ay s] f.\u201cplanning\u201d [p pl aa n ih ng]\nH.2 Ira Gershwin\u2019s lyric for Let\u2019s Call the Whole Thing Off talks about two pro-\nnunciations (each) of the words \u201ctomato\u201d, \u201cpotato\u201d, and \u201ceither\u201d. Transcribe\ninto the ARPAbet both pronunciations of each of these three words.\nH.3 Transcribe the following words in the ARPAbet:\n1. dark\n2. suit\n3. greasy\n4. wash\n5. water\nH.4 Take a wave\ufb01le of your choice. Some examples are on the textbook website.\nDownload the Praat software, and use it to transcribe the wave\ufb01les at the word\nlevel and into ARPAbet phones, using Praat to help you play pieces of each\nwave\ufb01le and to look at the wave\ufb01le and the spectrogram.\nH.5 Record yourself saying \ufb01ve of the English vowels: [aa], [eh], [ae], [iy], [uw].\nFind F1 and F2 for each of your vowels.",
    "metadata": {
      "source": "H",
      "chunk_id": 27,
      "token_count": 627,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23",
    "metadata": {
      "source": "H",
      "chunk_id": 28,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Exercises 23\nBaayen, R. H., R. Piepenbrock, and L. Gulikers. 1995.\nThe CELEX Lexical Database (Release 2) [CD-ROM] .\nLinguistic Data Consortium, University of Pennsylvania\n[Distributor].\nBeckman, M. E. and G. M. Ayers. 1997. Guidelines\nfor ToBI labelling. Unpublished manuscript, Ohio\nState University, http://www.ling.ohio-state.\nedu/research/phonetics/E_ToBI/ .\nBeckman, M. E. and J. Hirschberg. 1994. The ToBI annota-\ntion conventions. Manuscript, Ohio State University.\nBennett, R. and E. Elfner. 2019. The syntax\u2013prosody inter-\nface. Annual Review of Linguistics , 5:151\u2013171.\nBoersma, P. and D. Weenink. 2005. Praat: doing phonetics\nby computer (version 4.3.14). [Computer program]. Re-\ntrieved May 26, 2005, from http://www.praat.org/ .\nClark, J. and C. Yallop. 1995. An Introduction to Phonetics\nand Phonology , 2nd edition. Blackwell.\nCMU. 1993. The Carnegie Mellon Pronouncing Dictionary\nv0.1. Carnegie Mellon University.\nColeman, J. 2005. Introducing Speech and Language Pro-\ncessing . Cambridge University Press.\nFant, G. M. 1960. Acoustic Theory of Speech Production .\nMouton.\nFant, G. M. 2004. Speech Acoustics and Phonetics . Kluwer.\nFitt, S. 2002. Unisyn lexicon. http://www.cstr.ed.ac.\nuk/projects/unisyn/ .\nFry, D. B. 1955. Duration and intensity as physical correlates\nof linguistic stress. JASA , 27:765\u2013768.\nGreenberg, S., D. Ellis, and J. Hollenback. 1996. Insights\ninto spoken language gleaned from phonetic transcription\nof the Switchboard corpus. ICSLP .\nHeinz, J. M. and K. N. Stevens. 1961. On the properties of\nvoiceless fricative consonants. JASA , 33:589\u2013596.\nJohnson, K. 2003. Acoustic and Auditory Phonetics , 2nd\nedition. Blackwell.\nKoenig, W., H. K. Dunn, and L. Y . Lacy. 1946. The sound\nspectrograph. JASA , 18:19\u201349.\nLadefoged, P. 1993. A Course in Phonetics . Harcourt Brace\nJovanovich. (3rd ed.).\nLadefoged, P. 1996. Elements of Acoustic Phonetics , 2nd\nedition. University of Chicago.\nLehiste, I., ed. 1967. Readings in Acoustic Phonetics . MIT\nPress.\nLi, A., F. Zheng, W. Byrne, P. Fung, T. Kamm, L. Yi,\nZ. Song, U. Ruhi, V . Venkataramani, and X. Chen. 2000.\nCASS: A phonetically transcribed corpus of Mandarin\nspontaneous speech. ICSLP .",
    "metadata": {
      "source": "H",
      "chunk_id": 29,
      "token_count": 742,
      "chapter_title": ""
    }
  },
  {
    "content": "into spoken language gleaned from phonetic transcription\nof the Switchboard corpus. ICSLP .\nHeinz, J. M. and K. N. Stevens. 1961. On the properties of\nvoiceless fricative consonants. JASA , 33:589\u2013596.\nJohnson, K. 2003. Acoustic and Auditory Phonetics , 2nd\nedition. Blackwell.\nKoenig, W., H. K. Dunn, and L. Y . Lacy. 1946. The sound\nspectrograph. JASA , 18:19\u201349.\nLadefoged, P. 1993. A Course in Phonetics . Harcourt Brace\nJovanovich. (3rd ed.).\nLadefoged, P. 1996. Elements of Acoustic Phonetics , 2nd\nedition. University of Chicago.\nLehiste, I., ed. 1967. Readings in Acoustic Phonetics . MIT\nPress.\nLi, A., F. Zheng, W. Byrne, P. Fung, T. Kamm, L. Yi,\nZ. Song, U. Ruhi, V . Venkataramani, and X. Chen. 2000.\nCASS: A phonetically transcribed corpus of Mandarin\nspontaneous speech. ICSLP .\nLiberman, A. M., P. C. Delattre, and F. S. Cooper. 1952. The\nrole of selected stimulus variables in the perception of the\nunvoiced stop consonants. American Journal of Psychol-\nogy, 65:497\u2013516.\nMiller, G. A. and P. E. Nicely. 1955. An analysis of percep-\ntual confusions among some English consonants. JASA ,\n27:338\u2013352.\nNIST. 1990. TIMIT Acoustic-Phonetic Continuous Speech\nCorpus. National Institute of Standards and Technology\nSpeech Disc 1-1.1. NIST Order No. PB91-505065.Ostendorf, M., P. Price, and S. Shattuck-Hufnagel. 1995. The\nBoston University Radio News Corpus. Technical Report\nECS-95-001, Boston University.\nPeterson, G. E. and H. L. Barney. 1952. Control methods\nused in a study of the vowels. JASA , 24:175\u2013184.\nPitt, M. A., L. Dilley, K. Johnson, S. Kiesling, W. D. Ray-\nmond, E. Hume, and E. Fosler-Lussier. 2007. Buckeye\ncorpus of conversational speech (2nd release). Depart-\nment of Psychology, Ohio State University (Distributor).\nPitt, M. A., K. Johnson, E. Hume, S. Kiesling, and W. D.\nRaymond. 2005. The buckeye corpus of conversational\nspeech: Labeling conventions and a test of transcriber re-\nliability. Speech Communication , 45:90\u201395.\nPrice, P. J., M. Ostendorf, S. Shattuck-Hufnagel, and\nC. Fong. 1991. The use of prosody in syntactic disam-\nbiguation. JASA , 90(6).\nRobins, R. H. 1967. A Short History of Linguistics . Indiana\nUniversity Press, Bloomington.\nSeneff, S. and V . W. Zue. 1988. Transcription and align-",
    "metadata": {
      "source": "H",
      "chunk_id": 30,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "Peterson, G. E. and H. L. Barney. 1952. Control methods\nused in a study of the vowels. JASA , 24:175\u2013184.\nPitt, M. A., L. Dilley, K. Johnson, S. Kiesling, W. D. Ray-\nmond, E. Hume, and E. Fosler-Lussier. 2007. Buckeye\ncorpus of conversational speech (2nd release). Depart-\nment of Psychology, Ohio State University (Distributor).\nPitt, M. A., K. Johnson, E. Hume, S. Kiesling, and W. D.\nRaymond. 2005. The buckeye corpus of conversational\nspeech: Labeling conventions and a test of transcriber re-\nliability. Speech Communication , 45:90\u201395.\nPrice, P. J., M. Ostendorf, S. Shattuck-Hufnagel, and\nC. Fong. 1991. The use of prosody in syntactic disam-\nbiguation. JASA , 90(6).\nRobins, R. H. 1967. A Short History of Linguistics . Indiana\nUniversity Press, Bloomington.\nSeneff, S. and V . W. Zue. 1988. Transcription and align-\nment of the TIMIT database. Proceedings of the Second\nSymposium on Advanced Man-Machine Interface through\nSpoken Language .\nShoup, J. E. 1980. Phonological aspects of speech recogni-\ntion. In W. A. Lea, ed., Trends in Speech Recognition ,\n125\u2013138. Prentice Hall.\nSilverman, K., M. E. Beckman, J. F. Pitrelli, M. Ostendorf,\nC. W. Wightman, P. J. Price, J. B. Pierrehumbert, and\nJ. Hirschberg. 1992. ToBI: A standard for labelling En-\nglish prosody. ICSLP .\nStevens, K. N. 1998. Acoustic Phonetics . MIT Press.\nStevens, K. N. and A. S. House. 1955. Development of\na quantitative description of vowel articulation. JASA ,\n27:484\u2013493.\nStevens, K. N. and A. S. House. 1961. An acoustical theory\nof vowel production and some of its implications. Journal\nof Speech and Hearing Research , 4:303\u2013320.\nStevens, K. N., S. Kasowski, and G. M. Fant. 1953. An elec-\ntrical analog of the vocal tract. JASA , 25(4):734\u2013742.\nStevens, S. S. and J. V olkmann. 1940. The relation of pitch\nto frequency: A revised scale. The American Journal of\nPsychology , 53(3):329\u2013353.\nStevens, S. S., J. V olkmann, and E. B. Newman. 1937. A\nscale for the measurement of the psychological magni-\ntude pitch. JASA , 8:185\u2013190.\nSweet, H. 1877. A Handbook of Phonetics . Clarendon Press.\nWells, J. C. 1982. Accents of English . Cambridge University\nPress.\nXu, Y . 2005. Speech melody as articulatorily implemented\ncommunicative functions. Speech communication , 46(3-",
    "metadata": {
      "source": "H",
      "chunk_id": 31,
      "token_count": 766,
      "chapter_title": ""
    }
  },
  {
    "content": "a quantitative description of vowel articulation. JASA ,\n27:484\u2013493.\nStevens, K. N. and A. S. House. 1961. An acoustical theory\nof vowel production and some of its implications. Journal\nof Speech and Hearing Research , 4:303\u2013320.\nStevens, K. N., S. Kasowski, and G. M. Fant. 1953. An elec-\ntrical analog of the vocal tract. JASA , 25(4):734\u2013742.\nStevens, S. S. and J. V olkmann. 1940. The relation of pitch\nto frequency: A revised scale. The American Journal of\nPsychology , 53(3):329\u2013353.\nStevens, S. S., J. V olkmann, and E. B. Newman. 1937. A\nscale for the measurement of the psychological magni-\ntude pitch. JASA , 8:185\u2013190.\nSweet, H. 1877. A Handbook of Phonetics . Clarendon Press.\nWells, J. C. 1982. Accents of English . Cambridge University\nPress.\nXu, Y . 2005. Speech melody as articulatorily implemented\ncommunicative functions. Speech communication , 46(3-\n4):220\u2013251.",
    "metadata": {
      "source": "H",
      "chunk_id": 32,
      "token_count": 290,
      "chapter_title": ""
    }
  }
]