[
  {
    "content": "# 7_NN_Apr_28_2021\n\n## Page 1\n\nSimple Neural Networks and Neural Language ModelsUnits in Neural Networks\n\n## Page 2\n\nThis is in your brain\n2\nBy BruceBlaus-Own work, CC BY 3.0, https://commons.wikimedia.org/w/index.php?curid=28761830\n\n## Page 3\n\nNeural Network UnitThis is not in your brain\n3x1x2x3\ny\nw1w2w3\u2211b\u03c3+1zaWeightsInput layerWeighted sumNon-linear transformOutput valuebias",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 0,
      "token_count": 126,
      "chapter_title": "7_NN_Apr_28_2021"
    }
  },
  {
    "content": "## Page 4",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 1,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Neural unitTake weighted sum of inputs, plus a biasInstead of just using z, we'll apply a nonlinear activation function f:2CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELS7.1 UnitsThe building block of a neural network is a single computational unit. A unit takesa set of real valued numbers as input, performs some computation on them, andproduces an output.At its heart, a neural unit is taking a weighted sum of its inputs, with one addi-tional term in the sum called abias term. Given a set of inputsx1...xn, a unit hasbias terma set of corresponding weightsw1...wnand a biasb, so the weighted sumzcan berepresented as:z=b+Xiwixi(7.1)Often it\u2019s more convenient to express this weighted sum using vector notation; recallfrom linear algebra that avectoris, at heart, just a list or array of numbers. Thusvectorwe\u2019ll talk aboutzin terms of a weight vectorw, a scalar biasb, and an input vectorx, and we\u2019ll replace the sum with the convenientdot product:z=w\u00b7x+b(7.2)As de\ufb01ned in Eq.7.2,zis just a real valued number.Finally, instead of usingz, a linear function ofx, as the output, neural unitsapply a non-linear functionftoz. We will refer to the output of this function astheactivationvalue for the unit,a. Since we are just modeling a single unit, theactivationactivation for the node is in fact the \ufb01nal output of the network, which we\u2019ll generallycally. So the valueyis de\ufb01ned as:y=a=f(z)We\u2019ll discuss three popular non-linear functionsf()below (the sigmoid, the tanh,and the recti\ufb01ed linear ReLU) but it\u2019s pedagogically convenient to start with thesigmoidfunction since we saw it in Chapter 5:sigmoidy=s(z)=11+e\u0000z(7.3)The sigmoid (shown in Fig.7.1) has a number of advantages; it maps the outputinto the range[0,1], which is useful in squashing outliers toward 0 or 1. And it\u2019sdifferentiable, which as we saw in Section??will be handy for learning.",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 2,
      "token_count": 490,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 7.1The sigmoid function takes a real value and maps it to the range[0,1]. It isnearly linear around 0 but outlier values get squashed toward 0 or 1.2CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELS7.1 UnitsThe building block of a neural network is a single computational unit. A unit takesa set of real valued numbers as input, performs some computation on them, andproduces an output.At its heart, a neural unit is taking a weighted sum of its inputs, with one addi-tional term in the sum called abias term. Given a set of inputsx1...xn, a unit hasbias terma set of corresponding weightsw1...wnand a biasb, so the weighted sumzcan berepresented as:z=b+Xiwixi(7.1)Often it\u2019s more convenient to express this weighted sum using vector notation; recallfrom linear algebra that avectoris, at heart, just a list or array of numbers. Thusvectorwe\u2019ll talk aboutzin terms of a weight vectorw, a scalar biasb, and an input vectorx, and we\u2019ll replace the sum with the convenientdot product:z=w\u00b7x+b(7.2)As de\ufb01ned in Eq.7.2,zis just a real valued number.Finally, instead of usingz, a linear function ofx, as the output, neural unitsapply a non-linear functionftoz. We will refer to the output of this function astheactivationvalue for the unit,a. Since we are just modeling a single unit, theactivationactivation for the node is in fact the \ufb01nal output of the network, which we\u2019ll generallycally. So the valueyis de\ufb01ned as:y=a=f(z)We\u2019ll discuss three popular non-linear functionsf()below (the sigmoid, the tanh,and the recti\ufb01ed linear ReLU) but it\u2019s pedagogically convenient to start with thesigmoidfunction since we saw it in Chapter 5:sigmoidy=s(z)=11+e\u0000z(7.3)The sigmoid (shown in Fig.7.1) has a number of advantages; it maps the outputinto the range[0,1], which is useful in squashing outliers toward 0 or 1. And it\u2019sdifferentiable, which as we saw in Section??will be handy for learning.",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 3,
      "token_count": 506,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 7.1The sigmoid function takes a real value and maps it to the range[0,1]. It isnearly linear around 0 but outlier values get squashed toward 0 or 1.2CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELS7.1 UnitsThe building block of a neural network is a single computational unit. A unit takesa set of real valued numbers as input, performs some computation on them, andproduces an output.At its heart, a neural unit is taking a weighted sum of its inputs, with one addi-tional term in the sum called abias term. Given a set of inputsx1...xn, a unit hasbias terma set of corresponding weightsw1...wnand a biasb, so the weighted sumzcan berepresented as:z=b+Xiwixi(7.1)Often it\u2019s more convenient to express this weighted sum using vector notation; recallfrom linear algebra that avectoris, at heart, just a list or array of numbers. Thusvectorwe\u2019ll talk aboutzin terms of a weight vectorw, a scalar biasb, and an input vectorx, and we\u2019ll replace the sum with the convenientdot product:z=w\u00b7x+b(7.2)As de\ufb01ned in Eq.7.2,zis just a real valued number.Finally, instead of usingz, a linear function ofx, as the output, neural unitsapply a non-linear functionftoz. We will refer to the output of this function astheactivationvalue for the unit,a. Since we are just modeling a single unit, theactivationactivation for the node is in fact the \ufb01nal output of the network, which we\u2019ll generallycally. So the valueyis de\ufb01ned as:y=a=f(z)We\u2019ll discuss three popular non-linear functionsf()below (the sigmoid, the tanh,and the recti\ufb01ed linear ReLU) but it\u2019s pedagogically convenient to start with thesigmoidfunction since we saw it in Chapter 5:sigmoidy=s(z)=11+e\u0000z(7.3)The sigmoid (shown in Fig.7.1) has a number of advantages; it maps the outputinto the range[0,1], which is useful in squashing outliers toward 0 or 1. And it\u2019sdifferentiable, which as we saw in Section??will be handy for learning.\nFigure 7.1The sigmoid function takes a real value and maps it to the range[0,1]. It isnearly linear around 0 but outlier values get squashed toward 0 or 1.",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 4,
      "token_count": 549,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\nNon-Linear Activation Functions\n5\nSigmoidWe're already seen the sigmoid for logistic regression:2CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELS7.1 UnitsThe building block of a neural network is a single computational unit. A unit takesa set of real valued numbers as input, performs some computation on them, andproduces an output.At its heart, a neural unit is taking a weighted sum of its inputs, with one addi-tional term in the sum called abias term. Given a set of inputsx1...xn, a unit hasbias terma set of corresponding weightsw1...wnand a biasb, so the weighted sumzcan berepresented as:z=b+Xiwixi(7.1)Often it\u2019s more convenient to express this weighted sum using vector notation; recallfrom linear algebra that avectoris, at heart, just a list or array of numbers. Thusvectorwe\u2019ll talk aboutzin terms of a weight vectorw, a scalar biasb, and an input vectorx, and we\u2019ll replace the sum with the convenientdot product:z=w\u00b7x+b(7.2)As de\ufb01ned in Eq.7.2,zis just a real valued number.Finally, instead of usingz, a linear function ofx, as the output, neural unitsapply a non-linear functionftoz. We will refer to the output of this function astheactivationvalue for the unit,a. Since we are just modeling a single unit, theactivationactivation for the node is in fact the \ufb01nal output of the network, which we\u2019ll generallycally. So the valueyis de\ufb01ned as:y=a=f(z)We\u2019ll discuss three popular non-linear functionsf()below (the sigmoid, the tanh,and the recti\ufb01ed linear ReLU) but it\u2019s pedagogically convenient to start with thesigmoidfunction since we saw it in Chapter 5:sigmoidy=s(z)=11+e\u0000z(7.3)The sigmoid (shown in Fig.7.1) has a number of advantages; it maps the outputinto the range[0,1], which is useful in squashing outliers toward 0 or 1. And it\u2019sdifferentiable, which as we saw in Section??will be handy for learning.\nFigure 7.1The sigmoid function takes a real value and maps it to the range[0,1]. It isnearly linear around 0 but outlier values get squashed toward 0 or 1.",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 5,
      "token_count": 531,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\nFinal function the unit is computing7.1\u2022UNITS3Substituting Eq.7.2into Eq.7.3gives us the output of a neural unit:y=s(w\u00b7x+b)=11+exp(\u0000(w\u00b7x+b))(7.4)Fig.7.2shows a \ufb01nal schematic of a basic neural unit. In this example the unittakes 3 input valuesx1,x2, andx3, and computes a weighted sum, multiplying eachvalue by a weight (w1,w2, andw3, respectively), adds them to a bias termb, and thenpasses the resulting sum through a sigmoid function to result in a number between 0and 1.\nx1x2x3\ny\nw1w2w3\u2211b\u03c3+1za\nFigure 7.2A neural unit, taking 3 inputsx1,x2, andx3(and a biasbthat we represent as aweight for an input clamped at +1) and producing an output y. We include some convenientintermediate variables: the output of the summation,z, and the output of the sigmoid,a. Inthis case the output of the unityis the same asa, but in deeper networks we\u2019ll reserveytomean the \ufb01nal output of the entire network, leavingaas the activation of an individual node.Let\u2019s walk through an example just to get an intuition. Let\u2019s suppose we have aunit with the following weight vector and bias:w=[0.2,0.3,0.9]b=0.5What would this unit do with the following input vector:x=[0.5,0.6,0.1]The resulting outputywould be:y=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)=11+e\u0000(.5\u21e4.2+.6\u21e4.3+.1\u21e4.9+.5)=11+e\u00000.87=.70In practice, the sigmoid is not commonly used as an activation function. A functionthat is very similar but almost always better is thetanhfunction shown in Fig.7.3a;tanhtanh is a variant of the sigmoid that ranges from -1 to +1:y=ez\u0000e\u0000zez+e\u0000z(7.5)The simplest activation function, and perhaps the most commonly used, is the rec-ti\ufb01ed linear unit, also called theReLU, shown in Fig.7.3b. It\u2019s just the same asxReLUwhenxis positive, and 0 otherwise:y=max(x,0)(7.6)\n\n## Page 7\n\nFinal unit again\n7x1x2x3\ny\nw1w2w3\u2211b\u03c3+1zaWeightsInput layerWeighted sumNon-linear activation functionOutput valuebias",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 6,
      "token_count": 602,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\nFinal unit again\n7x1x2x3\ny\nw1w2w3\u2211b\u03c3+1zaWeightsInput layerWeighted sumNon-linear activation functionOutput valuebias\n\n## Page 8\n\nAn exampleSuppose a unit has:w = [0.2,0.3,0.9] b = 0.5 What happens with input x:x = [0.5,0.6,0.1] 7.1\u2022UNITS3Substituting Eq.7.2into Eq.7.3gives us the output of a neural unit:y=s(w\u00b7x+b)=11+exp(\u0000(w\u00b7x+b))(7.4)Fig.7.2shows a \ufb01nal schematic of a basic neural unit. In this example the unittakes 3 input valuesx1,x2, andx3, and computes a weighted sum, multiplying eachvalue by a weight (w1,w2, andw3, respectively), adds them to a bias termb, and thenpasses the resulting sum through a sigmoid function to result in a number between 0and 1.\nx1x2x3\ny\nw1w2w3\u2211b\u03c3+1za\nFigure 7.2A neural unit, taking 3 inputsx1,x2, andx3(and a biasbthat we represent as aweight for an input clamped at +1) and producing an output y. We include some convenientintermediate variables: the output of the summation,z, and the output of the sigmoid,a. Inthis case the output of the unityis the same asa, but in deeper networks we\u2019ll reserveytomean the \ufb01nal output of the entire network, leavingaas the activation of an individual node.Let\u2019s walk through an example just to get an intuition. Let\u2019s suppose we have aunit with the following weight vector and bias:w=[0.2,0.3,0.9]b=0.5What would this unit do with the following input vector:x=[0.5,0.6,0.1]The resulting outputywould be:y=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)=11+e\u0000(.5\u21e4.2+.6\u21e4.3+.1\u21e4.9+.5)=11+e\u00000.87=.70In practice, the sigmoid is not commonly used as an activation function. A functionthat is very similar but almost always better is thetanhfunction shown in Fig.7.3a;tanhtanh is a variant of the sigmoid that ranges from -1 to +1:y=ez\u0000e\u0000zez+e\u0000z(7.5)The simplest activation function, and perhaps the most commonly used, is the rec-ti\ufb01ed linear unit, also called theReLU, shown in Fig.7.3b. It\u2019s just the same asxReLUwhenxis positive, and 0 otherwise:y=max(x,0)(7.6)",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 7,
      "token_count": 646,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 8,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "An exampleSuppose a unit has:w = [0.2,0.3,0.9] b = 0.5 What happens with the following input x?x = [0.5,0.6,0.1] 7.1\u2022UNITS3Substituting Eq.7.2into Eq.7.3gives us the output of a neural unit:y=s(w\u00b7x+b)=11+exp(\u0000(w\u00b7x+b))(7.4)Fig.7.2shows a \ufb01nal schematic of a basic neural unit. In this example the unittakes 3 input valuesx1,x2, andx3, and computes a weighted sum, multiplying eachvalue by a weight (w1,w2, andw3, respectively), adds them to a bias termb, and thenpasses the resulting sum through a sigmoid function to result in a number between 0and 1.\nx1x2x3\ny\nw1w2w3\u2211b\u03c3+1za\nFigure 7.2A neural unit, taking 3 inputsx1,x2, andx3(and a biasbthat we represent as aweight for an input clamped at +1) and producing an output y. We include some convenientintermediate variables: the output of the summation,z, and the output of the sigmoid,a. Inthis case the output of the unityis the same asa, but in deeper networks we\u2019ll reserveytomean the \ufb01nal output of the entire network, leavingaas the activation of an individual node.Let\u2019s walk through an example just to get an intuition. Let\u2019s suppose we have aunit with the following weight vector and bias:w=[0.2,0.3,0.9]b=0.5What would this unit do with the following input vector:x=[0.5,0.6,0.1]The resulting outputywould be:y=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)=11+e\u0000(.5\u21e4.2+.6\u21e4.3+.1\u21e4.9+.5)=11+e\u00000.87=.70In practice, the sigmoid is not commonly used as an activation function. A functionthat is very similar but almost always better is thetanhfunction shown in Fig.7.3a;tanhtanh is a variant of the sigmoid that ranges from -1 to +1:y=ez\u0000e\u0000zez+e\u0000z(7.5)The simplest activation function, and perhaps the most commonly used, is the rec-ti\ufb01ed linear unit, also called theReLU, shown in Fig.7.3b. It\u2019s just the same asxReLUwhenxis positive, and 0 otherwise:y=max(x,0)(7.6)7.1\u2022UNITS3Substituting Eq.7.2into Eq.7.3gives us the output of a neural unit:y=s(w\u00b7x+b)=11+exp(\u0000(w\u00b7x+b))(7.4)Fig.7.2shows a \ufb01nal schematic of a basic neural unit. In this example the unittakes 3 input valuesx1,x2, andx3, and computes a weighted sum, multiplying eachvalue by a weight (w1,w2, andw3, respectively), adds them to a bias termb, and thenpasses the resulting sum through a sigmoid function to result in a number between 0and 1.\nx1x2x3\ny\nw1w2w3\u2211b\u03c3+1za",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 9,
      "token_count": 763,
      "chapter_title": ""
    }
  },
  {
    "content": "x1x2x3\ny\nw1w2w3\u2211b\u03c3+1za\nFigure 7.2A neural unit, taking 3 inputsx1,x2, andx3(and a biasbthat we represent as aweight for an input clamped at +1) and producing an output y. We include some convenientintermediate variables: the output of the summation,z, and the output of the sigmoid,a. Inthis case the output of the unityis the same asa, but in deeper networks we\u2019ll reserveytomean the \ufb01nal output of the entire network, leavingaas the activation of an individual node.Let\u2019s walk through an example just to get an intuition. Let\u2019s suppose we have aunit with the following weight vector and bias:w=[0.2,0.3,0.9]b=0.5What would this unit do with the following input vector:x=[0.5,0.6,0.1]The resulting outputywould be:y=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)=11+e\u0000(.5\u21e4.2+.6\u21e4.3+.1\u21e4.9+.5)=11+e\u00000.87=.70In practice, the sigmoid is not commonly used as an activation function. A functionthat is very similar but almost always better is thetanhfunction shown in Fig.7.3a;tanhtanh is a variant of the sigmoid that ranges from -1 to +1:y=ez\u0000e\u0000zez+e\u0000z(7.5)The simplest activation function, and perhaps the most commonly used, is the rec-ti\ufb01ed linear unit, also called theReLU, shown in Fig.7.3b. It\u2019s just the same asxReLUwhenxis positive, and 0 otherwise:y=max(x,0)(7.6)",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 10,
      "token_count": 403,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 11,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "An exampleSuppose a unit has:w = [0.2,0.3,0.9] b = 0.5 What happens with input x:x = [0.5,0.6,0.1] 7.1\u2022UNITS3Substituting Eq.7.2into Eq.7.3gives us the output of a neural unit:y=s(w\u00b7x+b)=11+exp(\u0000(w\u00b7x+b))(7.4)Fig.7.2shows a \ufb01nal schematic of a basic neural unit. In this example the unittakes 3 input valuesx1,x2, andx3, and computes a weighted sum, multiplying eachvalue by a weight (w1,w2, andw3, respectively), adds them to a bias termb, and thenpasses the resulting sum through a sigmoid function to result in a number between 0and 1.\nx1x2x3\ny\nw1w2w3\u2211b\u03c3+1za\nFigure 7.2A neural unit, taking 3 inputsx1,x2, andx3(and a biasbthat we represent as aweight for an input clamped at +1) and producing an output y. We include some convenientintermediate variables: the output of the summation,z, and the output of the sigmoid,a. Inthis case the output of the unityis the same asa, but in deeper networks we\u2019ll reserveytomean the \ufb01nal output of the entire network, leavingaas the activation of an individual node.Let\u2019s walk through an example just to get an intuition. Let\u2019s suppose we have aunit with the following weight vector and bias:w=[0.2,0.3,0.9]b=0.5What would this unit do with the following input vector:x=[0.5,0.6,0.1]The resulting outputywould be:y=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)=11+e\u0000(.5\u21e4.2+.6\u21e4.3+.1\u21e4.9+.5)=11+e\u00000.87=.70In practice, the sigmoid is not commonly used as an activation function. A functionthat is very similar but almost always better is thetanhfunction shown in Fig.7.3a;tanhtanh is a variant of the sigmoid that ranges from -1 to +1:y=ez\u0000e\u0000zez+e\u0000z(7.5)The simplest activation function, and perhaps the most commonly used, is the rec-ti\ufb01ed linear unit, also called theReLU, shown in Fig.7.3b. It\u2019s just the same asxReLUwhenxis positive, and 0 otherwise:y=max(x,0)(7.6)7.1\u2022UNITS3Substituting Eq.7.2into Eq.7.3gives us the output of a neural unit:y=s(w\u00b7x+b)=11+exp(\u0000(w\u00b7x+b))(7.4)Fig.7.2shows a \ufb01nal schematic of a basic neural unit. In this example the unittakes 3 input valuesx1,x2, andx3, and computes a weighted sum, multiplying eachvalue by a weight (w1,w2, andw3, respectively), adds them to a bias termb, and thenpasses the resulting sum through a sigmoid function to result in a number between 0and 1.\nx1x2x3\ny\nw1w2w3\u2211b\u03c3+1za",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 12,
      "token_count": 760,
      "chapter_title": ""
    }
  },
  {
    "content": "x1x2x3\ny\nw1w2w3\u2211b\u03c3+1za\nFigure 7.2A neural unit, taking 3 inputsx1,x2, andx3(and a biasbthat we represent as aweight for an input clamped at +1) and producing an output y. We include some convenientintermediate variables: the output of the summation,z, and the output of the sigmoid,a. Inthis case the output of the unityis the same asa, but in deeper networks we\u2019ll reserveytomean the \ufb01nal output of the entire network, leavingaas the activation of an individual node.Let\u2019s walk through an example just to get an intuition. Let\u2019s suppose we have aunit with the following weight vector and bias:w=[0.2,0.3,0.9]b=0.5What would this unit do with the following input vector:x=[0.5,0.6,0.1]The resulting outputywould be:y=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)=11+e\u0000(.5\u21e4.2+.6\u21e4.3+.1\u21e4.9+.5)=11+e\u00000.87=.70In practice, the sigmoid is not commonly used as an activation function. A functionthat is very similar but almost always better is thetanhfunction shown in Fig.7.3a;tanhtanh is a variant of the sigmoid that ranges from -1 to +1:y=ez\u0000e\u0000zez+e\u0000z(7.5)The simplest activation function, and perhaps the most commonly used, is the rec-ti\ufb01ed linear unit, also called theReLU, shown in Fig.7.3b. It\u2019s just the same asxReLUwhenxis positive, and 0 otherwise:y=max(x,0)(7.6)7.1\u2022UNITS3Substituting Eq.7.2into Eq.7.3gives us the output of a neural unit:y=s(w\u00b7x+b)=11+exp(\u0000(w\u00b7x+b))(7.4)Fig.7.2shows a \ufb01nal schematic of a basic neural unit. In this example the unittakes 3 input valuesx1,x2, andx3, and computes a weighted sum, multiplying eachvalue by a weight (w1,w2, andw3, respectively), adds them to a bias termb, and thenpasses the resulting sum through a sigmoid function to result in a number between 0and 1.\nx1x2x3\ny\nw1w2w3\u2211b\u03c3+1za",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 13,
      "token_count": 568,
      "chapter_title": ""
    }
  },
  {
    "content": "x1x2x3\ny\nw1w2w3\u2211b\u03c3+1za\nFigure 7.2A neural unit, taking 3 inputsx1,x2, andx3(and a biasbthat we represent as aweight for an input clamped at +1) and producing an output y. We include some convenientintermediate variables: the output of the summation,z, and the output of the sigmoid,a. Inthis case the output of the unityis the same asa, but in deeper networks we\u2019ll reserveytomean the \ufb01nal output of the entire network, leavingaas the activation of an individual node.Let\u2019s walk through an example just to get an intuition. Let\u2019s suppose we have aunit with the following weight vector and bias:w=[0.2,0.3,0.9]b=0.5What would this unit do with the following input vector:x=[0.5,0.6,0.1]The resulting outputywould be:y=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)=11+e\u0000(.5\u21e4.2+.6\u21e4.3+.1\u21e4.9+.5)=11+e\u00000.87=.70In practice, the sigmoid is not commonly used as an activation function. A functionthat is very similar but almost always better is thetanhfunction shown in Fig.7.3a;tanhtanh is a variant of the sigmoid that ranges from -1 to +1:y=ez\u0000e\u0000zez+e\u0000z(7.5)The simplest activation function, and perhaps the most commonly used, is the rec-ti\ufb01ed linear unit, also called theReLU, shown in Fig.7.3b. It\u2019s just the same asxReLUwhenxis positive, and 0 otherwise:y=max(x,0)(7.6)",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 14,
      "token_count": 403,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 15,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "An exampleSuppose a unit has:w = [0.2,0.3,0.9] b = 0.5 What happens with input x:x = [0.5,0.6,0.1] 7.1\u2022UNITS3Substituting Eq.7.2into Eq.7.3gives us the output of a neural unit:y=s(w\u00b7x+b)=11+exp(\u0000(w\u00b7x+b))(7.4)Fig.7.2shows a \ufb01nal schematic of a basic neural unit. In this example the unittakes 3 input valuesx1,x2, andx3, and computes a weighted sum, multiplying eachvalue by a weight (w1,w2, andw3, respectively), adds them to a bias termb, and thenpasses the resulting sum through a sigmoid function to result in a number between 0and 1.\nx1x2x3\ny\nw1w2w3\u2211b\u03c3+1za\nFigure 7.2A neural unit, taking 3 inputsx1,x2, andx3(and a biasbthat we represent as aweight for an input clamped at +1) and producing an output y. We include some convenientintermediate variables: the output of the summation,z, and the output of the sigmoid,a. Inthis case the output of the unityis the same asa, but in deeper networks we\u2019ll reserveytomean the \ufb01nal output of the entire network, leavingaas the activation of an individual node.Let\u2019s walk through an example just to get an intuition. Let\u2019s suppose we have aunit with the following weight vector and bias:w=[0.2,0.3,0.9]b=0.5What would this unit do with the following input vector:x=[0.5,0.6,0.1]The resulting outputywould be:y=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)=11+e\u0000(.5\u21e4.2+.6\u21e4.3+.1\u21e4.9+.5)=11+e\u00000.87=.70In practice, the sigmoid is not commonly used as an activation function. A functionthat is very similar but almost always better is thetanhfunction shown in Fig.7.3a;tanhtanh is a variant of the sigmoid that ranges from -1 to +1:y=ez\u0000e\u0000zez+e\u0000z(7.5)The simplest activation function, and perhaps the most commonly used, is the rec-ti\ufb01ed linear unit, also called theReLU, shown in Fig.7.3b. It\u2019s just the same asxReLUwhenxis positive, and 0 otherwise:y=max(x,0)(7.6)7.1\u2022UNITS3Substituting Eq.7.2into Eq.7.3gives us the output of a neural unit:y=s(w\u00b7x+b)=11+exp(\u0000(w\u00b7x+b))(7.4)Fig.7.2shows a \ufb01nal schematic of a basic neural unit. In this example the unittakes 3 input valuesx1,x2, andx3, and computes a weighted sum, multiplying eachvalue by a weight (w1,w2, andw3, respectively), adds them to a bias termb, and thenpasses the resulting sum through a sigmoid function to result in a number between 0and 1.\nx1x2x3\ny\nw1w2w3\u2211b\u03c3+1za",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 16,
      "token_count": 760,
      "chapter_title": ""
    }
  },
  {
    "content": "x1x2x3\ny\nw1w2w3\u2211b\u03c3+1za\nFigure 7.2A neural unit, taking 3 inputsx1,x2, andx3(and a biasbthat we represent as aweight for an input clamped at +1) and producing an output y. We include some convenientintermediate variables: the output of the summation,z, and the output of the sigmoid,a. Inthis case the output of the unityis the same asa, but in deeper networks we\u2019ll reserveytomean the \ufb01nal output of the entire network, leavingaas the activation of an individual node.Let\u2019s walk through an example just to get an intuition. Let\u2019s suppose we have aunit with the following weight vector and bias:w=[0.2,0.3,0.9]b=0.5What would this unit do with the following input vector:x=[0.5,0.6,0.1]The resulting outputywould be:y=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)=11+e\u0000(.5\u21e4.2+.6\u21e4.3+.1\u21e4.9+.5)=11+e\u00000.87=.70In practice, the sigmoid is not commonly used as an activation function. A functionthat is very similar but almost always better is thetanhfunction shown in Fig.7.3a;tanhtanh is a variant of the sigmoid that ranges from -1 to +1:y=ez\u0000e\u0000zez+e\u0000z(7.5)The simplest activation function, and perhaps the most commonly used, is the rec-ti\ufb01ed linear unit, also called theReLU, shown in Fig.7.3b. It\u2019s just the same asxReLUwhenxis positive, and 0 otherwise:y=max(x,0)(7.6)7.1\u2022UNITS3Substituting Eq.7.2into Eq.7.3gives us the output of a neural unit:y=s(w\u00b7x+b)=11+exp(\u0000(w\u00b7x+b))(7.4)Fig.7.2shows a \ufb01nal schematic of a basic neural unit. In this example the unittakes 3 input valuesx1,x2, andx3, and computes a weighted sum, multiplying eachvalue by a weight (w1,w2, andw3, respectively), adds them to a bias termb, and thenpasses the resulting sum through a sigmoid function to result in a number between 0and 1.\nx1x2x3\ny\nw1w2w3\u2211b\u03c3+1za",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 17,
      "token_count": 568,
      "chapter_title": ""
    }
  },
  {
    "content": "x1x2x3\ny\nw1w2w3\u2211b\u03c3+1za\nFigure 7.2A neural unit, taking 3 inputsx1,x2, andx3(and a biasbthat we represent as aweight for an input clamped at +1) and producing an output y. We include some convenientintermediate variables: the output of the summation,z, and the output of the sigmoid,a. Inthis case the output of the unityis the same asa, but in deeper networks we\u2019ll reserveytomean the \ufb01nal output of the entire network, leavingaas the activation of an individual node.Let\u2019s walk through an example just to get an intuition. Let\u2019s suppose we have aunit with the following weight vector and bias:w=[0.2,0.3,0.9]b=0.5What would this unit do with the following input vector:x=[0.5,0.6,0.1]The resulting outputywould be:y=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)=11+e\u0000(.5\u21e4.2+.6\u21e4.3+.1\u21e4.9+.5)=11+e\u00000.87=.70In practice, the sigmoid is not commonly used as an activation function. A functionthat is very similar but almost always better is thetanhfunction shown in Fig.7.3a;tanhtanh is a variant of the sigmoid that ranges from -1 to +1:y=ez\u0000e\u0000zez+e\u0000z(7.5)The simplest activation function, and perhaps the most commonly used, is the rec-ti\ufb01ed linear unit, also called theReLU, shown in Fig.7.3b. It\u2019s just the same asxReLUwhenxis positive, and 0 otherwise:y=max(x,0)(7.6)7.1\u2022UNITS3Substituting Eq.7.2into Eq.7.3gives us the output of a neural unit:y=s(w\u00b7x+b)=11+exp(\u0000(w\u00b7x+b))(7.4)Fig.7.2shows a \ufb01nal schematic of a basic neural unit. In this example the unittakes 3 input valuesx1,x2, andx3, and computes a weighted sum, multiplying eachvalue by a weight (w1,w2, andw3, respectively), adds them to a bias termb, and thenpasses the resulting sum through a sigmoid function to result in a number between 0and 1.\nx1x2x3\ny\nw1w2w3\u2211b\u03c3+1za",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 18,
      "token_count": 568,
      "chapter_title": ""
    }
  },
  {
    "content": "x1x2x3\ny\nw1w2w3\u2211b\u03c3+1za\nFigure 7.2A neural unit, taking 3 inputsx1,x2, andx3(and a biasbthat we represent as aweight for an input clamped at +1) and producing an output y. We include some convenientintermediate variables: the output of the summation,z, and the output of the sigmoid,a. Inthis case the output of the unityis the same asa, but in deeper networks we\u2019ll reserveytomean the \ufb01nal output of the entire network, leavingaas the activation of an individual node.Let\u2019s walk through an example just to get an intuition. Let\u2019s suppose we have aunit with the following weight vector and bias:w=[0.2,0.3,0.9]b=0.5What would this unit do with the following input vector:x=[0.5,0.6,0.1]The resulting outputywould be:y=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)=11+e\u0000(.5\u21e4.2+.6\u21e4.3+.1\u21e4.9+.5)=11+e\u00000.87=.70In practice, the sigmoid is not commonly used as an activation function. A functionthat is very similar but almost always better is thetanhfunction shown in Fig.7.3a;tanhtanh is a variant of the sigmoid that ranges from -1 to +1:y=ez\u0000e\u0000zez+e\u0000z(7.5)The simplest activation function, and perhaps the most commonly used, is the rec-ti\ufb01ed linear unit, also called theReLU, shown in Fig.7.3b. It\u2019s just the same asxReLUwhenxis positive, and 0 otherwise:y=max(x,0)(7.6)7.1\u2022UNITS3Substituting Eq.7.2into Eq.7.3gives us the output of a neural unit:y=s(w\u00b7x+b)=11+exp(\u0000(w\u00b7x+b))(7.4)Fig.7.2shows a \ufb01nal schematic of a basic neural unit. In this example the unittakes 3 input valuesx1,x2, andx3, and computes a weighted sum, multiplying eachvalue by a weight (w1,w2, andw3, respectively), adds them to a bias termb, and thenpasses the resulting sum through a sigmoid function to result in a number between 0and 1.\nx1x2x3\ny\nw1w2w3\u2211b\u03c3+1za",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 19,
      "token_count": 568,
      "chapter_title": ""
    }
  },
  {
    "content": "x1x2x3\ny\nw1w2w3\u2211b\u03c3+1za\nFigure 7.2A neural unit, taking 3 inputsx1,x2, andx3(and a biasbthat we represent as aweight for an input clamped at +1) and producing an output y. We include some convenientintermediate variables: the output of the summation,z, and the output of the sigmoid,a. Inthis case the output of the unityis the same asa, but in deeper networks we\u2019ll reserveytomean the \ufb01nal output of the entire network, leavingaas the activation of an individual node.Let\u2019s walk through an example just to get an intuition. Let\u2019s suppose we have aunit with the following weight vector and bias:w=[0.2,0.3,0.9]b=0.5What would this unit do with the following input vector:x=[0.5,0.6,0.1]The resulting outputywould be:y=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)=11+e\u0000(.5\u21e4.2+.6\u21e4.3+.1\u21e4.9+.5)=11+e\u00000.87=.70In practice, the sigmoid is not commonly used as an activation function. A functionthat is very similar but almost always better is thetanhfunction shown in Fig.7.3a;tanhtanh is a variant of the sigmoid that ranges from -1 to +1:y=ez\u0000e\u0000zez+e\u0000z(7.5)The simplest activation function, and perhaps the most commonly used, is the rec-ti\ufb01ed linear unit, also called theReLU, shown in Fig.7.3b. It\u2019s just the same asxReLUwhenxis positive, and 0 otherwise:y=max(x,0)(7.6)",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 20,
      "token_count": 403,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 21,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Non-Linear Activation Functions besides sigmoid\n12\ntanhReLURectified Linear Unit7.1\u2022UNITS3Substituting Eq.7.2into Eq.7.3gives us the output of a neural unit:y=s(w\u00b7x+b)=11+exp(\u0000(w\u00b7x+b))(7.4)Fig.7.2shows a \ufb01nal schematic of a basic neural unit. In this example the unittakes 3 input valuesx1,x2, andx3, and computes a weighted sum, multiplying eachvalue by a weight (w1,w2, andw3, respectively), adds them to a bias termb, and thenpasses the resulting sum through a sigmoid function to result in a number between 0and 1.\nx1x2x3\ny\nw1w2w3\u2211b\u03c3+1za\nFigure 7.2A neural unit, taking 3 inputsx1,x2, andx3(and a biasbthat we represent as aweight for an input clamped at +1) and producing an output y. We include some convenientintermediate variables: the output of the summation,z, and the output of the sigmoid,a. Inthis case the output of the unityis the same asa, but in deeper networks we\u2019ll reserveytomean the \ufb01nal output of the entire network, leavingaas the activation of an individual node.Let\u2019s walk through an example just to get an intuition. Let\u2019s suppose we have aunit with the following weight vector and bias:w=[0.2,0.3,0.9]b=0.5What would this unit do with the following input vector:x=[0.5,0.6,0.1]The resulting outputywould be:y=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)=11+e\u0000(.5\u21e4.2+.6\u21e4.3+.1\u21e4.9+.5)=11+e\u00000.87=.70In practice, the sigmoid is not commonly used as an activation function. A functionthat is very similar but almost always better is thetanhfunction shown in Fig.7.3a;tanhtanh is a variant of the sigmoid that ranges from -1 to +1:y=ez\u0000e\u0000zez+e\u0000z(7.5)The simplest activation function, and perhaps the most commonly used, is the rec-ti\ufb01ed linear unit, also called theReLU, shown in Fig.7.3b. It\u2019s just the same asxReLUwhenxis positive, and 0 otherwise:y=max(x,0)(7.6)\n7.1\u2022UNITS3Substituting Eq.7.2into Eq.7.3gives us the output of a neural unit:y=s(w\u00b7x+b)=11+exp(\u0000(w\u00b7x+b))(7.4)Fig.7.2shows a \ufb01nal schematic of a basic neural unit. In this example the unittakes 3 input valuesx1,x2, andx3, and computes a weighted sum, multiplying eachvalue by a weight (w1,w2, andw3, respectively), adds them to a bias termb, and thenpasses the resulting sum through a sigmoid function to result in a number between 0and 1.\nx1x2x3\ny\nw1w2w3\u2211b\u03c3+1za",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 22,
      "token_count": 730,
      "chapter_title": ""
    }
  },
  {
    "content": "7.1\u2022UNITS3Substituting Eq.7.2into Eq.7.3gives us the output of a neural unit:y=s(w\u00b7x+b)=11+exp(\u0000(w\u00b7x+b))(7.4)Fig.7.2shows a \ufb01nal schematic of a basic neural unit. In this example the unittakes 3 input valuesx1,x2, andx3, and computes a weighted sum, multiplying eachvalue by a weight (w1,w2, andw3, respectively), adds them to a bias termb, and thenpasses the resulting sum through a sigmoid function to result in a number between 0and 1.\nx1x2x3\ny\nw1w2w3\u2211b\u03c3+1za\nFigure 7.2A neural unit, taking 3 inputsx1,x2, andx3(and a biasbthat we represent as aweight for an input clamped at +1) and producing an output y. We include some convenientintermediate variables: the output of the summation,z, and the output of the sigmoid,a. Inthis case the output of the unityis the same asa, but in deeper networks we\u2019ll reserveytomean the \ufb01nal output of the entire network, leavingaas the activation of an individual node.Let\u2019s walk through an example just to get an intuition. Let\u2019s suppose we have aunit with the following weight vector and bias:w=[0.2,0.3,0.9]b=0.5What would this unit do with the following input vector:x=[0.5,0.6,0.1]The resulting outputywould be:y=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)=11+e\u0000(.5\u21e4.2+.6\u21e4.3+.1\u21e4.9+.5)=11+e\u00000.87=.70In practice, the sigmoid is not commonly used as an activation function. A functionthat is very similar but almost always better is thetanhfunction shown in Fig.7.3a;tanhtanh is a variant of the sigmoid that ranges from -1 to +1:y=ez\u0000e\u0000zez+e\u0000z(7.5)The simplest activation function, and perhaps the most commonly used, is the rec-ti\ufb01ed linear unit, also called theReLU, shown in Fig.7.3b. It\u2019s just the same aszReLUwhenzis positive, and 0 otherwise:y=max(z,0)(7.6)Most Common:",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 23,
      "token_count": 550,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\nSimple Neural Networks and Neural Language ModelsUnits in Neural Networks\n\n## Page 14\n\nSimple Neural Networks and Neural Language ModelsThe XOR problem\n\n## Page 15\n\nThe XOR problemCan neural units compute simple functions of input?Minsky and Papert(1969)4CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELS\n(a) (b)Figure 7.3The tanh and ReLU activation functions.These activation functions have different properties that make them useful fordifferent language applications or network architectures. For example, the tanh func-tion has the nice properties of being smoothly differentiable and mapping outliervalues toward the mean. The recti\ufb01er function, on the other hand has nice propertiesthat result from it being very close to linear. In the sigmoid or tanh functions, veryhigh values ofzresult in values ofythat aresaturated, i.e., extremely close to 1,saturatedand have derivatives very close to 0. Zero derivatives cause problems for learning,because as we\u2019ll see in Section7.4, we\u2019ll train networks by propagating an errorsignal backwards, multiplying gradients (partial derivatives) from each layer of thenetwork; gradients that are almost 0 cause the error signal to get smaller and smalleruntil it is too small to be used for training, a problem called thevanishing gradientvanishinggradientproblem. Recti\ufb01ers don\u2019t have this problem, since the derivative of ReLU for highvalues ofzis 1 rather than very close to 0.7.2 The XOR problemEarly in the history of neural networks it was realized that the power of neural net-works, as with the real neurons that inspired them, comes from combining theseunits into larger networks.One of the most clever demonstrations of the need for multi-layer networks wasthe proof byMinsky and Papert (1969)that a single neural unit cannot computesome very simple functions of its input. Consider the task of computing elementarylogical functions of two inputs, like AND, OR, and XOR. As a reminder, here arethe truth tables for those functions:AND OR XORx1 x2y x1 x2y x1 x2y0000 000 000100 110 111001 011 011111 111 10This example was \ufb01rst shown for theperceptron, which is a very simple neuralperceptronunit that has a binary output and doesnothave a non-linear activation function. The",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 24,
      "token_count": 521,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\nPerceptronsA very simple neural unit \u2022Binary output  (0 or 1)\u2022No non-linear activation function7.2\u2022THEXORPROBLEM5outputyof a perceptron is 0 or 1, and is computed as follows (using the same weightw, inputx, and biasbas in Eq.7.2):y=\u21e20,ifw\u00b7x+b\uf8ff01,ifw\u00b7x+b>0(7.7)It\u2019s very easy to build a perceptron that can compute the logical AND and ORfunctions of its binary inputs; Fig.7.4shows the necessary weights.x1x2+1-111x1x2+1011(a) (b)Figure 7.4The weightswand biasbfor perceptrons for computing logical functions. Theinputs are shown asx1andx2and the bias as a special node with value+1 which is multipliedwith the bias weightb. (a) logical AND, showing weightsw1=1 andw2=1 and bias weightb=\u00001. (b) logical OR, showing weightsw1=1 andw2=1 and bias weightb=0. Theseweights/biases are just one from an in\ufb01nite number of possible sets of weights and biases thatwould implement the functions.It turns out, however, that it\u2019s not possible to build a perceptron to computelogical XOR! (It\u2019s worth spending a moment to give it a try!)The intuition behind this important result relies on understanding that a percep-tron is a linear classi\ufb01er. For a two-dimensional inputx1andx2, the perceptionequation,w1x1+w2x2+b=0 is the equation of a line. (We can see this by puttingit in the standard linear format:x2=(\u0000w1/w2)x1+(\u0000b/w2).) This line acts as adecision boundaryin two-dimensional space in which the output 0 is assigned to alldecisionboundaryinputs lying on one side of the line, and the output 1 to all input points lying on theother side of the line. If we had more than 2 inputs, the decision boundary becomesa hyperplane instead of a line, but the idea is the same, separating the space into twocategories.Fig.7.5shows the possible logical inputs (00,01,10, and11) and the line drawnby one possible set of parameters for an AND and an OR classi\ufb01er. Notice that thereis simply no way to draw a line that separates the positive cases of XOR (01 and 10)from the negative cases (00 and 11). We say that XOR is not alinearly separablelinearlyseparablefunction. Of course we could draw a boundary with a curve, or some other function,but not a single line.7.2.1 The solution: neural networksWhile the XOR function cannot be calculated by a single perceptron, it can be cal-culated by a layered network of units. Let\u2019s see an example of how to do this fromGoodfellow et al. (2016)that computes XOR using two layers of ReLU-based units.Fig.7.6shows a \ufb01gure with the input being processed by two layers of neural units.The middle layer (calledh) has two units, and the output layer (calledy) has oneunit. A set of weights and biases are shown for each ReLU that correctly computesthe XOR function.Let\u2019s walk through what happens with the input x = [0 0]. If we multiply eachinput value by the appropriate weight, sum, and then add the biasb, we get the",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 25,
      "token_count": 782,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 26,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Easy to build AND or OR with perceptrons7.2\u2022THEXORPROBLEM5outputyof a perceptron is 0 or 1, and is computed as follows (using the same weightw, inputx, and biasbas in Eq.7.2):y=\u21e20,ifw\u00b7x+b\uf8ff01,ifw\u00b7x+b>0(7.7)It\u2019s very easy to build a perceptron that can compute the logical AND and ORfunctions of its binary inputs; Fig.7.4shows the necessary weights.x1x2+1-111x1x2+1011(a) (b)Figure 7.4The weightswand biasbfor perceptrons for computing logical functions. Theinputs are shown asx1andx2and the bias as a special node with value+1 which is multipliedwith the bias weightb. (a) logical AND, showing weightsw1=1 andw2=1 and bias weightb=\u00001. (b) logical OR, showing weightsw1=1 andw2=1 and bias weightb=0. Theseweights/biases are just one from an in\ufb01nite number of possible sets of weights and biases thatwould implement the functions.It turns out, however, that it\u2019s not possible to build a perceptron to computelogical XOR! (It\u2019s worth spending a moment to give it a try!)The intuition behind this important result relies on understanding that a percep-tron is a linear classi\ufb01er. For a two-dimensional inputx1andx2, the perceptionequation,w1x1+w2x2+b=0 is the equation of a line. (We can see this by puttingit in the standard linear format:x2=(\u0000w1/w2)x1+(\u0000b/w2).) This line acts as adecision boundaryin two-dimensional space in which the output 0 is assigned to alldecisionboundaryinputs lying on one side of the line, and the output 1 to all input points lying on theother side of the line. If we had more than 2 inputs, the decision boundary becomesa hyperplane instead of a line, but the idea is the same, separating the space into twocategories.Fig.7.5shows the possible logical inputs (00,01,10, and11) and the line drawnby one possible set of parameters for an AND and an OR classi\ufb01er. Notice that thereis simply no way to draw a line that separates the positive cases of XOR (01 and 10)from the negative cases (00 and 11). We say that XOR is not alinearly separablelinearlyseparablefunction. Of course we could draw a boundary with a curve, or some other function,but not a single line.7.2.1 The solution: neural networksWhile the XOR function cannot be calculated by a single perceptron, it can be cal-culated by a layered network of units. Let\u2019s see an example of how to do this fromGoodfellow et al. (2016)that computes XOR using two layers of ReLU-based units.Fig.7.6shows a \ufb01gure with the input being processed by two layers of neural units.The middle layer (calledh) has two units, and the output layer (calledy) has oneunit. A set of weights and biases are shown for each ReLU that correctly computesthe XOR function.Let\u2019s walk through what happens with the input x = [0 0]. If we multiply eachinput value by the appropriate weight, sum, and then add the biasb, we get the7.2\u2022THEXORPROBLEM5outputyof a perceptron is 0 or 1, and is computed as follows (using the same weightw,",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 27,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "line drawnby one possible set of parameters for an AND and an OR classi\ufb01er. Notice that thereis simply no way to draw a line that separates the positive cases of XOR (01 and 10)from the negative cases (00 and 11). We say that XOR is not alinearly separablelinearlyseparablefunction. Of course we could draw a boundary with a curve, or some other function,but not a single line.7.2.1 The solution: neural networksWhile the XOR function cannot be calculated by a single perceptron, it can be cal-culated by a layered network of units. Let\u2019s see an example of how to do this fromGoodfellow et al. (2016)that computes XOR using two layers of ReLU-based units.Fig.7.6shows a \ufb01gure with the input being processed by two layers of neural units.The middle layer (calledh) has two units, and the output layer (calledy) has oneunit. A set of weights and biases are shown for each ReLU that correctly computesthe XOR function.Let\u2019s walk through what happens with the input x = [0 0]. If we multiply eachinput value by the appropriate weight, sum, and then add the biasb, we get the7.2\u2022THEXORPROBLEM5outputyof a perceptron is 0 or 1, and is computed as follows (using the same weightw, inputx, and biasbas in Eq.7.2):y=\u21e20,ifw\u00b7x+b\uf8ff01,ifw\u00b7x+b>0(7.7)It\u2019s very easy to build a perceptron that can compute the logical AND and ORfunctions of its binary inputs; Fig.7.4shows the necessary weights.x1x2+1-111x1x2+1011(a) (b)Figure 7.4The weightswand biasbfor perceptrons for computing logical functions. Theinputs are shown asx1andx2and the bias as a special node with value+1 which is multipliedwith the bias weightb. (a) logical AND, showing weightsw1=1 andw2=1 and bias weightb=\u00001. (b) logical OR, showing weightsw1=1 andw2=1 and bias weightb=0. Theseweights/biases are just one from an in\ufb01nite number of possible sets of weights and biases thatwould implement the functions.It turns out, however, that it\u2019s not possible to build a perceptron to computelogical XOR! (It\u2019s worth spending a moment to give it a try!)The intuition behind this important result relies on understanding that a percep-tron is a linear classi\ufb01er. For a two-dimensional inputx1andx2, the perceptionequation,w1x1+w2x2+b=0 is the equation of a line. (We can see this by puttingit in the standard linear format:x2=(\u0000w1/w2)x1+(\u0000b/w2).) This line acts as adecision boundaryin two-dimensional space in which the output 0 is assigned to alldecisionboundaryinputs lying on one side of the line, and the output 1 to all input points lying on theother side of the line. If we had more than 2 inputs, the decision boundary becomesa hyperplane instead of a line, but the idea is the same, separating the space into twocategories.Fig.7.5shows the possible logical inputs (00,01,10, and11) and the line drawnby one possible set of parameters for an AND and an OR classi\ufb01er. Notice that thereis simply no way to draw a line that separates the positive cases of XOR (01 and 10)from the negative cases",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 28,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "are just one from an in\ufb01nite number of possible sets of weights and biases thatwould implement the functions.It turns out, however, that it\u2019s not possible to build a perceptron to computelogical XOR! (It\u2019s worth spending a moment to give it a try!)The intuition behind this important result relies on understanding that a percep-tron is a linear classi\ufb01er. For a two-dimensional inputx1andx2, the perceptionequation,w1x1+w2x2+b=0 is the equation of a line. (We can see this by puttingit in the standard linear format:x2=(\u0000w1/w2)x1+(\u0000b/w2).) This line acts as adecision boundaryin two-dimensional space in which the output 0 is assigned to alldecisionboundaryinputs lying on one side of the line, and the output 1 to all input points lying on theother side of the line. If we had more than 2 inputs, the decision boundary becomesa hyperplane instead of a line, but the idea is the same, separating the space into twocategories.Fig.7.5shows the possible logical inputs (00,01,10, and11) and the line drawnby one possible set of parameters for an AND and an OR classi\ufb01er. Notice that thereis simply no way to draw a line that separates the positive cases of XOR (01 and 10)from the negative cases (00 and 11). We say that XOR is not alinearly separablelinearlyseparablefunction. Of course we could draw a boundary with a curve, or some other function,but not a single line.7.2.1 The solution: neural networksWhile the XOR function cannot be calculated by a single perceptron, it can be cal-culated by a layered network of units. Let\u2019s see an example of how to do this fromGoodfellow et al. (2016)that computes XOR using two layers of ReLU-based units.Fig.7.6shows a \ufb01gure with the input being processed by two layers of neural units.The middle layer (calledh) has two units, and the output layer (calledy) has oneunit. A set of weights and biases are shown for each ReLU that correctly computesthe XOR function.Let\u2019s walk through what happens with the input x = [0 0]. If we multiply eachinput value by the appropriate weight, sum, and then add the biasb, we get theANDOR4CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELS",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 29,
      "token_count": 537,
      "chapter_title": ""
    }
  },
  {
    "content": "(a) (b)Figure 7.3The tanh and ReLU activation functions.These activation functions have different properties that make them useful fordifferent language applications or network architectures. For example, the tanh func-tion has the nice properties of being smoothly differentiable and mapping outliervalues toward the mean. The recti\ufb01er function, on the other hand has nice propertiesthat result from it being very close to linear. In the sigmoid or tanh functions, veryhigh values ofzresult in values ofythat aresaturated, i.e., extremely close to 1,saturatedand have derivatives very close to 0. Zero derivatives cause problems for learning,because as we\u2019ll see in Section7.4, we\u2019ll train networks by propagating an errorsignal backwards, multiplying gradients (partial derivatives) from each layer of thenetwork; gradients that are almost 0 cause the error signal to get smaller and smalleruntil it is too small to be used for training, a problem called thevanishing gradientvanishinggradientproblem. Recti\ufb01ers don\u2019t have this problem, since the derivative of ReLU for highvalues ofzis 1 rather than very close to 0.7.2 The XOR problemEarly in the history of neural networks it was realized that the power of neural net-works, as with the real neurons that inspired them, comes from combining theseunits into larger networks.One of the most clever demonstrations of the need for multi-layer networks wasthe proof byMinsky and Papert (1969)that a single neural unit cannot computesome very simple functions of its input. Consider the task of computing elementarylogical functions of two inputs, like AND, OR, and XOR. As a reminder, here arethe truth tables for those functions:AND OR XORx1 x2y x1 x2y x1 x2y0000 000 000100 110 111001 011 011111 111 10This example was \ufb01rst shown for theperceptron, which is a very simple neuralperceptronunit that has a binary output and doesnothave a non-linear activation function. The4CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELS",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 30,
      "token_count": 461,
      "chapter_title": ""
    }
  },
  {
    "content": "(a) (b)Figure 7.3The tanh and ReLU activation functions.These activation functions have different properties that make them useful fordifferent language applications or network architectures. For example, the tanh func-tion has the nice properties of being smoothly differentiable and mapping outliervalues toward the mean. The recti\ufb01er function, on the other hand has nice propertiesthat result from it being very close to linear. In the sigmoid or tanh functions, veryhigh values ofzresult in values ofythat aresaturated, i.e., extremely close to 1,saturatedand have derivatives very close to 0. Zero derivatives cause problems for learning,because as we\u2019ll see in Section7.4, we\u2019ll train networks by propagating an errorsignal backwards, multiplying gradients (partial derivatives) from each layer of thenetwork; gradients that are almost 0 cause the error signal to get smaller and smalleruntil it is too small to be used for training, a problem called thevanishing gradientvanishinggradientproblem. Recti\ufb01ers don\u2019t have this problem, since the derivative of ReLU for highvalues ofzis 1 rather than very close to 0.7.2 The XOR problemEarly in the history of neural networks it was realized that the power of neural net-works, as with the real neurons that inspired them, comes from combining theseunits into larger networks.One of the most clever demonstrations of the need for multi-layer networks wasthe proof byMinsky and Papert (1969)that a single neural unit cannot computesome very simple functions of its input. Consider the task of computing elementarylogical functions of two inputs, like AND, OR, and XOR. As a reminder, here arethe truth tables for those functions:AND OR XORx1 x2y x1 x2y x1 x2y0000 000 000100 110 111001 011 011111 111 10This example was \ufb01rst shown for theperceptron, which is a very simple neuralperceptronunit that has a binary output and doesnothave a non-linear activation function. The7.2\u2022THEXORPROBLEM5outputyof a perceptron is 0 or 1, and is computed as follows (using the same weightw, inputx, and biasbas in Eq.7.2):y=\u21e20,ifw\u00b7x+b\uf8ff01,ifw\u00b7x+b>0(7.7)It\u2019s very easy to build a perceptron that can compute the logical AND and ORfunctions of its binary inputs; Fig.7.4shows the necessary weights.x1x2+1-111x1x2+1011(a) (b)Figure 7.4The weightswand biasbfor perceptrons for computing logical functions. Theinputs are shown asx1andx2and the bias as a special node with value+1 which is multipliedwith the bias weightb. (a) logical AND, showing weightsw1=1 andw2=1 and bias weightb=\u00001. (b) logical OR, showing weightsw1=1 andw2=1 and bias weightb=0. Theseweights/biases are just one from an in\ufb01nite number of possible sets of weights and biases thatwould implement the functions.It turns out, however, that it\u2019s not possible to build a perceptron to computelogical XOR! (It\u2019s worth spending a moment to give it a try!)The intuition behind this important result relies on understanding that a percep-tron is a linear classi\ufb01er. For a two-dimensional inputx1andx2, the perceptionequation,w1x1+w2x2+b=0 is the equation of a line. (We can see this by",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 31,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "very easy to build a perceptron that can compute the logical AND and ORfunctions of its binary inputs; Fig.7.4shows the necessary weights.x1x2+1-111x1x2+1011(a) (b)Figure 7.4The weightswand biasbfor perceptrons for computing logical functions. Theinputs are shown asx1andx2and the bias as a special node with value+1 which is multipliedwith the bias weightb. (a) logical AND, showing weightsw1=1 andw2=1 and bias weightb=\u00001. (b) logical OR, showing weightsw1=1 andw2=1 and bias weightb=0. Theseweights/biases are just one from an in\ufb01nite number of possible sets of weights and biases thatwould implement the functions.It turns out, however, that it\u2019s not possible to build a perceptron to computelogical XOR! (It\u2019s worth spending a moment to give it a try!)The intuition behind this important result relies on understanding that a percep-tron is a linear classi\ufb01er. For a two-dimensional inputx1andx2, the perceptionequation,w1x1+w2x2+b=0 is the equation of a line. (We can see this by puttingit in the standard linear format:x2=(\u0000w1/w2)x1+(\u0000b/w2).) This line acts as adecision boundaryin two-dimensional space in which the output 0 is assigned to alldecisionboundaryinputs lying on one side of the line, and the output 1 to all input points lying on theother side of the line. If we had more than 2 inputs, the decision boundary becomesa hyperplane instead of a line, but the idea is the same, separating the space into twocategories.Fig.7.5shows the possible logical inputs (00,01,10, and11) and the line drawnby one possible set of parameters for an AND and an OR classi\ufb01er. Notice that thereis simply no way to draw a line that separates the positive cases of XOR (01 and 10)from the negative cases (00 and 11). We say that XOR is not alinearly separablelinearlyseparablefunction. Of course we could draw a boundary with a curve, or some other function,but not a single line.7.2.1 The solution: neural networksWhile the XOR function cannot be calculated by a single perceptron, it can be cal-culated by a layered network of units. Let\u2019s see an example of how to do this fromGoodfellow et al. (2016)that computes XOR using two layers of ReLU-based units.Fig.7.6shows a \ufb01gure with the input being processed by two layers of neural units.The middle layer (calledh) has two units, and the output layer (calledy) has oneunit. A set of weights and biases are shown for each ReLU that correctly computesthe XOR function.Let\u2019s walk through what happens with the input x = [0 0]. If we multiply eachinput value by the appropriate weight, sum, and then add the biasb, we get the",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 32,
      "token_count": 673,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 33,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Easy to build AND or OR with perceptrons7.2\u2022THEXORPROBLEM5outputyof a perceptron is 0 or 1, and is computed as follows (using the same weightw, inputx, and biasbas in Eq.7.2):y=\u21e20,ifw\u00b7x+b\uf8ff01,ifw\u00b7x+b>0(7.7)It\u2019s very easy to build a perceptron that can compute the logical AND and ORfunctions of its binary inputs; Fig.7.4shows the necessary weights.x1x2+1-111x1x2+1011(a) (b)Figure 7.4The weightswand biasbfor perceptrons for computing logical functions. Theinputs are shown asx1andx2and the bias as a special node with value+1 which is multipliedwith the bias weightb. (a) logical AND, showing weightsw1=1 andw2=1 and bias weightb=\u00001. (b) logical OR, showing weightsw1=1 andw2=1 and bias weightb=0. Theseweights/biases are just one from an in\ufb01nite number of possible sets of weights and biases thatwould implement the functions.It turns out, however, that it\u2019s not possible to build a perceptron to computelogical XOR! (It\u2019s worth spending a moment to give it a try!)The intuition behind this important result relies on understanding that a percep-tron is a linear classi\ufb01er. For a two-dimensional inputx1andx2, the perceptionequation,w1x1+w2x2+b=0 is the equation of a line. (We can see this by puttingit in the standard linear format:x2=(\u0000w1/w2)x1+(\u0000b/w2).) This line acts as adecision boundaryin two-dimensional space in which the output 0 is assigned to alldecisionboundaryinputs lying on one side of the line, and the output 1 to all input points lying on theother side of the line. If we had more than 2 inputs, the decision boundary becomesa hyperplane instead of a line, but the idea is the same, separating the space into twocategories.Fig.7.5shows the possible logical inputs (00,01,10, and11) and the line drawnby one possible set of parameters for an AND and an OR classi\ufb01er. Notice that thereis simply no way to draw a line that separates the positive cases of XOR (01 and 10)from the negative cases (00 and 11). We say that XOR is not alinearly separablelinearlyseparablefunction. Of course we could draw a boundary with a curve, or some other function,but not a single line.7.2.1 The solution: neural networksWhile the XOR function cannot be calculated by a single perceptron, it can be cal-culated by a layered network of units. Let\u2019s see an example of how to do this fromGoodfellow et al. (2016)that computes XOR using two layers of ReLU-based units.Fig.7.6shows a \ufb01gure with the input being processed by two layers of neural units.The middle layer (calledh) has two units, and the output layer (calledy) has oneunit. A set of weights and biases are shown for each ReLU that correctly computesthe XOR function.Let\u2019s walk through what happens with the input x = [0 0]. If we multiply eachinput value by the appropriate weight, sum, and then add the biasb, we get the7.2\u2022THEXORPROBLEM5outputyof a perceptron is 0 or 1, and is computed as follows (using the same weightw,",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 34,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "line drawnby one possible set of parameters for an AND and an OR classi\ufb01er. Notice that thereis simply no way to draw a line that separates the positive cases of XOR (01 and 10)from the negative cases (00 and 11). We say that XOR is not alinearly separablelinearlyseparablefunction. Of course we could draw a boundary with a curve, or some other function,but not a single line.7.2.1 The solution: neural networksWhile the XOR function cannot be calculated by a single perceptron, it can be cal-culated by a layered network of units. Let\u2019s see an example of how to do this fromGoodfellow et al. (2016)that computes XOR using two layers of ReLU-based units.Fig.7.6shows a \ufb01gure with the input being processed by two layers of neural units.The middle layer (calledh) has two units, and the output layer (calledy) has oneunit. A set of weights and biases are shown for each ReLU that correctly computesthe XOR function.Let\u2019s walk through what happens with the input x = [0 0]. If we multiply eachinput value by the appropriate weight, sum, and then add the biasb, we get the7.2\u2022THEXORPROBLEM5outputyof a perceptron is 0 or 1, and is computed as follows (using the same weightw, inputx, and biasbas in Eq.7.2):y=\u21e20,ifw\u00b7x+b\uf8ff01,ifw\u00b7x+b>0(7.7)It\u2019s very easy to build a perceptron that can compute the logical AND and ORfunctions of its binary inputs; Fig.7.4shows the necessary weights.x1x2+1-111x1x2+1011(a) (b)Figure 7.4The weightswand biasbfor perceptrons for computing logical functions. Theinputs are shown asx1andx2and the bias as a special node with value+1 which is multipliedwith the bias weightb. (a) logical AND, showing weightsw1=1 andw2=1 and bias weightb=\u00001. (b) logical OR, showing weightsw1=1 andw2=1 and bias weightb=0. Theseweights/biases are just one from an in\ufb01nite number of possible sets of weights and biases thatwould implement the functions.It turns out, however, that it\u2019s not possible to build a perceptron to computelogical XOR! (It\u2019s worth spending a moment to give it a try!)The intuition behind this important result relies on understanding that a percep-tron is a linear classi\ufb01er. For a two-dimensional inputx1andx2, the perceptionequation,w1x1+w2x2+b=0 is the equation of a line. (We can see this by puttingit in the standard linear format:x2=(\u0000w1/w2)x1+(\u0000b/w2).) This line acts as adecision boundaryin two-dimensional space in which the output 0 is assigned to alldecisionboundaryinputs lying on one side of the line, and the output 1 to all input points lying on theother side of the line. If we had more than 2 inputs, the decision boundary becomesa hyperplane instead of a line, but the idea is the same, separating the space into twocategories.Fig.7.5shows the possible logical inputs (00,01,10, and11) and the line drawnby one possible set of parameters for an AND and an OR classi\ufb01er. Notice that thereis simply no way to draw a line that separates the positive cases of XOR (01 and 10)from the negative cases",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 35,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "are just one from an in\ufb01nite number of possible sets of weights and biases thatwould implement the functions.It turns out, however, that it\u2019s not possible to build a perceptron to computelogical XOR! (It\u2019s worth spending a moment to give it a try!)The intuition behind this important result relies on understanding that a percep-tron is a linear classi\ufb01er. For a two-dimensional inputx1andx2, the perceptionequation,w1x1+w2x2+b=0 is the equation of a line. (We can see this by puttingit in the standard linear format:x2=(\u0000w1/w2)x1+(\u0000b/w2).) This line acts as adecision boundaryin two-dimensional space in which the output 0 is assigned to alldecisionboundaryinputs lying on one side of the line, and the output 1 to all input points lying on theother side of the line. If we had more than 2 inputs, the decision boundary becomesa hyperplane instead of a line, but the idea is the same, separating the space into twocategories.Fig.7.5shows the possible logical inputs (00,01,10, and11) and the line drawnby one possible set of parameters for an AND and an OR classi\ufb01er. Notice that thereis simply no way to draw a line that separates the positive cases of XOR (01 and 10)from the negative cases (00 and 11). We say that XOR is not alinearly separablelinearlyseparablefunction. Of course we could draw a boundary with a curve, or some other function,but not a single line.7.2.1 The solution: neural networksWhile the XOR function cannot be calculated by a single perceptron, it can be cal-culated by a layered network of units. Let\u2019s see an example of how to do this fromGoodfellow et al. (2016)that computes XOR using two layers of ReLU-based units.Fig.7.6shows a \ufb01gure with the input being processed by two layers of neural units.The middle layer (calledh) has two units, and the output layer (calledy) has oneunit. A set of weights and biases are shown for each ReLU that correctly computesthe XOR function.Let\u2019s walk through what happens with the input x = [0 0]. If we multiply eachinput value by the appropriate weight, sum, and then add the biasb, we get theANDOR4CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELS",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 36,
      "token_count": 537,
      "chapter_title": ""
    }
  },
  {
    "content": "(a) (b)Figure 7.3The tanh and ReLU activation functions.These activation functions have different properties that make them useful fordifferent language applications or network architectures. For example, the tanh func-tion has the nice properties of being smoothly differentiable and mapping outliervalues toward the mean. The recti\ufb01er function, on the other hand has nice propertiesthat result from it being very close to linear. In the sigmoid or tanh functions, veryhigh values ofzresult in values ofythat aresaturated, i.e., extremely close to 1,saturatedand have derivatives very close to 0. Zero derivatives cause problems for learning,because as we\u2019ll see in Section7.4, we\u2019ll train networks by propagating an errorsignal backwards, multiplying gradients (partial derivatives) from each layer of thenetwork; gradients that are almost 0 cause the error signal to get smaller and smalleruntil it is too small to be used for training, a problem called thevanishing gradientvanishinggradientproblem. Recti\ufb01ers don\u2019t have this problem, since the derivative of ReLU for highvalues ofzis 1 rather than very close to 0.7.2 The XOR problemEarly in the history of neural networks it was realized that the power of neural net-works, as with the real neurons that inspired them, comes from combining theseunits into larger networks.One of the most clever demonstrations of the need for multi-layer networks wasthe proof byMinsky and Papert (1969)that a single neural unit cannot computesome very simple functions of its input. Consider the task of computing elementarylogical functions of two inputs, like AND, OR, and XOR. As a reminder, here arethe truth tables for those functions:AND OR XORx1 x2y x1 x2y x1 x2y0000 000 000100 110 111001 011 011111 111 10This example was \ufb01rst shown for theperceptron, which is a very simple neuralperceptronunit that has a binary output and doesnothave a non-linear activation function. The4CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELS",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 37,
      "token_count": 461,
      "chapter_title": ""
    }
  },
  {
    "content": "(a) (b)Figure 7.3The tanh and ReLU activation functions.These activation functions have different properties that make them useful fordifferent language applications or network architectures. For example, the tanh func-tion has the nice properties of being smoothly differentiable and mapping outliervalues toward the mean. The recti\ufb01er function, on the other hand has nice propertiesthat result from it being very close to linear. In the sigmoid or tanh functions, veryhigh values ofzresult in values ofythat aresaturated, i.e., extremely close to 1,saturatedand have derivatives very close to 0. Zero derivatives cause problems for learning,because as we\u2019ll see in Section7.4, we\u2019ll train networks by propagating an errorsignal backwards, multiplying gradients (partial derivatives) from each layer of thenetwork; gradients that are almost 0 cause the error signal to get smaller and smalleruntil it is too small to be used for training, a problem called thevanishing gradientvanishinggradientproblem. Recti\ufb01ers don\u2019t have this problem, since the derivative of ReLU for highvalues ofzis 1 rather than very close to 0.7.2 The XOR problemEarly in the history of neural networks it was realized that the power of neural net-works, as with the real neurons that inspired them, comes from combining theseunits into larger networks.One of the most clever demonstrations of the need for multi-layer networks wasthe proof byMinsky and Papert (1969)that a single neural unit cannot computesome very simple functions of its input. Consider the task of computing elementarylogical functions of two inputs, like AND, OR, and XOR. As a reminder, here arethe truth tables for those functions:AND OR XORx1 x2y x1 x2y x1 x2y0000 000 000100 110 111001 011 011111 111 10This example was \ufb01rst shown for theperceptron, which is a very simple neuralperceptronunit that has a binary output and doesnothave a non-linear activation function. The7.2\u2022THEXORPROBLEM5outputyof a perceptron is 0 or 1, and is computed as follows (using the same weightw, inputx, and biasbas in Eq.7.2):y=\u21e20,ifw\u00b7x+b\uf8ff01,ifw\u00b7x+b>0(7.7)It\u2019s very easy to build a perceptron that can compute the logical AND and ORfunctions of its binary inputs; Fig.7.4shows the necessary weights.x1x2+1-111x1x2+1011(a) (b)Figure 7.4The weightswand biasbfor perceptrons for computing logical functions. Theinputs are shown asx1andx2and the bias as a special node with value+1 which is multipliedwith the bias weightb. (a) logical AND, showing weightsw1=1 andw2=1 and bias weightb=\u00001. (b) logical OR, showing weightsw1=1 andw2=1 and bias weightb=0. Theseweights/biases are just one from an in\ufb01nite number of possible sets of weights and biases thatwould implement the functions.It turns out, however, that it\u2019s not possible to build a perceptron to computelogical XOR! (It\u2019s worth spending a moment to give it a try!)The intuition behind this important result relies on understanding that a percep-tron is a linear classi\ufb01er. For a two-dimensional inputx1andx2, the perceptionequation,w1x1+w2x2+b=0 is the equation of a line. (We can see this by",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 38,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "very easy to build a perceptron that can compute the logical AND and ORfunctions of its binary inputs; Fig.7.4shows the necessary weights.x1x2+1-111x1x2+1011(a) (b)Figure 7.4The weightswand biasbfor perceptrons for computing logical functions. Theinputs are shown asx1andx2and the bias as a special node with value+1 which is multipliedwith the bias weightb. (a) logical AND, showing weightsw1=1 andw2=1 and bias weightb=\u00001. (b) logical OR, showing weightsw1=1 andw2=1 and bias weightb=0. Theseweights/biases are just one from an in\ufb01nite number of possible sets of weights and biases thatwould implement the functions.It turns out, however, that it\u2019s not possible to build a perceptron to computelogical XOR! (It\u2019s worth spending a moment to give it a try!)The intuition behind this important result relies on understanding that a percep-tron is a linear classi\ufb01er. For a two-dimensional inputx1andx2, the perceptionequation,w1x1+w2x2+b=0 is the equation of a line. (We can see this by puttingit in the standard linear format:x2=(\u0000w1/w2)x1+(\u0000b/w2).) This line acts as adecision boundaryin two-dimensional space in which the output 0 is assigned to alldecisionboundaryinputs lying on one side of the line, and the output 1 to all input points lying on theother side of the line. If we had more than 2 inputs, the decision boundary becomesa hyperplane instead of a line, but the idea is the same, separating the space into twocategories.Fig.7.5shows the possible logical inputs (00,01,10, and11) and the line drawnby one possible set of parameters for an AND and an OR classi\ufb01er. Notice that thereis simply no way to draw a line that separates the positive cases of XOR (01 and 10)from the negative cases (00 and 11). We say that XOR is not alinearly separablelinearlyseparablefunction. Of course we could draw a boundary with a curve, or some other function,but not a single line.7.2.1 The solution: neural networksWhile the XOR function cannot be calculated by a single perceptron, it can be cal-culated by a layered network of units. Let\u2019s see an example of how to do this fromGoodfellow et al. (2016)that computes XOR using two layers of ReLU-based units.Fig.7.6shows a \ufb01gure with the input being processed by two layers of neural units.The middle layer (calledh) has two units, and the output layer (calledy) has oneunit. A set of weights and biases are shown for each ReLU that correctly computesthe XOR function.Let\u2019s walk through what happens with the input x = [0 0]. If we multiply eachinput value by the appropriate weight, sum, and then add the biasb, we get the",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 39,
      "token_count": 673,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 40,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Easy to build AND or OR with perceptrons7.2\u2022THEXORPROBLEM5outputyof a perceptron is 0 or 1, and is computed as follows (using the same weightw, inputx, and biasbas in Eq.7.2):y=\u21e20,ifw\u00b7x+b\uf8ff01,ifw\u00b7x+b>0(7.7)It\u2019s very easy to build a perceptron that can compute the logical AND and ORfunctions of its binary inputs; Fig.7.4shows the necessary weights.x1x2+1-111x1x2+1011(a) (b)Figure 7.4The weightswand biasbfor perceptrons for computing logical functions. Theinputs are shown asx1andx2and the bias as a special node with value+1 which is multipliedwith the bias weightb. (a) logical AND, showing weightsw1=1 andw2=1 and bias weightb=\u00001. (b) logical OR, showing weightsw1=1 andw2=1 and bias weightb=0. Theseweights/biases are just one from an in\ufb01nite number of possible sets of weights and biases thatwould implement the functions.It turns out, however, that it\u2019s not possible to build a perceptron to computelogical XOR! (It\u2019s worth spending a moment to give it a try!)The intuition behind this important result relies on understanding that a percep-tron is a linear classi\ufb01er. For a two-dimensional inputx1andx2, the perceptionequation,w1x1+w2x2+b=0 is the equation of a line. (We can see this by puttingit in the standard linear format:x2=(\u0000w1/w2)x1+(\u0000b/w2).) This line acts as adecision boundaryin two-dimensional space in which the output 0 is assigned to alldecisionboundaryinputs lying on one side of the line, and the output 1 to all input points lying on theother side of the line. If we had more than 2 inputs, the decision boundary becomesa hyperplane instead of a line, but the idea is the same, separating the space into twocategories.Fig.7.5shows the possible logical inputs (00,01,10, and11) and the line drawnby one possible set of parameters for an AND and an OR classi\ufb01er. Notice that thereis simply no way to draw a line that separates the positive cases of XOR (01 and 10)from the negative cases (00 and 11). We say that XOR is not alinearly separablelinearlyseparablefunction. Of course we could draw a boundary with a curve, or some other function,but not a single line.7.2.1 The solution: neural networksWhile the XOR function cannot be calculated by a single perceptron, it can be cal-culated by a layered network of units. Let\u2019s see an example of how to do this fromGoodfellow et al. (2016)that computes XOR using two layers of ReLU-based units.Fig.7.6shows a \ufb01gure with the input being processed by two layers of neural units.The middle layer (calledh) has two units, and the output layer (calledy) has oneunit. A set of weights and biases are shown for each ReLU that correctly computesthe XOR function.Let\u2019s walk through what happens with the input x = [0 0]. If we multiply eachinput value by the appropriate weight, sum, and then add the biasb, we get the7.2\u2022THEXORPROBLEM5outputyof a perceptron is 0 or 1, and is computed as follows (using the same weightw,",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 41,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "line drawnby one possible set of parameters for an AND and an OR classi\ufb01er. Notice that thereis simply no way to draw a line that separates the positive cases of XOR (01 and 10)from the negative cases (00 and 11). We say that XOR is not alinearly separablelinearlyseparablefunction. Of course we could draw a boundary with a curve, or some other function,but not a single line.7.2.1 The solution: neural networksWhile the XOR function cannot be calculated by a single perceptron, it can be cal-culated by a layered network of units. Let\u2019s see an example of how to do this fromGoodfellow et al. (2016)that computes XOR using two layers of ReLU-based units.Fig.7.6shows a \ufb01gure with the input being processed by two layers of neural units.The middle layer (calledh) has two units, and the output layer (calledy) has oneunit. A set of weights and biases are shown for each ReLU that correctly computesthe XOR function.Let\u2019s walk through what happens with the input x = [0 0]. If we multiply eachinput value by the appropriate weight, sum, and then add the biasb, we get the7.2\u2022THEXORPROBLEM5outputyof a perceptron is 0 or 1, and is computed as follows (using the same weightw, inputx, and biasbas in Eq.7.2):y=\u21e20,ifw\u00b7x+b\uf8ff01,ifw\u00b7x+b>0(7.7)It\u2019s very easy to build a perceptron that can compute the logical AND and ORfunctions of its binary inputs; Fig.7.4shows the necessary weights.x1x2+1-111x1x2+1011(a) (b)Figure 7.4The weightswand biasbfor perceptrons for computing logical functions. Theinputs are shown asx1andx2and the bias as a special node with value+1 which is multipliedwith the bias weightb. (a) logical AND, showing weightsw1=1 andw2=1 and bias weightb=\u00001. (b) logical OR, showing weightsw1=1 andw2=1 and bias weightb=0. Theseweights/biases are just one from an in\ufb01nite number of possible sets of weights and biases thatwould implement the functions.It turns out, however, that it\u2019s not possible to build a perceptron to computelogical XOR! (It\u2019s worth spending a moment to give it a try!)The intuition behind this important result relies on understanding that a percep-tron is a linear classi\ufb01er. For a two-dimensional inputx1andx2, the perceptionequation,w1x1+w2x2+b=0 is the equation of a line. (We can see this by puttingit in the standard linear format:x2=(\u0000w1/w2)x1+(\u0000b/w2).) This line acts as adecision boundaryin two-dimensional space in which the output 0 is assigned to alldecisionboundaryinputs lying on one side of the line, and the output 1 to all input points lying on theother side of the line. If we had more than 2 inputs, the decision boundary becomesa hyperplane instead of a line, but the idea is the same, separating the space into twocategories.Fig.7.5shows the possible logical inputs (00,01,10, and11) and the line drawnby one possible set of parameters for an AND and an OR classi\ufb01er. Notice that thereis simply no way to draw a line that separates the positive cases of XOR (01 and 10)from the negative cases",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 42,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "are just one from an in\ufb01nite number of possible sets of weights and biases thatwould implement the functions.It turns out, however, that it\u2019s not possible to build a perceptron to computelogical XOR! (It\u2019s worth spending a moment to give it a try!)The intuition behind this important result relies on understanding that a percep-tron is a linear classi\ufb01er. For a two-dimensional inputx1andx2, the perceptionequation,w1x1+w2x2+b=0 is the equation of a line. (We can see this by puttingit in the standard linear format:x2=(\u0000w1/w2)x1+(\u0000b/w2).) This line acts as adecision boundaryin two-dimensional space in which the output 0 is assigned to alldecisionboundaryinputs lying on one side of the line, and the output 1 to all input points lying on theother side of the line. If we had more than 2 inputs, the decision boundary becomesa hyperplane instead of a line, but the idea is the same, separating the space into twocategories.Fig.7.5shows the possible logical inputs (00,01,10, and11) and the line drawnby one possible set of parameters for an AND and an OR classi\ufb01er. Notice that thereis simply no way to draw a line that separates the positive cases of XOR (01 and 10)from the negative cases (00 and 11). We say that XOR is not alinearly separablelinearlyseparablefunction. Of course we could draw a boundary with a curve, or some other function,but not a single line.7.2.1 The solution: neural networksWhile the XOR function cannot be calculated by a single perceptron, it can be cal-culated by a layered network of units. Let\u2019s see an example of how to do this fromGoodfellow et al. (2016)that computes XOR using two layers of ReLU-based units.Fig.7.6shows a \ufb01gure with the input being processed by two layers of neural units.The middle layer (calledh) has two units, and the output layer (calledy) has oneunit. A set of weights and biases are shown for each ReLU that correctly computesthe XOR function.Let\u2019s walk through what happens with the input x = [0 0]. If we multiply eachinput value by the appropriate weight, sum, and then add the biasb, we get theANDOR4CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELS",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 43,
      "token_count": 537,
      "chapter_title": ""
    }
  },
  {
    "content": "(a) (b)Figure 7.3The tanh and ReLU activation functions.These activation functions have different properties that make them useful fordifferent language applications or network architectures. For example, the tanh func-tion has the nice properties of being smoothly differentiable and mapping outliervalues toward the mean. The recti\ufb01er function, on the other hand has nice propertiesthat result from it being very close to linear. In the sigmoid or tanh functions, veryhigh values ofzresult in values ofythat aresaturated, i.e., extremely close to 1,saturatedand have derivatives very close to 0. Zero derivatives cause problems for learning,because as we\u2019ll see in Section7.4, we\u2019ll train networks by propagating an errorsignal backwards, multiplying gradients (partial derivatives) from each layer of thenetwork; gradients that are almost 0 cause the error signal to get smaller and smalleruntil it is too small to be used for training, a problem called thevanishing gradientvanishinggradientproblem. Recti\ufb01ers don\u2019t have this problem, since the derivative of ReLU for highvalues ofzis 1 rather than very close to 0.7.2 The XOR problemEarly in the history of neural networks it was realized that the power of neural net-works, as with the real neurons that inspired them, comes from combining theseunits into larger networks.One of the most clever demonstrations of the need for multi-layer networks wasthe proof byMinsky and Papert (1969)that a single neural unit cannot computesome very simple functions of its input. Consider the task of computing elementarylogical functions of two inputs, like AND, OR, and XOR. As a reminder, here arethe truth tables for those functions:AND OR XORx1 x2y x1 x2y x1 x2y0000 000 000100 110 111001 011 011111 111 10This example was \ufb01rst shown for theperceptron, which is a very simple neuralperceptronunit that has a binary output and doesnothave a non-linear activation function. The4CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELS",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 44,
      "token_count": 461,
      "chapter_title": ""
    }
  },
  {
    "content": "(a) (b)Figure 7.3The tanh and ReLU activation functions.These activation functions have different properties that make them useful fordifferent language applications or network architectures. For example, the tanh func-tion has the nice properties of being smoothly differentiable and mapping outliervalues toward the mean. The recti\ufb01er function, on the other hand has nice propertiesthat result from it being very close to linear. In the sigmoid or tanh functions, veryhigh values ofzresult in values ofythat aresaturated, i.e., extremely close to 1,saturatedand have derivatives very close to 0. Zero derivatives cause problems for learning,because as we\u2019ll see in Section7.4, we\u2019ll train networks by propagating an errorsignal backwards, multiplying gradients (partial derivatives) from each layer of thenetwork; gradients that are almost 0 cause the error signal to get smaller and smalleruntil it is too small to be used for training, a problem called thevanishing gradientvanishinggradientproblem. Recti\ufb01ers don\u2019t have this problem, since the derivative of ReLU for highvalues ofzis 1 rather than very close to 0.7.2 The XOR problemEarly in the history of neural networks it was realized that the power of neural net-works, as with the real neurons that inspired them, comes from combining theseunits into larger networks.One of the most clever demonstrations of the need for multi-layer networks wasthe proof byMinsky and Papert (1969)that a single neural unit cannot computesome very simple functions of its input. Consider the task of computing elementarylogical functions of two inputs, like AND, OR, and XOR. As a reminder, here arethe truth tables for those functions:AND OR XORx1 x2y x1 x2y x1 x2y0000 000 000100 110 111001 011 011111 111 10This example was \ufb01rst shown for theperceptron, which is a very simple neuralperceptronunit that has a binary output and doesnothave a non-linear activation function. The7.2\u2022THEXORPROBLEM5outputyof a perceptron is 0 or 1, and is computed as follows (using the same weightw, inputx, and biasbas in Eq.7.2):y=\u21e20,ifw\u00b7x+b\uf8ff01,ifw\u00b7x+b>0(7.7)It\u2019s very easy to build a perceptron that can compute the logical AND and ORfunctions of its binary inputs; Fig.7.4shows the necessary weights.x1x2+1-111x1x2+1011(a) (b)Figure 7.4The weightswand biasbfor perceptrons for computing logical functions. Theinputs are shown asx1andx2and the bias as a special node with value+1 which is multipliedwith the bias weightb. (a) logical AND, showing weightsw1=1 andw2=1 and bias weightb=\u00001. (b) logical OR, showing weightsw1=1 andw2=1 and bias weightb=0. Theseweights/biases are just one from an in\ufb01nite number of possible sets of weights and biases thatwould implement the functions.It turns out, however, that it\u2019s not possible to build a perceptron to computelogical XOR! (It\u2019s worth spending a moment to give it a try!)The intuition behind this important result relies on understanding that a percep-tron is a linear classi\ufb01er. For a two-dimensional inputx1andx2, the perceptionequation,w1x1+w2x2+b=0 is the equation of a line. (We can see this by",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 45,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "very easy to build a perceptron that can compute the logical AND and ORfunctions of its binary inputs; Fig.7.4shows the necessary weights.x1x2+1-111x1x2+1011(a) (b)Figure 7.4The weightswand biasbfor perceptrons for computing logical functions. Theinputs are shown asx1andx2and the bias as a special node with value+1 which is multipliedwith the bias weightb. (a) logical AND, showing weightsw1=1 andw2=1 and bias weightb=\u00001. (b) logical OR, showing weightsw1=1 andw2=1 and bias weightb=0. Theseweights/biases are just one from an in\ufb01nite number of possible sets of weights and biases thatwould implement the functions.It turns out, however, that it\u2019s not possible to build a perceptron to computelogical XOR! (It\u2019s worth spending a moment to give it a try!)The intuition behind this important result relies on understanding that a percep-tron is a linear classi\ufb01er. For a two-dimensional inputx1andx2, the perceptionequation,w1x1+w2x2+b=0 is the equation of a line. (We can see this by puttingit in the standard linear format:x2=(\u0000w1/w2)x1+(\u0000b/w2).) This line acts as adecision boundaryin two-dimensional space in which the output 0 is assigned to alldecisionboundaryinputs lying on one side of the line, and the output 1 to all input points lying on theother side of the line. If we had more than 2 inputs, the decision boundary becomesa hyperplane instead of a line, but the idea is the same, separating the space into twocategories.Fig.7.5shows the possible logical inputs (00,01,10, and11) and the line drawnby one possible set of parameters for an AND and an OR classi\ufb01er. Notice that thereis simply no way to draw a line that separates the positive cases of XOR (01 and 10)from the negative cases (00 and 11). We say that XOR is not alinearly separablelinearlyseparablefunction. Of course we could draw a boundary with a curve, or some other function,but not a single line.7.2.1 The solution: neural networksWhile the XOR function cannot be calculated by a single perceptron, it can be cal-culated by a layered network of units. Let\u2019s see an example of how to do this fromGoodfellow et al. (2016)that computes XOR using two layers of ReLU-based units.Fig.7.6shows a \ufb01gure with the input being processed by two layers of neural units.The middle layer (calledh) has two units, and the output layer (calledy) has oneunit. A set of weights and biases are shown for each ReLU that correctly computesthe XOR function.Let\u2019s walk through what happens with the input x = [0 0]. If we multiply eachinput value by the appropriate weight, sum, and then add the biasb, we get the",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 46,
      "token_count": 673,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\nNot possible to capture XOR with perceptronsPause the lecture and try for yourself!\n\n## Page 21\n\nWhy? Perceptronsare linear classifiersPerceptron equation givenx1and x2, is the equation of a linew1x1+ w2x2+ b = 0(in standard linear format:     x2= (\u2212w1/w2)x1+ (\u2212b/w2)    )This line acts as a decision boundary \u20220 if input is on one side of the line\u20221 if on the other side of the line \n\n## Page 22\n\nDecision boundaries\n0011x1x20011x1x20011x1x2\na)  x1 AND x2b)  x1 OR x2c)  x1 XOR x2?XOR is not a linearly separable function!\n\n## Page 23\n\nSolution to the XOR problemXOR can'tbe calculated by a single perceptronXOR canbe calculated by a layered network of units. x1x2h1h2y1+11-1111-201+104CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELS\n(a) (b)Figure 7.3The tanh and ReLU activation functions.These activation functions have different properties that make them useful fordifferent language applications or network architectures. For example, the tanh func-tion has the nice properties of being smoothly differentiable and mapping outliervalues toward the mean. The recti\ufb01er function, on the other hand has nice propertiesthat result from it being very close to linear. In the sigmoid or tanh functions, veryhigh values ofzresult in values ofythat aresaturated, i.e., extremely close to 1,saturatedand have derivatives very close to 0. Zero derivatives cause problems for learning,because as we\u2019ll see in Section7.4, we\u2019ll train networks by propagating an errorsignal backwards, multiplying gradients (partial derivatives) from each layer of thenetwork; gradients that are almost 0 cause the error signal to get smaller and smalleruntil it is too small to be used for training, a problem called thevanishing gradientvanishinggradientproblem. Recti\ufb01ers don\u2019t have this problem, since the derivative of ReLU for highvalues ofzis 1 rather than very close to 0.7.2 The XOR problemEarly in the history of neural networks it was realized that the power of neural net-works, as with the real neurons that inspired them, comes from combining theseunits into larger networks.One of the most clever demonstrations of the need for multi-layer networks wasthe proof byMinsky and Papert (1969)that a single neural unit cannot computesome very simple functions of its input. Consider the task of computing elementarylogical functions of two inputs, like AND, OR, and XOR. As a reminder, here arethe truth tables for those functions:AND OR XORx1 x2y x1 x2y x1 x2y0000 000 000100 110 111001 011 011111 111 10This example was \ufb01rst shown for theperceptron, which is a very simple neuralperceptronunit that has a binary output and doesnothave a non-linear activation function. TheReLUReLU",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 47,
      "token_count": 698,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24\n\nSolution to the XOR problemXOR can'tbe calculated by a single perceptronXOR canbe calculated by a layered network of units. x1x2h1h2y1+11-1111-201+104CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELS\n(a) (b)Figure 7.3The tanh and ReLU activation functions.These activation functions have different properties that make them useful fordifferent language applications or network architectures. For example, the tanh func-tion has the nice properties of being smoothly differentiable and mapping outliervalues toward the mean. The recti\ufb01er function, on the other hand has nice propertiesthat result from it being very close to linear. In the sigmoid or tanh functions, veryhigh values ofzresult in values ofythat aresaturated, i.e., extremely close to 1,saturatedand have derivatives very close to 0. Zero derivatives cause problems for learning,because as we\u2019ll see in Section7.4, we\u2019ll train networks by propagating an errorsignal backwards, multiplying gradients (partial derivatives) from each layer of thenetwork; gradients that are almost 0 cause the error signal to get smaller and smalleruntil it is too small to be used for training, a problem called thevanishing gradientvanishinggradientproblem. Recti\ufb01ers don\u2019t have this problem, since the derivative of ReLU for highvalues ofzis 1 rather than very close to 0.7.2 The XOR problemEarly in the history of neural networks it was realized that the power of neural net-works, as with the real neurons that inspired them, comes from combining theseunits into larger networks.One of the most clever demonstrations of the need for multi-layer networks wasthe proof byMinsky and Papert (1969)that a single neural unit cannot computesome very simple functions of its input. Consider the task of computing elementarylogical functions of two inputs, like AND, OR, and XOR. As a reminder, here arethe truth tables for those functions:AND OR XORx1 x2y x1 x2y x1 x2y0000 000 000100 110 111001 011 011111 111 10This example was \ufb01rst shown for theperceptron, which is a very simple neuralperceptronunit that has a binary output and doesnothave a non-linear activation function. The\n\n## Page 25\n\nThe hidden representation h\n0011x1x2\na) The original x space0011h1h2\n2b) The new (linearly separable) h spacex1x2h1h2y1+11-1111-201+10\n(With learning:  hidden layers will learn to form useful representations)\n\n## Page 26\n\nSimple Neural Networks and Neural Language ModelsThe XOR problem\n\n## Page 27\n\nSimple Neural Networks and Neural Language ModelsFeedforward Neural Networks",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 48,
      "token_count": 623,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 26\n\nSimple Neural Networks and Neural Language ModelsThe XOR problem\n\n## Page 27\n\nSimple Neural Networks and Neural Language ModelsFeedforward Neural Networks\n\n## Page 28\n\nFeedforward Neural NetworksCan also be called multi-layer perceptrons(or MLPs)  for historical reasons8CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELS\nx1x2y1xn0\u2026\u2026+1b\u2026UWy2yn2h1h2h3hn1Figure 7.8A simple 2-layer feedforward network, with one hidden layer, one output layer,and one input layer (the input layer is usually not counted when enumerating layers).Recall that a single hidden unit has parametersw(the weight vector) andb(thebias scalar). We represent the parameters for the entire hidden layer by combiningthe weight vectorwiand biasbifor each unitiinto a single weight matrixWanda single bias vectorbfor the whole layer (see Fig.7.8). Each elementWjiof theweight matrixWrepresents the weight of the connection from theith input unitxitothejth hidden unithj.The advantage of using a single matrixWfor the weights of the entire layer isthat now the hidden layer computation for a feedforward network can be done veryef\ufb01ciently with simple matrix operations. In fact, the computation only has threesteps: multiplying the weight matrix by the input vectorx, adding the bias vectorb,and applying the activation functiong(such as the sigmoid, tanh, or ReLU activationfunction de\ufb01ned above).The output of the hidden layer, the vectorh, is thus the following, using thesigmoid functions:h=s(Wx+b)(7.8)Notice that we\u2019re applying thesfunction here to a vector, while in Eq.7.3it wasapplied to a scalar. We\u2019re thus allowings(\u00b7), and indeed any activation functiong(\u00b7), to apply to a vector element-wise, sog[z1,z2,z3]=[g(z1),g(z2),g(z3)].Let\u2019s introduce some constants to represent the dimensionalities of these vectorsand matrices. We\u2019ll refer to the input layer as layer 0 of the network, and haven0represent the number of inputs, soxis a vector of real numbers of dimensionn0,or more formallyx2Rn0, a column vector of dimensionality[n0,1]. Let\u2019s call thehidden layer layer 1 and the output layer layer 2. The hidden layer has dimensional-ityn1, soh2Rn1and alsob2Rn1(since each hidden unit can take a different biasvalue). And the weight matrixWhas dimensionalityW2Rn1\u21e5n0, i.e.[n1,n0].Take a moment to convince yourself that the matrix multiplication in Eq.7.8willcompute the value of eachhjass\u0000Pn0i=1Wjixi+bj\u0000.As we saw in Section7.2, the resulting valueh(forhiddenbut also forhypoth-esis) forms arepresentationof the input. The role of the output layer is to takethis new representationhand compute a \ufb01nal output. This output could be a real-valued number, but in many cases the goal of the network is to make some sort ofclassi\ufb01cation decision, and so we will focus on the case of classi\ufb01cation.If we are doing a binary task like sentiment classi\ufb01cation, we might have a singleoutput node, and its valueyis the probability of positive versus negative sentiment.",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 49,
      "token_count": 760,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 29\n\nBinary Logistic Regression as a 1-layer Network\n29wxnx1\ud835\udc66=\ud835\udf0e(\ud835\udc64&\ud835\udc65+\ud835\udc4f)+1w1wnb(y is a scalar)\u03c3Output layer(\u03c3node)Input layervector x(we don't count the input layer in counting layers!)\n(vector)(scalar)\n\n## Page 30\n\nMultinomial Logistic Regression as a 1-layer Network\n30Wxnx1Fully connected single layer networkW is a matrix\ud835\udc66=softmax(\ud835\udc4a\ud835\udc65+\ud835\udc4f)+1y is a vectory1ynb is a vectorbsssOutput layer(softmaxnodes)Input layerscalars",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 50,
      "token_count": 149,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 31",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 51,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Reminder: softmax: a generalization of sigmoidFor a vector z of dimensionality k, the softmaxis:Example:5.6\u2022MULTINOMIAL LOGISTIC REGRESSION15distributed according to a Gaussian distribution with mean\u00b5=0. In a Gaussianor normal distribution, the further away a value is from the mean, the lower itsprobability (scaled by the variances). By using a Gaussian prior on the weights, weare saying that weights prefer to have the value 0. A Gaussian for a weightqjis1q2ps2jexp \u0000(qj\u0000\u00b5j)22s2j!(5.27)If we multiply each weight by a Gaussian prior on the weight, we are thus maximiz-ing the following constraint:\u02c6q=argmaxqMYi=1P(y(i)|x(i))\u21e5nYj=11q2ps2jexp \u0000(qj\u0000\u00b5j)22s2j!(5.28)which in log space, with\u00b5=0, and assuming 2s2=1, corresponds to\u02c6q=argmaxqmXi=1logP(y(i)|x(i))\u0000anXj=1q2j(5.29)which is in the same form as Eq.5.24.5.6 Multinomial logistic regressionSometimes we need more than two classes. Perhaps we might want to do 3-waysentiment classi\ufb01cation (positive, negative, or neutral). Or we could be assigningsome of the labels we will introduce in Chapter 8, like the part of speech of a word(choosing from 10, 30, or even 50 different parts of speech), or the named entitytype of a phrase (choosing from tags like person, location, organization).In such cases we usemultinomial logistic regression, also calledsoftmax re-multinomiallogisticregressiongression(or, historically, themaxent classi\ufb01er). In multinomial logistic regressionthe targetyis a variable that ranges over more than two classes; we want to knowthe probability ofybeing in each potential classc2C,p(y=c|x).The multinomial logistic classi\ufb01er uses a generalization of the sigmoid, calledthesoftmaxfunction, to compute the probabilityp(y=c|x). The softmax functionsoftmaxtakes a vectorz=[z1,z2,. . . ,zk]ofkarbitrary values and maps them to a probabilitydistribution, with each value in the range (0,1), and all the values summing to 1.Like the sigmoid, it is an exponential function.For a vectorzof dimensionalityk, the softmax is de\ufb01ned as:softmax(zi)=exp(zi)Pkj=1exp(zj)1\uf8ffi\uf8ffk(5.30)The softmax of an input vectorz=[z1,z2,. . . ,zk]is thus a vector itself:softmax(z)=\"exp(z1)Pki=1exp(zi),exp(z2)Pki=1exp(zi),. . . ,exp(zk)Pki=1exp(zi)#(5.31)5.6\u2022MULTINOMIAL LOGISTIC REGRESSION15distributed according to a Gaussian distribution with mean\u00b5=0. In a Gaussianor normal distribution, the further away a value is from the mean, the lower itsprobability (scaled by the variances). By using a Gaussian prior on the weights, weare saying that weights prefer to have the value 0. A Gaussian for a weightqjis1q2ps2jexp \u0000(qj\u0000\u00b5j)22s2j!(5.27)If we multiply each weight by a Gaussian prior on the weight, we are thus maximiz-ing the following",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 52,
      "token_count": 793,
      "chapter_title": ""
    }
  },
  {
    "content": ". . ,zk]ofkarbitrary values and maps them to a probabilitydistribution, with each value in the range (0,1), and all the values summing to 1.Like the sigmoid, it is an exponential function.For a vectorzof dimensionalityk, the softmax is de\ufb01ned as:softmax(zi)=exp(zi)Pkj=1exp(zj)1\uf8ffi\uf8ffk(5.30)The softmax of an input vectorz=[z1,z2,. . . ,zk]is thus a vector itself:softmax(z)=\"exp(z1)Pki=1exp(zi),exp(z2)Pki=1exp(zi),. . . ,exp(zk)Pki=1exp(zi)#(5.31)5.6\u2022MULTINOMIAL LOGISTIC REGRESSION15distributed according to a Gaussian distribution with mean\u00b5=0. In a Gaussianor normal distribution, the further away a value is from the mean, the lower itsprobability (scaled by the variances). By using a Gaussian prior on the weights, weare saying that weights prefer to have the value 0. A Gaussian for a weightqjis1q2ps2jexp \u0000(qj\u0000\u00b5j)22s2j!(5.27)If we multiply each weight by a Gaussian prior on the weight, we are thus maximiz-ing the following constraint:\u02c6q=argmaxqMYi=1P(y(i)|x(i))\u21e5nYj=11q2ps2jexp \u0000(qj\u0000\u00b5j)22s2j!(5.28)which in log space, with\u00b5=0, and assuming 2s2=1, corresponds to\u02c6q=argmaxqmXi=1logP(y(i)|x(i))\u0000anXj=1q2j(5.29)which is in the same form as Eq.5.24.5.6 Multinomial logistic regressionSometimes we need more than two classes. Perhaps we might want to do 3-waysentiment classi\ufb01cation (positive, negative, or neutral). Or we could be assigningsome of the labels we will introduce in Chapter 8, like the part of speech of a word(choosing from 10, 30, or even 50 different parts of speech), or the named entitytype of a phrase (choosing from tags like person, location, organization).In such cases we usemultinomial logistic regression, also calledsoftmax re-multinomiallogisticregressiongression(or, historically, themaxent classi\ufb01er). In multinomial logistic regressionthe targetyis a variable that ranges over more than two classes; we want to knowthe probability ofybeing in each potential classc2C,p(y=c|x).The multinomial logistic classi\ufb01er uses a generalization of the sigmoid, calledthesoftmaxfunction, to compute the probabilityp(y=c|x). The softmax functionsoftmaxtakes a vectorz=[z1,z2,. . . ,zk]ofkarbitrary values and maps them to a probabilitydistribution, with each value in the range (0,1), and all the values summing to 1.Like the sigmoid, it is an exponential function.For a vectorzof dimensionalityk, the softmax is de\ufb01ned as:softmax(zi)=exp(zi)Pkj=1exp(zj)1\uf8ffi\uf8ffk(5.30)The softmax of an input vectorz=[z1,z2,. . . ,zk]is thus a vector itself:softmax(z)=\"exp(z1)Pki=1exp(zi),exp(z2)Pki=1exp(zi),. . .",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 53,
      "token_count": 790,
      "chapter_title": ""
    }
  },
  {
    "content": "or even 50 different parts of speech), or the named entitytype of a phrase (choosing from tags like person, location, organization).In such cases we usemultinomial logistic regression, also calledsoftmax re-multinomiallogisticregressiongression(or, historically, themaxent classi\ufb01er). In multinomial logistic regressionthe targetyis a variable that ranges over more than two classes; we want to knowthe probability ofybeing in each potential classc2C,p(y=c|x).The multinomial logistic classi\ufb01er uses a generalization of the sigmoid, calledthesoftmaxfunction, to compute the probabilityp(y=c|x). The softmax functionsoftmaxtakes a vectorz=[z1,z2,. . . ,zk]ofkarbitrary values and maps them to a probabilitydistribution, with each value in the range (0,1), and all the values summing to 1.Like the sigmoid, it is an exponential function.For a vectorzof dimensionalityk, the softmax is de\ufb01ned as:softmax(zi)=exp(zi)Pkj=1exp(zj)1\uf8ffi\uf8ffk(5.30)The softmax of an input vectorz=[z1,z2,. . . ,zk]is thus a vector itself:softmax(z)=\"exp(z1)Pki=1exp(zi),exp(z2)Pki=1exp(zi),. . . ,exp(zk)Pki=1exp(zi)#(5.31)16CHAPTER5\u2022LOGISTICREGRESSIONThe denominatorPki=1exp(zi)is used to normalize all the values into probabil-ities. Thus for example given a vector:z=[0.6,1.1,\u00001.5,1.2,3.2,\u00001.1]the resulting (rounded) softmax(z) is[0.055,0.090,0.006,0.099,0.74,0.010]Again like the sigmoid, the input to the softmax will be the dot product betweena weight vectorwand an input vectorx(plus a bias). But now we\u2019ll need separateweight vectors (and bias) for each of theKclasses.p(y=c|x)=exp(wc\u00b7x+bc)kXj=1exp(wj\u00b7x+bj)(5.32)Like the sigmoid, the softmax has the property of squashing values toward 0 or 1.Thus if one of the inputs is larger than the others, it will tend to push its probabilitytoward 1, and suppress the probabilities of the smaller inputs.5.6.1 Features in Multinomial Logistic RegressionFeatures in multinomial logistic regression function similarly to binary logistic re-gression, with one difference that we\u2019ll need separate weight vectors (and biases) foreach of theKclasses. Recall our binary exclamation point featurex5from page4:x5=\u21e21 if \u201c!\u201d2doc0 otherwiseIn binary classi\ufb01cation a positive weightw5on a feature in\ufb02uences the classi\ufb01ertowardy=1 (positive sentiment) and a negative weight in\ufb02uences it towardy=0(negative sentiment) with the absolute value indicating how important the featureis. For multinominal logistic regression, by contrast, with separate weights for eachclass, a feature can be evidence for or against each individual class.In 3-way multiclass sentiment classi\ufb01cation, for example, we must assign eachdocument one of the 3 classes+,\u0000, or 0 (neutral). Now a feature related to excla-mation marks might have a negative weight for 0 documents, and a positive weightfor+or\u0000documents:Feature",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 54,
      "token_count": 780,
      "chapter_title": ""
    }
  },
  {
    "content": "the sigmoid, the softmax has the property of squashing values toward 0 or 1.Thus if one of the inputs is larger than the others, it will tend to push its probabilitytoward 1, and suppress the probabilities of the smaller inputs.5.6.1 Features in Multinomial Logistic RegressionFeatures in multinomial logistic regression function similarly to binary logistic re-gression, with one difference that we\u2019ll need separate weight vectors (and biases) foreach of theKclasses. Recall our binary exclamation point featurex5from page4:x5=\u21e21 if \u201c!\u201d2doc0 otherwiseIn binary classi\ufb01cation a positive weightw5on a feature in\ufb02uences the classi\ufb01ertowardy=1 (positive sentiment) and a negative weight in\ufb02uences it towardy=0(negative sentiment) with the absolute value indicating how important the featureis. For multinominal logistic regression, by contrast, with separate weights for eachclass, a feature can be evidence for or against each individual class.In 3-way multiclass sentiment classi\ufb01cation, for example, we must assign eachdocument one of the 3 classes+,\u0000, or 0 (neutral). Now a feature related to excla-mation marks might have a negative weight for 0 documents, and a positive weightfor+or\u0000documents:Feature De\ufb01nitionw5,+w5,\u0000w5,0f5(x)\u21e21 if \u201c!\u201d2doc0 otherwise3.53.1\u00005.35.6.2 Learning in Multinomial Logistic RegressionThe loss function for multinomial logistic regression generalizes the loss functionfor binary logistic regression from 2 toKclasses. Recall that that the cross-entropyloss for binary logistic regression (repeated from Eq.5.11) is:LCE(\u02c6y,y)=\u0000logp(y|x)=\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)](5.33)16CHAPTER5\u2022LOGISTICREGRESSIONThe denominatorPki=1exp(zi)is used to normalize all the values into probabil-ities. Thus for example given a vector:z=[0.6,1.1,\u00001.5,1.2,3.2,\u00001.1]the resulting (rounded) softmax(z) is[0.055,0.090,0.006,0.099,0.74,0.010]Again like the sigmoid, the input to the softmax will be the dot product betweena weight vectorwand an input vectorx(plus a bias). But now we\u2019ll need separateweight vectors (and bias) for each of theKclasses.p(y=c|x)=exp(wc\u00b7x+bc)kXj=1exp(wj\u00b7x+bj)(5.32)Like the sigmoid, the softmax has the property of squashing values toward 0 or 1.Thus if one of the inputs is larger than the others, it will tend to push its probabilitytoward 1, and suppress the probabilities of the smaller inputs.5.6.1 Features in Multinomial Logistic RegressionFeatures in multinomial logistic regression function similarly to binary logistic re-gression, with one difference that we\u2019ll need separate weight vectors (and biases) foreach of theKclasses. Recall our binary exclamation point featurex5from page4:x5=\u21e21 if \u201c!\u201d2doc0 otherwiseIn binary classi\ufb01cation a positive weightw5on a feature in\ufb02uences the classi\ufb01ertowardy=1 (positive sentiment) and a negative weight in\ufb02uences it towardy=0(negative sentiment) with the absolute value indicating how important the featureis. For multinominal logistic regression, by contrast,",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 55,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "like the sigmoid, the input to the softmax will be the dot product betweena weight vectorwand an input vectorx(plus a bias). But now we\u2019ll need separateweight vectors (and bias) for each of theKclasses.p(y=c|x)=exp(wc\u00b7x+bc)kXj=1exp(wj\u00b7x+bj)(5.32)Like the sigmoid, the softmax has the property of squashing values toward 0 or 1.Thus if one of the inputs is larger than the others, it will tend to push its probabilitytoward 1, and suppress the probabilities of the smaller inputs.5.6.1 Features in Multinomial Logistic RegressionFeatures in multinomial logistic regression function similarly to binary logistic re-gression, with one difference that we\u2019ll need separate weight vectors (and biases) foreach of theKclasses. Recall our binary exclamation point featurex5from page4:x5=\u21e21 if \u201c!\u201d2doc0 otherwiseIn binary classi\ufb01cation a positive weightw5on a feature in\ufb02uences the classi\ufb01ertowardy=1 (positive sentiment) and a negative weight in\ufb02uences it towardy=0(negative sentiment) with the absolute value indicating how important the featureis. For multinominal logistic regression, by contrast, with separate weights for eachclass, a feature can be evidence for or against each individual class.In 3-way multiclass sentiment classi\ufb01cation, for example, we must assign eachdocument one of the 3 classes+,\u0000, or 0 (neutral). Now a feature related to excla-mation marks might have a negative weight for 0 documents, and a positive weightfor+or\u0000documents:Feature De\ufb01nitionw5,+w5,\u0000w5,0f5(x)\u21e21 if \u201c!\u201d2doc0 otherwise3.53.1\u00005.35.6.2 Learning in Multinomial Logistic RegressionThe loss function for multinomial logistic regression generalizes the loss functionfor binary logistic regression from 2 toKclasses. Recall that that the cross-entropyloss for binary logistic regression (repeated from Eq.5.11) is:LCE(\u02c6y,y)=\u0000logp(y|x)=\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)](5.33)5.6\u2022MULTINOMIAL LOGISTIC REGRESSION15distributed according to a Gaussian distribution with mean\u00b5=0. In a Gaussianor normal distribution, the further away a value is from the mean, the lower itsprobability (scaled by the variances). By using a Gaussian prior on the weights, weare saying that weights prefer to have the value 0. A Gaussian for a weightqjis1q2ps2jexp \u0000(qj\u0000\u00b5j)22s2j!(5.27)If we multiply each weight by a Gaussian prior on the weight, we are thus maximiz-ing the following constraint:\u02c6q=argmaxqMYi=1P(y(i)|x(i))\u21e5nYj=11q2ps2jexp \u0000(qj\u0000\u00b5j)22s2j!(5.28)which in log space, with\u00b5=0, and assuming 2s2=1, corresponds to\u02c6q=argmaxqmXi=1logP(y(i)|x(i))\u0000anXj=1q2j(5.29)which is in the same form as Eq.5.24.5.6 Multinomial logistic regressionSometimes we need more than two classes. Perhaps we might want to do 3-waysentiment classi\ufb01cation (positive, negative, or neutral). Or we could be assigningsome of the labels we will introduce",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 56,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "LOGISTIC REGRESSION15distributed according to a Gaussian distribution with mean\u00b5=0. In a Gaussianor normal distribution, the further away a value is from the mean, the lower itsprobability (scaled by the variances). By using a Gaussian prior on the weights, weare saying that weights prefer to have the value 0. A Gaussian for a weightqjis1q2ps2jexp \u0000(qj\u0000\u00b5j)22s2j!(5.27)If we multiply each weight by a Gaussian prior on the weight, we are thus maximiz-ing the following constraint:\u02c6q=argmaxqMYi=1P(y(i)|x(i))\u21e5nYj=11q2ps2jexp \u0000(qj\u0000\u00b5j)22s2j!(5.28)which in log space, with\u00b5=0, and assuming 2s2=1, corresponds to\u02c6q=argmaxqmXi=1logP(y(i)|x(i))\u0000anXj=1q2j(5.29)which is in the same form as Eq.5.24.5.6 Multinomial logistic regressionSometimes we need more than two classes. Perhaps we might want to do 3-waysentiment classi\ufb01cation (positive, negative, or neutral). Or we could be assigningsome of the labels we will introduce in Chapter 8, like the part of speech of a word(choosing from 10, 30, or even 50 different parts of speech), or the named entitytype of a phrase (choosing from tags like person, location, organization).In such cases we usemultinomial logistic regression, also calledsoftmax re-multinomiallogisticregressiongression(or, historically, themaxent classi\ufb01er). In multinomial logistic regressionthe targetyis a variable that ranges over more than two classes; we want to knowthe probability ofybeing in each potential classc2C,p(y=c|x).The multinomial logistic classi\ufb01er uses a generalization of the sigmoid, calledthesoftmaxfunction, to compute the probabilityp(y=c|x). The softmax functionsoftmaxtakes a vectorz=[z1,z2,. . . ,zk]ofkarbitrary values and maps them to a probabilitydistribution, with each value in the range (0,1), and all the values summing to 1.Like the sigmoid, it is an exponential function.For a vectorzof dimensionalityk, the softmax is de\ufb01ned as:softmax(zi)=exp(zi)Pkj=1exp(zj)1\uf8ffi\uf8ffk(5.30)The softmax of an input vectorz=[z1,z2,. . . ,zk]is thus a vector itself:softmax(z)=\"exp(z1)Pki=1exp(zi),exp(z2)Pki=1exp(zi),. . . ,exp(zk)Pki=1exp(zi)#(5.31)",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 57,
      "token_count": 633,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 32\n\nTwo-Layer Network with scalar outputUWxnx1\n+1y is a scalarbhidden units(\u03c3node)Input layer(vector)Output layer(\u03c3node)\nCould be ReLUOr tanhz=\ud835\udc48\u210e\ud835\udc66=\ud835\udf0e(\ud835\udc67)\n\n## Page 33\n\nTwo-Layer Network with scalar outputUWxnx1\n+1bhidden units(\u03c3node)Input layer(vector)Output layer(\u03c3node)ijWjivectory is a scalarz=\ud835\udc48\u210e\ud835\udc66=\ud835\udf0e(\ud835\udc67)\n\n## Page 34\n\nTwo-Layer Network with scalar outputUWxnx1\n+1bhidden units(\u03c3node)Input layer(vector)Output layer(\u03c3node)\nCould be ReLUOr tanhy is a scalarz=\ud835\udc48\u210e\ud835\udc66=\ud835\udf0e(\ud835\udc67)\n\n## Page 35\n\nTwo-Layer Network with softmaxoutputUWxnx1\n+1bhidden units(\u03c3node)Input layer(vector)Output layer(\u03c3node)\nCould be ReLUOr tanhy is a vectorz=\ud835\udc48\u210e\ud835\udc66=softmax(\ud835\udc67)\n\n## Page 36\n\nMulti-layer NotationW[1]xnx1+1b[1]ijW[2]b[2]\ud835\udc67[\"]=\ud835\udc4a[\"]\ud835\udc4e[$]+\ud835\udc4f[\"]\ud835\udc4e[$]\ud835\udc4e[\"]=\ud835\udc54\"(\ud835\udc67\")\ud835\udc67[%]=\ud835\udc4a[%]\ud835\udc4e[\"]+\ud835\udc4f[%]\ud835\udc4e[%]=\ud835\udc54%(\ud835\udc67%)\ud835\udc66=\ud835\udc4e[%]sigmoid or softmaxReLU\n\n## Page 37\n\nMulti Layer Notation\n37\nx1x2x3\ny\nw1w2w3\u2211b\u03c3+1za\n\n## Page 38\n\nReplacing the bias unitLet's switch to a notation without the bias unitJust a notational change1.Add a dummy node a0=1 to each layer2.Its weight w0will be the bias3.So input layer a[0]0=1, \u25e6And a[1]0=1 , a[2]0=1,\u2026",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 58,
      "token_count": 489,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 39",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 59,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Replacing the bias unitInstead of:We'll do this:10CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELS(\ufb01rst) hidden layer, andb[1]will mean the bias vector for the (\ufb01rst) hidden layer.njwill mean the number of units at layer j. We\u2019ll useg(\u00b7)to stand for the activationfunction, which will tend to be ReLU or tanh for intermediate layers and softmaxfor output layers. We\u2019ll usea[i]to mean the output from layeri, andz[i]to mean thecombination of weights and biasesW[i]a[i\u00001]+b[i]. The 0th layer is for inputs, so theinputsxwe\u2019ll refer to more generally asa[0].Thus we can re-represent our 2-layer net from Eq.7.10as follows:z[1]=W[1]a[0]+b[1]a[1]=g[1](z[1])z[2]=W[2]a[1]+b[2]a[2]=g[2](z[2])\u02c6y=a[2](7.11)Note that with this notation, the equations for the computation done at each layer arethe same. The algorithm for computing the forward step in an n-layer feedforwardnetwork, given the input vectora[0]is thus simply:foriin1..nz[i]=W[i]a[i\u00001]+b[i]a[i]=g[i](z[i])\u02c6y=a[n]The activation functionsg(\u00b7)are generally different at the \ufb01nal layer. Thusg[2]might be softmax for multinomial classi\ufb01cation or sigmoid for binary classi\ufb01cation,while ReLU or tanh might be the activation functiong(\u00b7)at the internal layers.Replacing the bias unitIn describing networks, we will often use a slightly sim-pli\ufb01ed notation that represents exactly the same function without referring to an ex-plicit bias nodeb. Instead, we add a dummy nodea0to each layer whose value willalways be 1. Thus layer 0, the input layer, will have a dummy nodea[0]0=1, layer 1will havea[1]0=1, and so on. This dummy node still has an associated weight, andthat weight represents the bias valueb. For example instead of an equation likeh=s(Wx+b)(7.12)we\u2019ll use:h=s(Wx)(7.13)But now instead of our vectorxhavingnvalues:x=x1,...,xn, it will haven+1 values, with a new 0th dummy valuex0=1:x=x0,...,xn0. And instead ofcomputing eachhjas follows:hj=s n0Xi=1Wjixi+bj!,(7.14)we\u2019ll instead use:s n0Xi=0Wjixi!,(7.15)10CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELS(\ufb01rst) hidden layer, andb[1]will mean the bias vector for the (\ufb01rst) hidden layer.njwill mean the number of units at layer j. We\u2019ll useg(\u00b7)to stand for the activationfunction, which will tend to be ReLU or tanh for intermediate layers and softmaxfor output layers. We\u2019ll usea[i]to mean the output from layeri, andz[i]to mean thecombination of weights and biasesW[i]a[i\u00001]+b[i]. The 0th layer is for inputs, so theinputsxwe\u2019ll refer to more generally asa[0].Thus we can re-represent our 2-layer net from Eq.7.10as",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 60,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "andthat weight represents the bias valueb. For example instead of an equation likeh=s(Wx+b)(7.12)we\u2019ll use:h=s(Wx)(7.13)But now instead of our vectorxhavingnvalues:x=x1,...,xn, it will haven+1 values, with a new 0th dummy valuex0=1:x=x0,...,xn0. And instead ofcomputing eachhjas follows:hj=s n0Xi=1Wjixi+bj!,(7.14)we\u2019ll instead use:s n0Xi=0Wjixi!,(7.15)10CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELS(\ufb01rst) hidden layer, andb[1]will mean the bias vector for the (\ufb01rst) hidden layer.njwill mean the number of units at layer j. We\u2019ll useg(\u00b7)to stand for the activationfunction, which will tend to be ReLU or tanh for intermediate layers and softmaxfor output layers. We\u2019ll usea[i]to mean the output from layeri, andz[i]to mean thecombination of weights and biasesW[i]a[i\u00001]+b[i]. The 0th layer is for inputs, so theinputsxwe\u2019ll refer to more generally asa[0].Thus we can re-represent our 2-layer net from Eq.7.10as follows:z[1]=W[1]a[0]+b[1]a[1]=g[1](z[1])z[2]=W[2]a[1]+b[2]a[2]=g[2](z[2])\u02c6y=a[2](7.11)Note that with this notation, the equations for the computation done at each layer arethe same. The algorithm for computing the forward step in an n-layer feedforwardnetwork, given the input vectora[0]is thus simply:foriin1..nz[i]=W[i]a[i\u00001]+b[i]a[i]=g[i](z[i])\u02c6y=a[n]The activation functionsg(\u00b7)are generally different at the \ufb01nal layer. Thusg[2]might be softmax for multinomial classi\ufb01cation or sigmoid for binary classi\ufb01cation,while ReLU or tanh might be the activation functiong(\u00b7)at the internal layers.Replacing the bias unitIn describing networks, we will often use a slightly sim-pli\ufb01ed notation that represents exactly the same function without referring to an ex-plicit bias nodeb. Instead, we add a dummy nodea0to each layer whose value willalways be 1. Thus layer 0, the input layer, will have a dummy nodea[0]0=1, layer 1will havea[1]0=1, and so on. This dummy node still has an associated weight, andthat weight represents the bias valueb. For example instead of an equation likeh=s(Wx+b)(7.12)we\u2019ll use:h=s(Wx)(7.13)But now instead of our vectorxhavingnvalues:x=x1,...,xn, it will haven+1 values, with a new 0th dummy valuex0=1:x=x0,...,xn0. And instead ofcomputing eachhjas follows:hj=s n0Xi=1Wjixi+bj!,(7.14)we\u2019ll instead use:s n0Xi=0Wjixi!,(7.15)10CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELS(\ufb01rst) hidden layer, andb[1]will mean the bias vector for the (\ufb01rst) hidden layer.njwill mean the number",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 61,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "the activation functiong(\u00b7)at the internal layers.Replacing the bias unitIn describing networks, we will often use a slightly sim-pli\ufb01ed notation that represents exactly the same function without referring to an ex-plicit bias nodeb. Instead, we add a dummy nodea0to each layer whose value willalways be 1. Thus layer 0, the input layer, will have a dummy nodea[0]0=1, layer 1will havea[1]0=1, and so on. This dummy node still has an associated weight, andthat weight represents the bias valueb. For example instead of an equation likeh=s(Wx+b)(7.12)we\u2019ll use:h=s(Wx)(7.13)But now instead of our vectorxhavingnvalues:x=x1,...,xn, it will haven+1 values, with a new 0th dummy valuex0=1:x=x0,...,xn0. And instead ofcomputing eachhjas follows:hj=s n0Xi=1Wjixi+bj!,(7.14)we\u2019ll instead use:s n0Xi=0Wjixi!,(7.15)10CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELS(\ufb01rst) hidden layer, andb[1]will mean the bias vector for the (\ufb01rst) hidden layer.njwill mean the number of units at layer j. We\u2019ll useg(\u00b7)to stand for the activationfunction, which will tend to be ReLU or tanh for intermediate layers and softmaxfor output layers. We\u2019ll usea[i]to mean the output from layeri, andz[i]to mean thecombination of weights and biasesW[i]a[i\u00001]+b[i]. The 0th layer is for inputs, so theinputsxwe\u2019ll refer to more generally asa[0].Thus we can re-represent our 2-layer net from Eq.7.10as follows:z[1]=W[1]a[0]+b[1]a[1]=g[1](z[1])z[2]=W[2]a[1]+b[2]a[2]=g[2](z[2])\u02c6y=a[2](7.11)Note that with this notation, the equations for the computation done at each layer arethe same. The algorithm for computing the forward step in an n-layer feedforwardnetwork, given the input vectora[0]is thus simply:foriin1..nz[i]=W[i]a[i\u00001]+b[i]a[i]=g[i](z[i])\u02c6y=a[n]The activation functionsg(\u00b7)are generally different at the \ufb01nal layer. Thusg[2]might be softmax for multinomial classi\ufb01cation or sigmoid for binary classi\ufb01cation,while ReLU or tanh might be the activation functiong(\u00b7)at the internal layers.Replacing the bias unitIn describing networks, we will often use a slightly sim-pli\ufb01ed notation that represents exactly the same function without referring to an ex-plicit bias nodeb. Instead, we add a dummy nodea0to each layer whose value willalways be 1. Thus layer 0, the input layer, will have a dummy nodea[0]0=1, layer 1will havea[1]0=1, and so on. This dummy node still has an associated weight, andthat weight represents the bias valueb. For example instead of an equation likeh=s(Wx+b)(7.12)we\u2019ll use:h=s(Wx)(7.13)But now instead of our vectorxhavingnvalues:x=x1,...,xn, it will haven+1 values, with a",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 62,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "each layer arethe same. The algorithm for computing the forward step in an n-layer feedforwardnetwork, given the input vectora[0]is thus simply:foriin1..nz[i]=W[i]a[i\u00001]+b[i]a[i]=g[i](z[i])\u02c6y=a[n]The activation functionsg(\u00b7)are generally different at the \ufb01nal layer. Thusg[2]might be softmax for multinomial classi\ufb01cation or sigmoid for binary classi\ufb01cation,while ReLU or tanh might be the activation functiong(\u00b7)at the internal layers.Replacing the bias unitIn describing networks, we will often use a slightly sim-pli\ufb01ed notation that represents exactly the same function without referring to an ex-plicit bias nodeb. Instead, we add a dummy nodea0to each layer whose value willalways be 1. Thus layer 0, the input layer, will have a dummy nodea[0]0=1, layer 1will havea[1]0=1, and so on. This dummy node still has an associated weight, andthat weight represents the bias valueb. For example instead of an equation likeh=s(Wx+b)(7.12)we\u2019ll use:h=s(Wx)(7.13)But now instead of our vectorxhavingnvalues:x=x1,...,xn, it will haven+1 values, with a new 0th dummy valuex0=1:x=x0,...,xn0. And instead ofcomputing eachhjas follows:hj=s n0Xi=1Wjixi+bj!,(7.14)we\u2019ll instead use:s n0Xi=0Wjixi!,(7.15)10CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELS(\ufb01rst) hidden layer, andb[1]will mean the bias vector for the (\ufb01rst) hidden layer.njwill mean the number of units at layer j. We\u2019ll useg(\u00b7)to stand for the activationfunction, which will tend to be ReLU or tanh for intermediate layers and softmaxfor output layers. We\u2019ll usea[i]to mean the output from layeri, andz[i]to mean thecombination of weights and biasesW[i]a[i\u00001]+b[i]. The 0th layer is for inputs, so theinputsxwe\u2019ll refer to more generally asa[0].Thus we can re-represent our 2-layer net from Eq.7.10as follows:z[1]=W[1]a[0]+b[1]a[1]=g[1](z[1])z[2]=W[2]a[1]+b[2]a[2]=g[2](z[2])\u02c6y=a[2](7.11)Note that with this notation, the equations for the computation done at each layer arethe same. The algorithm for computing the forward step in an n-layer feedforwardnetwork, given the input vectora[0]is thus simply:foriin1..nz[i]=W[i]a[i\u00001]+b[i]a[i]=g[i](z[i])\u02c6y=a[n]The activation functionsg(\u00b7)are generally different at the \ufb01nal layer. Thusg[2]might be softmax for multinomial classi\ufb01cation or sigmoid for binary classi\ufb01cation,while ReLU or tanh might be the activation functiong(\u00b7)at the internal layers.Replacing the bias unitIn describing networks, we will often use a slightly sim-pli\ufb01ed notation that represents exactly the same function without referring to an ex-plicit bias nodeb. Instead, we add a dummy nodea0to each layer whose value",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 63,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "for inputs, so theinputsxwe\u2019ll refer to more generally asa[0].Thus we can re-represent our 2-layer net from Eq.7.10as follows:z[1]=W[1]a[0]+b[1]a[1]=g[1](z[1])z[2]=W[2]a[1]+b[2]a[2]=g[2](z[2])\u02c6y=a[2](7.11)Note that with this notation, the equations for the computation done at each layer arethe same. The algorithm for computing the forward step in an n-layer feedforwardnetwork, given the input vectora[0]is thus simply:foriin1..nz[i]=W[i]a[i\u00001]+b[i]a[i]=g[i](z[i])\u02c6y=a[n]The activation functionsg(\u00b7)are generally different at the \ufb01nal layer. Thusg[2]might be softmax for multinomial classi\ufb01cation or sigmoid for binary classi\ufb01cation,while ReLU or tanh might be the activation functiong(\u00b7)at the internal layers.Replacing the bias unitIn describing networks, we will often use a slightly sim-pli\ufb01ed notation that represents exactly the same function without referring to an ex-plicit bias nodeb. Instead, we add a dummy nodea0to each layer whose value willalways be 1. Thus layer 0, the input layer, will have a dummy nodea[0]0=1, layer 1will havea[1]0=1, and so on. This dummy node still has an associated weight, andthat weight represents the bias valueb. For example instead of an equation likeh=s(Wx+b)(7.12)we\u2019ll use:h=s(Wx)(7.13)But now instead of our vectorxhavingnvalues:x=x1,...,xn, it will haven+1 values, with a new 0th dummy valuex0=1:x=x0,...,xn0. And instead ofcomputing eachhjas follows:hj=s n0Xi=1Wjixi+bj!,(7.14)we\u2019ll instead use:s n0Xi=0Wjixi!,(7.15)x= x1, x2, \u2026, xn0x= x0, x1, x2, \u2026, xn0",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 64,
      "token_count": 511,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 40\n\nReplacing the bias unit\nx1x2y1xn0\u2026\u2026+1b\u2026UWy2yn2h1h2h3hn1x1x2y1xn0\u2026\u2026x0=1\u2026UWy2yn2h1h2h3hn1Instead of:We'll do this:\n\n## Page 41\n\nSimple Neural Networks and Neural Language ModelsFeedforward Neural Networks\n\n## Page 42\n\nSimple Neural Networks and Neural Language ModelsApplying feedforward networks to NLP tasks\n\n## Page 43\n\nUse cases for feedforward networksLet's consider 2 (simplified) sample tasks:1.Text classification2.Language modelingState of the art systems use more powerful neural architectures, but simple models are useful to consider!43\n\n## Page 44\n\nClassification: Sentiment AnalysisWe could do exactly what we did with logistic regressionInput layer are binary features as beforeOutput layer is 0 or 1UWxnx1\u03c3",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 65,
      "token_count": 201,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 41\n\nSimple Neural Networks and Neural Language ModelsFeedforward Neural Networks\n\n## Page 42\n\nSimple Neural Networks and Neural Language ModelsApplying feedforward networks to NLP tasks\n\n## Page 43\n\nUse cases for feedforward networksLet's consider 2 (simplified) sample tasks:1.Text classification2.Language modelingState of the art systems use more powerful neural architectures, but simple models are useful to consider!43\n\n## Page 44\n\nClassification: Sentiment AnalysisWe could do exactly what we did with logistic regressionInput layer are binary features as beforeOutput layer is 0 or 1UWxnx1\u03c3\n\n## Page 45\n\nSentiment Features\n454CHAPTER5\u2022LOGISTICREGRESSIONnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outliervalues toward 0 or 1. And it\u2019s differentiable, which as we\u2019ll see in Section5.8willbe handy for learning.We\u2019re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)P(y=0)=1\u0000s(w\u00b7x+b)=1\u000011+e\u0000(w\u00b7x+b)=e\u0000(w\u00b7x+b)1+e\u0000(w\u00b7x+b)(5.5)Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if the probabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecision boundary:decisionboundary\u02c6y=\u21e21 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classi\ufb01cationLet\u2019s have an example. Suppose we are doing binary sentiment classi\ufb01cation onmovie review text, and we would like to know whether to assign the sentiment class+or\u0000to a review documentdoc. We\u2019ll represent each input observation by the 6featuresx1...x6of the input shown in the following table; Fig.5.2shows the featuresin a sample mini test document.Var De\ufb01nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3\u21e21 if \u201cno\u201d2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5\u21e21 if \u201c!\u201d2doc0 otherwise0x6log(word count of doc)ln(66)=4.19Let\u2019s assume for the moment that we\u2019ve already learned a real-valued weight foreach of these features, and that the 6 weights corresponding to the 6 features are[2.5,\u00005.0,\u00001.2,0.5,2.0,0.7], whileb= 0.1. (We\u2019ll discuss in the next section howthe weights are learned.) The weightw1, for example indicates how important afeature the number of positive lexicon words (great,nice,enjoyable, etc.) is toa positive sentiment decision, whilew2tells us the importance of negative lexiconwords. Note thatw1=2.5 is positive, whilew2=\u00005.0, meaning that negative wordsare negatively associated with a positive sentiment decision, and are about twice asimportant as positive words.",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 66,
      "token_count": 788,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 46\n\nFeedforward nets for simple classificationJust adding a hidden layer to logistic regression\u2022allows the network to use non-linear interactions between features \u2022which may (or may not) improve performance.46UWxnx1f1f2fn46Wxnx1f1f2fnLogisticRegression2-layerfeedforwardnetwork\u03c3\u03c3\n\n## Page 47\n\nEven better: representation learningThe real power of deep learningcomes from the  ability to learnfeatures from the dataInstead of using hand-built human-engineered features for classificationUse learned representations like embeddings!47UWxnx1e1e2en\u03c3\n\n## Page 48\n\n48Neural Net Classification with embeddings as input features!\nh1h2h3hdh\u2026UWy\n3d\u2a091Hidden layerOutput layersigmoid\nThe...dessertiswt-1\nw2w1dh\u2a093ddh\u2a091|V|\u2a09dhProjection layerembeddingsp(positive sentiment|The dessert is\u2026)^\nembedding forword 7embedding for word 23864embedding forword 534w3E\u2026\n\n## Page 49\n\nIssue: texts come in different sizesThis assumes a fixed size length (3)!  Kind of unrealistic.   Some simple solutions (more sophisticated solutions later)1.Make the input the length of the longest review\u2022If shorter then pad with zero embeddings\u2022Truncate if you get longer reviews at test time2.Create a single \"sentence embedding\" (the same dimensionality as a word) to represent all the words\u2022Take the mean of all the word embeddings\u2022Take the element-wise max of all the word embeddings\u2022For each dimension, pick the max value from all words49h1h2h3hdh\u2026UWy\n3d\u2a091Hidden layerOutput layersigmoid\nThe...dessertiswt-1\nw2w1dh\u2a093ddh\u2a091|V|\u2a09dhProjection layerembeddingsp(positive sentiment|The dessert is\u2026)^\nembedding forword 7embedding for word 23864embedding forword 534w3E\u2026\n\n## Page 50\n\nReminder: Multiclass OutputsWhat if you have more than two output classes?\u25e6Add more output units (one for each class)\u25e6And use a \u201csoftmaxlayer\u201d\n50UWxnx1\n\n## Page 51\n\nNeural Language Models (LMs)Language Modeling: Calculating the probability of the next word in a sequence given some history. \u2022We've seen N-gram based LMs\u2022But neural network LMs far outperform n-gram language modelsState-of-the-art neural LMs are based on more powerful neural network technology like TransformersBut simple feedforward LMs can do almost as well!51\n\n## Page 52\n\nSimple feedforward Neural Language ModelsTask: predict next word wtgiven prior words wt-1, wt-2, wt-3, \u2026Problem: Now we\u2019re dealing with sequences of arbitrary length.Solution: Sliding windows (of fixed length)\n52\n\n## Page 53\n\n53Neural Language Model \n\n## Page 54\n\nWhy Neural LMs work better than N-gram LMsTraining data:We've seen:  I have to make sure that the cat gets fed. Never seen:   dog gets fedTest data:I forgot to make sure that the dog gets ___N-gram LM can't predict \"fed\"!Neural LM can use similarity of \"cat\" and \"dog\" embeddings to generalize and predict \u201cfed\u201d after dog\n\n## Page 55\n\nSimple Neural Networks and Neural Language ModelsApplying feedforward networks to NLP tasks\n\n## Page 56\n\nSimple Neural Networks and Neural Language ModelsTraining Neural Nets: Overview",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 67,
      "token_count": 778,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 51\n\nNeural Language Models (LMs)Language Modeling: Calculating the probability of the next word in a sequence given some history. \u2022We've seen N-gram based LMs\u2022But neural network LMs far outperform n-gram language modelsState-of-the-art neural LMs are based on more powerful neural network technology like TransformersBut simple feedforward LMs can do almost as well!51\n\n## Page 52\n\nSimple feedforward Neural Language ModelsTask: predict next word wtgiven prior words wt-1, wt-2, wt-3, \u2026Problem: Now we\u2019re dealing with sequences of arbitrary length.Solution: Sliding windows (of fixed length)\n52\n\n## Page 53\n\n53Neural Language Model \n\n## Page 54\n\nWhy Neural LMs work better than N-gram LMsTraining data:We've seen:  I have to make sure that the cat gets fed. Never seen:   dog gets fedTest data:I forgot to make sure that the dog gets ___N-gram LM can't predict \"fed\"!Neural LM can use similarity of \"cat\" and \"dog\" embeddings to generalize and predict \u201cfed\u201d after dog\n\n## Page 55\n\nSimple Neural Networks and Neural Language ModelsApplying feedforward networks to NLP tasks\n\n## Page 56\n\nSimple Neural Networks and Neural Language ModelsTraining Neural Nets: Overview\n\n## Page 57\n\nIntuition: training a 2-layer Network\n57UWxnx1System output !\ud835\udc66Actual answer \ud835\udc66\nTraining instanceLoss function L(!\ud835\udc66,\ud835\udc66)\nForward passBackward pass\n\n## Page 58\n\nIntuition: Training a 2-layer networkFor every training tuple (\ud835\udc65,\ud835\udc66)\u25e6Run forwardcomputation to find our estimate 3\ud835\udc66\u25e6Run backwardcomputation to update weights: \u25e6For every output node\u25e6Compute loss \ud835\udc3fbetween true \ud835\udc66and the estimated #\ud835\udc66\u25e6For every weight \ud835\udc64from hidden layer to the output layer\u25e6Update the weight\u25e6For every hidden node\u25e6Assess how much blame it deserves for the current answer\u25e6For every weight \ud835\udc64from input layer to the hidden layer\u25e6Update the weight58",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 68,
      "token_count": 475,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 59",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 69,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Reminder: Loss Function for binary logistic regressionA measure for how far off the current answer is to the right answerCross entropy loss for logistic regression:",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 70,
      "token_count": 29,
      "chapter_title": ""
    }
  },
  {
    "content": "595.3\u2022THE CROSS-ENTROPY LOSS FUNCTION7thecross-entropy loss.The second thing we need is an optimization algorithm for iteratively updatingthe weights so as to minimize this loss function. The standard algorithm for this isgradient descent; we\u2019ll introduce thestochastic gradient descentalgorithm in thefollowing section.5.3 The cross-entropy loss functionWe need a loss function that expresses, for an observationx, how close the classi\ufb01eroutput ( \u02c6y=s(w\u00b7x+b)) is to the correct output (y, which is 0 or 1). We\u2019ll call this:L(\u02c6y,y)=How much \u02c6ydiffers from the truey(5.8)We do this via a loss function that prefers the correct class labels of the train-ing examples to bemore likely. This is calledconditional maximum likelihoodestimation: we choose the parametersw,bthatmaximize the log probability ofthe trueylabels in the training datagiven the observationsx. The resulting lossfunction is the negative log likelihood loss, generally called thecross-entropy loss.cross-entropylossLet\u2019s derive this loss function, applied to a single observationx. We\u2019d like tolearn weights that maximize the probability of the correct labelp(y|x). Since thereare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we canexpress the probabilityp(y|x)that our classi\ufb01er produces for one observation asthe following (keeping in mind that if y=1, Eq.5.9simpli\ufb01es to \u02c6y; if y=0, Eq.5.9simpli\ufb01es to 1\u0000\u02c6y):p(y|x)=\u02c6yy(1\u0000\u02c6y)1\u0000y(5.9)Now we take the log of both sides. This will turn out to be handy mathematically,and doesn\u2019t hurt us; whatever values maximize a probability will also maximize thelog of the probability:logp(y|x)=log\u21e5\u02c6yy(1\u0000\u02c6y)1\u0000y\u21e4=ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)(5.10)Eq.5.10describes a log likelihood that should be maximized. In order to turn thisinto loss function (something that we need to minimize), we\u2019ll just \ufb02ip the sign onEq.5.10. The result is the cross-entropy lossLCE:LCE(\u02c6y,y)=\u0000logp(y|x)=\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)](5.11)Finally, we can plug in the de\ufb01nition of \u02c6y=s(w\u00b7x+b):LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.12)Let\u2019s see if this loss function does the right thing for our example from Fig.5.2.W ewant the loss to be smaller if the model\u2019s estimate is close to correct, and bigger ifthe model is confused. So \ufb01rst let\u2019s suppose the correct gold label for the sentimentexample in Fig.5.2is positive, i.e.,y=1. In this case our model is doing well, sincefrom Eq.5.7it indeed gave the example a higher probability of being positive (.69)than negative (.31). If we plugs(w\u00b7x+b)=.69 andy=1 into Eq.5.12, the right5.3\u2022THE CROSS-ENTROPY LOSS FUNCTION7thecross-entropy loss.The second thing we need is an optimization algorithm for iteratively updatingthe weights so as to",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 71,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "to minimize), we\u2019ll just \ufb02ip the sign onEq.5.10. The result is the cross-entropy lossLCE:LCE(\u02c6y,y)=\u0000logp(y|x)=\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)](5.11)Finally, we can plug in the de\ufb01nition of \u02c6y=s(w\u00b7x+b):LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.12)Let\u2019s see if this loss function does the right thing for our example from Fig.5.2.W ewant the loss to be smaller if the model\u2019s estimate is close to correct, and bigger ifthe model is confused. So \ufb01rst let\u2019s suppose the correct gold label for the sentimentexample in Fig.5.2is positive, i.e.,y=1. In this case our model is doing well, sincefrom Eq.5.7it indeed gave the example a higher probability of being positive (.69)than negative (.31). If we plugs(w\u00b7x+b)=.69 andy=1 into Eq.5.12, the right5.3\u2022THE CROSS-ENTROPY LOSS FUNCTION7thecross-entropy loss.The second thing we need is an optimization algorithm for iteratively updatingthe weights so as to minimize this loss function. The standard algorithm for this isgradient descent; we\u2019ll introduce thestochastic gradient descentalgorithm in thefollowing section.5.3 The cross-entropy loss functionWe need a loss function that expresses, for an observationx, how close the classi\ufb01eroutput ( \u02c6y=s(w\u00b7x+b)) is to the correct output (y, which is 0 or 1). We\u2019ll call this:L(\u02c6y,y)=How much \u02c6ydiffers from the truey(5.8)We do this via a loss function that prefers the correct class labels of the train-ing examples to bemore likely. This is calledconditional maximum likelihoodestimation: we choose the parametersw,bthatmaximize the log probability ofthe trueylabels in the training datagiven the observationsx. The resulting lossfunction is the negative log likelihood loss, generally called thecross-entropy loss.cross-entropylossLet\u2019s derive this loss function, applied to a single observationx. We\u2019d like tolearn weights that maximize the probability of the correct labelp(y|x). Since thereare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we canexpress the probabilityp(y|x)that our classi\ufb01er produces for one observation asthe following (keeping in mind that if y=1, Eq.5.9simpli\ufb01es to \u02c6y; if y=0, Eq.5.9simpli\ufb01es to 1\u0000\u02c6y):p(y|x)=\u02c6yy(1\u0000\u02c6y)1\u0000y(5.9)Now we take the log of both sides. This will turn out to be handy mathematically,and doesn\u2019t hurt us; whatever values maximize a probability will also maximize thelog of the probability:logp(y|x)=log\u21e5\u02c6yy(1\u0000\u02c6y)1\u0000y\u21e4=ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)(5.10)Eq.5.10describes a log likelihood that should be maximized. In order to turn thisinto loss function (something that we need to minimize), we\u2019ll just \ufb02ip the sign onEq.5.10. The result is the cross-entropy",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 72,
      "token_count": 788,
      "chapter_title": ""
    }
  },
  {
    "content": "loss.cross-entropylossLet\u2019s derive this loss function, applied to a single observationx. We\u2019d like tolearn weights that maximize the probability of the correct labelp(y|x). Since thereare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we canexpress the probabilityp(y|x)that our classi\ufb01er produces for one observation asthe following (keeping in mind that if y=1, Eq.5.9simpli\ufb01es to \u02c6y; if y=0, Eq.5.9simpli\ufb01es to 1\u0000\u02c6y):p(y|x)=\u02c6yy(1\u0000\u02c6y)1\u0000y(5.9)Now we take the log of both sides. This will turn out to be handy mathematically,and doesn\u2019t hurt us; whatever values maximize a probability will also maximize thelog of the probability:logp(y|x)=log\u21e5\u02c6yy(1\u0000\u02c6y)1\u0000y\u21e4=ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)(5.10)Eq.5.10describes a log likelihood that should be maximized. In order to turn thisinto loss function (something that we need to minimize), we\u2019ll just \ufb02ip the sign onEq.5.10. The result is the cross-entropy lossLCE:LCE(\u02c6y,y)=\u0000logp(y|x)=\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)](5.11)Finally, we can plug in the de\ufb01nition of \u02c6y=s(w\u00b7x+b):LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.12)Let\u2019s see if this loss function does the right thing for our example from Fig.5.2.W ewant the loss to be smaller if the model\u2019s estimate is close to correct, and bigger ifthe model is confused. So \ufb01rst let\u2019s suppose the correct gold label for the sentimentexample in Fig.5.2is positive, i.e.,y=1. In this case our model is doing well, sincefrom Eq.5.7it indeed gave the example a higher probability of being positive (.69)than negative (.31). If we plugs(w\u00b7x+b)=.69 andy=1 into Eq.5.12, the right",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 73,
      "token_count": 535,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 60",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 74,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Reminder: gradient descent for weight updatesUse the derivative of the loss function with respect to weights !!\"\ud835\udc3f(\ud835\udc53\ud835\udc65;\ud835\udc64,\ud835\udc66)To tell us how to adjust weights for each training item \u25e6Move them in the opposite direction of the gradient\u25e6For logistic regression10CHAPTER5\u2022LOGISTICREGRESSIONexample):wt+1=wt\u0000hddwL(f(x;w),y)(5.14)Now let\u2019s extend the intuition from a function of one scalar variablewto manyvariables, because we don\u2019t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If we\u2019re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.\nCost(w,b)",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 75,
      "token_count": 251,
      "chapter_title": ""
    }
  },
  {
    "content": "wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially we\u2019reasking: \u201cHow much would a small change in that variablewiin\ufb02uence the total lossfunctionL?\u201dIn each dimensionwi, we express the slope as a partial derivative\u2202\u2202wiof the lossfunction. The gradient is then de\ufb01ned as a vector of these partials. We\u2019ll represent \u02c6yasf(x;q)to make the dependence onqmore obvious:\u2014qL(f(x;q),y)) =266664\u2202\u2202w1L(f(x;q),y)\u2202\u2202w2L(f(x;q),y)...\u2202\u2202wnL(f(x;q),y)377775(5.15)The \ufb01nal equation for updatingqbased on the gradient is thusqt+1=qt\u0000h\u2014L(f(x;q),y)(5.16)5.4\u2022GRADIENTDESCENT115.4.1 The Gradient for Logistic RegressionIn order to updateq, we need a de\ufb01nition for the gradient\u2014L(f(x;q),y). Recall thatfor logistic regression, the cross-entropy loss function is:LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.17)It turns out that the derivative of this function for one observation vectorxisEq.5.18(the interested reader can see Section5.8for the derivation of this equation):\u2202LCE(\u02c6y,y)\u2202wj=[s(w\u00b7x+b)\u0000y]xj(5.18)Note in Eq.5.18that the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated \u02c6y=s(w\u00b7x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute \u02c6y(i)=f(x(i);q)# What is our estimated output \u02c6y?Compute the lossL(\u02c6y(i),y(i))# How far off is \u02c6y(i))from the true outputy(i)?2.g \u2014qL(f(x(i);q),y(i))# How should we moveqto maximize loss?3.q q\u0000hg# Go the other way insteadreturnqFigure 5.5The stochastic gradient descent algorithm. Step 1 (computing the loss) is usedto report how well we are doing on",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 76,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute \u02c6y(i)=f(x(i);q)# What is our estimated output \u02c6y?Compute the lossL(\u02c6y(i),y(i))# How far off is \u02c6y(i))from the true outputy(i)?2.g \u2014qL(f(x(i);q),y(i))# How should we moveqto maximize loss?3.q q\u0000hg# Go the other way insteadreturnqFigure 5.5The stochastic gradient descent algorithm. Step 1 (computing the loss) is usedto report how well we are doing on the current tuple. The algorithm can terminate when itconverges (or when the gradient norm<\u270f), or when progress halts (for example when theloss starts going up on a held-out set).The learning ratehis ahyperparameterthat must be adjusted. If it\u2019s too high,hyperparameterthe learner will take steps that are too large, overshooting the minimum of the lossfunction. If it\u2019s too low, the learner will take steps that are too small, and take toolong to get to the minimum. It is common to start with a higher learning rate and thenslowly decrease it, so that it is a function of the iterationkof training; the notationhkcan be used to mean the value of the learning rate at iterationk.We\u2019ll discuss hyperparameters in more detail in Chapter 7, but brie\ufb02y they area special kind of parameter for any machine learning model. Unlike regular param-eters of a model (weights likewandb), which are learned by the algorithm fromthe training set, hyperparameters are special parameters chosen by the algorithmdesigner that affect how the algorithm works.",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 77,
      "token_count": 532,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 61\n\nWhere did that derivative come from?Using the chain rule!   f (x) = u(v(x)) Intuition (see the text for details)\n61x1x2x3\ny\nw1w2w3\u2211b\u03c3+1zaDerivative of the LossDerivative of the ActivationDerivative of the weighted sum!\"!#!=!\"!$!$!%!%!#!14CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELSe=a+dd = 2bL=ce31-2e=5d=2L=-10forward passabcFigure 7.10Computation graph for the functionL(a,b,c)=c(a+2b), with values for inputnodesa=3,b=1,c=\u00002, showing the forward pass computation ofL.off(x)is the derivative ofu(x)with respect tov(x)times the derivative ofv(x)withrespect tox:dfdx=dudv\u00b7dvdx(7.23)The chain rule extends to more than two functions. If computing the derivative of acomposite functionf(x)=u(v(w(x))), the derivative off(x)is:dfdx=dudv\u00b7dvdw\u00b7dwdx(7.24)Let\u2019s now compute the 3 derivatives we need. Since in the computation graphL=ce, we can directly compute the derivative\u2202L\u2202c:\u2202L\u2202c=e(7.25)For the other two, we\u2019ll need to use the chain rule:\u2202L\u2202a=\u2202L\u2202e\u2202e\u2202a\u2202L\u2202b=\u2202L\u2202e\u2202e\u2202d\u2202d\u2202b(7.26)Eq.7.26thus requires \ufb01ve intermediate derivatives:\u2202L\u2202e,\u2202L\u2202c,\u2202e\u2202a,\u2202e\u2202d, and\u2202d\u2202b,which are as follows (making use of the fact that the derivative of a sum is the sumof the derivatives):L=ce:\u2202L\u2202e=c,\u2202L\u2202c=ee=a+d:\u2202e\u2202a=1,\u2202e\u2202d=1d=2b:\u2202d\u2202b=2In the backward pass, we compute each of these partials along each edge of the graphfrom right to left, multiplying the necessary partials to result in the \ufb01nal derivativewe need. Thus we begin by annotating the \ufb01nal node with\u2202L\u2202L=1. Moving to theleft, we then compute\u2202L\u2202cand\u2202L\u2202e, and so on, until we have annotated the graph allthe way to the input variables. The forward pass conveniently already will havecomputed the values of the forward intermediate variables we need (likedande)\n\n## Page 62\n\nHow can I find that gradient for every weight in the network?These derivatives on the prior slide only give the updates for one weight layer: the last one! What about deeper networks?\u2022Lots of layers, different activation functions?Solution in the next lecture:\u2022Even more use of the chain rule!! \u2022Computation graphs and backward differentiation!62\n\n## Page 63\n\nSimple Neural Networks and Neural Language ModelsTraining Neural Nets: Overview\n\n## Page 64\n\nSimple Neural Networks and Neural Language ModelsComputation Graphs and Backward Differentiation",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 78,
      "token_count": 737,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 62\n\nHow can I find that gradient for every weight in the network?These derivatives on the prior slide only give the updates for one weight layer: the last one! What about deeper networks?\u2022Lots of layers, different activation functions?Solution in the next lecture:\u2022Even more use of the chain rule!! \u2022Computation graphs and backward differentiation!62\n\n## Page 63\n\nSimple Neural Networks and Neural Language ModelsTraining Neural Nets: Overview\n\n## Page 64\n\nSimple Neural Networks and Neural Language ModelsComputation Graphs and Backward Differentiation\n\n## Page 65\n\nWhy Computation GraphsFor training, we need the derivative of the loss with respect to each weight in every layer of the network \u2022But the loss is computed only at the very end of the network! Solution: error backpropagation (Rumelhart, Hinton, Williams, 1986) \u2022Backpropis a special case of backward differentiation\u2022Which relies on computation graphs. \n65\n\n## Page 66\n\nComputation GraphsA computation graph represents the process of computing a mathematical expression\n66\n\n## Page 67\n\nExample: \n67\ne=a+dd = 2bL=ceabcComputations:\n\n## Page 68\n\nExample: \n68\nComputations:\n\n## Page 69\n\nBackwards differentiation in computation graphsThe importance of the computation graph comes from the backward passThis is used to compute the derivatives that we\u2019ll need for the weight update.",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 79,
      "token_count": 302,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 66\n\nComputation GraphsA computation graph represents the process of computing a mathematical expression\n66\n\n## Page 67\n\nExample: \n67\ne=a+dd = 2bL=ceabcComputations:\n\n## Page 68\n\nExample: \n68\nComputations:\n\n## Page 69\n\nBackwards differentiation in computation graphsThe importance of the computation graph comes from the backward passThis is used to compute the derivatives that we\u2019ll need for the weight update. \n\n## Page 70\n\nExample\n70\nThe derivative !\"!#, tells us how much a small change in a affects L. We want:7.4\u2022TRAININGNEURALNETS13Or for a network with one hidden layer and softmax output, we could use the deriva-tive of the softmax loss from Eq.??:\u2202LCE\u2202wk=(\n{y=k}\u0000p(y=k|x))xk= \n{y=k}\u0000exp(wk\u00b7x+bk)PKj=1exp(wj\u00b7x+bj)!xk(7.22)But these derivatives only give correct updates for one weight layer: the last one!For deep networks, computing the gradients for each weight is much more complex,since we are computing the derivative with respect to weight parameters that appearall the way back in the very early layers of the network, even though the loss iscomputed only at the very end of the network.The solution to computing this gradient is an algorithm callederror backprop-agationorbackprop(Rumelhart et al., 1986). While backprop was invented spe-error back-propagationcially for neural networks, it turns out to be the same as a more general procedurecalledbackward differentiation, which depends on the notion ofcomputationgraphs. Let\u2019s see how that works in the next subsection.7.4.3 Computation GraphsA computation graph is a representation of the process of computing a mathematicalexpression, in which the computation is broken down into separate operations, eachof which is modeled as a node in a graph.Consider computing the functionL(a,b,c)=c(a+2b). If we make each of thecomponent addition and multiplication operations explicit, and add names (dande)for the intermediate outputs, the resulting series of computations is:d=2\u21e4be=a+dL=c\u21e4eWe can now represent this as a graph, with nodes for each operation, and di-rected edges showing the outputs from each operation as the inputs to the next, asin Fig.7.10. The simplest use of computation graphs is to compute the value ofthe function with some given inputs. In the \ufb01gure, we\u2019ve assumed the inputsa=3,b=1,c=\u00002, and we\u2019ve shown the result of theforward passto compute the re-sultL(3,1,\u00002)=\u000010. In the forward pass of a computation graph, we apply eachoperation left to right, passing the outputs of each computation as the input to thenext node.7.4.4 Backward differentiation on computation graphsThe importance of the computation graph comes from thebackward pass, whichis used to compute the derivatives that we\u2019ll need for the weight update. In thisexample our goal is to compute the derivative of the output functionLwith respectto each of the input variables, i.e.,\u2202L\u2202a,\u2202L\u2202b, and\u2202L\u2202c. The derivative\u2202L\u2202a, tells us howmuch a small change inaaffectsL.Backwards differentiation makes use of thechain rulein calculus. Suppose wechain ruleare computing the derivative of a composite functionf(x)=u(v(x)). The derivative",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 80,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 71",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 81,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "The chain ruleComputing the derivative of a composite function:f (x) = u(v(x))f (x) = u(v(w(x))) 14CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELSe=a+dd = 2bL=ce31-2e=5d=2L=-10forward passabcFigure 7.10Computation graph for the functionL(a,b,c)=c(a+2b), with values for inputnodesa=3,b=1,c=\u00002, showing the forward pass computation ofL.off(x)is the derivative ofu(x)with respect tov(x)times the derivative ofv(x)withrespect tox:dfdx=dudv\u00b7dvdx(7.23)The chain rule extends to more than two functions. If computing the derivative of acomposite functionf(x)=u(v(w(x))), the derivative off(x)is:dfdx=dudv\u00b7dvdw\u00b7dwdx(7.24)Let\u2019s now compute the 3 derivatives we need. Since in the computation graphL=ce, we can directly compute the derivative\u2202L\u2202c:\u2202L\u2202c=e(7.25)For the other two, we\u2019ll need to use the chain rule:\u2202L\u2202a=\u2202L\u2202e\u2202e\u2202a\u2202L\u2202b=\u2202L\u2202e\u2202e\u2202d\u2202d\u2202b(7.26)Eq.7.26thus requires \ufb01ve intermediate derivatives:\u2202L\u2202e,\u2202L\u2202c,\u2202e\u2202a,\u2202e\u2202d, and\u2202d\u2202b,which are as follows (making use of the fact that the derivative of a sum is the sumof the derivatives):L=ce:\u2202L\u2202e=c,\u2202L\u2202c=ee=a+d:\u2202e\u2202a=1,\u2202e\u2202d=1d=2b:\u2202d\u2202b=2In the backward pass, we compute each of these partials along each edge of the graphfrom right to left, multiplying the necessary partials to result in the \ufb01nal derivativewe need. Thus we begin by annotating the \ufb01nal node with\u2202L\u2202L=1. Moving to theleft, we then compute\u2202L\u2202cand\u2202L\u2202e, and so on, until we have annotated the graph allthe way to the input variables. The forward pass conveniently already will havecomputed the values of the forward intermediate variables we need (likedande)14CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELSe=a+dd = 2bL=ce31-2e=5d=2L=-10forward passabcFigure 7.10Computation graph for the functionL(a,b,c)=c(a+2b), with values for inputnodesa=3,b=1,c=\u00002, showing the forward pass computation ofL.off(x)is the derivative ofu(x)with respect tov(x)times the derivative ofv(x)withrespect tox:dfdx=dudv\u00b7dvdx(7.23)The chain rule extends to more than two functions. If computing the derivative of acomposite functionf(x)=u(v(w(x))), the derivative off(x)is:dfdx=dudv\u00b7dvdw\u00b7dwdx(7.24)Let\u2019s now compute the 3 derivatives we need. Since in the computation graphL=ce, we can directly compute the derivative\u2202L\u2202c:\u2202L\u2202c=e(7.25)For the other two, we\u2019ll need to use",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 82,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "Moving to theleft, we then compute\u2202L\u2202cand\u2202L\u2202e, and so on, until we have annotated the graph allthe way to the input variables. The forward pass conveniently already will havecomputed the values of the forward intermediate variables we need (likedande)14CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELSe=a+dd = 2bL=ce31-2e=5d=2L=-10forward passabcFigure 7.10Computation graph for the functionL(a,b,c)=c(a+2b), with values for inputnodesa=3,b=1,c=\u00002, showing the forward pass computation ofL.off(x)is the derivative ofu(x)with respect tov(x)times the derivative ofv(x)withrespect tox:dfdx=dudv\u00b7dvdx(7.23)The chain rule extends to more than two functions. If computing the derivative of acomposite functionf(x)=u(v(w(x))), the derivative off(x)is:dfdx=dudv\u00b7dvdw\u00b7dwdx(7.24)Let\u2019s now compute the 3 derivatives we need. Since in the computation graphL=ce, we can directly compute the derivative\u2202L\u2202c:\u2202L\u2202c=e(7.25)For the other two, we\u2019ll need to use the chain rule:\u2202L\u2202a=\u2202L\u2202e\u2202e\u2202a\u2202L\u2202b=\u2202L\u2202e\u2202e\u2202d\u2202d\u2202b(7.26)Eq.7.26thus requires \ufb01ve intermediate derivatives:\u2202L\u2202e,\u2202L\u2202c,\u2202e\u2202a,\u2202e\u2202d, and\u2202d\u2202b,which are as follows (making use of the fact that the derivative of a sum is the sumof the derivatives):L=ce:\u2202L\u2202e=c,\u2202L\u2202c=ee=a+d:\u2202e\u2202a=1,\u2202e\u2202d=1d=2b:\u2202d\u2202b=2In the backward pass, we compute each of these partials along each edge of the graphfrom right to left, multiplying the necessary partials to result in the \ufb01nal derivativewe need. Thus we begin by annotating the \ufb01nal node with\u2202L\u2202L=1. Moving to theleft, we then compute\u2202L\u2202cand\u2202L\u2202e, and so on, until we have annotated the graph allthe way to the input variables. The forward pass conveniently already will havecomputed the values of the forward intermediate variables we need (likedande)",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 83,
      "token_count": 595,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 72",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 84,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "14CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELSe=a+dd = 2bL=ce31-2e=5d=2L=-10forward passabcFigure 7.10Computation graph for the functionL(a,b,c)=c(a+2b), with values for inputnodesa=3,b=1,c=\u00002, showing the forward pass computation ofL.off(x)is the derivative ofu(x)with respect tov(x)times the derivative ofv(x)withrespect tox:dfdx=dudv\u00b7dvdx(7.23)The chain rule extends to more than two functions. If computing the derivative of acomposite functionf(x)=u(v(w(x))), the derivative off(x)is:dfdx=dudv\u00b7dvdw\u00b7dwdx(7.24)Let\u2019s now compute the 3 derivatives we need. Since in the computation graphL=ce, we can directly compute the derivative\u2202L\u2202c:\u2202L\u2202c=e(7.25)For the other two, we\u2019ll need to use the chain rule:\u2202L\u2202a=\u2202L\u2202e\u2202e\u2202a\u2202L\u2202b=\u2202L\u2202e\u2202e\u2202d\u2202d\u2202b(7.26)Eq.7.26thus requires \ufb01ve intermediate derivatives:\u2202L\u2202e,\u2202L\u2202c,\u2202e\u2202a,\u2202e\u2202d, and\u2202d\u2202b,which are as follows (making use of the fact that the derivative of a sum is the sumof the derivatives):L=ce:\u2202L\u2202e=c,\u2202L\u2202c=ee=a+d:\u2202e\u2202a=1,\u2202e\u2202d=1d=2b:\u2202d\u2202b=2In the backward pass, we compute each of these partials along each edge of the graphfrom right to left, multiplying the necessary partials to result in the \ufb01nal derivativewe need. Thus we begin by annotating the \ufb01nal node with\u2202L\u2202L=1. Moving to theleft, we then compute\u2202L\u2202cand\u2202L\u2202e, and so on, until we have annotated the graph allthe way to the input variables. The forward pass conveniently already will havecomputed the values of the forward intermediate variables we need (likedande)Example\n72",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 85,
      "token_count": 537,
      "chapter_title": ""
    }
  },
  {
    "content": "14CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELSe=a+dd = 2bL=ce31-2e=5d=2L=-10forward passabcFigure 7.10Computation graph for the functionL(a,b,c)=c(a+2b), with values for inputnodesa=3,b=1,c=\u00002, showing the forward pass computation ofL.off(x)is the derivative ofu(x)with respect tov(x)times the derivative ofv(x)withrespect tox:dfdx=dudv\u00b7dvdx(7.23)The chain rule extends to more than two functions. If computing the derivative of acomposite functionf(x)=u(v(w(x))), the derivative off(x)is:dfdx=dudv\u00b7dvdw\u00b7dwdx(7.24)Let\u2019s now compute the 3 derivatives we need. Since in the computation graphL=ce, we can directly compute the derivative\u2202L\u2202c:\u2202L\u2202c=e(7.25)For the other two, we\u2019ll need to use the chain rule:\u2202L\u2202a=\u2202L\u2202e\u2202e\u2202a\u2202L\u2202b=\u2202L\u2202e\u2202e\u2202d\u2202d\u2202b(7.26)Eq.7.26thus requires \ufb01ve intermediate derivatives:\u2202L\u2202e,\u2202L\u2202c,\u2202e\u2202a,\u2202e\u2202d, and\u2202d\u2202b,which are as follows (making use of the fact that the derivative of a sum is the sumof the derivatives):L=ce:\u2202L\u2202e=c,\u2202L\u2202c=ee=a+d:\u2202e\u2202a=1,\u2202e\u2202d=1d=2b:\u2202d\u2202b=2In the backward pass, we compute each of these partials along each edge of the graphfrom right to left, multiplying the necessary partials to result in the \ufb01nal derivativewe need. Thus we begin by annotating the \ufb01nal node with\u2202L\u2202L=1. Moving to theleft, we then compute\u2202L\u2202cand\u2202L\u2202e, and so on, until we have annotated the graph allthe way to the input variables. The forward pass conveniently already will havecomputed the values of the forward intermediate variables we need (likedande)14CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELSe=a+dd = 2bL=ce31-2e=5d=2L=-10forward passabcFigure 7.10Computation graph for the functionL(a,b,c)=c(a+2b), with values for inputnodesa=3,b=1,c=\u00002, showing the forward pass computation ofL.off(x)is the derivative ofu(x)with respect tov(x)times the derivative ofv(x)withrespect tox:dfdx=dudv\u00b7dvdx(7.23)The chain rule extends to more than two functions. If computing the derivative of acomposite functionf(x)=u(v(w(x))), the derivative off(x)is:dfdx=dudv\u00b7dvdw\u00b7dwdx(7.24)Let\u2019s now compute the 3 derivatives we need. Since in the computation graphL=ce, we can directly compute the derivative\u2202L\u2202c:\u2202L\u2202c=e(7.25)For the other two, we\u2019ll need to use the chain",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 86,
      "token_count": 770,
      "chapter_title": ""
    }
  },
  {
    "content": "Moving to theleft, we then compute\u2202L\u2202cand\u2202L\u2202e, and so on, until we have annotated the graph allthe way to the input variables. The forward pass conveniently already will havecomputed the values of the forward intermediate variables we need (likedande)14CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELSe=a+dd = 2bL=ce31-2e=5d=2L=-10forward passabcFigure 7.10Computation graph for the functionL(a,b,c)=c(a+2b), with values for inputnodesa=3,b=1,c=\u00002, showing the forward pass computation ofL.off(x)is the derivative ofu(x)with respect tov(x)times the derivative ofv(x)withrespect tox:dfdx=dudv\u00b7dvdx(7.23)The chain rule extends to more than two functions. If computing the derivative of acomposite functionf(x)=u(v(w(x))), the derivative off(x)is:dfdx=dudv\u00b7dvdw\u00b7dwdx(7.24)Let\u2019s now compute the 3 derivatives we need. Since in the computation graphL=ce, we can directly compute the derivative\u2202L\u2202c:\u2202L\u2202c=e(7.25)For the other two, we\u2019ll need to use the chain rule:\u2202L\u2202a=\u2202L\u2202e\u2202e\u2202a\u2202L\u2202b=\u2202L\u2202e\u2202e\u2202d\u2202d\u2202b(7.26)Eq.7.26thus requires \ufb01ve intermediate derivatives:\u2202L\u2202e,\u2202L\u2202c,\u2202e\u2202a,\u2202e\u2202d, and\u2202d\u2202b,which are as follows (making use of the fact that the derivative of a sum is the sumof the derivatives):L=ce:\u2202L\u2202e=c,\u2202L\u2202c=ee=a+d:\u2202e\u2202a=1,\u2202e\u2202d=1d=2b:\u2202d\u2202b=2In the backward pass, we compute each of these partials along each edge of the graphfrom right to left, multiplying the necessary partials to result in the \ufb01nal derivativewe need. Thus we begin by annotating the \ufb01nal node with\u2202L\u2202L=1. Moving to theleft, we then compute\u2202L\u2202cand\u2202L\u2202e, and so on, until we have annotated the graph allthe way to the input variables. The forward pass conveniently already will havecomputed the values of the forward intermediate variables we need (likedande)14CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELSe=a+dd = 2bL=ce31-2e=5d=2L=-10forward passabcFigure 7.10Computation graph for the functionL(a,b,c)=c(a+2b), with values for inputnodesa=3,b=1,c=\u00002, showing the forward pass computation ofL.off(x)is the derivative ofu(x)with respect tov(x)times the derivative ofv(x)withrespect tox:dfdx=dudv\u00b7dvdx(7.23)The chain rule extends to more than two functions. If computing the derivative of acomposite functionf(x)=u(v(w(x))), the derivative off(x)is:dfdx=dudv\u00b7dvdw\u00b7dwdx(7.24)Let\u2019s now compute the 3 derivatives we need. Since in the computation graphL=ce, we can directly compute the",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 87,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "partials to result in the \ufb01nal derivativewe need. Thus we begin by annotating the \ufb01nal node with\u2202L\u2202L=1. Moving to theleft, we then compute\u2202L\u2202cand\u2202L\u2202e, and so on, until we have annotated the graph allthe way to the input variables. The forward pass conveniently already will havecomputed the values of the forward intermediate variables we need (likedande)14CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELSe=a+dd = 2bL=ce31-2e=5d=2L=-10forward passabcFigure 7.10Computation graph for the functionL(a,b,c)=c(a+2b), with values for inputnodesa=3,b=1,c=\u00002, showing the forward pass computation ofL.off(x)is the derivative ofu(x)with respect tov(x)times the derivative ofv(x)withrespect tox:dfdx=dudv\u00b7dvdx(7.23)The chain rule extends to more than two functions. If computing the derivative of acomposite functionf(x)=u(v(w(x))), the derivative off(x)is:dfdx=dudv\u00b7dvdw\u00b7dwdx(7.24)Let\u2019s now compute the 3 derivatives we need. Since in the computation graphL=ce, we can directly compute the derivative\u2202L\u2202c:\u2202L\u2202c=e(7.25)For the other two, we\u2019ll need to use the chain rule:\u2202L\u2202a=\u2202L\u2202e\u2202e\u2202a\u2202L\u2202b=\u2202L\u2202e\u2202e\u2202d\u2202d\u2202b(7.26)Eq.7.26thus requires \ufb01ve intermediate derivatives:\u2202L\u2202e,\u2202L\u2202c,\u2202e\u2202a,\u2202e\u2202d, and\u2202d\u2202b,which are as follows (making use of the fact that the derivative of a sum is the sumof the derivatives):L=ce:\u2202L\u2202e=c,\u2202L\u2202c=ee=a+d:\u2202e\u2202a=1,\u2202e\u2202d=1d=2b:\u2202d\u2202b=2In the backward pass, we compute each of these partials along each edge of the graphfrom right to left, multiplying the necessary partials to result in the \ufb01nal derivativewe need. Thus we begin by annotating the \ufb01nal node with\u2202L\u2202L=1. Moving to theleft, we then compute\u2202L\u2202cand\u2202L\u2202e, and so on, until we have annotated the graph allthe way to the input variables. The forward pass conveniently already will havecomputed the values of the forward intermediate variables we need (likedande)",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 88,
      "token_count": 629,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 73\n\nExample\n73\n14CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELSe=a+dd = 2bL=ce31-2e=5d=2L=-10forward passabcFigure 7.10Computation graph for the functionL(a,b,c)=c(a+2b), with values for inputnodesa=3,b=1,c=\u00002, showing the forward pass computation ofL.off(x)is the derivative ofu(x)with respect tov(x)times the derivative ofv(x)withrespect tox:dfdx=dudv\u00b7dvdx(7.23)The chain rule extends to more than two functions. If computing the derivative of acomposite functionf(x)=u(v(w(x))), the derivative off(x)is:dfdx=dudv\u00b7dvdw\u00b7dwdx(7.24)Let\u2019s now compute the 3 derivatives we need. Since in the computation graphL=ce, we can directly compute the derivative\u2202L\u2202c:\u2202L\u2202c=e(7.25)For the other two, we\u2019ll need to use the chain rule:\u2202L\u2202a=\u2202L\u2202e\u2202e\u2202a\u2202L\u2202b=\u2202L\u2202e\u2202e\u2202d\u2202d\u2202b(7.26)Eq.7.26thus requires \ufb01ve intermediate derivatives:\u2202L\u2202e,\u2202L\u2202c,\u2202e\u2202a,\u2202e\u2202d, and\u2202d\u2202b,which are as follows (making use of the fact that the derivative of a sum is the sumof the derivatives):L=ce:\u2202L\u2202e=c,\u2202L\u2202c=ee=a+d:\u2202e\u2202a=1,\u2202e\u2202d=1d=2b:\u2202d\u2202b=2In the backward pass, we compute each of these partials along each edge of the graphfrom right to left, multiplying the necessary partials to result in the \ufb01nal derivativewe need. Thus we begin by annotating the \ufb01nal node with\u2202L\u2202L=1. Moving to theleft, we then compute\u2202L\u2202cand\u2202L\u2202e, and so on, until we have annotated the graph allthe way to the input variables. The forward pass conveniently already will havecomputed the values of the forward intermediate variables we need (likedande)",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 89,
      "token_count": 543,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 74",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 90,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Examplee=d+ad = 2bL=cea=3b=1e=5d=2L=-10 abcbackward passc=-2",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 91,
      "token_count": 32,
      "chapter_title": ""
    }
  },
  {
    "content": "7414CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELSe=a+dd = 2bL=ce31-2e=5d=2L=-10forward passabcFigure 7.10Computation graph for the functionL(a,b,c)=c(a+2b), with values for inputnodesa=3,b=1,c=\u00002, showing the forward pass computation ofL.off(x)is the derivative ofu(x)with respect tov(x)times the derivative ofv(x)withrespect tox:dfdx=dudv\u00b7dvdx(7.23)The chain rule extends to more than two functions. If computing the derivative of acomposite functionf(x)=u(v(w(x))), the derivative off(x)is:dfdx=dudv\u00b7dvdw\u00b7dwdx(7.24)Let\u2019s now compute the 3 derivatives we need. Since in the computation graphL=ce, we can directly compute the derivative\u2202L\u2202c:\u2202L\u2202c=e(7.25)For the other two, we\u2019ll need to use the chain rule:\u2202L\u2202a=\u2202L\u2202e\u2202e\u2202a\u2202L\u2202b=\u2202L\u2202e\u2202e\u2202d\u2202d\u2202b(7.26)Eq.7.26thus requires \ufb01ve intermediate derivatives:\u2202L\u2202e,\u2202L\u2202c,\u2202e\u2202a,\u2202e\u2202d, and\u2202d\u2202b,which are as follows (making use of the fact that the derivative of a sum is the sumof the derivatives):L=ce:\u2202L\u2202e=c,\u2202L\u2202c=ee=a+d:\u2202e\u2202a=1,\u2202e\u2202d=1d=2b:\u2202d\u2202b=2In the backward pass, we compute each of these partials along each edge of the graphfrom right to left, multiplying the necessary partials to result in the \ufb01nal derivativewe need. Thus we begin by annotating the \ufb01nal node with\u2202L\u2202L=1. Moving to theleft, we then compute\u2202L\u2202cand\u2202L\u2202e, and so on, until we have annotated the graph allthe way to the input variables. The forward pass conveniently already will havecomputed the values of the forward intermediate variables we need (likedande)14CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELSe=a+dd = 2bL=ce31-2e=5d=2L=-10forward passabcFigure 7.10Computation graph for the functionL(a,b,c)=c(a+2b), with values for inputnodesa=3,b=1,c=\u00002, showing the forward pass computation ofL.off(x)is the derivative ofu(x)with respect tov(x)times the derivative ofv(x)withrespect tox:dfdx=dudv\u00b7dvdx(7.23)The chain rule extends to more than two functions. If computing the derivative of acomposite functionf(x)=u(v(w(x))), the derivative off(x)is:dfdx=dudv\u00b7dvdw\u00b7dwdx(7.24)Let\u2019s now compute the 3 derivatives we need. Since in the computation graphL=ce, we can directly compute the derivative\u2202L\u2202c:\u2202L\u2202c=e(7.25)For the other two, we\u2019ll need to use the chain",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 92,
      "token_count": 771,
      "chapter_title": ""
    }
  },
  {
    "content": "Moving to theleft, we then compute\u2202L\u2202cand\u2202L\u2202e, and so on, until we have annotated the graph allthe way to the input variables. The forward pass conveniently already will havecomputed the values of the forward intermediate variables we need (likedande)14CHAPTER7\u2022NEURALNETWORKS ANDNEURALLANGUAGEMODELSe=a+dd = 2bL=ce31-2e=5d=2L=-10forward passabcFigure 7.10Computation graph for the functionL(a,b,c)=c(a+2b), with values for inputnodesa=3,b=1,c=\u00002, showing the forward pass computation ofL.off(x)is the derivative ofu(x)with respect tov(x)times the derivative ofv(x)withrespect tox:dfdx=dudv\u00b7dvdx(7.23)The chain rule extends to more than two functions. If computing the derivative of acomposite functionf(x)=u(v(w(x))), the derivative off(x)is:dfdx=dudv\u00b7dvdw\u00b7dwdx(7.24)Let\u2019s now compute the 3 derivatives we need. Since in the computation graphL=ce, we can directly compute the derivative\u2202L\u2202c:\u2202L\u2202c=e(7.25)For the other two, we\u2019ll need to use the chain rule:\u2202L\u2202a=\u2202L\u2202e\u2202e\u2202a\u2202L\u2202b=\u2202L\u2202e\u2202e\u2202d\u2202d\u2202b(7.26)Eq.7.26thus requires \ufb01ve intermediate derivatives:\u2202L\u2202e,\u2202L\u2202c,\u2202e\u2202a,\u2202e\u2202d, and\u2202d\u2202b,which are as follows (making use of the fact that the derivative of a sum is the sumof the derivatives):L=ce:\u2202L\u2202e=c,\u2202L\u2202c=ee=a+d:\u2202e\u2202a=1,\u2202e\u2202d=1d=2b:\u2202d\u2202b=2In the backward pass, we compute each of these partials along each edge of the graphfrom right to left, multiplying the necessary partials to result in the \ufb01nal derivativewe need. Thus we begin by annotating the \ufb01nal node with\u2202L\u2202L=1. Moving to theleft, we then compute\u2202L\u2202cand\u2202L\u2202e, and so on, until we have annotated the graph allthe way to the input variables. The forward pass conveniently already will havecomputed the values of the forward intermediate variables we need (likedande)",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 93,
      "token_count": 595,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 75\n\nExample\n75\n\n## Page 76\n\nBackward differentiation on a two layer network\n76\u03c3W[2]W[1]y\nx2x1Sigmoid activationReLUactivation11b[1]b[2]7.4\u2022TRAININGNEURALNETS15to compute these derivatives. Fig.7.11shows the backward pass. At each node weneed to compute the local partial derivative with respect to the parent, multiply it bythe partial derivative that is being passed down from the parent, and then pass it tothe child.e=d+ad = 2bL=cea=3b=1e=5d=2L=-10 \u2202L=1\u2202L\u2202L=-4\u2202b\u2202L=-2\u2202dabc\u2202L=-2\u2202a\n\u2202L=5\u2202c\u2202L=-2\u2202e\u2202L=-2\u2202e\u2202e=1\u2202d\u2202L=5\u2202c\u2202d=2\u2202b\u2202e=1\u2202abackward passc=-2Figure 7.11Computation graph for the functionL(a,b,c)=c(a+2b), showing the back-ward pass computation of\u2202L\u2202a,\u2202L\u2202b, and\u2202L\u2202c.Backward differentiation for a neural networkOf course computation graphs for real neural networks are much more complex.Fig.7.12shows a sample computation graph for a 2-layer neural network withn0=2,n1=2, andn2=1, assuming binary classi\ufb01cation and hence using a sigmoidoutput unit for simplicity. The function that the computation graph is computing is:z[1]=W[1]x+b[1]a[1]=ReLU(z[1])z[2]=W[2]a[1]+b[2]a[2]=s(z[2])\u02c6y=a[2](7.27)The weights that need updating (those for which we need to know the partialderivative of the loss function) are shown in orange. In order to do the backwardpass, we\u2019ll need to know the derivatives of all the functions in the graph. We alreadysaw in Section??the derivative of the sigmoids:ds(z)dz=s(z)(1\u0000s(z))(7.28)We\u2019ll also need the derivatives of each of the other activation functions. Thederivative of tanh is:dtanh(z)dz=1\u0000tanh2(z)(7.29)The derivative of the ReLU isdReLU(z)dz=\u21e20f or x<01f or x\u00000(7.30)",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 94,
      "token_count": 582,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 77",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 95,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Backward differentiation on a two layer network\n777.4\u2022TRAININGNEURALNETS15to compute these derivatives. Fig.7.11shows the backward pass. At each node weneed to compute the local partial derivative with respect to the parent, multiply it bythe partial derivative that is being passed down from the parent, and then pass it tothe child.e=d+ad = 2bL=cea=3b=1e=5d=2L=-10 \u2202L=1\u2202L\u2202L=-4\u2202b\u2202L=-2\u2202dabc\u2202L=-2\u2202a\n\u2202L=5\u2202c\u2202L=-2\u2202e\u2202L=-2\u2202e\u2202e=1\u2202d\u2202L=5\u2202c\u2202d=2\u2202b\u2202e=1\u2202abackward passc=-2Figure 7.11Computation graph for the functionL(a,b,c)=c(a+2b), showing the back-ward pass computation of\u2202L\u2202a,\u2202L\u2202b, and\u2202L\u2202c.Backward differentiation for a neural networkOf course computation graphs for real neural networks are much more complex.Fig.7.12shows a sample computation graph for a 2-layer neural network withn0=2,n1=2, andn2=1, assuming binary classi\ufb01cation and hence using a sigmoidoutput unit for simplicity. The function that the computation graph is computing is:z[1]=W[1]x+b[1]a[1]=ReLU(z[1])z[2]=W[2]a[1]+b[2]a[2]=s(z[2])\u02c6y=a[2](7.27)The weights that need updating (those for which we need to know the partialderivative of the loss function) are shown in orange. In order to do the backwardpass, we\u2019ll need to know the derivatives of all the functions in the graph. We alreadysaw in Section??the derivative of the sigmoids:ds(z)dz=s(z)(1\u0000s(z))(7.28)We\u2019ll also need the derivatives of each of the other activation functions. Thederivative of tanh is:dtanh(z)dz=1\u0000tanh2(z)(7.29)The derivative of the ReLU isdReLU(z)dz=\u21e20f or x<01f or x\u00000(7.30)\n7.4\u2022TRAININGNEURALNETS15to compute these derivatives. Fig.7.11shows the backward pass. At each node weneed to compute the local partial derivative with respect to the parent, multiply it bythe partial derivative that is being passed down from the parent, and then pass it tothe child.e=d+ad = 2bL=cea=3b=1e=5d=2L=-10 \u2202L=1\u2202L\u2202L=-4\u2202b\u2202L=-2\u2202dabc\u2202L=-2\u2202a",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 96,
      "token_count": 662,
      "chapter_title": ""
    }
  },
  {
    "content": "7.4\u2022TRAININGNEURALNETS15to compute these derivatives. Fig.7.11shows the backward pass. At each node weneed to compute the local partial derivative with respect to the parent, multiply it bythe partial derivative that is being passed down from the parent, and then pass it tothe child.e=d+ad = 2bL=cea=3b=1e=5d=2L=-10 \u2202L=1\u2202L\u2202L=-4\u2202b\u2202L=-2\u2202dabc\u2202L=-2\u2202a\n\u2202L=5\u2202c\u2202L=-2\u2202e\u2202L=-2\u2202e\u2202e=1\u2202d\u2202L=5\u2202c\u2202d=2\u2202b\u2202e=1\u2202abackward passc=-2Figure 7.11Computation graph for the functionL(a,b,c)=c(a+2b), showing the back-ward pass computation of\u2202L\u2202a,\u2202L\u2202b, and\u2202L\u2202c.Backward differentiation for a neural networkOf course computation graphs for real neural networks are much more complex.Fig.7.12shows a sample computation graph for a 2-layer neural network withn0=2,n1=2, andn2=1, assuming binary classi\ufb01cation and hence using a sigmoidoutput unit for simplicity. The function that the computation graph is computing is:z[1]=W[1]x+b[1]a[1]=ReLU(z[1])z[2]=W[2]a[1]+b[2]a[2]=s(z[2])\u02c6y=a[2](7.27)The weights that need updating (those for which we need to know the partialderivative of the loss function) are shown in orange. In order to do the backwardpass, we\u2019ll need to know the derivatives of all the functions in the graph. We alreadysaw in Section??the derivative of the sigmoids:ds(z)dz=s(z)(1\u0000s(z))(7.28)We\u2019ll also need the derivatives of each of the other activation functions. Thederivative of tanh is:dtanh(z)dz=1\u0000tanh2(z)(7.29)The derivative of the ReLU isdReLU(z)dz=\u21e20f or z<01f or z\u00000(7.30)",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 97,
      "token_count": 529,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 78",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 98,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Backward differentiation on a 2-layer network \n7.4\u2022TRAININGNEURALNETS15to compute these derivatives. Fig.7.11shows the backward pass. At each node weneed to compute the local partial derivative with respect to the parent, multiply it bythe partial derivative that is being passed down from the parent, and then pass it tothe child.e=d+ad = 2bL=cea=3b=1e=5d=2L=-10 \u2202L=1\u2202L\u2202L=-4\u2202b\u2202L=-2\u2202dabc\u2202L=-2\u2202a\n\u2202L=5\u2202c\u2202L=-2\u2202e\u2202L=-2\u2202e\u2202e=1\u2202d\u2202L=5\u2202c\u2202d=2\u2202b\u2202e=1\u2202abackward passc=-2Figure 7.11Computation graph for the functionL(a,b,c)=c(a+2b), showing the back-ward pass computation of\u2202L\u2202a,\u2202L\u2202b, and\u2202L\u2202c.Backward differentiation for a neural networkOf course computation graphs for real neural networks are much more complex.Fig.7.12shows a sample computation graph for a 2-layer neural network withn0=2,n1=2, andn2=1, assuming binary classi\ufb01cation and hence using a sigmoidoutput unit for simplicity. The function that the computation graph is computing is:z[1]=W[1]x+b[1]a[1]=ReLU(z[1])z[2]=W[2]a[1]+b[2]a[2]=s(z[2])\u02c6y=a[2](7.27)The weights that need updating (those for which we need to know the partialderivative of the loss function) are shown in orange. In order to do the backwardpass, we\u2019ll need to know the derivatives of all the functions in the graph. We alreadysaw in Section??the derivative of the sigmoids:ds(z)dz=s(z)(1\u0000s(z))(7.28)We\u2019ll also need the derivatives of each of the other activation functions. Thederivative of tanh is:dtanh(z)dz=1\u0000tanh2(z)(7.29)The derivative of the ReLU isdReLU(z)dz=\u21e20f or z<01f or z\u00000(7.30)7.4\u2022TRAININGNEURALNETS15to compute these derivatives. Fig.7.11shows the backward pass. At each node weneed to compute the local partial derivative with respect to the parent, multiply it bythe partial derivative that is being passed down from the parent, and then pass it tothe child.e=d+ad = 2bL=cea=3b=1e=5d=2L=-10 \u2202L=1\u2202L\u2202L=-4\u2202b\u2202L=-2\u2202dabc\u2202L=-2\u2202a",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 99,
      "token_count": 663,
      "chapter_title": ""
    }
  },
  {
    "content": "\u2202L=5\u2202c\u2202L=-2\u2202e\u2202L=-2\u2202e\u2202e=1\u2202d\u2202L=5\u2202c\u2202d=2\u2202b\u2202e=1\u2202abackward passc=-2Figure 7.11Computation graph for the functionL(a,b,c)=c(a+2b), showing the back-ward pass computation of\u2202L\u2202a,\u2202L\u2202b, and\u2202L\u2202c.Backward differentiation for a neural networkOf course computation graphs for real neural networks are much more complex.Fig.7.12shows a sample computation graph for a 2-layer neural network withn0=2,n1=2, andn2=1, assuming binary classi\ufb01cation and hence using a sigmoidoutput unit for simplicity. The function that the computation graph is computing is:z[1]=W[1]x+b[1]a[1]=ReLU(z[1])z[2]=W[2]a[1]+b[2]a[2]=s(z[2])\u02c6y=a[2](7.27)The weights that need updating (those for which we need to know the partialderivative of the loss function) are shown in orange. In order to do the backwardpass, we\u2019ll need to know the derivatives of all the functions in the graph. We alreadysaw in Section??the derivative of the sigmoids:ds(z)dz=s(z)(1\u0000s(z))(7.28)We\u2019ll also need the derivatives of each of the other activation functions. Thederivative of tanh is:dtanh(z)dz=1\u0000tanh2(z)(7.29)The derivative of the ReLU isdReLU(z)dz=\u21e20f or z<01f or z\u00000(7.30)",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 100,
      "token_count": 404,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 79\n\nStarting off the backward pass: !\"!#(I'll write \ud835\udc4efor\ud835\udc4e[\"]and \ud835\udc67for \ud835\udc67[\"])\ud835\udc3f!\ud835\udc66,\ud835\udc66=\u2212ylog(!\ud835\udc66)+1\u2212\ud835\udc66log(1\u2212!\ud835\udc66)\ud835\udc3f\ud835\udc4e,\ud835\udc66=\u2212ylog\ud835\udc4e+1\u2212\ud835\udc66log(1\u2212\ud835\udc4e)!\"!$=!\"!#!#!$\ud835\udf15\ud835\udc3f\ud835\udf15\ud835\udc4e=\u2212\ud835\udc66\ud835\udf15log\ud835\udc4e\ud835\udf15\ud835\udc4e+(1\u2212y)\ud835\udf15log1\u2212\ud835\udc4e\ud835\udf15\ud835\udc4e=\u2212\ud835\udc661\ud835\udc4e+1\u2212y11\u2212\ud835\udc4e\u22121=\u2212\ud835\udc66\ud835\udc4e+\ud835\udc66\u221211\u2212\ud835\udc4e\ud835\udf15\ud835\udc4e\ud835\udf15\ud835\udc67=\ud835\udc4e(1\u2212\ud835\udc4e)\ud835\udf15\ud835\udc3f\ud835\udf15\ud835\udc67=\u2212\ud835\udc66\ud835\udc4e+\ud835\udc66\u221211\u2212\ud835\udc4e\ud835\udc4e1\u2212\ud835\udc4e=a\u2212y7.4\u2022TRAININGNEURALNETS15to compute these derivatives. Fig.7.11shows the backward pass. At each node weneed to compute the local partial derivative with respect to the parent, multiply it bythe partial derivative that is being passed down from the parent, and then pass it tothe child.e=d+ad = 2bL=cea=3b=1e=5d=2L=-10 \u2202L=1\u2202L\u2202L=-4\u2202b\u2202L=-2\u2202dabc\u2202L=-2\u2202a\n\u2202L=5\u2202c\u2202L=-2\u2202e\u2202L=-2\u2202e\u2202e=1\u2202d\u2202L=5\u2202c\u2202d=2\u2202b\u2202e=1\u2202abackward passc=-2Figure 7.11Computation graph for the functionL(a,b,c)=c(a+2b), showing the back-ward pass computation of\u2202L\u2202a,\u2202L\u2202b, and\u2202L\u2202c.Backward differentiation for a neural networkOf course computation graphs for real neural networks are much more complex.Fig.7.12shows a sample computation graph for a 2-layer neural network withn0=2,n1=2, andn2=1, assuming binary classi\ufb01cation and hence using a sigmoidoutput unit for simplicity. The function that the computation graph is computing is:z[1]=W[1]x+b[1]a[1]=ReLU(z[1])z[2]=W[2]a[1]+b[2]a[2]=s(z[2])\u02c6y=a[2](7.27)The weights that need updating (those for which we need to know the partialderivative of the loss function) are shown in orange. In order to do the backwardpass, we\u2019ll need to know the derivatives of all the functions in the graph. We alreadysaw in Section??the derivative of the sigmoids:ds(z)dz=s(z)(1\u0000s(z))(7.28)We\u2019ll also need the derivatives of each of the other activation functions. Thederivative of tanh is:dtanh(z)dz=1\u0000tanh2(z)(7.29)The derivative of the ReLU isdReLU(z)dz=\u21e20f or x<01f or x\u00000(7.30)",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 101,
      "token_count": 792,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 80\n\nSummaryFor training, we need the derivative of the loss with respect to weights in early layers of the network \u2022But loss is computed only at the very end of the network! Solution: backward differentiationGiven a computation graph and the derivatives of all the functions in it we can automatically compute the derivative of the loss with respect to these early weights.\n80\n\n## Page 81\n\nSimple Neural Networks and Neural Language ModelsComputation Graphs and Backward Differentiation",
    "metadata": {
      "source": "7_NN_Apr_28_2021",
      "chunk_id": 102,
      "token_count": 96,
      "chapter_title": ""
    }
  }
]