[
  {
    "content": "# rnnjan25\n\n## Page 1\n\nRNNs and LSTMsSimple Recurrent Networks (RNNs or Elman Nets)\n\n## Page 2\n\nModeling Time in Neural NetworksLanguage is inherently temporalYet the simple NLP classi\ufb01ers we've  seen (for example for sen=ment analysis) mostly ignore =me\u2022(Feedforward neural LMs (and the transformers we'll see later) use a \"moving window\" approach to =me.)Here we introduce a deep learning architecture with a di\ufb00erent way of represen=ng =me\u2022 RNNs and their variants like LSTMs\n\n## Page 3\n\nRecurrent Neural Networks (RNNs)Any network that contains a cycle within its network connec1ons.The value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. \n\n## Page 4\n\nSimple Recurrent Nets (Elman nets)xtythtThe hidden layer has a recurrence as part of its inputThe ac1va1on value ht depends on xt but also ht-1!\n\n## Page 5\n\nForward inference in simple RNNsVery similar to the feedforward networks we've seen!",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 0,
      "token_count": 252,
      "chapter_title": "rnnjan25"
    }
  },
  {
    "content": "## Page 6",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 1,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "+UVWyt\nxththt-1Simple recurrent neural network illustrated as a feedforward network 8.1\u2022RECURRENTNEURALNETWORKS3+UVWyt\nxththt-1Figure 8.2Simple recurrent neural network illustrated as a feedforward network. The hid-den layerht\u00001from the prior time step is multiplied by weight matrixUand then added tothe feedforward component from the current time step.output vector.ht=g(Uht\u00001+Wxt)(8.1)yt=f(Vht)(8.2)Let\u2019s refer to the input, hidden and output layer dimensions asdin,dh, anddoutrespectively. Given this, our three parameter matrices are:W2Rdh\u21e5din,U2Rdh\u21e5dh,andV2Rdout\u21e5dh.We computeytvia a softmax computation that gives a probability distributionover the possible output classes.yt=softmax(Vht)(8.3)The fact that the computation at timetrequires the value of the hidden layer fromtimet\u00001 mandates an incremental inference algorithm that proceeds from the startof the sequence to the end as illustrated in Fig.8.3. The sequential nature of simplerecurrent networks can also be seen byunrollingthe network in time as is shown inFig.8.4. In this \ufb01gure, the various layers of units are copied for each time step toillustrate that they will have differing values over time. However, the various weightmatrices are shared across time.functionFORWARDRNN(x,network)returnsoutput sequenceyh0 0fori 1toLENGTH(x)dohi g(Uhi\u00001+Wxi)yi f(Vhi)returnyFigure 8.3Forward inference in a simple recurrent network. The matricesU,VandWareshared across time, while new values forhandyare calculated with each time step.8.1.2 TrainingAs with feedforward networks, we\u2019ll use a training set, a loss function, and back-propagation to obtain the gradients needed to adjust the weights in these recurrentnetworks. As shown in Fig.8.2, we now have 3 sets of weights to update:W, the8.1\u2022RECURRENTNEURALNETWORKS3+UVWyt",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 2,
      "token_count": 474,
      "chapter_title": ""
    }
  },
  {
    "content": "xththt-1Figure 8.2Simple recurrent neural network illustrated as a feedforward network. The hid-den layerht\u00001from the prior time step is multiplied by weight matrixUand then added tothe feedforward component from the current time step.output vector.ht=g(Uht\u00001+Wxt)(8.1)yt=f(Vht)(8.2)Let\u2019s refer to the input, hidden and output layer dimensions asdin,dh, anddoutrespectively. Given this, our three parameter matrices are:W2Rdh\u21e5din,U2Rdh\u21e5dh,andV2Rdout\u21e5dh.We computeytvia a softmax computation that gives a probability distributionover the possible output classes.yt=softmax(Vht)(8.3)The fact that the computation at timetrequires the value of the hidden layer fromtimet\u00001 mandates an incremental inference algorithm that proceeds from the startof the sequence to the end as illustrated in Fig.8.3. The sequential nature of simplerecurrent networks can also be seen byunrollingthe network in time as is shown inFig.8.4. In this \ufb01gure, the various layers of units are copied for each time step toillustrate that they will have differing values over time. However, the various weightmatrices are shared across time.functionFORWARDRNN(x,network)returnsoutput sequenceyh0 0fori 1toLENGTH(x)dohi g(Uhi\u00001+Wxi)yi f(Vhi)returnyFigure 8.3Forward inference in a simple recurrent network. The matricesU,VandWareshared across time, while new values forhandyare calculated with each time step.8.1.2 TrainingAs with feedforward networks, we\u2019ll use a training set, a loss function, and back-propagation to obtain the gradients needed to adjust the weights in these recurrentnetworks. As shown in Fig.8.2, we now have 3 sets of weights to update:W, the",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 3,
      "token_count": 421,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\nInference has to be incrementalComputing h at time t requires that we first computed h at the previous time step!8.1\u2022RECURRENTNEURALNETWORKS3+UVWyt\nxththt-1Figure 8.2Simple recurrent neural network illustrated as a feedforward network. The hid-den layerht\u00001from the prior time step is multiplied by weight matrixUand then added tothe feedforward component from the current time step.output vector.ht=g(Uht\u00001+Wxt)(8.1)yt=f(Vht)(8.2)Let\u2019s refer to the input, hidden and output layer dimensions asdin,dh, anddoutrespectively. Given this, our three parameter matrices are:W2Rdh\u21e5din,U2Rdh\u21e5dh,andV2Rdout\u21e5dh.We computeytvia a softmax computation that gives a probability distributionover the possible output classes.yt=softmax(Vht)(8.3)The fact that the computation at timetrequires the value of the hidden layer fromtimet\u00001 mandates an incremental inference algorithm that proceeds from the startof the sequence to the end as illustrated in Fig.8.3. The sequential nature of simplerecurrent networks can also be seen byunrollingthe network in time as is shown inFig.8.4. In this \ufb01gure, the various layers of units are copied for each time step toillustrate that they will have differing values over time. However, the various weightmatrices are shared across time.functionFORWARDRNN(x,network)returnsoutput sequenceyh0 0fori 1toLENGTH(x)dohi g(Uhi\u00001+Wxi)yi f(Vhi)returnyFigure 8.3Forward inference in a simple recurrent network. The matricesU,VandWareshared across time, while new values forhandyare calculated with each time step.8.1.2 TrainingAs with feedforward networks, we\u2019ll use a training set, a loss function, and back-propagation to obtain the gradients needed to adjust the weights in these recurrentnetworks. As shown in Fig.8.2, we now have 3 sets of weights to update:W, the\n\n## Page 8\n\nTraining in simple RNNs+UVWyt\nxththt-1Just like feedforward training:\u2022training set, \u2022a loss func<on, \u2022backpropaga<on Weights that need to be updated:\u2022W, the weights from the input layer to the hidden layer, \u2022U, the weights from the previous hidden layer to the current hidden layer, \u2022V, the weights from the hidden layer to the output layer. \n\n## Page 9\n\nUVWUVWUVW\nx1x2x3y1y2y3\nh1h3h2\nh0Training in simple RNNs: unrolling in AmeUnlike feedforward networks:1. To compute loss func0on for the output at 0me t we need the hidden layer from 0me t \u2212 1. 2. hidden layer at 0me t in\ufb02uences the output at 0me t and hidden layer at 0me t+1 (and hence the output and loss at t+1).So: to  measure error accruing to ht,  \u2022need to know its in\ufb02uence on both the current output as well as the ones that follow.",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 4,
      "token_count": 727,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\nTraining in simple RNNs+UVWyt\nxththt-1Just like feedforward training:\u2022training set, \u2022a loss func<on, \u2022backpropaga<on Weights that need to be updated:\u2022W, the weights from the input layer to the hidden layer, \u2022U, the weights from the previous hidden layer to the current hidden layer, \u2022V, the weights from the hidden layer to the output layer. \n\n## Page 9\n\nUVWUVWUVW\nx1x2x3y1y2y3\nh1h3h2\nh0Training in simple RNNs: unrolling in AmeUnlike feedforward networks:1. To compute loss func0on for the output at 0me t we need the hidden layer from 0me t \u2212 1. 2. hidden layer at 0me t in\ufb02uences the output at 0me t and hidden layer at 0me t+1 (and hence the output and loss at t+1).So: to  measure error accruing to ht,  \u2022need to know its in\ufb02uence on both the current output as well as the ones that follow. \n\n## Page 10\n\nUnrolling in Ame (2)We unroll a recurrent network into a feedforward computa3onal graph elimina3ng recurrence1.Given an input sequence, 2.Generate an unrolled feedforward network speci\ufb01c to input 3.Use graph to train weights directly via ordinary backprop (or can do forward inference)UVWUVWUVW\nx1x2x3y1y2y3\nh1h3h2\nh0\n\n## Page 11\n\nRNNs and LSTMsSimple Recurrent Networks (RNNs or Elman Nets)\n\n## Page 12\n\nRNNs and LSTMsRNNs as Language Models",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 5,
      "token_count": 409,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 6,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Reminder: Language Modeling8.2\u2022RNNSA SLANGUAGEMODELS5For applications that involve much longer input sequences, such as speech recog-nition, character-level processing, or streaming continuous inputs, unrolling an en-tire input sequence may not be feasible. In these cases, we can unroll the input intomanageable \ufb01xed-length segments and treat each segment as a distinct training item.8.2 RNNs as Language ModelsLet\u2019s see how to apply RNNs to the language modeling task. Recall from Chapter 3that language models predict the next word in a sequence given some precedingcontext. For example, if the preceding context is\u201cThanks for all the\u201dand we wantto know how likely the next word is\u201c\ufb01sh\u201dwe would compute:P(\ufb01sh|Thanks for all the)Language models give us the ability to assign such a conditional probability to everypossible next word, giving us a distribution over the entire vocabulary. We can alsoassign probabilities to entire sequences by combining these conditional probabilitieswith the chain rule:P(w1:n)=nYi=1P(wi|w<i)The n-gram language models of Chapter 3 compute the probability of a word givencounts of its occurrence with then\u00001 prior words. The context is thus of sizen\u00001.For the feedforward language models of Chapter 7, the context is the window size.RNN language models (Mikolov et al.,2010) process the input sequence oneword at a time, attempting to predict the next word from the current word and theprevious hidden state. RNNs thus don\u2019t have the limited context problem that n-grammodels have, or the \ufb01xed context that feedforward language models have, since thehidden state can in principle represent information about all of the preceding wordsall the way back to the beginning of the sequence. Fig.8.5sketches this differencebetween a FFN language model and an RNN language model, showing that theRNN language model usesht\u00001, the hidden state from the previous time step, as arepresentation of the past context.8.2.1 Forward Inference in an RNN language modelForward inference in a recurrent language model proceeds exactly as described inSection8.1.1. The input sequenceX=[x1;...;xt;...;xN]consists of a series of wordseach represented as a one-hot vector of size|V|\u21e51, and the output prediction,y, is avector representing a probability distribution over the vocabulary. At each step, themodel uses the word embedding matrixEto retrieve the embedding for the currentword, multiples it by the weight matrixW, and then adds it to the hidden layer fromthe previous step (weighted by weight matrixU) to compute a new hidden layer.This hidden layer is then used to generate an output layer which is passed through asoftmax layer to generate a probability distribution over the entire vocabulary. Thatis, at timet:et=Ext(8.4)ht=g(Uht\u00001+Wet)(8.5)\u02c6yt=softmax(Vht)(8.6)8.2\u2022RNNSA SLANGUAGEMODELS5For applications that involve much longer input sequences, such as speech recog-nition, character-level processing, or streaming continuous inputs, unrolling an en-tire input sequence may not be feasible. In these cases, we can unroll the input intomanageable \ufb01xed-length segments and treat each segment as a distinct training item.8.2 RNNs as Language ModelsLet\u2019s see how to apply RNNs to the language modeling task. Recall from Chapter 3that language models predict the next word in a sequence given some precedingcontext. For example, if the preceding context is\u201cThanks for all",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 7,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "series of wordseach represented as a one-hot vector of size|V|\u21e51, and the output prediction,y, is avector representing a probability distribution over the vocabulary. At each step, themodel uses the word embedding matrixEto retrieve the embedding for the currentword, multiples it by the weight matrixW, and then adds it to the hidden layer fromthe previous step (weighted by weight matrixU) to compute a new hidden layer.This hidden layer is then used to generate an output layer which is passed through asoftmax layer to generate a probability distribution over the entire vocabulary. Thatis, at timet:et=Ext(8.4)ht=g(Uht\u00001+Wet)(8.5)\u02c6yt=softmax(Vht)(8.6)8.2\u2022RNNSA SLANGUAGEMODELS5For applications that involve much longer input sequences, such as speech recog-nition, character-level processing, or streaming continuous inputs, unrolling an en-tire input sequence may not be feasible. In these cases, we can unroll the input intomanageable \ufb01xed-length segments and treat each segment as a distinct training item.8.2 RNNs as Language ModelsLet\u2019s see how to apply RNNs to the language modeling task. Recall from Chapter 3that language models predict the next word in a sequence given some precedingcontext. For example, if the preceding context is\u201cThanks for all the\u201dand we wantto know how likely the next word is\u201c\ufb01sh\u201dwe would compute:P(\ufb01sh|Thanks for all the)Language models give us the ability to assign such a conditional probability to everypossible next word, giving us a distribution over the entire vocabulary. We can alsoassign probabilities to entire sequences by combining these conditional probabilitieswith the chain rule:P(w1:n)=nYi=1P(wi|w<i)The n-gram language models of Chapter 3 compute the probability of a word givencounts of its occurrence with then\u00001 prior words. The context is thus of sizen\u00001.For the feedforward language models of Chapter 7, the context is the window size.RNN language models (Mikolov et al.,2010) process the input sequence oneword at a time, attempting to predict the next word from the current word and theprevious hidden state. RNNs thus don\u2019t have the limited context problem that n-grammodels have, or the \ufb01xed context that feedforward language models have, since thehidden state can in principle represent information about all of the preceding wordsall the way back to the beginning of the sequence. Fig.8.5sketches this differencebetween a FFN language model and an RNN language model, showing that theRNN language model usesht\u00001, the hidden state from the previous time step, as arepresentation of the past context.8.2.1 Forward Inference in an RNN language modelForward inference in a recurrent language model proceeds exactly as described inSection8.1.1. The input sequenceX=[x1;...;xt;...;xN]consists of a series of wordseach represented as a one-hot vector of size|V|\u21e51, and the output prediction,y, is avector representing a probability distribution over the vocabulary. At each step, themodel uses the word embedding matrixEto retrieve the embedding for the currentword, multiples it by the weight matrixW, and then adds it to the hidden layer fromthe previous step (weighted by weight matrixU) to compute a new hidden layer.This hidden layer is then used to generate an output layer which is passed through asoftmax layer to generate a probability distribution over the entire vocabulary. Thatis, at",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 8,
      "token_count": 780,
      "chapter_title": ""
    }
  },
  {
    "content": "next word from the current word and theprevious hidden state. RNNs thus don\u2019t have the limited context problem that n-grammodels have, or the \ufb01xed context that feedforward language models have, since thehidden state can in principle represent information about all of the preceding wordsall the way back to the beginning of the sequence. Fig.8.5sketches this differencebetween a FFN language model and an RNN language model, showing that theRNN language model usesht\u00001, the hidden state from the previous time step, as arepresentation of the past context.8.2.1 Forward Inference in an RNN language modelForward inference in a recurrent language model proceeds exactly as described inSection8.1.1. The input sequenceX=[x1;...;xt;...;xN]consists of a series of wordseach represented as a one-hot vector of size|V|\u21e51, and the output prediction,y, is avector representing a probability distribution over the vocabulary. At each step, themodel uses the word embedding matrixEto retrieve the embedding for the currentword, multiples it by the weight matrixW, and then adds it to the hidden layer fromthe previous step (weighted by weight matrixU) to compute a new hidden layer.This hidden layer is then used to generate an output layer which is passed through asoftmax layer to generate a probability distribution over the entire vocabulary. Thatis, at timet:et=Ext(8.4)ht=g(Uht\u00001+Wet)(8.5)\u02c6yt=softmax(Vht)(8.6)",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 9,
      "token_count": 336,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\nThe size of the condi.oning context for di\ufb00erent LMsThe n-gram LM:  Context size is the  n \u2212 1 prior words we condi3on on. The feedforward LM: Context is the window size.The RNN LM:  No \ufb01xed context size;  ht-1 represents en3re history\n\n## Page 15\n\nFFN LMs vs RNN LMs\nVWethtUht-1ethtet-1et-2U       Wa)b)^yt\net-1^ytht-2WWet-2UFFNRNN\u2026VWethtUht-1ethtet-1et-2U       Wa)b)^yt\net-1^ytht-2WWet-2U",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 10,
      "token_count": 169,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 11,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Forward inference in the RNN LMGiven input X of of N tokens represented as one-hot vectorsUse embedding matrix to get the embedding for current token xtCombine \u20268.2\u2022RNNSA SLANGUAGEMODELS5For applications that involve much longer input sequences, such as speech recog-nition, character-level processing, or streaming continuous inputs, unrolling an en-tire input sequence may not be feasible. In these cases, we can unroll the input intomanageable \ufb01xed-length segments and treat each segment as a distinct training item.8.2 RNNs as Language ModelsLet\u2019s see how to apply RNNs to the language modeling task. Recall from Chapter 3that language models predict the next word in a sequence given some precedingcontext. For example, if the preceding context is\u201cThanks for all the\u201dand we wantto know how likely the next word is\u201c\ufb01sh\u201dwe would compute:P(\ufb01sh|Thanks for all the)Language models give us the ability to assign such a conditional probability to everypossible next word, giving us a distribution over the entire vocabulary. We can alsoassign probabilities to entire sequences by combining these conditional probabilitieswith the chain rule:P(w1:n)=nYi=1P(wi|w<i)The n-gram language models of Chapter 3 compute the probability of a word givencounts of its occurrence with then\u00001 prior words. The context is thus of sizen\u00001.For the feedforward language models of Chapter 7, the context is the window size.RNN language models (Mikolov et al.,2010) process the input sequence oneword at a time, attempting to predict the next word from the current word and theprevious hidden state. RNNs thus don\u2019t have the limited context problem that n-grammodels have, or the \ufb01xed context that feedforward language models have, since thehidden state can in principle represent information about all of the preceding wordsall the way back to the beginning of the sequence. Fig.8.5sketches this differencebetween a FFN language model and an RNN language model, showing that theRNN language model usesht\u00001, the hidden state from the previous time step, as arepresentation of the past context.8.2.1 Forward Inference in an RNN language modelForward inference in a recurrent language model proceeds exactly as described inSection8.1.1. The input sequenceX=[x1;...;xt;...;xN]consists of a series of wordseach represented as a one-hot vector of size|V|\u21e51, and the output prediction,y, is avector representing a probability distribution over the vocabulary. At each step, themodel uses the word embedding matrixEto retrieve the embedding for the currentword, multiples it by the weight matrixW, and then adds it to the hidden layer fromthe previous step (weighted by weight matrixU) to compute a new hidden layer.This hidden layer is then used to generate an output layer which is passed through asoftmax layer to generate a probability distribution over the entire vocabulary. Thatis, at timet:et=Ext(8.4)ht=g(Uht\u00001+Wet)(8.5)\u02c6yt=softmax(Vht)(8.6)8.2\u2022RNNSA SLANGUAGEMODELS5For applications that involve much longer input sequences, such as speech recog-nition, character-level processing, or streaming continuous inputs, unrolling an en-tire input sequence may not be feasible. In these cases, we can unroll the input intomanageable \ufb01xed-length segments and treat each segment as a distinct training item.8.2 RNNs as Language ModelsLet\u2019s see how to apply RNNs to the language modeling task. Recall from Chapter",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 12,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "inSection8.1.1. The input sequenceX=[x1;...;xt;...;xN]consists of a series of wordseach represented as a one-hot vector of size|V|\u21e51, and the output prediction,y, is avector representing a probability distribution over the vocabulary. At each step, themodel uses the word embedding matrixEto retrieve the embedding for the currentword, multiples it by the weight matrixW, and then adds it to the hidden layer fromthe previous step (weighted by weight matrixU) to compute a new hidden layer.This hidden layer is then used to generate an output layer which is passed through asoftmax layer to generate a probability distribution over the entire vocabulary. Thatis, at timet:et=Ext(8.4)ht=g(Uht\u00001+Wet)(8.5)\u02c6yt=softmax(Vht)(8.6)8.2\u2022RNNSA SLANGUAGEMODELS5For applications that involve much longer input sequences, such as speech recog-nition, character-level processing, or streaming continuous inputs, unrolling an en-tire input sequence may not be feasible. In these cases, we can unroll the input intomanageable \ufb01xed-length segments and treat each segment as a distinct training item.8.2 RNNs as Language ModelsLet\u2019s see how to apply RNNs to the language modeling task. Recall from Chapter 3that language models predict the next word in a sequence given some precedingcontext. For example, if the preceding context is\u201cThanks for all the\u201dand we wantto know how likely the next word is\u201c\ufb01sh\u201dwe would compute:P(\ufb01sh|Thanks for all the)Language models give us the ability to assign such a conditional probability to everypossible next word, giving us a distribution over the entire vocabulary. We can alsoassign probabilities to entire sequences by combining these conditional probabilitieswith the chain rule:P(w1:n)=nYi=1P(wi|w<i)The n-gram language models of Chapter 3 compute the probability of a word givencounts of its occurrence with then\u00001 prior words. The context is thus of sizen\u00001.For the feedforward language models of Chapter 7, the context is the window size.RNN language models (Mikolov et al.,2010) process the input sequence oneword at a time, attempting to predict the next word from the current word and theprevious hidden state. RNNs thus don\u2019t have the limited context problem that n-grammodels have, or the \ufb01xed context that feedforward language models have, since thehidden state can in principle represent information about all of the preceding wordsall the way back to the beginning of the sequence. Fig.8.5sketches this differencebetween a FFN language model and an RNN language model, showing that theRNN language model usesht\u00001, the hidden state from the previous time step, as arepresentation of the past context.8.2.1 Forward Inference in an RNN language modelForward inference in a recurrent language model proceeds exactly as described inSection8.1.1. The input sequenceX=[x1;...;xt;...;xN]consists of a series of wordseach represented as a one-hot vector of size|V|\u21e51, and the output prediction,y, is avector representing a probability distribution over the vocabulary. At each step, themodel uses the word embedding matrixEto retrieve the embedding for the currentword, multiples it by the weight matrixW, and then adds it to the hidden layer fromthe previous step (weighted by weight matrixU) to compute a new hidden layer.This hidden layer is then used to generate an output layer which is passed through asoftmax layer to generate a probability distribution",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 13,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "at a time, attempting to predict the next word from the current word and theprevious hidden state. RNNs thus don\u2019t have the limited context problem that n-grammodels have, or the \ufb01xed context that feedforward language models have, since thehidden state can in principle represent information about all of the preceding wordsall the way back to the beginning of the sequence. Fig.8.5sketches this differencebetween a FFN language model and an RNN language model, showing that theRNN language model usesht\u00001, the hidden state from the previous time step, as arepresentation of the past context.8.2.1 Forward Inference in an RNN language modelForward inference in a recurrent language model proceeds exactly as described inSection8.1.1. The input sequenceX=[x1;...;xt;...;xN]consists of a series of wordseach represented as a one-hot vector of size|V|\u21e51, and the output prediction,y, is avector representing a probability distribution over the vocabulary. At each step, themodel uses the word embedding matrixEto retrieve the embedding for the currentword, multiples it by the weight matrixW, and then adds it to the hidden layer fromthe previous step (weighted by weight matrixU) to compute a new hidden layer.This hidden layer is then used to generate an output layer which is passed through asoftmax layer to generate a probability distribution over the entire vocabulary. Thatis, at timet:et=Ext(8.4)ht=g(Uht\u00001+Wet)(8.5)\u02c6yt=softmax(Vht)(8.6)",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 14,
      "token_count": 344,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17\n\nShapesVWethtUht-1ethtet-1et-2U       Wa)b)^yt\net-1^ytht-2WWet-2Ud x 1d x dd x dd x 1d x 1|V| x d|V| x 1",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 15,
      "token_count": 69,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 16,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Compu.ng the probability that the next word is word k6CHAPTER8\u2022RNNSA N DLSTMS\nVWethtUht-1ethtet-1et-2U       Wa)b)^yt\net-1^ytht-2WWet-2UFigure 8.5Simpli\ufb01ed sketch of two LM architectures moving through a text, showing aschematic context of three tokens: (a) a feedforward neural language model which has a \ufb01xedcontext input to the weight matrixW, (b) an RNN language model, in which the hidden stateht\u00001summarizes the prior context.When we do language modeling with RNNs (and we\u2019ll see this again in Chapter 9with transformers), it\u2019s convenient to make the assumption that the embedding di-mensiondeand the hidden dimensiondhare the same. So we\u2019ll just call both ofthese themodel dimensiond. So the embedding matrixEis of shape[d\u21e5|V|], andxtis a one-hot vector of shape[|V|\u21e51]. The productetis thus of shape[d\u21e51].WandUare of shape[d\u21e5d], sohtis also of shape[d\u21e51].Vis of shape[|V|\u21e5d],so the result ofVhis a vector of shape[|V|\u21e51]. This vector can be thought of asa set of scores over the vocabulary given the evidence provided inh. Passing thesescores through the softmax normalizes the scores into a probability distribution. Theprobability that a particular wordkin the vocabulary is the next word is representedby\u02c6yt[k], thekth component of\u02c6yt:P(wt+1=k|w1,...,wt)=\u02c6yt[k](8.7)The probability of an entire sequence is just the product of the probabilities of eachitem in the sequence, where we\u2019ll use\u02c6yi[wi]to mean the probability of the true wordwiat time stepi.P(w1:n)=nYi=1P(wi|w1:i\u00001)(8.8)=nYi=1\u02c6yi[wi](8.9)8.2.2 Training an RNN language modelTo train an RNN as a language model, we use the sameself-supervision(orself-self-supervisiontraining) algorithm we saw in Section??: we take a corpus of text as trainingmaterial and at each time steptask the model to predict the next word. We callsuch a model self-supervised because we don\u2019t have to add any special gold labelsto the data; the natural sequence of words is its own supervision! We simply trainthe model to minimize the error in predicting the true next word in the trainingsequence, using cross-entropy as the loss function. Recall that the cross-entropyloss measures the difference between a predicted probability distribution and the6CHAPTER8\u2022RNNSA N DLSTMS\nVWethtUht-1ethtet-1et-2U       Wa)b)^yt",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 17,
      "token_count": 647,
      "chapter_title": ""
    }
  },
  {
    "content": "VWethtUht-1ethtet-1et-2U       Wa)b)^yt\net-1^ytht-2WWet-2UFigure 8.5Simpli\ufb01ed sketch of two LM architectures moving through a text, showing aschematic context of three tokens: (a) a feedforward neural language model which has a \ufb01xedcontext input to the weight matrixW, (b) an RNN language model, in which the hidden stateht\u00001summarizes the prior context.When we do language modeling with RNNs (and we\u2019ll see this again in Chapter 9with transformers), it\u2019s convenient to make the assumption that the embedding di-mensiondeand the hidden dimensiondhare the same. So we\u2019ll just call both ofthese themodel dimensiond. So the embedding matrixEis of shape[d\u21e5|V|], andxtis a one-hot vector of shape[|V|\u21e51]. The productetis thus of shape[d\u21e51].WandUare of shape[d\u21e5d], sohtis also of shape[d\u21e51].Vis of shape[|V|\u21e5d],so the result ofVhis a vector of shape[|V|\u21e51]. This vector can be thought of asa set of scores over the vocabulary given the evidence provided inh. Passing thesescores through the softmax normalizes the scores into a probability distribution. Theprobability that a particular wordkin the vocabulary is the next word is representedby\u02c6yt[k], thekth component of\u02c6yt:P(wt+1=k|w1,...,wt)=\u02c6yt[k](8.7)The probability of an entire sequence is just the product of the probabilities of eachitem in the sequence, where we\u2019ll use\u02c6yi[wi]to mean the probability of the true wordwiat time stepi.P(w1:n)=nYi=1P(wi|w1:i\u00001)(8.8)=nYi=1\u02c6yi[wi](8.9)8.2.2 Training an RNN language modelTo train an RNN as a language model, we use the sameself-supervision(orself-self-supervisiontraining) algorithm we saw in Section??: we take a corpus of text as trainingmaterial and at each time steptask the model to predict the next word. We callsuch a model self-supervised because we don\u2019t have to add any special gold labelsto the data; the natural sequence of words is its own supervision! We simply trainthe model to minimize the error in predicting the true next word in the trainingsequence, using cross-entropy as the loss function. Recall that the cross-entropyloss measures the difference between a predicted probability distribution and the",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 18,
      "token_count": 589,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\nTraining RNN LM\u2022Self-supervision\u2022take a corpus of text as training material \u2022at each time step t \u2022ask the model to predict the next word. \u2022Why called self-supervised: we don't need human labels; the text is its own supervision signal\u2022We train the model to \u2022minimize the error\u2022in predicting the true next word in the training sequence, \u2022using cross-entropy as the loss function.",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 19,
      "token_count": 93,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 20,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Cross-entropy lossThe di\ufb00erence between:\u2022a predicted probability distribu3on \u2022the correct distribu3on.CE loss for LMs is simpler!!!\u2022the correct distribu<on yt is a one-hot vector over the vocabulary \u2022where the entry for the actual next word is 1, and all the other entries are 0.\u2022So the CE loss for LMs is only determined by the probability of next word.\u2022So at <me t, CE loss is:8.2\u2022RNNSA SLANGUAGEMODELS7\nInputEmbeddingsSoftmax overVocabulary",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 21,
      "token_count": 126,
      "chapter_title": ""
    }
  },
  {
    "content": "SolongandthanksforlongandthanksforNext wordall\u2026Loss\u2026\u2026RNNhyVh<latexit sha1_base64=\"9tru+5ysH1zS9iUXRg/IsnxmpMA=\">AAAB/XicbVDLSsNAFL3xWesr6lKQwSK4sSQi1WXRjcsK9gFNCZPpJB06yYSZiRBCcOOvuBFxo+Av+Av+jUnbTVsPDBzOOcO993gxZ0pb1q+xsrq2vrFZ2apu7+zu7ZsHhx0lEklomwguZM/DinIW0bZmmtNeLCkOPU673viu9LtPVComokedxnQQ4iBiPiNYF5Jrnlw4XATIGWGdpbmbOSHWIxlmXERBnldds2bVrQnQMrFnpAYztFzzxxkKkoQ00oRjpfq2FetBhqVmhNO86iSKxpiMcUCzyfY5OiukIfKFLF6k0USdy+FQqTT0imS5nFr0SvE/r59o/2aQsShONI3IdJCfcKQFKqtAQyYp0TwtCCaSFRsiMsISE10UVp5uLx66TDqXdbtRbzxc1Zq3sxIqcAyncA42XEMT7qEFbSDwAm/wCV/Gs/FqvBsf0+iKMftzBHMwvv8ADJKVcA==</latexit>\u0000log \u02c6ylong<latexit sha1_base64=\"tuzkS/BeX/Xmg79qpWZlpeYDhtE=\">AAAB/HicbVDLSsNAFL3xWesr6lKEwSK4sSQi1WXRjcsK9gFNCZPJpB06mYSZiRBC3PgrbkTcKPgN/oJ/Y9J209YDA4dzznDvPV7MmdKW9WusrK6tb2xWtqrbO7t7++bBYUdFiSS0TSIeyZ6HFeVM0LZmmtNeLCkOPU673viu9LtPVCoWiUedxnQQ4qFgASNYF5Jrnlw4PBoiZ4R1luZu5oRYj2SYYeHnedU1a1bdmgAtE3tGajBDyzV/HD8iSUiFJhwr1betWA8yLDUjnOZVJ1E0xmSMhzSbLJ+js0LyURDJ4gmNJupcDodKpaFXJMvd1KJXiv95/UQHN4OMiTjRVJDpoCDhSEeobAL5TFKieVoQTCQrNkRkhCUmuuirPN1ePHSZdC7rdqPeeLiqNW9nJVTgGE7hHGy4hibcQwvaQOAF3uATvoxn49V4Nz6m0RVj9ucI5mB8/wEiupTp</latexit>\u0000log \u02c6yand<latexit",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 22,
      "token_count": 741,
      "chapter_title": ""
    }
  },
  {
    "content": "\u02c6yand<latexit sha1_base64=\"0zdsmbBovZ+hafWZN7Hvufo85tU=\">AAAB/3icbVDLSsNAFJ3UV62vqEs3g0VwY0lEqsuiG5cV7AOaEibTSTN0kgkzN0IIWbjxV9yIuFHwD/wF/8ak7aatBwYO55zh3nu8WHANlvVrVNbWNza3qtu1nd29/QPz8KirZaIo61AppOp7RDPBI9YBDoL1Y8VI6AnW8yZ3pd97YkpzGT1CGrNhSMYR9zklUEiuiS8cIcfYCQhkae5mTkggUGEGAYkmOs9rrlm3GtYUeJXYc1JHc7Rd88cZSZqELAIqiNYD24phmBEFnAqW15xEs5jQCRmzbLp/js8KaYR9qYoXAZ6qCzkSap2GXpEs19PLXin+5w0S8G+GGY/iBFhEZ4P8RGCQuCwDj7hiFERaEEIVLzbENCCKUCgqK0+3lw9dJd3Lht1sNB+u6q3beQlVdIJO0Tmy0TVqoXvURh1E0Qt6Q5/oy3g2Xo1342MWrRjzP8doAcb3H7Aall0=</latexit>\u0000log \u02c6ythanks<latexit sha1_base64=\"D3c31Jvxp3QWPr2h4tzQWmeenDs=\">AAAB/HicbVDLSsNAFL3xWesr6lKEwSK4sSQi1WXRjcsK9gFNCZPppB06yYSZiRBC3PgrbkTcKPgN/oJ/Y9Jm09YDA4dzznDvPV7EmdKW9WusrK6tb2xWtqrbO7t7++bBYUeJWBLaJoIL2fOwopyFtK2Z5rQXSYoDj9OuN7kr/O4TlYqJ8FEnER0EeBQynxGsc8k1Ty4cLkbIGWOdJpmbOgHWYxmkvpBZVnXNmlW3pkDLxC5JDUq0XPPHGQoSBzTUhGOl+rYV6UGKpWaE06zqxIpGmEzwiKbT5TN0lktDlM/LX6jRVJ3L4UCpJPDyZLGbWvQK8T+vH2v/ZpCyMIo1DclskB9zpAUqmkBDJinRPMkJJpLlGyIyxhITnfdVnG4vHrpMOpd1u1FvPFzVmrdlCRU4hlM4BxuuoQn30II2EHiBN/iEL+PZeDXejY9ZdMUo/xzBHIzvP0CJlP0=</latexit>\u0000log \u02c6yfor<latexit",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 23,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "\u02c6yfor<latexit sha1_base64=\"PI3y1fb9LhumoVCQRh2+Y84dRkc=\">AAAB/HicbVDLSsNAFL3xWesr6lKEwSK4sSQi1WXRjcsK9gFNCZPppB06yYSZiRBC3PgrbkTcKPgN/oJ/Y9Jm09YDA4dzznDvPV7EmdKW9WusrK6tb2xWtqrbO7t7++bBYUeJWBLaJoIL2fOwopyFtK2Z5rQXSYoDj9OuN7kr/O4TlYqJ8FEnER0EeBQynxGsc8k1Ty4cLkbIGWOdJpmbOgHWYxmkmPMsq7pmzapbU6BlYpekBiVarvnjDAWJAxpqwrFSfduK9CDFUjPCaVZ1YkUjTCZ4RNPp8hk6y6Uh8oXMX6jRVJ3L4UCpJPDyZLGbWvQK8T+vH2v/ZpCyMIo1DclskB9zpAUqmkBDJinRPMkJJpLlGyIyxhITnfdVnG4vHrpMOpd1u1FvPFzVmrdlCRU4hlM4BxuuoQn30II2EHiBN/iEL+PZeDXejY9ZdMUo/xzBHIzvPyumlO8=</latexit>\u0000log \u02c6yall",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 24,
      "token_count": 382,
      "chapter_title": ""
    }
  },
  {
    "content": "eFigure 8.6Training RNNs as language models.correct distribution.LCE=\u0000Xw2Vyt[w]log\u02c6yt[w](8.10)In the case of language modeling, the correct distributionytcomes from knowing thenext word. This is represented as a one-hot vector corresponding to the vocabularywhere the entry for the actual next word is 1, and all the other entries are 0. Thus,the cross-entropy loss for language modeling is determined by the probability themodel assigns to the correct next word. So at timetthe CE loss is the negative logprobability the model assigns to the next word in the training sequence.LCE(\u02c6yt,yt)=\u0000log\u02c6yt[wt+1](8.11)Thus at each word positiontof the input, the model takes as input the correct wordwttogether withht\u00001, encoding information from the precedingw1:t\u00001, and uses themto compute a probability distribution over possible next words so as to compute themodel\u2019s loss for the next tokenwt+1. Then we move to the next word, we ignorewhat the model predicted for the next word and instead use the correct wordwt+1along with the prior history encoded to estimate the probability of tokenwt+2. Thisidea that we always give the model the correct history sequence to predict the nextword (rather than feeding the model its best case from the previous time step) iscalledteacher forcing.teacher forcingThe weights in the network are adjusted to minimize the average CE loss overthe training sequence via gradient descent. Fig.8.6illustrates this training regimen.8.2.3 Weight TyingCareful readers may have noticed that the input embedding matrixEand the \ufb01nallayer matrixV, which feeds the output softmax, are quite similar.The columns ofErepresent the word embeddings for each word in the vocab-ulary learned during the training process with the goal that words that have similarmeaning and function will have similar embeddings. And, since when we use RNNsfor language modeling we make the assumption that the embedding dimension and8.2\u2022RNNSA SLANGUAGEMODELS7\nInputEmbeddingsSoftmax overVocabulary",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 25,
      "token_count": 457,
      "chapter_title": ""
    }
  },
  {
    "content": "SolongandthanksforlongandthanksforNext wordall\u2026Loss\u2026\u2026RNNhyVh<latexit sha1_base64=\"9tru+5ysH1zS9iUXRg/IsnxmpMA=\">AAAB/XicbVDLSsNAFL3xWesr6lKQwSK4sSQi1WXRjcsK9gFNCZPpJB06yYSZiRBCcOOvuBFxo+Av+Av+jUnbTVsPDBzOOcO993gxZ0pb1q+xsrq2vrFZ2apu7+zu7ZsHhx0lEklomwguZM/DinIW0bZmmtNeLCkOPU673viu9LtPVComokedxnQQ4iBiPiNYF5Jrnlw4XATIGWGdpbmbOSHWIxlmXERBnldds2bVrQnQMrFnpAYztFzzxxkKkoQ00oRjpfq2FetBhqVmhNO86iSKxpiMcUCzyfY5OiukIfKFLF6k0USdy+FQqTT0imS5nFr0SvE/r59o/2aQsShONI3IdJCfcKQFKqtAQyYp0TwtCCaSFRsiMsISE10UVp5uLx66TDqXdbtRbzxc1Zq3sxIqcAyncA42XEMT7qEFbSDwAm/wCV/Gs/FqvBsf0+iKMftzBHMwvv8ADJKVcA==</latexit>\u0000log \u02c6ylong<latexit sha1_base64=\"tuzkS/BeX/Xmg79qpWZlpeYDhtE=\">AAAB/HicbVDLSsNAFL3xWesr6lKEwSK4sSQi1WXRjcsK9gFNCZPJpB06mYSZiRBC3PgrbkTcKPgN/oJ/Y9J209YDA4dzznDvPV7MmdKW9WusrK6tb2xWtqrbO7t7++bBYUdFiSS0TSIeyZ6HFeVM0LZmmtNeLCkOPU673viu9LtPVCoWiUedxnQQ4qFgASNYF5Jrnlw4PBoiZ4R1luZu5oRYj2SYYeHnedU1a1bdmgAtE3tGajBDyzV/HD8iSUiFJhwr1betWA8yLDUjnOZVJ1E0xmSMhzSbLJ+js0LyURDJ4gmNJupcDodKpaFXJMvd1KJXiv95/UQHN4OMiTjRVJDpoCDhSEeobAL5TFKieVoQTCQrNkRkhCUmuuirPN1ePHSZdC7rdqPeeLiqNW9nJVTgGE7hHGy4hibcQwvaQOAF3uATvoxn49V4Nz6m0RVj9ucI5mB8/wEiupTp</latexit>\u0000log \u02c6yand<latexit",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 26,
      "token_count": 741,
      "chapter_title": ""
    }
  },
  {
    "content": "\u02c6yand<latexit sha1_base64=\"0zdsmbBovZ+hafWZN7Hvufo85tU=\">AAAB/3icbVDLSsNAFJ3UV62vqEs3g0VwY0lEqsuiG5cV7AOaEibTSTN0kgkzN0IIWbjxV9yIuFHwD/wF/8ak7aatBwYO55zh3nu8WHANlvVrVNbWNza3qtu1nd29/QPz8KirZaIo61AppOp7RDPBI9YBDoL1Y8VI6AnW8yZ3pd97YkpzGT1CGrNhSMYR9zklUEiuiS8cIcfYCQhkae5mTkggUGEGAYkmOs9rrlm3GtYUeJXYc1JHc7Rd88cZSZqELAIqiNYD24phmBEFnAqW15xEs5jQCRmzbLp/js8KaYR9qYoXAZ6qCzkSap2GXpEs19PLXin+5w0S8G+GGY/iBFhEZ4P8RGCQuCwDj7hiFERaEEIVLzbENCCKUCgqK0+3lw9dJd3Lht1sNB+u6q3beQlVdIJO0Tmy0TVqoXvURh1E0Qt6Q5/oy3g2Xo1342MWrRjzP8doAcb3H7Aall0=</latexit>\u0000log \u02c6ythanks<latexit sha1_base64=\"D3c31Jvxp3QWPr2h4tzQWmeenDs=\">AAAB/HicbVDLSsNAFL3xWesr6lKEwSK4sSQi1WXRjcsK9gFNCZPppB06yYSZiRBC3PgrbkTcKPgN/oJ/Y9Jm09YDA4dzznDvPV7EmdKW9WusrK6tb2xWtqrbO7t7++bBYUeJWBLaJoIL2fOwopyFtK2Z5rQXSYoDj9OuN7kr/O4TlYqJ8FEnER0EeBQynxGsc8k1Ty4cLkbIGWOdJpmbOgHWYxmkvpBZVnXNmlW3pkDLxC5JDUq0XPPHGQoSBzTUhGOl+rYV6UGKpWaE06zqxIpGmEzwiKbT5TN0lktDlM/LX6jRVJ3L4UCpJPDyZLGbWvQK8T+vH2v/ZpCyMIo1DclskB9zpAUqmkBDJinRPMkJJpLlGyIyxhITnfdVnG4vHrpMOpd1u1FvPFzVmrdlCRU4hlM4BxuuoQn30II2EHiBN/iEL+PZeDXejY9ZdMUo/xzBHIzvP0CJlP0=</latexit>\u0000log \u02c6yfor<latexit",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 27,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "\u02c6yfor<latexit sha1_base64=\"PI3y1fb9LhumoVCQRh2+Y84dRkc=\">AAAB/HicbVDLSsNAFL3xWesr6lKEwSK4sSQi1WXRjcsK9gFNCZPppB06yYSZiRBC3PgrbkTcKPgN/oJ/Y9Jm09YDA4dzznDvPV7EmdKW9WusrK6tb2xWtqrbO7t7++bBYUeJWBLaJoIL2fOwopyFtK2Z5rQXSYoDj9OuN7kr/O4TlYqJ8FEnER0EeBQynxGsc8k1Ty4cLkbIGWOdJpmbOgHWYxmkmPMsq7pmzapbU6BlYpekBiVarvnjDAWJAxpqwrFSfduK9CDFUjPCaVZ1YkUjTCZ4RNPp8hk6y6Uh8oXMX6jRVJ3L4UCpJPDyZLGbWvQK8T+vH2v/ZpCyMIo1DclskB9zpAUqmkBDJinRPMkJJpLlGyIyxhITnfdVnG4vHrpMOpd1u1FvPFzVmrdlCRU4hlM4BxuuoQn30II2EHiBN/iEL+PZeDXejY9ZdMUo/xzBHIzvPyumlO8=</latexit>\u0000log \u02c6yall",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 28,
      "token_count": 382,
      "chapter_title": ""
    }
  },
  {
    "content": "eFigure 8.6Training RNNs as language models.correct distribution.LCE=\u0000Xw2Vyt[w]log\u02c6yt[w](8.10)In the case of language modeling, the correct distributionytcomes from knowing thenext word. This is represented as a one-hot vector corresponding to the vocabularywhere the entry for the actual next word is 1, and all the other entries are 0. Thus,the cross-entropy loss for language modeling is determined by the probability themodel assigns to the correct next word. So at timetthe CE loss is the negative logprobability the model assigns to the next word in the training sequence.LCE(\u02c6yt,yt)=\u0000log\u02c6yt[wt+1](8.11)Thus at each word positiontof the input, the model takes as input the correct wordwttogether withht\u00001, encoding information from the precedingw1:t\u00001, and uses themto compute a probability distribution over possible next words so as to compute themodel\u2019s loss for the next tokenwt+1. Then we move to the next word, we ignorewhat the model predicted for the next word and instead use the correct wordwt+1along with the prior history encoded to estimate the probability of tokenwt+2. Thisidea that we always give the model the correct history sequence to predict the nextword (rather than feeding the model its best case from the previous time step) iscalledteacher forcing.teacher forcingThe weights in the network are adjusted to minimize the average CE loss overthe training sequence via gradient descent. Fig.8.6illustrates this training regimen.8.2.3 Weight TyingCareful readers may have noticed that the input embedding matrixEand the \ufb01nallayer matrixV, which feeds the output softmax, are quite similar.The columns ofErepresent the word embeddings for each word in the vocab-ulary learned during the training process with the goal that words that have similarmeaning and function will have similar embeddings. And, since when we use RNNsfor language modeling we make the assumption that the embedding dimension and",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 29,
      "token_count": 433,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21\n\nTeacher forcingWe always give the model the correct history to predict the next word (rather than feeding the model the possible buggy guess from the prior :me step).This is called teacher forcing (in training we force the context to be correct based on the gold words)What teacher forcing looks like: \u2022At word posi:on t \u2022the model takes as input the correct word wt together with ht\u22121, computes a probability distribu:on over possible next words \u2022That gives loss for the next token wt+1\u2022Then we move on to next word, ignore what the model predicted for the next word and instead use the correct word wt+1 along with the prior history encoded to es:mate the probability of token wt+2.",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 30,
      "token_count": 152,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22\n\nWeight tyingThe input embedding matrix E and the \ufb01nal layer matrix V, are similar\u2022The columns of E represent the word embeddings for each word in vocab. E is [d x |V|]\u2022The \ufb01nal layer matrix V helps give a score (logit) for each word in vocab .  V is [|V| x d ]Instead of having separate E and V, we just <e them together, using ET instead of V:8CHAPTER8\u2022RNNSA N DLSTMSthe hidden dimension are the same (= the model dimensiond), the embedding ma-trixEhas shape[d\u21e5|V|]. And the \ufb01nal layer matrixVprovides a way to scorethe likelihood of each word in the vocabulary given the evidence present in the \ufb01nalhidden layer of the network through the calculation ofVh.Vis of shape[|V|\u21e5d].That is, is, the rows ofVare shaped like a transpose ofE, meaning thatVprovidesasecond setof learned word embeddings.Instead of having two sets of embedding matrices, language models use a singleembedding matrix, which appears at both the input and softmax layers. That is,we dispense withVand useEat the start of the computation andE|(because theshape ofVis the transpose ofEat the end. Using the same matrix (transposed) intwo places is calledweight tying.1The weight-tied equations for an RNN languageweight tyingmodel then become:et=Ext(8.12)ht=g(Uht\u00001+Wet)(8.13)\u02c6yt=softmax(E|ht)(8.14)In addition to providing improved model perplexity, this approach signi\ufb01cantly re-duces the number of parameters required for the model.8.3 RNNs for other NLP tasksNow that we\u2019ve seen the basic RNN architecture, let\u2019s consider how to apply it tothree types of NLP tasks:sequence classi\ufb01cationtasks like sentiment analysis andtopic classi\ufb01cation,sequence labelingtasks like part-of-speech tagging, andtextgenerationtasks, including with a new architecture called theencoder-decoder.8.3.1 Sequence LabelingIn sequence labeling, the network\u2019s task is to assign a label chosen from a small\ufb01xed set of labels to each element of a sequence. One classic sequence labelingtasks is part-of-speech (POS) tagging (assigning grammatical tags likeNOUNandVERBto each word in a sentence). We\u2019ll discuss part-of-speech tagging in detailin Chapter 17, but let\u2019s give a motivating example here. In an RNN approach tosequence labeling, inputs are word embeddings and the outputs are tag probabilitiesgenerated by a softmax layer over the given tagset, as illustrated in Fig.8.7.In this \ufb01gure, the inputs at each time step are pretrained word embeddings cor-responding to the input tokens. The RNN block is an abstraction that representsan unrolled simple recurrent network consisting of an input layer, hidden layer, andoutput layer at each time step, as well as the sharedU,VandWweight matricesthat comprise the network. The outputs of the network at each time step representthe distribution over the POS tagset generated by a softmax layer.To generate a sequence of tags for a given input, we run forward inference overthe input sequence and select the most likely tag from the softmax at each step. Sincewe\u2019re using a softmax layer to generate the probability distribution over the outputtagset at each time step, we will again employ the cross-entropy loss during training.1We also do this for transformers (Chapter 9) where it\u2019s common to callE|theunembedding matrix.",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 31,
      "token_count": 782,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23\n\nRNNs and LSTMsRNNs as Language Models\n\n## Page 24\n\nRNNs and LSTMsRNNs for Sequences\n\n## Page 25\n\nRNNs for sequence labelingAssign a label to each element of a sequencePart-of-speech tagging\nJanetwillbackthebillNNDTVBMDNNPArgmax\nEmbeddingsWordsehVhyRNNLayer(s)Softmax overtags\n\n## Page 26\n\nRNNs for sequence classi\ufb01caAonText classification\nInstead of taking the last state, could use some pooling function of all the output states, like mean poolingx1\nRNNhnx2x3xnSoftmaxFFN10CHAPTER8\u2022RNNSA N DLSTMS\nx1\nRNNhnx2x3xnSoftmaxFFN\nFigure 8.8Sequence classi\ufb01cation using a simple RNN combined with a feedforward net-work. The \ufb01nal hidden state from the RNN is used as the input to a feedforward network thatperforms the classi\ufb01cation.pools all thenhidden states by taking their element-wise mean:hmean=1nnXi=1hi(8.15)Or we can take the element-wise max; the element-wise max of a set ofnvectors isa new vector whosekth element is the max of thekth elements of all thenvectors.The long contexts of RNNs makes it quite dif\ufb01cult to successfully backpropagateerror all the way through the entire input; we\u2019ll talk about this problem, and somestandard solutions, in Section8.5.8.3.3 Generation with RNN-Based Language ModelsRNN-based language models can also be used to generate text. Text generation isof enormous practical importance, part of tasks like question answering, machinetranslation, text summarization, grammar correction, story generation, and conver-sational dialogue; any task where a system needs to produce text, conditioned onsome other text. This use of a language model to generate text is one of the areasin which the impact of neural language models on NLP has been the largest. Textgeneration, along with image generation and code generation, constitute a new areaof AI that is often calledgenerative AI.Recall back in Chapter 3 we saw how to generate text from an n-gram languagemodel by adapting asamplingtechnique suggested at about the same time by ClaudeShannon (Shannon,1951) and the psychologists George Miller and Jennifer Self-ridge (Miller and Selfridge,1950). We \ufb01rst randomly sample a word to begin asequence based on its suitability as the start of a sequence. We then continue tosample wordsconditioned on our previous choicesuntil we reach a pre-determinedlength, or an end of sequence token is generated.Today, this approach of using a language model to incrementally generate wordsby repeatedly sampling the next word conditioned on our previous choices is calledautoregressive generationorcausal LM generation. The procedure is basicallyautoregressivegenerationthe same as that described on page??, but adapted to a neural context:\u2022Sample a word in the output from the softmax distribution that results fromusing the beginning of sentence marker,<s>, as the \ufb01rst input.\n\n## Page 27\n\nAutoregressive generation\nSolong\n<s>and\nSolongand?Sampled WordSoftmaxEmbeddingInput WordRNN\n\n## Page 28\n\nStacked RNNsy1y2y3yn\nx1x2x3xn\nRNN 1\nRNN 2\n RNN 3",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 32,
      "token_count": 755,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 29",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 33,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BidirecAonal RNNs\nRNN 2 \nRNN 1x1y2y1y3ynconcatenatedoutputs\nx2x3xn12CHAPTER8\u2022RNNSA N DLSTMSthe entire sequence of outputs from one RNN as an input sequence to another one.Stacked RNNsconsist of multiple networks where the output of one layer serves asStacked RNNsthe input to a subsequent layer, as shown in Fig.8.10.y1y2y3yn\nx1x2x3xn\nRNN 1\nRNN 2\n RNN 3\nFigure 8.10Stacked recurrent networks. The output of a lower level serves as the input tohigher levels with the output of the last network serving as the \ufb01nal output.Stacked RNNs generally outperform single-layer networks. One reason for thissuccess seems to be that the network induces representations at differing levels ofabstraction across layers. Just as the early stages of the human visual system detectedges that are then used for \ufb01nding larger regions and shapes, the initial layers ofstacked networks can induce representations that serve as useful abstractions forfurther layers\u2014representations that might prove dif\ufb01cult to induce in a single RNN.The optimal number of stacked RNNs is speci\ufb01c to each application and to eachtraining set. However, as the number of stacks is increased the training costs risequickly.8.4.2 Bidirectional RNNsThe RNN uses information from the left (prior) context to make its predictions attimet. But in many applications we have access to the entire input sequence; inthose cases we would like to use words from the context to the right oft. One wayto do this is to run two separate RNNs, one left-to-right, and one right-to-left, andconcatenate their representations.In the left-to-right RNNs we\u2019ve discussed so far, the hidden state at a given timetrepresents everything the network knows about the sequence up to that point. Thestate is a function of the inputsx1,. . . ,xtand represents the context of the network tothe left of the current time.hft=RNNforward(x1,...,xt)(8.16)This new notationhftsimply corresponds to the normal hidden state at timet, repre-senting everything the network has gleaned from the sequence so far.To take advantage of context to the right of the current input, we can train anRNN on areversedinput sequence. With this approach, the hidden state at timetrepresents information about the sequence to therightof the current input:hbt=RNNbackward(xt,...xn)(8.17)12CHAPTER8\u2022RNNSA N DLSTMSthe entire sequence of outputs from one RNN as an input sequence to another one.Stacked RNNsconsist of multiple networks where the output of one layer serves asStacked RNNsthe input to a subsequent layer, as shown in Fig.8.10.y1y2y3yn\nx1x2x3xn\nRNN 1\nRNN 2\n RNN 3",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 34,
      "token_count": 664,
      "chapter_title": ""
    }
  },
  {
    "content": "x1x2x3xn\nRNN 1\nRNN 2\n RNN 3\nFigure 8.10Stacked recurrent networks. The output of a lower level serves as the input tohigher levels with the output of the last network serving as the \ufb01nal output.Stacked RNNs generally outperform single-layer networks. One reason for thissuccess seems to be that the network induces representations at differing levels ofabstraction across layers. Just as the early stages of the human visual system detectedges that are then used for \ufb01nding larger regions and shapes, the initial layers ofstacked networks can induce representations that serve as useful abstractions forfurther layers\u2014representations that might prove dif\ufb01cult to induce in a single RNN.The optimal number of stacked RNNs is speci\ufb01c to each application and to eachtraining set. However, as the number of stacks is increased the training costs risequickly.8.4.2 Bidirectional RNNsThe RNN uses information from the left (prior) context to make its predictions attimet. But in many applications we have access to the entire input sequence; inthose cases we would like to use words from the context to the right oft. One wayto do this is to run two separate RNNs, one left-to-right, and one right-to-left, andconcatenate their representations.In the left-to-right RNNs we\u2019ve discussed so far, the hidden state at a given timetrepresents everything the network knows about the sequence up to that point. Thestate is a function of the inputsx1,. . . ,xtand represents the context of the network tothe left of the current time.hft=RNNforward(x1,...,xt)(8.16)This new notationhftsimply corresponds to the normal hidden state at timet, repre-senting everything the network has gleaned from the sequence so far.To take advantage of context to the right of the current input, we can train anRNN on areversedinput sequence. With this approach, the hidden state at timetrepresents information about the sequence to therightof the current input:hbt=RNNbackward(xt,...xn)(8.17)8.4\u2022STACKED ANDBIDIRECTIONALRNNARCHITECTURES13Here, the hidden statehbtrepresents all the information we have discerned about thesequence fromtto the end of the sequence.Abidirectional RNN(Schuster and Paliwal,1997) combines two independentbidirectionalRNNRNNs, one where the input is processed from the start to the end, and the other fromthe end to the start. We then concatenate the two representations computed by thenetworks into a single vector that captures both the left and right contexts of an inputat each point in time. Here we use either the semicolon \u201d;\u201d or the equivalent symbol\u0000to mean vector concatenation:ht=[hft;hbt]=hft\u0000hbt(8.18)Fig.8.11illustrates such a bidirectional network that concatenates the outputs ofthe forward and backward pass. Other simple ways to combine the forward andbackward contexts include element-wise addition or multiplication. The output ateach step in time thus captures information to the left and to the right of the currentinput. In sequence labeling applications, these concatenated outputs can serve as thebasis for a local labeling decision.\nRNN 2 \nRNN 1x1y2y1y3ynconcatenatedoutputs",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 35,
      "token_count": 729,
      "chapter_title": ""
    }
  },
  {
    "content": "RNN 2 \nRNN 1x1y2y1y3ynconcatenatedoutputs\nx2x3xnFigure 8.11A bidirectional RNN. Separate models are trained in the forward and backwarddirections, with the output of each model at each time point concatenated to represent thebidirectional state at that time point.Bidirectional RNNs have also proven to be quite effective for sequence classi\ufb01-cation. Recall from Fig.8.8that for sequence classi\ufb01cation we used the \ufb01nal hiddenstate of the RNN as the input to a subsequent feedforward classi\ufb01er. A dif\ufb01cultywith this approach is that the \ufb01nal state naturally re\ufb02ects more information aboutthe end of the sentence than its beginning. Bidirectional RNNs provide a simplesolution to this problem; as shown in Fig.8.12, we simply combine the \ufb01nal hiddenstates from the forward and backward passes (for example by concatenation) anduse that as input for follow-on processing.",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 36,
      "token_count": 224,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 30\n\nBidirecAonal RNNs for classi\ufb01caAon\nRNN 2 \nRNN 1x1x2x3xnhn\u2192h1\u2190hn\u2192SoftmaxFFNh1\u2190\n\n## Page 31\n\nRNNs and LSTMsRNNs for Sequences\n\n## Page 32\n\nRNNs and LSTMsThe LSTM\n\n## Page 33\n\nMoAvaAng the LSTM: dealing with distance\u2022It's hard to assign probabili<es accurately when context is very far away:\u2022The \ufb02ights the airline was canceling were full. \u2022Hidden layers are being forced to do two things:\u2022Provide informa<on useful for the current decision, \u2022Update and carry forward informa<on required for future decisions. \u2022Another problem: During backprop, we have to repeatedly mul<ply gradients through <me and many h's\u2022The \"vanishing gradient\" problem\n\n## Page 34\n\nThe LSTM: Long short-term memory networkLSTMs divide the context management problem into two subproblems: \u2022removing informa<on no longer needed from the context, \u2022adding informa<on likely to be needed for later decision making\u2022LSTMs add:\u2022 explicit context layer\u2022Neural circuits with gates to control informa0on \ufb02ow",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 37,
      "token_count": 284,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 34\n\nThe LSTM: Long short-term memory networkLSTMs divide the context management problem into two subproblems: \u2022removing informa<on no longer needed from the context, \u2022adding informa<on likely to be needed for later decision making\u2022LSTMs add:\u2022 explicit context layer\u2022Neural circuits with gates to control informa0on \ufb02ow\n\n## Page 35\n\nForget gateDeletes  information from the context that is no longer needed. 8.5\u2022THELSTM15called thevanishing gradientsproblem.vanishinggradientsTo address these issues, more complex network architectures have been designedto explicitly manage the task of maintaining relevant context over time, by enablingthe network to learn to forget information that is no longer needed and to rememberinformation required for decisions still to come.The most commonly used such extension to RNNs is thelong short-term mem-ory(LSTM) network (Hochreiter and Schmidhuber,1997). LSTMs divide the con-long short-termmemorytext management problem into two subproblems: removing information no longerneeded from the context, and adding information likely to be needed for later de-cision making. The key to solving both problems is to learn how to manage thiscontext rather than hard-coding a strategy into the architecture. LSTMs accomplishthis by \ufb01rst adding an explicit context layer to the architecture (in addition to theusual recurrent hidden layer), and through the use of specialized neural units thatmake use ofgatesto control the \ufb02ow of information into and out of the units thatcomprise the network layers. These gates are implemented through the use of addi-tional weights that operate sequentially on the input, and previous hidden layer, andprevious context layers.The gates in an LSTM share a common design pattern; each consists of a feed-forward layer, followed by a sigmoid activation function, followed by a pointwisemultiplication with the layer being gated. The choice of the sigmoid as the activationfunction arises from its tendency to push its outputs to either 0 or 1. Combining thiswith a pointwise multiplication has an effect similar to that of a binary mask. Valuesin the layer being gated that align with values near 1 in the mask are passed throughnearly unchanged; values corresponding to lower values are essentially erased.The \ufb01rst gate we\u2019ll consider is theforget gate. The purpose of this gate isforget gateto delete information from the context that is no longer needed. The forget gatecomputes a weighted sum of the previous state\u2019s hidden layer and the current in-put and passes that through a sigmoid. This mask is then multiplied element-wiseby the context vector to remove the information from context that is no longer re-quired. Element-wise multiplication of two vectors (represented by the operator\u0000,and sometimes called theHadamard product) is the vector of the same dimensionas the two input vectors, where each elementiis the product of elementiin the twoinput vectors:ft=s(Ufht\u00001+Wfxt)(8.20)kt=ct\u00001\u0000ft(8.21)The next task is to compute the actual information we need to extract from the previ-ous hidden state and current inputs\u2014the same basic computation we\u2019ve been usingfor all our recurrent networks.gt=tanh(Ught\u00001+Wgxt)(8.22)Next, we generate the mask for theadd gateto select the information to add to theadd gatecurrent context.it=s(Uiht\u00001+Wixt)(8.23)jt=gt\u0000it(8.24)Next, we add this to the modi\ufb01ed context vector to get our new context vector.ct=jt+kt(8.25)",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 38,
      "token_count": 774,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 36\n\nRegular passing of information8.5\u2022THELSTM15called thevanishing gradientsproblem.vanishinggradientsTo address these issues, more complex network architectures have been designedto explicitly manage the task of maintaining relevant context over time, by enablingthe network to learn to forget information that is no longer needed and to rememberinformation required for decisions still to come.The most commonly used such extension to RNNs is thelong short-term mem-ory(LSTM) network (Hochreiter and Schmidhuber,1997). LSTMs divide the con-long short-termmemorytext management problem into two subproblems: removing information no longerneeded from the context, and adding information likely to be needed for later de-cision making. The key to solving both problems is to learn how to manage thiscontext rather than hard-coding a strategy into the architecture. LSTMs accomplishthis by \ufb01rst adding an explicit context layer to the architecture (in addition to theusual recurrent hidden layer), and through the use of specialized neural units thatmake use ofgatesto control the \ufb02ow of information into and out of the units thatcomprise the network layers. These gates are implemented through the use of addi-tional weights that operate sequentially on the input, and previous hidden layer, andprevious context layers.The gates in an LSTM share a common design pattern; each consists of a feed-forward layer, followed by a sigmoid activation function, followed by a pointwisemultiplication with the layer being gated. The choice of the sigmoid as the activationfunction arises from its tendency to push its outputs to either 0 or 1. Combining thiswith a pointwise multiplication has an effect similar to that of a binary mask. Valuesin the layer being gated that align with values near 1 in the mask are passed throughnearly unchanged; values corresponding to lower values are essentially erased.The \ufb01rst gate we\u2019ll consider is theforget gate. The purpose of this gate isforget gateto delete information from the context that is no longer needed. The forget gatecomputes a weighted sum of the previous state\u2019s hidden layer and the current in-put and passes that through a sigmoid. This mask is then multiplied element-wiseby the context vector to remove the information from context that is no longer re-quired. Element-wise multiplication of two vectors (represented by the operator\u0000,and sometimes called theHadamard product) is the vector of the same dimensionas the two input vectors, where each elementiis the product of elementiin the twoinput vectors:ft=s(Ufht\u00001+Wfxt)(8.20)kt=ct\u00001\u0000ft(8.21)The next task is to compute the actual information we need to extract from the previ-ous hidden state and current inputs\u2014the same basic computation we\u2019ve been usingfor all our recurrent networks.gt=tanh(Ught\u00001+Wgxt)(8.22)Next, we generate the mask for theadd gateto select the information to add to theadd gatecurrent context.it=s(Uiht\u00001+Wixt)(8.23)jt=gt\u0000it(8.24)Next, we add this to the modi\ufb01ed context vector to get our new context vector.ct=jt+kt(8.25)",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 39,
      "token_count": 682,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 37",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 40,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Add gateSelec:ng informa:on to add to current contextAdd this to the modi\ufb01ed context vector to get our new context vector. 8.5\u2022THELSTM15called thevanishing gradientsproblem.vanishinggradientsTo address these issues, more complex network architectures have been designedto explicitly manage the task of maintaining relevant context over time, by enablingthe network to learn to forget information that is no longer needed and to rememberinformation required for decisions still to come.The most commonly used such extension to RNNs is thelong short-term mem-ory(LSTM) network (Hochreiter and Schmidhuber,1997). LSTMs divide the con-long short-termmemorytext management problem into two subproblems: removing information no longerneeded from the context, and adding information likely to be needed for later de-cision making. The key to solving both problems is to learn how to manage thiscontext rather than hard-coding a strategy into the architecture. LSTMs accomplishthis by \ufb01rst adding an explicit context layer to the architecture (in addition to theusual recurrent hidden layer), and through the use of specialized neural units thatmake use ofgatesto control the \ufb02ow of information into and out of the units thatcomprise the network layers. These gates are implemented through the use of addi-tional weights that operate sequentially on the input, and previous hidden layer, andprevious context layers.The gates in an LSTM share a common design pattern; each consists of a feed-forward layer, followed by a sigmoid activation function, followed by a pointwisemultiplication with the layer being gated. The choice of the sigmoid as the activationfunction arises from its tendency to push its outputs to either 0 or 1. Combining thiswith a pointwise multiplication has an effect similar to that of a binary mask. Valuesin the layer being gated that align with values near 1 in the mask are passed throughnearly unchanged; values corresponding to lower values are essentially erased.The \ufb01rst gate we\u2019ll consider is theforget gate. The purpose of this gate isforget gateto delete information from the context that is no longer needed. The forget gatecomputes a weighted sum of the previous state\u2019s hidden layer and the current in-put and passes that through a sigmoid. This mask is then multiplied element-wiseby the context vector to remove the information from context that is no longer re-quired. Element-wise multiplication of two vectors (represented by the operator\u0000,and sometimes called theHadamard product) is the vector of the same dimensionas the two input vectors, where each elementiis the product of elementiin the twoinput vectors:ft=s(Ufht\u00001+Wfxt)(8.20)kt=ct\u00001\u0000ft(8.21)The next task is to compute the actual information we need to extract from the previ-ous hidden state and current inputs\u2014the same basic computation we\u2019ve been usingfor all our recurrent networks.gt=tanh(Ught\u00001+Wgxt)(8.22)Next, we generate the mask for theadd gateto select the information to add to theadd gatecurrent context.it=s(Uiht\u00001+Wixt)(8.23)jt=gt\u0000it(8.24)Next, we add this to the modi\ufb01ed context vector to get our new context vector.ct=jt+kt(8.25)8.5\u2022THELSTM15called thevanishing gradientsproblem.vanishinggradientsTo address these issues, more complex network architectures have been designedto explicitly manage the task of maintaining relevant context over time, by enablingthe network to learn to forget information that is no longer needed and to rememberinformation required for decisions still to come.The most commonly used such extension to RNNs is thelong short-term mem-ory(LSTM) network",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 41,
      "token_count": 795,
      "chapter_title": ""
    }
  },
  {
    "content": "Element-wise multiplication of two vectors (represented by the operator\u0000,and sometimes called theHadamard product) is the vector of the same dimensionas the two input vectors, where each elementiis the product of elementiin the twoinput vectors:ft=s(Ufht\u00001+Wfxt)(8.20)kt=ct\u00001\u0000ft(8.21)The next task is to compute the actual information we need to extract from the previ-ous hidden state and current inputs\u2014the same basic computation we\u2019ve been usingfor all our recurrent networks.gt=tanh(Ught\u00001+Wgxt)(8.22)Next, we generate the mask for theadd gateto select the information to add to theadd gatecurrent context.it=s(Uiht\u00001+Wixt)(8.23)jt=gt\u0000it(8.24)Next, we add this to the modi\ufb01ed context vector to get our new context vector.ct=jt+kt(8.25)8.5\u2022THELSTM15called thevanishing gradientsproblem.vanishinggradientsTo address these issues, more complex network architectures have been designedto explicitly manage the task of maintaining relevant context over time, by enablingthe network to learn to forget information that is no longer needed and to rememberinformation required for decisions still to come.The most commonly used such extension to RNNs is thelong short-term mem-ory(LSTM) network (Hochreiter and Schmidhuber,1997). LSTMs divide the con-long short-termmemorytext management problem into two subproblems: removing information no longerneeded from the context, and adding information likely to be needed for later de-cision making. The key to solving both problems is to learn how to manage thiscontext rather than hard-coding a strategy into the architecture. LSTMs accomplishthis by \ufb01rst adding an explicit context layer to the architecture (in addition to theusual recurrent hidden layer), and through the use of specialized neural units thatmake use ofgatesto control the \ufb02ow of information into and out of the units thatcomprise the network layers. These gates are implemented through the use of addi-tional weights that operate sequentially on the input, and previous hidden layer, andprevious context layers.The gates in an LSTM share a common design pattern; each consists of a feed-forward layer, followed by a sigmoid activation function, followed by a pointwisemultiplication with the layer being gated. The choice of the sigmoid as the activationfunction arises from its tendency to push its outputs to either 0 or 1. Combining thiswith a pointwise multiplication has an effect similar to that of a binary mask. Valuesin the layer being gated that align with values near 1 in the mask are passed throughnearly unchanged; values corresponding to lower values are essentially erased.The \ufb01rst gate we\u2019ll consider is theforget gate. The purpose of this gate isforget gateto delete information from the context that is no longer needed. The forget gatecomputes a weighted sum of the previous state\u2019s hidden layer and the current in-put and passes that through a sigmoid. This mask is then multiplied element-wiseby the context vector to remove the information from context that is no longer re-quired. Element-wise multiplication of two vectors (represented by the operator\u0000,and sometimes called theHadamard product) is the vector of the same dimensionas the two input vectors, where each elementiis the product of elementiin the twoinput vectors:ft=s(Ufht\u00001+Wfxt)(8.20)kt=ct\u00001\u0000ft(8.21)The next task is to compute the actual information we need to extract from the previ-ous hidden state and current inputs\u2014the same basic computation we\u2019ve been usingfor all our recurrent",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 42,
      "token_count": 787,
      "chapter_title": ""
    }
  },
  {
    "content": "layer, followed by a sigmoid activation function, followed by a pointwisemultiplication with the layer being gated. The choice of the sigmoid as the activationfunction arises from its tendency to push its outputs to either 0 or 1. Combining thiswith a pointwise multiplication has an effect similar to that of a binary mask. Valuesin the layer being gated that align with values near 1 in the mask are passed throughnearly unchanged; values corresponding to lower values are essentially erased.The \ufb01rst gate we\u2019ll consider is theforget gate. The purpose of this gate isforget gateto delete information from the context that is no longer needed. The forget gatecomputes a weighted sum of the previous state\u2019s hidden layer and the current in-put and passes that through a sigmoid. This mask is then multiplied element-wiseby the context vector to remove the information from context that is no longer re-quired. Element-wise multiplication of two vectors (represented by the operator\u0000,and sometimes called theHadamard product) is the vector of the same dimensionas the two input vectors, where each elementiis the product of elementiin the twoinput vectors:ft=s(Ufht\u00001+Wfxt)(8.20)kt=ct\u00001\u0000ft(8.21)The next task is to compute the actual information we need to extract from the previ-ous hidden state and current inputs\u2014the same basic computation we\u2019ve been usingfor all our recurrent networks.gt=tanh(Ught\u00001+Wgxt)(8.22)Next, we generate the mask for theadd gateto select the information to add to theadd gatecurrent context.it=s(Uiht\u00001+Wixt)(8.23)jt=gt\u0000it(8.24)Next, we add this to the modi\ufb01ed context vector to get our new context vector.ct=jt+kt(8.25)",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 43,
      "token_count": 393,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 38\n\nOutput gateDecide what informa6on is required for the current hidden state (as opposed to what informa6on needs to be preserved for future decisions). 16CHAPTER8\u2022RNNSA N DLSTMS+\nxtht-1cthtcthtct-1ht-1xttanh\n+\u03c3tanh\u03c3\u03c3+++igf\no\u3f4b\u3f4b\u3f4bLSTMct-1\nFigure 8.13A single LSTM unit displayed as a computation graph. The inputs to each unit consists of thecurrent input,x, the previous hidden state,ht\u00001, and the previous context,ct\u00001. The outputs are a new hiddenstate,htand an updated context,ct.The \ufb01nal gate we\u2019ll use is theoutput gatewhich is used to decide what informa-output gatetion is required for the current hidden state (as opposed to what information needsto be preserved for future decisions).ot=s(Uoht\u00001+Woxt)(8.26)ht=ot\u0000tanh(ct)(8.27)Fig.8.13illustrates the complete computation for a single LSTM unit. Given theappropriate weights for the various gates, an LSTM accepts as input the contextlayer, and hidden layer from the previous time step, along with the current inputvector. It then generates updated context and hidden vectors as output.It is the hidden state,ht, that provides the output for the LSTM at each time step.This output can be used as the input to subsequent layers in a stacked RNN, or at the\ufb01nal layer of a networkhtcan be used to provide the \ufb01nal output of the LSTM.8.5.1 Gated Units, Layers and NetworksThe neural units used in LSTMs are obviously much more complex than those usedin basic feedforward networks. Fortunately, this complexity is encapsulated withinthe basic processing units, allowing us to maintain modularity and to easily exper-iment with different architectures. To see this, consider Fig.8.14which illustratesthe inputs and outputs associated with each kind of unit.At the far left, (a) is the basic feedforward unit where a single set of weights anda single activation function determine its output, and when arranged in a layer thereare no connections among the units in the layer. Next, (b) represents the unit in asimple recurrent network. Now there are two inputs and an additional set of weightsto go with it. However, there is still a single activation function and output.The increased complexity of the LSTM units is encapsulated within the unititself. The only additional external complexity for the LSTM over the basic recurrentunit (b) is the presence of the additional context vector as an input and output.This modularity is key to the power and widespread applicability of LSTM units.LSTM units (or other varieties, like GRUs) can be substituted into any of the networkarchitectures described in Section8.4. And, as with simple RNNs, multi-layerednetworks making use of gated units can be unrolled into deep feedforward networks\n\n## Page 39\n\nThe LSTM+\nxtht-1cthtcthtct-1ht-1xttanh\n+\u03c3tanh\u03c3\u03c3+++igf\no\u3f4b\u3f4b\u3f4bLSTMct-1\n\n## Page 40\n\nUnitsh\nxxtxtht-1htht\nct-1ct\nht-1(b)(a)(c)\u2303gza\u2303gzLSTMUnita\nFFNSRNLSTM\n\n## Page 41\n\nRNNs and LSTMsThe LSTM\n\n## Page 42\n\nRNNs and LSTMsThe LSTM Encoder-Decoder Architecture",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 44,
      "token_count": 780,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 39\n\nThe LSTM+\nxtht-1cthtcthtct-1ht-1xttanh\n+\u03c3tanh\u03c3\u03c3+++igf\no\u3f4b\u3f4b\u3f4bLSTMct-1\n\n## Page 40\n\nUnitsh\nxxtxtht-1htht\nct-1ct\nht-1(b)(a)(c)\u2303gza\u2303gzLSTMUnita\nFFNSRNLSTM\n\n## Page 41\n\nRNNs and LSTMsThe LSTM\n\n## Page 42\n\nRNNs and LSTMsThe LSTM Encoder-Decoder Architecture\n\n## Page 43\n\nFour architectures for NLP tasks with RNNs\n\u2026Encoder RNNDecoder RNNContext\u2026\nx1x2xny1y2ym\u2026RNNx1x2xn\u2026y1y2yn\u2026RNNx1x2xny\n\u2026RNNx1x2xt-1\u2026x2x3xta) sequence labeling b) sequence classification \nc) language modelingd) encoder-decoder\n\n## Page 44\n\n3 components of an encoder-decoder1. An encoder that accepts an input sequence, x1:n, and generates a corresponding sequence of contextualized representa3ons, h1:n. 2. A context vector, c, which is a func3on of h1:n, and conveys the essence of the input to the decoder. 3. A decoder, which accepts c as input and generates an arbitrary length sequence of hidden states h1:m, from which a corresponding sequence of output states y1:m, can be obtained\n\n## Page 45\n\nEncoder-decoder\n\u2026EncoderDecoderContext\u2026\nx1x2xny1y2ym",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 45,
      "token_count": 372,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 46",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 46,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Encoder-decoder for translaAon8.7\u2022THEENCODER-DECODERMODEL WITHRNNS19\n\u2026EncoderDecoderContext\u2026\nx1x2xny1y2ym\nFigure 8.16The encoder-decoder architecture. The context is a function of the hiddenrepresentations of the input, and may be used by the decoder in a variety of ways.by any kind of sequence architecture.In this section we\u2019ll describe an encoder-decoder network based on a pair ofRNNs, but we\u2019ll see in Chapter 13 how to apply them to transformers as well. We\u2019llbuild up the equations for encoder-decoder models by starting with the conditionalRNN language modelp(y), the probability of a sequencey.Recall that in any language model, we can break down the probability as follows:p(y)=p(y1)p(y2|y1)p(y3|y1,y2)...p(ym|y1,. . . ,ym\u00001)(8.28)In RNN language modeling, at a particular timet, we pass the pre\ufb01x oft\u00001tokens through the language model, using forward inference to produce a sequenceof hidden states, ending with the hidden state corresponding to the last word ofthe pre\ufb01x. We then use the \ufb01nal hidden state of the pre\ufb01x as our starting point togenerate the next token.More formally, ifgis an activation function liketanhor ReLU, a function ofthe input at timetand the hidden state at timet\u00001, and the softmax is over theset of possible vocabulary items, then at timetthe outputytand hidden statehtarecomputed as:ht=g(ht\u00001,xt)(8.29)\u02c6yt=softmax(ht)(8.30)We only have to make one slight change to turn this language model with au-toregressive generation into an encoder-decoder model that is a translation modelthat can translate from asource textin one language to atarget textin a second:add asentence separationmarker at the end of the source text, and then simplysentenceseparationconcatenate the target text.Let\u2019s use<s>for our sentence separator token, and let\u2019s think about translatingan English source text (\u201cthe green witch arrived\u201d), to a Spanish sentence (\u201clleg\u00b4ola bruja verde\u201d (which can be glossed word-by-word as \u2018arrived the witch green\u2019).We could also illustrate encoder-decoder models with a question-answer pair, or atext-summarization pair.Let\u2019s usexto refer to the source text (in this case in English) plus the separatortoken<s>, andyto refer to the target texty(in this case in Spanish). Then anencoder-decoder model computes the probabilityp(y|x)as follows:p(y|x)=p(y1|x)p(y2|y1,x)p(y3|y1,y2,x)...p(ym|y1,. . . ,ym\u00001,x)(8.31)Fig.8.17shows the setup for a simpli\ufb01ed version of the encoder-decoder model(we\u2019ll see the full model, which requires the new concept ofattention, in the nextsection).8.7\u2022THEENCODER-DECODERMODEL WITHRNNS19\n\u2026EncoderDecoderContext\u2026\nx1x2xny1y2ym",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 47,
      "token_count": 696,
      "chapter_title": ""
    }
  },
  {
    "content": "\u2026EncoderDecoderContext\u2026\nx1x2xny1y2ym\nFigure 8.16The encoder-decoder architecture. The context is a function of the hiddenrepresentations of the input, and may be used by the decoder in a variety of ways.by any kind of sequence architecture.In this section we\u2019ll describe an encoder-decoder network based on a pair ofRNNs, but we\u2019ll see in Chapter 13 how to apply them to transformers as well. We\u2019llbuild up the equations for encoder-decoder models by starting with the conditionalRNN language modelp(y), the probability of a sequencey.Recall that in any language model, we can break down the probability as follows:p(y)=p(y1)p(y2|y1)p(y3|y1,y2)...p(ym|y1,. . . ,ym\u00001)(8.28)In RNN language modeling, at a particular timet, we pass the pre\ufb01x oft\u00001tokens through the language model, using forward inference to produce a sequenceof hidden states, ending with the hidden state corresponding to the last word ofthe pre\ufb01x. We then use the \ufb01nal hidden state of the pre\ufb01x as our starting point togenerate the next token.More formally, ifgis an activation function liketanhor ReLU, a function ofthe input at timetand the hidden state at timet\u00001, and the softmax is over theset of possible vocabulary items, then at timetthe outputytand hidden statehtarecomputed as:ht=g(ht\u00001,xt)(8.29)\u02c6yt=softmax(ht)(8.30)We only have to make one slight change to turn this language model with au-toregressive generation into an encoder-decoder model that is a translation modelthat can translate from asource textin one language to atarget textin a second:add asentence separationmarker at the end of the source text, and then simplysentenceseparationconcatenate the target text.Let\u2019s use<s>for our sentence separator token, and let\u2019s think about translatingan English source text (\u201cthe green witch arrived\u201d), to a Spanish sentence (\u201clleg\u00b4ola bruja verde\u201d (which can be glossed word-by-word as \u2018arrived the witch green\u2019).We could also illustrate encoder-decoder models with a question-answer pair, or atext-summarization pair.Let\u2019s usexto refer to the source text (in this case in English) plus the separatortoken<s>, andyto refer to the target texty(in this case in Spanish). Then anencoder-decoder model computes the probabilityp(y|x)as follows:p(y|x)=p(y1|x)p(y2|y1,x)p(y3|y1,y2,x)...p(ym|y1,. . . ,ym\u00001,x)(8.31)Fig.8.17shows the setup for a simpli\ufb01ed version of the encoder-decoder model(we\u2019ll see the full model, which requires the new concept ofattention, in the nextsection).Regular language modeling",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 48,
      "token_count": 640,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 47\n\nEncoder-decoder for translaAon8.7\u2022THEENCODER-DECODERMODEL WITHRNNS19\n\u2026EncoderDecoderContext\u2026\nx1x2xny1y2ym\nFigure 8.16The encoder-decoder architecture. The context is a function of the hiddenrepresentations of the input, and may be used by the decoder in a variety of ways.by any kind of sequence architecture.In this section we\u2019ll describe an encoder-decoder network based on a pair ofRNNs, but we\u2019ll see in Chapter 13 how to apply them to transformers as well. We\u2019llbuild up the equations for encoder-decoder models by starting with the conditionalRNN language modelp(y), the probability of a sequencey.Recall that in any language model, we can break down the probability as follows:p(y)=p(y1)p(y2|y1)p(y3|y1,y2)...p(ym|y1,. . . ,ym\u00001)(8.28)In RNN language modeling, at a particular timet, we pass the pre\ufb01x oft\u00001tokens through the language model, using forward inference to produce a sequenceof hidden states, ending with the hidden state corresponding to the last word ofthe pre\ufb01x. We then use the \ufb01nal hidden state of the pre\ufb01x as our starting point togenerate the next token.More formally, ifgis an activation function liketanhor ReLU, a function ofthe input at timetand the hidden state at timet\u00001, and the softmax is over theset of possible vocabulary items, then at timetthe outputytand hidden statehtarecomputed as:ht=g(ht\u00001,xt)(8.29)\u02c6yt=softmax(ht)(8.30)We only have to make one slight change to turn this language model with au-toregressive generation into an encoder-decoder model that is a translation modelthat can translate from asource textin one language to atarget textin a second:add asentence separationmarker at the end of the source text, and then simplysentenceseparationconcatenate the target text.Let\u2019s use<s>for our sentence separator token, and let\u2019s think about translatingan English source text (\u201cthe green witch arrived\u201d), to a Spanish sentence (\u201clleg\u00b4ola bruja verde\u201d (which can be glossed word-by-word as \u2018arrived the witch green\u2019).We could also illustrate encoder-decoder models with a question-answer pair, or atext-summarization pair.Let\u2019s usexto refer to the source text (in this case in English) plus the separatortoken<s>, andyto refer to the target texty(in this case in Spanish). Then anencoder-decoder model computes the probabilityp(y|x)as follows:p(y|x)=p(y1|x)p(y2|y1,x)p(y3|y1,y2,x)...p(ym|y1,. . . ,ym\u00001,x)(8.31)Fig.8.17shows the setup for a simpli\ufb01ed version of the encoder-decoder model(we\u2019ll see the full model, which requires the new concept ofattention, in the nextsection).Let x be the source text plus a separate token <s> and y the targetLet x = The green witch arrive <s>Let y = llego \u0301 la bruja verde \n\n## Page 48\n\nEncoder-decoder simplified\nSource TextTarget Text\nhnembeddinglayerhiddenlayer(s)softmaxthegreenlleg\u00f3\nwitcharrived<s>lleg\u00f3la\nlabruja\nbrujaverde\nverde</s>(output of source is ignored)\nSeparator",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 49,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 48\n\nEncoder-decoder simplified\nSource TextTarget Text\nhnembeddinglayerhiddenlayer(s)softmaxthegreenlleg\u00f3\nwitcharrived<s>lleg\u00f3la\nlabruja\nbrujaverde\nverde</s>(output of source is ignored)\nSeparator\n\n## Page 49\n\nEncoder-decoder showing context\nEncoderDecoder\nhn hd1he3he2he1hd2hd3hd4embeddinglayerhiddenlayer(s)softmaxx1x2y1hdmx3xn<s>y1y2\ny2y3\ny3y4\nym</s>hen = c = hd0(output is ignored during encoding)20CHAPTER8\u2022RNNSA N DLSTMS\nSource TextTarget Text\nhnembeddinglayerhiddenlayer(s)softmaxthegreenlleg\u00f3\nwitcharrived<s>lleg\u00f3la\nlabruja\nbrujaverde\nverde</s>(output of source is ignored)\nSeparatorFigure 8.17Translating a single sentence (inference time) in the basic RNN version of encoder-decoder ap-proach to machine translation. Source and target sentences are concatenated with a separator token in between,and the decoder uses context information from the encoder\u2019s last hidden state.Fig.8.17shows an English source text (\u201cthe green witch arrived\u201d), a sentenceseparator token (<s>, and a Spanish target text (\u201clleg\u00b4o la bruja verde\u201d). To trans-late a source text, we run it through the network performing forward inference togenerate hidden states until we get to the end of the source. Then we begin autore-gressive generation, asking for a word in the context of the hidden layer from theend of the source input as well as the end-of-sentence marker. Subsequent wordsare conditioned on the previous hidden state and the embedding for the last wordgenerated.Let\u2019s formalize and generalize this model a bit in Fig.8.18. (To help keep thingsstraight, we\u2019ll use the superscriptseanddwhere needed to distinguish the hiddenstates of the encoder and the decoder.) The elements of the network on the leftprocess the input sequencexand comprise theencoder. While our simpli\ufb01ed \ufb01gureshows only a single network layer for the encoder, stacked architectures are thenorm, where the output states from the top layer of the stack are taken as the \ufb01nalrepresentation, and the encoder consists of stacked biLSTMs where the hidden statesfrom top layers from the forward and backward passes are concatenated to providethe contextualized representations for each time step.The entire purpose of the encoder is to generate a contextualized representationof the input. This representation is embodied in the \ufb01nal hidden state of the encoder,hen. This representation, also calledcforcontext, is then passed to the decoder.The simplest version of thedecodernetwork would take this state and use itjust to initialize the \ufb01rst hidden state of the decoder; the \ufb01rst decoder RNN cellwould usecas its prior hidden statehd0. The decoder would then autoregressivelygenerates a sequence of outputs, an element at a time, until an end-of-sequencemarker is generated. Each hidden state is conditioned on the previous hidden stateand the output generated in the previous state.As Fig.8.18shows, we do something more complex: we make the context vectorcavailable to more than just the \ufb01rst decoder hidden state, to ensure that the in\ufb02uenceof the context vector,c, doesn\u2019t wane as the output sequence is generated. We dothis by addingcas a parameter to the computation of the current hidden state. usingthe following equation:hdt=g(\u02c6yt\u00001,hdt\u00001,c)(8.32)",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 50,
      "token_count": 784,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 50\n\nEncoder-decoder equaAons\ng is a stand-in for some \ufb02avor of RNN y\u02c6t\u22121 is the embedding for the output sampled from the so9max at the previous step\u02c6yt is a vector of probabili=es over the vocabulary, represen=ng the probability of each word occurring at =me t. To generate text, we sample from this distribu=on \u02c6yt . 8.7\u2022THEENCODER-DECODERMODEL WITHRNNS21\nEncoderDecoder\nhn hd1he3he2he1hd2hd3hd4embeddinglayerhiddenlayer(s)softmaxx1x2y1hdmx3xn<s>y1y2\ny2y3\ny3y4\nym</s>hen = c = hd0(output is ignored during encoding)\nFigure 8.18A more formal version of translating a sentence at inference time in the basic RNN-basedencoder-decoder architecture. The \ufb01nal hidden state of the encoder RNN,hen, serves as the context for thedecoder in its role ashd0in the decoder RNN, and is also made available to each decoder hidden state.Now we\u2019re ready to see the full equations for this version of the decoder in the basicencoder-decoder model, with context available at each decoding timestep. Recallthatgis a stand-in for some \ufb02avor of RNN and \u02c6yt\u00001is the embedding for the outputsampled from the softmax at the previous step:c=henhd0=chdt=g(\u02c6yt\u00001,hdt\u00001,c)\u02c6yt=softmax(hdt)(8.33)Thus\u02c6ytis a vector of probabilities over the vocabulary, representing the probabilityof each word occurring at timet. To generate text, we sample from this distribution\u02c6yt. For example, the greedy choice is simply to choose the most probable word togenerate at each timestep. We\u2019ll introduce more sophisticated sampling methods inSection??.8.7.1 Training the Encoder-Decoder ModelEncoder-decoder architectures are trained end-to-end. Each training example is atuple of paired strings, a source and a target. Concatenated with a separator token,these source-target pairs can now serve as training data.For MT, the training data typically consists of sets of sentences and their transla-tions. These can be drawn from standard datasets of aligned sentence pairs, as we\u2019lldiscuss in Section??. Once we have a training set, the training itself proceeds aswith any RNN-based language model. The network is given the source text and thenstarting with the separator token is trained autoregressively to predict the next word,as shown in Fig.8.19.Note the differences between training (Fig.8.19) and inference (Fig.8.17) withrespect to the outputs at each time step. The decoder during inference uses its ownestimated output \u02c6ytas the input for the next time stepxt+1. Thus the decoder willtend to deviate more and more from the gold target sentence as it keeps generatingmore tokens. In training, therefore, it is more common to useteacher forcingin theteacher forcingdecoder. Teacher forcing means that we force the system to use the gold target token",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 51,
      "token_count": 688,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 51\n\nTraining the encoder-decoder with teacher forcing\nEncoderDecoder\nembeddinglayerhiddenlayer(s)softmaxthegreenlleg\u00f3\nwitcharrived<s>lleg\u00f3la\nlabruja\nbrujaverde\nverde</s>goldanswersL1 =-log P(y1)\nx1x2x3x4L2 =-log P(y2)L3 =-log P(y3)L4 =-log P(y4)L5 =-log P(y5)per-wordlossy1y2y3y4y5Total loss is the average cross-entropy loss per target word:\n\n## Page 52\n\nRNNs and LSTMsThe LSTM Encoder-Decoder Architecture\n\n## Page 53\n\nRNNs and LSTMsLSTM ABenCon\n\n## Page 54\n\nProblem with passing context c only from endRequiring the context c to be only the encoder\u2019s \ufb01nal hidden state forces all the informa<on from the en<re source sentence to pass through this representa<onal bo_leneck. EncoderDecoderbottleneckbottleneck",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 52,
      "token_count": 233,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 52\n\nRNNs and LSTMsThe LSTM Encoder-Decoder Architecture\n\n## Page 53\n\nRNNs and LSTMsLSTM ABenCon\n\n## Page 54\n\nProblem with passing context c only from endRequiring the context c to be only the encoder\u2019s \ufb01nal hidden state forces all the informa<on from the en<re source sentence to pass through this representa<onal bo_leneck. EncoderDecoderbottleneckbottleneck\n\n## Page 55\n\nSoluAon: aRenAoninstead of being taken from the last hidden state, the context it\u2019s a weighted average of all the hidden states of the decoder. this weighted average is also informed by part of the decoder state as well, the state of the decoder right before the current token i. 8.8\u2022ATTENTION23In the attention mechanism, as in the vanilla encoder-decoder model, the contextvectorcis a single vector that is a function of the hidden states of the encoder. Butinstead of being taken from the last hidden state, it\u2019s a weighted average ofallthehidden states of the decoder. And this weighted average is also informed by part ofthe decoder state as well, the state of the decoder right before the current tokeni.That is,c=f(he1...hen,hdi\u00001). The weights focus on (\u2018attend to\u2019) a particular part ofthe source text that is relevant for the tokenithat the decoder is currently producing.Attention thus replaces the static context vector with one that is dynamically derivedfrom the encoder hidden states, but also informed by and hence different for eachtoken in decoding.This context vector,ci, is generated anew with each decoding stepiand takesall of the encoder hidden states into account in its derivation. We then make thiscontext available during decoding by conditioning the computation of the currentdecoder hidden state on it (along with the prior hidden state and the previous outputgenerated by the decoder), as we see in this equation (and Fig.8.21):hdi=g(\u02c6yi\u00001,hdi\u00001,ci)(8.34)hd1hd2hdiy1y2yic1c2ci\u2026\u2026Figure 8.21The attention mechanism allows each hidden state of the decoder to see adifferent, dynamic, context, which is a function of all the encoder hidden states.The \ufb01rst step in computingciis to compute how much to focus on each encoderstate, howrelevanteach encoder state is to the decoder state captured inhdi\u00001.W ecapture relevance by computing\u2014 at each stateiduring decoding\u2014ascore(hdi\u00001,hej)for each encoder statej.The simplest such score, calleddot-product attention, implements relevance asdot-productattentionsimilarity: measuring how similar the decoder hidden state is to an encoder hiddenstate, by computing the dot product between them:score(hdi\u00001,hej)=hdi\u00001\u00b7hej(8.35)The score that results from this dot product is a scalar that re\ufb02ects the degree ofsimilarity between the two vectors. The vector of these scores across all the encoderhidden states gives us the relevance of each encoder state to the current step of thedecoder.To make use of these scores, we\u2019ll normalize them with a softmax to create avector of weights,aij, that tells us the proportional relevance of each encoder hiddenstatejto the prior hidden decoder state,hdi\u00001.aij=softmax(score(hdi\u00001,hej))=exp(score(hdi\u00001,hej)Pkexp(score(hdi\u00001,hek))(8.36)Finally, given the distribution ina, we can compute a \ufb01xed-length context vector forthe current decoder state by taking a weighted average over all the encoder hidden",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 53,
      "token_count": 785,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 56\n\nARenAon8.8\u2022ATTENTION23In the attention mechanism, as in the vanilla encoder-decoder model, the contextvectorcis a single vector that is a function of the hidden states of the encoder. Butinstead of being taken from the last hidden state, it\u2019s a weighted average ofallthehidden states of the decoder. And this weighted average is also informed by part ofthe decoder state as well, the state of the decoder right before the current tokeni.That is,c=f(he1...hen,hdi\u00001). The weights focus on (\u2018attend to\u2019) a particular part ofthe source text that is relevant for the tokenithat the decoder is currently producing.Attention thus replaces the static context vector with one that is dynamically derivedfrom the encoder hidden states, but also informed by and hence different for eachtoken in decoding.This context vector,ci, is generated anew with each decoding stepiand takesall of the encoder hidden states into account in its derivation. We then make thiscontext available during decoding by conditioning the computation of the currentdecoder hidden state on it (along with the prior hidden state and the previous outputgenerated by the decoder), as we see in this equation (and Fig.8.21):hdi=g(\u02c6yi\u00001,hdi\u00001,ci)(8.34)hd1hd2hdiy1y2yic1c2ci\u2026\u2026Figure 8.21The attention mechanism allows each hidden state of the decoder to see adifferent, dynamic, context, which is a function of all the encoder hidden states.The \ufb01rst step in computingciis to compute how much to focus on each encoderstate, howrelevanteach encoder state is to the decoder state captured inhdi\u00001.W ecapture relevance by computing\u2014 at each stateiduring decoding\u2014ascore(hdi\u00001,hej)for each encoder statej.The simplest such score, calleddot-product attention, implements relevance asdot-productattentionsimilarity: measuring how similar the decoder hidden state is to an encoder hiddenstate, by computing the dot product between them:score(hdi\u00001,hej)=hdi\u00001\u00b7hej(8.35)The score that results from this dot product is a scalar that re\ufb02ects the degree ofsimilarity between the two vectors. The vector of these scores across all the encoderhidden states gives us the relevance of each encoder state to the current step of thedecoder.To make use of these scores, we\u2019ll normalize them with a softmax to create avector of weights,aij, that tells us the proportional relevance of each encoder hiddenstatejto the prior hidden decoder state,hdi\u00001.aij=softmax(score(hdi\u00001,hej))=exp(score(hdi\u00001,hej)Pkexp(score(hdi\u00001,hek))(8.36)Finally, given the distribution ina, we can compute a \ufb01xed-length context vector forthe current decoder state by taking a weighted average over all the encoder hiddenhd1hd2hdiy1y2yic1c2ci\u2026\u2026",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 54,
      "token_count": 642,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 57",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 55,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "How to compute c?We'll create a score that tells us how much to focus on each encoder state, how relevant each encoder state is to the decoder state:We\u2019ll normalize them with a so`max to create weights \u03b1i j , that tell us the relevance of encoder hidden state j to hidden decoder state,  hdi-1And then use this to help create a weighted average:8.8\u2022ATTENTION23In the attention mechanism, as in the vanilla encoder-decoder model, the contextvectorcis a single vector that is a function of the hidden states of the encoder. Butinstead of being taken from the last hidden state, it\u2019s a weighted average ofallthehidden states of the decoder. And this weighted average is also informed by part ofthe decoder state as well, the state of the decoder right before the current tokeni.That is,c=f(he1...hen,hdi\u00001). The weights focus on (\u2018attend to\u2019) a particular part ofthe source text that is relevant for the tokenithat the decoder is currently producing.Attention thus replaces the static context vector with one that is dynamically derivedfrom the encoder hidden states, but also informed by and hence different for eachtoken in decoding.This context vector,ci, is generated anew with each decoding stepiand takesall of the encoder hidden states into account in its derivation. We then make thiscontext available during decoding by conditioning the computation of the currentdecoder hidden state on it (along with the prior hidden state and the previous outputgenerated by the decoder), as we see in this equation (and Fig.8.21):hdi=g(\u02c6yi\u00001,hdi\u00001,ci)(8.34)hd1hd2hdiy1y2yic1c2ci\u2026\u2026Figure 8.21The attention mechanism allows each hidden state of the decoder to see adifferent, dynamic, context, which is a function of all the encoder hidden states.The \ufb01rst step in computingciis to compute how much to focus on each encoderstate, howrelevanteach encoder state is to the decoder state captured inhdi\u00001.W ecapture relevance by computing\u2014 at each stateiduring decoding\u2014ascore(hdi\u00001,hej)for each encoder statej.The simplest such score, calleddot-product attention, implements relevance asdot-productattentionsimilarity: measuring how similar the decoder hidden state is to an encoder hiddenstate, by computing the dot product between them:score(hdi\u00001,hej)=hdi\u00001\u00b7hej(8.35)The score that results from this dot product is a scalar that re\ufb02ects the degree ofsimilarity between the two vectors. The vector of these scores across all the encoderhidden states gives us the relevance of each encoder state to the current step of thedecoder.To make use of these scores, we\u2019ll normalize them with a softmax to create avector of weights,aij, that tells us the proportional relevance of each encoder hiddenstatejto the prior hidden decoder state,hdi\u00001.aij=softmax(score(hdi\u00001,hej))=exp(score(hdi\u00001,hej)Pkexp(score(hdi\u00001,hek))(8.36)Finally, given the distribution ina, we can compute a \ufb01xed-length context vector forthe current decoder state by taking a weighted average over all the encoder hidden8.8\u2022ATTENTION23In the attention mechanism, as in the vanilla encoder-decoder model, the contextvectorcis a single vector that is a function of the hidden states of the encoder. Butinstead of being taken from the last hidden state, it\u2019s a weighted average ofallthehidden states of the decoder. And this weighted average is also informed by part ofthe decoder state as well, the state of the decoder right before the current tokeni.That",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 56,
      "token_count": 789,
      "chapter_title": ""
    }
  },
  {
    "content": "is to an encoder hiddenstate, by computing the dot product between them:score(hdi\u00001,hej)=hdi\u00001\u00b7hej(8.35)The score that results from this dot product is a scalar that re\ufb02ects the degree ofsimilarity between the two vectors. The vector of these scores across all the encoderhidden states gives us the relevance of each encoder state to the current step of thedecoder.To make use of these scores, we\u2019ll normalize them with a softmax to create avector of weights,aij, that tells us the proportional relevance of each encoder hiddenstatejto the prior hidden decoder state,hdi\u00001.aij=softmax(score(hdi\u00001,hej))=exp(score(hdi\u00001,hej)Pkexp(score(hdi\u00001,hek))(8.36)Finally, given the distribution ina, we can compute a \ufb01xed-length context vector forthe current decoder state by taking a weighted average over all the encoder hidden8.8\u2022ATTENTION23In the attention mechanism, as in the vanilla encoder-decoder model, the contextvectorcis a single vector that is a function of the hidden states of the encoder. Butinstead of being taken from the last hidden state, it\u2019s a weighted average ofallthehidden states of the decoder. And this weighted average is also informed by part ofthe decoder state as well, the state of the decoder right before the current tokeni.That is,c=f(he1...hen,hdi\u00001). The weights focus on (\u2018attend to\u2019) a particular part ofthe source text that is relevant for the tokenithat the decoder is currently producing.Attention thus replaces the static context vector with one that is dynamically derivedfrom the encoder hidden states, but also informed by and hence different for eachtoken in decoding.This context vector,ci, is generated anew with each decoding stepiand takesall of the encoder hidden states into account in its derivation. We then make thiscontext available during decoding by conditioning the computation of the currentdecoder hidden state on it (along with the prior hidden state and the previous outputgenerated by the decoder), as we see in this equation (and Fig.8.21):hdi=g(\u02c6yi\u00001,hdi\u00001,ci)(8.34)hd1hd2hdiy1y2yic1c2ci\u2026\u2026Figure 8.21The attention mechanism allows each hidden state of the decoder to see adifferent, dynamic, context, which is a function of all the encoder hidden states.The \ufb01rst step in computingciis to compute how much to focus on each encoderstate, howrelevanteach encoder state is to the decoder state captured inhdi\u00001.W ecapture relevance by computing\u2014 at each stateiduring decoding\u2014ascore(hdi\u00001,hej)for each encoder statej.The simplest such score, calleddot-product attention, implements relevance asdot-productattentionsimilarity: measuring how similar the decoder hidden state is to an encoder hiddenstate, by computing the dot product between them:score(hdi\u00001,hej)=hdi\u00001\u00b7hej(8.35)The score that results from this dot product is a scalar that re\ufb02ects the degree ofsimilarity between the two vectors. The vector of these scores across all the encoderhidden states gives us the relevance of each encoder state to the current step of thedecoder.To make use of these scores, we\u2019ll normalize them with a softmax to create avector of weights,aij, that tells us the proportional relevance of each encoder hiddenstatejto the prior hidden decoder state,hdi\u00001.aij=softmax(score(hdi\u00001,hej))=exp(score(hdi\u00001,hej)Pkexp(score(hdi\u00001,hek))(8.36)Finally, given the distribution ina, we can",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 57,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "state of the decoder to see adifferent, dynamic, context, which is a function of all the encoder hidden states.The \ufb01rst step in computingciis to compute how much to focus on each encoderstate, howrelevanteach encoder state is to the decoder state captured inhdi\u00001.W ecapture relevance by computing\u2014 at each stateiduring decoding\u2014ascore(hdi\u00001,hej)for each encoder statej.The simplest such score, calleddot-product attention, implements relevance asdot-productattentionsimilarity: measuring how similar the decoder hidden state is to an encoder hiddenstate, by computing the dot product between them:score(hdi\u00001,hej)=hdi\u00001\u00b7hej(8.35)The score that results from this dot product is a scalar that re\ufb02ects the degree ofsimilarity between the two vectors. The vector of these scores across all the encoderhidden states gives us the relevance of each encoder state to the current step of thedecoder.To make use of these scores, we\u2019ll normalize them with a softmax to create avector of weights,aij, that tells us the proportional relevance of each encoder hiddenstatejto the prior hidden decoder state,hdi\u00001.aij=softmax(score(hdi\u00001,hej))=exp(score(hdi\u00001,hej)Pkexp(score(hdi\u00001,hek))(8.36)Finally, given the distribution ina, we can compute a \ufb01xed-length context vector forthe current decoder state by taking a weighted average over all the encoder hidden24CHAPTER8\u2022RNNSA N DLSTMSstates.ci=Xjaijhej(8.37)With this, we \ufb01nally have a \ufb01xed-length context vector that takes into accountinformation from the entire encoder state that is dynamically updated to re\ufb02ect theneeds of the decoder at each step of decoding. Fig.8.22illustrates an encoder-decoder network with attention, focusing on the computation of one context vectorci.",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 58,
      "token_count": 420,
      "chapter_title": ""
    }
  },
  {
    "content": "EncoderDecoder\nhdi-1he3he2he1hdihiddenlayer(s)x1x2yi-1x3xnyi-2yi-1yihen ci.2.1.3.4attentionweightsci-1ci",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 59,
      "token_count": 53,
      "chapter_title": ""
    }
  },
  {
    "content": "<latexit sha1_base64=\"TNdNmv/RIlrhPa6LgQyjjQLqyBA=\">AAACAnicdVDLSsNAFJ3UV62vqCtxM1gEVyHpI9Vd0Y3LCvYBTQyT6bSddvJgZiKUUNz4K25cKOLWr3Dn3zhpK6jogQuHc+7l3nv8mFEhTfNDyy0tr6yu5dcLG5tb2zv67l5LRAnHpIkjFvGOjwRhNCRNSSUjnZgTFPiMtP3xRea3bwkXNAqv5SQmboAGIe1TjKSSPP3AEUngjVIHsXiIvJSOpnB4Q7zR1NOLpmGaVbtqQdOwLbtk24qY5Yp9VoOWsjIUwQINT393ehFOAhJKzJAQXcuMpZsiLilmZFpwEkFihMdoQLqKhiggwk1nL0zhsVJ6sB9xVaGEM/X7RIoCISaBrzoDJIfit5eJf3ndRPZP3ZSGcSJJiOeL+gmDMoJZHrBHOcGSTRRBmFN1K8RDxBGWKrWCCuHrU/g/aZUMyzbKV5Vi/XwRRx4cgiNwAixQA3VwCRqgCTC4Aw/gCTxr99qj9qK9zltz2mJmH/yA9vYJSymYCA==</latexit>Xj\u0000ijhej\u0000ij<latexit sha1_base64=\"y8s4mGdpwrGrBnuSR+p1gJJXYdo=\">AAAB/nicdVDJSgNBEO2JW4zbqHjy0hgEL4YeJyQBL0EvHiOYBbIMPT09mTY9C909QhgC/ooXD4p49Tu8+Td2FkFFHxQ83quiqp6bcCYVQh9Gbml5ZXUtv17Y2Nza3jF391oyTgWhTRLzWHRcLClnEW0qpjjtJILi0OW07Y4up377jgrJ4uhGjRPaD/EwYj4jWGnJMQ+Cgedk7NSa9IgXq955MKDOrWMWUQnNAFGpYtfsakUTZNtWGUFrYRXBAg3HfO95MUlDGinCsZRdCyWqn2GhGOF0UuilkiaYjPCQdjWNcEhlP5udP4HHWvGgHwtdkYIz9ftEhkMpx6GrO0OsAvnbm4p/ed1U+bV+xqIkVTQi80V+yqGK4TQL6DFBieJjTTARTN8KSYAFJkonVtAhfH0K/yets5JVKdnX5WL9YhFHHhyCI3ACLFAFdXAFGqAJCMjAA3gCz8a98Wi8GK/z1pyxmNkHP2C8fQICDpWK</latexit>hdi\u00001\u00b7hej\u2026\u2026Figure 8.22A sketch of the encoder-decoder network with attention, focusing on the computation ofci. Thecontext valueciis one of the",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 60,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "8.22A sketch of the encoder-decoder network with attention, focusing on the computation ofci. Thecontext valueciis one of the inputs to the computation ofhdi. It is computed by taking the weighted sum of allthe encoder hidden states, each weighted by their dot product with the prior decoder hidden statehdi\u00001.It\u2019s also possible to create more sophisticated scoring functions for attentionmodels. Instead of simple dot product attention, we can get a more powerful functionthat computes the relevance of each encoder hidden state to the decoder hidden stateby parameterizing the score with its own set of weights,Ws.score(hdi\u00001,hej)=hdt\u00001WshejThe weightsWs, which are then trained during normal end-to-end training, give thenetwork the ability to learn which aspects of similarity between the decoder andencoder states are important to the current application. This bilinear model alsoallows the encoder and decoder to use different dimensional vectors, whereas thesimple dot-product attention requires that the encoder and decoder hidden stateshave the same dimensionality.We\u2019ll return to the concept of attention when we de\ufb01ne the transformer archi-tecture in Chapter 9, which is based on a slight modi\ufb01cation of attention calledself-attention.8.9 SummaryThis chapter has introduced the concepts of recurrent neural networks and how theycan be applied to language problems. Here\u2019s a summary of the main points that we",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 61,
      "token_count": 290,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 58",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 62,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Encoder-decoder with a<en=on, focusing on the computa=on of c\nEncoderDecoder\nhdi-1he3he2he1hdihiddenlayer(s)x1x2yi-1x3xnyi-2yi-1yihen ci.2.1.3.4attentionweightsci-1ci",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 63,
      "token_count": 73,
      "chapter_title": ""
    }
  },
  {
    "content": "<latexit sha1_base64=\"TNdNmv/RIlrhPa6LgQyjjQLqyBA=\">AAACAnicdVDLSsNAFJ3UV62vqCtxM1gEVyHpI9Vd0Y3LCvYBTQyT6bSddvJgZiKUUNz4K25cKOLWr3Dn3zhpK6jogQuHc+7l3nv8mFEhTfNDyy0tr6yu5dcLG5tb2zv67l5LRAnHpIkjFvGOjwRhNCRNSSUjnZgTFPiMtP3xRea3bwkXNAqv5SQmboAGIe1TjKSSPP3AEUngjVIHsXiIvJSOpnB4Q7zR1NOLpmGaVbtqQdOwLbtk24qY5Yp9VoOWsjIUwQINT393ehFOAhJKzJAQXcuMpZsiLilmZFpwEkFihMdoQLqKhiggwk1nL0zhsVJ6sB9xVaGEM/X7RIoCISaBrzoDJIfit5eJf3ndRPZP3ZSGcSJJiOeL+gmDMoJZHrBHOcGSTRRBmFN1K8RDxBGWKrWCCuHrU/g/aZUMyzbKV5Vi/XwRRx4cgiNwAixQA3VwCRqgCTC4Aw/gCTxr99qj9qK9zltz2mJmH/yA9vYJSymYCA==</latexit>Xj\u21b5ijhej\u21b5ij<latexit sha1_base64=\"y8s4mGdpwrGrBnuSR+p1gJJXYdo=\">AAAB/nicdVDJSgNBEO2JW4zbqHjy0hgEL4YeJyQBL0EvHiOYBbIMPT09mTY9C909QhgC/ooXD4p49Tu8+Td2FkFFHxQ83quiqp6bcCYVQh9Gbml5ZXUtv17Y2Nza3jF391oyTgWhTRLzWHRcLClnEW0qpjjtJILi0OW07Y4up377jgrJ4uhGjRPaD/EwYj4jWGnJMQ+Cgedk7NSa9IgXq955MKDOrWMWUQnNAFGpYtfsakUTZNtWGUFrYRXBAg3HfO95MUlDGinCsZRdCyWqn2GhGOF0UuilkiaYjPCQdjWNcEhlP5udP4HHWvGgHwtdkYIz9ftEhkMpx6GrO0OsAvnbm4p/ed1U+bV+xqIkVTQi80V+yqGK4TQL6DFBieJjTTARTN8KSYAFJkonVtAhfH0K/yets5JVKdnX5WL9YhFHHhyCI3ACLFAFdXAFGqAJCMjAA3gCz8a98Wi8GK/z1pyxmNkHP2C8fQICDpWK</latexit>hdi\u00001\u00b7hej\u2026\u2026",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 64,
      "token_count": 770,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 59\n\nRNNs and LSTMsLSTM ABenCon",
    "metadata": {
      "source": "rnnjan25",
      "chunk_id": 65,
      "token_count": 17,
      "chapter_title": ""
    }
  }
]