[
  {
    "content": "# 5_LR_Apr_7_2021\n\n## Page 1\n\nLogistic RegressionBackground: Generative and Discriminative Classifiers\n\n## Page 2\n\nLogistic RegressionImportant analytic tool in natural and social sciencesBaseline supervised machine learning tool for classificationIs also the foundation of neural networks\n\n## Page 3\n\nGenerative and Discriminative ClassifiersNaive Bayes is a generativeclassifierby contrast:Logistic regression is a discriminativeclassifier\n\n## Page 4\n\nGenerative and Discriminative Classifiers\nSuppose we're distinguishing cat from dog images\nimagenetimagenet\n\n## Page 5\n\nGenerative Classifier:\n\u2022Build a model of what's in a cat image\u2022Knows about whiskers, ears, eyes\u2022Assigns a probability to any image: \u2022how cat-y is this image?Also build a model for dog imagesNow given a new image:Run both models and see which one fits better \n\n## Page 6\n\nDiscriminative Classifier\nJust try to distinguish dogs from catsOh look, dogs have collars!Let's ignore everything else",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 0,
      "token_count": 226,
      "chapter_title": "5_LR_Apr_7_2021"
    }
  },
  {
    "content": "## Page 7",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 1,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Finding the correct class c from a document d inGenerative vs Discriminative ClassifiersNaive BayesLogistic Regression",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 2,
      "token_count": 25,
      "chapter_title": ""
    }
  },
  {
    "content": "72CHAPTER5\u2022LOGISTICREGRESSIONMore formally, recall that the naive Bayes assigns a classcto a documentdnotby directly computingP(c|d)but by computing a likelihood and a prior\u02c6c=argmaxc2Clikelihoodz}|{P(d|c)priorz}|{P(c)(5.1)Agenerative modellike naive Bayes makes use of thislikelihoodterm, whichgenerativemodelexpresses how to generate the features of a documentif we knew it was of class c.By contrast adiscriminative modelin this text categorization scenario attemptsdiscriminativemodeltodirectlycomputeP(c|d). Perhaps it will learn to assign high weight to documentfeatures that directly improve its ability todiscriminatebetween possible classes,even if it couldn\u2019t generate an example of one of the classes.Components of a probabilistic machine learning classi\ufb01er:Like naive Bayes,logistic regression is a probabilistic classi\ufb01er that makes use of supervised machinelearning. Machine learning classi\ufb01ers require a training corpus ofMobservationsinput/output pairs(x(i),y(i)). (We\u2019ll use superscripts in parentheses to refer to indi-vidual instances in the training set\u2014for sentiment classi\ufb01cation each instance mightbe an individual document to be classi\ufb01ed). A machine learning system for classi\ufb01-cation then has four components:1.Afeature representationof the input. For each input observationx(i), thiswill be a vector of features[x1,x2,. . . ,xn]. We will generally refer to featureifor inputx(j)asx(j)i, sometimes simpli\ufb01ed asxi, but we will also see thenotationfi,fi(x), or, for multiclass classi\ufb01cation,fi(c,x).2.A classi\ufb01cation function that computes \u02c6y, the estimated class, viap(y|x). Inthe next section we will introduce thesigmoidandsoftmaxtools for classi\ufb01-cation.3.An objective function for learning, usually involving minimizing error ontraining examples. We will introduce thecross-entropy loss function4.An algorithm for optimizing the objective function. We introduce thestochas-tic gradient descentalgorithm.Logistic regression has two phases:training:we train the system (speci\ufb01cally the weightswandb) using stochasticgradient descent and the cross-entropy loss.test:Given a test examplexwe computep(y|x)and return the higher probabilitylabely=1 ory=0.5.1 Classi\ufb01cation: the sigmoidThe goal of binary logistic regression is to train a classi\ufb01er that can make a binarydecision about the class of a new input observation. Here we introduce thesigmoidclassi\ufb01er that will help us make this decision.Consider a single input observationx, which we will represent by a vector offeatures[x1,x2,. . . ,xn](we\u2019ll show sample features in the next subsection). The clas-si\ufb01er outputycan be 1 (meaning the observation is a member of the class) or 0(the observation is not a member of the class). We want to know the probabilityP(y=1|x)that this observation is a member of the class. So perhaps the decision2CHAPTER5\u2022LOGISTICREGRESSIONMore formally, recall that the naive Bayes assigns a classcto a documentdnotby directly computingP(c|d)but by computing a likelihood and a prior\u02c6c=argmaxc2Clikelihoodz}|{P(d|c)priorz}|{P(c)(5.1)Agenerative modellike naive Bayes makes use of thislikelihoodterm, whichgenerativemodelexpresses how to generate the features of a documentif we knew it was of class c.By",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 3,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "cross-entropy loss.test:Given a test examplexwe computep(y|x)and return the higher probabilitylabely=1 ory=0.5.1 Classi\ufb01cation: the sigmoidThe goal of binary logistic regression is to train a classi\ufb01er that can make a binarydecision about the class of a new input observation. Here we introduce thesigmoidclassi\ufb01er that will help us make this decision.Consider a single input observationx, which we will represent by a vector offeatures[x1,x2,. . . ,xn](we\u2019ll show sample features in the next subsection). The clas-si\ufb01er outputycan be 1 (meaning the observation is a member of the class) or 0(the observation is not a member of the class). We want to know the probabilityP(y=1|x)that this observation is a member of the class. So perhaps the decision2CHAPTER5\u2022LOGISTICREGRESSIONMore formally, recall that the naive Bayes assigns a classcto a documentdnotby directly computingP(c|d)but by computing a likelihood and a prior\u02c6c=argmaxc2Clikelihoodz}|{P(d|c)priorz}|{P(c)(5.1)Agenerative modellike naive Bayes makes use of thislikelihoodterm, whichgenerativemodelexpresses how to generate the features of a documentif we knew it was of class c.By contrast adiscriminative modelin this text categorization scenario attemptsdiscriminativemodeltodirectlycomputeP(c|d). Perhaps it will learn to assign high weight to documentfeatures that directly improve its ability todiscriminatebetween possible classes,even if it couldn\u2019t generate an example of one of the classes.Components of a probabilistic machine learning classi\ufb01er:Like naive Bayes,logistic regression is a probabilistic classi\ufb01er that makes use of supervised machinelearning. Machine learning classi\ufb01ers require a training corpus ofMobservationsinput/output pairs(x(i),y(i)). (We\u2019ll use superscripts in parentheses to refer to indi-vidual instances in the training set\u2014for sentiment classi\ufb01cation each instance mightbe an individual document to be classi\ufb01ed). A machine learning system for classi\ufb01-cation then has four components:1.Afeature representationof the input. For each input observationx(i), thiswill be a vector of features[x1,x2,. . . ,xn]. We will generally refer to featureifor inputx(j)asx(j)i, sometimes simpli\ufb01ed asxi, but we will also see thenotationfi,fi(x), or, for multiclass classi\ufb01cation,fi(c,x).2.A classi\ufb01cation function that computes \u02c6y, the estimated class, viap(y|x). Inthe next section we will introduce thesigmoidandsoftmaxtools for classi\ufb01-cation.3.An objective function for learning, usually involving minimizing error ontraining examples. We will introduce thecross-entropy loss function4.An algorithm for optimizing the objective function. We introduce thestochas-tic gradient descentalgorithm.Logistic regression has two phases:training:we train the system (speci\ufb01cally the weightswandb) using stochasticgradient descent and the cross-entropy loss.test:Given a test examplexwe computep(y|x)and return the higher probabilitylabely=1 ory=0.5.1 Classi\ufb01cation: the sigmoidThe goal of binary logistic regression is to train a classi\ufb01er that can make a binarydecision about the class of a new input observation. Here we introduce thesigmoidclassi\ufb01er that will help us make this decision.Consider a single input observationx, which we will represent by a vector offeatures[x1,x2,. .",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 4,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "be a vector of features[x1,x2,. . . ,xn]. We will generally refer to featureifor inputx(j)asx(j)i, sometimes simpli\ufb01ed asxi, but we will also see thenotationfi,fi(x), or, for multiclass classi\ufb01cation,fi(c,x).2.A classi\ufb01cation function that computes \u02c6y, the estimated class, viap(y|x). Inthe next section we will introduce thesigmoidandsoftmaxtools for classi\ufb01-cation.3.An objective function for learning, usually involving minimizing error ontraining examples. We will introduce thecross-entropy loss function4.An algorithm for optimizing the objective function. We introduce thestochas-tic gradient descentalgorithm.Logistic regression has two phases:training:we train the system (speci\ufb01cally the weightswandb) using stochasticgradient descent and the cross-entropy loss.test:Given a test examplexwe computep(y|x)and return the higher probabilitylabely=1 ory=0.5.1 Classi\ufb01cation: the sigmoidThe goal of binary logistic regression is to train a classi\ufb01er that can make a binarydecision about the class of a new input observation. Here we introduce thesigmoidclassi\ufb01er that will help us make this decision.Consider a single input observationx, which we will represent by a vector offeatures[x1,x2,. . . ,xn](we\u2019ll show sample features in the next subsection). The clas-si\ufb01er outputycan be 1 (meaning the observation is a member of the class) or 0(the observation is not a member of the class). We want to know the probabilityP(y=1|x)that this observation is a member of the class. So perhaps the decisionP(c|d)posterior",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 5,
      "token_count": 384,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\nComponents of a probabilistic machine learning classifier1.A feature representation of the input. For each input observation x(i), a vector of features [x1, x2, ... , xn]. Feature j for input x(i) is xj, more completely  xj(i), or sometimes fj(x).2.A classification function that computes !\ud835\udc66, the estimated class, via p(y|x), like the sigmoidor softmaxfunctions.3.An objective function for learning, like cross-entropy loss. 4.An algorithm for optimizing the objective function: stochastic gradient descent. Given m input/output pairs (x(i),y(i)):\n\n## Page 9\n\nThe two phases of logistic regression Training: we learn weights w and busing stochastic gradient descentand cross-entropy loss. Test: Given a test example x we compute p(y|x) using learned weights wand b, and return whichever label (y = 1 or y = 0) is higher probability\n\n## Page 10\n\nLogistic RegressionBackground: Generative and Discriminative Classifiers\n\n## Page 11\n\nLogistic RegressionClassification in Logistic Regression\n\n## Page 12\n\nClassification ReminderPositive/negative sentiment  Spam/not spamAuthorship attribution  (Hamilton or Madison?)\nAlexander Hamilton\n\n## Page 13\n\nText Classification: definitionInput:\u25e6a document x\u25e6a fixed set of classes  C ={c1, c2,\u2026, cJ}Output: a predicted class !\ud835\udc66\u00ceC\n\n## Page 14\n\nBinary Classification in Logistic RegressionGiven a series of input/output pairs:\u25e6(x(i), y(i))For each observation x(i)\u25e6We represent x(i)by a feature vector [x1, x2,\u2026, xn]\u25e6We compute an output: a predicted class !\ud835\udc66(i)\u00ce{0,1}\n\n## Page 15\n\nFeatures in logistic regression\u2022For feature xi, weight witells is how important is xi\u2022xi=\"review contains \u2018awesome\u2019\":      wi=  +10\u2022xj=\"review contains \u2018abysmal\u2019\":      wj= -10\u2022xk=\u201creview contains \u2018mediocre\u2019\":   wk= -2\n\n## Page 16\n\nLogistic Regression for one observation xInput observation: vector  x = [x1, x2,\u2026, xn]Weights: one per feature: W= [w1, w2,\u2026, wn]\u25e6Sometimes we call the weights \u03b8= [\u03b81, \u03b82,\u2026, \u03b8n]Output: a predicted class !\ud835\udc66\u00ce{0,1}(multinomial logistic regression: !\ud835\udc66\u00ce{0, 1, 2, 3, 4})",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 6,
      "token_count": 567,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 7,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "How to do classificationFor each feature xi, weight witells us importance of xi\u25e6(Plus we'll have a bias b)We'll sum up all the weighted features and the biasIf this sum is high, we say y=1; if low, then y=05.1\u2022CLASSIFICATION:THE SIGMOID3is \u201cpositive sentiment\u201d versus \u201cnegative sentiment\u201d, the features represent countsof words in a document, andP(y=1|x)is the probability that the document haspositive sentiment, while andP(y=0|x)is the probability that the document hasnegative sentiment.Logistic regression solves this task by learning, from a training set, a vector ofweightsand abias term. Each weightwiis a real number, and is associated with oneof the input featuresxi. The weightwirepresents how important that input feature isto the classi\ufb01cation decision, and can be positive (meaning the feature is associatedwith the class) or negative (meaning the feature is not associated with the class).Thus we might expect in a sentiment task the wordawesometo have a high positiveweight, andabysmalto have a very negative weight. Thebias term, also called thebias termintercept, is another real number that\u2019s added to the weighted inputs.interceptTo make a decision on a test instance\u2014 after we\u2019ve learned the weights intraining\u2014 the classi\ufb01er \ufb01rst multiplies eachxiby its weightwi, sums up the weightedfeatures, and adds the bias termb. The resulting single numberzexpresses theweighted sum of the evidence for the class.z= nXi=1wixi!+b(5.2)In the rest of the book we\u2019ll represent such sums using thedot productnotation fromdot productlinear algebra. The dot product of two vectorsaandb, written asa\u00b7bis the sum ofthe products of the corresponding elements of each vector. Thus the following is anequivalent formation to Eq.5.2:z=w\u00b7x+b(5.3)But note that nothing in Eq.5.3forceszto be a legal probability, that is, to liebetween 0 and 1. In fact, since weights are real-valued, the output might even benegative;zranges from\u0000\u2022to\u2022.",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 8,
      "token_count": 482,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 5.1The sigmoid functiony=11+e\u0000ztakes a real value and maps it to the range[0,1].Because it is nearly linear around 0 but has a sharp slope toward the ends, it tends to squashoutlier values toward 0 or 1.To create a probability, we\u2019ll passzthrough thesigmoidfunction,s(z). Thesigmoidsigmoid function (named because it looks like ans) is also called thelogistic func-tion, and gives logistic regression its name. The sigmoid has the following equation,logisticfunctionshown graphically in Fig.5.1:y=s(z)=11+e\u0000z(5.4)5.1\u2022CLASSIFICATION:THE SIGMOID3is \u201cpositive sentiment\u201d versus \u201cnegative sentiment\u201d, the features represent countsof words in a document, andP(y=1|x)is the probability that the document haspositive sentiment, while andP(y=0|x)is the probability that the document hasnegative sentiment.Logistic regression solves this task by learning, from a training set, a vector ofweightsand abias term. Each weightwiis a real number, and is associated with oneof the input featuresxi. The weightwirepresents how important that input feature isto the classi\ufb01cation decision, and can be positive (meaning the feature is associatedwith the class) or negative (meaning the feature is not associated with the class).Thus we might expect in a sentiment task the wordawesometo have a high positiveweight, andabysmalto have a very negative weight. Thebias term, also called thebias termintercept, is another real number that\u2019s added to the weighted inputs.interceptTo make a decision on a test instance\u2014 after we\u2019ve learned the weights intraining\u2014 the classi\ufb01er \ufb01rst multiplies eachxiby its weightwi, sums up the weightedfeatures, and adds the bias termb. The resulting single numberzexpresses theweighted sum of the evidence for the class.z= nXi=1wixi!+b(5.2)In the rest of the book we\u2019ll represent such sums using thedot productnotation fromdot productlinear algebra. The dot product of two vectorsaandb, written asa\u00b7bis the sum ofthe products of the corresponding elements of each vector. Thus the following is anequivalent formation to Eq.5.2:z=w\u00b7x+b(5.3)But note that nothing in Eq.5.3forceszto be a legal probability, that is, to liebetween 0 and 1. In fact, since weights are real-valued, the output might even benegative;zranges from\u0000\u2022to\u2022.\nFigure 5.1The sigmoid functiony=11+e\u0000ztakes a real value and maps it to the range[0,1].Because it is nearly linear around 0 but has a sharp slope toward the ends, it tends to squashoutlier values toward 0 or 1.To create a probability, we\u2019ll passzthrough thesigmoidfunction,s(z). Thesigmoidsigmoid function (named because it looks like ans) is also called thelogistic func-tion, and gives logistic regression its name. The sigmoid has the following equation,logisticfunctionshown graphically in Fig.5.1:y=s(z)=11+e\u0000z(5.4)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 9,
      "token_count": 706,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\nBut we want a probabilistic classifierWe need to formalize \u201csum is high\u201d.We\u2019d like a principled classifier that gives us a probability, just like Naive Bayes didWe want a model that can tell us:p(y=1|x;\u03b8)p(y=0|x; \u03b8)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 10,
      "token_count": 66,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 11,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "The problem:  z isn't a probability, it's just a number!Solution: use a function of z that goes from 0 to 15.1\u2022CLASSIFICATION:THE SIGMOID3is \u201cpositive sentiment\u201d versus \u201cnegative sentiment\u201d, the features represent countsof words in a document, andP(y=1|x)is the probability that the document haspositive sentiment, while andP(y=0|x)is the probability that the document hasnegative sentiment.Logistic regression solves this task by learning, from a training set, a vector ofweightsand abias term. Each weightwiis a real number, and is associated with oneof the input featuresxi. The weightwirepresents how important that input feature isto the classi\ufb01cation decision, and can be positive (meaning the feature is associatedwith the class) or negative (meaning the feature is not associated with the class).Thus we might expect in a sentiment task the wordawesometo have a high positiveweight, andabysmalto have a very negative weight. Thebias term, also called thebias termintercept, is another real number that\u2019s added to the weighted inputs.interceptTo make a decision on a test instance\u2014 after we\u2019ve learned the weights intraining\u2014 the classi\ufb01er \ufb01rst multiplies eachxiby its weightwi, sums up the weightedfeatures, and adds the bias termb. The resulting single numberzexpresses theweighted sum of the evidence for the class.z= nXi=1wixi!+b(5.2)In the rest of the book we\u2019ll represent such sums using thedot productnotation fromdot productlinear algebra. The dot product of two vectorsaandb, written asa\u00b7bis the sum ofthe products of the corresponding elements of each vector. Thus the following is anequivalent formation to Eq.5.2:z=w\u00b7x+b(5.3)But note that nothing in Eq.5.3forceszto be a legal probability, that is, to liebetween 0 and 1. In fact, since weights are real-valued, the output might even benegative;zranges from\u0000\u2022to\u2022.",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 12,
      "token_count": 456,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 5.1The sigmoid functiony=11+e\u0000ztakes a real value and maps it to the range[0,1].Because it is nearly linear around 0 but has a sharp slope toward the ends, it tends to squashoutlier values toward 0 or 1.To create a probability, we\u2019ll passzthrough thesigmoidfunction,s(z). Thesigmoidsigmoid function (named because it looks like ans) is also called thelogistic func-tion, and gives logistic regression its name. The sigmoid has the following equation,logisticfunctionshown graphically in Fig.5.1:y=s(z)=11+e\u0000z(5.4)5.1\u2022CLASSIFICATION:THE SIGMOID3sentiment\u201d versus \u201cnegative sentiment\u201d, the features represent counts of words in adocument,P(y=1|x)is the probability that the document has positive sentiment,andP(y=0|x)is the probability that the document has negative sentiment.Logistic regression solves this task by learning, from a training set, a vector ofweightsand abias term. Each weightwiis a real number, and is associated with oneof the input featuresxi. The weightwirepresents how important that input featureis to the classi\ufb01cation decision, and can be positive (providing evidence that the in-stance being classi\ufb01ed belongs in the positive class) or negative (providing evidencethat the instance being classi\ufb01ed belongs in the negative class). Thus we mightexpect in a sentiment task the wordawesometo have a high positive weight, andabysmalto have a very negative weight. Thebias term, also called theintercept, isbias terminterceptanother real number that\u2019s added to the weighted inputs.To make a decision on a test instance\u2014 after we\u2019ve learned the weights intraining\u2014 the classi\ufb01er \ufb01rst multiplies eachxiby its weightwi, sums up the weightedfeatures, and adds the bias termb. The resulting single numberzexpresses theweighted sum of the evidence for the class.z= nXi=1wixi!+b(5.2)In the rest of the book we\u2019ll represent such sums using thedot productnotation fromdot productlinear algebra. The dot product of two vectorsaandb, written asa\u00b7bis the sum ofthe products of the corresponding elements of each vector. Thus the following is anequivalent formation to Eq.5.2:z=w\u00b7x+b(5.3)But note that nothing in Eq.5.3forceszto be a legal probability, that is, to liebetween 0 and 1. In fact, since weights are real-valued, the output might even benegative;zranges from\u0000\u2022to\u2022.\nFigure 5.1The sigmoid functiony=11+e\u0000ztakes a real value and maps it to the range[0,1].It is nearly linear around 0 but outlier values get squashed toward 0 or 1.To create a probability, we\u2019ll passzthrough thesigmoidfunction,s(z). Thesigmoidsigmoid function (named because it looks like ans) is also called thelogistic func-tion, and gives logistic regression its name. The sigmoid has the following equation,logisticfunctionshown graphically in Fig.5.1:y=s(z)=11+e\u0000z=11+exp(\u0000z)(5.4)(For the rest of the book, we\u2019ll use the notation exp(x)to meanex.) The sigmoidhas a number of advantages; it takes a real-valued number and maps it into the range",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 13,
      "token_count": 756,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 14,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "The very useful sigmoid or logistic function\n205.1\u2022CLASSIFICATION:THE SIGMOID3is \u201cpositive sentiment\u201d versus \u201cnegative sentiment\u201d, the features represent countsof words in a document, andP(y=1|x)is the probability that the document haspositive sentiment, while andP(y=0|x)is the probability that the document hasnegative sentiment.Logistic regression solves this task by learning, from a training set, a vector ofweightsand abias term. Each weightwiis a real number, and is associated with oneof the input featuresxi. The weightwirepresents how important that input feature isto the classi\ufb01cation decision, and can be positive (meaning the feature is associatedwith the class) or negative (meaning the feature is not associated with the class).Thus we might expect in a sentiment task the wordawesometo have a high positiveweight, andabysmalto have a very negative weight. Thebias term, also called thebias termintercept, is another real number that\u2019s added to the weighted inputs.interceptTo make a decision on a test instance\u2014 after we\u2019ve learned the weights intraining\u2014 the classi\ufb01er \ufb01rst multiplies eachxiby its weightwi, sums up the weightedfeatures, and adds the bias termb. The resulting single numberzexpresses theweighted sum of the evidence for the class.z= nXi=1wixi!+b(5.2)In the rest of the book we\u2019ll represent such sums using thedot productnotation fromdot productlinear algebra. The dot product of two vectorsaandb, written asa\u00b7bis the sum ofthe products of the corresponding elements of each vector. Thus the following is anequivalent formation to Eq.5.2:z=w\u00b7x+b(5.3)But note that nothing in Eq.5.3forceszto be a legal probability, that is, to liebetween 0 and 1. In fact, since weights are real-valued, the output might even benegative;zranges from\u0000\u2022to\u2022.",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 15,
      "token_count": 434,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 5.1The sigmoid functiony=11+e\u0000ztakes a real value and maps it to the range[0,1].Because it is nearly linear around 0 but has a sharp slope toward the ends, it tends to squashoutlier values toward 0 or 1.To create a probability, we\u2019ll passzthrough thesigmoidfunction,s(z). Thesigmoidsigmoid function (named because it looks like ans) is also called thelogistic func-tion, and gives logistic regression its name. The sigmoid has the following equation,logisticfunctionshown graphically in Fig.5.1:y=s(z)=11+e\u0000z(5.4)5.1\u2022CLASSIFICATION:THE SIGMOID3is \u201cpositive sentiment\u201d versus \u201cnegative sentiment\u201d, the features represent countsof words in a document, andP(y=1|x)is the probability that the document haspositive sentiment, while andP(y=0|x)is the probability that the document hasnegative sentiment.Logistic regression solves this task by learning, from a training set, a vector ofweightsand abias term. Each weightwiis a real number, and is associated with oneof the input featuresxi. The weightwirepresents how important that input feature isto the classi\ufb01cation decision, and can be positive (meaning the feature is associatedwith the class) or negative (meaning the feature is not associated with the class).Thus we might expect in a sentiment task the wordawesometo have a high positiveweight, andabysmalto have a very negative weight. Thebias term, also called thebias termintercept, is another real number that\u2019s added to the weighted inputs.interceptTo make a decision on a test instance\u2014 after we\u2019ve learned the weights intraining\u2014 the classi\ufb01er \ufb01rst multiplies eachxiby its weightwi, sums up the weightedfeatures, and adds the bias termb. The resulting single numberzexpresses theweighted sum of the evidence for the class.z= nXi=1wixi!+b(5.2)In the rest of the book we\u2019ll represent such sums using thedot productnotation fromdot productlinear algebra. The dot product of two vectorsaandb, written asa\u00b7bis the sum ofthe products of the corresponding elements of each vector. Thus the following is anequivalent formation to Eq.5.2:z=w\u00b7x+b(5.3)But note that nothing in Eq.5.3forceszto be a legal probability, that is, to liebetween 0 and 1. In fact, since weights are real-valued, the output might even benegative;zranges from\u0000\u2022to\u2022.\nFigure 5.1The sigmoid functiony=11+e\u0000ztakes a real value and maps it to the range[0,1].Because it is nearly linear around 0 but has a sharp slope toward the ends, it tends to squashoutlier values toward 0 or 1.To create a probability, we\u2019ll passzthrough thesigmoidfunction,s(z). Thesigmoidsigmoid function (named because it looks like ans) is also called thelogistic func-tion, and gives logistic regression its name. The sigmoid has the following equation,logisticfunctionshown graphically in Fig.5.1:y=s(z)=11+e\u0000z(5.4)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 16,
      "token_count": 706,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21\n\nIdea of logistic regressionWe\u2019ll compute w\u2219x+bAnd then we\u2019ll pass it through the sigmoid function:\u03c3(w\u2219x+b)And we'll just treat it as a probability",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 17,
      "token_count": 45,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 18,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Making probabilities with sigmoids4CHAPTER5\u2022LOGISTICREGRESSION[0,1], which is just what we want for a probability. Because it is nearly linear around0 but \ufb02attens toward the ends, it tends to squash outlier values toward 0 or 1. Andit\u2019s differentiable, which as we\u2019ll see in Section5.8will be handy for learning.We\u2019re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(w\u00b7x+b)=11+exp(\u0000(w\u00b7x+b))P(y=0)=1\u0000s(w\u00b7x+b)=1\u000011+exp(\u0000(w\u00b7x+b))=exp(\u0000(w\u00b7x+b))1+exp(\u0000(w\u00b7x+b))(5.5)The sigmoid function has the property1\u0000s(x)=s(\u0000x)(5.6)so we could also have expressedP(y=0)ass(\u0000(w\u00b7x+b)).Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundary\u02c6y=\u21e21 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classi\ufb01cationLet\u2019s have an example. Suppose we are doing binary sentiment classi\ufb01cation onmovie review text, and we would like to know whether to assign the sentiment class+or\u0000to a review documentdoc. We\u2019ll represent each input observation by the 6featuresx1...x6of the input shown in the following table; Fig.5.2shows the featuresin a sample mini test document.Var De\ufb01nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3\u21e21 if \u201cno\u201d2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5\u21e21 if \u201c!\u201d2doc0 otherwise0x6log(word count of doc)ln(66)=4.19Let\u2019s assume for the moment that we\u2019ve already learned a real-valued weight foreach of these features, and that the 6 weights corresponding to the 6 features are[2.5,\u00005.0,\u00001.2,0.5,2.0,0.7], whileb= 0.1. (We\u2019ll discuss in the next section how4CHAPTER5\u2022LOGISTICREGRESSION[0,1], which is just what we want for a probability. Because it is nearly linear around0 but \ufb02attens toward the ends, it tends to squash outlier values toward 0 or 1. Andit\u2019s differentiable, which as we\u2019ll see in Section5.8will be handy for learning.We\u2019re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 19,
      "token_count": 756,
      "chapter_title": ""
    }
  },
  {
    "content": "lexicon)2doc)3x2count(negative lexicon)2doc)2x3\u21e21 if \u201cno\u201d2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5\u21e21 if \u201c!\u201d2doc0 otherwise0x6log(word count of doc)ln(66)=4.19Let\u2019s assume for the moment that we\u2019ve already learned a real-valued weight foreach of these features, and that the 6 weights corresponding to the 6 features are[2.5,\u00005.0,\u00001.2,0.5,2.0,0.7], whileb= 0.1. (We\u2019ll discuss in the next section how4CHAPTER5\u2022LOGISTICREGRESSION[0,1], which is just what we want for a probability. Because it is nearly linear around0 but \ufb02attens toward the ends, it tends to squash outlier values toward 0 or 1. Andit\u2019s differentiable, which as we\u2019ll see in Section5.8will be handy for learning.We\u2019re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(w\u00b7x+b)=11+exp(\u0000(w\u00b7x+b))P(y=0)=1\u0000s(w\u00b7x+b)=1\u000011+exp(\u0000(w\u00b7x+b))=exp(\u0000(w\u00b7x+b))1+exp(\u0000(w\u00b7x+b))(5.5)The sigmoid function has the property1\u0000s(x)=s(\u0000x)(5.6)so we could also have expressedP(y=0)ass(\u0000(w\u00b7x+b)).Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundary\u02c6y=\u21e21 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classi\ufb01cationLet\u2019s have an example. Suppose we are doing binary sentiment classi\ufb01cation onmovie review text, and we would like to know whether to assign the sentiment class+or\u0000to a review documentdoc. We\u2019ll represent each input observation by the 6featuresx1...x6of the input shown in the following table; Fig.5.2shows the featuresin a sample mini test document.Var De\ufb01nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3\u21e21 if \u201cno\u201d2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5\u21e21 if \u201c!\u201d2doc0 otherwise0x6log(word count of doc)ln(66)=4.19Let\u2019s assume for the moment that we\u2019ve already learned a real-valued weight foreach of these features, and that the 6 weights corresponding to the 6 features are[2.5,\u00005.0,\u00001.2,0.5,2.0,0.7], whileb= 0.1. (We\u2019ll discuss in the next section how",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 20,
      "token_count": 767,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 21,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "By the way:4CHAPTER5\u2022LOGISTICREGRESSION[0,1], which is just what we want for a probability. Because it is nearly linear around0 but \ufb02attens toward the ends, it tends to squash outlier values toward 0 or 1. Andit\u2019s differentiable, which as we\u2019ll see in Section5.8will be handy for learning.We\u2019re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(w\u00b7x+b)=11+exp(\u0000(w\u00b7x+b))P(y=0)=1\u0000s(w\u00b7x+b)=1\u000011+exp(\u0000(w\u00b7x+b))=exp(\u0000(w\u00b7x+b))1+exp(\u0000(w\u00b7x+b))(5.5)The sigmoid function has the property1\u0000s(x)=s(\u0000x)(5.6)so we could also have expressedP(y=0)ass(\u0000(w\u00b7x+b)).Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundary\u02c6y=\u21e21 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classi\ufb01cationLet\u2019s have an example. Suppose we are doing binary sentiment classi\ufb01cation onmovie review text, and we would like to know whether to assign the sentiment class+or\u0000to a review documentdoc. We\u2019ll represent each input observation by the 6featuresx1...x6of the input shown in the following table; Fig.5.2shows the featuresin a sample mini test document.Var De\ufb01nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3\u21e21 if \u201cno\u201d2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5\u21e21 if \u201c!\u201d2doc0 otherwise0x6log(word count of doc)ln(66)=4.19Let\u2019s assume for the moment that we\u2019ve already learned a real-valued weight foreach of these features, and that the 6 weights corresponding to the 6 features are[2.5,\u00005.0,\u00001.2,0.5,2.0,0.7], whileb= 0.1. (We\u2019ll discuss in the next section how4CHAPTER5\u2022LOGISTICREGRESSION[0,1], which is just what we want for a probability. Because it is nearly linear around0 but \ufb02attens toward the ends, it tends to squash outlier values toward 0 or 1. Andit\u2019s differentiable, which as we\u2019ll see in Section5.8will be handy for learning.We\u2019re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 22,
      "token_count": 754,
      "chapter_title": ""
    }
  },
  {
    "content": "lexicon)2doc)3x2count(negative lexicon)2doc)2x3\u21e21 if \u201cno\u201d2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5\u21e21 if \u201c!\u201d2doc0 otherwise0x6log(word count of doc)ln(66)=4.19Let\u2019s assume for the moment that we\u2019ve already learned a real-valued weight foreach of these features, and that the 6 weights corresponding to the 6 features are[2.5,\u00005.0,\u00001.2,0.5,2.0,0.7], whileb= 0.1. (We\u2019ll discuss in the next section how4CHAPTER5\u2022LOGISTICREGRESSION[0,1], which is just what we want for a probability. Because it is nearly linear around0 but \ufb02attens toward the ends, it tends to squash outlier values toward 0 or 1. Andit\u2019s differentiable, which as we\u2019ll see in Section5.8will be handy for learning.We\u2019re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(w\u00b7x+b)=11+exp(\u0000(w\u00b7x+b))P(y=0)=1\u0000s(w\u00b7x+b)=1\u000011+exp(\u0000(w\u00b7x+b))=exp(\u0000(w\u00b7x+b))1+exp(\u0000(w\u00b7x+b))(5.5)The sigmoid function has the property1\u0000s(x)=s(\u0000x)(5.6)so we could also have expressedP(y=0)ass(\u0000(w\u00b7x+b)).Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundary\u02c6y=\u21e21 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classi\ufb01cationLet\u2019s have an example. Suppose we are doing binary sentiment classi\ufb01cation onmovie review text, and we would like to know whether to assign the sentiment class+or\u0000to a review documentdoc. We\u2019ll represent each input observation by the 6featuresx1...x6of the input shown in the following table; Fig.5.2shows the featuresin a sample mini test document.Var De\ufb01nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3\u21e21 if \u201cno\u201d2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5\u21e21 if \u201c!\u201d2doc0 otherwise0x6log(word count of doc)ln(66)=4.19Let\u2019s assume for the moment that we\u2019ve already learned a real-valued weight foreach of these features, and that the 6 weights corresponding to the 6 features are[2.5,\u00005.0,\u00001.2,0.5,2.0,0.7], whileb= 0.1. (We\u2019ll discuss in the next section how=Because4CHAPTER5\u2022LOGISTICREGRESSION[0,1], which is just what we want for a probability. Because it is nearly linear around0 but",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 23,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "Example: sentiment classi\ufb01cationLet\u2019s have an example. Suppose we are doing binary sentiment classi\ufb01cation onmovie review text, and we would like to know whether to assign the sentiment class+or\u0000to a review documentdoc. We\u2019ll represent each input observation by the 6featuresx1...x6of the input shown in the following table; Fig.5.2shows the featuresin a sample mini test document.Var De\ufb01nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3\u21e21 if \u201cno\u201d2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5\u21e21 if \u201c!\u201d2doc0 otherwise0x6log(word count of doc)ln(66)=4.19Let\u2019s assume for the moment that we\u2019ve already learned a real-valued weight foreach of these features, and that the 6 weights corresponding to the 6 features are[2.5,\u00005.0,\u00001.2,0.5,2.0,0.7], whileb= 0.1. (We\u2019ll discuss in the next section how=Because4CHAPTER5\u2022LOGISTICREGRESSION[0,1], which is just what we want for a probability. Because it is nearly linear around0 but \ufb02attens toward the ends, it tends to squash outlier values toward 0 or 1. Andit\u2019s differentiable, which as we\u2019ll see in Section5.8will be handy for learning.We\u2019re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(w\u00b7x+b)=11+exp(\u0000(w\u00b7x+b))P(y=0)=1\u0000s(w\u00b7x+b)=1\u000011+exp(\u0000(w\u00b7x+b))=exp(\u0000(w\u00b7x+b))1+exp(\u0000(w\u00b7x+b))(5.5)The sigmoid function has the property1\u0000s(x)=s(\u0000x)(5.6)so we could also have expressedP(y=0)ass(\u0000(w\u00b7x+b)).Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundary\u02c6y=\u21e21 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classi\ufb01cationLet\u2019s have an example. Suppose we are doing binary sentiment classi\ufb01cation onmovie review text, and we would like to know whether to assign the sentiment class+or\u0000to a review documentdoc. We\u2019ll represent each input observation by the 6featuresx1...x6of the input shown in the following table; Fig.5.2shows the featuresin a sample mini test document.Var De\ufb01nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3\u21e21 if \u201cno\u201d2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5\u21e21 if \u201c!\u201d2doc0 otherwise0x6log(word count of",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 24,
      "token_count": 790,
      "chapter_title": ""
    }
  },
  {
    "content": "we could also have expressedP(y=0)ass(\u0000(w\u00b7x+b)).Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundary\u02c6y=\u21e21 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classi\ufb01cationLet\u2019s have an example. Suppose we are doing binary sentiment classi\ufb01cation onmovie review text, and we would like to know whether to assign the sentiment class+or\u0000to a review documentdoc. We\u2019ll represent each input observation by the 6featuresx1...x6of the input shown in the following table; Fig.5.2shows the featuresin a sample mini test document.Var De\ufb01nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3\u21e21 if \u201cno\u201d2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5\u21e21 if \u201c!\u201d2doc0 otherwise0x6log(word count of doc)ln(66)=4.19Let\u2019s assume for the moment that we\u2019ve already learned a real-valued weight foreach of these features, and that the 6 weights corresponding to the 6 features are[2.5,\u00005.0,\u00001.2,0.5,2.0,0.7], whileb= 0.1. (We\u2019ll discuss in the next section how",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 25,
      "token_count": 375,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24\n\nTurning a probability into a classifier4CHAPTER5\u2022LOGISTICREGRESSIONThe sigmoid has a number of advantages; it take a real-valued number and mapsit into the range[0,1], which is just what we want for a probability. Because it isnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outliervalues toward 0 or 1. And it\u2019s differentiable, which as we\u2019ll see in Section5.8willbe handy for learning.We\u2019re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)P(y=0)=1\u0000s(w\u00b7x+b)=1\u000011+e\u0000(w\u00b7x+b)=e\u0000(w\u00b7x+b)1+e\u0000(w\u00b7x+b)(5.5)Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundary\u02c6y=\u21e21 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classi\ufb01cationLet\u2019s have an example. Suppose we are doing binary sentiment classi\ufb01cation onmovie review text, and we would like to know whether to assign the sentiment class+or\u0000to a review documentdoc. We\u2019ll represent each input observation by thefollowing 6 featuresx1...x6of the input; Fig.5.2shows the features in a sample minitest document.Var De\ufb01nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3\u21e21 if \u201cno\u201d2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5\u21e21 if \u201c!\u201d2doc0 otherwise0x6log(word count of doc)ln(64)=4.15Let\u2019s assume for the moment that we\u2019ve already learned a real-valued weightfor each of these features, and that the 6 weights corresponding to the 6 featuresare[2.5,\u00005.0,\u00001.2,0.5,2.0,0.7], whileb= 0.1. (We\u2019ll discuss in the next sectionhow the weights are learned.) The weightw1, for example indicates how important0.5 here is called the decision boundary",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 26,
      "token_count": 620,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 25",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 27,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "The probabilistic classifier5.1\u2022CLASSIFICATION:THE SIGMOID3is \u201cpositive sentiment\u201d versus \u201cnegative sentiment\u201d, the features represent countsof words in a document, andP(y=1|x)is the probability that the document haspositive sentiment, while andP(y=0|x)is the probability that the document hasnegative sentiment.Logistic regression solves this task by learning, from a training set, a vector ofweightsand abias term. Each weightwiis a real number, and is associated with oneof the input featuresxi. The weightwirepresents how important that input feature isto the classi\ufb01cation decision, and can be positive (meaning the feature is associatedwith the class) or negative (meaning the feature is not associated with the class).Thus we might expect in a sentiment task the wordawesometo have a high positiveweight, andabysmalto have a very negative weight. Thebias term, also called thebias termintercept, is another real number that\u2019s added to the weighted inputs.interceptTo make a decision on a test instance\u2014 after we\u2019ve learned the weights intraining\u2014 the classi\ufb01er \ufb01rst multiplies eachxiby its weightwi, sums up the weightedfeatures, and adds the bias termb. The resulting single numberzexpresses theweighted sum of the evidence for the class.z= nXi=1wixi!+b(5.2)In the rest of the book we\u2019ll represent such sums using thedot productnotation fromdot productlinear algebra. The dot product of two vectorsaandb, written asa\u00b7bis the sum ofthe products of the corresponding elements of each vector. Thus the following is anequivalent formation to Eq.5.2:z=w\u00b7x+b(5.3)But note that nothing in Eq.5.3forceszto be a legal probability, that is, to liebetween 0 and 1. In fact, since weights are real-valued, the output might even benegative;zranges from\u0000\u2022to\u2022.",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 28,
      "token_count": 430,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 5.1The sigmoid functiony=11+e\u0000ztakes a real value and maps it to the range[0,1].Because it is nearly linear around 0 but has a sharp slope toward the ends, it tends to squashoutlier values toward 0 or 1.To create a probability, we\u2019ll passzthrough thesigmoidfunction,s(z). Thesigmoidsigmoid function (named because it looks like ans) is also called thelogistic func-tion, and gives logistic regression its name. The sigmoid has the following equation,logisticfunctionshown graphically in Fig.5.1:y=s(z)=11+e\u0000z(5.4)wx+ b4CHAPTER5\u2022LOGISTICREGRESSIONThe sigmoid has a number of advantages; it take a real-valued number and mapsit into the range[0,1], which is just what we want for a probability. Because it isnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outliervalues toward 0 or 1. And it\u2019s differentiable, which as we\u2019ll see in Section5.8willbe handy for learning.We\u2019re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)P(y=0)=1\u0000s(w\u00b7x+b)=1\u000011+e\u0000(w\u00b7x+b)=e\u0000(w\u00b7x+b)1+e\u0000(w\u00b7x+b)(5.5)Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundary\u02c6y=\u21e21 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classi\ufb01cationLet\u2019s have an example. Suppose we are doing binary sentiment classi\ufb01cation onmovie review text, and we would like to know whether to assign the sentiment class+or\u0000to a review documentdoc. We\u2019ll represent each input observation by thefollowing 6 featuresx1...x6of the input; Fig.5.2shows the features in a sample minitest document.Var De\ufb01nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3\u21e21 if \u201cno\u201d2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5\u21e21 if \u201c!\u201d2doc0 otherwise0x6log(word count of doc)ln(64)=4.15Let\u2019s assume for the moment that we\u2019ve already learned a real-valued weightfor each of these features, and that the 6 weights corresponding to the 6 featuresare[2.5,\u00005.0,\u00001.2,0.5,2.0,0.7], whileb= 0.1. (We\u2019ll discuss in the next sectionhow the weights are learned.) The weightw1, for example indicates how importantP(y=1)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 29,
      "token_count": 748,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 26\n\nTurning a probability into a classifier4CHAPTER5\u2022LOGISTICREGRESSIONThe sigmoid has a number of advantages; it take a real-valued number and mapsit into the range[0,1], which is just what we want for a probability. Because it isnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outliervalues toward 0 or 1. And it\u2019s differentiable, which as we\u2019ll see in Section5.8willbe handy for learning.We\u2019re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)P(y=0)=1\u0000s(w\u00b7x+b)=1\u000011+e\u0000(w\u00b7x+b)=e\u0000(w\u00b7x+b)1+e\u0000(w\u00b7x+b)(5.5)Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundary\u02c6y=\u21e21 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classi\ufb01cationLet\u2019s have an example. Suppose we are doing binary sentiment classi\ufb01cation onmovie review text, and we would like to know whether to assign the sentiment class+or\u0000to a review documentdoc. We\u2019ll represent each input observation by thefollowing 6 featuresx1...x6of the input; Fig.5.2shows the features in a sample minitest document.Var De\ufb01nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3\u21e21 if \u201cno\u201d2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5\u21e21 if \u201c!\u201d2doc0 otherwise0x6log(word count of doc)ln(64)=4.15Let\u2019s assume for the moment that we\u2019ve already learned a real-valued weightfor each of these features, and that the 6 weights corresponding to the 6 featuresare[2.5,\u00005.0,\u00001.2,0.5,2.0,0.7], whileb= 0.1. (We\u2019ll discuss in the next sectionhow the weights are learned.) The weightw1, for example indicates how importantif w\u2219x+b> 0if w\u2219x+b\u2264 0\n\n## Page 27\n\nLogistic RegressionClassification in Logistic Regression\n\n## Page 28\n\nLogistic RegressionLogistic Regression: a text example on sentiment classification\n\n## Page 29\n\nSentiment example: does y=1 or y=0?It's hokey . There are virtually no surprises , and the writing is second-rate .         So why was it so enjoyable ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing . It sucked me in , and it'll do the same to you .\n29",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 30,
      "token_count": 753,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 30",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 31,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "305.1\u2022CLASSIFICATION:THE SIGMOID5 It's hokey . There are virtually no surprises , and the writing is second-rate . So why was it so enjoyable  ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing .  It sucked me in , and it'll do the same to you  .x1=3x6=4.19x3=1x4=3x5=0x2=2",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 32,
      "token_count": 111,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 5.2A sample mini test document showing the extracted features in the vectorx.Given these 6 features and the input reviewx,P(+|x)andP(\u0000|x)can be com-puted using Eq.5.5:p(+|x)=P(Y=1|x)=s(w\u00b7x+b)=s([2.5,\u00005.0,\u00001.2,0.5,2.0,0.7]\u00b7[3,2,1,3,0,4.19]+0.1)=s(.833)=0.70(5.6)p(\u0000|x)=P(Y=0|x)=1\u0000s(w\u00b7x+b)=0.30Logistic regression is commonly applied to all sorts of NLP tasks, and any propertyof the input can be a feature. Consider the task ofperiod disambiguation: decidingif a period is the end of a sentence or part of a word, by classifying each periodinto one of two classes EOS (end-of-sentence) and not-EOS. We might use featureslikex1below expressing that the current word is lower case and the class is EOS(perhaps with a positive weight), or that the current word is in our abbreviationsdictionary (\u201cProf.\u201d) and the class is EOS (perhaps with a negative weight). A featurecan also express a quite complex combination of properties. For example a periodfollowing an upper case word is likely to be an EOS, but if the word itself isSt.andthe previous word is capitalized, then the period is likely part of a shortening of thewordstreet.x1=\u21e21 if \u201cCase(wi)=Lower\u201d0 otherwisex2=\u21e21 if \u201cwi2AcronymDict\u201d0 otherwisex3=\u21e21 if \u201cwi=St. &Case(wi\u00001)=Cap\u201d0 otherwiseDesigning features:Features are generally designed by examining the trainingset with an eye to linguistic intuitions and the linguistic literature on the domain. Acareful error analysis on the training set or devset of an early version of a systemoften provides insights into features.For some tasks it is especially helpful to build complex features that are combi-nations of more primitive features. We saw such a feature for period disambiguationabove, where a period on the wordSt.was less likely to be the end of the sentenceif the previous word was capitalized. For logistic regression and naive Bayes thesecombination features orfeature interactionshave to be designed by hand.featureinteractions4CHAPTER5\u2022LOGISTICREGRESSIONnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outliervalues toward 0 or 1. And it\u2019s differentiable, which as we\u2019ll see in Section5.8willbe handy for learning.We\u2019re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)P(y=0)=1\u0000s(w\u00b7x+b)=1\u000011+e\u0000(w\u00b7x+b)=e\u0000(w\u00b7x+b)1+e\u0000(w\u00b7x+b)(5.5)Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if the probabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecision",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 33,
      "token_count": 788,
      "chapter_title": ""
    }
  },
  {
    "content": "wordSt.was less likely to be the end of the sentenceif the previous word was capitalized. For logistic regression and naive Bayes thesecombination features orfeature interactionshave to be designed by hand.featureinteractions4CHAPTER5\u2022LOGISTICREGRESSIONnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outliervalues toward 0 or 1. And it\u2019s differentiable, which as we\u2019ll see in Section5.8willbe handy for learning.We\u2019re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)P(y=0)=1\u0000s(w\u00b7x+b)=1\u000011+e\u0000(w\u00b7x+b)=e\u0000(w\u00b7x+b)1+e\u0000(w\u00b7x+b)(5.5)Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if the probabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecision boundary:decisionboundary\u02c6y=\u21e21 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classi\ufb01cationLet\u2019s have an example. Suppose we are doing binary sentiment classi\ufb01cation onmovie review text, and we would like to know whether to assign the sentiment class+or\u0000to a review documentdoc. We\u2019ll represent each input observation by the 6featuresx1...x6of the input shown in the following table; Fig.5.2shows the featuresin a sample mini test document.Var De\ufb01nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3\u21e21 if \u201cno\u201d2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5\u21e21 if \u201c!\u201d2doc0 otherwise0x6log(word count of doc)ln(66)=4.19Let\u2019s assume for the moment that we\u2019ve already learned a real-valued weight foreach of these features, and that the 6 weights corresponding to the 6 features are[2.5,\u00005.0,\u00001.2,0.5,2.0,0.7], whileb= 0.1. (We\u2019ll discuss in the next section howthe weights are learned.) The weightw1, for example indicates how important afeature the number of positive lexicon words (great,nice,enjoyable, etc.) is toa positive sentiment decision, whilew2tells us the importance of negative lexiconwords. Note thatw1=2.5 is positive, whilew2=\u00005.0, meaning that negative wordsare negatively associated with a positive sentiment decision, and are about twice asimportant as positive words.",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 34,
      "token_count": 691,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 31",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 35,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Classifying sentiment for input x",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 36,
      "token_count": 6,
      "chapter_title": ""
    }
  },
  {
    "content": "314CHAPTER5\u2022LOGISTICREGRESSIONnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outliervalues toward 0 or 1. And it\u2019s differentiable, which as we\u2019ll see in Section5.8willbe handy for learning.We\u2019re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)P(y=0)=1\u0000s(w\u00b7x+b)=1\u000011+e\u0000(w\u00b7x+b)=e\u0000(w\u00b7x+b)1+e\u0000(w\u00b7x+b)(5.5)Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if the probabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecision boundary:decisionboundary\u02c6y=\u21e21 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classi\ufb01cationLet\u2019s have an example. Suppose we are doing binary sentiment classi\ufb01cation onmovie review text, and we would like to know whether to assign the sentiment class+or\u0000to a review documentdoc. We\u2019ll represent each input observation by the 6featuresx1...x6of the input shown in the following table; Fig.5.2shows the featuresin a sample mini test document.Var De\ufb01nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3\u21e21 if \u201cno\u201d2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5\u21e21 if \u201c!\u201d2doc0 otherwise0x6log(word count of doc)ln(66)=4.19Let\u2019s assume for the moment that we\u2019ve already learned a real-valued weight foreach of these features, and that the 6 weights corresponding to the 6 features are[2.5,\u00005.0,\u00001.2,0.5,2.0,0.7], whileb= 0.1. (We\u2019ll discuss in the next section howthe weights are learned.) The weightw1, for example indicates how important afeature the number of positive lexicon words (great,nice,enjoyable, etc.) is toa positive sentiment decision, whilew2tells us the importance of negative lexiconwords. Note thatw1=2.5 is positive, whilew2=\u00005.0, meaning that negative wordsare negatively associated with a positive sentiment decision, and are about twice asimportant as positive words.4CHAPTER5\u2022LOGISTICREGRESSIONThe sigmoid has a number of advantages; it take a real-valued number and mapsit into the range[0,1], which is just what we want for a probability. Because it isnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outliervalues toward 0 or 1. And it\u2019s differentiable, which as we\u2019ll see in Section5.8willbe handy for learning.We\u2019re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 37,
      "token_count": 789,
      "chapter_title": ""
    }
  },
  {
    "content": "of these features, and that the 6 weights corresponding to the 6 features are[2.5,\u00005.0,\u00001.2,0.5,2.0,0.7], whileb= 0.1. (We\u2019ll discuss in the next section howthe weights are learned.) The weightw1, for example indicates how important afeature the number of positive lexicon words (great,nice,enjoyable, etc.) is toa positive sentiment decision, whilew2tells us the importance of negative lexiconwords. Note thatw1=2.5 is positive, whilew2=\u00005.0, meaning that negative wordsare negatively associated with a positive sentiment decision, and are about twice asimportant as positive words.4CHAPTER5\u2022LOGISTICREGRESSIONThe sigmoid has a number of advantages; it take a real-valued number and mapsit into the range[0,1], which is just what we want for a probability. Because it isnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outliervalues toward 0 or 1. And it\u2019s differentiable, which as we\u2019ll see in Section5.8willbe handy for learning.We\u2019re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)P(y=0)=1\u0000s(w\u00b7x+b)=1\u000011+e\u0000(w\u00b7x+b)=e\u0000(w\u00b7x+b)1+e\u0000(w\u00b7x+b)(5.5)Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundary\u02c6y=\u21e21 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classi\ufb01cationLet\u2019s have an example. Suppose we are doing binary sentiment classi\ufb01cation onmovie review text, and we would like to know whether to assign the sentiment class+or\u0000to a review documentdoc. We\u2019ll represent each input observation by thefollowing 6 featuresx1...x6of the input; Fig.5.2shows the features in a sample minitest document.Var De\ufb01nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3\u21e21 if \u201cno\u201d2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5\u21e21 if \u201c!\u201d2doc0 otherwise0x6log(word count of doc)ln(64)=4.15Let\u2019s assume for the moment that we\u2019ve already learned a real-valued weightfor each of these features, and that the 6 weights corresponding to the 6 featuresare[2.5,\u00005.0,\u00001.2,0.5,2.0,0.7], whileb= 0.1. (We\u2019ll discuss in the next sectionhow the weights are learned.) The weightw1, for example indicates how importantSuppose w =b = 0.1",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 38,
      "token_count": 769,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 32",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 39,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Classifying sentiment for input x5.1\u2022CLASSIFICATION:THE SIGMOID5 It's hokey . There are virtually no surprises , and the writing is second-rate . So why was it so enjoyable  ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing .  It sucked me in , and it'll do the same to you  .x1=3x6=4.19x3=1x4=3x5=0x2=2\nFigure 5.2A sample mini test document showing the extracted features in the vectorx.Given these 6 features and the input reviewx,P(+|x)andP(\u0000|x)can be com-puted using Eq.5.5:p(+|x)=P(Y=1|x)=s(w\u00b7x+b)=s([2.5,\u00005.0,\u00001.2,0.5,2.0,0.7]\u00b7[3,2,1,3,0,4.19]+0.1)=s(.833)=0.70(5.6)p(\u0000|x)=P(Y=0|x)=1\u0000s(w\u00b7x+b)=0.30Logistic regression is commonly applied to all sorts of NLP tasks, and any propertyof the input can be a feature. Consider the task ofperiod disambiguation: decidingif a period is the end of a sentence or part of a word, by classifying each periodinto one of two classes EOS (end-of-sentence) and not-EOS. We might use featureslikex1below expressing that the current word is lower case and the class is EOS(perhaps with a positive weight), or that the current word is in our abbreviationsdictionary (\u201cProf.\u201d) and the class is EOS (perhaps with a negative weight). A featurecan also express a quite complex combination of properties. For example a periodfollowing an upper case word is likely to be an EOS, but if the word itself isSt.andthe previous word is capitalized, then the period is likely part of a shortening of thewordstreet.x1=\u21e21 if \u201cCase(wi)=Lower\u201d0 otherwisex2=\u21e21 if \u201cwi2AcronymDict\u201d0 otherwisex3=\u21e21 if \u201cwi=St. &Case(wi\u00001)=Cap\u201d0 otherwiseDesigning features:Features are generally designed by examining the trainingset with an eye to linguistic intuitions and the linguistic literature on the domain. Acareful error analysis on the training set or devset of an early version of a systemoften provides insights into features.For some tasks it is especially helpful to build complex features that are combi-nations of more primitive features. We saw such a feature for period disambiguationabove, where a period on the wordSt.was less likely to be the end of the sentenceif the previous word was capitalized. For logistic regression and naive Bayes thesecombination features orfeature interactionshave to be designed by hand.featureinteractions325.1\u2022CLASSIFICATION:THE SIGMOID5 It's hokey . There are virtually no surprises , and the writing is second-rate . So why was it so enjoyable  ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing .  It sucked me in , and it'll do the same to you  .x1=3x6=4.19x3=1x4=3x5=0x2=2",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 40,
      "token_count": 760,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 5.2A sample mini test document showing the extracted features in the vectorx.Given these 6 features and the input reviewx,P(+|x)andP(\u0000|x)can be com-puted using Eq.5.5:p(+|x)=P(Y=1|x)=s(w\u00b7x+b)=s([2.5,\u00005.0,\u00001.2,0.5,2.0,0.7]\u00b7[3,2,1,3,0,4.19]+0.1)=s(.833)=0.70(5.6)p(\u0000|x)=P(Y=0|x)=1\u0000s(w\u00b7x+b)=0.30Logistic regression is commonly applied to all sorts of NLP tasks, and any propertyof the input can be a feature. Consider the task ofperiod disambiguation: decidingif a period is the end of a sentence or part of a word, by classifying each periodinto one of two classes EOS (end-of-sentence) and not-EOS. We might use featureslikex1below expressing that the current word is lower case and the class is EOS(perhaps with a positive weight), or that the current word is in our abbreviationsdictionary (\u201cProf.\u201d) and the class is EOS (perhaps with a negative weight). A featurecan also express a quite complex combination of properties. For example a periodfollowing an upper case word is likely to be an EOS, but if the word itself isSt.andthe previous word is capitalized, then the period is likely part of a shortening of thewordstreet.x1=\u21e21 if \u201cCase(wi)=Lower\u201d0 otherwisex2=\u21e21 if \u201cwi2AcronymDict\u201d0 otherwisex3=\u21e21 if \u201cwi=St. &Case(wi\u00001)=Cap\u201d0 otherwiseDesigning features:Features are generally designed by examining the trainingset with an eye to linguistic intuitions and the linguistic literature on the domain. Acareful error analysis on the training set or devset of an early version of a systemoften provides insights into features.For some tasks it is especially helpful to build complex features that are combi-nations of more primitive features. We saw such a feature for period disambiguationabove, where a period on the wordSt.was less likely to be the end of the sentenceif the previous word was capitalized. For logistic regression and naive Bayes thesecombination features orfeature interactionshave to be designed by hand.featureinteractions",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 41,
      "token_count": 531,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 33\n\nWe can build features for logistic regression for any classification task: period disambiguation5.1\u2022CLASSIFICATION:THE SIGMOID5 It's hokey. There are virtually no surprises , and the writing is second-rate . So why was it so enjoyable? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing .  It sucked me in , and it'll do the same to you .x1=3x6=4.15x3=1x4=3x5=0x2=2\nFigure 5.2A sample mini test document showing the extracted features in the vectorx.a feature the number of positive lexicon words (great,nice,enjoyable, etc.) is toa positive sentiment decision, whilew2tells us the importance of negative lexiconwords. Note thatw1=2.5 is positive, whilew2=\u00005.0, meaning that negative wordsare negatively associated with a positive sentiment decision, and are about twice asimportant as positive words.Given these 6 features and the input reviewx,P(+|x)andP(\u0000|x)can be com-puted using Eq.5.5:p(+|x)=P(Y=1|x)=s(w\u00b7x+b)=s([2.5,\u00005.0,\u00001.2,0.5,2.0,0.7]\u00b7[3,2,1,3,0,4.15]+0.1)=s(1.805)=0.86p(\u0000|x)=P(Y=0|x)=1\u0000s(w\u00b7x+b)=0.14Logistic regression is commonly applied to all sorts of NLP tasks, and any prop-erty of the input can be a feature. Consider the task ofperiod disambiguation:deciding if a period is the end of a sentence or part of a word, by classifying eachperiod into one of two classes EOS (end-of-sentence) and not-EOS. We might usefeatures likex1below expressing that the current word is lower case and the classis EOS (perhaps with a positive weight), or that the current word is in our abbrevia-tions dictionary (\u201cProf.\u201d) and the class is EOS (perhaps with a negative weight). Afeature can also express a quite complex combination of properties. For example aperiod following a upper cased word is a likely to be an EOS, but if the word itself isSt.and the previous word is capitalized, then the period is likely part of a shorteningof the wordstreet.x1=\u21e21 if \u201cCase(wi)=Lower\u201d0 otherwisex2=\u21e21 if \u201cwi2AcronymDict\u201d0 otherwisex3=\u21e21 if \u201cwi=St. &Case(wi\u00001)=Cap\u201d0 otherwiseDesigning features:Features are generally designed by examining the trainingset with an eye to linguistic intuitions and the linguistic literature on the domain. Acareful error analysis on the training or dev set. of an early version of a system oftenprovides insights into features.33This ends in a period.The house at 465 Main St. is new.End of sentenceNot end",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 42,
      "token_count": 690,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 34\n\nClassification in (binary)logistic regression: summaryGiven:\u25e6a set of classes:  (+ sentiment,-sentiment)\u25e6a vector xof features [x1, x2, \u2026, xn]\u25e6x1= count( \"awesome\")\u25e6x2 = log(number of words in review)\u25e6A vector wof weights  [w1, w2, \u2026, wn]\u25e6wifor each feature fi4CHAPTER5\u2022LOGISTICREGRESSIONThe sigmoid has a number of advantages; it take a real-valued number and mapsit into the range[0,1], which is just what we want for a probability. Because it isnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outliervalues toward 0 or 1. And it\u2019s differentiable, which as we\u2019ll see in Section5.8willbe handy for learning.We\u2019re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(w\u00b7x+b)=11+e\u0000(w\u00b7x+b)P(y=0)=1\u0000s(w\u00b7x+b)=1\u000011+e\u0000(w\u00b7x+b)=e\u0000(w\u00b7x+b)1+e\u0000(w\u00b7x+b)(5.5)Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundary\u02c6y=\u21e21 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classi\ufb01cationLet\u2019s have an example. Suppose we are doing binary sentiment classi\ufb01cation onmovie review text, and we would like to know whether to assign the sentiment class+or\u0000to a review documentdoc. We\u2019ll represent each input observation by thefollowing 6 featuresx1...x6of the input; Fig.5.2shows the features in a sample minitest document.Var De\ufb01nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3\u21e21 if \u201cno\u201d2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5\u21e21 if \u201c!\u201d2doc0 otherwise0x6log(word count of doc)ln(64)=4.15Let\u2019s assume for the moment that we\u2019ve already learned a real-valued weightfor each of these features, and that the 6 weights corresponding to the 6 featuresare[2.5,\u00005.0,\u00001.2,0.5,2.0,0.7], whileb= 0.1. (We\u2019ll discuss in the next sectionhow the weights are learned.) The weightw1, for example indicates how important\n\n## Page 35\n\nLogistic RegressionLogistic Regression: a text example on sentiment classification\n\n## Page 36\n\nLogistic RegressionLearning: Cross-Entropy Loss",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 43,
      "token_count": 732,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 35\n\nLogistic RegressionLogistic Regression: a text example on sentiment classification\n\n## Page 36\n\nLogistic RegressionLearning: Cross-Entropy Loss\n\n## Page 37\n\nWait, where did the W\u2019s come from?Supervised classification: \u2022We know the correct label y(either 0 or 1) for each x. \u2022But what the system produces is an estimate, !\ud835\udc66We want to set w and bto minimize the distancebetween our estimate !\ud835\udc66(i)and the true y(i). \u2022We need a distance estimator: a loss function or a cost function\u2022We need an optimization algorithm to update wand bto minimize the loss.37\n\n## Page 38\n\nLearning componentsA loss function:\u25e6cross-entropy lossAn optimization algorithm:\u25e6stochastic gradient descent\n\n## Page 39\n\nThe distance between !\ud835\udc66and yWe want to know how far is the classifier output:!\ud835\udc66= \u03c3(w\u00b7x+b)from the true output:y        [= either 0 or 1]We'll call this difference:L(!\ud835\udc66,y) = how much !\ud835\udc66differs from the true y \n\n## Page 40\n\nIntuition of negative log likelihood loss= cross-entropy lossA case of conditional maximum likelihood estimation We choose the parameters w,bthat maximize\u2022the log probability \u2022of the true y labels in the training data \u2022given the observations x",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 44,
      "token_count": 297,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 41",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 45,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Deriving cross-entropy loss for a single observation xGoal: maximize probability of the correct label p(y|x) Since there are only 2 discrete outcomes (0 or 1) we can express the probability p(y|x) from our classifier (the thing we want to maximize) asnoting:if y=1, this simplifies to !\ud835\udc66if y=0, this simplifies to 1-!\ud835\udc665.3\u2022THE CROSS-ENTROPY LOSS FUNCTION7thecross-entropy loss.The second thing we need is an optimization algorithm for iteratively updatingthe weights so as to minimize this loss function. The standard algorithm for this isgradient descent; we\u2019ll introduce thestochastic gradient descentalgorithm in thefollowing section.5.3 The cross-entropy loss functionWe need a loss function that expresses, for an observationx, how close the classi\ufb01eroutput ( \u02c6y=s(w\u00b7x+b)) is to the correct output (y, which is 0 or 1). We\u2019ll call this:L(\u02c6y,y)=How much \u02c6ydiffers from the truey(5.8)We do this via a loss function that prefers the correct class labels of the train-ing examples to bemore likely. This is calledconditional maximum likelihoodestimation: we choose the parametersw,bthatmaximize the log probability ofthe trueylabels in the training datagiven the observationsx. The resulting lossfunction is the negative log likelihood loss, generally called thecross-entropy loss.cross-entropylossLet\u2019s derive this loss function, applied to a single observationx. We\u2019d like tolearn weights that maximize the probability of the correct labelp(y|x). Since thereare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we canexpress the probabilityp(y|x)that our classi\ufb01er produces for one observation asthe following (keeping in mind that if y=1, Eq.5.9simpli\ufb01es to \u02c6y; if y=0, Eq.5.9simpli\ufb01es to 1\u0000\u02c6y):p(y|x)=\u02c6yy(1\u0000\u02c6y)1\u0000y(5.9)Now we take the log of both sides. This will turn out to be handy mathematically,and doesn\u2019t hurt us; whatever values maximize a probability will also maximize thelog of the probability:logp(y|x)=log\u21e5\u02c6yy(1\u0000\u02c6y)1\u0000y\u21e4=ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)(5.10)Eq.5.10describes a log likelihood that should be maximized. In order to turn thisinto loss function (something that we need to minimize), we\u2019ll just \ufb02ip the sign onEq.5.10. The result is the cross-entropy lossLCE:LCE(\u02c6y,y)=\u0000logp(y|x)=\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)](5.11)Finally, we can plug in the de\ufb01nition of \u02c6y=s(w\u00b7x+b):LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.12)Let\u2019s see if this loss function does the right thing for our example from Fig.5.2.W ewant the loss to be smaller if the model\u2019s estimate is close to correct, and bigger ifthe model is confused. So \ufb01rst let\u2019s suppose the correct gold label for the sentimentexample in Fig.5.2is positive, i.e.,y=1. In this case our model",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 46,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "maximize a probability will also maximize thelog of the probability:logp(y|x)=log\u21e5\u02c6yy(1\u0000\u02c6y)1\u0000y\u21e4=ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)(5.10)Eq.5.10describes a log likelihood that should be maximized. In order to turn thisinto loss function (something that we need to minimize), we\u2019ll just \ufb02ip the sign onEq.5.10. The result is the cross-entropy lossLCE:LCE(\u02c6y,y)=\u0000logp(y|x)=\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)](5.11)Finally, we can plug in the de\ufb01nition of \u02c6y=s(w\u00b7x+b):LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.12)Let\u2019s see if this loss function does the right thing for our example from Fig.5.2.W ewant the loss to be smaller if the model\u2019s estimate is close to correct, and bigger ifthe model is confused. So \ufb01rst let\u2019s suppose the correct gold label for the sentimentexample in Fig.5.2is positive, i.e.,y=1. In this case our model is doing well, sincefrom Eq.5.7it indeed gave the example a higher probability of being positive (.69)than negative (.31). If we plugs(w\u00b7x+b)=.69 andy=1 into Eq.5.12, the right",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 47,
      "token_count": 354,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 42",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 48,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Deriving cross-entropy loss for a single observation xNow take the log of both sides (mathematically handy)Whatever values maximize log p(y|x) will also maximize p(y|x)5.3\u2022THE CROSS-ENTROPY LOSS FUNCTION7thecross-entropy loss.The second thing we need is an optimization algorithm for iteratively updatingthe weights so as to minimize this loss function. The standard algorithm for this isgradient descent; we\u2019ll introduce thestochastic gradient descentalgorithm in thefollowing section.5.3 The cross-entropy loss functionWe need a loss function that expresses, for an observationx, how close the classi\ufb01eroutput ( \u02c6y=s(w\u00b7x+b)) is to the correct output (y, which is 0 or 1). We\u2019ll call this:L(\u02c6y,y)=How much \u02c6ydiffers from the truey(5.8)We do this via a loss function that prefers the correct class labels of the train-ing examples to bemore likely. This is calledconditional maximum likelihoodestimation: we choose the parametersw,bthatmaximize the log probability ofthe trueylabels in the training datagiven the observationsx. The resulting lossfunction is the negative log likelihood loss, generally called thecross-entropy loss.cross-entropylossLet\u2019s derive this loss function, applied to a single observationx. We\u2019d like tolearn weights that maximize the probability of the correct labelp(y|x). Since thereare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we canexpress the probabilityp(y|x)that our classi\ufb01er produces for one observation asthe following (keeping in mind that if y=1, Eq.5.9simpli\ufb01es to \u02c6y; if y=0, Eq.5.9simpli\ufb01es to 1\u0000\u02c6y):p(y|x)=\u02c6yy(1\u0000\u02c6y)1\u0000y(5.9)Now we take the log of both sides. This will turn out to be handy mathematically,and doesn\u2019t hurt us; whatever values maximize a probability will also maximize thelog of the probability:logp(y|x)=log\u21e5\u02c6yy(1\u0000\u02c6y)1\u0000y\u21e4=ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)(5.10)Eq.5.10describes a log likelihood that should be maximized. In order to turn thisinto loss function (something that we need to minimize), we\u2019ll just \ufb02ip the sign onEq.5.10. The result is the cross-entropy lossLCE:LCE(\u02c6y,y)=\u0000logp(y|x)=\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)](5.11)Finally, we can plug in the de\ufb01nition of \u02c6y=s(w\u00b7x+b):LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.12)Let\u2019s see if this loss function does the right thing for our example from Fig.5.2.W ewant the loss to be smaller if the model\u2019s estimate is close to correct, and bigger ifthe model is confused. So \ufb01rst let\u2019s suppose the correct gold label for the sentimentexample in Fig.5.2is positive, i.e.,y=1. In this case our model is doing well, sincefrom Eq.5.7it indeed gave the example a higher probability of being positive (.69)than negative (.31). If we plugs(w\u00b7x+b)=.69 andy=1 into Eq.5.12,",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 49,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "a log likelihood that should be maximized. In order to turn thisinto loss function (something that we need to minimize), we\u2019ll just \ufb02ip the sign onEq.5.10. The result is the cross-entropy lossLCE:LCE(\u02c6y,y)=\u0000logp(y|x)=\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)](5.11)Finally, we can plug in the de\ufb01nition of \u02c6y=s(w\u00b7x+b):LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.12)Let\u2019s see if this loss function does the right thing for our example from Fig.5.2.W ewant the loss to be smaller if the model\u2019s estimate is close to correct, and bigger ifthe model is confused. So \ufb01rst let\u2019s suppose the correct gold label for the sentimentexample in Fig.5.2is positive, i.e.,y=1. In this case our model is doing well, sincefrom Eq.5.7it indeed gave the example a higher probability of being positive (.69)than negative (.31). If we plugs(w\u00b7x+b)=.69 andy=1 into Eq.5.12, the right5.3\u2022THE CROSS-ENTROPY LOSS FUNCTION7thecross-entropy loss.The second thing we need is an optimization algorithm for iteratively updatingthe weights so as to minimize this loss function. The standard algorithm for this isgradient descent; we\u2019ll introduce thestochastic gradient descentalgorithm in thefollowing section.5.3 The cross-entropy loss functionWe need a loss function that expresses, for an observationx, how close the classi\ufb01eroutput ( \u02c6y=s(w\u00b7x+b)) is to the correct output (y, which is 0 or 1). We\u2019ll call this:L(\u02c6y,y)=How much \u02c6ydiffers from the truey(5.8)We do this via a loss function that prefers the correct class labels of the train-ing examples to bemore likely. This is calledconditional maximum likelihoodestimation: we choose the parametersw,bthatmaximize the log probability ofthe trueylabels in the training datagiven the observationsx. The resulting lossfunction is the negative log likelihood loss, generally called thecross-entropy loss.cross-entropylossLet\u2019s derive this loss function, applied to a single observationx. We\u2019d like tolearn weights that maximize the probability of the correct labelp(y|x). Since thereare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we canexpress the probabilityp(y|x)that our classi\ufb01er produces for one observation asthe following (keeping in mind that if y=1, Eq.5.9simpli\ufb01es to \u02c6y; if y=0, Eq.5.9simpli\ufb01es to 1\u0000\u02c6y):p(y|x)=\u02c6yy(1\u0000\u02c6y)1\u0000y(5.9)Now we take the log of both sides. This will turn out to be handy mathematically,and doesn\u2019t hurt us; whatever values maximize a probability will also maximize thelog of the probability:logp(y|x)=log\u21e5\u02c6yy(1\u0000\u02c6y)1\u0000y\u21e4=ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)(5.10)Eq.5.10describes a log likelihood that should be maximized. In order to turn thisinto loss function (something that we need to minimize), we\u2019ll just \ufb02ip the sign",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 50,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "resulting lossfunction is the negative log likelihood loss, generally called thecross-entropy loss.cross-entropylossLet\u2019s derive this loss function, applied to a single observationx. We\u2019d like tolearn weights that maximize the probability of the correct labelp(y|x). Since thereare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we canexpress the probabilityp(y|x)that our classi\ufb01er produces for one observation asthe following (keeping in mind that if y=1, Eq.5.9simpli\ufb01es to \u02c6y; if y=0, Eq.5.9simpli\ufb01es to 1\u0000\u02c6y):p(y|x)=\u02c6yy(1\u0000\u02c6y)1\u0000y(5.9)Now we take the log of both sides. This will turn out to be handy mathematically,and doesn\u2019t hurt us; whatever values maximize a probability will also maximize thelog of the probability:logp(y|x)=log\u21e5\u02c6yy(1\u0000\u02c6y)1\u0000y\u21e4=ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)(5.10)Eq.5.10describes a log likelihood that should be maximized. In order to turn thisinto loss function (something that we need to minimize), we\u2019ll just \ufb02ip the sign onEq.5.10. The result is the cross-entropy lossLCE:LCE(\u02c6y,y)=\u0000logp(y|x)=\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)](5.11)Finally, we can plug in the de\ufb01nition of \u02c6y=s(w\u00b7x+b):LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.12)Let\u2019s see if this loss function does the right thing for our example from Fig.5.2.W ewant the loss to be smaller if the model\u2019s estimate is close to correct, and bigger ifthe model is confused. So \ufb01rst let\u2019s suppose the correct gold label for the sentimentexample in Fig.5.2is positive, i.e.,y=1. In this case our model is doing well, sincefrom Eq.5.7it indeed gave the example a higher probability of being positive (.69)than negative (.31). If we plugs(w\u00b7x+b)=.69 andy=1 into Eq.5.12, the rightGoal: maximize probability of the correct label p(y|x) Maximize:Maximize:",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 51,
      "token_count": 570,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 43",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 52,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Deriving cross-entropy loss for a single observation xNow flip sign to turn this into a loss: something to minimizeCross-entropy loss (because is formula for cross-entropy(y,!\ud835\udc66))Or, plugging in definition of !\ud835\udc66:5.3\u2022THE CROSS-ENTROPY LOSS FUNCTION7thecross-entropy loss.The second thing we need is an optimization algorithm for iteratively updatingthe weights so as to minimize this loss function. The standard algorithm for this isgradient descent; we\u2019ll introduce thestochastic gradient descentalgorithm in thefollowing section.5.3 The cross-entropy loss functionWe need a loss function that expresses, for an observationx, how close the classi\ufb01eroutput ( \u02c6y=s(w\u00b7x+b)) is to the correct output (y, which is 0 or 1). We\u2019ll call this:L(\u02c6y,y)=How much \u02c6ydiffers from the truey(5.8)We do this via a loss function that prefers the correct class labels of the train-ing examples to bemore likely. This is calledconditional maximum likelihoodestimation: we choose the parametersw,bthatmaximize the log probability ofthe trueylabels in the training datagiven the observationsx. The resulting lossfunction is the negative log likelihood loss, generally called thecross-entropy loss.cross-entropylossLet\u2019s derive this loss function, applied to a single observationx. We\u2019d like tolearn weights that maximize the probability of the correct labelp(y|x). Since thereare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we canexpress the probabilityp(y|x)that our classi\ufb01er produces for one observation asthe following (keeping in mind that if y=1, Eq.5.9simpli\ufb01es to \u02c6y; if y=0, Eq.5.9simpli\ufb01es to 1\u0000\u02c6y):p(y|x)=\u02c6yy(1\u0000\u02c6y)1\u0000y(5.9)Now we take the log of both sides. This will turn out to be handy mathematically,and doesn\u2019t hurt us; whatever values maximize a probability will also maximize thelog of the probability:logp(y|x)=log\u21e5\u02c6yy(1\u0000\u02c6y)1\u0000y\u21e4=ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)(5.10)Eq.5.10describes a log likelihood that should be maximized. In order to turn thisinto loss function (something that we need to minimize), we\u2019ll just \ufb02ip the sign onEq.5.10. The result is the cross-entropy lossLCE:LCE(\u02c6y,y)=\u0000logp(y|x)=\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)](5.11)Finally, we can plug in the de\ufb01nition of \u02c6y=s(w\u00b7x+b):LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.12)Let\u2019s see if this loss function does the right thing for our example from Fig.5.2.W ewant the loss to be smaller if the model\u2019s estimate is close to correct, and bigger ifthe model is confused. So \ufb01rst let\u2019s suppose the correct gold label for the sentimentexample in Fig.5.2is positive, i.e.,y=1. In this case our model is doing well, sincefrom Eq.5.7it indeed gave the example a higher probability of being positive (.69)than negative (.31). If we",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 53,
      "token_count": 795,
      "chapter_title": ""
    }
  },
  {
    "content": "\u02c6y+(1\u0000y)log(1\u0000\u02c6y)(5.10)Eq.5.10describes a log likelihood that should be maximized. In order to turn thisinto loss function (something that we need to minimize), we\u2019ll just \ufb02ip the sign onEq.5.10. The result is the cross-entropy lossLCE:LCE(\u02c6y,y)=\u0000logp(y|x)=\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)](5.11)Finally, we can plug in the de\ufb01nition of \u02c6y=s(w\u00b7x+b):LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.12)Let\u2019s see if this loss function does the right thing for our example from Fig.5.2.W ewant the loss to be smaller if the model\u2019s estimate is close to correct, and bigger ifthe model is confused. So \ufb01rst let\u2019s suppose the correct gold label for the sentimentexample in Fig.5.2is positive, i.e.,y=1. In this case our model is doing well, sincefrom Eq.5.7it indeed gave the example a higher probability of being positive (.69)than negative (.31). If we plugs(w\u00b7x+b)=.69 andy=1 into Eq.5.12, the rightGoal: maximize probability of the correct label p(y|x) Maximize:Minimize:5.3\u2022THE CROSS-ENTROPY LOSS FUNCTION7thecross-entropy loss.The second thing we need is an optimization algorithm for iteratively updatingthe weights so as to minimize this loss function. The standard algorithm for this isgradient descent; we\u2019ll introduce thestochastic gradient descentalgorithm in thefollowing section.5.3 The cross-entropy loss functionWe need a loss function that expresses, for an observationx, how close the classi\ufb01eroutput ( \u02c6y=s(w\u00b7x+b)) is to the correct output (y, which is 0 or 1). We\u2019ll call this:L(\u02c6y,y)=How much \u02c6ydiffers from the truey(5.8)We do this via a loss function that prefers the correct class labels of the train-ing examples to bemore likely. This is calledconditional maximum likelihoodestimation: we choose the parametersw,bthatmaximize the log probability ofthe trueylabels in the training datagiven the observationsx. The resulting lossfunction is the negative log likelihood loss, generally called thecross-entropy loss.cross-entropylossLet\u2019s derive this loss function, applied to a single observationx. We\u2019d like tolearn weights that maximize the probability of the correct labelp(y|x). Since thereare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we canexpress the probabilityp(y|x)that our classi\ufb01er produces for one observation asthe following (keeping in mind that if y=1, Eq.5.9simpli\ufb01es to \u02c6y; if y=0, Eq.5.9simpli\ufb01es to 1\u0000\u02c6y):p(y|x)=\u02c6yy(1\u0000\u02c6y)1\u0000y(5.9)Now we take the log of both sides. This will turn out to be handy mathematically,and doesn\u2019t hurt us; whatever values maximize a probability will also maximize thelog of the probability:logp(y|x)=log\u21e5\u02c6yy(1\u0000\u02c6y)1\u0000y\u21e4=ylog",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 54,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "do this via a loss function that prefers the correct class labels of the train-ing examples to bemore likely. This is calledconditional maximum likelihoodestimation: we choose the parametersw,bthatmaximize the log probability ofthe trueylabels in the training datagiven the observationsx. The resulting lossfunction is the negative log likelihood loss, generally called thecross-entropy loss.cross-entropylossLet\u2019s derive this loss function, applied to a single observationx. We\u2019d like tolearn weights that maximize the probability of the correct labelp(y|x). Since thereare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we canexpress the probabilityp(y|x)that our classi\ufb01er produces for one observation asthe following (keeping in mind that if y=1, Eq.5.9simpli\ufb01es to \u02c6y; if y=0, Eq.5.9simpli\ufb01es to 1\u0000\u02c6y):p(y|x)=\u02c6yy(1\u0000\u02c6y)1\u0000y(5.9)Now we take the log of both sides. This will turn out to be handy mathematically,and doesn\u2019t hurt us; whatever values maximize a probability will also maximize thelog of the probability:logp(y|x)=log\u21e5\u02c6yy(1\u0000\u02c6y)1\u0000y\u21e4=ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)(5.10)Eq.5.10describes a log likelihood that should be maximized. In order to turn thisinto loss function (something that we need to minimize), we\u2019ll just \ufb02ip the sign onEq.5.10. The result is the cross-entropy lossLCE:LCE(\u02c6y,y)=\u0000logp(y|x)=\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)](5.11)Finally, we can plug in the de\ufb01nition of \u02c6y=s(w\u00b7x+b):LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.12)Let\u2019s see if this loss function does the right thing for our example from Fig.5.2.W ewant the loss to be smaller if the model\u2019s estimate is close to correct, and bigger ifthe model is confused. So \ufb01rst let\u2019s suppose the correct gold label for the sentimentexample in Fig.5.2is positive, i.e.,y=1. In this case our model is doing well, sincefrom Eq.5.7it indeed gave the example a higher probability of being positive (.69)than negative (.31). If we plugs(w\u00b7x+b)=.69 andy=1 into Eq.5.12, the right5.3\u2022THE CROSS-ENTROPY LOSS FUNCTION7thecross-entropy loss.The second thing we need is an optimization algorithm for iteratively updatingthe weights so as to minimize this loss function. The standard algorithm for this isgradient descent; we\u2019ll introduce thestochastic gradient descentalgorithm in thefollowing section.5.3 The cross-entropy loss functionWe need a loss function that expresses, for an observationx, how close the classi\ufb01eroutput ( \u02c6y=s(w\u00b7x+b)) is to the correct output (y, which is 0 or 1). We\u2019ll call this:L(\u02c6y,y)=How much \u02c6ydiffers from the truey(5.8)We do this via a loss function that prefers the correct class labels of the train-ing examples to bemore likely. This is calledconditional maximum likelihoodestimation: we choose the",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 55,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "ewant the loss to be smaller if the model\u2019s estimate is close to correct, and bigger ifthe model is confused. So \ufb01rst let\u2019s suppose the correct gold label for the sentimentexample in Fig.5.2is positive, i.e.,y=1. In this case our model is doing well, sincefrom Eq.5.7it indeed gave the example a higher probability of being positive (.69)than negative (.31). If we plugs(w\u00b7x+b)=.69 andy=1 into Eq.5.12, the right5.3\u2022THE CROSS-ENTROPY LOSS FUNCTION7thecross-entropy loss.The second thing we need is an optimization algorithm for iteratively updatingthe weights so as to minimize this loss function. The standard algorithm for this isgradient descent; we\u2019ll introduce thestochastic gradient descentalgorithm in thefollowing section.5.3 The cross-entropy loss functionWe need a loss function that expresses, for an observationx, how close the classi\ufb01eroutput ( \u02c6y=s(w\u00b7x+b)) is to the correct output (y, which is 0 or 1). We\u2019ll call this:L(\u02c6y,y)=How much \u02c6ydiffers from the truey(5.8)We do this via a loss function that prefers the correct class labels of the train-ing examples to bemore likely. This is calledconditional maximum likelihoodestimation: we choose the parametersw,bthatmaximize the log probability ofthe trueylabels in the training datagiven the observationsx. The resulting lossfunction is the negative log likelihood loss, generally called thecross-entropy loss.cross-entropylossLet\u2019s derive this loss function, applied to a single observationx. We\u2019d like tolearn weights that maximize the probability of the correct labelp(y|x). Since thereare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we canexpress the probabilityp(y|x)that our classi\ufb01er produces for one observation asthe following (keeping in mind that if y=1, Eq.5.9simpli\ufb01es to \u02c6y; if y=0, Eq.5.9simpli\ufb01es to 1\u0000\u02c6y):p(y|x)=\u02c6yy(1\u0000\u02c6y)1\u0000y(5.9)Now we take the log of both sides. This will turn out to be handy mathematically,and doesn\u2019t hurt us; whatever values maximize a probability will also maximize thelog of the probability:logp(y|x)=log\u21e5\u02c6yy(1\u0000\u02c6y)1\u0000y\u21e4=ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)(5.10)Eq.5.10describes a log likelihood that should be maximized. In order to turn thisinto loss function (something that we need to minimize), we\u2019ll just \ufb02ip the sign onEq.5.10. The result is the cross-entropy lossLCE:LCE(\u02c6y,y)=\u0000logp(y|x)=\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)](5.11)Finally, we can plug in the de\ufb01nition of \u02c6y=s(w\u00b7x+b):LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.12)Let\u2019s see if this loss function does the right thing for our example from Fig.5.2.W ewant the loss to be smaller if the model\u2019s estimate is close to correct, and bigger ifthe model is confused. So \ufb01rst let\u2019s suppose the correct gold label for the",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 56,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "log of both sides. This will turn out to be handy mathematically,and doesn\u2019t hurt us; whatever values maximize a probability will also maximize thelog of the probability:logp(y|x)=log\u21e5\u02c6yy(1\u0000\u02c6y)1\u0000y\u21e4=ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)(5.10)Eq.5.10describes a log likelihood that should be maximized. In order to turn thisinto loss function (something that we need to minimize), we\u2019ll just \ufb02ip the sign onEq.5.10. The result is the cross-entropy lossLCE:LCE(\u02c6y,y)=\u0000logp(y|x)=\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)](5.11)Finally, we can plug in the de\ufb01nition of \u02c6y=s(w\u00b7x+b):LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.12)Let\u2019s see if this loss function does the right thing for our example from Fig.5.2.W ewant the loss to be smaller if the model\u2019s estimate is close to correct, and bigger ifthe model is confused. So \ufb01rst let\u2019s suppose the correct gold label for the sentimentexample in Fig.5.2is positive, i.e.,y=1. In this case our model is doing well, sincefrom Eq.5.7it indeed gave the example a higher probability of being positive (.69)than negative (.31). If we plugs(w\u00b7x+b)=.69 andy=1 into Eq.5.12, the right",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 57,
      "token_count": 376,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 44\n\nLet's see if this works for our sentiment exampleWe want loss to be:\u2022smaller if the model estimate is close to correct\u2022bigger if model is confusedLet's first suppose the true label of this is y=1 (positive)It's hokey . There are virtually no surprises , and the writing is second-rate .         So why was it so enjoyable ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing . It sucked me in , and it'll do the same to you .",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 58,
      "token_count": 126,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 45",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 59,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Let's see if this works for our sentiment exampleTrue value is y=1.  How well is our model doing?Pretty well!  What's the loss?5.1\u2022CLASSIFICATION:THE SIGMOID5 It's hokey . There are virtually no surprises , and the writing is second-rate . So why was it so enjoyable  ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing .  It sucked me in , and it'll do the same to you  .x1=3x6=4.19x3=1x4=3x5=0x2=2",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 60,
      "token_count": 145,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 5.2A sample mini test document showing the extracted features in the vectorx.Given these 6 features and the input reviewx,P(+|x)andP(\u0000|x)can be com-puted using Eq.5.5:p(+|x)=P(Y=1|x)=s(w\u00b7x+b)=s([2.5,\u00005.0,\u00001.2,0.5,2.0,0.7]\u00b7[3,2,1,3,0,4.19]+0.1)=s(.833)=0.70(5.6)p(\u0000|x)=P(Y=0|x)=1\u0000s(w\u00b7x+b)=0.30Logistic regression is commonly applied to all sorts of NLP tasks, and any propertyof the input can be a feature. Consider the task ofperiod disambiguation: decidingif a period is the end of a sentence or part of a word, by classifying each periodinto one of two classes EOS (end-of-sentence) and not-EOS. We might use featureslikex1below expressing that the current word is lower case and the class is EOS(perhaps with a positive weight), or that the current word is in our abbreviationsdictionary (\u201cProf.\u201d) and the class is EOS (perhaps with a negative weight). A featurecan also express a quite complex combination of properties. For example a periodfollowing an upper case word is likely to be an EOS, but if the word itself isSt.andthe previous word is capitalized, then the period is likely part of a shortening of thewordstreet.x1=\u21e21 if \u201cCase(wi)=Lower\u201d0 otherwisex2=\u21e21 if \u201cwi2AcronymDict\u201d0 otherwisex3=\u21e21 if \u201cwi=St. &Case(wi\u00001)=Cap\u201d0 otherwiseDesigning features:Features are generally designed by examining the trainingset with an eye to linguistic intuitions and the linguistic literature on the domain. Acareful error analysis on the training set or devset of an early version of a systemoften provides insights into features.For some tasks it is especially helpful to build complex features that are combi-nations of more primitive features. We saw such a feature for period disambiguationabove, where a period on the wordSt.was less likely to be the end of the sentenceif the previous word was capitalized. For logistic regression and naive Bayes thesecombination features orfeature interactionshave to be designed by hand.featureinteractions8CHAPTER5\u2022LOGISTICREGRESSIONside of the equation drops out, leading to the following loss (we\u2019ll use log to meannatural log when the base is not speci\ufb01ed):LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))]=\u0000[logs(w\u00b7x+b)]=\u0000log(.70)=.36By contrast, let\u2019s pretend instead that the example in Fig.5.2was actually negative,i.e.,y=0 (perhaps the reviewer went on to say \u201cBut bottom line, the movie isterrible! I beg you not to see it!\u201d). In this case our model is confused and we\u2019d wantthe loss to be higher. Now if we plugy=0 and 1\u0000s(w\u00b7x+b)=.31 from Eq.5.7into Eq.5.12, the left side of the equation drops out:LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))]=\u0000[log(1\u0000s(w\u00b7x+b))]=\u0000log(.30)=1.2Sure enough, the loss for the \ufb01rst classi\ufb01er",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 61,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "of the sentenceif the previous word was capitalized. For logistic regression and naive Bayes thesecombination features orfeature interactionshave to be designed by hand.featureinteractions8CHAPTER5\u2022LOGISTICREGRESSIONside of the equation drops out, leading to the following loss (we\u2019ll use log to meannatural log when the base is not speci\ufb01ed):LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))]=\u0000[logs(w\u00b7x+b)]=\u0000log(.70)=.36By contrast, let\u2019s pretend instead that the example in Fig.5.2was actually negative,i.e.,y=0 (perhaps the reviewer went on to say \u201cBut bottom line, the movie isterrible! I beg you not to see it!\u201d). In this case our model is confused and we\u2019d wantthe loss to be higher. Now if we plugy=0 and 1\u0000s(w\u00b7x+b)=.31 from Eq.5.7into Eq.5.12, the left side of the equation drops out:LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))]=\u0000[log(1\u0000s(w\u00b7x+b))]=\u0000log(.30)=1.2Sure enough, the loss for the \ufb01rst classi\ufb01er (.37) is less than the loss for the secondclassi\ufb01er (1.17).Why does minimizing this negative log probability do what we want? A per-fect classi\ufb01er would assign probability 1 to the correct outcome (y=1 or y=0) andprobability 0 to the incorrect outcome. That means the higher \u02c6y(the closer it isto 1), the better the classi\ufb01er; the lower \u02c6yis (the closer it is to 0), the worse theclassi\ufb01er. The negative log of this probability is a convenient loss metric since itgoes from 0 (negative log of 1, no loss) to in\ufb01nity (negative log of 0, in\ufb01nite loss).This loss function also ensures that as the probability of the correct answer is max-imized, the probability of the incorrect answer is minimized; since the two sum toone, any increase in the probability of the correct answer is coming at the expenseof the incorrect answer. It\u2019s called the cross-entropy loss, because Eq.5.10is alsothe formula for thecross-entropybetween the true probability distributionyand ourestimated distribution \u02c6y.Now we know what we want to minimize; in the next section, we\u2019ll see how to\ufb01nd the minimum.5.4 Gradient DescentOur goal with gradient descent is to \ufb01nd the optimal weights: minimize the lossfunction we\u2019ve de\ufb01ned for the model. In Eq.5.13below, we\u2019ll explicitly representthe fact that the loss functionLis parameterized by the weights, which we\u2019ll referto in machine learning in general asq(in the case of logistic regressionq=w,b).So the goal is to \ufb01nd the set of weights which minimizes the loss function, averagedover all examples:\u02c6q=argminq1mmXi=1LCE(f(x(i);q),y(i))(5.13)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 62,
      "token_count": 716,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 46",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 63,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Let's see if this works for our sentiment exampleSuppose true value instead  was y=0.  What's the loss?8CHAPTER5\u2022LOGISTICREGRESSIONside of the equation drops out, leading to the following loss (we\u2019ll use log to meannatural log when the base is not speci\ufb01ed):LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))]=\u0000[logs(w\u00b7x+b)]=\u0000log(.70)=.36By contrast, let\u2019s pretend instead that the example in Fig.5.2was actually negative,i.e.,y=0 (perhaps the reviewer went on to say \u201cBut bottom line, the movie isterrible! I beg you not to see it!\u201d). In this case our model is confused and we\u2019d wantthe loss to be higher. Now if we plugy=0 and 1\u0000s(w\u00b7x+b)=.31 from Eq.5.7into Eq.5.12, the left side of the equation drops out:LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))]=\u0000[log(1\u0000s(w\u00b7x+b))]=\u0000log(.30)=1.2Sure enough, the loss for the \ufb01rst classi\ufb01er (.37) is less than the loss for the secondclassi\ufb01er (1.17).Why does minimizing this negative log probability do what we want? A per-fect classi\ufb01er would assign probability 1 to the correct outcome (y=1 or y=0) andprobability 0 to the incorrect outcome. That means the higher \u02c6y(the closer it isto 1), the better the classi\ufb01er; the lower \u02c6yis (the closer it is to 0), the worse theclassi\ufb01er. The negative log of this probability is a convenient loss metric since itgoes from 0 (negative log of 1, no loss) to in\ufb01nity (negative log of 0, in\ufb01nite loss).This loss function also ensures that as the probability of the correct answer is max-imized, the probability of the incorrect answer is minimized; since the two sum toone, any increase in the probability of the correct answer is coming at the expenseof the incorrect answer. It\u2019s called the cross-entropy loss, because Eq.5.10is alsothe formula for thecross-entropybetween the true probability distributionyand ourestimated distribution \u02c6y.Now we know what we want to minimize; in the next section, we\u2019ll see how to\ufb01nd the minimum.5.4 Gradient DescentOur goal with gradient descent is to \ufb01nd the optimal weights: minimize the lossfunction we\u2019ve de\ufb01ned for the model. In Eq.5.13below, we\u2019ll explicitly representthe fact that the loss functionLis parameterized by the weights, which we\u2019ll referto in machine learning in general asq(in the case of logistic regressionq=w,b).So the goal is to \ufb01nd the set of weights which minimizes the loss function, averagedover all examples:\u02c6q=argminq1mmXi=1LCE(f(x(i);q),y(i))(5.13)5.1\u2022CLASSIFICATION:THE SIGMOID5 It's hokey . There are virtually no surprises , and the writing is second-rate . So why was it so enjoyable  ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing .  It sucked me in , and it'll do the same to you",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 64,
      "token_count": 793,
      "chapter_title": ""
    }
  },
  {
    "content": "any increase in the probability of the correct answer is coming at the expenseof the incorrect answer. It\u2019s called the cross-entropy loss, because Eq.5.10is alsothe formula for thecross-entropybetween the true probability distributionyand ourestimated distribution \u02c6y.Now we know what we want to minimize; in the next section, we\u2019ll see how to\ufb01nd the minimum.5.4 Gradient DescentOur goal with gradient descent is to \ufb01nd the optimal weights: minimize the lossfunction we\u2019ve de\ufb01ned for the model. In Eq.5.13below, we\u2019ll explicitly representthe fact that the loss functionLis parameterized by the weights, which we\u2019ll referto in machine learning in general asq(in the case of logistic regressionq=w,b).So the goal is to \ufb01nd the set of weights which minimizes the loss function, averagedover all examples:\u02c6q=argminq1mmXi=1LCE(f(x(i);q),y(i))(5.13)5.1\u2022CLASSIFICATION:THE SIGMOID5 It's hokey . There are virtually no surprises , and the writing is second-rate . So why was it so enjoyable  ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing .  It sucked me in , and it'll do the same to you  .x1=3x6=4.19x3=1x4=3x5=0x2=2",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 65,
      "token_count": 327,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 5.2A sample mini test document showing the extracted features in the vectorx.Given these 6 features and the input reviewx,P(+|x)andP(\u0000|x)can be com-puted using Eq.5.5:p(+|x)=P(Y=1|x)=s(w\u00b7x+b)=s([2.5,\u00005.0,\u00001.2,0.5,2.0,0.7]\u00b7[3,2,1,3,0,4.19]+0.1)=s(.833)=0.70(5.6)p(\u0000|x)=P(Y=0|x)=1\u0000s(w\u00b7x+b)=0.30Logistic regression is commonly applied to all sorts of NLP tasks, and any propertyof the input can be a feature. Consider the task ofperiod disambiguation: decidingif a period is the end of a sentence or part of a word, by classifying each periodinto one of two classes EOS (end-of-sentence) and not-EOS. We might use featureslikex1below expressing that the current word is lower case and the class is EOS(perhaps with a positive weight), or that the current word is in our abbreviationsdictionary (\u201cProf.\u201d) and the class is EOS (perhaps with a negative weight). A featurecan also express a quite complex combination of properties. For example a periodfollowing an upper case word is likely to be an EOS, but if the word itself isSt.andthe previous word is capitalized, then the period is likely part of a shortening of thewordstreet.x1=\u21e21 if \u201cCase(wi)=Lower\u201d0 otherwisex2=\u21e21 if \u201cwi2AcronymDict\u201d0 otherwisex3=\u21e21 if \u201cwi=St. &Case(wi\u00001)=Cap\u201d0 otherwiseDesigning features:Features are generally designed by examining the trainingset with an eye to linguistic intuitions and the linguistic literature on the domain. Acareful error analysis on the training set or devset of an early version of a systemoften provides insights into features.For some tasks it is especially helpful to build complex features that are combi-nations of more primitive features. We saw such a feature for period disambiguationabove, where a period on the wordSt.was less likely to be the end of the sentenceif the previous word was capitalized. For logistic regression and naive Bayes thesecombination features orfeature interactionshave to be designed by hand.featureinteractions",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 66,
      "token_count": 531,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 47",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 67,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Let's see if this works for our sentiment exampleThe loss when model was right (if true y=1) Is lower than the loss when model was wrong (if true y=0):Sure enough, loss was bigger when model was wrong!8CHAPTER5\u2022LOGISTICREGRESSIONside of the equation drops out, leading to the following loss (we\u2019ll use log to meannatural log when the base is not speci\ufb01ed):LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))]=\u0000[logs(w\u00b7x+b)]=\u0000log(.70)=.36By contrast, let\u2019s pretend instead that the example in Fig.5.2was actually negative,i.e.,y=0 (perhaps the reviewer went on to say \u201cBut bottom line, the movie isterrible! I beg you not to see it!\u201d). In this case our model is confused and we\u2019d wantthe loss to be higher. Now if we plugy=0 and 1\u0000s(w\u00b7x+b)=.31 from Eq.5.7into Eq.5.12, the left side of the equation drops out:LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))]=\u0000[log(1\u0000s(w\u00b7x+b))]=\u0000log(.30)=1.2Sure enough, the loss for the \ufb01rst classi\ufb01er (.37) is less than the loss for the secondclassi\ufb01er (1.17).Why does minimizing this negative log probability do what we want? A per-fect classi\ufb01er would assign probability 1 to the correct outcome (y=1 or y=0) andprobability 0 to the incorrect outcome. That means the higher \u02c6y(the closer it isto 1), the better the classi\ufb01er; the lower \u02c6yis (the closer it is to 0), the worse theclassi\ufb01er. The negative log of this probability is a convenient loss metric since itgoes from 0 (negative log of 1, no loss) to in\ufb01nity (negative log of 0, in\ufb01nite loss).This loss function also ensures that as the probability of the correct answer is max-imized, the probability of the incorrect answer is minimized; since the two sum toone, any increase in the probability of the correct answer is coming at the expenseof the incorrect answer. It\u2019s called the cross-entropy loss, because Eq.5.10is alsothe formula for thecross-entropybetween the true probability distributionyand ourestimated distribution \u02c6y.Now we know what we want to minimize; in the next section, we\u2019ll see how to\ufb01nd the minimum.5.4 Gradient DescentOur goal with gradient descent is to \ufb01nd the optimal weights: minimize the lossfunction we\u2019ve de\ufb01ned for the model. In Eq.5.13below, we\u2019ll explicitly representthe fact that the loss functionLis parameterized by the weights, which we\u2019ll referto in machine learning in general asq(in the case of logistic regressionq=w,b).So the goal is to \ufb01nd the set of weights which minimizes the loss function, averagedover all examples:\u02c6q=argminq1mmXi=1LCE(f(x(i);q),y(i))(5.13)8CHAPTER5\u2022LOGISTICREGRESSIONside of the equation drops out, leading to the following loss (we\u2019ll use log to meannatural log when the base is not",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 68,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "(negative log of 0, in\ufb01nite loss).This loss function also ensures that as the probability of the correct answer is max-imized, the probability of the incorrect answer is minimized; since the two sum toone, any increase in the probability of the correct answer is coming at the expenseof the incorrect answer. It\u2019s called the cross-entropy loss, because Eq.5.10is alsothe formula for thecross-entropybetween the true probability distributionyand ourestimated distribution \u02c6y.Now we know what we want to minimize; in the next section, we\u2019ll see how to\ufb01nd the minimum.5.4 Gradient DescentOur goal with gradient descent is to \ufb01nd the optimal weights: minimize the lossfunction we\u2019ve de\ufb01ned for the model. In Eq.5.13below, we\u2019ll explicitly representthe fact that the loss functionLis parameterized by the weights, which we\u2019ll referto in machine learning in general asq(in the case of logistic regressionq=w,b).So the goal is to \ufb01nd the set of weights which minimizes the loss function, averagedover all examples:\u02c6q=argminq1mmXi=1LCE(f(x(i);q),y(i))(5.13)8CHAPTER5\u2022LOGISTICREGRESSIONside of the equation drops out, leading to the following loss (we\u2019ll use log to meannatural log when the base is not speci\ufb01ed):LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))]=\u0000[logs(w\u00b7x+b)]=\u0000log(.70)=.36By contrast, let\u2019s pretend instead that the example in Fig.5.2was actually negative,i.e.,y=0 (perhaps the reviewer went on to say \u201cBut bottom line, the movie isterrible! I beg you not to see it!\u201d). In this case our model is confused and we\u2019d wantthe loss to be higher. Now if we plugy=0 and 1\u0000s(w\u00b7x+b)=.31 from Eq.5.7into Eq.5.12, the left side of the equation drops out:LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))]=\u0000[log(1\u0000s(w\u00b7x+b))]=\u0000log(.30)=1.2Sure enough, the loss for the \ufb01rst classi\ufb01er (.37) is less than the loss for the secondclassi\ufb01er (1.17).Why does minimizing this negative log probability do what we want? A per-fect classi\ufb01er would assign probability 1 to the correct outcome (y=1 or y=0) andprobability 0 to the incorrect outcome. That means the higher \u02c6y(the closer it isto 1), the better the classi\ufb01er; the lower \u02c6yis (the closer it is to 0), the worse theclassi\ufb01er. The negative log of this probability is a convenient loss metric since itgoes from 0 (negative log of 1, no loss) to in\ufb01nity (negative log of 0, in\ufb01nite loss).This loss function also ensures that as the probability of the correct answer is max-imized, the probability of the incorrect answer is minimized; since the two sum toone, any increase in the probability of the correct answer is coming at the expenseof the incorrect answer. It\u2019s called the cross-entropy loss, because Eq.5.10is alsothe formula for thecross-entropybetween the true probability distributionyand ourestimated distribution \u02c6y.Now we know what we want to minimize; in the next",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 69,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "enough, the loss for the \ufb01rst classi\ufb01er (.37) is less than the loss for the secondclassi\ufb01er (1.17).Why does minimizing this negative log probability do what we want? A per-fect classi\ufb01er would assign probability 1 to the correct outcome (y=1 or y=0) andprobability 0 to the incorrect outcome. That means the higher \u02c6y(the closer it isto 1), the better the classi\ufb01er; the lower \u02c6yis (the closer it is to 0), the worse theclassi\ufb01er. The negative log of this probability is a convenient loss metric since itgoes from 0 (negative log of 1, no loss) to in\ufb01nity (negative log of 0, in\ufb01nite loss).This loss function also ensures that as the probability of the correct answer is max-imized, the probability of the incorrect answer is minimized; since the two sum toone, any increase in the probability of the correct answer is coming at the expenseof the incorrect answer. It\u2019s called the cross-entropy loss, because Eq.5.10is alsothe formula for thecross-entropybetween the true probability distributionyand ourestimated distribution \u02c6y.Now we know what we want to minimize; in the next section, we\u2019ll see how to\ufb01nd the minimum.5.4 Gradient DescentOur goal with gradient descent is to \ufb01nd the optimal weights: minimize the lossfunction we\u2019ve de\ufb01ned for the model. In Eq.5.13below, we\u2019ll explicitly representthe fact that the loss functionLis parameterized by the weights, which we\u2019ll referto in machine learning in general asq(in the case of logistic regressionq=w,b).So the goal is to \ufb01nd the set of weights which minimizes the loss function, averagedover all examples:\u02c6q=argminq1mmXi=1LCE(f(x(i);q),y(i))(5.13)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 70,
      "token_count": 431,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 48\n\nLogistic RegressionCross-Entropy Loss\n\n## Page 49\n\nLogistic RegressionStochastic Gradient Descent\n\n## Page 50\n\nOur goal: minimize the lossLet's make explicit that the loss function is parameterized by weights \ud835\udef3=(w,b)\u2022And we\u2019ll represent !\ud835\udc66as f (x; \u03b8 ) to make the dependence on \u03b8 more obviousWe want the weights that minimize the loss, averaged over all examples:8CHAPTER5\u2022LOGISTICREGRESSIONside of the equation drops out, leading to the following loss (we\u2019ll use log to meannatural log when the base is not speci\ufb01ed):LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))]=\u0000[logs(w\u00b7x+b)]=\u0000log(.70)=.36By contrast, let\u2019s pretend instead that the example in Fig.5.2was actually negative,i.e.,y=0 (perhaps the reviewer went on to say \u201cBut bottom line, the movie isterrible! I beg you not to see it!\u201d). In this case our model is confused and we\u2019d wantthe loss to be higher. Now if we plugy=0 and 1\u0000s(w\u00b7x+b)=.31 from Eq.5.7into Eq.5.12, the left side of the equation drops out:LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))]=\u0000[log(1\u0000s(w\u00b7x+b))]=\u0000log(.30)=1.2Sure enough, the loss for the \ufb01rst classi\ufb01er (.37) is less than the loss for the secondclassi\ufb01er (1.17).Why does minimizing this negative log probability do what we want? A per-fect classi\ufb01er would assign probability 1 to the correct outcome (y=1 or y=0) andprobability 0 to the incorrect outcome. That means the higher \u02c6y(the closer it isto 1), the better the classi\ufb01er; the lower \u02c6yis (the closer it is to 0), the worse theclassi\ufb01er. The negative log of this probability is a convenient loss metric since itgoes from 0 (negative log of 1, no loss) to in\ufb01nity (negative log of 0, in\ufb01nite loss).This loss function also ensures that as the probability of the correct answer is max-imized, the probability of the incorrect answer is minimized; since the two sum toone, any increase in the probability of the correct answer is coming at the expenseof the incorrect answer. It\u2019s called the cross-entropy loss, because Eq.5.10is alsothe formula for thecross-entropybetween the true probability distributionyand ourestimated distribution \u02c6y.Now we know what we want to minimize; in the next section, we\u2019ll see how to\ufb01nd the minimum.5.4 Gradient DescentOur goal with gradient descent is to \ufb01nd the optimal weights: minimize the lossfunction we\u2019ve de\ufb01ned for the model. In Eq.5.13below, we\u2019ll explicitly representthe fact that the loss functionLis parameterized by the weights, which we\u2019ll referto in machine learning in general asq(in the case of logistic regressionq=w,b).So the goal is to \ufb01nd the set of weights which minimizes the loss function, averagedover all examples:\u02c6q=argminq1mmXi=1LCE(f(x(i);q),y(i))(5.13)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 71,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 51\n\nIntuition of gradient descentHow do I get to the bottom of this river canyon?\nxLook around me 360\u2218Find the direction of steepest slope downGo that way\n\n## Page 52\n\nOur goal: minimize the lossFor logistic regression, loss function is convex\u2022A convex function has just one minimum\u2022Gradient descent starting from any point is guaranteed to find the minimum\u2022(Loss for neural networks is non-convex)\n\n## Page 53\n\nLet's first visualize for a single scalar w\nwLoss\n0w1wmin(goal)Should we move right or left from here?Q: Given current w, should we make it bigger or smaller?A: Move w in the reverse direction from the slope of the function \n\n## Page 54\n\nLet's first visualize for a single scalar w\nwLoss\n0w1wminslope of loss at w1 is negative(goal)Q: Given current w, should we make it bigger or smaller?A: Move w in the reverse direction from the slope of the function \nSo we'll move positive\n\n## Page 55\n\nLet's first visualize for a single scalar w\nwLoss\n0w1wminslope of loss at w1 is negative(goal)one stepof gradientdescentQ: Given current w, should we make it bigger or smaller?A: Move w in the reverse direction from the slope of the function \nSo we'll move positive\n\n## Page 56\n\nGradientsThe gradientof a function of many variables is a vector pointing in the direction of the greatest increase in a function. Gradient Descent: Find the gradient of the loss function at the current point and move in the oppositedirection.",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 72,
      "token_count": 353,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 53\n\nLet's first visualize for a single scalar w\nwLoss\n0w1wmin(goal)Should we move right or left from here?Q: Given current w, should we make it bigger or smaller?A: Move w in the reverse direction from the slope of the function \n\n## Page 54\n\nLet's first visualize for a single scalar w\nwLoss\n0w1wminslope of loss at w1 is negative(goal)Q: Given current w, should we make it bigger or smaller?A: Move w in the reverse direction from the slope of the function \nSo we'll move positive\n\n## Page 55\n\nLet's first visualize for a single scalar w\nwLoss\n0w1wminslope of loss at w1 is negative(goal)one stepof gradientdescentQ: Given current w, should we make it bigger or smaller?A: Move w in the reverse direction from the slope of the function \nSo we'll move positive\n\n## Page 56\n\nGradientsThe gradientof a function of many variables is a vector pointing in the direction of the greatest increase in a function. Gradient Descent: Find the gradient of the loss function at the current point and move in the oppositedirection. \n\n## Page 57\n\nHow much do we move in that direction ?\u2022The value of the gradient (slope in our example)  !!\"\ud835\udc3f(\ud835\udc53\ud835\udc65;\ud835\udc64,\ud835\udc66)weighted by a learning rate \u03b7 \u2022Higher learning rate means move wfaster10CHAPTER5\u2022LOGISTICREGRESSIONexample):wt+1=wt\u0000hddwL(f(x;w),y)(5.14)Now let\u2019s extend the intuition from a function of one scalar variablewto manyvariables, because we don\u2019t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If we\u2019re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.\nCost(w,b)\nwbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially we\u2019reasking: \u201cHow much would a small change in that variablewiin\ufb02uence the total lossfunctionL?\u201dIn each dimensionwi, we express the slope as a partial derivative\u2202\u2202wiof the lossfunction. The gradient is then de\ufb01ned as a vector of these partials. We\u2019ll represent \u02c6yasf(x;q)to make the dependence onqmore obvious:\u2014qL(f(x;q),y)) =266664\u2202\u2202w1L(f(x;q),y)\u2202\u2202w2L(f(x;q),y)...\u2202\u2202wnL(f(x;q),y)377775(5.15)The \ufb01nal equation for updatingqbased on the gradient is thusqt+1=qt\u0000h\u2014L(f(x;q),y)(5.16)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 73,
      "token_count": 789,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 58\n\nNow let's consider N dimensionsWe want to know where in the N-dimensional space (of the N parameters that make up \u03b8 ) we should move. The gradient is just such a vector; it expresses the directional components of the sharpest slope along each of the N dimensions. \n\n## Page 59\n\nImagine 2 dimensions, w and bVisualizing the gradient vector at the red pointIt has two dimensions shown in the x-y plane10CHAPTER5\u2022LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt\u0000hddwf(x;w)(5.14)Now let\u2019s extend the intuition from a function of one scalar variablewto manyvariables, because we don\u2019t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If we\u2019re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.\nCost(w,b)\nwbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially we\u2019reasking: \u201cHow much would a small change in that variablewiin\ufb02uence the total lossfunctionL?\u201dIn each dimensionwi, we express the slope as a partial derivative\u2202\u2202wiof the lossfunction. The gradient is then de\ufb01ned as a vector of these partials. We\u2019ll represent \u02c6yasf(x;q)to make the dependence onqmore obvious:\u2014qL(f(x;q),y)) =266664\u2202\u2202w1L(f(x;q),y)\u2202\u2202w2L(f(x;q),y)...\u2202\u2202wnL(f(x;q),y)377775(5.15)The \ufb01nal equation for updatingqbased on the gradient is thusqt+1=qt\u0000h\u2014L(f(x;q),y)(5.16)\n\n## Page 60\n\nReal gradientsAre much longer; lots and lots of weightsFor each dimension withe gradient component itells us the slope with respect to that variable. \u25e6\u201cHow much would a small change in wiinfluence the total loss function L?\u201d \u25e6We express the slope as a partial derivative \u2202 of the loss \u2202wiThe gradient is then defined as a vector of these partials.",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 74,
      "token_count": 655,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 61",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 75,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "The gradient10CHAPTER5\u2022LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt\u0000hddwf(x;w)(5.14)Now let\u2019s extend the intuition from a function of one scalar variablewto manyvariables, because we don\u2019t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If we\u2019re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.\nCost(w,b)\nwbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially we\u2019reasking: \u201cHow much would a small change in that variablewiin\ufb02uence the total lossfunctionL?\u201dIn each dimensionwi, we express the slope as a partial derivative\u2202\u2202wiof the lossfunction. The gradient is then de\ufb01ned as a vector of these partials. We\u2019ll represent \u02c6yasf(x;q)to make the dependence onqmore obvious:\u2014qL(f(x;q),y)) =266664\u2202\u2202w1L(f(x;q),y)\u2202\u2202w2L(f(x;q),y)...\u2202\u2202wnL(f(x;q),y)377775(5.15)The \ufb01nal equation for updatingqbased on the gradient is thusqt+1=qt\u0000h\u2014L(f(x;q),y)(5.16)We\u2019ll represent !\ud835\udc66as f (x; \u03b8 ) to make the dependence on \u03b8 more obvious: The final equation for updating \u03b8 based on the gradient is thus 10CHAPTER5\u2022LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt\u0000hddwf(x;w)(5.14)Now let\u2019s extend the intuition from a function of one scalar variablewto manyvariables, because we don\u2019t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If we\u2019re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.\nCost(w,b)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 76,
      "token_count": 713,
      "chapter_title": ""
    }
  },
  {
    "content": "Cost(w,b)\nwbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially we\u2019reasking: \u201cHow much would a small change in that variablewiin\ufb02uence the total lossfunctionL?\u201dIn each dimensionwi, we express the slope as a partial derivative\u2202\u2202wiof the lossfunction. The gradient is then de\ufb01ned as a vector of these partials. We\u2019ll represent \u02c6yasf(x;q)to make the dependence onqmore obvious:\u2014qL(f(x;q),y)) =266664\u2202\u2202w1L(f(x;q),y)\u2202\u2202w2L(f(x;q),y)...\u2202\u2202wnL(f(x;q),y)377775(5.15)The \ufb01nal equation for updatingqbased on the gradient is thusqt+1=qt\u0000h\u2014L(f(x;q),y)(5.16)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 77,
      "token_count": 286,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 62",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 78,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "What are these partial derivatives for logistic regression?The loss function5.4\u2022GRADIENTDESCENT115.4.1 The Gradient for Logistic RegressionIn order to updateq, we need a de\ufb01nition for the gradient\u2014L(f(x;q),y). Recall thatfor logistic regression, the cross-entropy loss function is:LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.17)It turns out that the derivative of this function for one observation vectorxisEq.5.18(the interested reader can see Section5.8for the derivation of this equation):\u2202LCE(\u02c6y,y)\u2202wj=[s(w\u00b7x+b)\u0000y]xj(5.18)Note in Eq.5.18that the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated \u02c6y=s(w\u00b7x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute \u02c6y(i)=f(x(i);q)# What is our estimated output \u02c6y?Compute the lossL(\u02c6y(i),y(i))# How far off is \u02c6y(i))from the true outputy(i)?2.g \u2014qL(f(x(i);q),y(i))# How should we moveqto maximize loss?3.q q\u0000hg# Go the other way insteadreturnqFigure 5.5The stochastic gradient descent algorithm. Step 1 (computing the loss) is usedto report how well we are doing on the current tuple. The algorithm can terminate when itconverges (or when the gradient norm<\u270f), or when progress halts (for example when theloss starts going up on a held-out set).The learning ratehis ahyperparameterthat must be adjusted. If it\u2019s too high,hyperparameterthe learner will take steps that are too large, overshooting the minimum of the lossfunction. If it\u2019s too low, the learner will take steps that are too small, and take toolong to get to the minimum. It is common to start with a higher learning rate and thenslowly decrease it, so that it is a function of the iterationkof training; the notationhkcan be used to mean the value of the learning rate at iterationk.We\u2019ll discuss hyperparameters in more detail in Chapter 7, but brie\ufb02y they area special kind of parameter for any machine learning model. Unlike regular param-eters of a model (weights likewandb), which are learned by the algorithm fromthe training set, hyperparameters are special parameters chosen by the algorithmdesigner that affect how the algorithm works.5.4\u2022GRADIENTDESCENT115.4.1 The Gradient for Logistic RegressionIn order to updateq, we need a de\ufb01nition for the",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 79,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "5.5The stochastic gradient descent algorithm. Step 1 (computing the loss) is usedto report how well we are doing on the current tuple. The algorithm can terminate when itconverges (or when the gradient norm<\u270f), or when progress halts (for example when theloss starts going up on a held-out set).The learning ratehis ahyperparameterthat must be adjusted. If it\u2019s too high,hyperparameterthe learner will take steps that are too large, overshooting the minimum of the lossfunction. If it\u2019s too low, the learner will take steps that are too small, and take toolong to get to the minimum. It is common to start with a higher learning rate and thenslowly decrease it, so that it is a function of the iterationkof training; the notationhkcan be used to mean the value of the learning rate at iterationk.We\u2019ll discuss hyperparameters in more detail in Chapter 7, but brie\ufb02y they area special kind of parameter for any machine learning model. Unlike regular param-eters of a model (weights likewandb), which are learned by the algorithm fromthe training set, hyperparameters are special parameters chosen by the algorithmdesigner that affect how the algorithm works.5.4\u2022GRADIENTDESCENT115.4.1 The Gradient for Logistic RegressionIn order to updateq, we need a de\ufb01nition for the gradient\u2014L(f(x;q),y). Recall thatfor logistic regression, the cross-entropy loss function is:LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.17)It turns out that the derivative of this function for one observation vectorxisEq.5.18(the interested reader can see Section5.8for the derivation of this equation):\u2202LCE(\u02c6y,y)\u2202wj=[s(w\u00b7x+b)\u0000y]xj(5.18)Note in Eq.5.18that the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated \u02c6y=s(w\u00b7x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute \u02c6y(i)=f(x(i);q)# What is our estimated output \u02c6y?Compute the lossL(\u02c6y(i),y(i))# How far off is \u02c6y(i))from the true outputy(i)?2.g \u2014qL(f(x(i);q),y(i))# How should we moveqto maximize loss?3.q q\u0000hg# Go the other way insteadreturnqFigure 5.5The stochastic gradient descent algorithm. Step 1 (computing the loss) is usedto report how well we are doing on the current tuple. The algorithm can terminate when itconverges (or when the gradient",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 80,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute \u02c6y(i)=f(x(i);q)# What is our estimated output \u02c6y?Compute the lossL(\u02c6y(i),y(i))# How far off is \u02c6y(i))from the true outputy(i)?2.g \u2014qL(f(x(i);q),y(i))# How should we moveqto maximize loss?3.q q\u0000hg# Go the other way insteadreturnqFigure 5.5The stochastic gradient descent algorithm. Step 1 (computing the loss) is usedto report how well we are doing on the current tuple. The algorithm can terminate when itconverges (or when the gradient norm<\u270f), or when progress halts (for example when theloss starts going up on a held-out set).The learning ratehis ahyperparameterthat must be adjusted. If it\u2019s too high,hyperparameterthe learner will take steps that are too large, overshooting the minimum of the lossfunction. If it\u2019s too low, the learner will take steps that are too small, and take toolong to get to the minimum. It is common to start with a higher learning rate and thenslowly decrease it, so that it is a function of the iterationkof training; the notationhkcan be used to mean the value of the learning rate at iterationk.We\u2019ll discuss hyperparameters in more detail in Chapter 7, but brie\ufb02y they area special kind of parameter for any machine learning model. Unlike regular param-eters of a model (weights likewandb), which are learned by the algorithm fromthe training set, hyperparameters are special parameters chosen by the algorithmdesigner that affect how the algorithm works.The elegant derivative of this function(see textbook 5.8 for derivation)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 81,
      "token_count": 532,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 63\n\n5.4\u2022GRADIENTDESCENT115.4.1 The Gradient for Logistic RegressionIn order to updateq, we need a de\ufb01nition for the gradient\u2014L(f(x;q),y). Recall thatfor logistic regression, the cross-entropy loss function is:LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.17)It turns out that the derivative of this function for one observation vectorxisEq.5.18(the interested reader can see Section5.8for the derivation of this equation):\u2202LCE(\u02c6y,y)\u2202wj=[s(w\u00b7x+b)\u0000y]xj(5.18)Note in Eq.5.18that the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated \u02c6y=s(w\u00b7x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute \u02c6y(i)=f(x(i);q)# What is our estimated output \u02c6y?Compute the lossL(\u02c6y(i),y(i))# How far off is \u02c6y(i))from the true outputy(i)?2.g \u2014qL(f(x(i);q),y(i))# How should we moveqto maximize loss?3.q q\u0000hg# Go the other way insteadreturnqFigure 5.5The stochastic gradient descent algorithm. Step 1 (computing the loss) is usedto report how well we are doing on the current tuple. The algorithm can terminate when itconverges (or when the gradient norm<\u270f), or when progress halts (for example when theloss starts going up on a held-out set).The learning ratehis ahyperparameterthat must be adjusted. If it\u2019s too high,hyperparameterthe learner will take steps that are too large, overshooting the minimum of the lossfunction. If it\u2019s too low, the learner will take steps that are too small, and take toolong to get to the minimum. It is common to start with a higher learning rate and thenslowly decrease it, so that it is a function of the iterationkof training; the notationhkcan be used to mean the value of the learning rate at iterationk.We\u2019ll discuss hyperparameters in more detail in Chapter 7, but brie\ufb02y they area special kind of parameter for any machine learning model. Unlike regular param-eters of a model (weights likewandb), which are learned by the algorithm fromthe training set, hyperparameters are special parameters chosen by the algorithmdesigner that affect how the algorithm works.",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 82,
      "token_count": 756,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 64\n\nHyperparametersThe learning rate \u03b7is a hyperparameter\u25e6too high: the learner will take big steps and overshoot\u25e6too low: the learner will take too longHyperparameters:\u2022Briefly, a special kind of parameter for an ML model\u2022Instead of being learned by algorithm from supervision (like regular parameters), they are chosen by algorithm designer.\n\n## Page 65\n\nLogistic RegressionStochastic Gradient Descent\n\n## Page 66\n\nLogistic RegressionStochastic Gradient Descent: An example and more details\n\n## Page 67\n\nWorking through an exampleOne step of gradient descentA mini-sentiment example, where the true y=1 (positive)Two features:x1= 3    (count of positive lexicon words) x2= 2    (count of negative lexicon words) Assume 3 parameters (2 weights and 1 bias) in \u03980are zero:w1= w2= b  = 0 \u03b7 = 0.1",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 83,
      "token_count": 208,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 68",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 84,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Example of gradient descentUpdate step for update \u03b8 is:whereGradient vector has 3 dimensions:10CHAPTER5\u2022LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt\u0000hddwf(x;w)(5.14)Now let\u2019s extend the intuition from a function of one scalar variablewto manyvariables, because we don\u2019t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If we\u2019re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.\nCost(w,b)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 85,
      "token_count": 215,
      "chapter_title": ""
    }
  },
  {
    "content": "wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially we\u2019reasking: \u201cHow much would a small change in that variablewiin\ufb02uence the total lossfunctionL?\u201dIn each dimensionwi, we express the slope as a partial derivative\u2202\u2202wiof the lossfunction. The gradient is then de\ufb01ned as a vector of these partials. We\u2019ll represent \u02c6yasf(x;q)to make the dependence onqmore obvious:\u2014qL(f(x;q),y)) =266664\u2202\u2202w1L(f(x;q),y)\u2202\u2202w2L(f(x;q),y)...\u2202\u2202wnL(f(x;q),y)377775(5.15)The \ufb01nal equation for updatingqbased on the gradient is thusqt+1=qt\u0000h\u2014L(f(x;q),y)(5.16)12CHAPTER5\u2022LOGISTICREGRESSION5.4.3 Working through an exampleLet\u2019s walk though a single step of the gradient descent algorithm. We\u2019ll use a sim-pli\ufb01ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Let\u2019s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt\u0000h\u2014qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the \ufb01rst gradient as follows:\u2014w,b=264\u2202LCE(\u02c6y,y)\u2202w1\u2202LCE(\u02c6y,y)\u2202w2\u2202LCE(\u02c6y,y)\u2202b375=24(s(w\u00b7x+b)\u0000y)x1(s(w\u00b7x+b)\u0000y)x2s(w\u00b7x+b)\u0000y35=24(s(0)\u00001)x1(s(0)\u00001)x2s(0)\u0000135=24\u00000.5x1\u00000.5x2\u00000.535=24\u00001.5\u00001.0\u00000.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 86,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so it\u2019s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,w1= w2= b  = 0;    x1= 3;   x2= 2   5.4\u2022GRADIENTDESCENT115.4.1 The Gradient for Logistic RegressionIn order to updateq, we need a de\ufb01nition for the gradient\u2014L(f(x;q),y). Recall thatfor logistic regression, the cross-entropy loss function is:LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.17)It turns out that the derivative of this function for one observation vectorxisEq.5.18(the interested reader can see Section5.8for the derivation of this equation):\u2202LCE(\u02c6y,y)\u2202wj=[s(w\u00b7x+b)\u0000y]xj(5.18)Note in Eq.5.18that the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated \u02c6y=s(w\u00b7x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute \u02c6y(i)=f(x(i);q)# What is our estimated output \u02c6y?Compute the lossL(\u02c6y(i),y(i))# How far off is \u02c6y(i))from the true outputy(i)?2.g \u2014qL(f(x(i);q),y(i))# How",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 87,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated \u02c6y=s(w\u00b7x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute \u02c6y(i)=f(x(i);q)# What is our estimated output \u02c6y?Compute the lossL(\u02c6y(i),y(i))# How far off is \u02c6y(i))from the true outputy(i)?2.g \u2014qL(f(x(i);q),y(i))# How should we moveqto maximize loss?3.q q\u0000hg# Go the other way insteadreturnqFigure 5.5The stochastic gradient descent algorithm. Step 1 (computing the loss) is usedto report how well we are doing on the current tuple. The algorithm can terminate when itconverges (or when the gradient norm<\u270f), or when progress halts (for example when theloss starts going up on a held-out set).The learning ratehis ahyperparameterthat must be adjusted. If it\u2019s too high,hyperparameterthe learner will take steps that are too large, overshooting the minimum of the lossfunction. If it\u2019s too low, the learner will take steps that are too small, and take toolong to get to the minimum. It is common to start with a higher learning rate and thenslowly decrease it, so that it is a function of the iterationkof training; the notationhkcan be used to mean the value of the learning rate at iterationk.We\u2019ll discuss hyperparameters in more detail in Chapter 7, but brie\ufb02y they area special kind of parameter for any machine learning model. Unlike regular param-eters of a model (weights likewandb), which are learned by the algorithm fromthe training set, hyperparameters are special parameters chosen by the algorithmdesigner that affect how the algorithm works.",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 88,
      "token_count": 585,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 69",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 89,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Example of gradient descentUpdate step for update \u03b8 is:whereGradient vector has 3 dimensions:10CHAPTER5\u2022LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt\u0000hddwf(x;w)(5.14)Now let\u2019s extend the intuition from a function of one scalar variablewto manyvariables, because we don\u2019t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If we\u2019re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.\nCost(w,b)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 90,
      "token_count": 215,
      "chapter_title": ""
    }
  },
  {
    "content": "wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially we\u2019reasking: \u201cHow much would a small change in that variablewiin\ufb02uence the total lossfunctionL?\u201dIn each dimensionwi, we express the slope as a partial derivative\u2202\u2202wiof the lossfunction. The gradient is then de\ufb01ned as a vector of these partials. We\u2019ll represent \u02c6yasf(x;q)to make the dependence onqmore obvious:\u2014qL(f(x;q),y)) =266664\u2202\u2202w1L(f(x;q),y)\u2202\u2202w2L(f(x;q),y)...\u2202\u2202wnL(f(x;q),y)377775(5.15)The \ufb01nal equation for updatingqbased on the gradient is thusqt+1=qt\u0000h\u2014L(f(x;q),y)(5.16)12CHAPTER5\u2022LOGISTICREGRESSION5.4.3 Working through an exampleLet\u2019s walk though a single step of the gradient descent algorithm. We\u2019ll use a sim-pli\ufb01ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Let\u2019s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt\u0000h\u2014qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the \ufb01rst gradient as follows:\u2014w,b=264\u2202LCE(\u02c6y,y)\u2202w1\u2202LCE(\u02c6y,y)\u2202w2\u2202LCE(\u02c6y,y)\u2202b375=24(s(w\u00b7x+b)\u0000y)x1(s(w\u00b7x+b)\u0000y)x2s(w\u00b7x+b)\u0000y35=24(s(0)\u00001)x1(s(0)\u00001)x2s(0)\u0000135=24\u00000.5x1\u00000.5x2\u00000.535=24\u00001.5\u00001.0\u00000.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 91,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so it\u2019s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,w1= w2= b  = 0;    x1= 3;   x2= 2   5.4\u2022GRADIENTDESCENT115.4.1 The Gradient for Logistic RegressionIn order to updateq, we need a de\ufb01nition for the gradient\u2014L(f(x;q),y). Recall thatfor logistic regression, the cross-entropy loss function is:LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.17)It turns out that the derivative of this function for one observation vectorxisEq.5.18(the interested reader can see Section5.8for the derivation of this equation):\u2202LCE(\u02c6y,y)\u2202wj=[s(w\u00b7x+b)\u0000y]xj(5.18)Note in Eq.5.18that the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated \u02c6y=s(w\u00b7x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute \u02c6y(i)=f(x(i);q)# What is our estimated output \u02c6y?Compute the lossL(\u02c6y(i),y(i))# How far off is \u02c6y(i))from the true outputy(i)?2.g \u2014qL(f(x(i);q),y(i))# How",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 92,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated \u02c6y=s(w\u00b7x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute \u02c6y(i)=f(x(i);q)# What is our estimated output \u02c6y?Compute the lossL(\u02c6y(i),y(i))# How far off is \u02c6y(i))from the true outputy(i)?2.g \u2014qL(f(x(i);q),y(i))# How should we moveqto maximize loss?3.q q\u0000hg# Go the other way insteadreturnqFigure 5.5The stochastic gradient descent algorithm. Step 1 (computing the loss) is usedto report how well we are doing on the current tuple. The algorithm can terminate when itconverges (or when the gradient norm<\u270f), or when progress halts (for example when theloss starts going up on a held-out set).The learning ratehis ahyperparameterthat must be adjusted. If it\u2019s too high,hyperparameterthe learner will take steps that are too large, overshooting the minimum of the lossfunction. If it\u2019s too low, the learner will take steps that are too small, and take toolong to get to the minimum. It is common to start with a higher learning rate and thenslowly decrease it, so that it is a function of the iterationkof training; the notationhkcan be used to mean the value of the learning rate at iterationk.We\u2019ll discuss hyperparameters in more detail in Chapter 7, but brie\ufb02y they area special kind of parameter for any machine learning model. Unlike regular param-eters of a model (weights likewandb), which are learned by the algorithm fromthe training set, hyperparameters are special parameters chosen by the algorithmdesigner that affect how the algorithm works.",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 93,
      "token_count": 585,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 70",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 94,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Example of gradient descentUpdate step for update \u03b8 is:whereGradient vector has 3 dimensions:10CHAPTER5\u2022LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt\u0000hddwf(x;w)(5.14)Now let\u2019s extend the intuition from a function of one scalar variablewto manyvariables, because we don\u2019t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If we\u2019re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.\nCost(w,b)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 95,
      "token_count": 215,
      "chapter_title": ""
    }
  },
  {
    "content": "wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially we\u2019reasking: \u201cHow much would a small change in that variablewiin\ufb02uence the total lossfunctionL?\u201dIn each dimensionwi, we express the slope as a partial derivative\u2202\u2202wiof the lossfunction. The gradient is then de\ufb01ned as a vector of these partials. We\u2019ll represent \u02c6yasf(x;q)to make the dependence onqmore obvious:\u2014qL(f(x;q),y)) =266664\u2202\u2202w1L(f(x;q),y)\u2202\u2202w2L(f(x;q),y)...\u2202\u2202wnL(f(x;q),y)377775(5.15)The \ufb01nal equation for updatingqbased on the gradient is thusqt+1=qt\u0000h\u2014L(f(x;q),y)(5.16)12CHAPTER5\u2022LOGISTICREGRESSION5.4.3 Working through an exampleLet\u2019s walk though a single step of the gradient descent algorithm. We\u2019ll use a sim-pli\ufb01ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Let\u2019s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt\u0000h\u2014qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the \ufb01rst gradient as follows:\u2014w,b=264\u2202LCE(\u02c6y,y)\u2202w1\u2202LCE(\u02c6y,y)\u2202w2\u2202LCE(\u02c6y,y)\u2202b375=24(s(w\u00b7x+b)\u0000y)x1(s(w\u00b7x+b)\u0000y)x2s(w\u00b7x+b)\u0000y35=24(s(0)\u00001)x1(s(0)\u00001)x2s(0)\u0000135=24\u00000.5x1\u00000.5x2\u00000.535=24\u00001.5\u00001.0\u00000.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 96,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so it\u2019s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,w1= w2= b  = 0;    x1= 3;   x2= 2   5.4\u2022GRADIENTDESCENT115.4.1 The Gradient for Logistic RegressionIn order to updateq, we need a de\ufb01nition for the gradient\u2014L(f(x;q),y). Recall thatfor logistic regression, the cross-entropy loss function is:LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.17)It turns out that the derivative of this function for one observation vectorxisEq.5.18(the interested reader can see Section5.8for the derivation of this equation):\u2202LCE(\u02c6y,y)\u2202wj=[s(w\u00b7x+b)\u0000y]xj(5.18)Note in Eq.5.18that the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated \u02c6y=s(w\u00b7x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute \u02c6y(i)=f(x(i);q)# What is our estimated output \u02c6y?Compute the lossL(\u02c6y(i),y(i))# How far off is \u02c6y(i))from the true outputy(i)?2.g \u2014qL(f(x(i);q),y(i))# How",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 97,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated \u02c6y=s(w\u00b7x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute \u02c6y(i)=f(x(i);q)# What is our estimated output \u02c6y?Compute the lossL(\u02c6y(i),y(i))# How far off is \u02c6y(i))from the true outputy(i)?2.g \u2014qL(f(x(i);q),y(i))# How should we moveqto maximize loss?3.q q\u0000hg# Go the other way insteadreturnqFigure 5.5The stochastic gradient descent algorithm. Step 1 (computing the loss) is usedto report how well we are doing on the current tuple. The algorithm can terminate when itconverges (or when the gradient norm<\u270f), or when progress halts (for example when theloss starts going up on a held-out set).The learning ratehis ahyperparameterthat must be adjusted. If it\u2019s too high,hyperparameterthe learner will take steps that are too large, overshooting the minimum of the lossfunction. If it\u2019s too low, the learner will take steps that are too small, and take toolong to get to the minimum. It is common to start with a higher learning rate and thenslowly decrease it, so that it is a function of the iterationkof training; the notationhkcan be used to mean the value of the learning rate at iterationk.We\u2019ll discuss hyperparameters in more detail in Chapter 7, but brie\ufb02y they area special kind of parameter for any machine learning model. Unlike regular param-eters of a model (weights likewandb), which are learned by the algorithm fromthe training set, hyperparameters are special parameters chosen by the algorithmdesigner that affect how the algorithm works.",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 98,
      "token_count": 585,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 71",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 99,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Example of gradient descentUpdate step for update \u03b8 is:whereGradient vector has 3 dimensions:10CHAPTER5\u2022LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt\u0000hddwf(x;w)(5.14)Now let\u2019s extend the intuition from a function of one scalar variablewto manyvariables, because we don\u2019t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If we\u2019re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.\nCost(w,b)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 100,
      "token_count": 215,
      "chapter_title": ""
    }
  },
  {
    "content": "wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially we\u2019reasking: \u201cHow much would a small change in that variablewiin\ufb02uence the total lossfunctionL?\u201dIn each dimensionwi, we express the slope as a partial derivative\u2202\u2202wiof the lossfunction. The gradient is then de\ufb01ned as a vector of these partials. We\u2019ll represent \u02c6yasf(x;q)to make the dependence onqmore obvious:\u2014qL(f(x;q),y)) =266664\u2202\u2202w1L(f(x;q),y)\u2202\u2202w2L(f(x;q),y)...\u2202\u2202wnL(f(x;q),y)377775(5.15)The \ufb01nal equation for updatingqbased on the gradient is thusqt+1=qt\u0000h\u2014L(f(x;q),y)(5.16)12CHAPTER5\u2022LOGISTICREGRESSION5.4.3 Working through an exampleLet\u2019s walk though a single step of the gradient descent algorithm. We\u2019ll use a sim-pli\ufb01ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Let\u2019s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt\u0000h\u2014qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the \ufb01rst gradient as follows:\u2014w,b=264\u2202LCE(\u02c6y,y)\u2202w1\u2202LCE(\u02c6y,y)\u2202w2\u2202LCE(\u02c6y,y)\u2202b375=24(s(w\u00b7x+b)\u0000y)x1(s(w\u00b7x+b)\u0000y)x2s(w\u00b7x+b)\u0000y35=24(s(0)\u00001)x1(s(0)\u00001)x2s(0)\u0000135=24\u00000.5x1\u00000.5x2\u00000.535=24\u00001.5\u00001.0\u00000.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 101,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so it\u2019s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,w1= w2= b  = 0;    x1= 3;   x2= 2   5.4\u2022GRADIENTDESCENT115.4.1 The Gradient for Logistic RegressionIn order to updateq, we need a de\ufb01nition for the gradient\u2014L(f(x;q),y). Recall thatfor logistic regression, the cross-entropy loss function is:LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.17)It turns out that the derivative of this function for one observation vectorxisEq.5.18(the interested reader can see Section5.8for the derivation of this equation):\u2202LCE(\u02c6y,y)\u2202wj=[s(w\u00b7x+b)\u0000y]xj(5.18)Note in Eq.5.18that the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated \u02c6y=s(w\u00b7x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute \u02c6y(i)=f(x(i);q)# What is our estimated output \u02c6y?Compute the lossL(\u02c6y(i),y(i))# How far off is \u02c6y(i))from the true outputy(i)?2.g \u2014qL(f(x(i);q),y(i))# How",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 102,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated \u02c6y=s(w\u00b7x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute \u02c6y(i)=f(x(i);q)# What is our estimated output \u02c6y?Compute the lossL(\u02c6y(i),y(i))# How far off is \u02c6y(i))from the true outputy(i)?2.g \u2014qL(f(x(i);q),y(i))# How should we moveqto maximize loss?3.q q\u0000hg# Go the other way insteadreturnqFigure 5.5The stochastic gradient descent algorithm. Step 1 (computing the loss) is usedto report how well we are doing on the current tuple. The algorithm can terminate when itconverges (or when the gradient norm<\u270f), or when progress halts (for example when theloss starts going up on a held-out set).The learning ratehis ahyperparameterthat must be adjusted. If it\u2019s too high,hyperparameterthe learner will take steps that are too large, overshooting the minimum of the lossfunction. If it\u2019s too low, the learner will take steps that are too small, and take toolong to get to the minimum. It is common to start with a higher learning rate and thenslowly decrease it, so that it is a function of the iterationkof training; the notationhkcan be used to mean the value of the learning rate at iterationk.We\u2019ll discuss hyperparameters in more detail in Chapter 7, but brie\ufb02y they area special kind of parameter for any machine learning model. Unlike regular param-eters of a model (weights likewandb), which are learned by the algorithm fromthe training set, hyperparameters are special parameters chosen by the algorithmdesigner that affect how the algorithm works.",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 103,
      "token_count": 585,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 72",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 104,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Example of gradient descentUpdate step for update \u03b8 is:whereGradient vector has 3 dimensions:10CHAPTER5\u2022LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt\u0000hddwf(x;w)(5.14)Now let\u2019s extend the intuition from a function of one scalar variablewto manyvariables, because we don\u2019t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If we\u2019re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.\nCost(w,b)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 105,
      "token_count": 215,
      "chapter_title": ""
    }
  },
  {
    "content": "wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially we\u2019reasking: \u201cHow much would a small change in that variablewiin\ufb02uence the total lossfunctionL?\u201dIn each dimensionwi, we express the slope as a partial derivative\u2202\u2202wiof the lossfunction. The gradient is then de\ufb01ned as a vector of these partials. We\u2019ll represent \u02c6yasf(x;q)to make the dependence onqmore obvious:\u2014qL(f(x;q),y)) =266664\u2202\u2202w1L(f(x;q),y)\u2202\u2202w2L(f(x;q),y)...\u2202\u2202wnL(f(x;q),y)377775(5.15)The \ufb01nal equation for updatingqbased on the gradient is thusqt+1=qt\u0000h\u2014L(f(x;q),y)(5.16)12CHAPTER5\u2022LOGISTICREGRESSION5.4.3 Working through an exampleLet\u2019s walk though a single step of the gradient descent algorithm. We\u2019ll use a sim-pli\ufb01ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Let\u2019s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt\u0000h\u2014qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the \ufb01rst gradient as follows:\u2014w,b=264\u2202LCE(\u02c6y,y)\u2202w1\u2202LCE(\u02c6y,y)\u2202w2\u2202LCE(\u02c6y,y)\u2202b375=24(s(w\u00b7x+b)\u0000y)x1(s(w\u00b7x+b)\u0000y)x2s(w\u00b7x+b)\u0000y35=24(s(0)\u00001)x1(s(0)\u00001)x2s(0)\u0000135=24\u00000.5x1\u00000.5x2\u00000.535=24\u00001.5\u00001.0\u00000.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 106,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so it\u2019s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,w1= w2= b  = 0;    x1= 3;   x2= 2   5.4\u2022GRADIENTDESCENT115.4.1 The Gradient for Logistic RegressionIn order to updateq, we need a de\ufb01nition for the gradient\u2014L(f(x;q),y). Recall thatfor logistic regression, the cross-entropy loss function is:LCE(\u02c6y,y)=\u0000[ylogs(w\u00b7x+b)+(1\u0000y)log(1\u0000s(w\u00b7x+b))](5.17)It turns out that the derivative of this function for one observation vectorxisEq.5.18(the interested reader can see Section5.8for the derivation of this equation):\u2202LCE(\u02c6y,y)\u2202wj=[s(w\u00b7x+b)\u0000y]xj(5.18)Note in Eq.5.18that the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated \u02c6y=s(w\u00b7x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute \u02c6y(i)=f(x(i);q)# What is our estimated output \u02c6y?Compute the lossL(\u02c6y(i),y(i))# How far off is \u02c6y(i))from the true outputy(i)?2.g \u2014qL(f(x(i);q),y(i))# How",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 107,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated \u02c6y=s(w\u00b7x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute \u02c6y(i)=f(x(i);q)# What is our estimated output \u02c6y?Compute the lossL(\u02c6y(i),y(i))# How far off is \u02c6y(i))from the true outputy(i)?2.g \u2014qL(f(x(i);q),y(i))# How should we moveqto maximize loss?3.q q\u0000hg# Go the other way insteadreturnqFigure 5.5The stochastic gradient descent algorithm. Step 1 (computing the loss) is usedto report how well we are doing on the current tuple. The algorithm can terminate when itconverges (or when the gradient norm<\u270f), or when progress halts (for example when theloss starts going up on a held-out set).The learning ratehis ahyperparameterthat must be adjusted. If it\u2019s too high,hyperparameterthe learner will take steps that are too large, overshooting the minimum of the lossfunction. If it\u2019s too low, the learner will take steps that are too small, and take toolong to get to the minimum. It is common to start with a higher learning rate and thenslowly decrease it, so that it is a function of the iterationkof training; the notationhkcan be used to mean the value of the learning rate at iterationk.We\u2019ll discuss hyperparameters in more detail in Chapter 7, but brie\ufb02y they area special kind of parameter for any machine learning model. Unlike regular param-eters of a model (weights likewandb), which are learned by the algorithm fromthe training set, hyperparameters are special parameters chosen by the algorithmdesigner that affect how the algorithm works.",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 108,
      "token_count": 585,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 73",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 109,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Example of gradient descent10CHAPTER5\u2022LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt\u0000hddwf(x;w)(5.14)Now let\u2019s extend the intuition from a function of one scalar variablewto manyvariables, because we don\u2019t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If we\u2019re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.\nCost(w,b)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 110,
      "token_count": 199,
      "chapter_title": ""
    }
  },
  {
    "content": "wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially we\u2019reasking: \u201cHow much would a small change in that variablewiin\ufb02uence the total lossfunctionL?\u201dIn each dimensionwi, we express the slope as a partial derivative\u2202\u2202wiof the lossfunction. The gradient is then de\ufb01ned as a vector of these partials. We\u2019ll represent \u02c6yasf(x;q)to make the dependence onqmore obvious:\u2014qL(f(x;q),y)) =266664\u2202\u2202w1L(f(x;q),y)\u2202\u2202w2L(f(x;q),y)...\u2202\u2202wnL(f(x;q),y)377775(5.15)The \ufb01nal equation for updatingqbased on the gradient is thusqt+1=qt\u0000h\u2014L(f(x;q),y)(5.16)12CHAPTER5\u2022LOGISTICREGRESSION5.4.3 Working through an exampleLet\u2019s walk though a single step of the gradient descent algorithm. We\u2019ll use a sim-pli\ufb01ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Let\u2019s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt\u0000h\u2014qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the \ufb01rst gradient as follows:\u2014w,b=264\u2202LCE(\u02c6y,y)\u2202w1\u2202LCE(\u02c6y,y)\u2202w2\u2202LCE(\u02c6y,y)\u2202b375=24(s(w\u00b7x+b)\u0000y)x1(s(w\u00b7x+b)\u0000y)x2s(w\u00b7x+b)\u0000y35=24(s(0)\u00001)x1(s(0)\u00001)x2s(0)\u0000135=24\u00000.5x1\u00000.5x2\u00000.535=24\u00001.5\u00001.0\u00000.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 111,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so it\u2019s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,\u03b7 = 0.1; 12CHAPTER5\u2022LOGISTICREGRESSION5.4.3 Working through an exampleLet\u2019s walk though a single step of the gradient descent algorithm. We\u2019ll use a sim-pli\ufb01ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Let\u2019s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt\u0000h\u2014qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the \ufb01rst gradient as follows:\u2014w,b=264\u2202LCE(\u02c6y,y)\u2202w1\u2202LCE(\u02c6y,y)\u2202w2\u2202LCE(\u02c6y,y)\u2202b375=24(s(w\u00b7x+b)\u0000y)x1(s(w\u00b7x+b)\u0000y)x2s(w\u00b7x+b)\u0000y35=24(s(0)\u00001)x1(s(0)\u00001)x2s(0)\u0000135=24\u00000.5x1\u00000.5x2\u00000.535=24\u00001.5\u00001.0\u00000.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 112,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the \ufb01rst gradient as follows:\u2014w,b=264\u2202LCE(\u02c6y,y)\u2202w1\u2202LCE(\u02c6y,y)\u2202w2\u2202LCE(\u02c6y,y)\u2202b375=24(s(w\u00b7x+b)\u0000y)x1(s(w\u00b7x+b)\u0000y)x2s(w\u00b7x+b)\u0000y35=24(s(0)\u00001)x1(s(0)\u00001)x2s(0)\u0000135=24\u00000.5x1\u00000.5x2\u00000.535=24\u00001.5\u00001.0\u00000.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so it\u2019s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,Now that we have a gradient, we compute the new parameter vector \u03b81by moving \u03b80in the opposite direction from the gradient:",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 113,
      "token_count": 498,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 74",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 114,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Example of gradient descent10CHAPTER5\u2022LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt\u0000hddwf(x;w)(5.14)Now let\u2019s extend the intuition from a function of one scalar variablewto manyvariables, because we don\u2019t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If we\u2019re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.\nCost(w,b)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 115,
      "token_count": 199,
      "chapter_title": ""
    }
  },
  {
    "content": "wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially we\u2019reasking: \u201cHow much would a small change in that variablewiin\ufb02uence the total lossfunctionL?\u201dIn each dimensionwi, we express the slope as a partial derivative\u2202\u2202wiof the lossfunction. The gradient is then de\ufb01ned as a vector of these partials. We\u2019ll represent \u02c6yasf(x;q)to make the dependence onqmore obvious:\u2014qL(f(x;q),y)) =266664\u2202\u2202w1L(f(x;q),y)\u2202\u2202w2L(f(x;q),y)...\u2202\u2202wnL(f(x;q),y)377775(5.15)The \ufb01nal equation for updatingqbased on the gradient is thusqt+1=qt\u0000h\u2014L(f(x;q),y)(5.16)12CHAPTER5\u2022LOGISTICREGRESSION5.4.3 Working through an exampleLet\u2019s walk though a single step of the gradient descent algorithm. We\u2019ll use a sim-pli\ufb01ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Let\u2019s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt\u0000h\u2014qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the \ufb01rst gradient as follows:\u2014w,b=264\u2202LCE(\u02c6y,y)\u2202w1\u2202LCE(\u02c6y,y)\u2202w2\u2202LCE(\u02c6y,y)\u2202b375=24(s(w\u00b7x+b)\u0000y)x1(s(w\u00b7x+b)\u0000y)x2s(w\u00b7x+b)\u0000y35=24(s(0)\u00001)x1(s(0)\u00001)x2s(0)\u0000135=24\u00000.5x1\u00000.5x2\u00000.535=24\u00001.5\u00001.0\u00000.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 116,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so it\u2019s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,\u03b7 = 0.1; 12CHAPTER5\u2022LOGISTICREGRESSION5.4.3 Working through an exampleLet\u2019s walk though a single step of the gradient descent algorithm. We\u2019ll use a sim-pli\ufb01ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Let\u2019s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt\u0000h\u2014qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the \ufb01rst gradient as follows:\u2014w,b=264\u2202LCE(\u02c6y,y)\u2202w1\u2202LCE(\u02c6y,y)\u2202w2\u2202LCE(\u02c6y,y)\u2202b375=24(s(w\u00b7x+b)\u0000y)x1(s(w\u00b7x+b)\u0000y)x2s(w\u00b7x+b)\u0000y35=24(s(0)\u00001)x1(s(0)\u00001)x2s(0)\u0000135=24\u00000.5x1\u00000.5x2\u00000.535=24\u00001.5\u00001.0\u00000.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 117,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the \ufb01rst gradient as follows:\u2014w,b=264\u2202LCE(\u02c6y,y)\u2202w1\u2202LCE(\u02c6y,y)\u2202w2\u2202LCE(\u02c6y,y)\u2202b375=24(s(w\u00b7x+b)\u0000y)x1(s(w\u00b7x+b)\u0000y)x2s(w\u00b7x+b)\u0000y35=24(s(0)\u00001)x1(s(0)\u00001)x2s(0)\u0000135=24\u00000.5x1\u00000.5x2\u00000.535=24\u00001.5\u00001.0\u00000.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so it\u2019s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,Now that we have a gradient, we compute the new parameter vector \u03b81by moving \u03b80in the opposite direction from the gradient:",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 118,
      "token_count": 498,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 75",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 119,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Example of gradient descent10CHAPTER5\u2022LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt\u0000hddwf(x;w)(5.14)Now let\u2019s extend the intuition from a function of one scalar variablewto manyvariables, because we don\u2019t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If we\u2019re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.\nCost(w,b)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 120,
      "token_count": 199,
      "chapter_title": ""
    }
  },
  {
    "content": "wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially we\u2019reasking: \u201cHow much would a small change in that variablewiin\ufb02uence the total lossfunctionL?\u201dIn each dimensionwi, we express the slope as a partial derivative\u2202\u2202wiof the lossfunction. The gradient is then de\ufb01ned as a vector of these partials. We\u2019ll represent \u02c6yasf(x;q)to make the dependence onqmore obvious:\u2014qL(f(x;q),y)) =266664\u2202\u2202w1L(f(x;q),y)\u2202\u2202w2L(f(x;q),y)...\u2202\u2202wnL(f(x;q),y)377775(5.15)The \ufb01nal equation for updatingqbased on the gradient is thusqt+1=qt\u0000h\u2014L(f(x;q),y)(5.16)12CHAPTER5\u2022LOGISTICREGRESSION5.4.3 Working through an exampleLet\u2019s walk though a single step of the gradient descent algorithm. We\u2019ll use a sim-pli\ufb01ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Let\u2019s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt\u0000h\u2014qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the \ufb01rst gradient as follows:\u2014w,b=264\u2202LCE(\u02c6y,y)\u2202w1\u2202LCE(\u02c6y,y)\u2202w2\u2202LCE(\u02c6y,y)\u2202b375=24(s(w\u00b7x+b)\u0000y)x1(s(w\u00b7x+b)\u0000y)x2s(w\u00b7x+b)\u0000y35=24(s(0)\u00001)x1(s(0)\u00001)x2s(0)\u0000135=24\u00000.5x1\u00000.5x2\u00000.535=24\u00001.5\u00001.0\u00000.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 121,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so it\u2019s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,\u03b7 = 0.1; 12CHAPTER5\u2022LOGISTICREGRESSION5.4.3 Working through an exampleLet\u2019s walk though a single step of the gradient descent algorithm. We\u2019ll use a sim-pli\ufb01ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Let\u2019s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt\u0000h\u2014qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the \ufb01rst gradient as follows:\u2014w,b=264\u2202LCE(\u02c6y,y)\u2202w1\u2202LCE(\u02c6y,y)\u2202w2\u2202LCE(\u02c6y,y)\u2202b375=24(s(w\u00b7x+b)\u0000y)x1(s(w\u00b7x+b)\u0000y)x2s(w\u00b7x+b)\u0000y35=24(s(0)\u00001)x1(s(0)\u00001)x2s(0)\u0000135=24\u00000.5x1\u00000.5x2\u00000.535=24\u00001.5\u00001.0\u00000.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 122,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the \ufb01rst gradient as follows:\u2014w,b=264\u2202LCE(\u02c6y,y)\u2202w1\u2202LCE(\u02c6y,y)\u2202w2\u2202LCE(\u02c6y,y)\u2202b375=24(s(w\u00b7x+b)\u0000y)x1(s(w\u00b7x+b)\u0000y)x2s(w\u00b7x+b)\u0000y35=24(s(0)\u00001)x1(s(0)\u00001)x2s(0)\u0000135=24\u00000.5x1\u00000.5x2\u00000.535=24\u00001.5\u00001.0\u00000.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so it\u2019s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,Now that we have a gradient, we compute the new parameter vector \u03b81by moving \u03b80in the opposite direction from the gradient:",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 123,
      "token_count": 498,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 76",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 124,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Example of gradient descent10CHAPTER5\u2022LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt\u0000hddwf(x;w)(5.14)Now let\u2019s extend the intuition from a function of one scalar variablewto manyvariables, because we don\u2019t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If we\u2019re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.\nCost(w,b)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 125,
      "token_count": 199,
      "chapter_title": ""
    }
  },
  {
    "content": "wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially we\u2019reasking: \u201cHow much would a small change in that variablewiin\ufb02uence the total lossfunctionL?\u201dIn each dimensionwi, we express the slope as a partial derivative\u2202\u2202wiof the lossfunction. The gradient is then de\ufb01ned as a vector of these partials. We\u2019ll represent \u02c6yasf(x;q)to make the dependence onqmore obvious:\u2014qL(f(x;q),y)) =266664\u2202\u2202w1L(f(x;q),y)\u2202\u2202w2L(f(x;q),y)...\u2202\u2202wnL(f(x;q),y)377775(5.15)The \ufb01nal equation for updatingqbased on the gradient is thusqt+1=qt\u0000h\u2014L(f(x;q),y)(5.16)12CHAPTER5\u2022LOGISTICREGRESSION5.4.3 Working through an exampleLet\u2019s walk though a single step of the gradient descent algorithm. We\u2019ll use a sim-pli\ufb01ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Let\u2019s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt\u0000h\u2014qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the \ufb01rst gradient as follows:\u2014w,b=264\u2202LCE(\u02c6y,y)\u2202w1\u2202LCE(\u02c6y,y)\u2202w2\u2202LCE(\u02c6y,y)\u2202b375=24(s(w\u00b7x+b)\u0000y)x1(s(w\u00b7x+b)\u0000y)x2s(w\u00b7x+b)\u0000y35=24(s(0)\u00001)x1(s(0)\u00001)x2s(0)\u0000135=24\u00000.5x1\u00000.5x2\u00000.535=24\u00001.5\u00001.0\u00000.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 126,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so it\u2019s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,\u03b7 = 0.1; 12CHAPTER5\u2022LOGISTICREGRESSION5.4.3 Working through an exampleLet\u2019s walk though a single step of the gradient descent algorithm. We\u2019ll use a sim-pli\ufb01ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Let\u2019s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt\u0000h\u2014qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the \ufb01rst gradient as follows:\u2014w,b=264\u2202LCE(\u02c6y,y)\u2202w1\u2202LCE(\u02c6y,y)\u2202w2\u2202LCE(\u02c6y,y)\u2202b375=24(s(w\u00b7x+b)\u0000y)x1(s(w\u00b7x+b)\u0000y)x2s(w\u00b7x+b)\u0000y35=24(s(0)\u00001)x1(s(0)\u00001)x2s(0)\u0000135=24\u00000.5x1\u00000.5x2\u00000.535=24\u00001.5\u00001.0\u00000.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 127,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the \ufb01rst gradient as follows:\u2014w,b=264\u2202LCE(\u02c6y,y)\u2202w1\u2202LCE(\u02c6y,y)\u2202w2\u2202LCE(\u02c6y,y)\u2202b375=24(s(w\u00b7x+b)\u0000y)x1(s(w\u00b7x+b)\u0000y)x2s(w\u00b7x+b)\u0000y35=24(s(0)\u00001)x1(s(0)\u00001)x2s(0)\u0000135=24\u00000.5x1\u00000.5x2\u00000.535=24\u00001.5\u00001.0\u00000.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35\u0000h24\u00001.5\u00001.0\u00000.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so it\u2019s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,Now that we have a gradient, we compute the new parameter vector \u03b81by moving \u03b80in the opposite direction from the gradient:",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 128,
      "token_count": 498,
      "chapter_title": ""
    }
  },
  {
    "content": "Note that enough negative examples would eventually make w2negative",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 129,
      "token_count": 11,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 77\n\nMini-batch trainingStochastic gradient descent chooses a single random example at a time.That can result in choppy movementsMore common to compute gradient over batches of training instances.Batch training: entire datasetMini-batch training: mexamples (512, or 1024)\n\n## Page 78\n\nLogistic RegressionStochastic Gradient Descent: An example and more details\n\n## Page 79\n\nLogistic RegressionRegularization\n\n## Page 80\n\nOverfittingA model that perfectly match the training data has a problem.It will also overfitto the data, modeling noise \u25e6A random word that perfectly predicts y(it happens to only occur in one class) will get a very high weight. \u25e6Failing to generalize to a test set without this word. A good model should be able to generalize\n\n## Page 81\n\nOverfittingThis movie drew me in, and it'll do the same to you.81X1 = \"this\"X2 = \"movieX3 = \"hated\"I can't tell you how much I hated this movie. It sucked.X5 = \"the same to you\"X7 = \"tell you how much\"X4 = \"drew me in\"+-Useful or harmless features4gram features that just \"memorize\" training set and might cause problems\n\n## Page 82\n\nOverfitting4-gram model on tiny data will just memorize the data\u25e6100% accuracy on the training setBut it will be surprised by the novel 4-grams in the test data\u25e6Low accuracy on test setModels that are too powerful can overfitthe data\u25e6Fitting the details of the training data so exactly that the model doesn't generalize well to the test set\u25e6How to avoid overfitting?\u25e6Regularization in logistic regression \u25e6Dropout in neural networks82",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 130,
      "token_count": 382,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 83\n\nRegularizationA solution for overfittingAdd a regularization term R(\u03b8) to the loss function (for now written as maximizing logprobrather than minimizing loss) Idea: choose an R(\u03b8)that penalizes large weights\u25e6fitting the data well with lots of big weights not as good as fitting the data a little less well, with small weights14CHAPTER5\u2022LOGISTICREGRESSIONdata to the unseen test set, but a model that over\ufb01ts will have poor generalization.To avoid over\ufb01tting, a newregularizationtermR(q)is added to the objectiveregularizationfunction in Eq.5.13, resulting in the following objective for a batch ofmexam-ples (slightly rewritten from Eq.5.13to be maximizing log probability rather thanminimizing loss, and removing the1mterm which doesn\u2019t affect the argmax):\u02c6q=argmaxqmXi=1logP(y(i)|x(i))\u0000aR(q)(5.22)The new regularization termR(q)is used to penalize large weights. Thus a settingof the weights that matches the training data perfectly\u2014 but uses many weights withhigh values to do so\u2014will be penalized more than a setting that matches the data alittle less well, but does so using smaller weights. There are two common ways tocompute this regularization termR(q).L2 regularizationis a quadratic function ofL2regularizationthe weight values, named because it uses the (square of the) L2 norm of the weightvalues. The L2 norm,||q||2, is the same as theEuclidean distanceof the vectorqfrom the origin. Ifqconsists ofnweights, then:R(q)=||q||22=nXj=1q2j(5.23)The L2 regularized objective function becomes:\u02c6q=argmaxq\"mXi=1logP(y(i)|x(i))#\u0000anXj=1q2j(5.24)L1 regularizationis a linear function of the weight values, named after the L1 normL1regularization||W||1, the sum of the absolute values of the weights, orManhattan distance(theManhattan distance is the distance you\u2019d have to walk between two points in a citywith a street grid like New York):R(q)=||q||1=nXi=1|qi|(5.25)The L1 regularized objective function becomes:\u02c6q=argmaxq\"mX1=ilogP(y(i)|x(i))#\u0000anXj=1|qj|(5.26)These kinds of regularization come from statistics, where L1 regularization is calledlasso regression(Tibshirani, 1996)and L2 regularization is calledridge regression,lassoridgeand both are commonly used in language processing. L2 regularization is easier tooptimize because of its simple derivative (the derivative ofq2is just 2q), whileL1 regularization is more complex (the derivative of|q|is non-continuous at zero).But where L2 prefers weight vectors with many small weights, L1 prefers sparsesolutions with some larger weights but many more weights set to zero. Thus L1regularization leads to much sparser weight vectors, that is, far fewer features.Both L1 and L2 regularization have Bayesian interpretations as constraints onthe prior of how weights should look. L1 regularization can be viewed as a Laplaceprior on the weights. L2 regularization corresponds to assuming that weights are",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 131,
      "token_count": 744,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 84",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 132,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "L2 Regularization (= ridge regression)The sum of the squares of the weightsThe name is because this is the (square of the)        L2 norm||\u03b8||2, = Euclidean distance of \u03b8 to the origin.L2 regularized objective function:14CHAPTER5\u2022LOGISTICREGRESSIONdata to the unseen test set, but a model that over\ufb01ts will have poor generalization.To avoid over\ufb01tting, a newregularizationtermR(q)is added to the objectiveregularizationfunction in Eq.5.13, resulting in the following objective for a batch ofmexam-ples (slightly rewritten from Eq.5.13to be maximizing log probability rather thanminimizing loss, and removing the1mterm which doesn\u2019t affect the argmax):\u02c6q=argmaxqmXi=1logP(y(i)|x(i))\u0000aR(q)(5.22)The new regularization termR(q)is used to penalize large weights. Thus a settingof the weights that matches the training data perfectly\u2014 but uses many weights withhigh values to do so\u2014will be penalized more than a setting that matches the data alittle less well, but does so using smaller weights. There are two common ways tocompute this regularization termR(q).L2 regularizationis a quadratic function ofL2regularizationthe weight values, named because it uses the (square of the) L2 norm of the weightvalues. The L2 norm,||q||2, is the same as theEuclidean distanceof the vectorqfrom the origin. Ifqconsists ofnweights, then:R(q)=||q||22=nXj=1q2j(5.23)The L2 regularized objective function becomes:\u02c6q=argmaxq\"mXi=1logP(y(i)|x(i))#\u0000anXj=1q2j(5.24)L1 regularizationis a linear function of the weight values, named after the L1 normL1regularization||W||1, the sum of the absolute values of the weights, orManhattan distance(theManhattan distance is the distance you\u2019d have to walk between two points in a citywith a street grid like New York):R(q)=||q||1=nXi=1|qi|(5.25)The L1 regularized objective function becomes:\u02c6q=argmaxq\"mX1=ilogP(y(i)|x(i))#\u0000anXj=1|qj|(5.26)These kinds of regularization come from statistics, where L1 regularization is calledlasso regression(Tibshirani, 1996)and L2 regularization is calledridge regression,lassoridgeand both are commonly used in language processing. L2 regularization is easier tooptimize because of its simple derivative (the derivative ofq2is just 2q), whileL1 regularization is more complex (the derivative of|q|is non-continuous at zero).But where L2 prefers weight vectors with many small weights, L1 prefers sparsesolutions with some larger weights but many more weights set to zero. Thus L1regularization leads to much sparser weight vectors, that is, far fewer features.Both L1 and L2 regularization have Bayesian interpretations as constraints onthe prior of how weights should look. L1 regularization can be viewed as a Laplaceprior on the weights. L2 regularization corresponds to assuming that weights are14CHAPTER5\u2022LOGISTICREGRESSIONdata to the unseen test set, but a model that over\ufb01ts will have poor generalization.To avoid over\ufb01tting, a newregularizationtermR(q)is added to the objectiveregularizationfunction in Eq.5.13, resulting in the following objective for a batch ofmexam-ples",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 133,
      "token_count": 793,
      "chapter_title": ""
    }
  },
  {
    "content": "kinds of regularization come from statistics, where L1 regularization is calledlasso regression(Tibshirani, 1996)and L2 regularization is calledridge regression,lassoridgeand both are commonly used in language processing. L2 regularization is easier tooptimize because of its simple derivative (the derivative ofq2is just 2q), whileL1 regularization is more complex (the derivative of|q|is non-continuous at zero).But where L2 prefers weight vectors with many small weights, L1 prefers sparsesolutions with some larger weights but many more weights set to zero. Thus L1regularization leads to much sparser weight vectors, that is, far fewer features.Both L1 and L2 regularization have Bayesian interpretations as constraints onthe prior of how weights should look. L1 regularization can be viewed as a Laplaceprior on the weights. L2 regularization corresponds to assuming that weights are14CHAPTER5\u2022LOGISTICREGRESSIONdata to the unseen test set, but a model that over\ufb01ts will have poor generalization.To avoid over\ufb01tting, a newregularizationtermR(q)is added to the objectiveregularizationfunction in Eq.5.13, resulting in the following objective for a batch ofmexam-ples (slightly rewritten from Eq.5.13to be maximizing log probability rather thanminimizing loss, and removing the1mterm which doesn\u2019t affect the argmax):\u02c6q=argmaxqmXi=1logP(y(i)|x(i))\u0000aR(q)(5.22)The new regularization termR(q)is used to penalize large weights. Thus a settingof the weights that matches the training data perfectly\u2014 but uses many weights withhigh values to do so\u2014will be penalized more than a setting that matches the data alittle less well, but does so using smaller weights. There are two common ways tocompute this regularization termR(q).L2 regularizationis a quadratic function ofL2regularizationthe weight values, named because it uses the (square of the) L2 norm of the weightvalues. The L2 norm,||q||2, is the same as theEuclidean distanceof the vectorqfrom the origin. Ifqconsists ofnweights, then:R(q)=||q||22=nXj=1q2j(5.23)The L2 regularized objective function becomes:\u02c6q=argmaxq\"mXi=1logP(y(i)|x(i))#\u0000anXj=1q2j(5.24)L1 regularizationis a linear function of the weight values, named after the L1 normL1regularization||W||1, the sum of the absolute values of the weights, orManhattan distance(theManhattan distance is the distance you\u2019d have to walk between two points in a citywith a street grid like New York):R(q)=||q||1=nXi=1|qi|(5.25)The L1 regularized objective function becomes:\u02c6q=argmaxq\"mX1=ilogP(y(i)|x(i))#\u0000anXj=1|qj|(5.26)These kinds of regularization come from statistics, where L1 regularization is calledlasso regression(Tibshirani, 1996)and L2 regularization is calledridge regression,lassoridgeand both are commonly used in language processing. L2 regularization is easier tooptimize because of its simple derivative (the derivative ofq2is just 2q), whileL1 regularization is more complex (the derivative of|q|is non-continuous at zero).But where L2 prefers weight vectors with many small weights, L1 prefers sparsesolutions with some larger weights but many more weights set to zero. Thus L1regularization leads to much sparser weight",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 134,
      "token_count": 801,
      "chapter_title": ""
    }
  },
  {
    "content": "regularizationis a linear function of the weight values, named after the L1 normL1regularization||W||1, the sum of the absolute values of the weights, orManhattan distance(theManhattan distance is the distance you\u2019d have to walk between two points in a citywith a street grid like New York):R(q)=||q||1=nXi=1|qi|(5.25)The L1 regularized objective function becomes:\u02c6q=argmaxq\"mX1=ilogP(y(i)|x(i))#\u0000anXj=1|qj|(5.26)These kinds of regularization come from statistics, where L1 regularization is calledlasso regression(Tibshirani, 1996)and L2 regularization is calledridge regression,lassoridgeand both are commonly used in language processing. L2 regularization is easier tooptimize because of its simple derivative (the derivative ofq2is just 2q), whileL1 regularization is more complex (the derivative of|q|is non-continuous at zero).But where L2 prefers weight vectors with many small weights, L1 prefers sparsesolutions with some larger weights but many more weights set to zero. Thus L1regularization leads to much sparser weight vectors, that is, far fewer features.Both L1 and L2 regularization have Bayesian interpretations as constraints onthe prior of how weights should look. L1 regularization can be viewed as a Laplaceprior on the weights. L2 regularization corresponds to assuming that weights are",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 135,
      "token_count": 318,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 85",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 136,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "L1 Regularization (= lasso regression)The sum of the (absolute value of the) weightsNamed after the L1 norm ||W||1, = sum of the absolute values of the weights, = Manhattan distanceL1 regularized objective function:14CHAPTER5\u2022LOGISTICREGRESSIONdata to the unseen test set, but a model that over\ufb01ts will have poor generalization.To avoid over\ufb01tting, a newregularizationtermR(q)is added to the objectiveregularizationfunction in Eq.5.13, resulting in the following objective for a batch ofmexam-ples (slightly rewritten from Eq.5.13to be maximizing log probability rather thanminimizing loss, and removing the1mterm which doesn\u2019t affect the argmax):\u02c6q=argmaxqmXi=1logP(y(i)|x(i))\u0000aR(q)(5.22)The new regularization termR(q)is used to penalize large weights. Thus a settingof the weights that matches the training data perfectly\u2014 but uses many weights withhigh values to do so\u2014will be penalized more than a setting that matches the data alittle less well, but does so using smaller weights. There are two common ways tocompute this regularization termR(q).L2 regularizationis a quadratic function ofL2regularizationthe weight values, named because it uses the (square of the) L2 norm of the weightvalues. The L2 norm,||q||2, is the same as theEuclidean distanceof the vectorqfrom the origin. Ifqconsists ofnweights, then:R(q)=||q||22=nXj=1q2j(5.23)The L2 regularized objective function becomes:\u02c6q=argmaxq\"mXi=1logP(y(i)|x(i))#\u0000anXj=1q2j(5.24)L1 regularizationis a linear function of the weight values, named after the L1 normL1regularization||W||1, the sum of the absolute values of the weights, orManhattan distance(theManhattan distance is the distance you\u2019d have to walk between two points in a citywith a street grid like New York):R(q)=||q||1=nXi=1|qi|(5.25)The L1 regularized objective function becomes:\u02c6q=argmaxq\"mX1=ilogP(y(i)|x(i))#\u0000anXj=1|qj|(5.26)These kinds of regularization come from statistics, where L1 regularization is calledlasso regression(Tibshirani, 1996)and L2 regularization is calledridge regression,lassoridgeand both are commonly used in language processing. L2 regularization is easier tooptimize because of its simple derivative (the derivative ofq2is just 2q), whileL1 regularization is more complex (the derivative of|q|is non-continuous at zero).But where L2 prefers weight vectors with many small weights, L1 prefers sparsesolutions with some larger weights but many more weights set to zero. Thus L1regularization leads to much sparser weight vectors, that is, far fewer features.Both L1 and L2 regularization have Bayesian interpretations as constraints onthe prior of how weights should look. L1 regularization can be viewed as a Laplaceprior on the weights. L2 regularization corresponds to assuming that weights are14CHAPTER5\u2022LOGISTICREGRESSIONdata to the unseen test set, but a model that over\ufb01ts will have poor generalization.To avoid over\ufb01tting, a newregularizationtermR(q)is added to the objectiveregularizationfunction in Eq.5.13, resulting in the following objective for a batch ofmexam-ples (slightly rewritten from",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 137,
      "token_count": 795,
      "chapter_title": ""
    }
  },
  {
    "content": "kinds of regularization come from statistics, where L1 regularization is calledlasso regression(Tibshirani, 1996)and L2 regularization is calledridge regression,lassoridgeand both are commonly used in language processing. L2 regularization is easier tooptimize because of its simple derivative (the derivative ofq2is just 2q), whileL1 regularization is more complex (the derivative of|q|is non-continuous at zero).But where L2 prefers weight vectors with many small weights, L1 prefers sparsesolutions with some larger weights but many more weights set to zero. Thus L1regularization leads to much sparser weight vectors, that is, far fewer features.Both L1 and L2 regularization have Bayesian interpretations as constraints onthe prior of how weights should look. L1 regularization can be viewed as a Laplaceprior on the weights. L2 regularization corresponds to assuming that weights are14CHAPTER5\u2022LOGISTICREGRESSIONdata to the unseen test set, but a model that over\ufb01ts will have poor generalization.To avoid over\ufb01tting, a newregularizationtermR(q)is added to the objectiveregularizationfunction in Eq.5.13, resulting in the following objective for a batch ofmexam-ples (slightly rewritten from Eq.5.13to be maximizing log probability rather thanminimizing loss, and removing the1mterm which doesn\u2019t affect the argmax):\u02c6q=argmaxqmXi=1logP(y(i)|x(i))\u0000aR(q)(5.22)The new regularization termR(q)is used to penalize large weights. Thus a settingof the weights that matches the training data perfectly\u2014 but uses many weights withhigh values to do so\u2014will be penalized more than a setting that matches the data alittle less well, but does so using smaller weights. There are two common ways tocompute this regularization termR(q).L2 regularizationis a quadratic function ofL2regularizationthe weight values, named because it uses the (square of the) L2 norm of the weightvalues. The L2 norm,||q||2, is the same as theEuclidean distanceof the vectorqfrom the origin. Ifqconsists ofnweights, then:R(q)=||q||22=nXj=1q2j(5.23)The L2 regularized objective function becomes:\u02c6q=argmaxq\"mXi=1logP(y(i)|x(i))#\u0000anXj=1q2j(5.24)L1 regularizationis a linear function of the weight values, named after the L1 normL1regularization||W||1, the sum of the absolute values of the weights, orManhattan distance(theManhattan distance is the distance you\u2019d have to walk between two points in a citywith a street grid like New York):R(q)=||q||1=nXi=1|qi|(5.25)The L1 regularized objective function becomes:\u02c6q=argmaxq\"mX1=ilogP(y(i)|x(i))#\u0000anXj=1|qj|(5.26)These kinds of regularization come from statistics, where L1 regularization is calledlasso regression(Tibshirani, 1996)and L2 regularization is calledridge regression,lassoridgeand both are commonly used in language processing. L2 regularization is easier tooptimize because of its simple derivative (the derivative ofq2is just 2q), whileL1 regularization is more complex (the derivative of|q|is non-continuous at zero).But where L2 prefers weight vectors with many small weights, L1 prefers sparsesolutions with some larger weights but many more weights set to zero. Thus L1regularization leads to much sparser weight",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 138,
      "token_count": 801,
      "chapter_title": ""
    }
  },
  {
    "content": "regularizationis a linear function of the weight values, named after the L1 normL1regularization||W||1, the sum of the absolute values of the weights, orManhattan distance(theManhattan distance is the distance you\u2019d have to walk between two points in a citywith a street grid like New York):R(q)=||q||1=nXi=1|qi|(5.25)The L1 regularized objective function becomes:\u02c6q=argmaxq\"mX1=ilogP(y(i)|x(i))#\u0000anXj=1|qj|(5.26)These kinds of regularization come from statistics, where L1 regularization is calledlasso regression(Tibshirani, 1996)and L2 regularization is calledridge regression,lassoridgeand both are commonly used in language processing. L2 regularization is easier tooptimize because of its simple derivative (the derivative ofq2is just 2q), whileL1 regularization is more complex (the derivative of|q|is non-continuous at zero).But where L2 prefers weight vectors with many small weights, L1 prefers sparsesolutions with some larger weights but many more weights set to zero. Thus L1regularization leads to much sparser weight vectors, that is, far fewer features.Both L1 and L2 regularization have Bayesian interpretations as constraints onthe prior of how weights should look. L1 regularization can be viewed as a Laplaceprior on the weights. L2 regularization corresponds to assuming that weights are",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 139,
      "token_count": 318,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 86\n\nLogistic RegressionRegularization\n\n## Page 87\n\nLogistic RegressionMultinomial Logistic Regression\n\n## Page 88\n\nMultinomial Logistic RegressionOften we need more than 2 classes\u25e6Positive/negative/neutral\u25e6Parts of speech (noun, verb, adjective, adverb, preposition, etc.)\u25e6Classify emergency SMSs into different actionable classesIf >2 classes we use multinomial logistic regression= Softmaxregression= Multinomial logit= (defunct names : Maximum entropy modeling or MaxEntSo \"logistic regression\" will just mean binary (2 output classes)88\n\n## Page 89\n\nMultinomial Logistic RegressionThe probability of everything must still sum to 1P(positive|doc) + P(negative|doc) + P(neutral|doc) = 1Need a generalization of the sigmoid called the softmax\u25e6Takes a vector z = [z1, z2, ..., zk] of k arbitrary values \u25e6Outputs a probability distribution\u25e6each value in the range [0,1]\u25e6all the values summing to 189",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 140,
      "token_count": 234,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 90",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 141,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "The softmaxfunctionTurns a vector z = [z1, z2, ... , zk]of k arbitrary values into probabilities",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 142,
      "token_count": 26,
      "chapter_title": ""
    }
  },
  {
    "content": "905.6\u2022MULTINOMIAL LOGISTIC REGRESSION15distributed according to a Gaussian distribution with mean\u00b5=0. In a Gaussianor normal distribution, the further away a value is from the mean, the lower itsprobability (scaled by the variances). By using a Gaussian prior on the weights, weare saying that weights prefer to have the value 0. A Gaussian for a weightqjis1q2ps2jexp \u0000(qj\u0000\u00b5j)22s2j!(5.27)If we multiply each weight by a Gaussian prior on the weight, we are thus maximiz-ing the following constraint:\u02c6q=argmaxqMYi=1P(y(i)|x(i))\u21e5nYj=11q2ps2jexp \u0000(qj\u0000\u00b5j)22s2j!(5.28)which in log space, with\u00b5=0, and assuming 2s2=1, corresponds to\u02c6q=argmaxqmXi=1logP(y(i)|x(i))\u0000anXj=1q2j(5.29)which is in the same form as Eq.5.24.5.6 Multinomial logistic regressionSometimes we need more than two classes. Perhaps we might want to do 3-waysentiment classi\ufb01cation (positive, negative, or neutral). Or we could be assigningsome of the labels we will introduce in Chapter 8, like the part of speech of a word(choosing from 10, 30, or even 50 different parts of speech), or the named entitytype of a phrase (choosing from tags like person, location, organization).In such cases we usemultinomial logistic regression, also calledsoftmax re-multinomiallogisticregressiongression(or, historically, themaxent classi\ufb01er). In multinomial logistic regressionthe targetyis a variable that ranges over more than two classes; we want to knowthe probability ofybeing in each potential classc2C,p(y=c|x).The multinomial logistic classi\ufb01er uses a generalization of the sigmoid, calledthesoftmaxfunction, to compute the probabilityp(y=c|x). The softmax functionsoftmaxtakes a vectorz=[z1,z2,. . . ,zk]ofkarbitrary values and maps them to a probabilitydistribution, with each value in the range (0,1), and all the values summing to 1.Like the sigmoid, it is an exponential function.For a vectorzof dimensionalityk, the softmax is de\ufb01ned as:softmax(zi)=exp(zi)Pkj=1exp(zj)1\uf8ffi\uf8ffk(5.30)The softmax of an input vectorz=[z1,z2,. . . ,zk]is thus a vector itself:softmax(z)=\"exp(z1)Pki=1exp(zi),exp(z2)Pki=1exp(zi),. . . ,exp(zk)Pki=1exp(zi)#(5.31)5.6\u2022MULTINOMIAL LOGISTIC REGRESSION15distributed according to a Gaussian distribution with mean\u00b5=0. In a Gaussianor normal distribution, the further away a value is from the mean, the lower itsprobability (scaled by the variances). By using a Gaussian prior on the weights, weare saying that weights prefer to have the value 0. A Gaussian for a weightqjis1q2ps2jexp \u0000(qj\u0000\u00b5j)22s2j!(5.27)If we multiply each weight by a Gaussian prior on the weight, we are thus maximiz-ing the following",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 143,
      "token_count": 769,
      "chapter_title": ""
    }
  },
  {
    "content": ". . ,zk]ofkarbitrary values and maps them to a probabilitydistribution, with each value in the range (0,1), and all the values summing to 1.Like the sigmoid, it is an exponential function.For a vectorzof dimensionalityk, the softmax is de\ufb01ned as:softmax(zi)=exp(zi)Pkj=1exp(zj)1\uf8ffi\uf8ffk(5.30)The softmax of an input vectorz=[z1,z2,. . . ,zk]is thus a vector itself:softmax(z)=\"exp(z1)Pki=1exp(zi),exp(z2)Pki=1exp(zi),. . . ,exp(zk)Pki=1exp(zi)#(5.31)5.6\u2022MULTINOMIAL LOGISTIC REGRESSION15distributed according to a Gaussian distribution with mean\u00b5=0. In a Gaussianor normal distribution, the further away a value is from the mean, the lower itsprobability (scaled by the variances). By using a Gaussian prior on the weights, weare saying that weights prefer to have the value 0. A Gaussian for a weightqjis1q2ps2jexp \u0000(qj\u0000\u00b5j)22s2j!(5.27)If we multiply each weight by a Gaussian prior on the weight, we are thus maximiz-ing the following constraint:\u02c6q=argmaxqMYi=1P(y(i)|x(i))\u21e5nYj=11q2ps2jexp \u0000(qj\u0000\u00b5j)22s2j!(5.28)which in log space, with\u00b5=0, and assuming 2s2=1, corresponds to\u02c6q=argmaxqmXi=1logP(y(i)|x(i))\u0000anXj=1q2j(5.29)which is in the same form as Eq.5.24.5.6 Multinomial logistic regressionSometimes we need more than two classes. Perhaps we might want to do 3-waysentiment classi\ufb01cation (positive, negative, or neutral). Or we could be assigningsome of the labels we will introduce in Chapter 8, like the part of speech of a word(choosing from 10, 30, or even 50 different parts of speech), or the named entitytype of a phrase (choosing from tags like person, location, organization).In such cases we usemultinomial logistic regression, also calledsoftmax re-multinomiallogisticregressiongression(or, historically, themaxent classi\ufb01er). In multinomial logistic regressionthe targetyis a variable that ranges over more than two classes; we want to knowthe probability ofybeing in each potential classc2C,p(y=c|x).The multinomial logistic classi\ufb01er uses a generalization of the sigmoid, calledthesoftmaxfunction, to compute the probabilityp(y=c|x). The softmax functionsoftmaxtakes a vectorz=[z1,z2,. . . ,zk]ofkarbitrary values and maps them to a probabilitydistribution, with each value in the range (0,1), and all the values summing to 1.Like the sigmoid, it is an exponential function.For a vectorzof dimensionalityk, the softmax is de\ufb01ned as:softmax(zi)=exp(zi)Pkj=1exp(zj)1\uf8ffi\uf8ffk(5.30)The softmax of an input vectorz=[z1,z2,. . . ,zk]is thus a vector itself:softmax(z)=\"exp(z1)Pki=1exp(zi),exp(z2)Pki=1exp(zi),. . .",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 144,
      "token_count": 790,
      "chapter_title": ""
    }
  },
  {
    "content": "or even 50 different parts of speech), or the named entitytype of a phrase (choosing from tags like person, location, organization).In such cases we usemultinomial logistic regression, also calledsoftmax re-multinomiallogisticregressiongression(or, historically, themaxent classi\ufb01er). In multinomial logistic regressionthe targetyis a variable that ranges over more than two classes; we want to knowthe probability ofybeing in each potential classc2C,p(y=c|x).The multinomial logistic classi\ufb01er uses a generalization of the sigmoid, calledthesoftmaxfunction, to compute the probabilityp(y=c|x). The softmax functionsoftmaxtakes a vectorz=[z1,z2,. . . ,zk]ofkarbitrary values and maps them to a probabilitydistribution, with each value in the range (0,1), and all the values summing to 1.Like the sigmoid, it is an exponential function.For a vectorzof dimensionalityk, the softmax is de\ufb01ned as:softmax(zi)=exp(zi)Pkj=1exp(zj)1\uf8ffi\uf8ffk(5.30)The softmax of an input vectorz=[z1,z2,. . . ,zk]is thus a vector itself:softmax(z)=\"exp(z1)Pki=1exp(zi),exp(z2)Pki=1exp(zi),. . . ,exp(zk)Pki=1exp(zi)#(5.31)5.6\u2022MULTINOMIAL LOGISTIC REGRESSION15For a vectorzof dimensionalityk, the softmax is de\ufb01ned as:softmax(zi)=eziPkj=1ezj1\uf8ffi\uf8ffk(5.32)The softmax of an input vectorz=[z1,z2,. . . ,zk]is thus a vector itself:softmax(z)=\"ez1Pki=1ezi,ez2Pki=1ezi,. . . ,ezkPki=1ezi#(5.33)The denominatorPki=1eziis used to normalize all the values into probabilities.Thus for example given a vector:z=[0.6,1.1,\u00001.5,1.2,3.2,\u00001.1]the result softmax(z) is[0.055,0.090,0.0067,0.10,0.74,0.010]Again like the sigmoid, the input to the softmax will be the dot product betweena weight vectorwand an input vectorx(plus a bias). But now we\u2019ll need separateweight vectors (and bias) for each of theKclasses.p(y=c|x)=ewc\u00b7x+bckXj=1ewj\u00b7x+bj(5.34)Like the sigmoid, the softmax has the property of squashing values toward 0 or1. thus if one of the inputs is larger than the others, will tend to push its probabilitytoward 1, and suppress the probabilities of the smaller inputs.5.6.1 Features in Multinomial Logistic RegressionFor multiclass classi\ufb01cation the input features need to be a function of both theobservationxand the candidate output classc. Thus instead of the notationxi,fiorfi(x), when we\u2019re discussing features we will use the notationfi(c,x), meaningfeatureifor a particular classcfor a given observationx.In binary classi\ufb01cation, a positive weight on a feature pointed toward y=1 anda negative weight toward y=0... but in multiclass a feature could be evidence for oragainst an individual class.Let\u2019s look at some sample features for a few NLP tasks to help understand thisperhaps unintuitive use of features that are functions of both the observationxandthe",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 145,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "like the sigmoid, the input to the softmax will be the dot product betweena weight vectorwand an input vectorx(plus a bias). But now we\u2019ll need separateweight vectors (and bias) for each of theKclasses.p(y=c|x)=ewc\u00b7x+bckXj=1ewj\u00b7x+bj(5.34)Like the sigmoid, the softmax has the property of squashing values toward 0 or1. thus if one of the inputs is larger than the others, will tend to push its probabilitytoward 1, and suppress the probabilities of the smaller inputs.5.6.1 Features in Multinomial Logistic RegressionFor multiclass classi\ufb01cation the input features need to be a function of both theobservationxand the candidate output classc. Thus instead of the notationxi,fiorfi(x), when we\u2019re discussing features we will use the notationfi(c,x), meaningfeatureifor a particular classcfor a given observationx.In binary classi\ufb01cation, a positive weight on a feature pointed toward y=1 anda negative weight toward y=0... but in multiclass a feature could be evidence for oragainst an individual class.Let\u2019s look at some sample features for a few NLP tasks to help understand thisperhaps unintuitive use of features that are functions of both the observationxandthe classc,Suppose we are doing text classi\ufb01cation, and instead of binary classi\ufb01cation ourtask is to assign one of the 3 classes+,\u0000, or 0 (neutral) to a document. Now afeature related to exclamation marks might have a negative weight for 0 documents,and a positive weight for+or\u0000documents:",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 146,
      "token_count": 352,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 91",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 147,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "The softmaxfunction\u25e6Turns  a vector z = [z1,z2,...,zk]of k arbitrary values into probabilities",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 148,
      "token_count": 27,
      "chapter_title": ""
    }
  },
  {
    "content": "915.6\u2022MULTINOMIAL LOGISTIC REGRESSION15For a vectorzof dimensionalityk, the softmax is de\ufb01ned as:softmax(zi)=eziPkj=1ezj1\uf8ffi\uf8ffk(5.32)The softmax of an input vectorz=[z1,z2,. . . ,zk]is thus a vector itself:softmax(z)=\"ez1Pki=1ezi,ez2Pki=1ezi,. . . ,ezkPki=1ezi#(5.33)The denominatorPki=1eziis used to normalize all the values into probabilities.Thus for example given a vector:z=[0.6,1.1,\u00001.5,1.2,3.2,\u00001.1]the result softmax(z) is[0.055,0.090,0.0067,0.10,0.74,0.010]Again like the sigmoid, the input to the softmax will be the dot product betweena weight vectorwand an input vectorx(plus a bias). But now we\u2019ll need separateweight vectors (and bias) for each of theKclasses.p(y=c|x)=ewc\u00b7x+bckXj=1ewj\u00b7x+bj(5.34)Like the sigmoid, the softmax has the property of squashing values toward 0 or1. thus if one of the inputs is larger than the others, will tend to push its probabilitytoward 1, and suppress the probabilities of the smaller inputs.5.6.1 Features in Multinomial Logistic RegressionFor multiclass classi\ufb01cation the input features need to be a function of both theobservationxand the candidate output classc. Thus instead of the notationxi,fiorfi(x), when we\u2019re discussing features we will use the notationfi(c,x), meaningfeatureifor a particular classcfor a given observationx.In binary classi\ufb01cation, a positive weight on a feature pointed toward y=1 anda negative weight toward y=0... but in multiclass a feature could be evidence for oragainst an individual class.Let\u2019s look at some sample features for a few NLP tasks to help understand thisperhaps unintuitive use of features that are functions of both the observationxandthe classc,Suppose we are doing text classi\ufb01cation, and instead of binary classi\ufb01cation ourtask is to assign one of the 3 classes+,\u0000, or 0 (neutral) to a document. Now afeature related to exclamation marks might have a negative weight for 0 documents,and a positive weight for+or\u0000documents:5.6\u2022MULTINOMIAL LOGISTIC REGRESSION15For a vectorzof dimensionalityk, the softmax is de\ufb01ned as:softmax(zi)=eziPkj=1ezj1\uf8ffi\uf8ffk(5.32)The softmax of an input vectorz=[z1,z2,. . . ,zk]is thus a vector itself:softmax(z)=\"ez1Pki=1ezi,ez2Pki=1ezi,. . . ,ezkPki=1ezi#(5.33)The denominatorPki=1eziis used to normalize all the values into probabilities.Thus for example given a vector:z=[0.6,1.1,\u00001.5,1.2,3.2,\u00001.1]the result softmax(z) is[0.055,0.090,0.0067,0.10,0.74,0.010]Again like the sigmoid, the input to the softmax will be the dot product betweena weight vectorwand an input vectorx(plus a bias). But now we\u2019ll need separateweight vectors",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 149,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "instead of binary classi\ufb01cation ourtask is to assign one of the 3 classes+,\u0000, or 0 (neutral) to a document. Now afeature related to exclamation marks might have a negative weight for 0 documents,and a positive weight for+or\u0000documents:5.6\u2022MULTINOMIAL LOGISTIC REGRESSION15For a vectorzof dimensionalityk, the softmax is de\ufb01ned as:softmax(zi)=eziPkj=1ezj1\uf8ffi\uf8ffk(5.32)The softmax of an input vectorz=[z1,z2,. . . ,zk]is thus a vector itself:softmax(z)=\"ez1Pki=1ezi,ez2Pki=1ezi,. . . ,ezkPki=1ezi#(5.33)The denominatorPki=1eziis used to normalize all the values into probabilities.Thus for example given a vector:z=[0.6,1.1,\u00001.5,1.2,3.2,\u00001.1]the result softmax(z) is[0.055,0.090,0.0067,0.10,0.74,0.010]Again like the sigmoid, the input to the softmax will be the dot product betweena weight vectorwand an input vectorx(plus a bias). But now we\u2019ll need separateweight vectors (and bias) for each of theKclasses.p(y=c|x)=ewc\u00b7x+bckXj=1ewj\u00b7x+bj(5.34)Like the sigmoid, the softmax has the property of squashing values toward 0 or1. thus if one of the inputs is larger than the others, will tend to push its probabilitytoward 1, and suppress the probabilities of the smaller inputs.5.6.1 Features in Multinomial Logistic RegressionFor multiclass classi\ufb01cation the input features need to be a function of both theobservationxand the candidate output classc. Thus instead of the notationxi,fiorfi(x), when we\u2019re discussing features we will use the notationfi(c,x), meaningfeatureifor a particular classcfor a given observationx.In binary classi\ufb01cation, a positive weight on a feature pointed toward y=1 anda negative weight toward y=0... but in multiclass a feature could be evidence for oragainst an individual class.Let\u2019s look at some sample features for a few NLP tasks to help understand thisperhaps unintuitive use of features that are functions of both the observationxandthe classc,Suppose we are doing text classi\ufb01cation, and instead of binary classi\ufb01cation ourtask is to assign one of the 3 classes+,\u0000, or 0 (neutral) to a document. Now afeature related to exclamation marks might have a negative weight for 0 documents,and a positive weight for+or\u0000documents:5.6\u2022MULTINOMIAL LOGISTIC REGRESSION15distributed according to a Gaussian distribution with mean\u00b5=0. In a Gaussianor normal distribution, the further away a value is from the mean, the lower itsprobability (scaled by the variances). By using a Gaussian prior on the weights, weare saying that weights prefer to have the value 0. A Gaussian for a weightqjis1q2ps2jexp \u0000(qj\u0000\u00b5j)22s2j!(5.27)If we multiply each weight by a Gaussian prior on the weight, we are thus maximiz-ing the following constraint:\u02c6q=argmaxqMYi=1P(y(i)|x(i))\u21e5nYj=11q2ps2jexp \u0000(qj\u0000\u00b5j)22s2j!(5.28)which in log",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 150,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "be evidence for oragainst an individual class.Let\u2019s look at some sample features for a few NLP tasks to help understand thisperhaps unintuitive use of features that are functions of both the observationxandthe classc,Suppose we are doing text classi\ufb01cation, and instead of binary classi\ufb01cation ourtask is to assign one of the 3 classes+,\u0000, or 0 (neutral) to a document. Now afeature related to exclamation marks might have a negative weight for 0 documents,and a positive weight for+or\u0000documents:5.6\u2022MULTINOMIAL LOGISTIC REGRESSION15distributed according to a Gaussian distribution with mean\u00b5=0. In a Gaussianor normal distribution, the further away a value is from the mean, the lower itsprobability (scaled by the variances). By using a Gaussian prior on the weights, weare saying that weights prefer to have the value 0. A Gaussian for a weightqjis1q2ps2jexp \u0000(qj\u0000\u00b5j)22s2j!(5.27)If we multiply each weight by a Gaussian prior on the weight, we are thus maximiz-ing the following constraint:\u02c6q=argmaxqMYi=1P(y(i)|x(i))\u21e5nYj=11q2ps2jexp \u0000(qj\u0000\u00b5j)22s2j!(5.28)which in log space, with\u00b5=0, and assuming 2s2=1, corresponds to\u02c6q=argmaxqmXi=1logP(y(i)|x(i))\u0000anXj=1q2j(5.29)which is in the same form as Eq.5.24.5.6 Multinomial logistic regressionSometimes we need more than two classes. Perhaps we might want to do 3-waysentiment classi\ufb01cation (positive, negative, or neutral). Or we could be assigningsome of the labels we will introduce in Chapter 8, like the part of speech of a word(choosing from 10, 30, or even 50 different parts of speech), or the named entitytype of a phrase (choosing from tags like person, location, organization).In such cases we usemultinomial logistic regression, also calledsoftmax re-multinomiallogisticregressiongression(or, historically, themaxent classi\ufb01er). In multinomial logistic regressionthe targetyis a variable that ranges over more than two classes; we want to knowthe probability ofybeing in each potential classc2C,p(y=c|x).The multinomial logistic classi\ufb01er uses a generalization of the sigmoid, calledthesoftmaxfunction, to compute the probabilityp(y=c|x). The softmax functionsoftmaxtakes a vectorz=[z1,z2,. . . ,zk]ofkarbitrary values and maps them to a probabilitydistribution, with each value in the range (0,1), and all the values summing to 1.Like the sigmoid, it is an exponential function.For a vectorzof dimensionalityk, the softmax is de\ufb01ned as:softmax(zi)=exp(zi)Pkj=1exp(zj)1\uf8ffi\uf8ffk(5.30)The softmax of an input vectorz=[z1,z2,. . . ,zk]is thus a vector itself:softmax(z)=\"exp(z1)Pki=1exp(zi),exp(z2)Pki=1exp(zi),. . . ,exp(zk)Pki=1exp(zi)#(5.31)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 151,
      "token_count": 758,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 92\n\nSoftmaxin multinomial logistic regression16CHAPTER5\u2022LOGISTICREGRESSIONThe denominatorPki=1exp(zi)is used to normalize all the values into probabil-ities. Thus for example given a vector:z=[0.6,1.1,\u00001.5,1.2,3.2,\u00001.1]the resulting (rounded) softmax(z) is[0.055,0.090,0.006,0.099,0.74,0.010]Again like the sigmoid, the input to the softmax will be the dot product betweena weight vectorwand an input vectorx(plus a bias). But now we\u2019ll need separateweight vectors (and bias) for each of theKclasses.p(y=c|x)=exp(wc\u00b7x+bc)kXj=1exp(wj\u00b7x+bj)(5.32)Like the sigmoid, the softmax has the property of squashing values toward 0 or 1.Thus if one of the inputs is larger than the others, it will tend to push its probabilitytoward 1, and suppress the probabilities of the smaller inputs.5.6.1 Features in Multinomial Logistic RegressionFeatures in multinomial logistic regression function similarly to binary logistic re-gression, with one difference that we\u2019ll need separate weight vectors (and biases) foreach of theKclasses. Recall our binary exclamation point featurex5from page79:x5=\u21e21 if \u201c!\u201d2doc0 otherwiseIn binary classi\ufb01cation a positive weightw5on a feature in\ufb02uences the classi\ufb01ertowardy=1 (positive sentiment) and a negative weight in\ufb02uences it towardy=0(negative sentiment) with the absolute value indicating how important the featureis. For multinominal logistic regression, by contrast, with separate weights for eachclass, a feature can be evidence for or against each individual class.In 3-way multiclass sentiment classi\ufb01cation, for example, we must assign eachdocument one of the 3 classes+,\u0000, or 0 (neutral). Now a feature related to excla-mation marks might have a negative weight for 0 documents, and a positive weightfor+or\u0000documents:Feature De\ufb01nitionw5,+w5,\u0000w5,0f5(x)\u21e21 if \u201c!\u201d2doc0 otherwise3.53.1\u00005.35.6.2 Learning in Multinomial Logistic RegressionThe loss function for multinomial logistic regression generalizes the loss functionfor binary logistic regression from 2 toKclasses. Recall that that the cross-entropyloss for binary logistic regression (repeated from Eq.5.11) is:LCE(\u02c6y,y)=\u0000logp(y|x)=\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)](5.33)92Input is still the dot product between weight vector w and input vector xBut now we\u2019ll need separate weight vectors for each of the K classes.",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 152,
      "token_count": 642,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 93",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 153,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Features in binary versus multinomial logistic regressionBinary: positive weight \u00e0y=1  neg weight \u00e0y=0Multinominal: separate weights for each class:",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 154,
      "token_count": 34,
      "chapter_title": ""
    }
  },
  {
    "content": "9316CHAPTER5\u2022LOGISTICREGRESSIONThe denominatorPki=1exp(zi)is used to normalize all the values into probabil-ities. Thus for example given a vector:z=[0.6,1.1,\u00001.5,1.2,3.2,\u00001.1]the resulting (rounded) softmax(z) is[0.055,0.090,0.006,0.099,0.74,0.010]Again like the sigmoid, the input to the softmax will be the dot product betweena weight vectorwand an input vectorx(plus a bias). But now we\u2019ll need separateweight vectors (and bias) for each of theKclasses.p(y=c|x)=exp(wc\u00b7x+bc)kXj=1exp(wj\u00b7x+bj)(5.32)Like the sigmoid, the softmax has the property of squashing values toward 0 or 1.Thus if one of the inputs is larger than the others, it will tend to push its probabilitytoward 1, and suppress the probabilities of the smaller inputs.5.6.1 Features in Multinomial Logistic RegressionFeatures in multinomial logistic regression function similarly to binary logistic re-gression, with one difference that we\u2019ll need separate weight vectors (and biases) foreach of theKclasses. Recall our binary exclamation point featurex5from page79:x5=\u21e21 if \u201c!\u201d2doc0 otherwiseIn binary classi\ufb01cation a positive weightw5on a feature in\ufb02uences the classi\ufb01ertowardy=1 (positive sentiment) and a negative weight in\ufb02uences it towardy=0(negative sentiment) with the absolute value indicating how important the featureis. For multinominal logistic regression, by contrast, with separate weights for eachclass, a feature can be evidence for or against each individual class.In 3-way multiclass sentiment classi\ufb01cation, for example, we must assign eachdocument one of the 3 classes+,\u0000, or 0 (neutral). Now a feature related to excla-mation marks might have a negative weight for 0 documents, and a positive weightfor+or\u0000documents:Feature De\ufb01nitionw5,+w5,\u0000w5,0f5(x)\u21e21 if \u201c!\u201d2doc0 otherwise3.53.1\u00005.35.6.2 Learning in Multinomial Logistic RegressionThe loss function for multinomial logistic regression generalizes the loss functionfor binary logistic regression from 2 toKclasses. Recall that that the cross-entropyloss for binary logistic regression (repeated from Eq.5.11) is:LCE(\u02c6y,y)=\u0000logp(y|x)=\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)](5.33)w5= 3.016CHAPTER5\u2022LOGISTICREGRESSIONThe denominatorPki=1exp(zi)is used to normalize all the values into probabil-ities. Thus for example given a vector:z=[0.6,1.1,\u00001.5,1.2,3.2,\u00001.1]the resulting (rounded) softmax(z) is[0.055,0.090,0.006,0.099,0.74,0.010]Again like the sigmoid, the input to the softmax will be the dot product betweena weight vectorwand an input vectorx(plus a bias). But now we\u2019ll need separateweight vectors (and bias) for each of theKclasses.p(y=c|x)=exp(wc\u00b7x+bc)kXj=1exp(wj\u00b7x+bj)(5.32)Like the sigmoid, the softmax has the property of squashing values toward 0",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 155,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "Learning in Multinomial Logistic RegressionThe loss function for multinomial logistic regression generalizes the loss functionfor binary logistic regression from 2 toKclasses. Recall that that the cross-entropyloss for binary logistic regression (repeated from Eq.5.11) is:LCE(\u02c6y,y)=\u0000logp(y|x)=\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)](5.33)w5= 3.016CHAPTER5\u2022LOGISTICREGRESSIONThe denominatorPki=1exp(zi)is used to normalize all the values into probabil-ities. Thus for example given a vector:z=[0.6,1.1,\u00001.5,1.2,3.2,\u00001.1]the resulting (rounded) softmax(z) is[0.055,0.090,0.006,0.099,0.74,0.010]Again like the sigmoid, the input to the softmax will be the dot product betweena weight vectorwand an input vectorx(plus a bias). But now we\u2019ll need separateweight vectors (and bias) for each of theKclasses.p(y=c|x)=exp(wc\u00b7x+bc)kXj=1exp(wj\u00b7x+bj)(5.32)Like the sigmoid, the softmax has the property of squashing values toward 0 or 1.Thus if one of the inputs is larger than the others, it will tend to push its probabilitytoward 1, and suppress the probabilities of the smaller inputs.5.6.1 Features in Multinomial Logistic RegressionFeatures in multinomial logistic regression function similarly to binary logistic re-gression, with one difference that we\u2019ll need separate weight vectors (and biases) foreach of theKclasses. Recall our binary exclamation point featurex5from page79:x5=\u21e21 if \u201c!\u201d2doc0 otherwiseIn binary classi\ufb01cation a positive weightw5on a feature in\ufb02uences the classi\ufb01ertowardy=1 (positive sentiment) and a negative weight in\ufb02uences it towardy=0(negative sentiment) with the absolute value indicating how important the featureis. For multinominal logistic regression, by contrast, with separate weights for eachclass, a feature can be evidence for or against each individual class.In 3-way multiclass sentiment classi\ufb01cation, for example, we must assign eachdocument one of the 3 classes+,\u0000, or 0 (neutral). Now a feature related to excla-mation marks might have a negative weight for 0 documents, and a positive weightfor+or\u0000documents:Feature De\ufb01nitionw5,+w5,\u0000w5,0f5(x)\u21e21 if \u201c!\u201d2doc0 otherwise3.53.1\u00005.35.6.2 Learning in Multinomial Logistic RegressionThe loss function for multinomial logistic regression generalizes the loss functionfor binary logistic regression from 2 toKclasses. Recall that that the cross-entropyloss for binary logistic regression (repeated from Eq.5.11) is:LCE(\u02c6y,y)=\u0000logp(y|x)=\u0000[ylog \u02c6y+(1\u0000y)log(1\u0000\u02c6y)](5.33)",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 156,
      "token_count": 700,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 94\n\nLogistic RegressionMultinomial Logistic Regression",
    "metadata": {
      "source": "5_LR_Apr_7_2021",
      "chunk_id": 157,
      "token_count": 13,
      "chapter_title": ""
    }
  }
]