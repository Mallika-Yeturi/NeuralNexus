[
  {
    "content": "# 24_Dialogue_May_6_2021\n\n## Page 1\n\nChatbots and Dialogue SystemsIntroduction to Chatbots and Dialogue Systems\n\n## Page 2\n\nConversational Agents  (AKA  Dialogue Systems AKA Dialogue Agents AKA Chatbots)Personal Assistants on phones or other devicesSIRI, Alexa, Cortana, Google AssistantPlaying music, setting timers and clocksChatting for funBooking travel reservationsClinical uses for mental health\n\n## Page 3\n\nTwo kind of conversational agents1.Chatbots-mimic informal human chatting-for fun, or even for therapy2.(Task-based) Dialogue Agents-interfaces to personal assistants-cars, robots, appliances-booking flights or restaurants\n\n## Page 4\n\nChatbotArchitecturesRule-based1.Pattern-action rules (ELIZA)2.+ A mental model (PARRY):The first system to pass the Turing Test! Corpus-based3.Information Retrieval (XiaoIce)4.Neural encoder-decoder (BlenderBot)\n\n## Page 5\n\nSure, what do you want to be sung to? I can sing you a song about baking.Will you sing me a song?Yes, sing me a song about baking!Do you want me to sing it to the tune of \u201cSing Along with Mitch\u201d?Yes, I would love that.OK, I\u2019ll try to get it to you by the end of the day. What else do you like to do?No, sing it to me now!OK, give me a few days. I\u2019m a teacher, so I don\u2019t have much free time.Can you at least tell me what the lyrics are?Sure, it goes like this: \u201cBaking is my favorite thing to do in the world.\u201dHow does the chorus go?It goes: BAKING IS MY FAVORITE THING TO DO IN THE WORLD!Wow, sounds like a bop.I know, right?  It\u2019s so catchy. I can\u2019t get it out of my head.BlenderBot(Roller et al. 2020)",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 0,
      "token_count": 422,
      "chapter_title": "24_Dialogue_May_6_2021"
    }
  },
  {
    "content": "## Page 6\n\nXiaoIce(Zhou et al., 2020)6CHAPTER24\u2022CHATBOTS&DIALOGUESYSTEMSThese systems often have an entertainment value, such as Facebook\u2019s Blender-Bot, a neural chatbot capable of carrying on the kinds of conversations shown inFig.24.2, or Microsoft\u2019s XiaoIce system2(Zhou et al., 2020), a system that chatswith people on text messaging platforms (Fig.24.3).Sure, what do you want to be sung to? I can sing you a song about baking.Will you sing me a song?Yes, sing me a song about baking!Do you want me to sign it to the tune of \u201cSing Along with Mitch\u201d?Yes, I would love that.OK, I\u2019ll try to get it to you by the end of the day. What else do you like to do?No, sing it to me now!OK, give me a few days. I\u2019m a teacher, so I don\u2019t have much free time.Can you at least tell me what the lyrics are?Sure, it goes like this: \u201cBaking is my favorite thing to do in the world.\u201dHow does the chorus go?It goes: BAKING IS MY FAVORITE THING TO DO IN THE WORLD!Wow, sounds like a bop.I know, right?  It\u2019s so catchy. I can\u2019t get it out of my head.Figure 24.2A conversation between a user (left) and BlenderBot(Roller et al., 2020).;\u222b{=\u270f\u221e`}[Human] Hello, XiaoIce.;\u270f\u221e=`}@\u0000`4\u0153} \u00a3\u0000\u0000\u2026(\u00cd\u00a1\u00ab[XiaoIce]Hello, your profile pic isinteresting. Like I\u2019ve seenit somewhere.;\u222b{=/\u232b[Human] Really?;\u270f\u221e=\u2013\u0000/\u00cd\u00cb\u00ae;\u00c3\u00d1@[XiaoIce]Is it a comic character?;\u222b{=`\u0000H\u00c2S\u00d1[Human] How do you know that?;\u270f\u221e=`\u2327[XiaoIce]Guess.;\u222b{=\u00e6S`/\u0000\u222b\u232b\u0000[Human] Are you human?;\u270f\u221e=\u0178*\u0000`\u00e5\u2318\u21e2JJ1\u00c2S\u00dc[XiaoIce]Well, you\u2019ll know it afterchatting with me more.;\u222b{=`\u0000/\u222b\u00c2z\u02dd:h\u222bH[Human] Aren\u2019t you an Al chatbot?;\u270f\u221e=/\u00bb\u00007\u0000\u2318\u0000\u21e2\u2211\u221e\u221e\u00d1\u02d8\u00d6`\u00d1[XiaoIce]So what? I won\u2019t beindifferent to you.;\u222b{=`\u02dd \u00bfH\u00fc\u02ddJ[Human] What functions do you have?;\u270f\u221e=\u2318 200\u21e2*skills\u0000e\u0000\u00d9\u02d9`\u00d1\u0000B[XiaoIce]I have over 200 skills. Justname one you need.Figure 24.3A sample Chinese conversation from the XiaoIce chatbot with translations(Zhou et al., 2020).Yet starting from the very \ufb01rst system, ELIZA(Weizenbaum, 1966), chatbotshave also been used for practical purposes like testing theories of psychological2XiaoIce is a somewhat confusing translation of the systems Chinese name\u270f\u221e, \u2018Xiao Bing\u2019, whichmeans Little Bing or Little Ice.",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 1,
      "token_count": 747,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\nTask-based dialogue agents\"Task-based\" or\"goal-based\" dialogueagents\u2022Systems that have the goal of helping a user solve a task\u2022Setting a timer\u2022Making a travel reservation\u2022Playing a song\u2022Buying a productArchitecture: \u2022Frames with slots and values\u2022A knowledge structure representing user intentions\n\n## Page 8\n\nThe FrameA set of slots, to be filled with information of a given typeEach associated with a questionto the userSlotTypeQuestionORIGINcity\"What city are you leaving from?DESTcity\"Where are you going?DEP DATE date\"What day would you like to leave?DEP TIME time\"What time would you like to leave?AIRLINEline\"What is your preferred airline?\n\n## Page 9\n\nChatbots and Dialogue SystemsIntroduction to Chatbots and Dialogue Systems\n\n## Page 10\n\nChatbots and Dialogue SystemsProperties of Human Conversation",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 2,
      "token_count": 181,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\nChatbots and Dialogue SystemsIntroduction to Chatbots and Dialogue Systems\n\n## Page 10\n\nChatbots and Dialogue SystemsProperties of Human Conversation\n\n## Page 11\n\nA telephone conversation between a human travel agent (A) and a human client (C)2CHAPTER24\u2022CHATBOTS&DIALOGUESYSTEMScontrast,chatbotsare systems designed for extended conversations, set up to mimicthe unstructured conversations or \u2018chats\u2019 characteristic of human-human interaction,mainly for entertainment, but also for practical purposes like making task-orientedagents more natural.1In Section24.2we\u2019ll discuss the three major chatbot architec-tures: rule-based systems, information retrieval systems, and encoder-decoder gen-erators. In Section24.3we turn to task-oriented agents, introducing the frame-basedarchitecture (theGUSarchitecture) that underlies most task-based systems.24.1 Properties of Human ConversationConversation between humans is an intricate and complex joint activity. Before weattempt to design a conversational agent to converse with humans, it is crucial tounderstand something about how humans converse with each other. Consider someof the phenomena that occur in the conversation between a human travel agent anda human client excerpted in Fig.24.1.C1: . . . I need to travel in May.A2: And, what day in May did you want to travel?C3: OK uh I need to be there for a meeting that\u2019s from the 12th to the 15th.A4: And you\u2019re \ufb02ying into what city?C5: Seattle.A6: And what time would you like to leave Pittsburgh?C7: Uh hmm I don\u2019t think there\u2019s many options for non-stop.A8: Right. There\u2019s three non-stops today.C9: What are they?A10: The \ufb01rst one departs PGH at 10:00am arrives Seattle at 12:05 their time.The second \ufb02ight departs PGH at 5:55pm, arrives Seattle at 8pm. And thelast \ufb02ight departs PGH at 8:15pm arrives Seattle at 10:28pm.C11: OK I\u2019ll take the 5ish \ufb02ight on the night before on the 11th.A12: On the 11th? OK. Departing at 5:55pm arrives Seattle at 8pm, U.S. Air\ufb02ight 115.C13: OK.A14: And you said returning on May 15th?C15: Uh, yeah, at the end of the day.A16: OK. There\u2019s #two non-stops . . . #C17: #Act. . . actually #, what day of the week is the 15th?A18: It\u2019s a Friday.C19: Uh hmm. I would consider staying there an extra day til Sunday.A20: OK. . . OK. On Sunday I have . . .Figure 24.1Part of a phone conversation between a human travel agent (A) and humanclient (C). The passages framed by # in A16and C17indicate overlaps in speech.TurnsA dialogue is a sequence ofturns(C1,A2,C3, and so on), each a single contributionturnfrom one speaker to the dialogue (as if in a game: I take a turn, then you take a turn,1By contrast, in popular usage, the wordchatbotis often generalized to refer to both task-oriented andchit-chat systems; we\u2019ll be usingdialogue systemsfor the former.\n\n## Page 12\n\nProperties of Human ConversationTurns\u2022We call each contribution a \"turn\"\u2022As if conversation was the kind of game where everyone takes turns.\n12",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 3,
      "token_count": 782,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\nProperties of Human ConversationTurns\u2022We call each contribution a \"turn\"\u2022As if conversation was the kind of game where everyone takes turns.\n12\n\n## Page 13\n\n2CHAPTER24\u2022CHATBOTS&DIALOGUESYSTEMScontrast,chatbotsare systems designed for extended conversations, set up to mimicthe unstructured conversations or \u2018chats\u2019 characteristic of human-human interaction,mainly for entertainment, but also for practical purposes like making task-orientedagents more natural.1In Section24.2we\u2019ll discuss the three major chatbot architec-tures: rule-based systems, information retrieval systems, and encoder-decoder gen-erators. In Section24.3we turn to task-oriented agents, introducing the frame-basedarchitecture (theGUSarchitecture) that underlies most task-based systems.24.1 Properties of Human ConversationConversation between humans is an intricate and complex joint activity. Before weattempt to design a conversational agent to converse with humans, it is crucial tounderstand something about how humans converse with each other. Consider someof the phenomena that occur in the conversation between a human travel agent anda human client excerpted in Fig.24.1.C1: . . . I need to travel in May.A2: And, what day in May did you want to travel?C3: OK uh I need to be there for a meeting that\u2019s from the 12th to the 15th.A4: And you\u2019re \ufb02ying into what city?C5: Seattle.A6: And what time would you like to leave Pittsburgh?C7: Uh hmm I don\u2019t think there\u2019s many options for non-stop.A8: Right. There\u2019s three non-stops today.C9: What are they?A10: The \ufb01rst one departs PGH at 10:00am arrives Seattle at 12:05 their time.The second \ufb02ight departs PGH at 5:55pm, arrives Seattle at 8pm. And thelast \ufb02ight departs PGH at 8:15pm arrives Seattle at 10:28pm.C11: OK I\u2019ll take the 5ish \ufb02ight on the night before on the 11th.A12: On the 11th? OK. Departing at 5:55pm arrives Seattle at 8pm, U.S. Air\ufb02ight 115.C13: OK.A14: And you said returning on May 15th?C15: Uh, yeah, at the end of the day.A16: OK. There\u2019s #two non-stops . . . #C17: #Act. . . actually #, what day of the week is the 15th?A18: It\u2019s a Friday.C19: Uh hmm. I would consider staying there an extra day til Sunday.A20: OK. . . OK. On Sunday I have . . .Figure 24.1Part of a phone conversation between a human travel agent (A) and humanclient (C). The passages framed by # in A16and C17indicate overlaps in speech.TurnsA dialogue is a sequence ofturns(C1,A2,C3, and so on), each a single contributionturnfrom one speaker to the dialogue (as if in a game: I take a turn, then you take a turn,1By contrast, in popular usage, the wordchatbotis often generalized to refer to both task-oriented andchit-chat systems; we\u2019ll be usingdialogue systemsfor the former.\n\n## Page 14\n\nProperties of Human ConversationTurn-taking issues\u25e6When to take the floor?\u25e6When to yield the floor?Interruptions\n14",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 4,
      "token_count": 766,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\nProperties of Human ConversationTurn-taking issues\u25e6When to take the floor?\u25e6When to yield the floor?Interruptions\n14\n\n## Page 15\n\n2CHAPTER24\u2022CHATBOTS&DIALOGUESYSTEMScontrast,chatbotsare systems designed for extended conversations, set up to mimicthe unstructured conversations or \u2018chats\u2019 characteristic of human-human interaction,mainly for entertainment, but also for practical purposes like making task-orientedagents more natural.1In Section24.2we\u2019ll discuss the three major chatbot architec-tures: rule-based systems, information retrieval systems, and encoder-decoder gen-erators. In Section24.3we turn to task-oriented agents, introducing the frame-basedarchitecture (theGUSarchitecture) that underlies most task-based systems.24.1 Properties of Human ConversationConversation between humans is an intricate and complex joint activity. Before weattempt to design a conversational agent to converse with humans, it is crucial tounderstand something about how humans converse with each other. Consider someof the phenomena that occur in the conversation between a human travel agent anda human client excerpted in Fig.24.1.C1: . . . I need to travel in May.A2: And, what day in May did you want to travel?C3: OK uh I need to be there for a meeting that\u2019s from the 12th to the 15th.A4: And you\u2019re \ufb02ying into what city?C5: Seattle.A6: And what time would you like to leave Pittsburgh?C7: Uh hmm I don\u2019t think there\u2019s many options for non-stop.A8: Right. There\u2019s three non-stops today.C9: What are they?A10: The \ufb01rst one departs PGH at 10:00am arrives Seattle at 12:05 their time.The second \ufb02ight departs PGH at 5:55pm, arrives Seattle at 8pm. And thelast \ufb02ight departs PGH at 8:15pm arrives Seattle at 10:28pm.C11: OK I\u2019ll take the 5ish \ufb02ight on the night before on the 11th.A12: On the 11th? OK. Departing at 5:55pm arrives Seattle at 8pm, U.S. Air\ufb02ight 115.C13: OK.A14: And you said returning on May 15th?C15: Uh, yeah, at the end of the day.A16: OK. There\u2019s #two non-stops . . . #C17: #Act. . . actually #, what day of the week is the 15th?A18: It\u2019s a Friday.C19: Uh hmm. I would consider staying there an extra day til Sunday.A20: OK. . . OK. On Sunday I have . . .Figure 24.1Part of a phone conversation between a human travel agent (A) and humanclient (C). The passages framed by # in A16and C17indicate overlaps in speech.TurnsA dialogue is a sequence ofturns(C1,A2,C3, and so on), each a single contributionturnfrom one speaker to the dialogue (as if in a game: I take a turn, then you take a turn,1By contrast, in popular usage, the wordchatbotis often generalized to refer to both task-oriented andchit-chat systems; we\u2019ll be usingdialogue systemsfor the former.\n\n## Page 16\n\nImplications for Conversational AgentsBarge-in\u25e6Allowing the user to interruptEnd-pointing\u25e6The task for a speech system of deciding whether the user has stopped talking.\u25e6Very hard, since people often pause in the middle of turns\n16",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 5,
      "token_count": 789,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\nImplications for Conversational AgentsBarge-in\u25e6Allowing the user to interruptEnd-pointing\u25e6The task for a speech system of deciding whether the user has stopped talking.\u25e6Very hard, since people often pause in the middle of turns\n16\n\n## Page 17\n\nLanguage as Action\nEach turn in a dialogue is a kind of actionWittgenstein (1953) and Austin (1962)\n\n## Page 18\n\nSpeech Acts (aka Dialogue Acts)Constatives: committing the speaker to something\u2019s being the case (answering, claiming, confirming, denying, disagreeing, stating) Directives: attempts by the speaker to get the addressee to do something (advising, asking, forbidding, inviting, ordering, requesting) Commissives: committing the speaker to some future course of action (promising, planning, vowing, betting, opposing) Acknowledgments: express the speaker\u2019s attitude regarding the hearer with respect to some social action (apologizing, greeting, thanking, accepting an acknowledgment) Bach and Harnish (1979)\n\n## Page 19\n\nSpeech acts\"Turn up the music!\" DIRECTIVE\"What day in May do you want to travel?\"DIRECTIVE\"I need to travel in May\"CONSTATIVEThanksACKNOWLEDGEMENT\n\n## Page 20\n\nGrounding\u2022Participants in conversation or any joint activity need to establish common ground.\u2022Principle of closure.  Agents performing an action require evidence, sufficient for current purposes, that they have succeeded in performing it  (Clark 1996, after Norman 1988)\u2022Speech is an action too!  Sospeakers need to groundeach other\u2019s utterances. \u2022Grounding: acknowledging that the hearer has understood\n\n## Page 21\n\nGrounding\u2022Grounding is relevant for human-machine interaction\u2022Why do elevator buttons light up?\n\n## Page 22\n\nGrounding: Establishing Common GroundA: And you said returning on May 15th?C: Uh, yeah, at the end of the day.A: OKC: OK I\u2019ll take the 5ish flight on the night before on the 11th.A: On the 11th? OK. C: ...I need to travel in May.A: And, what day in May did you want to travel?\n\n## Page 23\n\nGrounding is important for computers too!System: Did you want to review some more of your profile?User: No.System: What\u2019s next?System: Did you want to review some more of your profile?User: No.System: Okay, what\u2019s next?Cohen et al (2004)\n\n## Page 24\n\nConversations have structureLocal structure between adjacent speech acts, from the field of conversational analysis(Sacks et al. 1974)Calledadjacency pairs:QUESTION\u2026 ANSWERPROPOSAL\u2026 ACCEPTANCE/REJECTIONCOMPLIMENTS(\"Nice jacket!\")\u2026 DOWNPLAYER(\"Oh, this old thing?\")\n\n## Page 25\n\nAnother kind of structure: SubdialoguesAgent:  OK.  There's #two non-stops#Client:                     #Act-actually#, what day of the week is the 15th?Agent: It\u2019s a Friday.Client: Uh hmm. I would consider staying there an extra day tilSunday. Agent: OK...OK. On Sunday I have ... \n25Correctionsubdialogue\n\n## Page 26\n\nClarification SubdialoguesUser: What do you have going to UNKNOWN WORD on the 5th?System:  Let\u2019s see, going where on the 5th?User: Going to Hong Kong. System: OK, here are some flights... \n26",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 6,
      "token_count": 763,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23\n\nGrounding is important for computers too!System: Did you want to review some more of your profile?User: No.System: What\u2019s next?System: Did you want to review some more of your profile?User: No.System: Okay, what\u2019s next?Cohen et al (2004)\n\n## Page 24\n\nConversations have structureLocal structure between adjacent speech acts, from the field of conversational analysis(Sacks et al. 1974)Calledadjacency pairs:QUESTION\u2026 ANSWERPROPOSAL\u2026 ACCEPTANCE/REJECTIONCOMPLIMENTS(\"Nice jacket!\")\u2026 DOWNPLAYER(\"Oh, this old thing?\")\n\n## Page 25\n\nAnother kind of structure: SubdialoguesAgent:  OK.  There's #two non-stops#Client:                     #Act-actually#, what day of the week is the 15th?Agent: It\u2019s a Friday.Client: Uh hmm. I would consider staying there an extra day tilSunday. Agent: OK...OK. On Sunday I have ... \n25Correctionsubdialogue\n\n## Page 26\n\nClarification SubdialoguesUser: What do you have going to UNKNOWN WORD on the 5th?System:  Let\u2019s see, going where on the 5th?User: Going to Hong Kong. System: OK, here are some flights... \n26\n\n## Page 27\n\nPresequencesUser: Can you make train reservations?System: YesI can.User: Great, I\u2019d like to reserve a seat on the 4pm train to New York. \n\n## Page 28\n\nConversational InitiativeSome conversations are controlled by one person\u2022A reporter interviewing a chef asks questions, and the chef responds.\u2022This reporter has the conversational initiative (Walker and Whittaker 1990)Most human conversations have mixed initiative: \u2022I lead, then you lead, then I lead.\u2022Mixed initiative is very hard for NLP systems, which often default to simpler styles that can be frustrating for humans:\u2022User initiative (user asks or commands, system responds)\u2022System initiative (system asks user questions to fill out a form, user can't change the direction)\n\n## Page 29\n\nEven harder problems:  InferenceAgent: And, what day in May did you want to travel?Client: OK, uh, I need to be there for a meeting that\u2019s from the 12th to the 15th. \n\n## Page 30\n\nChatbots and Dialogue SystemsProperties of Human Conversation\n\n## Page 31\n\nChatbots and Dialogue SystemsRule-based Chatbots: ELIZA and PARRY",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 7,
      "token_count": 536,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 27\n\nPresequencesUser: Can you make train reservations?System: YesI can.User: Great, I\u2019d like to reserve a seat on the 4pm train to New York. \n\n## Page 28\n\nConversational InitiativeSome conversations are controlled by one person\u2022A reporter interviewing a chef asks questions, and the chef responds.\u2022This reporter has the conversational initiative (Walker and Whittaker 1990)Most human conversations have mixed initiative: \u2022I lead, then you lead, then I lead.\u2022Mixed initiative is very hard for NLP systems, which often default to simpler styles that can be frustrating for humans:\u2022User initiative (user asks or commands, system responds)\u2022System initiative (system asks user questions to fill out a form, user can't change the direction)\n\n## Page 29\n\nEven harder problems:  InferenceAgent: And, what day in May did you want to travel?Client: OK, uh, I need to be there for a meeting that\u2019s from the 12th to the 15th. \n\n## Page 30\n\nChatbots and Dialogue SystemsProperties of Human Conversation\n\n## Page 31\n\nChatbots and Dialogue SystemsRule-based Chatbots: ELIZA and PARRY\n\n## Page 32\n\nELIZA: Weizenbaum(1966)Men are all alike.IN WHAT WAYThey're always bugging us about something or other. CAN YOU THINK OF A SPECIFIC EXAMPLE Well, my boyfriend made me come here.YOUR BOYFRIEND MADE YOU COME HERE He says I'm depressed much of the time.I AM SORRY TO HEAR YOU ARE DEPRESSED ...You are like my father in some ways.WHAT RESEMBLANCE DO YOU SEEYou are not very aggressivebut I think you don\u2019t want me to notice that.WHAT MAKES YOU THINK I AM NOT AGGRESSIVEYou don\u2019t argue with me.WHY DO YOU THINK I DON\u2019T ARGUE WITH YOUYou are afraid of me.DOES IT PLEASE YOU TO BELIEVE I\u2019M AFRAID OF YOUMyfatherisafraidof everybody.WHAT ELSE COMES TO MIND WHEN YOU THINK OF YOUR FATHERBullies.DOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR BOYFRIEND MADE YOU COME HEREWeizenbaum, Joseph. 1966,\"ELIZA -A Computer Program For the Study of Natural Language Communication Between Man And Machine\",Communications of the ACM 9 (1): 36-45\n\n## Page 33\n\nELIZA's trick:be a Rogerian psychologistDraw the patient out by reflecting patient\u2019s statements back at themRare type of conversation in which one can \u201cassume the pose of knowing almost nothing of the real world\u201d\n\n## Page 34\n\nRogerian psychologistPatient: \"I went for a long boat ride\u201dPsychologist: \"Tell me about boats\u201dYou don\u2019t assume she didn\u2019t know what a boat isYou assume she had some conversational goalChatbotstrying to pass the Turing test usually choose some such domain\n\n## Page 35\n\nEliza pattern/transform rules(0 YOU 0 ME)    [pattern]\u00e0(WHAT MAKES YOU THINK I 3 YOU) [transform]0  means Kleene *The 3 is the constituent # in patternYou hate meWHAT MAKES YOU THINK I HATE YOU",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 8,
      "token_count": 697,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 36",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 9,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Eliza RulesA formal notation in which to represent the decomposition template is: (0 YOU 0 ME) and the reassembly rule (WHAT MAKES YOU THINK I 3 YOU). The \"0\" in the decomposition rule stands for \"an in- definite number of words\" (analogous to the indefinite dollar sign of COMIT) [6] while the \"3\" in the reassembly rule indicates that the third component of the subject decomposition is to be inserted in its place. The decom- position rule (0 YOU 1 ME) would have worked just as well in this specific example. A nonzero integer \"n\" appearing in a decomposition rule indicates that the component in question should consist of exactly \"n\" words. However, of the two rules shown, only the first would have matched the sentence, \"It seems you hate and love me,\" the second failing because there is more than one word between \"you\" and \"me\".",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 10,
      "token_count": 193,
      "chapter_title": ""
    }
  },
  {
    "content": "~1 RI,1 R1,2\"'\" Rl,ml D2 R2J RZ,Z\" \" \"Rz,m2 De Ra,t FIn'2~ '\" \" R%nan Fro. 1. :Keyword and rule list structure in ELIZA the question of which decomposition rules to apply to an input text is of course a crucial one. The input sentence might have been, for example, \"It seems that you hate,\" in which ease the decomposition rule (0 YOU 0 ME) would have failed in that the word \"ME\" would not have been found at all, let alone in its assigned place. Some other decomposition rule would then have to be tried and, failing that, still another until a match could be made or a total failure reported. ELIZA must therefm~ have a mechanism to sharply delimit the set of decom- position rules which are potentially applicable to a cur- rently active input sentence. This is the keyword mecha- nism. An input sentence is scanned from left to right. Each word is looked up in a dictionary of keywords. If a word is identified as a keyword, then (apart from the issue of precedence of keywords) only decomposition rules con- taining that keyword need to be tried. The trial sequence can even be partially ordered. For example, the decom- position rule (0 YOU 0) associated with the keyword \"YOU\" (and decomposing an input sentence into (1) all words in front of \"YOU\", (2) the word \"YOU\", and (3) all words following \"YOU\") should be the last, one tried since it is bound to succeed. Two problems now arise. One stems from the fact that almost none of the words in any given sentence are repre. sented in the keyword dictionary. The other is that of \"associating\" both decomposit;ion and reassembly rules with keyword~-;. The iirst is serious in that the determina- tion that a word is not in a dictionary may well require more computation (i.e., time) than the location of a word which is represented. The attack on both problems begins by placing both a keyword trod its associated rules on a list. The basic format of a typical key list is the following: (K ((D0 (R~. t) (R~, 2) \"'\" (Rt .... )) ((D2) (R2. ~) (R2.2) \"\" (R2.,,,~)) : ((D,~) (R,,, ~) (taw, ~) ... (R,~, .~))) where K is the keyword, D, the it, h decomposition rule associated with K and R\u00a2, ~ the jth reassembly rule ass0. ciated with the ith decomposition rule. A common pictorial representation of such a structure is the tree diagram shown in Figure 1. The top level of this structure contains the keyword followed by the names of lists; each one of which is again a list structure beginning with a decomposition rule and followed by reassembly rules. Since list structures of this type have no predeter- mined dimensionality limitations, any number of decom- position rules may be associated with a given keyword and any number of reassembly rules witch any specific decom- position rule. SLiP i8 rich in functions that sequence over structures of this type efficiently. Hence programmin problems are minimized. An ELIZA script consists mainly of a set of list struc- tures of the type shown. The actual keyword dictionary is constructed when such a script is first read into the hitherto empty program. The basic structural component of the keyword dictionary is a vector KEY of (currently) 128 contiguous computer words. As a particular key list structure is read the keyword K at its top is randomized (hashed) by a procedure that produces (currently) a",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 11,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "~) (R2.2) \"\" (R2.,,,~)) : ((D,~) (R,,, ~) (taw, ~) ... (R,~, .~))) where K is the keyword, D, the it, h decomposition rule associated with K and R\u00a2, ~ the jth reassembly rule ass0. ciated with the ith decomposition rule. A common pictorial representation of such a structure is the tree diagram shown in Figure 1. The top level of this structure contains the keyword followed by the names of lists; each one of which is again a list structure beginning with a decomposition rule and followed by reassembly rules. Since list structures of this type have no predeter- mined dimensionality limitations, any number of decom- position rules may be associated with a given keyword and any number of reassembly rules witch any specific decom- position rule. SLiP i8 rich in functions that sequence over structures of this type efficiently. Hence programmin problems are minimized. An ELIZA script consists mainly of a set of list struc- tures of the type shown. The actual keyword dictionary is constructed when such a script is first read into the hitherto empty program. The basic structural component of the keyword dictionary is a vector KEY of (currently) 128 contiguous computer words. As a particular key list structure is read the keyword K at its top is randomized (hashed) by a procedure that produces (currently) a 7 bit integer \"i\". The word \"always\", for example, yields the integer 14. KEY(i), i.e., the ith word of the vector KEY, is then examined to determine whether it contains a list. name. If it does not, then an empty list is created, its name placed in KEY(i), and the key list structure in question placed on that list. If KEY(i) already contains a list name, then the name of the key list structure is placed on the bottom of the list named in KEY(i). The largest dictionary so far attempted contains about 50 keywords. No list named in any of the words of the KEY vector contains more than two key list structures. Every word encountered in the scan of an input text, i.e., during the actual operations of ELIZA, is randomized by the same hashing algorithm as was originally applied to the incoming keywords, hence yields an integer which points to the only possible list structure which could potentially contain that word as a keyword. Even then, only the tops of any key list structures that may be found there need be interrogated to determine whether or not a keyword has been found. By virtue of the various list 38 Communications of tile ACM Volume 9 / Number 1 / January, 1966",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 12,
      "token_count": 557,
      "chapter_title": ""
    }
  },
  {
    "content": "keywordpatternlist of transformsKeyword:  YOUPattern: (0 YOU 0 ME)    Transforms: (WHAT MAKES YOU THINK I 3 YOU)(WHY DO YOU THINK I 3 YOU)Rules are organized by keywordsEach keyword has a pattern and a list of possible transform",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 13,
      "token_count": 61,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 37\n\nKeywords are ranked from specific to generalUser: I know everybody laughed at me\u201cI\u201d is very general:I: (I *) -> (YOU SAY YOU 2)ELIZA: YOU SAY YOU KNOW EVERYBODY LAUGHED AT YOU\u201cEverybody\u201d is more specific and interestingEverybody: (Everybody *) -> (WHO IN PARTICULAR ARE YOU THINKING OF)ELIZA: WHO IN PARTICULAR ARE YOU THINKING OF?Implementation: keywords stored with their rank Everybody 5 (list of transformation rules)I 0 (list of transformation rules)\n\n## Page 38\n\n8CHAPTER24\u2022CHATBOTS&DIALOGUESYSTEMSMen are all alike.IN WHAT WAYThey\u2019re always bugging us about something or other.CAN YOU THINK OF A SPECIFIC EXAMPLEWell my boyfriend made me come here.YOUR BOYFRIEND MADE YOU COME HEREHe says I\u2019m depressed much of the time.I AM SORRY TO HEAR YOU ARE DEPRESSED....WHO ELSE IN YOUR FAMILY TAKES CARE OF YOU?My fatherYOUR FATHERYou are like my father in some ways.WHAT RESEMBLANCE DO YOU SEEYou are not very aggressive but I think you don\u2019t want me to notice that.WHAT MAKES YOU THINK I AM NOT AGGRESSIVEYou don\u2019t argue with me.WHY DO YOU THINK I DON\u2019T ARGUE WITH YOUYou are afraid of me.DOES IT PLEASE YOU TO BELIEVE I\u2019M AFRAID OF YOUMy father is afraid of everybody.WHAT ELSE COMES TO MIND WHEN YOU THINK OF YOUR FATHERBullies.DOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOURBOYFRIEND MADE YOU COME HEREFigure 24.4Sample ELIZA dialogue fromWeizenbaum (1966).functionELIZAGENERATOR(usersentence)returnsresponseFind the wordwinsentencethat has the highest keyword rankifwexistsChoose the highest ranked rulerforwthat matchessentenceresponse Apply the transform inrtosentenceifw= \u2018my\u2019future Apply a transformation from the \u2018memory\u2019 rule list tosentencePushfutureonto memory stackelse(no keyword applies)eitherresponse Apply the transform for the NONE keyword tosentenceorresponse Pop the top response from the memory stackreturn(response)Figure 24.5A simpli\ufb01ed sketch of the ELIZA algorithm. The power of the algorithmcomes from the particular transforms associated with each keyword.to some quite speci\ufb01c event or person\u201d. Therefore, ELIZA prefers to respond withthe pattern associated with the more speci\ufb01c keywordeverybody(implementing byjust assigning \u201ceverybody\u201d rank 5 and \u201cI\u201d rank 0 in the lexicon), whose rule thus\n\n## Page 39\n\nNONEPLEASE GO ONTHAT\u2019S VERY INTERESTINGI SEE\n\n## Page 40\n\nMemory(MEMORY MY(0 MY 0 = LETS DISCUSS FURTHER WHY YOUR 3)(0 MY 0 = EARLIER YOU SAID YOUR 3)(0 MY 0 = DOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR 3))Whenever \u201cMY\u201d is highest keyword\u25e6Randomly select a transform on the MEMORY list \u25e6Apply to sentence\u25e6Store on a (first-in-first-out) queueLater, if no keyword matches a sentence\u25e6Return the top of the MEMORY queue instead",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 14,
      "token_count": 707,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 39\n\nNONEPLEASE GO ONTHAT\u2019S VERY INTERESTINGI SEE\n\n## Page 40\n\nMemory(MEMORY MY(0 MY 0 = LETS DISCUSS FURTHER WHY YOUR 3)(0 MY 0 = EARLIER YOU SAID YOUR 3)(0 MY 0 = DOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR 3))Whenever \u201cMY\u201d is highest keyword\u25e6Randomly select a transform on the MEMORY list \u25e6Apply to sentence\u25e6Store on a (first-in-first-out) queueLater, if no keyword matches a sentence\u25e6Return the top of the MEMORY queue instead\n\n## Page 41\n\nEthical implications: Anthropomorphism and PrivacyPeople  became deeply emotionally involved with the programOne of Weizenbaum'sstaff asked him to leave the room when she talked with ELIZAWhen he suggested that he might want to store all the ELIZA conversations for later analysis, people immediately pointed out the privacy implications\u25e6Suggesting that they were having quite private conversations with ELIZA\u25e6Despite knowing that it was just software. \n\n## Page 42\n\nEthical implicationsIt worried Weizenbaumthat people confided in ELIZA\u2022Were people misled about how much computers understood?Turkle studied users of ELIZA and other systems\u2022Turkle has shown human face-to-face interaction is vital\u2022But people also develop specific relationships with artifacts\u2022Some users told her ELIZA was more like a kind of diary, a way to privately explore their thoughts.Importance of value-sensitive designJoseph Weizenbaum.1976.Computer Power and Human Reason: From Judgment to Calculation.WH Freeman.Sherry Turkle. 2011. Taking Things at Interface Value, chapter in Life on the Screen.Simon and Schuster.Sherry Turkle. 2007. Authenticity in the age of digital companions. Interaction Studies, 8(3), pp.501-517\n\n## Page 43\n\nPARRY: A computational model of schizophreniaAnother chatbot with a clinical psychology focus\u2022Colby, K. M., Weber, S., and Hilf, F. D. (1971). Artificial paranoia. Artificial Intelligence 2(1), 1\u201325. Used to study schizophreniaSame pattern-response structure as ElizaBut a much richer:\u25e6control structure \u25e6language understanding capabilities\u25e6model of mental state.\u25e6variables modeling levels of Anger, Fear, Mistrust43\n\n## Page 44\n\nAffect variablesFear(0-20)                    Anger(0-20)                  Mistrust(0-15)Start with all variables lowAfter each user turn\u25e6Eachuser statement can change Fear and Anger\u25e6E.g., Insults increases Anger, Flattery decreases Anger\u25e6Mentions of his delusions increase Fear\u25e6Else if nothing malevolent in input\u25e6Anger, Fear, Mistrust all drop\n\n## Page 45\n\nParry's responses depend on mental stateModifyAffect variablesexcessive fearEscapeexcessive angerHostilityInput mentions delusion topicquestionFearanswerUser Inputconditionresponse\u2026\n\n## Page 46\n\nPARRY passes the Turing test in 1972The first system to pass a version of the Turing test Psychiatrists couldn\u2019t distinguish interviews with PARRY from (text transcripts of) interviews with people diagnosed with paranoid schizophrenia\u2022Colby, K. M., Hilf, F. D., Weber, S., and Kraemer, H. C. (1972). Turing-like indistinguishability tests for the validation of a computer simulation of paranoid processes. Artificial Intelligence 3, 199\u2013221. 46\n\n## Page 47\n\nChatbots and Dialogue SystemsRule-based Chatbots: ELIZA and PARRY",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 15,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 44\n\nAffect variablesFear(0-20)                    Anger(0-20)                  Mistrust(0-15)Start with all variables lowAfter each user turn\u25e6Eachuser statement can change Fear and Anger\u25e6E.g., Insults increases Anger, Flattery decreases Anger\u25e6Mentions of his delusions increase Fear\u25e6Else if nothing malevolent in input\u25e6Anger, Fear, Mistrust all drop\n\n## Page 45\n\nParry's responses depend on mental stateModifyAffect variablesexcessive fearEscapeexcessive angerHostilityInput mentions delusion topicquestionFearanswerUser Inputconditionresponse\u2026\n\n## Page 46\n\nPARRY passes the Turing test in 1972The first system to pass a version of the Turing test Psychiatrists couldn\u2019t distinguish interviews with PARRY from (text transcripts of) interviews with people diagnosed with paranoid schizophrenia\u2022Colby, K. M., Hilf, F. D., Weber, S., and Kraemer, H. C. (1972). Turing-like indistinguishability tests for the validation of a computer simulation of paranoid processes. Artificial Intelligence 3, 199\u2013221. 46\n\n## Page 47\n\nChatbots and Dialogue SystemsRule-based Chatbots: ELIZA and PARRY\n\n## Page 48\n\nChatbots and Dialogue SystemsCorpus-based Chatbots\n\n## Page 49\n\nTwo architectures for corpus-based chabotsResponse by retrieval\u2022Use information retrieval to grab a response (that is appropriate to the context) from some corpusResponse by generation\u2022Use a language model or encoder-decoder to generate the response given the dialogue context49\n\n## Page 50\n\nCorpus-based chatbots require corpora\u2022Modern corpus-based chatbots are very data-intensive\u2022They commonly require hundreds of millions or billions of words50\n\n## Page 51\n\nWhat conversations to draw on?Transcripts of telephone conversations between volunteers\u2022Switchboard corpus of American English telephone conversationsMovie dialogue\u2022Various corpora of movie subtitlesHire human crowdworkersto have conversations\u2022Topical-Chat 11K crowdsourced conversations on 8 topics\u2022EMPATHETICDIALOGUES25K crowdsourced conversations grounded in a situation where a speaker was feeling a specific emotionPseudo-conversations from public posts on social media\u2022Drawn from Twitter, Reddit, Weibo (\u5fae\u535a), etc. \u2022Tend to be noisy; often used just as pre-training.Crucial to remove personally identifiable information (PII)",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 16,
      "token_count": 515,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 52",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 17,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Response by retrieval: classic IR method1.Given a user turn q, and a training corpus Cof conversation2.Find in C the turn rthat is most similar (tf-idfcosine) to q3.Say r10CHAPTER24\u2022CHATBOTS&DIALOGUESYSTEMStalk about knowledge provided to them. For example the Topical-Chat dataset has11K crowdsourced conversations spanning 8 broad topics(Gopalakrishnan et al.,2019), and the EMPATHETICDIALOGUESincludes 25K crowdsourced conversa-tions grounded in a speci\ufb01c situation where a speaker was feeling a speci\ufb01c emotion(Rashkin et al., 2019).All of these datasets, although large, don\u2019t reach the size of billions of words,and so many systems \ufb01rst pretrain on large datasets of pseudo-conversations drawnfrom Twitter(Ritter et al., 2010), Reddit(Roller et al., 2020), Weibo (\u00c6Z), andother social media platforms.Another common technique is to extract possible responses from knowledgesources (Wikipedia, news stories) so that a chatbot can tell stories or mention factsacquired in that way.Finally, once a chatbot has been put into practice, the turns that humans use torespond to the chatbot can be used as additional conversational data for training or\ufb01netuning. Here it\u2019s important to have con\ufb01dence metrics to make sure that theseturns come from conversations that are going well(Hancock et al., 2019). It\u2019s alsocrucial in these cases to remove personally identi\ufb01able information (PII); see Sec-tion24.6.1.Most corpus based chatbots produce their responses to a user\u2019s turn in contexteither byretrievalmethods (using information retrieval to grab a response fromsome corpus that is appropriate given the dialogue context) orgenerationmethods(using a language model or encoder-decoder to generate the response given the di-alogue context) In either case, systems mostly generate a single response turn thatis appropriate given the entire conversation so far (for conversations that are shortenough to \ufb01t into a single model\u2019s window). For this reason they are often calledresponse generationsystems. Corpus-based chatbot algorithms thus draw on algo-responsegenerationrithms for question answering systems, which similarly focus on single responseswhile ignoring longer-term conversational goals.Response by retrievalTheretrievalmethod of responding is to think of the user\u2019sturn as a queryq, and our job is to retrieve and repeat some appropriate turnras theresponse from a corpus of conversationsC. GenerallyCis the training set for thesystem, and we score each turn inCas a potential response to the contextqselectingthe highest-scoring one. The scoring metric is similarity: we choose therthat ismost similar toq, using any of the IR methods we saw in Section??. This can bedone using classic IR techniques to compute tf-idf models forCandq, choosing therthat has the highest tf-idf cosine withq:response(q,C)=argmaxr2Cq\u00b7r|q||r|(24.1)Or, we can use the neural IR techniques of Section??. The simplest of those is abi-encoder model, in which we train two separate encoders, one to encode the userquery and one to encode the candidate response, and use the dot product betweenthese two vectors as the score (Fig.24.6a). For example to implement this usingBERT, we would have two encoders BERTQand BERTRand we could represent thequery and candidate response as the[CLS]token of the respective",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 18,
      "token_count": 765,
      "chapter_title": ""
    }
  },
  {
    "content": "generationsystems. Corpus-based chatbot algorithms thus draw on algo-responsegenerationrithms for question answering systems, which similarly focus on single responseswhile ignoring longer-term conversational goals.Response by retrievalTheretrievalmethod of responding is to think of the user\u2019sturn as a queryq, and our job is to retrieve and repeat some appropriate turnras theresponse from a corpus of conversationsC. GenerallyCis the training set for thesystem, and we score each turn inCas a potential response to the contextqselectingthe highest-scoring one. The scoring metric is similarity: we choose therthat ismost similar toq, using any of the IR methods we saw in Section??. This can bedone using classic IR techniques to compute tf-idf models forCandq, choosing therthat has the highest tf-idf cosine withq:response(q,C)=argmaxr2Cq\u00b7r|q||r|(24.1)Or, we can use the neural IR techniques of Section??. The simplest of those is abi-encoder model, in which we train two separate encoders, one to encode the userquery and one to encode the candidate response, and use the dot product betweenthese two vectors as the score (Fig.24.6a). For example to implement this usingBERT, we would have two encoders BERTQand BERTRand we could represent thequery and candidate response as the[CLS]token of the respective encoders:hq=BERTQ(q)[CLS]hr=BERTR(r)[CLS]response(q,C)=argmaxr2Chq\u00b7hr(24.2)",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 19,
      "token_count": 337,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 53",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 20,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Response by retrieval: neural IR method1.Given a user turn q, and a training corpus Cof conversation2.Find in C the turn rthat is most similar (BERT dot product) to q3.Say r10CHAPTER24\u2022CHATBOTS&DIALOGUESYSTEMStalk about knowledge provided to them. For example the Topical-Chat dataset has11K crowdsourced conversations spanning 8 broad topics(Gopalakrishnan et al.,2019), and the EMPATHETICDIALOGUESincludes 25K crowdsourced conversa-tions grounded in a speci\ufb01c situation where a speaker was feeling a speci\ufb01c emotion(Rashkin et al., 2019).All of these datasets, although large, don\u2019t reach the size of billions of words,and so many systems \ufb01rst pretrain on large datasets of pseudo-conversations drawnfrom Twitter(Ritter et al., 2010), Reddit(Roller et al., 2020), Weibo (\u00c6Z), andother social media platforms.Another common technique is to extract possible responses from knowledgesources (Wikipedia, news stories) so that a chatbot can tell stories or mention factsacquired in that way.Finally, once a chatbot has been put into practice, the turns that humans use torespond to the chatbot can be used as additional conversational data for training or\ufb01netuning. Here it\u2019s important to have con\ufb01dence metrics to make sure that theseturns come from conversations that are going well(Hancock et al., 2019). It\u2019s alsocrucial in these cases to remove personally identi\ufb01able information (PII); see Sec-tion24.6.1.Most corpus based chatbots produce their responses to a user\u2019s turn in contexteither byretrievalmethods (using information retrieval to grab a response fromsome corpus that is appropriate given the dialogue context) orgenerationmethods(using a language model or encoder-decoder to generate the response given the di-alogue context) In either case, systems mostly generate a single response turn thatis appropriate given the entire conversation so far (for conversations that are shortenough to \ufb01t into a single model\u2019s window). For this reason they are often calledresponse generationsystems. Corpus-based chatbot algorithms thus draw on algo-responsegenerationrithms for question answering systems, which similarly focus on single responseswhile ignoring longer-term conversational goals.Response by retrievalTheretrievalmethod of responding is to think of the user\u2019sturn as a queryq, and our job is to retrieve and repeat some appropriate turnras theresponse from a corpus of conversationsC. GenerallyCis the training set for thesystem, and we score each turn inCas a potential response to the contextqselectingthe highest-scoring one. The scoring metric is similarity: we choose therthat ismost similar toq, using any of the IR methods we saw in Section??. This can bedone using classic IR techniques to compute tf-idf models forCandq, choosing therthat has the highest tf-idf cosine withq:response(q,C)=argmaxr2Cq\u00b7r|q||r|(24.1)Or, we can use the neural IR techniques of Section??. The simplest of those is abi-encoder model, in which we train two separate encoders, one to encode the userquery and one to encode the candidate response, and use the dot product betweenthese two vectors as the score (Fig.24.6a). For example to implement this usingBERT, we would have two encoders BERTQand BERTRand we could represent thequery and candidate response as the[CLS]token of the respective encoders:hq=BERTQ(q)[CLS]hr=BERTR(r)[CLS]response(q,C)=argmaxr2Chq\u00b7hr(24.2)",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 21,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 54",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 22,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Response by generationThink of response production as an encoder-decoder taskGenerate each token rtof the response by conditioning on the encoding of the entire query q and the response so far r1...rt\u2212124.2\u2022CHATBOTS11The IR-based approach can be extended in various ways, such as by using moresophisticated neural architectures(Humeau et al., 2020), or by using a longer contextfor the query than just the user\u2019s last turn, up to the whole preceding conversation.Information about the user or sentiment or other information can also play a role.Response by generationAn alternate way to use a corpus to generate dialogue isto think of response production as an encoder-decoder task\u2014 transducing from theuser\u2019s prior turn to the system\u2019s turn. We can think of this as a machine learningversion of ELIZA; the system learns from a corpus to transduce a question to ananswer.Ritter et al. (2011)proposed early on to think of response generation asa kind of translation, and this idea was generalized to the encoder-decoder modelroughly contemporaneously byShang et al. (2015),Vinyals and Le (2015), andSordoni et al. (2015).As we saw in Chapter 11, encoder decoder models generate each tokenrtof theresponse by conditioning on the encoding of the entire queryqand the response sofarr1...rt\u00001:\u02c6rt=argmaxw2VP(w|q,r1...rt\u00001)(24.3)Fig.24.6shows the intuition of the generator and retriever methods for responsegeneration. In the generator architecture, we normally include a longer context,forming the query not just from the user\u2019s turn but from the entire conversation-so-far. Fig.24.7shows an \ufb02eshed-out example.ENCODERqueryq1\u2026qnENCODERresponser1\u2026rndot-producthqhrENCODERq1\u2026qnDECODERr1\u2026<S>r1r2\u2026rn(a) Response by Retrieval (b) Response by GenerationFigure 24.6Two architectures for generating responses for a neural chatbot. In response byretrieval (a) we choose a response by using a \ufb01nding the turn in the corpus whose encodinghas the highest dot-product with the user\u2019s turn. In response by generation (b) we use anencoder-decoder to generate the response.That is quite an accomplishment and you should be proud!ENCODERDECODER[U:] I \ufb01nally got promoted today at work!  [S:] Congrats! That\u2019s great! [U:] Thank you! I\u2019ve been trying to get it for a while now!Figure 24.7Example of encoder decoder for dialogue response generation; the encoder sees the entire dia-logue context.A number of modi\ufb01cations are required to the basic encoder-decoder model toadapt it for the task of response generation. For example basic encoder-decoder24.2\u2022CHATBOTS11The IR-based approach can be extended in various ways, such as by using moresophisticated neural architectures(Humeau et al., 2020), or by using a longer contextfor the query than just the user\u2019s last turn, up to the whole preceding conversation.Information about the user or sentiment or other information can also play a role.Response by generationAn alternate way to use a corpus to generate dialogue isto think of response production as an encoder-decoder task\u2014 transducing from theuser\u2019s prior turn to the system\u2019s turn. We can think of this as a machine learningversion of ELIZA; the system learns from a corpus to transduce a question to ananswer.Ritter et al. (2011)proposed early on to think of response generation asa kind of translation, and this",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 23,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "we use anencoder-decoder to generate the response.That is quite an accomplishment and you should be proud!ENCODERDECODER[U:] I \ufb01nally got promoted today at work!  [S:] Congrats! That\u2019s great! [U:] Thank you! I\u2019ve been trying to get it for a while now!Figure 24.7Example of encoder decoder for dialogue response generation; the encoder sees the entire dia-logue context.A number of modi\ufb01cations are required to the basic encoder-decoder model toadapt it for the task of response generation. For example basic encoder-decoder24.2\u2022CHATBOTS11The IR-based approach can be extended in various ways, such as by using moresophisticated neural architectures(Humeau et al., 2020), or by using a longer contextfor the query than just the user\u2019s last turn, up to the whole preceding conversation.Information about the user or sentiment or other information can also play a role.Response by generationAn alternate way to use a corpus to generate dialogue isto think of response production as an encoder-decoder task\u2014 transducing from theuser\u2019s prior turn to the system\u2019s turn. We can think of this as a machine learningversion of ELIZA; the system learns from a corpus to transduce a question to ananswer.Ritter et al. (2011)proposed early on to think of response generation asa kind of translation, and this idea was generalized to the encoder-decoder modelroughly contemporaneously byShang et al. (2015),Vinyals and Le (2015), andSordoni et al. (2015).As we saw in Chapter 11, encoder decoder models generate each tokenrtof theresponse by conditioning on the encoding of the entire queryqand the response sofarr1...rt\u00001:\u02c6rt=argmaxw2VP(w|q,r1...rt\u00001)(24.3)Fig.24.6shows the intuition of the generator and retriever methods for responsegeneration. In the generator architecture, we normally include a longer context,forming the query not just from the user\u2019s turn but from the entire conversation-so-far. Fig.24.7shows an \ufb02eshed-out example.ENCODERqueryq1\u2026qnENCODERresponser1\u2026rndot-producthqhrENCODERq1\u2026qnDECODERr1\u2026<S>r1r2\u2026rn(a) Response by Retrieval (b) Response by GenerationFigure 24.6Two architectures for generating responses for a neural chatbot. In response byretrieval (a) we choose a response by using a \ufb01nding the turn in the corpus whose encodinghas the highest dot-product with the user\u2019s turn. In response by generation (b) we use anencoder-decoder to generate the response.That is quite an accomplishment and you should be proud!ENCODERDECODER[U:] I \ufb01nally got promoted today at work!  [S:] Congrats! That\u2019s great! [U:] Thank you! I\u2019ve been trying to get it for a while now!Figure 24.7Example of encoder decoder for dialogue response generation; the encoder sees the entire dia-logue context.A number of modi\ufb01cations are required to the basic encoder-decoder model toadapt it for the task of response generation. For example basic encoder-decoder",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 24,
      "token_count": 711,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 55\n\nResponse by generation24.2\u2022CHATBOTS11The IR-based approach can be extended in various ways, such as by using moresophisticated neural architectures(Humeau et al., 2020), or by using a longer contextfor the query than just the user\u2019s last turn, up to the whole preceding conversation.Information about the user or sentiment or other information can also play a role.Response by generationAn alternate way to use a corpus to generate dialogue isto think of response production as an encoder-decoder task\u2014 transducing from theuser\u2019s prior turn to the system\u2019s turn. We can think of this as a machine learningversion of ELIZA; the system learns from a corpus to transduce a question to ananswer.Ritter et al. (2011)proposed early on to think of response generation asa kind of translation, and this idea was generalized to the encoder-decoder modelroughly contemporaneously byShang et al. (2015),Vinyals and Le (2015), andSordoni et al. (2015).As we saw in Chapter 11, encoder decoder models generate each tokenrtof theresponse by conditioning on the encoding of the entire queryqand the response sofarr1...rt\u00001:\u02c6rt=argmaxw2VP(w|q,r1...rt\u00001)(24.3)Fig.24.6shows the intuition of the generator and retriever methods for responsegeneration. In the generator architecture, we normally include a longer context,forming the query not just from the user\u2019s turn but from the entire conversation-so-far. Fig.24.7shows an \ufb02eshed-out example.ENCODERqueryq1\u2026qnENCODERresponser1\u2026rndot-producthqhrENCODERq1\u2026qnDECODERr1\u2026<S>r1r2\u2026rn(a) Response by Retrieval (b) Response by GenerationFigure 24.6Two architectures for generating responses for a neural chatbot. In response byretrieval (a) we choose a response by using a \ufb01nding the turn in the corpus whose encodinghas the highest dot-product with the user\u2019s turn. In response by generation (b) we use anencoder-decoder to generate the response.That is quite an accomplishment and you should be proud!ENCODERDECODER[U:] I \ufb01nally got promoted today at work!  [S:] Congrats! That\u2019s great! [U:] Thank you! I\u2019ve been trying to get it for a while now!Figure 24.7Example of encoder decoder for dialogue response generation; the encoder sees the entire dia-logue context.A number of modi\ufb01cations are required to the basic encoder-decoder model toadapt it for the task of response generation. For example basic encoder-decoder\n\n## Page 56\n\nResponse by generationAlternative approach: fine-tune a large language model  on conversational dataThe Chirpy Cardinal system (Paranjape et al., 2020):\u2022fine-tunes GPT-2 \u2022on the EMPATHETICDIALOGUESdataset (Rashkinet al., 2019) \n\n## Page 57\n\nP. G .  Wo d e h o u s e  p r e d i c t s  n e u r a l  c h a t b o t s\u201cWhat ho!\u201d I said.\u201cWhat ho!\u201d said Motty.\u201cWhat ho! What ho!\u201d\u201cWhat ho! What ho! What ho!\u201dAfter that it seemed rather difficult to go on with the conversation.\u201d Wodehouse My Man Jeeves, 1919",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 25,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 56\n\nResponse by generationAlternative approach: fine-tune a large language model  on conversational dataThe Chirpy Cardinal system (Paranjape et al., 2020):\u2022fine-tunes GPT-2 \u2022on the EMPATHETICDIALOGUESdataset (Rashkinet al., 2019) \n\n## Page 57\n\nP. G .  Wo d e h o u s e  p r e d i c t s  n e u r a l  c h a t b o t s\u201cWhat ho!\u201d I said.\u201cWhat ho!\u201d said Motty.\u201cWhat ho! What ho!\u201d\u201cWhat ho! What ho! What ho!\u201dAfter that it seemed rather difficult to go on with the conversation.\u201d Wodehouse My Man Jeeves, 1919\n\n## Page 58\n\nOngoing research problem:Neural chatbots can get repetitive and boringA: Where are you going?B: I\u2019m going to the restroom.A: See you later.B: See you later.A: See you later.B: See you later.A: How old are you?B: I\u2019m 16.A: 16?B: I don\u2019t know what you are talking about.A: You don\u2019t know what you are saying.B: I don\u2019t know what you are talking about.A: You don\u2019t know what you are saying.\n\n## Page 59\n\nResponse by retrieving and refining knowledge Can generate responses from informative text rather than dialogue\u2022To respond to turns like \u201cTell me something about Beijing\u201d\u2022XiaoIcecollects sentences from public lectures and news articles.\u2022And searches them using IR based on query expansion from user\u2019s turn\u2022Can augment encoder-decoder model\u2022use IR to retrieve passages from Wikipedia \u2022concatenate each Wikipedia sentence to the dialogue context with a separator token. \u2022Give as encoder context to the encoder-decoder model, which learns to incorporate text into its response\n\n## Page 60\n\nHybrid ArchitecturesChirpy Cardinal (Paranjape et al., 2020) response generation from a series of different generators:\u2022GPT-2 finetuned on EmpatheticDialogues\u2022GPT-2 finetuned to paraphrase content from Wikipedia\u2022Rule-based movie or music generators that produce scripted conversation about a movie or a musician\u2022asking the user\u2019s opinion about a movie, \u2022giving a fun fact, \u2022asking the user their opinion on an actor in the movie.",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 26,
      "token_count": 503,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 61",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 27,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Chirpy Cardinal (Paranjape et al. 2020)24.3\u2022GUS: SIMPLEFRAME-BASEDDIALOGUESYSTEMS13linker speci\ufb01es what entity is currently being discussed. Dialogue act classi\ufb01cationis used to detect when the user is asking a question or giving an af\ufb01rmative versusnegative response.Bot responses are generated by a series of response generators. Some responsegenerators use \ufb01ne-tuned neural language models: a GPT-2(Radford et al., 2019)language model \ufb01ne-tuned on the EmpatheticDialogues dataset, and a separately\ufb01ne-tuned GPT-2 language model that learns in \ufb01ne-tuning to paraphrase contentfrom Wikipedia in response to questions.Other response generators are closer to rule-based; the movie or music genera-tors produce scripted conversation about a movie or a musician, such as asking asksthe user\u2019s opinion about a movie, giving a fun fact, asks the user their opinion on anactor in the movie, and so on. These generators use regexes and sentiment classi\ufb01ersto classify user responses and handwritten templates to generate bot utterances.Each response generators can specify a priority, and a ranker is used to passcontrol between response generators. Fig.24.8shows part of a conversation.# Utterance Entity Commentary1User:let\u2019s chat NoneBot:Hi, this is an Alexa Prize Socialbot. I\u2019d love toget to know you a bit better before we chat! Is it allright if I ask for your name?[ Launch ]NoneLaunchgivesFORCE_STARTresponse to be-gin the conversation.2User:my name is chris NoneBot:Well it\u2019s nice to meet you, Chris! I\u2019m excitedto chat with you today.[ Launch ]I hope your after-noon is going well. What are your plans for the restof today?[ Neural Chat ]NoneLaunchgives aSTRONG_CONTINUEre-sponse that needs a prompt, which is suppliedbyNeural ChatwithFORCE_STARTpriority(scripted transition).3User:hang out with my friends NoneBot:That sounds great, I love spending time withfriends. What will you do together?[ Neural Chat ]NoneNeural Chatgenerates a GPT-2 response withSTRONG_CONTINUEpriority.4User:maybe watch a movie NoneBot:Me too! I love watching movies; I get to learnso much about what the world is like outside ofthe cloud! Have you seen any movies recently?[ Movies ]FilmMoviesdetects the \u2018movie\u2019 keyword in theuser\u2019s utterance and gives aFORCE_STARTresponse (overridingNeural Chat), settingcurrent entity toFilm.5User:i saw the matrixThe MatrixTheEntity LinkerandEntity Trackeriden-tifyThe Matrixas the current entity.Bot:Nice! Did you like The Matrix?[ Movies ]The Matrix6User:i loved it neo is amazingThe MatrixBot:Oooh, yeah, I agree. Hey, isn\u2019t Keanu Reevesin that movie? What do you think about KeanuReeves?[ Movies ]KeanuReevesMoviessets current entity toKeanu Reeves.7User:i want to talk about something else NoneNavigational Intentdetectsnegative intent, soEntity Trackerrejects the current entity.Bot:OK, no problem.[ Movies ]There\u2019s actuallysomething else I wanted to ask you about. What\u2019syour favorite animal?[ Categories ]AnimalMoviesacknowledges and asks for a prompt.Categoriesgives aGENERICprompt.8User:i love cats CatEntity LinkerandEntity TrackeridentifyCatas current entity.Bot:Good to hear you like cats. I have to be honestthough, I\u2019m not a big fan of cats. I feel like catsdon\u2019t respect my personal space, but I would love tohear why you like cats?[",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 28,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "I get to learnso much about what the world is like outside ofthe cloud! Have you seen any movies recently?[ Movies ]FilmMoviesdetects the \u2018movie\u2019 keyword in theuser\u2019s utterance and gives aFORCE_STARTresponse (overridingNeural Chat), settingcurrent entity toFilm.5User:i saw the matrixThe MatrixTheEntity LinkerandEntity Trackeriden-tifyThe Matrixas the current entity.Bot:Nice! Did you like The Matrix?[ Movies ]The Matrix6User:i loved it neo is amazingThe MatrixBot:Oooh, yeah, I agree. Hey, isn\u2019t Keanu Reevesin that movie? What do you think about KeanuReeves?[ Movies ]KeanuReevesMoviessets current entity toKeanu Reeves.7User:i want to talk about something else NoneNavigational Intentdetectsnegative intent, soEntity Trackerrejects the current entity.Bot:OK, no problem.[ Movies ]There\u2019s actuallysomething else I wanted to ask you about. What\u2019syour favorite animal?[ Categories ]AnimalMoviesacknowledges and asks for a prompt.Categoriesgives aGENERICprompt.8User:i love cats CatEntity LinkerandEntity TrackeridentifyCatas current entity.Bot:Good to hear you like cats. I have to be honestthough, I\u2019m not a big fan of cats. I feel like catsdon\u2019t respect my personal space, but I would love tohear why you like cats?[ Opinion ]CatOpiniongives aCAN_STARTresponse aboutcats.9User:hmm i love cats because they are \ufb02uffy CatBot:That make sense. Now that I think aboutit, one good reason to like cats is that they purrand I de\ufb01nitely need that kind of positive feed-back.[ Opinion ]Wanna know something interest-ing about cat?[ Wiki ]CatOpinion\ufb01nishes talking.Wikigives aCUR-RENT_TOPICprompt aboutCat.10User:you are not very smart CatBot:I know you feel frustrated. I\u2019m always tryingto get better.[ Offensive User ]I\u2019ve been listeningto some new music today and I wanted to chat aboutinstruments. If you were a musical instrument whichone would you be?[ Music ]Musical in-strumentOffensive Usergives aFORCE_STARTre-sponse to the critical utterance, then[ Music ]provides aGENERICprompt to steer the con-versation in a new direction.11User:what do you \ufb01nd interesting None The user gives an unexpected response.Bot:I like the violin, but I\u2019m more of a classicalmusic player.[ Neural Fallback ]I remember youmentioned Neo. Would you like to talk more aboutit?[ Wiki ]Neo (TheMatrix)Neural Fallbackgives a GPT-2-generated re-sponse, which is selected as no other RG (ex-ceptFallback) has a response.Wikigives aCONTEXTUALprompt.12User:sureNeo (TheMatrix)Bot:Neo has got into so many adventures! Whatare some adventures that you\u2019ve enjoyed?[ Wiki ]Neo (TheMatrix)Wikiasks an open-ended question (speci\ufb01c tomovie characters) aboutNeo (The Matrix).13User:morpheus teaching jujitsu to neoNeo (TheMatrix)Bot:I liked that Neo and Trinity were able to res-cue Morpheus from a building protected by armedguards and agents. Morpheus has done so manyamazing things! What are some of your favoriteMorpheus moments?[ Wiki ]Morpheus(TheMatrix)Wikigives a response (generated by the GPT-2 conversational paraphrasing model), sets thecurrent entity toMorpheus (The Matrix), andasks another open-ended question.14User:i want to stop talking None We detectstop intent; the conversation ends.Table 1: An example dialogue.",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 29,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "a new direction.11User:what do you \ufb01nd interesting None The user gives an unexpected response.Bot:I like the violin, but I\u2019m more of a classicalmusic player.[ Neural Fallback ]I remember youmentioned Neo. Would you like to talk more aboutit?[ Wiki ]Neo (TheMatrix)Neural Fallbackgives a GPT-2-generated re-sponse, which is selected as no other RG (ex-ceptFallback) has a response.Wikigives aCONTEXTUALprompt.12User:sureNeo (TheMatrix)Bot:Neo has got into so many adventures! Whatare some adventures that you\u2019ve enjoyed?[ Wiki ]Neo (TheMatrix)Wikiasks an open-ended question (speci\ufb01c tomovie characters) aboutNeo (The Matrix).13User:morpheus teaching jujitsu to neoNeo (TheMatrix)Bot:I liked that Neo and Trinity were able to res-cue Morpheus from a building protected by armedguards and agents. Morpheus has done so manyamazing things! What are some of your favoriteMorpheus moments?[ Wiki ]Morpheus(TheMatrix)Wikigives a response (generated by the GPT-2 conversational paraphrasing model), sets thecurrent entity toMorpheus (The Matrix), andasks another open-ended question.14User:i want to stop talking None We detectstop intent; the conversation ends.Table 1: An example dialogue. To respect users\u2019 privacy, this is not a real user conversation.5Figure 24.8Part of a conversation with Chirpy Cardinal(Paranjape et al., 2020).24.3 GUS: Simple Frame-based Dialogue SystemsWe turn now totask-based dialogue, in which a dialogue system has the goal ofhelping a user solve some task like making an airplane reservation or buying a prod-uct. In this section we introduce the very simpleGUSarchitecture for task-basedGUSdialogue. Although it is quite old, (it was \ufb01rst introduced in 1977 in the in\ufb02uen-tial GUS system for travel planning(Bobrow et al., 1977)) the GUS architecturehas been astonishingly long-lived and underlies most if not all modern commercialdigital assistants. Figure24.9shows a conversation with the original GUS system.All modern task-based dialogue systems, whether the simple GUS architecturewe describe here, or the more sophisticateddialogue statearchitectures we turn todialogue statein the following section, are based aroundframes. A frame is a kind of knowledgeframestructure representing the kinds of intentions the system can extract from user sen-tences, and consists of a collection ofslots, each of which can take a set of possibleslot",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 30,
      "token_count": 569,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 62\n\nChatbots: pro and conPro:\u25e6Fun\u25e6Good for narrow, scriptable applicationsCons:\u25e6They don't really understand\u25e6Giving the appearance of understanding may be problematic\u25e6Rule-based chatbotsare expensive and brittle\u25e6IR-based chatbotscan only mirror training data\u25e6We'll discuss later the case of Microsoft Tay\u25e6(or, Garbage-in, Garbage-out)Next steps: integrating chatbot ability into frame-based agents\n\n## Page 63\n\nChatbots and Dialogue SystemsCorpus-based Chatbots\n\n## Page 64\n\nChatbots and Dialogue SystemsThe Frame-based (\"GUS\") Dialogue Architecture\n\n## Page 65\n\nFrame-based dialogue agentsSometimes called \"task-based dialogue agents\"\u2022Systems that have the goal of helping a user solve a task like making a travel reservation or buying a productArchitecture: \u2022First proposed in the GUS system of 1977\u2022A knowledge structure representing user intentions\u2022One or more frames (eachconsistingofslots withvalues)\n\n## Page 66\n\nThe FrameA set of slots, to be filled with information of a given typeEach associated with a questionto the userSometimes called a domain ontologySlotTypeQuestionORIGINcity\"What city are you leaving from?DESTcity\"Where are you going?DEP DATE date\"What day would you like to leave?DEP TIME time\"What time would you like to leave?AIRLINEline\"What is your preferred airline?\n\n## Page 67\n\nTwo basic architecturesThe GUSarchitecture\u2022Sometimes just called \"frame-based\" architecture\u2022Over 40 years old, but still used in most industrial task-based dialogue agents\u2022Bobrow, Daniel G., Ronald M. Kaplan, Martin Kay, Donald A. Norman, Henry Thompson, and Terry Winograd. 1977. \"GUS, a frame-driven dialog system.\"Artificial Intelligence8, 2:155-173.The dialogue-statearchitecture\u2022Extension of GUS\u2022More common in research systems\u2022Some aspects making their way into industrial systems",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 31,
      "token_count": 414,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 68\n\n14CHAPTER24\u2022CHATBOTS&DIALOGUESYSTEMSvalues. Together this set of frames is sometimes called adomain ontology.GUS: Hello. My name is GUS. I can help you plan a simple trip by air. Where do youwant to go ?Client: I want to go to San Diego on May 28GUS: What time do you want to leave?Client: I must be in San Diego before 10 amGUS: Would you like P.S.A. \ufb02ight 102 that arrives at 9.15 am ?Client: What is the next \ufb02ight ?GUS: Air California \ufb02ight 310 that leaves at 8.30am and arrives at 10.00amClient: I\u2019ll take the \ufb01rst oneGUS: In what name should I make a reservation, please?Client: Dan BobrowGUS: I have con\ufb01rmed the following \ufb02ight: P.S.A. \ufb02ight 102 on Wednesday May 28from San Jose to San Diego that leaves at 7.30 am and arrives at 9.15 am.GUS: What date do you want to return on ?Client: On Friday in the evening.GUS: Would you like the \ufb02ight that leaves at 7.45 pm ?Client: That\u2019s \ufb01ne.GUS: I have con\ufb01rmed the following \ufb02ight: P.S.A. \ufb02ight 307 on Friday May 30 fromSan Diego to San Jose that leaves at 7.45 pm and arrives at 9.30 pm Thank youfor calling. GoodbyeFigure 24.9The travel domain: A transcript of an actual dialogue with the GUS system ofBobrow et al. (1977). P.S.A. and Air California were airlines of that period.The set of slots in a task-based dialogue frame speci\ufb01es what the system needsto know, and the \ufb01ller of each slot is constrained to values of a particular semantictype. In the travel domain, for example, a slot might be of type city (hence take onvalues likeSan Francisco, orHong Kong) or of type date, airline, or time.SlotTypeQuestion TemplateORIGIN CITYcity\u201cFrom what city are you leaving?\u201dDESTINATION CITYcity\u201cWhere are you going?\u201dDEPARTURE TIMEtime\u201cWhen would you like to leave?\u201dDEPARTURE DATEdate\u201cWhat day would you like to leave?\u201dARRIV AL TIMEtime\u201cWhen do you want to arrive?\u201dARRIV AL DATEdate\u201cWhat day would you like to arrive?\u201dFigure 24.10A frame in a frame-based dialogue system, showing the type of each slot anda question used to \ufb01ll the slot.Types in GUS, as in modern frame-based dialogue agents, have hierarchicalstructure; for example thedatetype in GUS is itself a frame with slots with typeslikeintegeror members of sets of weekday names:DATEMONTH:NAME YEAR:INTEGER DAY:(BOUNDED-INTEGER 1 31)WEEKDAY:(MEMBER (Sunday Monday Tuesday WednesdayThursday Friday Saturday))24.3.1 Control structure for frame-based dialogueThe control architecture for frame-based dialogue systems, used in various formsin modern systems like Apple\u2019s Siri, Amazon\u2019s Alexa, and the Google Assistant, isdesigned around the frame. The system\u2019s goal is to \ufb01ll the slots in the frame with the\n\n## Page 69\n\nControl structure for GUS frame architectureSystem asks questions of user, filling any slots that user specifiesUser might fill many slots at a time:\u2022I want a flight from San Francisco to Denver one wayleaving after five p.m. on Tuesday. When frame is filled, do database query",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 32,
      "token_count": 781,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 69\n\nControl structure for GUS frame architectureSystem asks questions of user, filling any slots that user specifiesUser might fill many slots at a time:\u2022I want a flight from San Francisco to Denver one wayleaving after five p.m. on Tuesday. When frame is filled, do database query\n\n## Page 70\n\nGUS slots have condition-action rules attachedSome rules attached to the DESTINATION slot for the plane booking frame1.Once the user has specified the destination\u2022Enter that city as the default StayLocationfor the hotel booking frame. 2.Once the user has specified DESTINATION DAY for a short trip \u2022Automatically copy as ARRIVAL DAY . \n\n## Page 71\n\nGUS systems have multiple framesFrames like:\u2022Car or hotel reservations\u2022General route information\u2022Which airlines fly from Boston to San Francisco?, \u2022Information about airfare practices \u2022Do I have to stay a specific number of days to get a decent airfare?). Frame detection:\u2022System must detect which slot of which frame user is filling\u2022And switch dialogue control to that frame. \n\n## Page 72\n\nGUS: Natural Language Understanding for filling dialog slots1.Domain classificationAsking weather? Booking a flight? Programming alarm clock?2.Intent DeterminationFind a Movie, Show Flight, Remove Calendar Appt3.Slot FillingExtract the actual slots and fillers",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 33,
      "token_count": 276,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 71\n\nGUS systems have multiple framesFrames like:\u2022Car or hotel reservations\u2022General route information\u2022Which airlines fly from Boston to San Francisco?, \u2022Information about airfare practices \u2022Do I have to stay a specific number of days to get a decent airfare?). Frame detection:\u2022System must detect which slot of which frame user is filling\u2022And switch dialogue control to that frame. \n\n## Page 72\n\nGUS: Natural Language Understanding for filling dialog slots1.Domain classificationAsking weather? Booking a flight? Programming alarm clock?2.Intent DeterminationFind a Movie, Show Flight, Remove Calendar Appt3.Slot FillingExtract the actual slots and fillers\n\n## Page 73\n\nNatural Language Understanding for filling slotsShow me morning flights from Boston to SF on Tuesday.29.2\u2022FRAMEBASEDDIALOGAGENTS1329.2.2 Natural language understanding for \ufb01lling slotsThe goal of the natural language understanding component is to extract three thingsfrom the user\u2019s utterance. The \ufb01rst task isdomain classi\ufb01cation: is this user fordomainclassi\ufb01cationexample talking about airlines, programming an alarm clocks, or dealing with theircalendar? Of course this 1-of-n classi\ufb01cation tasks is unnecessary for single-domainsystems that are focused on, say, only calendar management, but multi-domain di-alog systems are the modern standard. The second is userintent determination:intentdeterminationwhat general task or goal is the user trying to accomplish? For example the taskcould be to Find a Movie, or Show a Flight, or Remove a Calendar Appointment.Finally, we need to doslot \ufb01lling: extract the particular slots and \ufb01llers that the userslot \ufb01llingintends the system to understand from their utterance with respect to their intent.From a user utterance like this one:Show me morning flights from Boston to San Francisco on Tuesdaya system might want to build a representation like:DOMAIN: AIR-TRAVELINTENT: SHOW-FLIGHTSORIGIN-CITY: BostonORIGIN-DATE: TuesdayORIGIN-TIME: morningDEST-CITY: San Franciscowhile an utterance likeWake me tomorrow at 6should give an intent like this:DOMAIN: ALARM-CLOCKINTENT: SET-ALARMTIME: 2017-07-01 0600-0800The task of slot-\ufb01lling, and the simpler tasks of domain and intent classi\ufb01cation,are special cases of the task of semantic parsing discussed in Chapter??. Dialogueagents can thus extract slots, domains, and intents from user utterances by applyingany of the semantic parsing approaches discussed in that chapter.The method used in the original GUS system, and still quite common in indus-trial applications, is to use hand-written rules, often as part of the condition-actionrules attached to slots or concepts.For example we might just de\ufb01ne a regular expression consisting of a set stringsthat map to the SET-ALARM intent:wake me (up) | set (the|an) alarm | get me upWe can build more complex automata that instantiate sets of rules like thosediscussed in Chapter 20, for example extracting a slot \ufb01ller by turning a stringlikeMonday at 2pminto an object of typedatewith parameters (DAY, MONTH,YEAR, HOURS, MINUTES).Rule-based systems can be even implemented with full grammars. Research sys-tems like the Phoenix system(Ward and Issar, 1994)consists of large hand-designedsemantic grammarswith thousands of rules. A semantic grammar is a context-freesemanticgrammargrammar in which the left-hand side of each rule corresponds to the semantic entitiesbeing expressed (i.e., the slot names) as in the following fragment:",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 34,
      "token_count": 795,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 74\n\nNatural Language Understanding for filling slotsWake me tomorrow at six.29.2\u2022FRAMEBASEDDIALOGAGENTS1329.2.2 Natural language understanding for \ufb01lling slotsThe goal of the natural language understanding component is to extract three thingsfrom the user\u2019s utterance. The \ufb01rst task isdomain classi\ufb01cation: is this user fordomainclassi\ufb01cationexample talking about airlines, programming an alarm clocks, or dealing with theircalendar? Of course this 1-of-n classi\ufb01cation tasks is unnecessary for single-domainsystems that are focused on, say, only calendar management, but multi-domain di-alog systems are the modern standard. The second is userintent determination:intentdeterminationwhat general task or goal is the user trying to accomplish? For example the taskcould be to Find a Movie, or Show a Flight, or Remove a Calendar Appointment.Finally, we need to doslot \ufb01lling: extract the particular slots and \ufb01llers that the userslot \ufb01llingintends the system to understand from their utterance with respect to their intent.From a user utterance like this one:Show me morning flights from Boston to San Francisco on Tuesdaya system might want to build a representation like:DOMAIN: AIR-TRAVELINTENT: SHOW-FLIGHTSORIGIN-CITY: BostonORIGIN-DATE: TuesdayORIGIN-TIME: morningDEST-CITY: San Franciscowhile an utterance likeWake me tomorrow at 6should give an intent like this:DOMAIN: ALARM-CLOCKINTENT: SET-ALARMTIME: 2017-07-01 0600-0800The task of slot-\ufb01lling, and the simpler tasks of domain and intent classi\ufb01cation,are special cases of the task of semantic parsing discussed in Chapter??. Dialogueagents can thus extract slots, domains, and intents from user utterances by applyingany of the semantic parsing approaches discussed in that chapter.The method used in the original GUS system, and still quite common in indus-trial applications, is to use hand-written rules, often as part of the condition-actionrules attached to slots or concepts.For example we might just de\ufb01ne a regular expression consisting of a set stringsthat map to the SET-ALARM intent:wake me (up) | set (the|an) alarm | get me upWe can build more complex automata that instantiate sets of rules like thosediscussed in Chapter 20, for example extracting a slot \ufb01ller by turning a stringlikeMonday at 2pminto an object of typedatewith parameters (DAY, MONTH,YEAR, HOURS, MINUTES).Rule-based systems can be even implemented with full grammars. Research sys-tems like the Phoenix system(Ward and Issar, 1994)consists of large hand-designedsemantic grammarswith thousands of rules. A semantic grammar is a context-freesemanticgrammargrammar in which the left-hand side of each rule corresponds to the semantic entitiesbeing expressed (i.e., the slot names) as in the following fragment:\n\n## Page 75\n\nHow to fill slots?(1) Rule-based Slot-fillingWrite regular expressions or grammar rulesWake me (up) | set (the|an) alarm | get me upDo text normalization\n\n## Page 76\n\nGenerating responses: template-based generationA template is a pre-built response stringTemplates can be fixed:\"Hello, how can I help you?\"Or have variables:\"What time do you want to leave CITY-ORIG?\"\"Will you return to CITY-ORIG from CITY-DEST?\"",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 35,
      "token_count": 756,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 75\n\nHow to fill slots?(1) Rule-based Slot-fillingWrite regular expressions or grammar rulesWake me (up) | set (the|an) alarm | get me upDo text normalization\n\n## Page 76\n\nGenerating responses: template-based generationA template is a pre-built response stringTemplates can be fixed:\"Hello, how can I help you?\"Or have variables:\"What time do you want to leave CITY-ORIG?\"\"Will you return to CITY-ORIG from CITY-DEST?\"\n\n## Page 77\n\nSummary: simple frame-based architectureLike many rule-based approaches\u2022Positives:\u2022High precision\u2022Can provide coverage if the domain is narrow\u2022Negatives:\u2022Can be expensive and slow to create rules\u2022Can suffer from recall problems\n\n## Page 78\n\nChatbots and Dialogue SystemsThe Frame-based (\"GUS\") Dialogue Architecture\n\n## Page 79\n\nChatbots and Dialogue SystemsThe Dialogue-State Architecture\n\n## Page 80\n\nDialogue-State or Belief-State ArchitectureA more sophisticated version of the frame-based architecture\u2022Has dialogue acts, more ML, better generationThe basis for modern research systemsSlowly making its way into industrial systems\u2022Some aspects (ML for slot-understanding) already widely used industrially",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 36,
      "token_count": 253,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 79\n\nChatbots and Dialogue SystemsThe Dialogue-State Architecture\n\n## Page 80\n\nDialogue-State or Belief-State ArchitectureA more sophisticated version of the frame-based architecture\u2022Has dialogue acts, more ML, better generationThe basis for modern research systemsSlowly making its way into industrial systems\u2022Some aspects (ML for slot-understanding) already widely used industrially\n\n## Page 81\n\nDIALOGSTATETRACKINGOVERVIEW\nLEAVING FROM DOWNTOWNLEAVING AT ONE P MARRIVING AT ONE P M0.60.20.1{ from: downtown }{ depart-time: 1300 }{ arrive-time: 1300 }0.50.30.1\nfrom:        CMUto:          airportdepart-time: 1300confirmed:   noscore:       0.10from:        CMUto:          airportdepart-time: 1300confirmed:   noscore:       0.15from:        downtownto:          airportdepart-time: --confirmed:   noscore:       0.65Automatic Speech Recognition (ASR)Spoken Language Understanding (SLU)Dialog State Tracker (DST)\nDialog Policyact:  confirmfrom: downtownFROM DOWNTOWN, IS THAT RIGHT?Natural Language Generation (NLG)Text to Speech (TTS)\nFigure 1: Principal components of a spoken dialog system.The topic of this paper is thedialog state tracker(DST). The DST takes as input all of the dialoghistory so far, and outputs its estimate of the currentdialog state\u2013 for example, in a restaurantinformation system, the dialog state might indicate the user\u2019s preferred price range and cuisine,what information they are seeking such as the phone number of a restaurant, and which conceptshave been stated vs. con\ufb01rmed. Dialog state tracking is dif\ufb01cult because ASR and SLU errors arecommon, and can cause the system to misunderstand the user. At the same time, state tracking iscrucial because the dialog policy relies on the estimated dialog state to choose actions \u2013 for example,which restaurants to suggest.In the literature, numerous methods for dialog state tracking have been proposed. These arecovered in detail in Section 3; illustrative examples include hand-crafted rules (Larsson and Traum,2000; Bohus and Rudnicky, 2003), heuristic scores (Higashinaka et al., 2003), Bayesian networks(Paek and Horvitz, 2000; Williams and Young, 2007), and discriminative models (Bohus and Rud-nicky, 2006). Techniques have been \ufb01elded which scale to realistically sized dialog problems andoperate in real time (Young et al., 2010; Thomson and Young, 2010; Williams, 2010; Mehta et al.,2010). In end-to-end dialog systems, dialog state tracking has been shown to improve overall systemperformance (Young et al., 2010; Thomson and Young, 2010).Despite this progress, direct comparisons between methodshavenot been possible because paststudies use different domains and different system components for ASR, SLU, dialog policy, etc.Moreover, there has not been a standard task or methodology for evaluating dialog state tracking.Together these issues have limited progress in this research area.The Dialog State Tracking Challenge (DSTC) series has provided a \ufb01rst common testbed andevaluation suite for dialog state tracking. Three instances of the DSTC have been run over a three5The Dialogue-State ArchitectureWilliams, Jason D., Antoine Raux, and Matthew Henderson. \"The dialog state tracking challenge series: A review.\" Dialogue & Discourse 7, no. 3 (2016): 4-33.",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 37,
      "token_count": 780,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 82\n\nComponents in a dialogue-state architectureNLU: extracts slot fillers from the user\u2019s utterance using machine learningDialogue state tracker: maintains the current state of the dialogue (user\u2019s most recent dialogue act, set of slot-filler constraints from userDialogue policy: decides what the system should do or say next\u2022GUS policy: ask questions until the frame was full then report back\u2022More sophisticated: know when to answer questions, when to ask a clarification question, etc.NLG: produce more natural, less templated utterances",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 38,
      "token_count": 111,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 83",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 39,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Dialogue ActsCombine the ideas of speech acts and groundinginto a single representation24.4\u2022THEDIALOGUE-STATEARCHITECTURE19Tag Sys User DescriptionHELLO(a=x,b=y,. . .)XXOpen a dialogue and give infoa=x,b=y,. . .INFORM(a=x,b=y,. . .)XXGive infoa=x,b=y,. . .REQUEST(a,b=x,. . .)XXRequest value for a givenb=x,. . .REQALTS(a=x,. . .)cXRequest alternative witha=x,. . .CONFIRM(a=x,b=y,. . .)XXExplicitly con\ufb01rma=x,b=y,. . .CONFREQ(a=x,. . . ,d)XcImplicitly con\ufb01rma=x,. . .and request value ofdSELECT(a=x,a=y)XcImplicitly con\ufb01rma=x,. . .and request value ofdAFFIRM(a=x,b=y,. . .)XXAf\ufb01rm and give further infoa=x,b=y,. . .NEGATE(a=x)cXNegate and give corrected valuea=xDENY(a=x)cXDeny thata=xBYE()XXClose a dialogueFigure 24.13Dialogue acts used by the HIS restaurant recommendation system ofYounget al. (2010). The Sys and User columns indicate which acts are valid as system outputs anduser inputs, respectively.Utterance Dialogue actU: Hi, I am looking for somewhere to eat.hello(task = find,type=restaurant)S: You are looking for a restaurant. Whattype of food do you like?confreq(type = restaurant, food)U: I\u2019d like an Italian somewhere near themuseum.inform(food = Italian, near=museum)S: Roma is a nice Italian restaurant nearthe museum.inform(name = \"Roma\", type = restaurant,food = Italian, near = museum)U: Is it reasonably priced?confirm(pricerange = moderate)S: Yes, Roma is in the moderate pricerange.affirm(name = \"Roma\", pricerange =moderate)U: What is the phone number?request(phone)S: The number of Roma is 385456.inform(name = \"Roma\", phone = \"385456\")U: Ok, thank you goodbye. bye()Figure 24.14A sample dialogue from the HIS System ofYoung et al. (2010)using the dialogue acts inFig.24.13.24.4.2 Slot FillingThe task of slot-\ufb01lling, and the simpler tasks of domain and intent classi\ufb01cation, arespecial cases of the task of supervised semantic parsing discussed in Chapter 16, inwhich we have a training set that associates each sentence with the correct set ofslots, domain, and intent.A simple method is to train a a sequence model to map from input words repre-sentation to slot \ufb01llers, domain and intent. For example given the sentence:I want to fly to San Francisco on Monday afternoon pleasewe compute a sentence representation, for example by passing the sentence througha contextual embedding network like BERT. The resulting sentence representationcan be passed through a feedforward layer and then a simple 1-of-N classi\ufb01er todetermine that the domain isAIRLINEand and the intent isSHOWFLIGHT.Our training data is sentences paired with sequences ofBIOlabels:O O O O O B-DES I-DES O B-DEPTIME I-DEPTIME OI want to fly to San Francisco on Monday afternoon pleaseRecall from Chapter 8 that in BIO tagging we introduce a tag for the beginning(B) and inside (I) of each slot label, and one for tokens outside (O) any slot label.The number of tags is thus 2n+1 tags, wherenis the number",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 40,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "(2010)using the dialogue acts inFig.24.13.24.4.2 Slot FillingThe task of slot-\ufb01lling, and the simpler tasks of domain and intent classi\ufb01cation, arespecial cases of the task of supervised semantic parsing discussed in Chapter 16, inwhich we have a training set that associates each sentence with the correct set ofslots, domain, and intent.A simple method is to train a a sequence model to map from input words repre-sentation to slot \ufb01llers, domain and intent. For example given the sentence:I want to fly to San Francisco on Monday afternoon pleasewe compute a sentence representation, for example by passing the sentence througha contextual embedding network like BERT. The resulting sentence representationcan be passed through a feedforward layer and then a simple 1-of-N classi\ufb01er todetermine that the domain isAIRLINEand and the intent isSHOWFLIGHT.Our training data is sentences paired with sequences ofBIOlabels:O O O O O B-DES I-DES O B-DEPTIME I-DEPTIME OI want to fly to San Francisco on Monday afternoon pleaseRecall from Chapter 8 that in BIO tagging we introduce a tag for the beginning(B) and inside (I) of each slot label, and one for tokens outside (O) any slot label.The number of tags is thus 2n+1 tags, wherenis the number of slots.Young et al., 2010:",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 41,
      "token_count": 311,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 84",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 42,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Dialogue Acts24.4\u2022THEDIALOGUE-STATEARCHITECTURE19Tag Sys User DescriptionHELLO(a=x,b=y,. . .)XXOpen a dialogue and give infoa=x,b=y,. . .INFORM(a=x,b=y,. . .)XXGive infoa=x,b=y,. . .REQUEST(a,b=x,. . .)XXRequest value for a givenb=x,. . .REQALTS(a=x,. . .)cXRequest alternative witha=x,. . .CONFIRM(a=x,b=y,. . .)XXExplicitly con\ufb01rma=x,b=y,. . .CONFREQ(a=x,. . . ,d)XcImplicitly con\ufb01rma=x,. . .and request value ofdSELECT(a=x,a=y)XcImplicitly con\ufb01rma=x,. . .and request value ofdAFFIRM(a=x,b=y,. . .)XXAf\ufb01rm and give further infoa=x,b=y,. . .NEGATE(a=x)cXNegate and give corrected valuea=xDENY(a=x)cXDeny thata=xBYE()XXClose a dialogueFigure 24.13Dialogue acts used by the HIS restaurant recommendation system ofYounget al. (2010). The Sys and User columns indicate which acts are valid as system outputs anduser inputs, respectively.Utterance Dialogue actU: Hi, I am looking for somewhere to eat.hello(task = find,type=restaurant)S: You are looking for a restaurant. Whattype of food do you like?confreq(type = restaurant, food)U: I\u2019d like an Italian somewhere near themuseum.inform(food = Italian, near=museum)S: Roma is a nice Italian restaurant nearthe museum.inform(name = \"Roma\", type = restaurant,food = Italian, near = museum)U: Is it reasonably priced?confirm(pricerange = moderate)S: Yes, Roma is in the moderate pricerange.affirm(name = \"Roma\", pricerange =moderate)U: What is the phone number?request(phone)S: The number of Roma is 385456.inform(name = \"Roma\", phone = \"385456\")U: Ok, thank you goodbye. bye()Figure 24.14A sample dialogue from the HIS System ofYoung et al. (2010)using the dialogue acts inFig.24.13.24.4.2 Slot FillingThe task of slot-\ufb01lling, and the simpler tasks of domain and intent classi\ufb01cation, arespecial cases of the task of supervised semantic parsing discussed in Chapter 16, inwhich we have a training set that associates each sentence with the correct set ofslots, domain, and intent.A simple method is to train a a sequence model to map from input words repre-sentation to slot \ufb01llers, domain and intent. For example given the sentence:I want to fly to San Francisco on Monday afternoon pleasewe compute a sentence representation, for example by passing the sentence througha contextual embedding network like BERT. The resulting sentence representationcan be passed through a feedforward layer and then a simple 1-of-N classi\ufb01er todetermine that the domain isAIRLINEand and the intent isSHOWFLIGHT.Our training data is sentences paired with sequences ofBIOlabels:O O O O O B-DES I-DES O B-DEPTIME I-DEPTIME OI want to fly to San Francisco on Monday afternoon pleaseRecall from Chapter 8 that in BIO tagging we introduce a tag for the beginning(B) and inside (I) of each slot label, and one for tokens outside (O) any slot label.The number of tags is thus 2n+1 tags, wherenis the number of slots.Young et al., 2010:",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 43,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 85\n\nSlot filling: Machine learningMachine learning classifiers to map words to semantic frame-fillersGiven a set of labeled sentencesInput: \"I want to fly to San Francisco on Monday please\"Output: Destination: SFDepart-time: MondayBuild a classifier to map from one to the otherRequirements: Lots of labeled data",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 44,
      "token_count": 66,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 86",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 45,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Slot filling as sequence labeling: BIO taggingThe BIO tagging paradigmIdea: Train a classifier to label each input word with a tag that tells us what slot (if any) it fillsWe create a B and I tag for each slot-typeAnd convert the training data to this format24.4\u2022THEDIALOGUE-STATEARCHITECTURE19Tag Sys User DescriptionHELLO(a=x,b=y,. . .)XXOpen a dialogue and give infoa=x,b=y,. . .INFORM(a=x,b=y,. . .)XXGive infoa=x,b=y,. . .REQUEST(a,b=x,. . .)XXRequest value for a givenb=x,. . .REQALTS(a=x,. . .)cXRequest alternative witha=x,. . .CONFIRM(a=x,b=y,. . .)XXExplicitly con\ufb01rma=x,b=y,. . .CONFREQ(a=x,. . . ,d)XcImplicitly con\ufb01rma=x,. . .and request value ofdSELECT(a=x,a=y)XcImplicitly con\ufb01rma=x,. . .and request value ofdAFFIRM(a=x,b=y,. . .)XXAf\ufb01rm and give further infoa=x,b=y,. . .NEGATE(a=x)cXNegate and give corrected valuea=xDENY(a=x)cXDeny thata=xBYE()XXClose a dialogueFigure 24.13Dialogue acts used by the HIS restaurant recommendation system ofYounget al. (2010). The Sys and User columns indicate which acts are valid as system outputs anduser inputs, respectively.Utterance Dialogue actU: Hi, I am looking for somewhere to eat.hello(task = find,type=restaurant)S: You are looking for a restaurant. Whattype of food do you like?confreq(type = restaurant, food)U: I\u2019d like an Italian somewhere near themuseum.inform(food = Italian, near=museum)S: Roma is a nice Italian restaurant nearthe museum.inform(name = \"Roma\", type = restaurant,food = Italian, near = museum)U: Is it reasonably priced?confirm(pricerange = moderate)S: Yes, Roma is in the moderate pricerange.affirm(name = \"Roma\", pricerange =moderate)U: What is the phone number?request(phone)S: The number of Roma is 385456.inform(name = \"Roma\", phone = \"385456\")U: Ok, thank you goodbye. bye()Figure 24.14A sample dialogue from the HIS System ofYoung et al. (2010)using the dialogue acts inFig.24.13.24.4.2 Slot FillingThe task of slot-\ufb01lling, and the simpler tasks of domain and intent classi\ufb01cation, arespecial cases of the task of supervised semantic parsing discussed in Chapter 16, inwhich we have a training set that associates each sentence with the correct set ofslots, domain, and intent.A simple method is to train a a sequence model to map from input words repre-sentation to slot \ufb01llers, domain and intent. For example given the sentence:I want to fly to San Francisco on Monday afternoon pleasewe compute a sentence representation, for example by passing the sentence througha contextual embedding network like BERT. The resulting sentence representationcan be passed through a feedforward layer and then a simple 1-of-N classi\ufb01er todetermine that the domain isAIRLINEand and the intent isSHOWFLIGHT.Our training data is sentences paired with sequences ofBIOlabels:O O O O O B-DES I-DES O B-DEPTIME I-DEPTIME OI want to fly to San Francisco on Monday afternoon pleaseRecall from Chapter 8 that in BIO tagging we introduce a tag for the",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 46,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "= \"Roma\", phone = \"385456\")U: Ok, thank you goodbye. bye()Figure 24.14A sample dialogue from the HIS System ofYoung et al. (2010)using the dialogue acts inFig.24.13.24.4.2 Slot FillingThe task of slot-\ufb01lling, and the simpler tasks of domain and intent classi\ufb01cation, arespecial cases of the task of supervised semantic parsing discussed in Chapter 16, inwhich we have a training set that associates each sentence with the correct set ofslots, domain, and intent.A simple method is to train a a sequence model to map from input words repre-sentation to slot \ufb01llers, domain and intent. For example given the sentence:I want to fly to San Francisco on Monday afternoon pleasewe compute a sentence representation, for example by passing the sentence througha contextual embedding network like BERT. The resulting sentence representationcan be passed through a feedforward layer and then a simple 1-of-N classi\ufb01er todetermine that the domain isAIRLINEand and the intent isSHOWFLIGHT.Our training data is sentences paired with sequences ofBIOlabels:O O O O O B-DES I-DES O B-DEPTIME I-DEPTIME OI want to fly to San Francisco on Monday afternoon pleaseRecall from Chapter 8 that in BIO tagging we introduce a tag for the beginning(B) and inside (I) of each slot label, and one for tokens outside (O) any slot label.The number of tags is thus 2n+1 tags, wherenis the number of slots.",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 47,
      "token_count": 341,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 87\n\nSlot filling using contextual embeddings20CHAPTER24\u2022CHATBOTS&DIALOGUESYSTEMSFig.24.15shows the architecture. The input is a series of wordsw1...wn, whichis passed through a contextual embedding model to get contextual word representa-tions. This is followed by a feedforward layer and a softmax at each token positionover possible BIO tags, with the output a series of BIO tagss1...sn. We can alsocombine the domain-classi\ufb01cation and intent-extraction tasks with slot-\ufb01lling sim-ply by adding a domain concatenated with an intent as the desired output for the\ufb01nal EOS token.\nSanFranciscoonMondayEncodingsClassi\ufb01er+softmaxB-DESI-DESOB-DTIME\n\u2026d+i\n<EOS>Encoder (BERT)Figure 24.15A simple architecture for slot \ufb01lling, mapping the words in the input throughcontextual embeddings like BERT to an output classi\ufb01er layer (which can be linear or some-thing more complex), followed by softmax to generate a series of BIO tags (and including a\ufb01nal state consisting of a domain concatenated with an intent).Once the sequence labeler has tagged the user utterance, a \ufb01ller string can beextracted for each slot from the tags (e.g., \u201cSan Francisco\u201d), and these word stringscan then be normalized to the correct form in the ontology (perhaps the airport code\u2018SFO\u2019). This normalization can take place by using homonym dictionaries (specify-ing, for example, that SF, SFO, and San Francisco are the same place).In industrial contexts, machine learning-based systems for slot-\ufb01lling are of-ten bootstrapped from GUS-style rule-based systems in a semi-supervised learningmanner. A rule-based system is \ufb01rst built for the domain, and a test set is carefullylabeled. As new user utterances come in, they are paired with the labeling providedby the rule-based system to create training tuples. A classi\ufb01er can then be trainedon these tuples, using the test set to test the performance of the classi\ufb01er againstthe rule-based system. Some heuristics can be used to eliminate errorful trainingtuples, with the goal of increasing precision. As suf\ufb01cient training samples becomeavailable the resulting classi\ufb01er can often outperform the original rule-based system(Suendermann et al., 2009), although rule-based systems may still remain higher-precision for dealing with complex cases like negation.24.4.3 Dialogue State TrackingThe job of the dialogue-state tracker is to determine both the current state of theframe (the \ufb01llers of each slot), as well as the user\u2019s most recent dialogue act. Thedialogue-state thus includes more than just the slot-\ufb01llers expressed in the currentsentence; it includes the entire state of the frame at this point, summarizing all ofthe user\u2019s constraints. The following example fromMrk\u02c7si\u00b4c et al. (2017)shows therequired output of the dialogue state tracker after each turn:Can do domain and intent too: e.g.,  generate the label  \"AIRLINE_TRAVEL + SEARCH_FLIGHT\"",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 48,
      "token_count": 686,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 88",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 49,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Once we have the BIO tag of the sentence\u2022We can extract the filler string for each slot\u2022And then normalize it to the correct form in the ontology\u2022Like \"SFO\" for San Francisco\u2022Using homonym dictionaries (SF=SFO=San Francisco)24.4\u2022THEDIALOGUE-STATEARCHITECTURE19Tag Sys User DescriptionHELLO(a=x,b=y,. . .)XXOpen a dialogue and give infoa=x,b=y,. . .INFORM(a=x,b=y,. . .)XXGive infoa=x,b=y,. . .REQUEST(a,b=x,. . .)XXRequest value for a givenb=x,. . .REQALTS(a=x,. . .)cXRequest alternative witha=x,. . .CONFIRM(a=x,b=y,. . .)XXExplicitly con\ufb01rma=x,b=y,. . .CONFREQ(a=x,. . . ,d)XcImplicitly con\ufb01rma=x,. . .and request value ofdSELECT(a=x,a=y)XcImplicitly con\ufb01rma=x,. . .and request value ofdAFFIRM(a=x,b=y,. . .)XXAf\ufb01rm and give further infoa=x,b=y,. . .NEGATE(a=x)cXNegate and give corrected valuea=xDENY(a=x)cXDeny thata=xBYE()XXClose a dialogueFigure 24.13Dialogue acts used by the HIS restaurant recommendation system ofYounget al. (2010). The Sys and User columns indicate which acts are valid as system outputs anduser inputs, respectively.Utterance Dialogue actU: Hi, I am looking for somewhere to eat.hello(task = find,type=restaurant)S: You are looking for a restaurant. Whattype of food do you like?confreq(type = restaurant, food)U: I\u2019d like an Italian somewhere near themuseum.inform(food = Italian, near=museum)S: Roma is a nice Italian restaurant nearthe museum.inform(name = \"Roma\", type = restaurant,food = Italian, near = museum)U: Is it reasonably priced?confirm(pricerange = moderate)S: Yes, Roma is in the moderate pricerange.affirm(name = \"Roma\", pricerange =moderate)U: What is the phone number?request(phone)S: The number of Roma is 385456.inform(name = \"Roma\", phone = \"385456\")U: Ok, thank you goodbye. bye()Figure 24.14A sample dialogue from the HIS System ofYoung et al. (2010)using the dialogue acts inFig.24.13.24.4.2 Slot FillingThe task of slot-\ufb01lling, and the simpler tasks of domain and intent classi\ufb01cation, arespecial cases of the task of supervised semantic parsing discussed in Chapter 16, inwhich we have a training set that associates each sentence with the correct set ofslots, domain, and intent.A simple method is to train a a sequence model to map from input words repre-sentation to slot \ufb01llers, domain and intent. For example given the sentence:I want to fly to San Francisco on Monday afternoon pleasewe compute a sentence representation, for example by passing the sentence througha contextual embedding network like BERT. The resulting sentence representationcan be passed through a feedforward layer and then a simple 1-of-N classi\ufb01er todetermine that the domain isAIRLINEand and the intent isSHOWFLIGHT.Our training data is sentences paired with sequences ofBIOlabels:O O O O O B-DES I-DES O B-DEPTIME I-DEPTIME OI want to fly to San Francisco on Monday afternoon pleaseRecall from Chapter 8 that in BIO tagging we introduce a tag for the beginning(B)",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 50,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "= \"Roma\", phone = \"385456\")U: Ok, thank you goodbye. bye()Figure 24.14A sample dialogue from the HIS System ofYoung et al. (2010)using the dialogue acts inFig.24.13.24.4.2 Slot FillingThe task of slot-\ufb01lling, and the simpler tasks of domain and intent classi\ufb01cation, arespecial cases of the task of supervised semantic parsing discussed in Chapter 16, inwhich we have a training set that associates each sentence with the correct set ofslots, domain, and intent.A simple method is to train a a sequence model to map from input words repre-sentation to slot \ufb01llers, domain and intent. For example given the sentence:I want to fly to San Francisco on Monday afternoon pleasewe compute a sentence representation, for example by passing the sentence througha contextual embedding network like BERT. The resulting sentence representationcan be passed through a feedforward layer and then a simple 1-of-N classi\ufb01er todetermine that the domain isAIRLINEand and the intent isSHOWFLIGHT.Our training data is sentences paired with sequences ofBIOlabels:O O O O O B-DES I-DES O B-DEPTIME I-DEPTIME OI want to fly to San Francisco on Monday afternoon pleaseRecall from Chapter 8 that in BIO tagging we introduce a tag for the beginning(B) and inside (I) of each slot label, and one for tokens outside (O) any slot label.The number of tags is thus 2n+1 tags, wherenis the number of slots.",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 51,
      "token_count": 341,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 89",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 52,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "The task of dialogue state tracking24.4\u2022THEDIALOGUE-STATEARCHITECTURE21User: I\u2019m looking for a cheaper restaurantinform(price=cheap)System: Sure. What kind - and where?User: Thai food, somewhere downtowninform(price=cheap, food=Thai, area=centre)System: The House serves cheap Thai foodUser: Where is it?inform(price=cheap, food=Thai, area=centre); request(address)System: The House is at 106 Regent StreetSince dialogue acts place some constraints on the slots and values, the tasks ofdialogue-act detection and slot-\ufb01lling are often performed jointly. Consider the taskof determining thatI\u2019d like Cantonese food near the Mission Districthas the structureinform(food=cantonese,area=mission).Dialogue act interpretation\u2014in this example choosinginformfrom the set ofdialogue acts for this task\u2014is done by supervised classi\ufb01cation trained on hand-labeled dialog acts, predicting the dialogue act tag based on embeddings represent-ing the current input sentence and the prior dialogue acts.The simplest dialogue state tracker might just take the output of a slot-\ufb01llingsequence-model (Section24.4.2) after each sentence. Alternatively, a more complexmodel can make use of the reading-comprehension architectures from Chapter 23.For example the model ofGao et al. (2019)trains a classi\ufb01er for each slot to decidewhether its value is being changed in the current sentence or should be carried overfrom the previous sentences. If the slot value is being changed, a span-predictionmodel is used to predict the start and end of the span with the slot \ufb01ller.A special case: detecting correction actsSome dialogue acts are important because of their implications for dialogue control.If a dialogue system misrecognizes or misunderstands an utterance, the user willgenerally correct the error by repeating or reformulating the utterance. Detectingtheseuser correction actsis therefore quite important. Ironically, it turns out thatuser correctionactscorrections are actuallyharderto recognize than normal sentences! In fact, correc-tions in one early dialogue system (the TOOT system) had double the ASR worderror rate of non-corrections(Swerts et al., 2000)! One reason for this is that speak-ers sometimes use a speci\ufb01c prosodic style for corrections calledhyperarticulation,hyperarticula-tionin which the utterance contains exaggerated energy, duration, or F0 contours, suchasI said BAL-TI-MORE, not Boston(Wade et al. 1992,Levow 1998,Hirschberget al. 2001). Even when they are not hyperarticulating, users who are frustratedseem to speak in a way that is harder for speech recognizers(Goldberg et al., 2003).What are the characteristics of these corrections? User corrections tend to beeither exact repetitions or repetitions with one or more words omitted, although theymay also be paraphrases of the original utterance.(Swerts et al., 2000). Detect-ing these reformulations or correction acts can be part of the general dialogue actdetection classi\ufb01er. Alternatively, because the cues to these acts tend to appear indifferent ways than for simple acts (likeINFORMorrequest), we can make use offeatures orthogonal to simple contextual embedding features; some typical featuresare shown below (Levow 1998,Litman et al. 1999,Hirschberg et al. 2001,Bulykoet al. 2005,Awadallah et al. 2015):Example from Mrk\u0161i\u0107, N., O S\u00e9aghdha, D., Wen, T.-H., Thomson, B., and Young, S. (2017). Neural belief tracker:",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 53,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "corrections calledhyperarticulation,hyperarticula-tionin which the utterance contains exaggerated energy, duration, or F0 contours, suchasI said BAL-TI-MORE, not Boston(Wade et al. 1992,Levow 1998,Hirschberget al. 2001). Even when they are not hyperarticulating, users who are frustratedseem to speak in a way that is harder for speech recognizers(Goldberg et al., 2003).What are the characteristics of these corrections? User corrections tend to beeither exact repetitions or repetitions with one or more words omitted, although theymay also be paraphrases of the original utterance.(Swerts et al., 2000). Detect-ing these reformulations or correction acts can be part of the general dialogue actdetection classi\ufb01er. Alternatively, because the cues to these acts tend to appear indifferent ways than for simple acts (likeINFORMorrequest), we can make use offeatures orthogonal to simple contextual embedding features; some typical featuresare shown below (Levow 1998,Litman et al. 1999,Hirschberg et al. 2001,Bulykoet al. 2005,Awadallah et al. 2015):Example from Mrk\u0161i\u0107, N., O S\u00e9aghdha, D., Wen, T.-H., Thomson, B., and Young, S. (2017). Neural belief tracker: Data-driven dialogue state tracking. ACL.",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 54,
      "token_count": 309,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 90\n\nDialogue state trackingI'd like Cantonese food near the Mission district.\u00e0inform(food=cantonese, area=mission). Dialogue act interpretation algorithm: \u20221-of-N supervised classification to choose inform\u2022Based on encodings of current sentence + prior dialogue actsSimple dialogue state tracker:\u2022Run a slot-filler after each sentence \n\n## Page 91\n\nAn special case of dialogue act detection: Detecting Correction ActsIf system misrecognizes an utteranceUser might make a correction\u25e6Repeat themselves\u25e6Rephrasing\u25e6Saying \u201cno\u201d to a confirmation question\n\n## Page 92\n\nCorrections are harder to recognize!\u25e6From speech, corrections are misrecognized twice as often (in terms of word error rate) as non-corrections! (Swertset al 2000)\u25e6Hyperarticulation(exaggerated prosody) is a large factor: \u25e6Shriberg, E., Wade, E., Price, P ., 1992. Human-machine problem solving using spoken language systems (SLS): Factors affect-ng performance and user satisfaction. DARPA Speech and Natural Language Workshop.\u25e6\"I said BAL-TI-MORE, not Boston\"",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 55,
      "token_count": 247,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 93\n\nFeatures for detecting corrections in spoken dialogue22CHAPTER24\u2022CHATBOTS&DIALOGUESYSTEMSfeaturesexampleslexicalwords like \u201cno\u201d, \u201ccorrection\u201d, \u201cI don\u2019t\u201d, swear words, utterance lengthsemanticsimilarity (word overlap or embedding dot product) between the candidatecorrection act and the user\u2019s prior utterancephoneticphonetic overlap between the candidate correction act and the user\u2019s prior ut-terance (i.e. \u201cWhatsApp\u201d may be incorrectly recognized as \u201cWhat\u2019s up\u201d)prosodichyperarticulation, increases in F0 range, pause duration, and word duration,generally normalized by the values for previous sentencesASRASR con\ufb01dence, language model probability24.4.4 Dialogue PolicyThe goal of thedialogue policyis to decide what action the system should take next,dialogue policythat is, what dialogue act to generate.More formally, at turniin the conversation we want to predict which actionAito take, based on the entire dialogue state. The state could mean the entire sequenceof dialogue acts from the system (A) and from the user (U), in which case the taskwould be to compute:\u02c6Ai=argmaxAi2AP(Ai|(A1,U1,. . . ,Ai\u00001,Ui\u00001)(24.7)We can simplify this by maintaining as the dialogue state mainly just the set ofslot-\ufb01llers that the user has expressed, collapsing across the many different conver-sational paths that could lead to the same set of \ufb01lled slots.Such a policy might then just condition on the current dialogue state as repre-sented just by the current state of the frame Framei(which slots are \ufb01lled and withwhat) and the last turn by the system and user:\u02c6Ai=argmaxAi2AP(Ai|Framei\u00001,Ai\u00001,Ui\u00001)(24.8)These probabilities can be estimated by a neural classi\ufb01er using neural representa-tions of the slot \ufb01llers (for example as spans) and the utterances (for example assentence embeddings computed over contextual embeddings)More sophisticated models train the policy viareinforcement learning. To de-cide which action to take, a reinforcement learning system gets a reward at the endof the dialogue, and uses that reward to train a policy to take actions. For example inthe movie-recommendation dialogue system ofFazel-Zarandi et al. (2017), the ac-tion space has only three actions:EXECUTE,CONFIRM, andELICIT. TheEXECUTEsends a query to the database and answers the user\u2019s question,CONFIRMclari\ufb01esthe intent or slot with the users (e.g., \u201cDo you want movies directed by ChristopherNolan?\u201d) whileELICITasks the user for missing information (e.g., \u201cWhich movieare you talking about?\u201d). The system gets a large positive reward if the dialogue sys-tem terminates with the correct slot representation at the end, a large negative rewardif the slots are wrong, and a small negative reward for con\ufb01rmation and elicitationquestions to keep the system from re-con\ufb01rming everything.Policy Example: Con\ufb01rmation and RejectionModern dialogue systems often make mistakes. It is therefore important for dialoguesystems to make sure that they have achieved the correct interpretation of the user\u2019s\n\n## Page 94\n\nChatbots and Dialogue SystemsThe Dialogue-State Architecture\n\n## Page 95\n\nChatbots and Dialogue SystemsThe Dialogue-State Architecture Continued: Policy and Generation",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 56,
      "token_count": 749,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 96",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 57,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Dialogue PolicyAt turn ipredict action Aito take, given entire history:Simplify by just conditioning on the current dialogue state (filled frame slots) and the last turn and turn by system and user:22CHAPTER24\u2022CHATBOTS&DIALOGUESYSTEMSfeaturesexampleslexicalwords like \u201cno\u201d, \u201ccorrection\u201d, \u201cI don\u2019t\u201d, swear words, utterance lengthsemanticsimilarity (word overlap or embedding dot product) between the candidatecorrection act and the user\u2019s prior utterancephoneticphonetic overlap between the candidate correction act and the user\u2019s prior ut-terance (i.e. \u201cWhatsApp\u201d may be incorrectly recognized as \u201cWhat\u2019s up\u201d)prosodichyperarticulation, increases in F0 range, pause duration, and word duration,generally normalized by the values for previous sentencesASRASR con\ufb01dence, language model probability24.4.4 Dialogue PolicyThe goal of thedialogue policyis to decide what action the system should take next,dialogue policythat is, what dialogue act to generate.More formally, at turniin the conversation we want to predict which actionAito take, based on the entire dialogue state. The state could mean the entire sequenceof dialogue acts from the system (A) and from the user (U), in which case the taskwould be to compute:\u02c6Ai=argmaxAi2AP(Ai|(A1,U1,. . . ,Ai\u00001,Ui\u00001)(24.7)We can simplify this by maintaining as the dialogue state mainly just the set ofslot-\ufb01llers that the user has expressed, collapsing across the many different conver-sational paths that could lead to the same set of \ufb01lled slots.Such a policy might then just condition on the current dialogue state as repre-sented just by the current state of the frame Framei(which slots are \ufb01lled and withwhat) and the last turn by the system and user:\u02c6Ai=argmaxAi2AP(Ai|Framei\u00001,Ai\u00001,Ui\u00001)(24.8)These probabilities can be estimated by a neural classi\ufb01er using neural representa-tions of the slot \ufb01llers (for example as spans) and the utterances (for example assentence embeddings computed over contextual embeddings)More sophisticated models train the policy viareinforcement learning. To de-cide which action to take, a reinforcement learning system gets a reward at the endof the dialogue, and uses that reward to train a policy to take actions. For example inthe movie-recommendation dialogue system ofFazel-Zarandi et al. (2017), the ac-tion space has only three actions:EXECUTE,CONFIRM, andELICIT. TheEXECUTEsends a query to the database and answers the user\u2019s question,CONFIRMclari\ufb01esthe intent or slot with the users (e.g., \u201cDo you want movies directed by ChristopherNolan?\u201d) whileELICITasks the user for missing information (e.g., \u201cWhich movieare you talking about?\u201d). The system gets a large positive reward if the dialogue sys-tem terminates with the correct slot representation at the end, a large negative rewardif the slots are wrong, and a small negative reward for con\ufb01rmation and elicitationquestions to keep the system from re-con\ufb01rming everything.Policy Example: Con\ufb01rmation and RejectionModern dialogue systems often make mistakes. It is therefore important for dialoguesystems to make sure that they have achieved the correct interpretation of the user\u2019s22CHAPTER24\u2022CHATBOTS&DIALOGUESYSTEMSfeaturesexampleslexicalwords like \u201cno\u201d, \u201ccorrection\u201d, \u201cI don\u2019t\u201d, swear words, utterance lengthsemanticsimilarity (word overlap or embedding dot product) between the candidatecorrection act and the",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 58,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "to take, a reinforcement learning system gets a reward at the endof the dialogue, and uses that reward to train a policy to take actions. For example inthe movie-recommendation dialogue system ofFazel-Zarandi et al. (2017), the ac-tion space has only three actions:EXECUTE,CONFIRM, andELICIT. TheEXECUTEsends a query to the database and answers the user\u2019s question,CONFIRMclari\ufb01esthe intent or slot with the users (e.g., \u201cDo you want movies directed by ChristopherNolan?\u201d) whileELICITasks the user for missing information (e.g., \u201cWhich movieare you talking about?\u201d). The system gets a large positive reward if the dialogue sys-tem terminates with the correct slot representation at the end, a large negative rewardif the slots are wrong, and a small negative reward for con\ufb01rmation and elicitationquestions to keep the system from re-con\ufb01rming everything.Policy Example: Con\ufb01rmation and RejectionModern dialogue systems often make mistakes. It is therefore important for dialoguesystems to make sure that they have achieved the correct interpretation of the user\u2019s22CHAPTER24\u2022CHATBOTS&DIALOGUESYSTEMSfeaturesexampleslexicalwords like \u201cno\u201d, \u201ccorrection\u201d, \u201cI don\u2019t\u201d, swear words, utterance lengthsemanticsimilarity (word overlap or embedding dot product) between the candidatecorrection act and the user\u2019s prior utterancephoneticphonetic overlap between the candidate correction act and the user\u2019s prior ut-terance (i.e. \u201cWhatsApp\u201d may be incorrectly recognized as \u201cWhat\u2019s up\u201d)prosodichyperarticulation, increases in F0 range, pause duration, and word duration,generally normalized by the values for previous sentencesASRASR con\ufb01dence, language model probability24.4.4 Dialogue PolicyThe goal of thedialogue policyis to decide what action the system should take next,dialogue policythat is, what dialogue act to generate.More formally, at turniin the conversation we want to predict which actionAito take, based on the entire dialogue state. The state could mean the entire sequenceof dialogue acts from the system (A) and from the user (U), in which case the taskwould be to compute:\u02c6Ai=argmaxAi2AP(Ai|(A1,U1,. . . ,Ai\u00001,Ui\u00001)(24.7)We can simplify this by maintaining as the dialogue state mainly just the set ofslot-\ufb01llers that the user has expressed, collapsing across the many different conver-sational paths that could lead to the same set of \ufb01lled slots.Such a policy might then just condition on the current dialogue state as repre-sented just by the current state of the frame Framei(which slots are \ufb01lled and withwhat) and the last turn by the system and user:\u02c6Ai=argmaxAi2AP(Ai|Framei\u00001,Ai\u00001,Ui\u00001)(24.8)These probabilities can be estimated by a neural classi\ufb01er using neural representa-tions of the slot \ufb01llers (for example as spans) and the utterances (for example assentence embeddings computed over contextual embeddings)More sophisticated models train the policy viareinforcement learning. To de-cide which action to take, a reinforcement learning system gets a reward at the endof the dialogue, and uses that reward to train a policy to take actions. For example inthe movie-recommendation dialogue system ofFazel-Zarandi et al. (2017), the ac-tion space has only three actions:EXECUTE,CONFIRM, andELICIT. TheEXECUTEsends a query to the database and answers the user\u2019s question,CONFIRMclari\ufb01esthe",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 59,
      "token_count": 800,
      "chapter_title": ""
    }
  },
  {
    "content": "can simplify this by maintaining as the dialogue state mainly just the set ofslot-\ufb01llers that the user has expressed, collapsing across the many different conver-sational paths that could lead to the same set of \ufb01lled slots.Such a policy might then just condition on the current dialogue state as repre-sented just by the current state of the frame Framei(which slots are \ufb01lled and withwhat) and the last turn by the system and user:\u02c6Ai=argmaxAi2AP(Ai|Framei\u00001,Ai\u00001,Ui\u00001)(24.8)These probabilities can be estimated by a neural classi\ufb01er using neural representa-tions of the slot \ufb01llers (for example as spans) and the utterances (for example assentence embeddings computed over contextual embeddings)More sophisticated models train the policy viareinforcement learning. To de-cide which action to take, a reinforcement learning system gets a reward at the endof the dialogue, and uses that reward to train a policy to take actions. For example inthe movie-recommendation dialogue system ofFazel-Zarandi et al. (2017), the ac-tion space has only three actions:EXECUTE,CONFIRM, andELICIT. TheEXECUTEsends a query to the database and answers the user\u2019s question,CONFIRMclari\ufb01esthe intent or slot with the users (e.g., \u201cDo you want movies directed by ChristopherNolan?\u201d) whileELICITasks the user for missing information (e.g., \u201cWhich movieare you talking about?\u201d). The system gets a large positive reward if the dialogue sys-tem terminates with the correct slot representation at the end, a large negative rewardif the slots are wrong, and a small negative reward for con\ufb01rmation and elicitationquestions to keep the system from re-con\ufb01rming everything.Policy Example: Con\ufb01rmation and RejectionModern dialogue systems often make mistakes. It is therefore important for dialoguesystems to make sure that they have achieved the correct interpretation of the user\u2019s",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 60,
      "token_count": 434,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 97\n\nPolicy example: Confirmation and RejectionDialogue systems make errorsSo they to make sure they have understood userTwo important mechanisms:\u2022confirmingunderstandings with the user \u2022rejectingutterances that the system is likely to have misunderstood. \n\n## Page 98\n\nExplicit confirmation strategy24.4\u2022THEDIALOGUE-STATEARCHITECTURE23input. This is generally done by two methods:con\ufb01rmingunderstandings with theuser andrejectingutterances that the system is likely to have misunderstood.As we saw in the prior section, most systems introduce particular strategies andactions related to con\ufb01rmation and rejection. When using theexplicit con\ufb01rmationexplicitcon\ufb01rmationstrategy, a system asks the user a direct question to con\ufb01rm the system\u2019s under-standing, like the two examples below in which the system asks a (boldface) yes-nocon\ufb01rmation question:S:Which city do you want to leave from?U:Baltimore.S:Do you want to leave from Baltimore?U:Yes.U:I\u2019d like to \ufb02y from Denver Colorado to New York City on Septembertwenty \ufb01rst in the morning on United AirlinesS:Let\u2019s see then. I have you going from Denver Colorado to New Yorkon September twenty \ufb01rst. Is that correct?U:YesWhen using theimplicit con\ufb01rmationstrategy, a system instead can demon-implicitcon\ufb01rmationstrate its understanding as agroundingstrategy, for example repeating back thesystem\u2019s understanding as part of asking the next question, as in the two examplesbelow:U:I want to travel to BerlinS:When do you want to travel to Berlin?U2:Hi I\u2019d like to \ufb02y to Seattle Tuesday MorningA3:Traveling to Seattle on Tuesday, August eleventh in the morning.Your full name?Explicit and implicit con\ufb01rmation have complementary strengths. Explicit con-\ufb01rmation makes it easier for users to correct the system\u2019s misrecognitions since auser can just answer \u201cno\u201d to the con\ufb01rmation question. But explicit con\ufb01rmation isawkward and increases the length of the conversation (Danieli and Gerbino 1995,Walker et al. 1998). The explicit con\ufb01rmation dialogue fragments above sound non-natural and de\ufb01nitely non-human; implicit con\ufb01rmation is much more conversation-ally natural.Con\ufb01rmation is just one kind of conversational action by which a system canexpress lack of understanding. Another option isrejection, in which a system givesrejectionthe user a prompt likeI\u2019m sorry, I didn\u2019t understand that.Sometimes utterances are rejected multiple times. This might mean that the useris using language that the system is unable to follow. Thus, when an utterance isrejected, systems often follow a strategy ofprogressive promptingorescalatingprogressivepromptingdetail(Yankelovich et al. 1995,Weinschenk and Barker 2000), as in this examplefromCohen et al. (2004):System:When would you like to leave?Caller:Well, um, I need to be in New York in time for the \ufb01rst World Series game.System:<reject>. Sorry, I didn\u2019t get that. Please say the month and day you\u2019d liketo leave.Caller:I wanna go on October \ufb01fteenth.",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 61,
      "token_count": 719,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 99\n\nImplicit confirmation strategy24.4\u2022THEDIALOGUE-STATEARCHITECTURE23input. This is generally done by two methods:con\ufb01rmingunderstandings with theuser andrejectingutterances that the system is likely to have misunderstood.As we saw in the prior section, most systems introduce particular strategies andactions related to con\ufb01rmation and rejection. When using theexplicit con\ufb01rmationexplicitcon\ufb01rmationstrategy, a system asks the user a direct question to con\ufb01rm the system\u2019s under-standing, like the two examples below in which the system asks a (boldface) yes-nocon\ufb01rmation question:S:Which city do you want to leave from?U:Baltimore.S:Do you want to leave from Baltimore?U:Yes.U:I\u2019d like to \ufb02y from Denver Colorado to New York City on Septembertwenty \ufb01rst in the morning on United AirlinesS:Let\u2019s see then. I have you going from Denver Colorado to New Yorkon September twenty \ufb01rst. Is that correct?U:YesWhen using theimplicit con\ufb01rmationstrategy, a system instead can demon-implicitcon\ufb01rmationstrate its understanding as agroundingstrategy, for example repeating back thesystem\u2019s understanding as part of asking the next question, as in the two examplesbelow:U:I want to travel to BerlinS:When do you want to travel to Berlin?U2:Hi I\u2019d like to \ufb02y to Seattle Tuesday MorningA3:Traveling to Seattle on Tuesday, August eleventh in the morning.Your full name?Explicit and implicit con\ufb01rmation have complementary strengths. Explicit con-\ufb01rmation makes it easier for users to correct the system\u2019s misrecognitions since auser can just answer \u201cno\u201d to the con\ufb01rmation question. But explicit con\ufb01rmation isawkward and increases the length of the conversation (Danieli and Gerbino 1995,Walker et al. 1998). The explicit con\ufb01rmation dialogue fragments above sound non-natural and de\ufb01nitely non-human; implicit con\ufb01rmation is much more conversation-ally natural.Con\ufb01rmation is just one kind of conversational action by which a system canexpress lack of understanding. Another option isrejection, in which a system givesrejectionthe user a prompt likeI\u2019m sorry, I didn\u2019t understand that.Sometimes utterances are rejected multiple times. This might mean that the useris using language that the system is unable to follow. Thus, when an utterance isrejected, systems often follow a strategy ofprogressive promptingorescalatingprogressivepromptingdetail(Yankelovich et al. 1995,Weinschenk and Barker 2000), as in this examplefromCohen et al. (2004):System:When would you like to leave?Caller:Well, um, I need to be in New York in time for the \ufb01rst World Series game.System:<reject>. Sorry, I didn\u2019t get that. Please say the month and day you\u2019d liketo leave.Caller:I wanna go on October \ufb01fteenth.\n\n## Page 100\n\nConfirmation strategy  tradeoffsExplicit confirmation makes it easier for users to correct the system\u2019s misrecognitions since a user can just answer \u201cno\u201d to the confirmation question. But explicit confirmation is also awkward and increases the length of the conversation (Danieliand Gerbino1995, Walker et al. 1998). \n\n## Page 101\n\nRejectionI\u2019m sorry, I didn\u2019t understand that.",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 62,
      "token_count": 754,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 100\n\nConfirmation strategy  tradeoffsExplicit confirmation makes it easier for users to correct the system\u2019s misrecognitions since a user can just answer \u201cno\u201d to the confirmation question. But explicit confirmation is also awkward and increases the length of the conversation (Danieliand Gerbino1995, Walker et al. 1998). \n\n## Page 101\n\nRejectionI\u2019m sorry, I didn\u2019t understand that. \n\n## Page 102\n\nProgressive prompting for rejection24.4\u2022THEDIALOGUE-STATEARCHITECTURE23input. This is generally done by two methods:con\ufb01rmingunderstandings with theuser andrejectingutterances that the system is likely to have misunderstood.As we saw in the prior section, most systems introduce particular strategies andactions related to con\ufb01rmation and rejection. When using theexplicit con\ufb01rmationexplicitcon\ufb01rmationstrategy, a system asks the user a direct question to con\ufb01rm the system\u2019s under-standing, like the two examples below in which the system asks a (boldface) yes-nocon\ufb01rmation question:S:Which city do you want to leave from?U:Baltimore.S:Do you want to leave from Baltimore?U:Yes.U:I\u2019d like to \ufb02y from Denver Colorado to New York City on Septembertwenty \ufb01rst in the morning on United AirlinesS:Let\u2019s see then. I have you going from Denver Colorado to New Yorkon September twenty \ufb01rst. Is that correct?U:YesWhen using theimplicit con\ufb01rmationstrategy, a system instead can demon-implicitcon\ufb01rmationstrate its understanding as agroundingstrategy, for example repeating back thesystem\u2019s understanding as part of asking the next question, as in the two examplesbelow:U:I want to travel to BerlinS:When do you want to travel to Berlin?U2:Hi I\u2019d like to \ufb02y to Seattle Tuesday MorningA3:Traveling to Seattle on Tuesday, August eleventh in the morning.Your full name?Explicit and implicit con\ufb01rmation have complementary strengths. Explicit con-\ufb01rmation makes it easier for users to correct the system\u2019s misrecognitions since auser can just answer \u201cno\u201d to the con\ufb01rmation question. But explicit con\ufb01rmation isawkward and increases the length of the conversation (Danieli and Gerbino 1995,Walker et al. 1998). The explicit con\ufb01rmation dialogue fragments above sound non-natural and de\ufb01nitely non-human; implicit con\ufb01rmation is much more conversation-ally natural.Con\ufb01rmation is just one kind of conversational action by which a system canexpress lack of understanding. Another option isrejection, in which a system givesrejectionthe user a prompt likeI\u2019m sorry, I didn\u2019t understand that.Sometimes utterances are rejected multiple times. This might mean that the useris using language that the system is unable to follow. Thus, when an utterance isrejected, systems often follow a strategy ofprogressive promptingorescalatingprogressivepromptingdetail(Yankelovich et al. 1995,Weinschenk and Barker 2000), as in this examplefromCohen et al. (2004):System:When would you like to leave?Caller:Well, um, I need to be in New York in time for the \ufb01rst World Series game.System:<reject>. Sorry, I didn\u2019t get that. Please say the month and day you\u2019d liketo leave.Caller:I wanna go on October \ufb01fteenth.Don't just repeat the question \"When would you like to leave?\"Give user guidance about what they can say:",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 63,
      "token_count": 780,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 103",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 64,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Using confidence to decide whether to confirm:ASR  or NLU systems can assign a confidencevalue, indicating how likely they are that they understood the user. \u2022Acoustic log-likelihood of the utterance\u2022Prosodic features\u2022Ratio of score  of best to second-best interpretationSystems could use set confidence thresholds:24CHAPTER24\u2022CHATBOTS&DIALOGUESYSTEMSIn this example, instead of just repeating \u201cWhen would you like to leave?\u201d, therejection prompt gives the caller more guidance about how to formulate an utter-ance the system will understand. Theseyou-can-sayhelp messages are important inhelping improve systems\u2019 understanding performance(Bohus and Rudnicky, 2005).If the caller\u2019s utterance gets rejected yet again, the prompt can re\ufb02ect this (\u201cIstilldidn\u2019t get that\u201d), and give the caller even more guidance.An alternative strategy for error handling israpid reprompting, in which therapidrepromptingsystem rejects an utterance just by saying \u201cI\u2019m sorry?\u201d or \u201cWhat was that?\u201d Onlyif the caller\u2019s utterance is rejected a second time does the system start applyingprogressive prompting.Cohen et al. (2004)summarize experiments showing thatusers greatly prefer rapid reprompting as a \ufb01rst-level error prompt.It is common to use rich features other than just the dialogue state representa-tion to make policy decisions. For example, thecon\ufb01dencethat the ASR systemassigns to an utterance can be used by explicitly con\ufb01rming low-con\ufb01dence sen-tences. Condience is a metric that the speech recognizer can assign to its transcrip-tion of a sentence to indicate how con\ufb01dent it is in that transcription. Con\ufb01dence isoften computed from the acoustic log-likelihood of the utterance (greater probabil-ity means higher con\ufb01dence), but prosodic features can also be used in con\ufb01denceprediction. For example, utterances with large F0 excursions or longer durations,or those preceded by longer pauses, are likely to be misrecognized(Litman et al.,2000).Another common feature in con\ufb01rmation is thecostof making an error. For ex-ample, explicit con\ufb01rmation is common before a \ufb02ight is actually booked or moneyin an account is moved. Systems might have a four-tiered level of con\ufb01dence withthree thresholdsa,b, andg:<alow con\ufb01dence reject\u0000aabove the threshold con\ufb01rm explicitly\u0000bhigh con\ufb01dence con\ufb01rm implictly\u0000gvery high con\ufb01dence don\u2019t con\ufb01rm at all24.4.5 Natural language generation in the dialogue-state modelFinally, once the policy has decided what speech act to generate, the natural languagegeneration component needs to generate the text of a response to the user.Once a dialogue act has been decided, we need to generate the text of the re-sponse to the user. The task of natural language generation (NLG) in the information-state architecture is often modeled in two stages,content planning(what to say),contentplanningandsentence realization(how to say it).sentencerealizationHere we\u2019ll assume content planning has been done by the dialogue policy, whichhas chosen the dialogue act to generate, and chosen some attributes (slots and values)that the planner wants to say to the user (either to give the user the answer, or as partof a con\ufb01rmation strategy).Fig.24.16shows some sample input/outputs for the sentence realization phase.In the \ufb01rst example, the content planner has chosen the dialogue actRECOMMENDand some particular slots (name, neighborhood, cuisine) and their \ufb01llers. The goalof the sentence realizer is",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 65,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "four-tiered level of con\ufb01dence withthree thresholdsa,b, andg:<alow con\ufb01dence reject\u0000aabove the threshold con\ufb01rm explicitly\u0000bhigh con\ufb01dence con\ufb01rm implictly\u0000gvery high con\ufb01dence don\u2019t con\ufb01rm at all24.4.5 Natural language generation in the dialogue-state modelFinally, once the policy has decided what speech act to generate, the natural languagegeneration component needs to generate the text of a response to the user.Once a dialogue act has been decided, we need to generate the text of the re-sponse to the user. The task of natural language generation (NLG) in the information-state architecture is often modeled in two stages,content planning(what to say),contentplanningandsentence realization(how to say it).sentencerealizationHere we\u2019ll assume content planning has been done by the dialogue policy, whichhas chosen the dialogue act to generate, and chosen some attributes (slots and values)that the planner wants to say to the user (either to give the user the answer, or as partof a con\ufb01rmation strategy).Fig.24.16shows some sample input/outputs for the sentence realization phase.In the \ufb01rst example, the content planner has chosen the dialogue actRECOMMENDand some particular slots (name, neighborhood, cuisine) and their \ufb01llers. The goalof the sentence realizer is to generate a sentence like lines 1 or 2 shown in the \ufb01gure,by training on many such examples of representation/sentence pairs from a largecorpus of labeled dialogues.Training data is hard to come by; we are unlikely to see every possible restaurantwith every possible attribute in many possible differently worded sentences. There-",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 66,
      "token_count": 370,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 104\n\nNatural Language GenerationNLG  in information-state architecture modeled in two stages:\u2022content planning (what to say)\u2022sentence realization (how to say it). We'll focus on sentence realization here.\n\n## Page 105\n\nSentence RealizationAssume content planning has been done by the dialogue policy\u2022Chosen the dialogue act to generate \u2022Chosen some attributes (slots and values) that the planner wants to say to the user \u2022Either to give the user the answer, or as part of a confirmation strategy) \n\n## Page 106\n\n2 samples of Input and Output for Sentence Realizer24.4\u2022THEDIALOGUE-STATEARCHITECTURE25recommend(restaurant name= Au Midi, neighborhood = midtown,cuisine = french1 Au Midi is in Midtown and serves French food.2 There is a French restaurant in Midtown called Au Midi.recommend(restaurant name= Loch Fyne, neighborhood = citycentre, cuisine = seafood)3 Loch Fyne is in the City Center and serves seafood food.4 There is a seafood restaurant in the City Centre called Loch Fyne.Figure 24.16Two examples of inputs to the sentence realization phase of NLG, showingthe dialogue act and attributes prespeci\ufb01ed by the content planner. Line 1-2 and 3-4 show dif-ferent possible output sentences to be generated by the sentence realizer. From the restaurantrecommendation system ofNayak et al. (2017).fore it is common in sentence realization to increase the generality of the trainingexamples bydelexicalization. Delexicalization is the process of replacing speci\ufb01cdelexicalizationwords in the training set that represent slot values with a generic placeholder to-ken representing the slot. Fig.24.17shows the result of delexicalizing the trainingsentences in Fig.24.16.recommend(restaurant name= Au Midi, neighborhood = midtown,cuisine = french1restaurantnameis inneighborhoodand servescuisinefood.2 There is acuisinerestaurant inneighborhoodcalledrestaurantname.Figure 24.17Delexicalized sentences that can be used for generating many different relex-icalized sentences. From the restaurant recommendation system ofNayak et al. (2017).Mapping from frames to delexicalized sentences is generally done by encoderdecoder models (Wen et al. 2015a,Wen et al. 2015b,Mrk\u02c7si\u00b4c et al. 2017, interalia), trained on large hand-labeled corpora of task-oriented dialogue(Budzianowskiet al., 2018). The input to the encoder is a sequence of tokensxtthat representthe dialogue act and its arguments. Thus the dialogue actRECOMMENDand theattribute/value pairs service:decent, cuisine:null might be represented as a \ufb02at se-quence of tokens(Nayak et al., 2017), each mapped to a learned embeddingwt, asshown in Fig.24.18.\ndecentservice:RECOMMENDcuisine:null[name]hasdecentserviceENCODERDECODERFigure 24.18An encoder decoder sentence realizer mapping slots/\ufb01llers to English.The encoder reads all the input slot/value representations, and the decoder out-puts the following delexicalized English sentence:restaurantname has decent serviceWe can then use the input frame from the content planner torelexicalize(\ufb01ll in therelexicalizeexact restaurant or neighborhood or cuisine) resulting in:Au Midi has decent service",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 67,
      "token_count": 736,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 107\n\n24.4\u2022THEDIALOGUE-STATEARCHITECTURE25recommend(restaurant name= Au Midi, neighborhood = midtown,cuisine = french1 Au Midi is in Midtown and serves French food.2 There is a French restaurant in Midtown called Au Midi.recommend(restaurant name= Loch Fyne, neighborhood = citycentre, cuisine = seafood)3 Loch Fyne is in the City Center and serves seafood food.4 There is a seafood restaurant in the City Centre called Loch Fyne.Figure 24.16Two examples of inputs to the sentence realization phase of NLG, showingthe dialogue act and attributes prespeci\ufb01ed by the content planner. Line 1-2 and 3-4 show dif-ferent possible output sentences to be generated by the sentence realizer. From the restaurantrecommendation system ofNayak et al. (2017).fore it is common in sentence realization to increase the generality of the trainingexamples bydelexicalization. Delexicalization is the process of replacing speci\ufb01cdelexicalizationwords in the training set that represent slot values with a generic placeholder to-ken representing the slot. Fig.24.17shows the result of delexicalizing the trainingsentences in Fig.24.16.recommend(restaurant name= Au Midi, neighborhood = midtown,cuisine = french1restaurantnameis inneighborhoodand servescuisinefood.2 There is acuisinerestaurant inneighborhoodcalledrestaurantname.Figure 24.17Delexicalized sentences that can be used for generating many different relex-icalized sentences. From the restaurant recommendation system ofNayak et al. (2017).Mapping from frames to delexicalized sentences is generally done by encoderdecoder models (Wen et al. 2015a,Wen et al. 2015b,Mrk\u02c7si\u00b4c et al. 2017, interalia), trained on large hand-labeled corpora of task-oriented dialogue(Budzianowskiet al., 2018). The input to the encoder is a sequence of tokensxtthat representthe dialogue act and its arguments. Thus the dialogue actRECOMMENDand theattribute/value pairs service:decent, cuisine:null might be represented as a \ufb02at se-quence of tokens(Nayak et al., 2017), each mapped to a learned embeddingwt, asshown in Fig.24.18.\ndecentservice:RECOMMENDcuisine:null[name]hasdecentserviceENCODERDECODERFigure 24.18An encoder decoder sentence realizer mapping slots/\ufb01llers to English.The encoder reads all the input slot/value representations, and the decoder out-puts the following delexicalized English sentence:restaurantname has decent serviceWe can then use the input frame from the content planner torelexicalize(\ufb01ll in therelexicalizeexact restaurant or neighborhood or cuisine) resulting in:Au Midi has decent serviceSentence RealizationTraining data is hard to come by \u2022Don't see each restaurant in each situationCommon way to improve generalization:\u2022Delexicalization: replacing words in the training set that represent slot values with a generic placeholder token:",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 68,
      "token_count": 664,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 108\n\nSentence RealizationTraining data is hard to come by \u2022Don't see each restaurant in each situationCommon way to improve generalization:\u2022Delexicalization: replacing words in the training set that represent slot values with a generic placeholder token:24.4\u2022THEDIALOGUE-STATEARCHITECTURE25recommend(restaurant name= Au Midi, neighborhood = midtown,cuisine = french1 Au Midi is in Midtown and serves French food.2 There is a French restaurant in Midtown called Au Midi.recommend(restaurant name= Loch Fyne, neighborhood = citycentre, cuisine = seafood)3 Loch Fyne is in the City Center and serves seafood food.4 There is a seafood restaurant in the City Centre called Loch Fyne.Figure 24.16Two examples of inputs to the sentence realization phase of NLG, showingthe dialogue act and attributes prespeci\ufb01ed by the content planner. Line 1-2 and 3-4 show dif-ferent possible output sentences to be generated by the sentence realizer. From the restaurantrecommendation system ofNayak et al. (2017).fore it is common in sentence realization to increase the generality of the trainingexamples bydelexicalization. Delexicalization is the process of replacing speci\ufb01cdelexicalizationwords in the training set that represent slot values with a generic placeholder to-ken representing the slot. Fig.24.17shows the result of delexicalizing the trainingsentences in Fig.24.16.recommend(restaurant name= Au Midi, neighborhood = midtown,cuisine = french1restaurantnameis inneighborhoodand servescuisinefood.2 There is acuisinerestaurant inneighborhoodcalledrestaurantname.Figure 24.17Delexicalized sentences that can be used for generating many different relex-icalized sentences. From the restaurant recommendation system ofNayak et al. (2017).Mapping from frames to delexicalized sentences is generally done by encoderdecoder models (Wen et al. 2015a,Wen et al. 2015b,Mrk\u02c7si\u00b4c et al. 2017, interalia), trained on large hand-labeled corpora of task-oriented dialogue(Budzianowskiet al., 2018). The input to the encoder is a sequence of tokensxtthat representthe dialogue act and its arguments. Thus the dialogue actRECOMMENDand theattribute/value pairs service:decent, cuisine:null might be represented as a \ufb02at se-quence of tokens(Nayak et al., 2017), each mapped to a learned embeddingwt, asshown in Fig.24.18.\ndecentservice:RECOMMENDcuisine:null[name]hasdecentserviceENCODERDECODERFigure 24.18An encoder decoder sentence realizer mapping slots/\ufb01llers to English.The encoder reads all the input slot/value representations, and the decoder out-puts the following delexicalized English sentence:restaurantname has decent serviceWe can then use the input frame from the content planner torelexicalize(\ufb01ll in therelexicalizeexact restaurant or neighborhood or cuisine) resulting in:Au Midi has decent service",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 69,
      "token_count": 664,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 109\n\n24.4\u2022THEDIALOGUE-STATEARCHITECTURE25recommend(restaurant name= Au Midi, neighborhood = midtown,cuisine = french1 Au Midi is in Midtown and serves French food.2 There is a French restaurant in Midtown called Au Midi.recommend(restaurant name= Loch Fyne, neighborhood = citycentre, cuisine = seafood)3 Loch Fyne is in the City Center and serves seafood food.4 There is a seafood restaurant in the City Centre called Loch Fyne.Figure 24.16Two examples of inputs to the sentence realization phase of NLG, showingthe dialogue act and attributes prespeci\ufb01ed by the content planner. Line 1-2 and 3-4 show dif-ferent possible output sentences to be generated by the sentence realizer. From the restaurantrecommendation system ofNayak et al. (2017).fore it is common in sentence realization to increase the generality of the trainingexamples bydelexicalization. Delexicalization is the process of replacing speci\ufb01cdelexicalizationwords in the training set that represent slot values with a generic placeholder to-ken representing the slot. Fig.24.17shows the result of delexicalizing the trainingsentences in Fig.24.16.recommend(restaurant name= Au Midi, neighborhood = midtown,cuisine = french1restaurantnameis inneighborhoodand servescuisinefood.2 There is acuisinerestaurant inneighborhoodcalledrestaurantname.Figure 24.17Delexicalized sentences that can be used for generating many different relex-icalized sentences. From the restaurant recommendation system ofNayak et al. (2017).Mapping from frames to delexicalized sentences is generally done by encoderdecoder models (Wen et al. 2015a,Wen et al. 2015b,Mrk\u02c7si\u00b4c et al. 2017, interalia), trained on large hand-labeled corpora of task-oriented dialogue(Budzianowskiet al., 2018). The input to the encoder is a sequence of tokensxtthat representthe dialogue act and its arguments. Thus the dialogue actRECOMMENDand theattribute/value pairs service:decent, cuisine:null might be represented as a \ufb02at se-quence of tokens(Nayak et al., 2017), each mapped to a learned embeddingwt, asshown in Fig.24.18.\ndecentservice:RECOMMENDcuisine:null[name]hasdecentserviceENCODERDECODERFigure 24.18An encoder decoder sentence realizer mapping slots/\ufb01llers to English.The encoder reads all the input slot/value representations, and the decoder out-puts the following delexicalized English sentence:restaurantname has decent serviceWe can then use the input frame from the content planner torelexicalize(\ufb01ll in therelexicalizeexact restaurant or neighborhood or cuisine) resulting in:Au Midi has decent serviceSentence Realization: mapping from frames to delexicalized sentencesEncoder-decoder models:Output:restaurant_namehas decent serviceRelexicalize to: Au Midi has decent service \n\n## Page 110\n\nGenerating clarification questionsUser: What do you have going to UNKNOWN WORD on the 5th? System: Going where on the 5th? The system repeats \u201cgoing\u201d and \u201con the 5th\u201d to make it clear which aspect of the user\u2019s turn the system needs to be clarifiedMethods for generating clarification questions:\u2022Rules like 'replace \u201cgoing to UNKNOWN WORD\u201d with \u201cgoing where\u201d'\u2022Classifiers that  guess which slots were misrecognized \n\n## Page 111\n\nChatbots and Dialogue SystemsThe Dialogue-State Architecture Continued: Policy and Generation\n\n## Page 112\n\nChatbots and Dialogue SystemsEvaluating Dialogue Systems",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 70,
      "token_count": 787,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 110\n\nGenerating clarification questionsUser: What do you have going to UNKNOWN WORD on the 5th? System: Going where on the 5th? The system repeats \u201cgoing\u201d and \u201con the 5th\u201d to make it clear which aspect of the user\u2019s turn the system needs to be clarifiedMethods for generating clarification questions:\u2022Rules like 'replace \u201cgoing to UNKNOWN WORD\u201d with \u201cgoing where\u201d'\u2022Classifiers that  guess which slots were misrecognized \n\n## Page 111\n\nChatbots and Dialogue SystemsThe Dialogue-State Architecture Continued: Policy and Generation\n\n## Page 112\n\nChatbots and Dialogue SystemsEvaluating Dialogue Systems\n\n## Page 113\n\nEvaluating chatbots and task-based dialogueTask-based dialogue:  \u2022mainly by measuring task performanceChatbots: \u2022mainly by human evaluation\n\n## Page 114\n\nChatbots are evaluated by humansParticipant evaluation: The human who talked to the chatbot assigns a scoreObserver evaluation: third party who reads a transcript of a human/chatbot conversation assigns a score.\n\n## Page 115\n\nParticipant evaluationHuman chats with model for 6 turns and rates 8 dimensions of quality:\u2022avoiding repetition, interestingness, making sense, fluency, listening, inquisitiveness, humanness, engagingness, (1) Avoiding Repetition: How repetitive was this user? \u25e6\u2022Repeated themselves over and over \u2022Sometimes said the same thing twice \u2022 Always said something new (3) Making sense: How often did this user say something which didn't make sense? \u25e6\u2022Never made any sense \u2022Most responses didn\u2019t make sense \u2022Some responses didn\u2019t make sense \u2022Everything made perfect sense (8) Engagingness:How much did you enjoy talking to this user?\u25e6\u2022Not at all \u2022A little \u2022Somewhat \u2022A lotAbigail See, Stephen Roller, DouweKiela, Jason Weston. 2019.What makes a good conversation? How controllable attributes affect human judgments. NAACL.\n\n## Page 116\n\nObserver evaluation: acute-evalAnnotators look at two conversations (A + B) and decide which is better:Engagingness: Who would you prefer to talk to for a long conversation? Interestingness: If you had to say one of these speakers is interesting and one is boring, who would you say is more interesting? Humanness: Which speaker sounds more human? Knowledgeable: If you had to say that one speaker is more knowledgeable and one is more ignorant, who is more knowledgeable? Li, M., Weston, J., and Roller, S. (2019). Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. NeurIPS19 Workshop on Conversational AI.",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 71,
      "token_count": 562,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 117",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 72,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "ACUTE-EV AL: Improved dialogue evaluation with optimized questions andmulti-turn comparisonsMargaret LiFacebook AI Researchmargaretli@fb.comJason WestonFacebook AI Researchjase@fb.comStephen RollerFacebook AI Researchroller@fb.comAbstractWhile dialogue remains an important end-goal of natural lan-guage research, the dif\ufb01culty of evaluation is an oft-quotedreason why it remains troublesome to make real progress to-wards its solution. Evaluation dif\ufb01culties are actually two-fold:not only do automatic metrics not correlate well with humanjudgments, but also human judgments themselves are in factdif\ufb01cult to measure. The two most used human judgment tests,single-turn pairwise evaluation and multi-turn Likert scores,both have serious \ufb02aws as we discuss in this work.We instead provide a novel procedure involving comparingtwo full dialogues, where a human judge is asked to pay at-tention to only one speaker within each, and make a pairwisejudgment. The questions themselves are optimized to maxi-mize the robustness of judgments across different annotators,resulting in better tests. We also show how these tests work inself-play model chat setups, resulting in faster, cheaper tests.We hope these tests become the de facto standard, and willrelease open-source code to that end.IntroductionDialogue between human and machine is an important end-goal of natural language research. The open-ended nature ofgenerating sequences in a multi-turn setup naturally makesthe task dif\ufb01cult to evaluate \u2013 with full evaluation pos-sessing many of the dif\ufb01culties of the task itself as it re-quires deep understanding of the content of the conversa-tion. As in many other natural language generation (NLG)tasks, automatic metrics have not been shown to have aclear correlation with human evaluations (Liu et al.2016;Lowe et al.2017). This means the current standard for alldialogue research involves human trials, which slows downresearch and greatly increases the cost of model development.Unfortunately, human judgments are themselves dif\ufb01-cult to measure. The two most used approaches, single-turn pairwise evaluation (Vinyals and Le 2015; Li et al.2016b), and multi-turn Likert scores (Venkatesh et al.2017;Zhang et al.2018; See et al.2019; Dinan et al.2019b;Dinan et al.2019a) have serious limitations. Single-turn pair-wise evaluation provides the bene\ufb01ts and simplicity of anA/B test, allowing for cheap and fast annotations, with com-parisons that are robust to annotator score bias, but fail to takeinto account the multi-turn aspect of conversations. To givea trivial example, such comparisons fail to capture whether",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 73,
      "token_count": 573,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 1:ACUTE-EV ALasks humans to compare two multi-turn dialogues, and independent of the gray speakers, choosebetween Speaker 1 (light blue) and Speaker 2 (dark blue).the model would repeat itself in a multi-turn conversationbecause they only look at one turn; repetition is a knownissue that humans dislike (See et al. 2019).Multi-turn Likert scores require the annotator to have amulti-turn conversation and then provide an integer score,which is more costly and time-consuming to run but evalu-ates full conversations more accurately. The integer scoreshowever suffer from differing bias and variance per annotator,which researchers have tried to mitigate (Kulikov et al.2018),but nevertheless due to its lack of sensitivity often yields com-parisons that are not statistically signi\ufb01cant. Furthermore, dueto strong anchoring effects during model evaluation, i.e. thatannotators are affected by the \ufb01rst systems they evaluate, Lik-ert comparisons are generally not comparable across multiplepapers. This mandates that evaluations of new models bearXiv:1909.03087v1  [cs.CL]  6 Sep 2019The ACUTE-EVAL methodLi et el., 2019\nFigure from Li, M., Weston, J., and Roller, S. (2019). Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. NeurIPS19 Workshop on Conversational AI.",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 74,
      "token_count": 304,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 118\n\nAutomatic evaluation is an open problemAutomatic evaluation methods (like the BLEU scores used for Machine Translation) are generally not used for chatbots. \u2022They correlate poorly with human judgements.One current research direction: Adversarial Evaluation\u2022Inspired by the Turing Test\u2022train a ``Turing-like'' classifier to distinguish between human responses and machine responses.\u2022The more successful a dialogue system is at fooling the evaluator, the better the system.\n\n## Page 119\n\nTask-based systems are evaluated by task success!1.End-to-end evaluation (Task Success)2.Slot Error Rate for a Sentence# of inserted/deleted/subsitutedslots# of total reference slots for sentence\n\n## Page 120\n\nEvaluation Metrics: Slot error rate\nSlot error rate: 1/3Task success: At end, was the correct meeting added to the calendar?\u201cMake an appointment with Chris at 10:30 in Gates 104\u201dSlotFillerPERSONChrisTIME11:30 a.m.ROOMGates 104",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 75,
      "token_count": 209,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 120\n\nEvaluation Metrics: Slot error rate\nSlot error rate: 1/3Task success: At end, was the correct meeting added to the calendar?\u201cMake an appointment with Chris at 10:30 in Gates 104\u201dSlotFillerPERSONChrisTIME11:30 a.m.ROOMGates 104\n\n## Page 121\n\nMore fine-grained metrics: User Satisfaction Survey28CHAPTER24\u2022CHATBOTS&DIALOGUESYSTEMSthe Turing test. The idea is to train a \u201cTuring-like\u201d evaluator classi\ufb01er to distinguishbetween human-generated responses and machine-generated responses. The moresuccessful a response generation system is at fooling this evaluator, the better thesystem.24.5.2 Evaluating Task-Based DialogueFor task-based dialogue, if the task is unambiguous, we can simply measure absolutetask success (did the system book the right plane \ufb02ight, or put the right event on thecalendar).To get a more \ufb01ne-grained idea of user happiness, we can compute auser sat-isfaction rating, having users interact with a dialogue system to perform a task andthen having them complete a questionnaire. For example, Fig.24.20shows samplemultiple-choice questions(Walker et al., 2001); responses are mapped into the rangeof 1 to 5, and then averaged over all questions to get a total user satisfaction rating.TTS PerformanceWas the system easy to understand ?ASR PerformanceDid the system understand what you said?Task EaseWas it easy to \ufb01nd the message/\ufb02ight/train you wanted?Interaction PaceWas the pace of interaction with the system appropriate?User ExpertiseDid you know what you could say at each point?System ResponseHow often was the system sluggish and slow to reply to you?Expected BehaviorDid the system work the way you expected it to?Future UseDo you think you\u2019d use the system in the future?Figure 24.20User satisfaction survey, adapted fromWalker et al. (2001).It is often economically infeasible to run complete user satisfaction studies afterevery change in a system. For this reason, it is useful to have performance evaluationheuristics that correlate well with human satisfaction. A number of such factors andheuristics have been studied, often grouped into two kinds of criteria: how well thesystem allows users to accomplish their goals (maximizing task success) with thefewest problems (minimizing costs):Task completion success:Task success can be measured by evaluating the cor-rectness of the total solution. For a frame-based architecture, this might besloterror rate, the percentage of slots that were \ufb01lled with the correct values:Slot Error Rate for a Sentence=# of inserted/deleted/subsituted slots# of total reference slots for sentence(24.9)For example consider a system given this sentence:(24.10)Make an appointment with Chris at 10:30 in Gates 104which extracted the following candidate slot structure:Slot FillerPERSON ChrisTIME 11:30 a.m.ROOM Gates 104Here the slot error rate is 1/3, since the TIME is wrong. Instead of error rate,slot precision, recall, and F-score can also be used. Slot error rate is also sometimescalledconcept error rate.Interestingly, sometimes the user\u2019sperceptionof whether they completed thetask is a better predictor of user satisfaction than the actual task completion success.(Walker et al., 2001).Walker, Marilyn, Candace Kamm, and Diane Litman. \"Towards developing general models of usability with PARADISE.\"Natural Language Engineering6, no. 3 & 4 (2000): 363-377.",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 76,
      "token_count": 767,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 122\n\nOther HeuristicsEfficiency cost: \u2022total elapsed time for the dialogue in seconds, \u2022the number of total turns or of system turns\u2022total number of queries \u2022\u201cturn correction ratio\u201d: % of turns that were used to correct errorsQuality cost: \u2022number of ASR rejection prompts. \u2022number of times the user had to barge in\n\n## Page 123\n\nChatbots and Dialogue SystemsEvaluating Dialogue Systems\n\n## Page 124\n\nChatbots and Dialogue SystemsDesign and Ethical Issues\n\n## Page 125\n\nDialog System Design: User-centered Design1.Study the users and task\u2022value-sensitive design2.Build simulations\u2022Wizard of Oz study3.Iteratively test design on users\nGould, John D., and Clayton Lewis. \"Designing for usability: key principles and what designers think.\"Communications of the ACM28, no. 3 (1985): 300-311.Bender, Emily M., and BatyaFriedman. \"Data statements for natural language processing: Toward mitigating system bias and enabling better science.\" TACL 6 (2018): 587-604.\n\n## Page 126\n\nEthical designEthical issues have long been known to be crucial in artificial agentsMary Shelley's Frankenstein\u2022creating agents without a consideration of ethical and humanistic concerns Ethical issues:\u2022Safety:  Systems abusing users, distracting drivers, or giving bad medical advice\u2022Representational harm: Systems demeaning particular social groups\u2022Privacy: Information Leakage \n\n## Page 127\n\nSafetyChatbots for mental health\u25e6Extremely important not to say the wrong thingIn-vehicle conversational agents\u25e6Must be aware of environment, driver's level of attention\nPeter Henderson, KoustuvSinha, Nicolas Angelard-Gontier, Nan Rosemary Ke, Genevieve Fried, Ryan Lowe, and Joelle Pineau. 2018. Ethical Challenges in Data-Driven Dialogue Systems. In 2018 AAAI/ACM Conference on AI, Ethics, and Society (AIES \u201918), \n\n## Page 128\n\nAbuse and Representation Harm: The case of Microsoft TayExperimental Twitter chatbot launched in 2016\u25e6given the profile personality of an 18-to 24-year-old American woman\u25e6could share horoscopes, tell jokes, \u25e6asked people to send selfies\u25e6used informal language, slang, emojis, and GIFs, \u25e6Designed to learn from users (IR-based) \n\n## Page 129\n\nThe case of Microsoft TayImmediately Tay turned offensive and abusive\u25e6Obscene and inflammatory tweets\u25e6Nazi propaganda, conspiracy theories\u25e6Began harassing women online\u25e6Reflecting racism and misogyny of Twitter usersMicrosoft took Tay down after 16 hoursLessons:\u25e6User response must be considered in the design phaseGina Neff and Peter Nagy 2016. Talking to Bots: Symbiotic Agency and the Case of Tay. International Journal of Communication 10(2016), 4915\u20134931 \n\n## Page 130\n\nBias in training datasetsHenderson et al. ran hate-speech and bias detectors on standard training sets for dialogue systems:\u25e6Twitter, Reddit, other dialogue datasetsFound bias and hate-speech\u25e6In training data\u25e6In dialogue models trained on the data\nPeter Henderson, KoustuvSinha, Nicolas Angelard-Gontier, Nan Rosemary Ke, Genevieve Fried, Ryan Lowe, and Joelle Pineau. 2018. Ethical Challenges in Data-Driven Dialogue Systems. In 2018 AAAI/ACM Conference on AI, Ethics, and Society (AIES \u201918),",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 77,
      "token_count": 757,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 129\n\nThe case of Microsoft TayImmediately Tay turned offensive and abusive\u25e6Obscene and inflammatory tweets\u25e6Nazi propaganda, conspiracy theories\u25e6Began harassing women online\u25e6Reflecting racism and misogyny of Twitter usersMicrosoft took Tay down after 16 hoursLessons:\u25e6User response must be considered in the design phaseGina Neff and Peter Nagy 2016. Talking to Bots: Symbiotic Agency and the Case of Tay. International Journal of Communication 10(2016), 4915\u20134931 \n\n## Page 130\n\nBias in training datasetsHenderson et al. ran hate-speech and bias detectors on standard training sets for dialogue systems:\u25e6Twitter, Reddit, other dialogue datasetsFound bias and hate-speech\u25e6In training data\u25e6In dialogue models trained on the data\nPeter Henderson, KoustuvSinha, Nicolas Angelard-Gontier, Nan Rosemary Ke, Genevieve Fried, Ryan Lowe, and Joelle Pineau. 2018. Ethical Challenges in Data-Driven Dialogue Systems. In 2018 AAAI/ACM Conference on AI, Ethics, and Society (AIES \u201918), \n\n## Page 131\n\nPrivacy: Training on user dataAccidental information leakage\u2022\u201cComputer, turn on the lights [answers the phone] Hi, yes, my password is...\u201d\u2022Henderson show in simulation that this leakage can occur.Intentional information leakage\u2022Dialogue systems that are designed to send user data to developer or advertisers\u2022Important to consider privacy-preserving dialogue systemsPeter Henderson, KoustuvSinha, Nicolas Angelard-Gontier, Nan Rosemary Ke, Genevieve Fried, Ryan Lowe, and Joelle Pineau. 2018. Ethical Challenges in Data-Driven Dialogue Systems. In 2018 AAAI/ACM Conference on AI, Ethics, and Society (AIES \u201918), Campagna, Giovanni, Rakesh Ramesh, SileiXu, Michael Fischer, and Monica S. Lam. \"Almond: The architecture of an open, crowdsourced, privacy-preserving, programmable virtual assistant.\" InProceedings of the 26th International Conference on World Wide Web, pp. 341-350. 2017.\n\n## Page 132\n\nChatbots and Dialogue SystemsDesign and Ethical Issues",
    "metadata": {
      "source": "24_Dialogue_May_6_2021",
      "chunk_id": 78,
      "token_count": 486,
      "chapter_title": ""
    }
  }
]