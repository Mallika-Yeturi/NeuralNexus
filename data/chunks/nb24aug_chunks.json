[
  {
    "content": "# nb24aug\n\n## Page 1\n\nText Classification and Naive BayesThe Task of Text Classification\n\n## Page 2\n\nIs this spam?\n\n## Page 3\n\nWho wrote which Federalist Papers?1787-8: essays anonymously written by:     Alexander Hamilton, James Madison, and John Jay to convince New York to ratify U.S Constitution  Authorship of 12 of the letters unclear between:1963: solved by Mosteller and Wallace using Bayesian methods\nJames MadisonAlexander Hamilton\n\n## Page 4\n\nPositive or negative movie review?unbelievably disappointing Full of zany characters and richly applied satire, and some great plot twiststhis is the greatest screwball comedy ever filmedIt was pathetic. The worst part about it was the boxing scenes.4\n\n## Page 5\n\nWhat is the subject of this article?Antogonists and InhibitorsBlood SupplyChemistryDrug TherapyEmbryologyEpidemiology\u20265MeSH Subject Category Hierarchy?MEDLINE Article\n\n## Page 6\n\nText ClassificationAssigning subject categories, topics, or genresSpam detectionAuthorship identification (who wrote this?)Language Identification (is this Portuguese?)Sentiment analysis\u2026\n\n## Page 7\n\nText Classification: definitionInput:\u25e6 a document d\u25e6 a fixed set of classes  C = {c1, c2,\u2026, cJ}Output: a predicted class c \u00ce C\n\n## Page 8\n\nBasic Classification Method: Hand-coded rulesRules based on combinations of words or other features\u25e6 spam: black-list-address OR (\u201cdollars\u201d AND \u201chave been selected\u201d)Accuracy can be high\u2022In very specific domains\u2022If rules are carefully refined by expertsBut:\u2022building and maintaining rules is expensive\u2022they are too literal and specific: \"high-precision, low-recall\"\n\n## Page 9\n\nClassification Method:Supervised Machine LearningInput: \u25e6a document d\u25e6 a fixed set of classes  C = {c1, c2,\u2026, cJ}\u25e6A training set of m hand-labeled documents (d1,c1),....,(dm,cm)Output: \u25e6a learned classifier \u03b3:d \u00e0 c\n9\n\n## Page 10\n\nClassification Methods:Supervised Machine LearningMany kinds of classifiers!\u2022Na\u00efve Bayes (this lecture)\u2022Logistic regression  \u2022Neural networks\u2022k-nearest neighbors\u2022\u2026We can also use pretrained large language models!\u2022Fine-tuned as classifiers\u2022Prompted to give a classification\n\n## Page 11\n\nText Classification and Naive BayesThe Naive Bayes Classifier\n\n## Page 12\n\nNaive Bayes IntuitionSimple (\"naive\") classification method based on Bayes ruleRelies on very simple representation of document\u25e6Bag of words",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 0,
      "token_count": 572,
      "chapter_title": "nb24aug"
    }
  },
  {
    "content": "## Page 10\n\nClassification Methods:Supervised Machine LearningMany kinds of classifiers!\u2022Na\u00efve Bayes (this lecture)\u2022Logistic regression  \u2022Neural networks\u2022k-nearest neighbors\u2022\u2026We can also use pretrained large language models!\u2022Fine-tuned as classifiers\u2022Prompted to give a classification\n\n## Page 11\n\nText Classification and Naive BayesThe Naive Bayes Classifier\n\n## Page 12\n\nNaive Bayes IntuitionSimple (\"naive\") classification method based on Bayes ruleRelies on very simple representation of document\u25e6Bag of words\n\n## Page 13\n\nThe Bag of Words Representation\n13ititititititIIII\nIloverecommendmoviethethethetheto\ntotoand\nandandseenseenyetwouldwithwhowhimsical\nwhilewhenevertimessweetseveralscenessatiricalromanticofmanageshumorhavehappyfunfriendfairydialoguebutconventionsareanyoneadventurealwaysagainaboutI love this movie! It's sweet, but with satirical humor. The dialogue is great and the adventure scenes are fun... It manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre. I would recommend it to just about anyone. I've seen it several times, and I'm always happy to see it again whenever I have a friend who hasn't seen it yet!it Ithetoandseenyetwouldwhimsicaltimessweetsatiricaladventuregenrefairyhumorhavegreat\u20266 54332111111111111\u2026ititititititIIII\nIloverecommendmoviethethethetheto\ntotoand\nandandseenseenyetwouldwithwhowhimsical\nwhilewhenevertimessweetseveralscenessatiricalromanticofmanageshumorhavehappyfunfriendfairydialoguebutconventionsareanyoneadventurealwaysagainaboutI love this movie! It's sweet, but with satirical humor. The dialogue is great and the adventure scenes are fun... It manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre. I would recommend it to just about anyone. I've seen it several times, and I'm always happy to see it again whenever I have a friend who hasn't seen it yet!it Ithetoandseenyetwouldwhimsicaltimessweetsatiricaladventuregenrefairyhumorhavegreat\u20266 54332111111111111\u2026ititititititIIII\nIloverecommendmoviethethethetheto\ntotoand\nandandseenseenyetwouldwithwhowhimsical\nwhilewhenevertimessweetseveralscenessatiricalromanticofmanageshumorhavehappyfunfriendfairydialoguebutconventionsareanyoneadventurealwaysagainaboutI love this movie! It's sweet, but with satirical humor. The dialogue is great and the adventure scenes are fun... It manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre. I would recommend it to just about anyone. I've seen it several times, and I'm always happy to see it again whenever I have a friend who hasn't seen it yet!it Ithetoandseenyetwouldwhimsicaltimessweetsatiricaladventuregenrefairyhumorhavegreat\u20266 54332111111111111\u2026\n\n## Page 14\n\nThe bag of words representation\u03b3()=cseen2sweet1whimsical1recommend1happy1......\n\n## Page 15\n\nBayes\u2019 Rule Applied to Documents and Classes\u2022For a document d and a class cP(c|d)=P(d|c)P(c)P(d)",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 1,
      "token_count": 789,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\nThe bag of words representation\u03b3()=cseen2sweet1whimsical1recommend1happy1......\n\n## Page 15\n\nBayes\u2019 Rule Applied to Documents and Classes\u2022For a document d and a class cP(c|d)=P(d|c)P(c)P(d)\n\n## Page 16\n\nNaive Bayes Classifier (I)cMAP=argmaxc\u2208CP(c|d)=argmaxc\u2208CP(d|c)P(c)P(d)=argmaxc\u2208CP(d|c)P(c)\nMAP is \u201cmaximum a posteriori\u201d  = most likely class\nBayes Rule\nDropping the denominator\n\n## Page 17\n\nNaive Bayes Classifier (II)cMAP=argmaxc\u2208CP(d|c)P(c)\nDocument d represented as features x1..xn=argmaxc\u2208CP(x1,x2,\u2026,xn|c)P(c)\n\"Likelihood\"\n\"Prior\"\n\n## Page 18\n\nNa\u00efve Bayes Classifier (IV)\nHow often does this class occur?cMAP=argmaxc\u2208CP(x1,x2,\u2026,xn|c)P(c)\nO(|X|n\u2022|C|) parameters\nWe can just count the relative frequencies in a corpus\nCould only be estimated if a very, very large number of training examples was available.\n\n## Page 19\n\nMultinomial Naive Bayes Independence AssumptionsBag of Words assumption: Assume position doesn\u2019t matterConditional Independence: Assume the feature probabilities P(xi|cj) are independent given the class c.P(x1,x2,\u2026,xn|c)P(x1,\u2026,xn|c)=P(x1|c)\u2022P(x2|c)\u2022P(x3|c)\u2022...\u2022P(xn|c)\n\n## Page 20\n\nMultinomial Naive Bayes ClassifiercMAP=argmaxc\u2208CP(x1,x2,\u2026,xn|c)P(c)cNB=argmaxc\u2208CP(cj)P(x|c)x\u2208X\u220f\n\n## Page 21\n\nApplying Multinomial Naive Bayes Classifiers to Text ClassificationcNB=argmaxcj\u2208CP(cj)P(xi|cj)i\u2208positions\u220fpositions \u00ac all word positions in test document        \n\n## Page 22\n\nProblems with multiplying lots of probsThere's a problem with this:Multiplying lots of probabilities can result in floating-point underflow!  .0006 * .0007 * .0009 * .01 * .5 * .000008\u2026.Idea:   Use logs, because  log(ab) = log(a) + log(b)  We'll sum logs of probabilities instead of multiplying probabilities!cNB=argmaxcj\u2208CP(cj)P(xi|cj)i\u2208positions\u220f",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 2,
      "token_count": 599,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21\n\nApplying Multinomial Naive Bayes Classifiers to Text ClassificationcNB=argmaxcj\u2208CP(cj)P(xi|cj)i\u2208positions\u220fpositions \u00ac all word positions in test document        \n\n## Page 22\n\nProblems with multiplying lots of probsThere's a problem with this:Multiplying lots of probabilities can result in floating-point underflow!  .0006 * .0007 * .0009 * .01 * .5 * .000008\u2026.Idea:   Use logs, because  log(ab) = log(a) + log(b)  We'll sum logs of probabilities instead of multiplying probabilities!cNB=argmaxcj\u2208CP(cj)P(xi|cj)i\u2208positions\u220f\n\n## Page 23\n\nWe actually do everything in log spaceInstead of this:This:Notes:1) Taking log doesn't change the ranking of classes! The class with highest probability also has highest log probability!2) It's a linear model: Just a max of a sum of weights: a linear function of the inputs So naive bayes is a linear classifier\n<latexit sha1_base64=\"o0LQfSf3I3G0xas3oLJOwQZR0GU=\">AAACoXicbVFdaxQxFM2MH63r16qPggQXoSIsMwWxL0JpfdAHyypuW5gMQyZ7ZzZ2koxJRnaJ+V/+Dt/8N2Z2R6itF0IO597Dvffcsm24sUnyO4pv3Lx1e2f3zujuvfsPHo4fPT41qtMM5kw1Sp+X1EDDJcwttw2ctxqoKBs4Ky+O+/zZd9CGK/nFrlvIBa0lrzijNlDF+CdZQEWorgVdOSKoXarWES3wlvJ+REqouXTwTVKt6dqPWOGIhZV1J0fe47d4UBeOFV8Jl/jYY9JAZbPwqRrP9gL/Er/CxHSicLwvCZ1KtXKtMrwfw3jv/xavCv6jFxDN66XNMZFKdqIETUAuLk1RjCfJNNkEvg7SAUzQELNi/IssFOsESMsaakyWJq3NHdWWswbCnp2BlrILWkMWoKQCTO42Dnv8IjALXCkdnrR4w15WOCqMWYsyVPYemqu5nvxfLutsdZA7LtvOgmTbRlXXYKtwfy684BqYbdYBUKaDWwyzJdWU2XDU3oT06srXwen+NH09TT7tTw6PBjt20VP0HO2hFL1Bh+g9mqE5YtGz6F30MTqJJ/GHeBZ/3pbG0aB5gv6JOPsD0yvRAA==</latexit>cNB= argmaxcj2C24logP(cj)+Xi2positionslogP(xi|cj)35cNB=argmaxcj\u2208CP(cj)P(xi|cj)i\u2208positions\u220f\n\n## Page 24\n\nText Classification and Naive BayesThe Naive Bayes Classifier\n\n## Page 25\n\nText Classification and Na\u00efve BayesNaive Bayes: Learning",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 3,
      "token_count": 785,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24\n\nText Classification and Naive BayesThe Naive Bayes Classifier\n\n## Page 25\n\nText Classification and Na\u00efve BayesNaive Bayes: Learning\n\n## Page 26\n\nLearning the Multinomial Naive Bayes ModelFirst attempt: maximum likelihood estimates\u25e6simply use the frequencies in the dataSec.13.3\n\u02c6P(wi|cj)=count(wi,cj)count(w,cj)w\u2208V\u2211!\ud835\udc43\ud835\udc50!=\ud835\udc41\"!\ud835\udc41#$#%&\n\n## Page 27\n\nParameter estimationCreate mega-document for topic j by concatenating all docs in this topic\u25e6Use frequency of w in mega-documentfraction of times word wi appears among all words in documents of topic cj\u02c6P(wi|cj)=count(wi,cj)count(w,cj)w\u2208V\u2211\n\n## Page 28\n\nProblem with Maximum LikelihoodWhat if we have seen no training documents with the word fantastic and classified in the topic positive (thumbs-up)?Zero probabilities cannot be conditioned away, no matter the other evidence!\u02c6P(\"fantastic\" positive) = count(\"fantastic\", positive)count(w,positivew\u2208V\u2211) = 0cMAP=argmaxc\u02c6P(c)\u02c6P(xi|c)i\u220fSec.13.3\n\n## Page 29\n\nLaplace (add-1) smoothing for Na\u00efve Bayes\u02c6P(wi|c)=count(wi,c)+1count(w,c)+1()w\u2208V\u2211=count(wi,c)+1count(w,cw\u2208V\u2211)#$%%&'(( + V\u02c6P(wi|c)=count(wi,c)count(w,c)()w\u2208V\u2211\n\n## Page 30\n\nMultinomial Na\u00efve Bayes: LearningCalculate P(cj) terms\u25e6For each cj in C do docsj \u00ac all docs with  class =cjP(wk|cj)\u2190nk+\u03b1n+\u03b1|Vocabulary|P(cj)\u2190|docsj||total # documents|\u2022Calculate P(wk | cj) terms\u2022Textj \u00ac single doc containing all docsj\u2022For each word wk in Vocabulary    nk \u00ac # of occurrences of wk in Textj\u2022From training corpus, extract Vocabulary\n\n## Page 31\n\nUnknown wordsWhat about unknown words\u25e6that appear in our test data \u25e6but not in our training data or vocabulary?We ignore them\u25e6Remove them from the test document!\u25e6Pretend they weren't there!\u25e6Don't include any probability for them at all!Why don't we build an unknown word model?\u25e6It doesn't help: knowing which class has more unknown words is not generally helpful!\n\n## Page 32\n\nStop wordsSome systems ignore stop words\u25e6Stop words: very frequent words like the and a.\u25e6Sort the vocabulary by word frequency in training set\u25e6Call the top 10 or 50 words the stopword list.\u25e6Remove all stop words from both training and test sets\u25e6As if they were never there!But removing stop words doesn't usually help\u2022So in practice most NB algorithms use all words and don't use stopword lists\n\n## Page 33\n\nText Classification and Naive BayesNaive Bayes: Learning\n\n## Page 34\n\nText Classification and Naive BayesSentiment and Binary Naive Bayes",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 4,
      "token_count": 734,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 31\n\nUnknown wordsWhat about unknown words\u25e6that appear in our test data \u25e6but not in our training data or vocabulary?We ignore them\u25e6Remove them from the test document!\u25e6Pretend they weren't there!\u25e6Don't include any probability for them at all!Why don't we build an unknown word model?\u25e6It doesn't help: knowing which class has more unknown words is not generally helpful!\n\n## Page 32\n\nStop wordsSome systems ignore stop words\u25e6Stop words: very frequent words like the and a.\u25e6Sort the vocabulary by word frequency in training set\u25e6Call the top 10 or 50 words the stopword list.\u25e6Remove all stop words from both training and test sets\u25e6As if they were never there!But removing stop words doesn't usually help\u2022So in practice most NB algorithms use all words and don't use stopword lists\n\n## Page 33\n\nText Classification and Naive BayesNaive Bayes: Learning\n\n## Page 34\n\nText Classification and Naive BayesSentiment and Binary Naive Bayes\n\n## Page 35\n\nLet's do a worked sentiment example!4.3\u2022WORKED EXAMPLE74.3 Worked exampleLet\u2019s walk through an example of training and testing naive Bayes with add-onesmoothing. We\u2019ll use a sentiment analysis domain with the two classes positive(+) and negative (-), and take the following miniature training and test documentssimpli\ufb01ed from actual movie reviews.CatDocumentsTraining-just plain boring-entirely predictable and lacks energy-no surprises and very few laughs+very powerful+the most fun \ufb01lm of the summerTest?predictable with no funThe priorP(c)for the two classes is computed via Eq.4.11asNcNdoc:P(\u0000)=35P(+) =25The wordwithdoesn\u2019t occur in the training set, so we drop it completely (asmentioned above, we don\u2019t use unknown word models for naive Bayes). The like-lihoods from the training set for the remaining three words \u201cpredictable\u201d, \u201cno\u201d, and\u201cfun\u201d, are as follows, from Eq.4.14(computing the probabilities for the remainderof the words in the training set is left as an exercise for the reader):P(\u201cpredictable\u201d|\u0000)=1+114+20P(\u201cpredictable\u201d|+) =0+19+20P(\u201cno\u201d|\u0000)=1+114+20P(\u201cno\u201d|+) =0+19+20P(\u201cfun\u201d|\u0000)=0+114+20P(\u201cfun\u201d|+) =1+19+20For the test sentence S = \u201cpredictable with no fun\u201d, after removing the word \u2018with\u2019,the chosen class, via Eq.4.9, is therefore computed as follows:P(\u0000)P(S|\u0000)=35\u21e52\u21e52\u21e51343=6.1\u21e510\u00005P(+)P(S|+) =25\u21e51\u21e51\u21e52293=3.2\u21e510\u00005The model thus predicts the classnegativefor the test sentence.4.4 Optimizing for Sentiment AnalysisWhile standard naive Bayes text classi\ufb01cation can work well for sentiment analysis,some small changes are generally employed that improve performance.First, for sentiment classi\ufb01cation and a number of other text classi\ufb01cation tasks,whether a word occurs or not seems to matter more than its frequency. Thus itoften improves performance to clip the word counts in each document at 1 (seethe end of the chapter for pointers to these results). This variant is calledbinary",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 5,
      "token_count": 780,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 36",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 6,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "A worked sentiment example with add-1 smoothing4.3\u2022WORKED EXAMPLE74.3 Worked exampleLet\u2019s walk through an example of training and testing naive Bayes with add-onesmoothing. We\u2019ll use a sentiment analysis domain with the two classes positive(+) and negative (-), and take the following miniature training and test documentssimpli\ufb01ed from actual movie reviews.CatDocumentsTraining-just plain boring-entirely predictable and lacks energy-no surprises and very few laughs+very powerful+the most fun \ufb01lm of the summerTest?predictable with no funThe priorP(c)for the two classes is computed via Eq.4.11asNcNdoc:P(\u0000)=35P(+) =25The wordwithdoesn\u2019t occur in the training set, so we drop it completely (asmentioned above, we don\u2019t use unknown word models for naive Bayes). The like-lihoods from the training set for the remaining three words \u201cpredictable\u201d, \u201cno\u201d, and\u201cfun\u201d, are as follows, from Eq.4.14(computing the probabilities for the remainderof the words in the training set is left as an exercise for the reader):P(\u201cpredictable\u201d|\u0000)=1+114+20P(\u201cpredictable\u201d|+) =0+19+20P(\u201cno\u201d|\u0000)=1+114+20P(\u201cno\u201d|+) =0+19+20P(\u201cfun\u201d|\u0000)=0+114+20P(\u201cfun\u201d|+) =1+19+20For the test sentence S = \u201cpredictable with no fun\u201d, after removing the word \u2018with\u2019,the chosen class, via Eq.4.9, is therefore computed as follows:P(\u0000)P(S|\u0000)=35\u21e52\u21e52\u21e51343=6.1\u21e510\u00005P(+)P(S|+) =25\u21e51\u21e51\u21e52293=3.2\u21e510\u00005The model thus predicts the classnegativefor the test sentence.4.4 Optimizing for Sentiment AnalysisWhile standard naive Bayes text classi\ufb01cation can work well for sentiment analysis,some small changes are generally employed that improve performance.First, for sentiment classi\ufb01cation and a number of other text classi\ufb01cation tasks,whether a word occurs or not seems to matter more than its frequency. Thus itoften improves performance to clip the word counts in each document at 1 (seethe end of the chapter for pointers to these results). This variant is calledbinary1. Prior from training:P(-) = 3/5P(+) = 2/52. Drop \"with\"4.3\u2022WORKED EXAMPLE74.3 Worked exampleLet\u2019s walk through an example of training and testing naive Bayes with add-onesmoothing. We\u2019ll use a sentiment analysis domain with the two classes positive(+) and negative (-), and take the following miniature training and test documentssimpli\ufb01ed from actual movie reviews.CatDocumentsTraining-just plain boring-entirely predictable and lacks energy-no surprises and very few laughs+very powerful+the most fun \ufb01lm of the summerTest?predictable with no funThe priorP(c)for the two classes is computed via Eq.4.11asNcNdoc:P(\u0000)=35P(+) =25The wordwithdoesn\u2019t occur in the training set, so we drop it completely (asmentioned above, we don\u2019t use unknown word models for naive Bayes). The like-lihoods from the training set for the remaining three words \u201cpredictable\u201d, \u201cno\u201d, and\u201cfun\u201d, are as follows, from Eq.4.14(computing the probabilities for the remainderof the words in the training set is left",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 7,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "occurs or not seems to matter more than its frequency. Thus itoften improves performance to clip the word counts in each document at 1 (seethe end of the chapter for pointers to these results). This variant is calledbinary1. Prior from training:P(-) = 3/5P(+) = 2/52. Drop \"with\"4.3\u2022WORKED EXAMPLE74.3 Worked exampleLet\u2019s walk through an example of training and testing naive Bayes with add-onesmoothing. We\u2019ll use a sentiment analysis domain with the two classes positive(+) and negative (-), and take the following miniature training and test documentssimpli\ufb01ed from actual movie reviews.CatDocumentsTraining-just plain boring-entirely predictable and lacks energy-no surprises and very few laughs+very powerful+the most fun \ufb01lm of the summerTest?predictable with no funThe priorP(c)for the two classes is computed via Eq.4.11asNcNdoc:P(\u0000)=35P(+) =25The wordwithdoesn\u2019t occur in the training set, so we drop it completely (asmentioned above, we don\u2019t use unknown word models for naive Bayes). The like-lihoods from the training set for the remaining three words \u201cpredictable\u201d, \u201cno\u201d, and\u201cfun\u201d, are as follows, from Eq.4.14(computing the probabilities for the remainderof the words in the training set is left as an exercise for the reader):P(\u201cpredictable\u201d|\u0000)=1+114+20P(\u201cpredictable\u201d|+) =0+19+20P(\u201cno\u201d|\u0000)=1+114+20P(\u201cno\u201d|+) =0+19+20P(\u201cfun\u201d|\u0000)=0+114+20P(\u201cfun\u201d|+) =1+19+20For the test sentence S = \u201cpredictable with no fun\u201d, after removing the word \u2018with\u2019,the chosen class, via Eq.4.9, is therefore computed as follows:P(\u0000)P(S|\u0000)=35\u21e52\u21e52\u21e51343=6.1\u21e510\u00005P(+)P(S|+) =25\u21e51\u21e51\u21e52293=3.2\u21e510\u00005The model thus predicts the classnegativefor the test sentence.4.4 Optimizing for Sentiment AnalysisWhile standard naive Bayes text classi\ufb01cation can work well for sentiment analysis,some small changes are generally employed that improve performance.First, for sentiment classi\ufb01cation and a number of other text classi\ufb01cation tasks,whether a word occurs or not seems to matter more than its frequency. Thus itoften improves performance to clip the word counts in each document at 1 (seethe end of the chapter for pointers to these results). This variant is calledbinary4.3\u2022WORKED EXAMPLE74.3 Worked exampleLet\u2019s walk through an example of training and testing naive Bayes with add-onesmoothing. We\u2019ll use a sentiment analysis domain with the two classes positive(+) and negative (-), and take the following miniature training and test documentssimpli\ufb01ed from actual movie reviews.CatDocumentsTraining-just plain boring-entirely predictable and lacks energy-no surprises and very few laughs+very powerful+the most fun \ufb01lm of the summerTest?predictable with no funThe priorP(c)for the two classes is computed via Eq.4.11asNcNdoc:P(\u0000)=35P(+) =25The wordwithdoesn\u2019t occur in the training set, so we drop it completely (asmentioned above, we don\u2019t use unknown word models for naive Bayes). The like-lihoods from the training set for the remaining three words",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 8,
      "token_count": 798,
      "chapter_title": ""
    }
  },
  {
    "content": "test sentence.4.4 Optimizing for Sentiment AnalysisWhile standard naive Bayes text classi\ufb01cation can work well for sentiment analysis,some small changes are generally employed that improve performance.First, for sentiment classi\ufb01cation and a number of other text classi\ufb01cation tasks,whether a word occurs or not seems to matter more than its frequency. Thus itoften improves performance to clip the word counts in each document at 1 (seethe end of the chapter for pointers to these results). This variant is calledbinary4.3\u2022WORKED EXAMPLE74.3 Worked exampleLet\u2019s walk through an example of training and testing naive Bayes with add-onesmoothing. We\u2019ll use a sentiment analysis domain with the two classes positive(+) and negative (-), and take the following miniature training and test documentssimpli\ufb01ed from actual movie reviews.CatDocumentsTraining-just plain boring-entirely predictable and lacks energy-no surprises and very few laughs+very powerful+the most fun \ufb01lm of the summerTest?predictable with no funThe priorP(c)for the two classes is computed via Eq.4.11asNcNdoc:P(\u0000)=35P(+) =25The wordwithdoesn\u2019t occur in the training set, so we drop it completely (asmentioned above, we don\u2019t use unknown word models for naive Bayes). The like-lihoods from the training set for the remaining three words \u201cpredictable\u201d, \u201cno\u201d, and\u201cfun\u201d, are as follows, from Eq.4.14(computing the probabilities for the remainderof the words in the training set is left as an exercise for the reader):P(\u201cpredictable\u201d|\u0000)=1+114+20P(\u201cpredictable\u201d|+) =0+19+20P(\u201cno\u201d|\u0000)=1+114+20P(\u201cno\u201d|+) =0+19+20P(\u201cfun\u201d|\u0000)=0+114+20P(\u201cfun\u201d|+) =1+19+20For the test sentence S = \u201cpredictable with no fun\u201d, after removing the word \u2018with\u2019,the chosen class, via Eq.4.9, is therefore computed as follows:P(\u0000)P(S|\u0000)=35\u21e52\u21e52\u21e51343=6.1\u21e510\u00005P(+)P(S|+) =25\u21e51\u21e51\u21e52293=3.2\u21e510\u00005The model thus predicts the classnegativefor the test sentence.4.4 Optimizing for Sentiment AnalysisWhile standard naive Bayes text classi\ufb01cation can work well for sentiment analysis,some small changes are generally employed that improve performance.First, for sentiment classi\ufb01cation and a number of other text classi\ufb01cation tasks,whether a word occurs or not seems to matter more than its frequency. Thus itoften improves performance to clip the word counts in each document at 1 (seethe end of the chapter for pointers to these results). This variant is calledbinary3. Likelihoods from training:4. Scoring the test set:\ud835\udc5d\ud835\udc64!\ud835\udc50=\ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc61\ud835\udc64!,\ud835\udc50+1\u2211\"\u2208$\ud835\udc50\ud835\udc5c\ud835\udc62\ud835\udc5b\ud835\udc61\ud835\udc64,\ud835\udc50+|\ud835\udc49|/\ud835\udc43\ud835\udc50%=\ud835\udc41&!\ud835\udc41'(')*",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 9,
      "token_count": 740,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 37\n\nOptimizing for sentiment analysisFor tasks like sentiment, word occurrence seems to be more important than word frequency.\u25e6The occurrence of the word fantastic tells us a lot\u25e6The fact that it occurs 5 times may not tell us much more.Binary multinominal naive bayes, or binary NB\u25e6Clip our word counts at 1\u25e6Note: this is different than Bernoulli naive bayes; see the textbook at the end of the chapter.\n\n## Page 38\n\nBinary Multinomial Na\u00efve Bayes: LearningCalculate P(cj) terms\u25e6For each cj in C do docsj \u00ac all docs with  class =cjP(cj)\u2190|docsj||total # documents|P(wk|cj)\u2190nk+\u03b1n+\u03b1|Vocabulary|\u2022Textj \u00ac single doc containing all docsj\u2022For each word wk in Vocabulary    nk \u00ac # of occurrences of wk in Textj\u2022From training corpus, extract Vocabulary\u2022Calculate P(wk | cj) terms\u2022Remove duplicates in each doc:\u2022For each word type w in docj  \u2022Retain only a single instance of w\n\n## Page 39\n\nBinary Multinomial Naive Bayes on a test document d\n39First remove all duplicate words from dThen compute NB using the same equation: cNB=argmaxcj\u2208CP(cj)P(wi|cj)i\u2208positions\u220f",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 10,
      "token_count": 299,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 39\n\nBinary Multinomial Naive Bayes on a test document d\n39First remove all duplicate words from dThen compute NB using the same equation: cNB=argmaxcj\u2208CP(cj)P(wi|cj)i\u2208positions\u220f\n\n## Page 40\n\nBinary multinominal naive Bayes8CHAPTER4\u2022NAIVEBAYES ANDSENTIMENTCLASSIFICATIONmultinomial naive Bayesorbinary NB. The variant uses the same Eq.4.10exceptbinary NBthat for each document we remove all duplicate words before concatenating theminto the single big document. Fig.4.3shows an example in which a set of fourdocuments (shortened and text-normalized for this example) are remapped to binary,with the modi\ufb01ed counts shown in the table on the right. The example is workedwithout add-1 smoothing to make the differences clearer. Note that the results countsneed not be 1; the wordgreathas a count of 2 even for Binary NB, because it appearsin multiple documents.Four original documents:\u0000it was pathetic the worst part was theboxing scenes\u0000no plot twists or great scenes+and satire and great plot twists+great scenes great \ufb01lmAfter per-document binarization:\u0000it was pathetic the worst part boxingscenes\u0000no plot twists or great scenes+and satire great plot twists+great scenes \ufb01lmNB BinaryCounts Counts+\u0000+\u0000and2010boxing 0 1 0 1\ufb01lm 1 0 1 0great3121it 0 1 0 1no 0 1 0 1or 0 1 0 1part 0 1 0 1pathetic 0 1 0 1plot 1 1 1 1satire 1 0 1 0scenes 1 2 1 2the0201twists 1 1 1 1was0201worst 0 1 0 1Figure 4.3An example of binarization for the binary naive Bayes algorithm.A second important addition commonly made when doing text classi\ufb01cation forsentiment is to deal with negation. Consider the difference betweenI really like thismovie(positive) andI didn\u2019t like this movie(negative). The negation expressed bydidn\u2019tcompletely alters the inferences we draw from the predicatelike. Similarly,negation can modify a negative word to produce a positive review (don\u2019t dismiss this\ufb01lm,doesn\u2019t let us get bored).A very simple baseline that is commonly used in sentiment analysis to deal withnegation is the following: during text normalization, prepend the pre\ufb01xNOTtoevery word after a token of logical negation (n\u2019t, not, no, never) until the next punc-tuation mark. Thus the phrasedidn\u2019t like this movie , but Ibecomesdidn\u2019t NOT_like NOT_this NOT_movie , but INewly formed \u2018words\u2019 likeNOTlike,NOTrecommendwill thus occur more of-ten in negative document and act as cues for negative sentiment, while words likeNOTbored,NOTdismisswill acquire positive associations. We will return in Chap-ter 16 to the use of parsing to deal more accurately with the scope relationship be-tween these negation words and the predicates they modify, but this simple baselineworks quite well in practice.Finally, in some situations we might have insuf\ufb01cient labeled training data totrain accurate naive Bayes classi\ufb01ers using all words in the training set to estimatepositive and negative sentiment. In such cases we can instead derive the positive",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 11,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 41\n\nBinary multinominal naive Bayes8CHAPTER4\u2022NAIVEBAYES ANDSENTIMENTCLASSIFICATIONmultinomial naive Bayesorbinary NB. The variant uses the same Eq.4.10exceptbinary NBthat for each document we remove all duplicate words before concatenating theminto the single big document. Fig.4.3shows an example in which a set of fourdocuments (shortened and text-normalized for this example) are remapped to binary,with the modi\ufb01ed counts shown in the table on the right. The example is workedwithout add-1 smoothing to make the differences clearer. Note that the results countsneed not be 1; the wordgreathas a count of 2 even for Binary NB, because it appearsin multiple documents.Four original documents:\u0000it was pathetic the worst part was theboxing scenes\u0000no plot twists or great scenes+and satire and great plot twists+great scenes great \ufb01lmAfter per-document binarization:\u0000it was pathetic the worst part boxingscenes\u0000no plot twists or great scenes+and satire great plot twists+great scenes \ufb01lmNB BinaryCounts Counts+\u0000+\u0000and2010boxing 0 1 0 1\ufb01lm 1 0 1 0great3121it 0 1 0 1no 0 1 0 1or 0 1 0 1part 0 1 0 1pathetic 0 1 0 1plot 1 1 1 1satire 1 0 1 0scenes 1 2 1 2the0201twists 1 1 1 1was0201worst 0 1 0 1Figure 4.3An example of binarization for the binary naive Bayes algorithm.A second important addition commonly made when doing text classi\ufb01cation forsentiment is to deal with negation. Consider the difference betweenI really like thismovie(positive) andI didn\u2019t like this movie(negative). The negation expressed bydidn\u2019tcompletely alters the inferences we draw from the predicatelike. Similarly,negation can modify a negative word to produce a positive review (don\u2019t dismiss this\ufb01lm,doesn\u2019t let us get bored).A very simple baseline that is commonly used in sentiment analysis to deal withnegation is the following: during text normalization, prepend the pre\ufb01xNOTtoevery word after a token of logical negation (n\u2019t, not, no, never) until the next punc-tuation mark. Thus the phrasedidn\u2019t like this movie , but Ibecomesdidn\u2019t NOT_like NOT_this NOT_movie , but INewly formed \u2018words\u2019 likeNOTlike,NOTrecommendwill thus occur more of-ten in negative document and act as cues for negative sentiment, while words likeNOTbored,NOTdismisswill acquire positive associations. We will return in Chap-ter 16 to the use of parsing to deal more accurately with the scope relationship be-tween these negation words and the predicates they modify, but this simple baselineworks quite well in practice.Finally, in some situations we might have insuf\ufb01cient labeled training data totrain accurate naive Bayes classi\ufb01ers using all words in the training set to estimatepositive and negative sentiment. In such cases we can instead derive the positive",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 12,
      "token_count": 713,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 42\n\nBinary multinominal naive Bayes8CHAPTER4\u2022NAIVEBAYES ANDSENTIMENTCLASSIFICATIONmultinomial naive Bayesorbinary NB. The variant uses the same Eq.4.10exceptbinary NBthat for each document we remove all duplicate words before concatenating theminto the single big document. Fig.4.3shows an example in which a set of fourdocuments (shortened and text-normalized for this example) are remapped to binary,with the modi\ufb01ed counts shown in the table on the right. The example is workedwithout add-1 smoothing to make the differences clearer. Note that the results countsneed not be 1; the wordgreathas a count of 2 even for Binary NB, because it appearsin multiple documents.Four original documents:\u0000it was pathetic the worst part was theboxing scenes\u0000no plot twists or great scenes+and satire and great plot twists+great scenes great \ufb01lmAfter per-document binarization:\u0000it was pathetic the worst part boxingscenes\u0000no plot twists or great scenes+and satire great plot twists+great scenes \ufb01lmNB BinaryCounts Counts+\u0000+\u0000and2010boxing 0 1 0 1\ufb01lm 1 0 1 0great3121it 0 1 0 1no 0 1 0 1or 0 1 0 1part 0 1 0 1pathetic 0 1 0 1plot 1 1 1 1satire 1 0 1 0scenes 1 2 1 2the0201twists 1 1 1 1was0201worst 0 1 0 1Figure 4.3An example of binarization for the binary naive Bayes algorithm.A second important addition commonly made when doing text classi\ufb01cation forsentiment is to deal with negation. Consider the difference betweenI really like thismovie(positive) andI didn\u2019t like this movie(negative). The negation expressed bydidn\u2019tcompletely alters the inferences we draw from the predicatelike. Similarly,negation can modify a negative word to produce a positive review (don\u2019t dismiss this\ufb01lm,doesn\u2019t let us get bored).A very simple baseline that is commonly used in sentiment analysis to deal withnegation is the following: during text normalization, prepend the pre\ufb01xNOTtoevery word after a token of logical negation (n\u2019t, not, no, never) until the next punc-tuation mark. Thus the phrasedidn\u2019t like this movie , but Ibecomesdidn\u2019t NOT_like NOT_this NOT_movie , but INewly formed \u2018words\u2019 likeNOTlike,NOTrecommendwill thus occur more of-ten in negative document and act as cues for negative sentiment, while words likeNOTbored,NOTdismisswill acquire positive associations. We will return in Chap-ter 16 to the use of parsing to deal more accurately with the scope relationship be-tween these negation words and the predicates they modify, but this simple baselineworks quite well in practice.Finally, in some situations we might have insuf\ufb01cient labeled training data totrain accurate naive Bayes classi\ufb01ers using all words in the training set to estimatepositive and negative sentiment. In such cases we can instead derive the positive",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 13,
      "token_count": 713,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 43\n\nBinary multinominal naive Bayes8CHAPTER4\u2022NAIVEBAYES ANDSENTIMENTCLASSIFICATIONmultinomial naive Bayesorbinary NB. The variant uses the same Eq.4.10exceptbinary NBthat for each document we remove all duplicate words before concatenating theminto the single big document. Fig.4.3shows an example in which a set of fourdocuments (shortened and text-normalized for this example) are remapped to binary,with the modi\ufb01ed counts shown in the table on the right. The example is workedwithout add-1 smoothing to make the differences clearer. Note that the results countsneed not be 1; the wordgreathas a count of 2 even for Binary NB, because it appearsin multiple documents.Four original documents:\u0000it was pathetic the worst part was theboxing scenes\u0000no plot twists or great scenes+and satire and great plot twists+great scenes great \ufb01lmAfter per-document binarization:\u0000it was pathetic the worst part boxingscenes\u0000no plot twists or great scenes+and satire great plot twists+great scenes \ufb01lmNB BinaryCounts Counts+\u0000+\u0000and2010boxing 0 1 0 1\ufb01lm 1 0 1 0great3121it 0 1 0 1no 0 1 0 1or 0 1 0 1part 0 1 0 1pathetic 0 1 0 1plot 1 1 1 1satire 1 0 1 0scenes 1 2 1 2the0201twists 1 1 1 1was0201worst 0 1 0 1Figure 4.3An example of binarization for the binary naive Bayes algorithm.A second important addition commonly made when doing text classi\ufb01cation forsentiment is to deal with negation. Consider the difference betweenI really like thismovie(positive) andI didn\u2019t like this movie(negative). The negation expressed bydidn\u2019tcompletely alters the inferences we draw from the predicatelike. Similarly,negation can modify a negative word to produce a positive review (don\u2019t dismiss this\ufb01lm,doesn\u2019t let us get bored).A very simple baseline that is commonly used in sentiment analysis to deal withnegation is the following: during text normalization, prepend the pre\ufb01xNOTtoevery word after a token of logical negation (n\u2019t, not, no, never) until the next punc-tuation mark. Thus the phrasedidn\u2019t like this movie , but Ibecomesdidn\u2019t NOT_like NOT_this NOT_movie , but INewly formed \u2018words\u2019 likeNOTlike,NOTrecommendwill thus occur more of-ten in negative document and act as cues for negative sentiment, while words likeNOTbored,NOTdismisswill acquire positive associations. We will return in Chap-ter 16 to the use of parsing to deal more accurately with the scope relationship be-tween these negation words and the predicates they modify, but this simple baselineworks quite well in practice.Finally, in some situations we might have insuf\ufb01cient labeled training data totrain accurate naive Bayes classi\ufb01ers using all words in the training set to estimatepositive and negative sentiment. In such cases we can instead derive the positiveCounts can still be 2! Binarization is within-doc!\n\n## Page 44\n\nText Classification and Naive BayesSentiment and Binary Naive Bayes\n\n## Page 45\n\nText Classification and Naive BayesMore on Sentiment Classification",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 14,
      "token_count": 765,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 44\n\nText Classification and Naive BayesSentiment and Binary Naive Bayes\n\n## Page 45\n\nText Classification and Naive BayesMore on Sentiment Classification\n\n## Page 46\n\nSentiment Classification: Dealing with NegationI really like this movieI really don't like this movieNegation changes the meaning of \"like\" to negative.Negation can also change negative to positive-ish \u25e6Don't dismiss this film\u25e6Doesn't let us get bored\n\n## Page 47\n\nSentiment Classification: Dealing with NegationSimple baseline method:Add NOT_ to every word between negation and following punctuation:didn\u2019t like this movie , but Ididn\u2019t NOT_like NOT_this NOT_movie but I\nDas, Sanjiv and Mike Chen. 2001. Yahoo! for Amazon: Extracting market sentiment from stock message boards. In Proceedings of the Asia Pacific Finance Association Annual Conference (APFA).Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.  2002.  Thumbs up? Sentiment Classification using Machine Learning Techniques. EMNLP-2002, 79\u201486.\n\n## Page 48\n\nSentiment Classification: LexiconsSometimes we don't have enough labeled training dataIn that case, we can make use of pre-built word listsCalled lexiconsThere are various publically available lexicons\n\n## Page 49\n\nMPQA Subjectivity Cues LexiconHome page: https://mpqa.cs.pitt.edu/lexicons/subj_lexicon/6885 words from 8221 lemmas, annotated for intensity (strong/weak)\u25e62718 positive\u25e64912 negative+ : admirable, beautiful, confident, dazzling, ecstatic, favor, glee, great \u2212 : awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate 49TheresaWilson, JanyceWiebe, and Paul Hoffmann (2005). RecognizingContextualPolarityin Phrase-Level SentimentAnalysis. Proc. of HLT-EMNLP-2005.Riloff and Wiebe (2003). Learning extraction patterns for subjective expressions. EMNLP-2003.\n\n## Page 50\n\nThe General Inquirer\u25e6Home page: http://www.wjh.harvard.edu/~inquirer\u25e6List of Categories:  http://www.wjh.harvard.edu/~inquirer/homecat.htm\u25e6Spreadsheet: http://www.wjh.harvard.edu/~inquirer/inquirerbasic.xlsCategories:\u25e6Positiv (1915 words) and Negativ (2291 words)\u25e6Strong vs Weak, Active vs Passive, Overstated versus Understated\u25e6Pleasure, Pain, Virtue, Vice, Motivation, Cognitive Orientation, etcFree for Research UsePhilip J. Stone, Dexter C Dunphy, Marshall S. Smith, Daniel M. Ogilvie. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT Press\n\n## Page 51\n\nUsing Lexicons in Sentiment ClassificationAdd a feature that gets a count whenever a word from the lexicon occurs\u25e6E.g., a feature called \"this word occurs in the positive lexicon\" or \"this word occurs in the negative lexicon\"Now all positive words (good, great, beautiful, wonderful) or negative words count for that feature.Using 1-2 features isn't as good as using all the words.\u2022But when training data is sparse or not representative of the test set, dense lexicon features can help",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 15,
      "token_count": 734,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 50\n\nThe General Inquirer\u25e6Home page: http://www.wjh.harvard.edu/~inquirer\u25e6List of Categories:  http://www.wjh.harvard.edu/~inquirer/homecat.htm\u25e6Spreadsheet: http://www.wjh.harvard.edu/~inquirer/inquirerbasic.xlsCategories:\u25e6Positiv (1915 words) and Negativ (2291 words)\u25e6Strong vs Weak, Active vs Passive, Overstated versus Understated\u25e6Pleasure, Pain, Virtue, Vice, Motivation, Cognitive Orientation, etcFree for Research UsePhilip J. Stone, Dexter C Dunphy, Marshall S. Smith, Daniel M. Ogilvie. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT Press\n\n## Page 51\n\nUsing Lexicons in Sentiment ClassificationAdd a feature that gets a count whenever a word from the lexicon occurs\u25e6E.g., a feature called \"this word occurs in the positive lexicon\" or \"this word occurs in the negative lexicon\"Now all positive words (good, great, beautiful, wonderful) or negative words count for that feature.Using 1-2 features isn't as good as using all the words.\u2022But when training data is sparse or not representative of the test set, dense lexicon features can help\n\n## Page 52\n\nNaive Bayes in Other tasks: Spam FilteringSpamAssassin Features:\u25e6Mentions millions of (dollar) ((dollar) NN,NNN,NNN.NN)\u25e6From: starts with many numbers\u25e6Subject is all capitals\u25e6HTML has a low ratio of text to image area\u25e6\"One hundred percent guaranteed\"\u25e6Claims you can be removed from the list\n\n## Page 53\n\nNaive Bayes in Language IDDetermining what language a piece of text is written in.Features based on character n-grams do very wellImportant to train on lots of varieties of each language(e.g., American English varieties like African-American English, or English varieties around the world like Indian English)\n\n## Page 54\n\nSummary: Naive Bayes is Not So NaiveVery Fast, low storage requirementsWork well with very small amounts of training dataRobust to Irrelevant Features Irrelevant Features cancel each other without affecting resultsVery good in domains with many equally important features Decision Trees suffer from fragmentation in such cases \u2013 especially if little dataOptimal if the independence assumptions hold: If assumed independence is correct, then it is the Bayes Optimal Classifier for problemA good dependable baseline for text classification\u25e6But we will see other classifiers that give better accuracySlide from Chris Manning\n\n## Page 55\n\nText Classification and Naive BayesMore on Sentiment Classification\n\n## Page 56\n\nText Classification and Na\u00efve BayesNa\u00efve Bayes: Relationship to Language Modeling\n\n## Page 57\n\nDan JurafskyGenerative Model for Multinomial Na\u00efve Bayes\n57c=ChinaX1=ShanghaiX2=andX3=ShenzhenX4=issueX5=bonds\n\n## Page 58\n\nDan JurafskyNa\u00efve Bayes and Language Modeling\u2022Na\u00efve bayes classifiers can use any sort of feature\u2022URL, email address, dictionaries, network features\u2022But if, as in the previous slides\u2022We use only word features \u2022we use all of the words in the text (not a subset)\u2022Then \u2022Na\u00efve bayes has an important similarity to language modeling.58",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 16,
      "token_count": 730,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 55\n\nText Classification and Naive BayesMore on Sentiment Classification\n\n## Page 56\n\nText Classification and Na\u00efve BayesNa\u00efve Bayes: Relationship to Language Modeling\n\n## Page 57\n\nDan JurafskyGenerative Model for Multinomial Na\u00efve Bayes\n57c=ChinaX1=ShanghaiX2=andX3=ShenzhenX4=issueX5=bonds\n\n## Page 58\n\nDan JurafskyNa\u00efve Bayes and Language Modeling\u2022Na\u00efve bayes classifiers can use any sort of feature\u2022URL, email address, dictionaries, network features\u2022But if, as in the previous slides\u2022We use only word features \u2022we use all of the words in the text (not a subset)\u2022Then \u2022Na\u00efve bayes has an important similarity to language modeling.58\n\n## Page 59\n\nDan JurafskyEach class = a unigram language model\u2022Assigning each word: P(word | c)\u2022Assigning each sentence: P(s|c)=\u03a0 P(word|c)0.1 I0.1 love0.01 this0.05 fun0.1 film\u2026Ilovethisfunfilm0.10.1.050.010.1Class posP(s | pos) = 0.0000005 Sec.13.2.1\n\n## Page 60\n\nDan JurafskyNa\u00efve Bayes as a Language Model\u2022Which class assigns the higher probability to s?0.1 I0.1 love0.01 this0.05 fun0.1 filmModel posModel negfilmlovethisfunI0.10.10.010.050.10.10.0010.010.0050.2P(s|pos)  >  P(s|neg)0.2 I0.001 love0.01 this0.005 fun0.1 filmSec.13.2.1\n\n## Page 61\n\nText Classification and Na\u00efve BayesNa\u00efve Bayes: Relationship to Language Modeling\n\n## Page 62\n\nText Classification and Naive BayesPrecision, Recall, and F1\n\n## Page 63\n\nEvaluating Classifiers: How well does our classifier work?Let's first address binary classifiers:\u2022Is this email spam? spam (+)     or   not spam (-)\u2022Is this post about Delicious Pie Company? about Del. Pie Co (+)   or    not about Del. Pie Co(-)We'll need to know1.What did our classifier say about each email or post?2.What should our classifier have said, i.e.,  the correct answer, usually as defined by humans (\"gold label\")\n\n## Page 64\n\nFirst step in evaluation: The confusion matrixtrue positivefalse negativefalse positivetrue negativegold positivegold negativesystempositivesystemnegativegold standard labelssystemoutputlabelsrecall = tptp+fnprecision = tptp+fpaccuracy = tp+tntp+fp+tn+fn\n\n## Page 65\n\nAccuracy on the confusion matrixtrue positivefalse negativefalse positivetrue negativegold positivegold negativesystempositivesystemnegativegold standard labelssystemoutputlabelsrecall = tptp+fnprecision = tptp+fpaccuracy = tp+tntp+fp+tn+fn\n\n## Page 66\n\nWhy don't we use accuracy?Accuracy doesn't work well when we're dealing with uncommon or imbalanced classesSuppose we look at 1,000,000 social media posts to find Delicious Pie-lovers (or haters)\u2022100 of them talk about our pie\u2022999,900 are posts about something unrelatedImagine the following simple classifier Every post is \"not about pie\"",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 17,
      "token_count": 769,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 64\n\nFirst step in evaluation: The confusion matrixtrue positivefalse negativefalse positivetrue negativegold positivegold negativesystempositivesystemnegativegold standard labelssystemoutputlabelsrecall = tptp+fnprecision = tptp+fpaccuracy = tp+tntp+fp+tn+fn\n\n## Page 65\n\nAccuracy on the confusion matrixtrue positivefalse negativefalse positivetrue negativegold positivegold negativesystempositivesystemnegativegold standard labelssystemoutputlabelsrecall = tptp+fnprecision = tptp+fpaccuracy = tp+tntp+fp+tn+fn\n\n## Page 66\n\nWhy don't we use accuracy?Accuracy doesn't work well when we're dealing with uncommon or imbalanced classesSuppose we look at 1,000,000 social media posts to find Delicious Pie-lovers (or haters)\u2022100 of them talk about our pie\u2022999,900 are posts about something unrelatedImagine the following simple classifier Every post is \"not about pie\"\n\n## Page 67\n\nAccuracy re: pie poststrue positivefalse negativefalse positivetrue negativegold positivegold negativesystempositivesystemnegativegold standard labelssystemoutputlabelsrecall = tptp+fnprecision = tptp+fpaccuracy = tp+tntp+fp+tn+fn100 posts are about pie; 999,900 aren't\n\n## Page 68\n\nWhy don't we use accuracy?Accuracy of our \"nothing is pie\" classifier 999,900 true negatives  and 100 false negatives Accuracy is 999,900/1,000,000 = 99.99%! But useless at finding pie-lovers (or haters)!! Which was our goal!Accuracy doesn't work well for unbalanced classes  Most tweets are not about pie!\n\n## Page 69\n\nInstead of accuracy we use precision and recalltrue positivefalse negativefalse positivetrue negativegold positivegold negativesystempositivesystemnegativegold standard labelssystemoutputlabelsrecall = tptp+fnprecision = tptp+fpaccuracy = tp+tntp+fp+tn+fnPrecision: % of selected items that are correctRecall: % of correct items that are selected",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 18,
      "token_count": 453,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 70",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 19,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Precision/Recall aren't fooled by the\"just call everything negative\" classifier!Stupid classifier: Just say no: every tweet is \"not about pie\"\u2022100 tweets  talk about pie,   999,900 tweets don't\u2022Accuracy = 999,900/1,000,000 = 99.99%But the Recall and Precision for this classifier are terrible:12CHAPTER4\u2022NAIVEBAYES,TEXTCLASSIFICATION,ANDSENTIMENTwhile the other 999,900 are tweets about something completely unrelated. Imagine asimple classi\ufb01er that stupidly classi\ufb01ed every tweet as \u201cnot about pie\u201d. This classi\ufb01erwould have 999,900 true negatives and only 100 false negatives for an accuracy of999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we shouldbe happy with this classi\ufb01er? But of course this fabulous \u2018no pie\u2019 classi\ufb01er wouldbe completely useless, since it wouldn\u2019t \ufb01nd a single one of the customer commentswe are looking for. In other words, accuracy is not a good metric when the goal isto discover something that is rare, or at least not completely balanced in frequency,which is a very common situation in the world.That\u2019s why instead of accuracy we generally turn to two other metrics shown inFig.4.4:precisionandrecall.Precisionmeasures the percentage of the items thatprecisionthe system detected (i.e., the system labeled as positive) that are in fact positive (i.e.,are positive according to the human gold labels). Precision is de\ufb01ned asPrecision=true positivestrue positives + false positivesRecallmeasures the percentage of items actually present in the input that wererecallcorrectly identi\ufb01ed by the system. Recall is de\ufb01ned asRecall=true positivestrue positives + false negativesPrecision and recall will help solve the problem with the useless \u201cnothing ispie\u201d classi\ufb01er. This classi\ufb01er, despite having a fabulous accuracy of 99.99%, hasa terrible recall of 0 (since there are no true positives, and 100 false negatives, therecall is 0/100). You should convince yourself that the precision at \ufb01nding relevanttweets is equally problematic. Thus precision and recall, unlike accuracy, emphasizetrue positives: \ufb01nding the things that we are supposed to be looking for.There are many ways to de\ufb01ne a single metric that incorporates aspects of bothprecision and recall. The simplest of these combinations is theF-measure(vanF-measureRijsbergen,1975) , de\ufb01ned as:Fb=(b2+1)PRb2P+RThebparameter differentially weights the importance of recall and precision,based perhaps on the needs of an application. Values ofb>1 favor recall, whilevalues ofb<1 favor precision. Whenb=1, precision and recall are equally bal-anced; this is the most frequently used metric, and is called Fb=1or just F1:F1F1=2PRP+R(4.16)F-measure comes from a weighted harmonic mean of precision and recall. Theharmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-rocals:HarmonicMean(a1,a2,a3,a4,...,an)=n1a1+1a2+1a3+...+1an(4.17)and hence F-measure",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 20,
      "token_count": 743,
      "chapter_title": ""
    }
  },
  {
    "content": "0 (since there are no true positives, and 100 false negatives, therecall is 0/100). You should convince yourself that the precision at \ufb01nding relevanttweets is equally problematic. Thus precision and recall, unlike accuracy, emphasizetrue positives: \ufb01nding the things that we are supposed to be looking for.There are many ways to de\ufb01ne a single metric that incorporates aspects of bothprecision and recall. The simplest of these combinations is theF-measure(vanF-measureRijsbergen,1975) , de\ufb01ned as:Fb=(b2+1)PRb2P+RThebparameter differentially weights the importance of recall and precision,based perhaps on the needs of an application. Values ofb>1 favor recall, whilevalues ofb<1 favor precision. Whenb=1, precision and recall are equally bal-anced; this is the most frequently used metric, and is called Fb=1or just F1:F1F1=2PRP+R(4.16)F-measure comes from a weighted harmonic mean of precision and recall. Theharmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-rocals:HarmonicMean(a1,a2,a3,a4,...,an)=n1a1+1a2+1a3+...+1an(4.17)and hence F-measure isF=1a1P+(1\u0000a)1Ror\u2713withb2=1\u0000aa\u25c6F=(b2+1)PRb2P+R(4.18)12CHAPTER4\u2022NAIVEBAYES,TEXTCLASSIFICATION,ANDSENTIMENTwhile the other 999,900 are tweets about something completely unrelated. Imagine asimple classi\ufb01er that stupidly classi\ufb01ed every tweet as \u201cnot about pie\u201d. This classi\ufb01erwould have 999,900 true negatives and only 100 false negatives for an accuracy of999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we shouldbe happy with this classi\ufb01er? But of course this fabulous \u2018no pie\u2019 classi\ufb01er wouldbe completely useless, since it wouldn\u2019t \ufb01nd a single one of the customer commentswe are looking for. In other words, accuracy is not a good metric when the goal isto discover something that is rare, or at least not completely balanced in frequency,which is a very common situation in the world.That\u2019s why instead of accuracy we generally turn to two other metrics shown inFig.4.4:precisionandrecall.Precisionmeasures the percentage of the items thatprecisionthe system detected (i.e., the system labeled as positive) that are in fact positive (i.e.,are positive according to the human gold labels). Precision is de\ufb01ned asPrecision=true positivestrue positives + false positivesRecallmeasures the percentage of items actually present in the input that wererecallcorrectly identi\ufb01ed by the system. Recall is de\ufb01ned asRecall=true positivestrue positives + false negativesPrecision and recall will help solve the problem with the useless \u201cnothing ispie\u201d classi\ufb01er. This classi\ufb01er, despite having a fabulous accuracy of 99.99%, hasa terrible recall of 0 (since there are no true positives, and 100 false negatives, therecall is 0/100). You should convince yourself that the precision at \ufb01nding relevanttweets is equally problematic. Thus precision and recall, unlike accuracy, emphasizetrue positives: \ufb01nding the things that we are supposed to be looking for.There are many ways to de\ufb01ne a single metric that incorporates aspects of bothprecision and recall. The",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 21,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "is not a good metric when the goal isto discover something that is rare, or at least not completely balanced in frequency,which is a very common situation in the world.That\u2019s why instead of accuracy we generally turn to two other metrics shown inFig.4.4:precisionandrecall.Precisionmeasures the percentage of the items thatprecisionthe system detected (i.e., the system labeled as positive) that are in fact positive (i.e.,are positive according to the human gold labels). Precision is de\ufb01ned asPrecision=true positivestrue positives + false positivesRecallmeasures the percentage of items actually present in the input that wererecallcorrectly identi\ufb01ed by the system. Recall is de\ufb01ned asRecall=true positivestrue positives + false negativesPrecision and recall will help solve the problem with the useless \u201cnothing ispie\u201d classi\ufb01er. This classi\ufb01er, despite having a fabulous accuracy of 99.99%, hasa terrible recall of 0 (since there are no true positives, and 100 false negatives, therecall is 0/100). You should convince yourself that the precision at \ufb01nding relevanttweets is equally problematic. Thus precision and recall, unlike accuracy, emphasizetrue positives: \ufb01nding the things that we are supposed to be looking for.There are many ways to de\ufb01ne a single metric that incorporates aspects of bothprecision and recall. The simplest of these combinations is theF-measure(vanF-measureRijsbergen,1975) , de\ufb01ned as:Fb=(b2+1)PRb2P+RThebparameter differentially weights the importance of recall and precision,based perhaps on the needs of an application. Values ofb>1 favor recall, whilevalues ofb<1 favor precision. Whenb=1, precision and recall are equally bal-anced; this is the most frequently used metric, and is called Fb=1or just F1:F1F1=2PRP+R(4.16)F-measure comes from a weighted harmonic mean of precision and recall. Theharmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-rocals:HarmonicMean(a1,a2,a3,a4,...,an)=n1a1+1a2+1a3+...+1an(4.17)and hence F-measure isF=1a1P+(1\u0000a)1Ror\u2713withb2=1\u0000aa\u25c6F=(b2+1)PRb2P+R(4.18)",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 22,
      "token_count": 552,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 71\n\nA combined measure: F1F1 is a  combination of precision and recall.12CHAPTER4\u2022NAIVEBAYES,TEXTCLASSIFICATION,ANDSENTIMENTwhile the other 999,900 are tweets about something completely unrelated. Imagine asimple classi\ufb01er that stupidly classi\ufb01ed every tweet as \u201cnot about pie\u201d. This classi\ufb01erwould have 999,900 true negatives and only 100 false negatives for an accuracy of999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we shouldbe happy with this classi\ufb01er? But of course this fabulous \u2018no pie\u2019 classi\ufb01er wouldbe completely useless, since it wouldn\u2019t \ufb01nd a single one of the customer commentswe are looking for. In other words, accuracy is not a good metric when the goal isto discover something that is rare, or at least not completely balanced in frequency,which is a very common situation in the world.That\u2019s why instead of accuracy we generally turn to two other metrics shown inFig.4.4:precisionandrecall.Precisionmeasures the percentage of the items thatprecisionthe system detected (i.e., the system labeled as positive) that are in fact positive (i.e.,are positive according to the human gold labels). Precision is de\ufb01ned asPrecision=true positivestrue positives + false positivesRecallmeasures the percentage of items actually present in the input that wererecallcorrectly identi\ufb01ed by the system. Recall is de\ufb01ned asRecall=true positivestrue positives + false negativesPrecision and recall will help solve the problem with the useless \u201cnothing ispie\u201d classi\ufb01er. This classi\ufb01er, despite having a fabulous accuracy of 99.99%, hasa terrible recall of 0 (since there are no true positives, and 100 false negatives, therecall is 0/100). You should convince yourself that the precision at \ufb01nding relevanttweets is equally problematic. Thus precision and recall, unlike accuracy, emphasizetrue positives: \ufb01nding the things that we are supposed to be looking for.There are many ways to de\ufb01ne a single metric that incorporates aspects of bothprecision and recall. The simplest of these combinations is theF-measure(vanF-measureRijsbergen,1975) , de\ufb01ned as:Fb=(b2+1)PRb2P+RThebparameter differentially weights the importance of recall and precision,based perhaps on the needs of an application. Values ofb>1 favor recall, whilevalues ofb<1 favor precision. Whenb=1, precision and recall are equally bal-anced; this is the most frequently used metric, and is called Fb=1or just F1:F1F1=2PRP+R(4.16)F-measure comes from a weighted harmonic mean of precision and recall. Theharmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-rocals:HarmonicMean(a1,a2,a3,a4,...,an)=n1a1+1a2+1a3+...+1an(4.17)and hence F-measure isF=1a1P+(1\u0000a)1Ror\u2713withb2=1\u0000aa\u25c6F=(b2+1)PRb2P+R(4.18)",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 23,
      "token_count": 730,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 72",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 24,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "F1 is a special case of the general \"F-measure\"F-measure is the (weighted) harmonic mean of precision and recallF1 is a special case of F-measure with \u03b2=1, \u03b1=\u00bd12CHAPTER4\u2022NAIVEBAYES,TEXTCLASSIFICATION,ANDSENTIMENTwhile the other 999,900 are tweets about something completely unrelated. Imagine asimple classi\ufb01er that stupidly classi\ufb01ed every tweet as \u201cnot about pie\u201d. This classi\ufb01erwould have 999,900 true negatives and only 100 false negatives for an accuracy of999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we shouldbe happy with this classi\ufb01er? But of course this fabulous \u2018no pie\u2019 classi\ufb01er wouldbe completely useless, since it wouldn\u2019t \ufb01nd a single one of the customer commentswe are looking for. In other words, accuracy is not a good metric when the goal isto discover something that is rare, or at least not completely balanced in frequency,which is a very common situation in the world.That\u2019s why instead of accuracy we generally turn to two other metrics shown inFig.4.4:precisionandrecall.Precisionmeasures the percentage of the items thatprecisionthe system detected (i.e., the system labeled as positive) that are in fact positive (i.e.,are positive according to the human gold labels). Precision is de\ufb01ned asPrecision=true positivestrue positives + false positivesRecallmeasures the percentage of items actually present in the input that wererecallcorrectly identi\ufb01ed by the system. Recall is de\ufb01ned asRecall=true positivestrue positives + false negativesPrecision and recall will help solve the problem with the useless \u201cnothing ispie\u201d classi\ufb01er. This classi\ufb01er, despite having a fabulous accuracy of 99.99%, hasa terrible recall of 0 (since there are no true positives, and 100 false negatives, therecall is 0/100). You should convince yourself that the precision at \ufb01nding relevanttweets is equally problematic. Thus precision and recall, unlike accuracy, emphasizetrue positives: \ufb01nding the things that we are supposed to be looking for.There are many ways to de\ufb01ne a single metric that incorporates aspects of bothprecision and recall. The simplest of these combinations is theF-measure(vanF-measureRijsbergen,1975) , de\ufb01ned as:Fb=(b2+1)PRb2P+RThebparameter differentially weights the importance of recall and precision,based perhaps on the needs of an application. Values ofb>1 favor recall, whilevalues ofb<1 favor precision. Whenb=1, precision and recall are equally bal-anced; this is the most frequently used metric, and is called Fb=1or just F1:F1F1=2PRP+R(4.16)F-measure comes from a weighted harmonic mean of precision and recall. Theharmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-rocals:HarmonicMean(a1,a2,a3,a4,...,an)=n1a1+1a2+1a3+...+1an(4.17)and hence F-measure isF=1a1P+(1\u0000a)1Ror\u2713withb2=1\u0000aa\u25c6F=(b2+1)PRb2P+R(4.18)12CHAPTER4\u2022NAIVEBAYES,TEXTCLASSIFICATION,ANDSENTIMENTwhile the other 999,900 are tweets about something completely unrelated. Imagine asimple classi\ufb01er that stupidly",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 25,
      "token_count": 795,
      "chapter_title": ""
    }
  },
  {
    "content": "bothprecision and recall. The simplest of these combinations is theF-measure(vanF-measureRijsbergen,1975) , de\ufb01ned as:Fb=(b2+1)PRb2P+RThebparameter differentially weights the importance of recall and precision,based perhaps on the needs of an application. Values ofb>1 favor recall, whilevalues ofb<1 favor precision. Whenb=1, precision and recall are equally bal-anced; this is the most frequently used metric, and is called Fb=1or just F1:F1F1=2PRP+R(4.16)F-measure comes from a weighted harmonic mean of precision and recall. Theharmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-rocals:HarmonicMean(a1,a2,a3,a4,...,an)=n1a1+1a2+1a3+...+1an(4.17)and hence F-measure isF=1a1P+(1\u0000a)1Ror\u2713withb2=1\u0000aa\u25c6F=(b2+1)PRb2P+R(4.18)12CHAPTER4\u2022NAIVEBAYES,TEXTCLASSIFICATION,ANDSENTIMENTwhile the other 999,900 are tweets about something completely unrelated. Imagine asimple classi\ufb01er that stupidly classi\ufb01ed every tweet as \u201cnot about pie\u201d. This classi\ufb01erwould have 999,900 true negatives and only 100 false negatives for an accuracy of999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we shouldbe happy with this classi\ufb01er? But of course this fabulous \u2018no pie\u2019 classi\ufb01er wouldbe completely useless, since it wouldn\u2019t \ufb01nd a single one of the customer commentswe are looking for. In other words, accuracy is not a good metric when the goal isto discover something that is rare, or at least not completely balanced in frequency,which is a very common situation in the world.That\u2019s why instead of accuracy we generally turn to two other metrics shown inFig.4.4:precisionandrecall.Precisionmeasures the percentage of the items thatprecisionthe system detected (i.e., the system labeled as positive) that are in fact positive (i.e.,are positive according to the human gold labels). Precision is de\ufb01ned asPrecision=true positivestrue positives + false positivesRecallmeasures the percentage of items actually present in the input that wererecallcorrectly identi\ufb01ed by the system. Recall is de\ufb01ned asRecall=true positivestrue positives + false negativesPrecision and recall will help solve the problem with the useless \u201cnothing ispie\u201d classi\ufb01er. This classi\ufb01er, despite having a fabulous accuracy of 99.99%, hasa terrible recall of 0 (since there are no true positives, and 100 false negatives, therecall is 0/100). You should convince yourself that the precision at \ufb01nding relevanttweets is equally problematic. Thus precision and recall, unlike accuracy, emphasizetrue positives: \ufb01nding the things that we are supposed to be looking for.There are many ways to de\ufb01ne a single metric that incorporates aspects of bothprecision and recall. The simplest of these combinations is theF-measure(vanF-measureRijsbergen,1975) , de\ufb01ned as:Fb=(b2+1)PRb2P+RThebparameter differentially weights the importance of recall and precision,based perhaps on the needs of an application. Values ofb>1 favor recall, whilevalues ofb<1 favor precision.",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 26,
      "token_count": 796,
      "chapter_title": ""
    }
  },
  {
    "content": "that are in fact positive (i.e.,are positive according to the human gold labels). Precision is de\ufb01ned asPrecision=true positivestrue positives + false positivesRecallmeasures the percentage of items actually present in the input that wererecallcorrectly identi\ufb01ed by the system. Recall is de\ufb01ned asRecall=true positivestrue positives + false negativesPrecision and recall will help solve the problem with the useless \u201cnothing ispie\u201d classi\ufb01er. This classi\ufb01er, despite having a fabulous accuracy of 99.99%, hasa terrible recall of 0 (since there are no true positives, and 100 false negatives, therecall is 0/100). You should convince yourself that the precision at \ufb01nding relevanttweets is equally problematic. Thus precision and recall, unlike accuracy, emphasizetrue positives: \ufb01nding the things that we are supposed to be looking for.There are many ways to de\ufb01ne a single metric that incorporates aspects of bothprecision and recall. The simplest of these combinations is theF-measure(vanF-measureRijsbergen,1975) , de\ufb01ned as:Fb=(b2+1)PRb2P+RThebparameter differentially weights the importance of recall and precision,based perhaps on the needs of an application. Values ofb>1 favor recall, whilevalues ofb<1 favor precision. Whenb=1, precision and recall are equally bal-anced; this is the most frequently used metric, and is called Fb=1or just F1:F1F1=2PRP+R(4.16)F-measure comes from a weighted harmonic mean of precision and recall. Theharmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-rocals:HarmonicMean(a1,a2,a3,a4,...,an)=n1a1+1a2+1a3+...+1an(4.17)and hence F-measure isF=1a1P+(1\u0000a)1Ror\u2713withb2=1\u0000aa\u25c6F=(b2+1)PRb2P+R(4.18)",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 27,
      "token_count": 470,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 73\n\nSuppose we have more than 2 classes?Lots of text classification tasks have more than two classes.\u25e6Sentiment analysis (positive, negative, neutral) , named entities (person, location, organization)We can define precision and recall for multiple classes like this 3-way email task:851060urgentnormalgold labelssystemoutputrecallu = 88+5+3precisionu= 88+10+115030200spamurgentnormalspam3recalln = recalls = precisionn= 605+60+50precisions= 2003+30+2006010+60+302001+50+200\n\n## Page 74\n\nHow to combine P/R values for different classes:Microaveraging vs Macroaveraging8811340trueurgenttruenotsystemurgentsystemnot604055212truenormaltruenotsystemnormalsystemnot200513383truespamtruenotsystemspamsystemnot2689999635trueyestruenosystemyessystemnoprecision =8+118= .42precision =200+33200= .86precision =60+5560= .52microaverageprecision268+99268= .73=macroaverageprecision3.42+.52+.86= .60=PooledClass 3: SpamClass 2: NormalClass 1: Urgent\n\n## Page 75\n\nText Classification and Naive BayesPrecision, Recall, and F1\n\n## Page 76\n\nText Classification and Naive BayesAvoiding Harms in Classification\n\n## Page 77\n\nHarms of classificationClassifiers, like any NLP algorithm, can cause harmsThis is true for any classifier, whether Naive Bayes or other algorithms\n\n## Page 78\n\nRepresentational Harms\u2022Harms caused by a system that demeans a social group\u2022Such as by perpetuating negative stereotypes about them. \u2022Kiritchenko and Mohammad 2018 study\u2022Examined 200 sentiment analysis systems on pairs of sentences\u2022Identical except for names:\u2022common African American (Shaniqua) or European American (Stephanie).\u2022Like \"I talked to Shaniqua yesterday\" vs \"I talked to Stephanie yesterday\"\u2022Result: systems assigned lower sentiment and more negative emotion to sentences with African American names\u2022Downstream harm: \u2022Perpetuates stereotypes about African Americans \u2022African Americans treated differently by NLP tools like sentiment (widely used in marketing research, mental health studies, etc.)\n\n## Page 79\n\nHarms of Censorship\u2022Toxicity detection is the text classification task of detecting hate speech, abuse, harassment, or other kinds of toxic language.\u2022Widely used in online content moderation\u2022Toxicity classifiers incorrectly flag non-toxic sentences that simply mention minority identities (like the words \"blind\" or \"gay\")\u2022women (Park et al., 2018), \u2022disabled people (Hutchinson et al., 2020) \u2022gay people (Dixon et al., 2018; Oliva et al., 2021)\u2022Downstream harms:\u2022Censorship of speech by disabled people and other groups\u2022Speech by these groups becomes less visible online\u2022Writers might be nudged by these algorithms to avoid these words making people less likely to write about themselves or these groups.\n\n## Page 80\n\nPerformance Disparities1.Text classifiers perform worse on many languages of the world due to lack of data or labels2.Text classifiers perform worse on varieties of even high-resource languages like English\u2022Example task: language identification, a first step in NLP pipeline (\"Is this post in English or not?\") \u2022English language detection performance worse for writers who are African American (Blodgett and O'Connor 2017) or from India (Jurgens et al., 2017)",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 28,
      "token_count": 789,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 79\n\nHarms of Censorship\u2022Toxicity detection is the text classification task of detecting hate speech, abuse, harassment, or other kinds of toxic language.\u2022Widely used in online content moderation\u2022Toxicity classifiers incorrectly flag non-toxic sentences that simply mention minority identities (like the words \"blind\" or \"gay\")\u2022women (Park et al., 2018), \u2022disabled people (Hutchinson et al., 2020) \u2022gay people (Dixon et al., 2018; Oliva et al., 2021)\u2022Downstream harms:\u2022Censorship of speech by disabled people and other groups\u2022Speech by these groups becomes less visible online\u2022Writers might be nudged by these algorithms to avoid these words making people less likely to write about themselves or these groups.\n\n## Page 80\n\nPerformance Disparities1.Text classifiers perform worse on many languages of the world due to lack of data or labels2.Text classifiers perform worse on varieties of even high-resource languages like English\u2022Example task: language identification, a first step in NLP pipeline (\"Is this post in English or not?\") \u2022English language detection performance worse for writers who are African American (Blodgett and O'Connor 2017) or from India (Jurgens et al., 2017)\n\n## Page 81\n\nHarms in text classification\u2022Causes:\u2022Issues in the data; NLP systems amplify biases in training data\u2022Problems in the labels\u2022Problems in the algorithms (like what the model is trained to optimize) \u2022Prevalence: The same problems occur throughout NLP (including large language models)  \u2022Solutions: There are no general mitigations or solutions\u2022But harm mitigation is an active area of research\u2022And there are standard benchmarks and tools that we can use for measuring some of the harms\n\n## Page 82\n\nText Classification and Naive BayesAvoiding Harms in Classification",
    "metadata": {
      "source": "nb24aug",
      "chunk_id": 29,
      "token_count": 401,
      "chapter_title": ""
    }
  }
]