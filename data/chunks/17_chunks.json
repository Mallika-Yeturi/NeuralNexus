[
  {
    "content": "# 17\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n17Sequence Labeling for Parts of\nSpeech and Named Entities\nTo each word a warbling note\nA Midsummer Night\u2019s Dream , V .I\nDionysius Thrax of Alexandria ( c.100 B.C.), or perhaps someone else (it was a long\ntime ago), wrote a grammatical sketch of Greek (a \u201c techn \u00afe\u201d) that summarized the\nlinguistic knowledge of his day. This work is the source of an astonishing proportion\nof modern linguistic vocabulary, including the words syntax ,diphthong ,clitic , and\nanalogy . Also included are a description of eight parts of speech : noun, verb, parts of speech\npronoun, preposition, adverb, conjunction, participle, and article. Although earlier\nscholars (including Aristotle as well as the Stoics) had their own lists of parts of\nspeech, it was Thrax\u2019s set of eight that became the basis for descriptions of European\nlanguages for the next 2000 years. (All the way to the Schoolhouse Rock educational\ntelevision shows of our childhood, which had songs about 8 parts of speech, like the\nlate great Bob Dorough\u2019s Conjunction Junction .) The durability of parts of speech\nthrough two millennia speaks to their centrality in models of human language.\nProper names are another important and anciently studied linguistic category.\nWhile parts of speech are generally assigned to individual words or morphemes, a\nproper name is often an entire multiword phrase, like the name \u201cMarie Curie\u201d, the\nlocation \u201cNew York City\u201d, or the organization \u201cStanford University\u201d. We\u2019ll use the\nterm named entity for, roughly speaking, anything that can be referred to with a named entity\nproper name: a person, a location, an organization, although as we\u2019ll see the term is\ncommonly extended to include things that aren\u2019t entities per se.\nParts of speech (also known as POS ) and named entities are useful clues to POS\nsentence structure and meaning. Knowing whether a word is a noun or a verb tells us\nabout likely neighboring words (nouns in English are preceded by determiners and\nadjectives, verbs by nouns) and syntactic structure (verbs have dependency links to\nnouns), making part-of-speech tagging a key aspect of parsing. Knowing if a named\nentity like Washington is a name of a person, a place, or a university is important to\nmany natural language processing tasks like question answering, stance detection,\nor information extraction.\nIn this chapter we\u2019ll introduce the task of part-of-speech tagging , taking a se-\nquence of words and assigning each word a part of speech like NOUN orVERB , and\nthe task of named entity recognition (NER ), assigning words or phrases tags like\nPERSON ,LOCATION , or ORGANIZATION .\nSuch tasks in which we assign, to each word xiin an input word sequence, a\nlabel yi, so that the output sequence Yhas the same length as the input sequence X\nare called sequence labeling tasks. We\u2019ll introduce classic sequence labeling algo-sequence\nlabeling\nrithms, one generative\u2014 the Hidden Markov Model (HMM)\u2014and one discriminative\u2014\nthe Conditional Random Field (CRF). In following chapters we\u2019ll introduce modern\nsequence labelers based on RNNs and Transformers.",
    "metadata": {
      "source": "17",
      "chunk_id": 0,
      "token_count": 735,
      "chapter_title": "17"
    }
  },
  {
    "content": "## Page 2",
    "metadata": {
      "source": "17",
      "chunk_id": 1,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "2CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\n17.1 (Mostly) English Word Classes\nUntil now we have been using part-of-speech terms like noun andverb rather freely.\nIn this section we give more complete de\ufb01nitions. While word classes do have\nsemantic tendencies\u2014adjectives, for example, often describe properties and nouns\npeople \u2014 parts of speech are de\ufb01ned instead based on their grammatical relationship\nwith neighboring words or the morphological properties about their af\ufb01xes.\nTag Description ExampleOpen ClassADJ Adjective: noun modi\ufb01ers describing properties red,young ,awesome\nADV Adverb: verb modi\ufb01ers of time, place, manner very,slowly ,home ,yesterday\nNOUN words for persons, places, things, etc. algorithm ,cat,mango ,beauty\nVERB words for actions and processes draw ,provide ,go\nPROPN Proper noun: name of a person, organization, place, etc.. Regina ,IBM,Colorado\nINTJ Interjection: exclamation, greeting, yes/no response, etc. oh,um,yes,helloClosed Class WordsADP Adposition (Preposition/Postposition): marks a noun\u2019s\nspacial, temporal, or other relationin, on, by, under\nAUX Auxiliary: helping verb marking tense, aspect, mood, etc., can, may, should, are\nCCONJ Coordinating Conjunction: joins two phrases/clauses and,or,but\nDET Determiner: marks noun phrase properties a, an, the, this\nNUM Numeral one, two, 2026, 11:00, hundred\nPART Particle: a function word that must be associated with an-\nother word\u2019s, not, (in\ufb01nitive) to\nPRON Pronoun: a shorthand for referring to an entity or event she, who, I, others\nSCONJ Subordinating Conjunction: joins a main clause with a\nsubordinate clause such as a sentential complementwhether ,becauseOtherPUNCT Punctuation\u02d9, , ()\nSYM Symbols like $ or emoji $, %\nX Other asdf, qwfg\nFigure 17.1 The 17 parts of speech in the Universal Dependencies tagset (de Marneffe et al., 2021). Features\ncan be added to make \ufb01ner-grained distinctions (with properties like number, case, de\ufb01niteness, and so on).\nParts of speech fall into two broad categories: closed class andopen class . closed class\nopen class Closed classes are those with relatively \ufb01xed membership, such as prepositions\u2014\nnew prepositions are rarely coined. By contrast, nouns and verbs are open classes\u2014\nnew nouns and verbs like iPhone orto fax are continually being created or borrowed.\nClosed class words are generally function words likeof,it,and, oryou, which tend function word\nto be very short, occur frequently, and often have structuring uses in grammar.\nFour major open classes occur in the languages of the world: nouns (including\nproper nouns), verbs ,adjectives , and adverbs , as well as the smaller open class of\ninterjections . English has all \ufb01ve, although not every language does.\nNouns are words for people, places, or things, but include others as well. Com- noun\nmon nouns include concrete terms like catandmango , abstractions like algorithm common noun\nandbeauty , and verb-like terms like pacing as in His pacing to and fro became quite\nannoying . Nouns in English can occur with determiners ( a goat, this bandwidth )\ntake possessives ( IBM\u2019s annual revenue ), and may occur in the plural ( goats, abaci ).",
    "metadata": {
      "source": "17",
      "chunk_id": 2,
      "token_count": 787,
      "chapter_title": ""
    }
  },
  {
    "content": "can be added to make \ufb01ner-grained distinctions (with properties like number, case, de\ufb01niteness, and so on).\nParts of speech fall into two broad categories: closed class andopen class . closed class\nopen class Closed classes are those with relatively \ufb01xed membership, such as prepositions\u2014\nnew prepositions are rarely coined. By contrast, nouns and verbs are open classes\u2014\nnew nouns and verbs like iPhone orto fax are continually being created or borrowed.\nClosed class words are generally function words likeof,it,and, oryou, which tend function word\nto be very short, occur frequently, and often have structuring uses in grammar.\nFour major open classes occur in the languages of the world: nouns (including\nproper nouns), verbs ,adjectives , and adverbs , as well as the smaller open class of\ninterjections . English has all \ufb01ve, although not every language does.\nNouns are words for people, places, or things, but include others as well. Com- noun\nmon nouns include concrete terms like catandmango , abstractions like algorithm common noun\nandbeauty , and verb-like terms like pacing as in His pacing to and fro became quite\nannoying . Nouns in English can occur with determiners ( a goat, this bandwidth )\ntake possessives ( IBM\u2019s annual revenue ), and may occur in the plural ( goats, abaci ).\nMany languages, including English, divide common nouns into count nouns and count noun\nmass nouns . Count nouns can occur in the singular and plural ( goat/goats, rela- mass noun\ntionship/relationships ) and can be counted ( one goat, two goats ). Mass nouns are\nused when something is conceptualized as a homogeneous group. So snow, salt , and\ncommunism are not counted (i.e., *two snows or*two communisms ).Proper nouns , proper noun\nlikeRegina ,Colorado , and IBM, are names of speci\ufb01c persons or entities.",
    "metadata": {
      "source": "17",
      "chunk_id": 3,
      "token_count": 418,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3",
    "metadata": {
      "source": "17",
      "chunk_id": 4,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "17.1 \u2022 (M OSTLY ) ENGLISH WORD CLASSES 3\nVerbs refer to actions and processes, including main verbs like draw ,provide , verb\nandgo. English verbs have in\ufb02ections (non-third-person-singular ( eat), third-person-\nsingular ( eats), progressive ( eating ), past participle ( eaten )). While many scholars\nbelieve that all human languages have the categories of noun and verb, others have\nargued that some languages, such as Riau Indonesian and Tongan, don\u2019t even make\nthis distinction (Broschart 1997; Evans 2000; Gil 2000) .\nAdjectives often describe properties or qualities of nouns, like color ( white , adjective\nblack ), age ( old,young ), and value ( good ,bad), but there are languages without\nadjectives. In Korean, for example, the words corresponding to English adjectives\nact as a subclass of verbs, so what is in English an adjective \u201cbeautiful\u201d acts in\nKorean like a verb meaning \u201cto be beautiful\u201d.\nAdverbs are a hodge-podge. All the italicized words in this example are adverbs: adverb\nActually , I ran home extremely quickly yesterday\nAdverbs generally modify something (often verbs, hence the name \u201cadverb\u201d, but\nalso other adverbs and entire verb phrases). Directional adverbs orlocative ad- locative\nverbs (home ,here,downhill ) specify the direction or location of some action; degree degree\nadverbs (extremely ,very,somewhat ) specify the extent of some action, process, or\nproperty; manner adverbs (slowly ,slinkily ,delicately ) describe the manner of some manner\naction or process; and temporal adverbs describe the time that some action or event temporal\ntook place ( yesterday ,Monday ).\nInterjections (oh, hey, alas, uh, um ) are a smaller open class that also includes interjection\ngreetings ( hello, goodbye ) and question responses ( yes, no, uh-huh ).\nEnglish adpositions occur before nouns, hence are called prepositions . They can preposition\nindicate spatial or temporal relations, whether literal ( on it,before then ,by the house )\nor metaphorical ( on time ,with gusto ,beside herself ), and relations like marking the\nagent in Hamlet was written by Shakespeare .\nAparticle resembles a preposition or an adverb and is used in combination with particle\na verb. Particles often have extended meanings that aren\u2019t quite the same as the\nprepositions they resemble, as in the particle over inshe turned the paper over . A\nverb and a particle acting as a single unit is called a phrasal verb . The meaning phrasal verb\nof phrasal verbs is often non-compositional \u2014not predictable from the individual\nmeanings of the verb and the particle. Thus, turn down means \u2018reject\u2019, rule out\n\u2018eliminate\u2019, and go on \u2018continue\u2019.\nDeterminers likethisandthat(this chapter ,that page ) can mark the start of an determiner\nEnglish noun phrase. Articles likea,an, and the, are a type of determiner that mark article\ndiscourse properties of the noun and are quite frequent; theis the most common\nword in written English, with aandanright behind.\nConjunctions join two phrases, clauses, or sentences. Coordinating conjunc- conjunction\ntions like and,or, and butjoin two elements of equal status. Subordinating conjunc-\ntions are used when one of the elements has some embedded status. For example,\nthe subordinating conjunction thatin\u201cI thought that you might like some milk\u201d links",
    "metadata": {
      "source": "17",
      "chunk_id": 5,
      "token_count": 775,
      "chapter_title": ""
    }
  },
  {
    "content": "agent in Hamlet was written by Shakespeare .\nAparticle resembles a preposition or an adverb and is used in combination with particle\na verb. Particles often have extended meanings that aren\u2019t quite the same as the\nprepositions they resemble, as in the particle over inshe turned the paper over . A\nverb and a particle acting as a single unit is called a phrasal verb . The meaning phrasal verb\nof phrasal verbs is often non-compositional \u2014not predictable from the individual\nmeanings of the verb and the particle. Thus, turn down means \u2018reject\u2019, rule out\n\u2018eliminate\u2019, and go on \u2018continue\u2019.\nDeterminers likethisandthat(this chapter ,that page ) can mark the start of an determiner\nEnglish noun phrase. Articles likea,an, and the, are a type of determiner that mark article\ndiscourse properties of the noun and are quite frequent; theis the most common\nword in written English, with aandanright behind.\nConjunctions join two phrases, clauses, or sentences. Coordinating conjunc- conjunction\ntions like and,or, and butjoin two elements of equal status. Subordinating conjunc-\ntions are used when one of the elements has some embedded status. For example,\nthe subordinating conjunction thatin\u201cI thought that you might like some milk\u201d links\nthe main clause I thought with the subordinate clause you might like some milk . This\nclause is called subordinate because this entire clause is the \u201ccontent\u201d of the main\nverb thought . Subordinating conjunctions like thatwhich link a verb to its argument\nin this way are also called complementizers . complementizer\nPronouns act as a shorthand for referring to an entity or event. Personal pro- pronoun\nnouns refer to persons or entities ( you,she,I,it,me, etc.). Possessive pronouns are\nforms of personal pronouns that indicate either actual possession or more often just\nan abstract relation between the person and some object ( my, your, his, her, its, one\u2019s,\nour, their ).Wh-pronouns (what, who, whom, whoever ) are used in certain question wh",
    "metadata": {
      "source": "17",
      "chunk_id": 6,
      "token_count": 458,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4",
    "metadata": {
      "source": "17",
      "chunk_id": 7,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "4CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\nforms, or act as complementizers ( Frida, who married Diego. . . ).\nAuxiliary verbs mark semantic features of a main verb such as its tense, whether auxiliary\nit is completed (aspect), whether it is negated (polarity), and whether an action is\nnecessary, possible, suggested, or desired (mood). English auxiliaries include the\ncopula verb be, the two verbs doandhave , forms, as well as modal verbs used to copula\nmodal mark the mood associated with the event depicted by the main verb: canindicates\nability or possibility, may permission or possibility, must necessity.\nAn English-speci\ufb01c tagset, the Penn Treebank tagset (Marcus et al., 1993), shown\nin Fig. 17.2, has been used to label many syntactically annotated corpora like the\nPenn Treebank corpora, so it is worth knowing about.\nTag Description Example Tag Description Example Tag Description Example\nCC coord. conj. and, but, or NNP proper noun, sing. IBM TO in\ufb01nitive to to\nCD cardinal number one, two NNPS proper noun, plu. Carolinas UH interjection ah, oops\nDT determiner a, the NNS noun, plural llamas VB verb base eat\nEX existential \u2018there\u2019 there PDT predeterminer all, both VBD verb past tense ate\nFW foreign word mea culpa POS possessive ending \u2019s VBG verb gerund eating\nIN preposition/\nsubordin-conjof, in, by PRP personal pronoun I, you, he VBN verb past partici-\npleeaten\nJJ adjective yellow PRP$ possess. pronoun your VBP verb non-3sg-pr eat\nJJR comparative adj bigger RB adverb quickly VBZ verb 3sg pres eats\nJJS superlative adj wildest RBR comparative adv faster WDT wh-determ. which, that\nLS list item marker 1, 2, One RBS superlatv. adv fastest WP wh-pronoun what, who\nMD modal can, should RP particle up, off WP$ wh-possess. whose\nNN sing or mass noun llama SYM symbol +,%,& WRB wh-adverb how, where\nFigure 17.2 Penn Treebank core 36 part-of-speech tags.\nBelow we show some examples with each word tagged according to both the UD\n(in blue) and Penn (in red) tagsets. Notice that the Penn tagset distinguishes tense\nand participles on verbs, and has a special tag for the existential there construction in\nEnglish. Note that since London Journal of Medicine is a proper noun, both tagsets\nmark its component nouns as PROPN/NNP, including journal andmedicine , which\nmight otherwise be labeled as common nouns (NOUN/NN).\n(17.1) There/ PRON /EXare/ VERB /VBP 70/NUM /CDchildren/ NOUN /NNS\nthere/ ADV/RB./PUNC /.\n(17.2) Preliminary/ ADJ/JJ\ufb01ndings/ NOUN /NNS were/ AUX/VBD\nreported/ VERB /VBN in/ADP/INtoday/ NOUN /NN\u2019s/PART /POS\nLondon/ PROPN /NNP Journal/ PROPN /NNP of/ADP/INMedicine/ PROPN /NNP\n17.2 Part-of-Speech Tagging\nPart-of-speech tagging is the process of assigning a part-of-speech to each word inpart-of-speech\ntagging",
    "metadata": {
      "source": "17",
      "chunk_id": 8,
      "token_count": 780,
      "chapter_title": ""
    }
  },
  {
    "content": "Figure 17.2 Penn Treebank core 36 part-of-speech tags.\nBelow we show some examples with each word tagged according to both the UD\n(in blue) and Penn (in red) tagsets. Notice that the Penn tagset distinguishes tense\nand participles on verbs, and has a special tag for the existential there construction in\nEnglish. Note that since London Journal of Medicine is a proper noun, both tagsets\nmark its component nouns as PROPN/NNP, including journal andmedicine , which\nmight otherwise be labeled as common nouns (NOUN/NN).\n(17.1) There/ PRON /EXare/ VERB /VBP 70/NUM /CDchildren/ NOUN /NNS\nthere/ ADV/RB./PUNC /.\n(17.2) Preliminary/ ADJ/JJ\ufb01ndings/ NOUN /NNS were/ AUX/VBD\nreported/ VERB /VBN in/ADP/INtoday/ NOUN /NN\u2019s/PART /POS\nLondon/ PROPN /NNP Journal/ PROPN /NNP of/ADP/INMedicine/ PROPN /NNP\n17.2 Part-of-Speech Tagging\nPart-of-speech tagging is the process of assigning a part-of-speech to each word inpart-of-speech\ntagging\na text. The input is a sequence x1;x2;:::;xnof (tokenized) words and a tagset, and\nthe output is a sequence y1;y2;:::;ynof tags, each output yicorresponding exactly to\none input xi, as shown in the intuition in Fig. 17.3.\nTagging is a disambiguation task; words are ambiguous \u2014have more than one ambiguous\npossible part-of-speech\u2014and the goal is to \ufb01nd the correct tag for the situation.\nFor example, book can be a verb ( book that \ufb02ight ) or a noun ( hand me that book ).\nThat can be a determiner ( Does that \ufb02ight serve dinner ) or a complementizer ( I",
    "metadata": {
      "source": "17",
      "chunk_id": 9,
      "token_count": 443,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5\n\n17.2 \u2022 P ART-OF-SPEECH TAGGING 5\nwillNOUNAUXVERBDETNOUNJanetbackthebillPart of Speech Taggerx1x2x3x4x5y1y2y3y4y5\nFigure 17.3 The task of part-of-speech tagging: mapping from input words x1;x2;:::;xnto\noutput POS tags y1;y2;:::;yn.\nthought that your \ufb02ight was earlier ). The goal of POS-tagging is to resolve theseambiguity\nresolution\nambiguities, choosing the proper tag for the context.\nThe accuracy of part-of-speech tagging algorithms (the percentage of test set accuracy\ntags that match human gold labels) is extremely high. One study found accuracies\nover 97% across 15 languages from the Universal Dependency (UD) treebank (Wu\nand Dredze, 2019). Accuracies on various English treebanks are also 97% (no matter\nthe algorithm; HMMs, CRFs, BERT perform similarly). This 97% number is also\nabout the human performance on this task, at least for English (Manning, 2011).\nTypes: WSJ Brown\nUnambiguous (1 tag) 44,432 ( 86% ) 45,799 ( 85% )\nAmbiguous (2+ tags) 7,025 ( 14% ) 8,050 ( 15% )\nTokens :\nUnambiguous (1 tag) 577,421 ( 45% ) 384,349 ( 33% )\nAmbiguous (2+ tags) 711,780 ( 55% ) 786,646 ( 67% )\nFigure 17.4 Tag ambiguity in the Brown and WSJ corpora (Treebank-3 45-tag tagset).\nWe\u2019ll introduce algorithms for the task in the next few sections, but \ufb01rst let\u2019s\nexplore the task. Exactly how hard is it? Fig. 17.4 shows that most word types\n(85-86%) are unambiguous ( Janet is always NNP, hesitantly is always RB). But the\nambiguous words, though accounting for only 14-15% of the vocabulary, are very\ncommon, and 55-67% of word tokens in running text are ambiguous. Particularly\nambiguous common words include that,back,down ,putandset; here are some\nexamples of the 6 different parts of speech for the word back:\nearnings growth took a back/JJ seat\na small building in the back/NN\na clear majority of senators back/VBP the bill\nDave began to back/VB toward the door\nenable the country to buy back/RP debt\nI was twenty-one back/RB then\nNonetheless, many words are easy to disambiguate, because their different tags\naren\u2019t equally likely. For example, acan be a determiner or the letter a, but the\ndeterminer sense is much more likely.\nThis idea suggests a useful baseline : given an ambiguous word, choose the tag\nwhich is most frequent in the training corpus. This is a key concept:\nMost Frequent Class Baseline: Always compare a classi\ufb01er against a baseline at\nleast as good as the most frequent class baseline (assigning each token to the class\nit occurred in most often in the training set).",
    "metadata": {
      "source": "17",
      "chunk_id": 10,
      "token_count": 724,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6\n\n6CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\nThe most-frequent-tag baseline has an accuracy of about 92%1. The baseline\nthus differs from the state-of-the-art and human ceiling (97%) by only 5%.\n17.3 Named Entities and Named Entity Tagging\nPart of speech tagging can tell us that words like Janet ,Stanford University , and\nColorado are all proper nouns; being a proper noun is a grammatical property of\nthese words. But viewed from a semantic perspective, these proper nouns refer to\ndifferent kinds of entities: Janet is a person, Stanford University is an organization,\nand Colorado is a location.\nHere we re-introduce the concept of a named entity , which was also introduced named entity\nin Section ??for readers who haven\u2019t yet read Chapter 11.\nAnamed entity is, roughly speaking, anything that can be referred to with a named entity\nproper name: a person, a location, an organization. The task of named entity recog-\nnition (NER ) is to \ufb01nd spans of text that constitute proper names and tag the type ofnamed entity\nrecognition\nNER the entity. Four entity tags are most common: PER (person), LOC (location), ORG\n(organization), or GPE (geo-political entity). However, the term named entity is\ncommonly extended to include things that aren\u2019t entities per se, including dates,\ntimes, and other kinds of temporal expressions, and even numerical expressions like\nprices. Here\u2019s an example of the output of an NER tagger:\nCiting high fuel prices, [ ORG United Airlines ] said [ TIME Friday ] it\nhas increased fares by [ MONEY $6] per round trip on \ufb02ights to some\ncities also served by lower-cost carriers. [ ORG American Airlines ], a\nunit of [ ORG AMR Corp.] , immediately matched the move, spokesman\n[PER Tim Wagner ] said. [ ORG United] , a unit of [ ORG UAL Corp.] ,\nsaid the increase took effect [ TIME Thursday] and applies to most\nroutes where it competes against discount carriers, such as [ LOC Chicago]\nto [LOC Dallas] and [ LOC Denver] to [LOC San Francisco] .\nThe text contains 13 mentions of named entities including 5 organizations, 4 loca-\ntions, 2 times, 1 person, and 1 mention of money. Figure 17.5 shows typical generic\nnamed entity types. Many applications will also need to use speci\ufb01c entity types like\nproteins, genes, commercial products, or works of art.\nType Tag Sample Categories Example sentences\nPeople PER people, characters Turing is a giant of computer science.\nOrganization ORG companies, sports teams The IPCC warned about the cyclone.\nLocation LOC regions, mountains, seas Mt. Sanitas is in Sunshine Canyon .\nGeo-Political Entity GPE countries, states Palo Alto is raising the fees for parking.\nFigure 17.5 A list of generic named entity types with the kinds of entities they refer to.\nNamed entity tagging is a useful \ufb01rst step in lots of natural language processing\ntasks. In sentiment analysis we might want to know a consumer\u2019s sentiment toward a\nparticular entity. Entities are a useful \ufb01rst stage in question answering, or for linking\ntext to information in structured knowledge sources like Wikipedia. And named\nentity tagging is also central to tasks involving building semantic representations,\nlike extracting events and the relationship between participants.\n1In English, on the WSJ corpus, tested on sections 22-24.",
    "metadata": {
      "source": "17",
      "chunk_id": 11,
      "token_count": 758,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7\n\n17.3 \u2022 N AMED ENTITIES AND NAMED ENTITY TAGGING 7\nUnlike part-of-speech tagging, where there is no segmentation problem since\neach word gets one tag, the task of named entity recognition is to \ufb01nd and label\nspans of text, and is dif\ufb01cult partly because of the ambiguity of segmentation; we\nneed to decide what\u2019s an entity and what isn\u2019t, and where the boundaries are. Indeed,\nmost words in a text will not be named entities. Another dif\ufb01culty is caused by type\nambiguity. The mention JFKcan refer to a person, the airport in New York, or any\nnumber of schools, bridges, and streets around the United States. Some examples of\nthis kind of cross-type confusion are given in Figure 17.6.\n[PER Washington] was born into slavery on the farm of James Burroughs.\n[ORG Washington] went up 2 games to 1 in the four-game series.\nBlair arrived in [ LOC Washington] for what may well be his last state visit.\nIn June, [ GPE Washington] passed a primary seatbelt law.\nFigure 17.6 Examples of type ambiguities in the use of the name Washington .\nThe standard approach to sequence labeling for a span-recognition problem like\nNER is BIO tagging (Ramshaw and Marcus, 1995). This is a method that allows us\nto treat NER like a word-by-word sequence labeling task, via tags that capture both\nthe boundary and the named entity type. Consider the following sentence:\n[PER Jane Villanueva ] of [ ORG United ] , a unit of [ ORG United Airlines\nHolding ] , said the fare applies to the [ LOC Chicago ] route.\nFigure 17.7 shows the same excerpt represented with BIO tagging, as well as BIO\nvariants called IOtagging and BIOES tagging. In BIO tagging we label any token\nthatbegins a span of interest with the label B, tokens that occur inside a span are\ntagged with an I, and any tokens outside of any span of interest are labeled O. While\nthere is only one Otag, we\u2019ll have distinct Band Itags for each named entity class.\nThe number of tags is thus 2 n+1 tags, where nis the number of entity types. BIO\ntagging can represent exactly the same information as the bracketed notation, but has\nthe advantage that we can represent the task in the same simple sequence modeling\nway as part-of-speech tagging: assigning a single label yito each input word xi:\nWords IO Label BIO Label BIOES Label\nJane I-PER B-PER B-PER\nVillanueva I-PER I-PER E-PER\nof O O O\nUnited I-ORG B-ORG B-ORG\nAirlines I-ORG I-ORG I-ORG\nHolding I-ORG I-ORG E-ORG\ndiscussed O O O\nthe O O O\nChicago I-LOC B-LOC S-LOC\nroute O O O\n. O O O\nFigure 17.7 NER as a sequence model, showing IO, BIO, and BIOES taggings.\nWe\u2019ve also shown two variant tagging schemes: IO tagging, which loses some\ninformation by eliminating the B tag, and BIOES tagging, which adds an end tag\nEfor the end of a span, and a span tag Sfor a span consisting of only one word.\nA sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label each\ntoken in a text with tags that indicate the presence (or absence) of particular kinds\nof named entities.",
    "metadata": {
      "source": "17",
      "chunk_id": 12,
      "token_count": 772,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8\n\n8CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\n17.4 HMM Part-of-Speech Tagging\nIn this section we introduce our \ufb01rst sequence labeling algorithm, the Hidden Markov\nModel, and show how to apply it to part-of-speech tagging. Recall that a sequence\nlabeler is a model whose job is to assign a label to each unit in a sequence, thus\nmapping a sequence of observations to a sequence of labels of the same length.\nThe HMM is a classic model that introduces many of the key concepts of sequence\nmodeling that we will see again in more modern models.\nAn HMM is a probabilistic sequence model: given a sequence of units (words,\nletters, morphemes, sentences, whatever), it computes a probability distribution over\npossible sequences of labels and chooses the best label sequence.\n17.4.1 Markov Chains\nThe HMM is based on augmenting the Markov chain. A Markov chain is a model Markov chain\nthat tells us something about the probabilities of sequences of random variables,\nstates , each of which can take on values from some set. These sets can be words, or\ntags, or symbols representing anything, for example the weather. A Markov chain\nmakes a very strong assumption that if we want to predict the future in the sequence,\nall that matters is the current state. All the states before the current state have no im-\npact on the future except via the current state. It\u2019s as if to predict tomorrow\u2019s weather\nyou could examine today\u2019s weather but you weren\u2019t allowed to look at yesterday\u2019s\nweather.\nWARM3HOT1COLD2.8.6.1.1.3.6.1.1.3\ncharminguniformlyare.1.4.5.5.5.2.6.2\n(a) (b)\nFigure 17.8 A Markov chain for weather (a) and one for words (b), showing states and\ntransitions. A start distribution pis required; setting p= [0:1;0:7;0:2]for (a) would mean a\nprobability 0.7 of starting in state 2 (cold), probability 0.1 of starting in state 1 (hot), etc.\nMore formally, consider a sequence of state variables q1;q2;:::;qi. A Markov\nmodel embodies the Markov assumption on the probabilities of this sequence: thatMarkov\nassumption\nwhen predicting the future, the past doesn\u2019t matter, only the present.\nMarkov Assumption: P(qi=ajq1:::qi\u00001) =P(qi=ajqi\u00001) (17.3)\nFigure 17.8a shows a Markov chain for assigning a probability to a sequence of\nweather events, for which the vocabulary consists of HOT,COLD , and WARM . The\nstates are represented as nodes in the graph, and the transitions, with their probabil-\nities, as edges. The transitions are probabilities: the values of arcs leaving a given\nstate must sum to 1. Figure 17.8b shows a Markov chain for assigning a probabil-\nity to a sequence of words w1:::wt. This Markov chain should be familiar; in fact,\nit represents a bigram language model, with each edge expressing the probability\np(wijwj)! Given the two models in Fig. 17.8, we can assign a probability to any\nsequence from our vocabulary.",
    "metadata": {
      "source": "17",
      "chunk_id": 13,
      "token_count": 746,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\n17.4 \u2022 HMM P ART-OF-SPEECH TAGGING 9\nFormally, a Markov chain is speci\ufb01ed by the following components:\nQ=q1q2:::qN a set of Nstates\nA=a11a12:::aN1:::aNN atransition probability matrix A, each ai jrepresent-\ning the probability of moving from state ito state j, s.t.Pn\nj=1ai j=18i\np=p1;p2;:::;pN aninitial probability distribution over states. piis the\nprobability that the Markov chain will start in state i.\nSome states jmay have pj=0, meaning that they cannot\nbe initial states. Also,Pn\ni=1pi=1\nBefore you go on, use the sample probabilities in Fig. 17.8a (with p= [0:1;0:7;0:2])\nto compute the probability of each of the following sequences:\n(17.4) hot hot hot hot\n(17.5) cold hot cold hot\nWhat does the difference in these probabilities tell you about a real-world weather\nfact encoded in Fig. 17.8a?\n17.4.2 The Hidden Markov Model\nA Markov chain is useful when we need to compute a probability for a sequence\nof observable events. In many cases, however, the events we are interested in are\nhidden : we don\u2019t observe them directly. For example we don\u2019t normally observe hidden\npart-of-speech tags in a text. Rather, we see words, and must infer the tags from the\nword sequence. We call the tags hidden because they are not observed.\nAhidden Markov model (HMM ) allows us to talk about both observed eventshidden Markov\nmodel\n(like words that we see in the input) and hidden events (like part-of-speech tags) that\nwe think of as causal factors in our probabilistic model. An HMM is speci\ufb01ed by\nthe following components:\nQ=q1q2:::qN a set of Nstates\nA=a11:::ai j:::aNNatransition probability matrix A, each ai jrepresenting the probability\nof moving from state ito state j, s.t.PN\nj=1ai j=18i\nB=bi(ot) a sequence of observation likelihoods , also called emission probabili-\nties, each expressing the probability of an observation ot(drawn from a\nvocabulary V=v1;v2;:::;vV) being generated from a state qi\np=p1;p2;:::;pN aninitial probability distribution over states. piis the probability that\nthe Markov chain will start in state i. Some states jmay have pj=0,\nmeaning that they cannot be initial states. Also,Pn\ni=1pi=1\nThe HMM is given as input O=o1o2:::oT: a sequence of Tobservations , each\none drawn from the vocabulary V.\nA \ufb01rst-order hidden Markov model instantiates two simplifying assumptions.\nFirst, as with a \ufb01rst-order Markov chain, the probability of a particular state depends\nonly on the previous state:\nMarkov Assumption: P(qijq1;:::;qi\u00001) =P(qijqi\u00001) (17.6)\nSecond, the probability of an output observation oidepends only on the state that\nproduced the observation qiand not on any other states or any other observations:\nOutput Independence: P(oijq1;:::qi;:::; qT;o1;:::; oi;:::; oT) =P(oijqi)(17.7)",
    "metadata": {
      "source": "17",
      "chunk_id": 14,
      "token_count": 799,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\n17.4.3 The components of an HMM tagger\nAn HMM has two components, the AandBprobabilities, both estimated by counting\non a tagged training corpus. (For this example we\u2019ll use the tagged WSJ corpus.)\nTheAmatrix contains the tag transition probabilities P(tijti\u00001)which represent\nthe probability of a tag occurring given the previous tag. For example, modal verbs\nlikewillare very likely to be followed by a verb in the base form, a VB, like race, so\nwe expect this probability to be high. We compute the maximum likelihood estimate\nof this transition probability by counting, out of the times we see the \ufb01rst tag in a\nlabeled corpus, how often the \ufb01rst tag is followed by the second:\nP(tijti\u00001) =C(ti\u00001;ti)\nC(ti\u00001)(17.8)\nIn the WSJ corpus, for example, MD occurs 13124 times of which it is followed\nby VB 10471, for an MLE estimate of\nP(V BjMD) =C(MD;V B)\nC(MD)=10471\n13124=:80 (17.9)\nTheBemission probabilities, P(wijti), represent the probability, given a tag (say\nMD), that it will be associated with a given word (say will). The MLE of the emis-\nsion probability is\nP(wijti) =C(ti;wi)\nC(ti)(17.10)\nOf the 13124 occurrences of MD in the WSJ corpus, it is associated with will4046\ntimes:\nP(willjMD) =C(MD;will)\nC(MD)=4046\n13124=:31 (17.11)\nWe saw this kind of Bayesian modeling in Chapter 4; recall that this likelihood\nterm is not asking \u201cwhich is the most likely tag for the word will?\u201d That would be\nthe posterior P(MDjwill). Instead, P(willjMD)answers the slightly counterintuitive\nquestion \u201cIf we were going to generate a MD, how likely is it that this modal would\nbewill?\u201d\nNN3VB1MD2a22\na11a12a21a13a33a32a23a31P(\"aardvark\" | NN)...P(\u201cwill\u201d | NN)...P(\"the\" | NN)...P(\u201cback\u201d | NN)...P(\"zebra\" | NN)B3P(\"aardvark\" | VB)...P(\u201cwill\u201d | VB)...P(\"the\" | VB)...P(\u201cback\u201d | VB)...P(\"zebra\" | VB)B1P(\"aardvark\" | MD)...P(\u201cwill\u201d | MD)...P(\"the\" | MD)...P(\u201cback\u201d | MD)...P(\"zebra\" | MD)B2\nFigure 17.9 An illustration of the two parts of an HMM representation: the Atransition\nprobabilities used to compute the prior probability, and the Bobservation likelihoods that are\nassociated with each state, one likelihood for each possible observation word.",
    "metadata": {
      "source": "17",
      "chunk_id": 15,
      "token_count": 690,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\n17.4 \u2022 HMM P ART-OF-SPEECH TAGGING 11\nThe Atransition probabilities, and Bobservation likelihoods of the HMM are\nillustrated in Fig. 17.9 for three states in an HMM part-of-speech tagger; the full\ntagger would have one state for each tag.\n17.4.4 HMM tagging as decoding\nFor any model, such as an HMM, that contains hidden variables, the task of deter-\nmining the hidden variables sequence corresponding to the sequence of observations\nis called decoding . More formally, decoding\nDecoding : Given as input an HMM l= (A;B)and a sequence of ob-\nservations O=o1;o2;:::;oT, \ufb01nd the most probable sequence of states\nQ=q1q2q3:::qT.\nFor part-of-speech tagging, the goal of HMM decoding is to choose the tag\nsequence t1:::tnthat is most probable given the observation sequence of nwords\nw1:::wn:\n\u02c6t1:n=argmax\nt1:::tnP(t1:::tnjw1:::wn) (17.12)\nThe way we\u2019ll do this in the HMM is to use Bayes\u2019 rule to instead compute:\n\u02c6t1:n=argmax\nt1:::tnP(w1:::wnjt1:::tn)P(t1:::tn)\nP(w1:::wn)(17.13)\nFurthermore, we simplify Eq. 17.13 by dropping the denominator P(wn\n1):\n\u02c6t1:n=argmax\nt1:::tnP(w1:::wnjt1:::tn)P(t1:::tn) (17.14)\nHMM taggers make two further simplifying assumptions. The \ufb01rst (output in-\ndependence, from Eq. 17.7) is that the probability of a word appearing depends only\non its own tag and is independent of neighboring words and tags:\nP(w1:::wnjt1:::tn)\u0019nY\ni=1P(wijti) (17.15)\nThe second assumption (the Markov assumption, Eq. 17.6) is that the probability of\na tag is dependent only on the previous tag, rather than the entire tag sequence;\nP(t1:::tn)\u0019nY\ni=1P(tijti\u00001) (17.16)\nPlugging the simplifying assumptions from Eq. 17.15 and Eq. 17.16 into Eq. 17.14\nresults in the following equation for the most probable tag sequence from a bigram\ntagger:\n\u02c6t1:n=argmax\nt1:::tnP(t1:::tnjw1:::wn)\u0019argmax\nt1:::tnnY\ni=1emissionz}|{\nP(wijti)transitionz}|{\nP(tijti\u00001) (17.17)\nThe two parts of Eq. 17.17 correspond neatly to the Bemission probability andA\ntransition probability that we just de\ufb01ned above!",
    "metadata": {
      "source": "17",
      "chunk_id": 16,
      "token_count": 678,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\nfunction VITERBI (observations of len T,state-graph of len N)returns best-path ,path-prob\ncreate a path probability matrix viterbi[N,T]\nforeach state sfrom 1toNdo ; initialization step\nviterbi [s,1] ps\u0003bs(o1)\nbackpointer [s,1] 0\nforeach time step tfrom 2toTdo ; recursion step\nforeach state sfrom 1toNdo\nviterbi [s,t] Nmax\ns0=1viterbi [s0;t\u00001]\u0003as0;s\u0003bs(ot)\nbackpointer [s,t] Nargmax\ns0=1viterbi [s0;t\u00001]\u0003as0;s\u0003bs(ot)\nbestpathprob Nmax\ns=1viterbi [s;T] ; termination step\nbestpathpointer Nargmax\ns=1viterbi [s;T] ; termination step\nbestpath the path starting at state bestpathpointer , that follows backpointer[] to states back in time\nreturn bestpath ,bestpathprob\nFigure 17.10 Viterbi algorithm for \ufb01nding the optimal sequence of tags. Given an observation sequence and\nan HMM l= (A;B), the algorithm returns the state path through the HMM that assigns maximum likelihood\nto the observation sequence.\n17.4.5 The Viterbi Algorithm\nThe decoding algorithm for HMMs is the Viterbi algorithm shown in Fig. 17.10.Viterbi\nalgorithm\nAs an instance of dynamic programming , Viterbi resembles the dynamic program-\nming minimum edit distance algorithm of Chapter 2.\nThe Viterbi algorithm \ufb01rst sets up a probability matrix or lattice , with one col-\numn for each observation otand one row for each state in the state graph. Each col-\numn thus has a cell for each state qiin the single combined automaton. Figure 17.11\nshows an intuition of this lattice for the sentence Janet will back the bill .\nEach cell of the lattice, vt(j), represents the probability that the HMM is in state\njafter seeing the \ufb01rst tobservations and passing through the most probable state\nsequence q1;:::;qt\u00001, given the HMM l. The value of each cell vt(j)is computed\nby recursively taking the most probable path that could lead us to this cell. Formally,\neach cell expresses the probability\nvt(j) = max\nq1;:::;qt\u00001P(q1:::qt\u00001;o1;o2:::ot;qt=jjl) (17.18)\nWe represent the most probable path by taking the maximum over all possible\nprevious state sequences max\nq1;:::;qt\u00001. Like other dynamic programming algorithms,\nViterbi \ufb01lls each cell recursively. Given that we had already computed the probabil-\nity of being in every state at time t\u00001, we compute the Viterbi probability by taking\nthe most probable of the extensions of the paths that lead to the current cell. For a\ngiven state qjat time t, the value vt(j)is computed as\nvt(j) =Nmax\ni=1vt\u00001(i)ai jbj(ot) (17.19)\nThe three factors that are multiplied in Eq. 17.19 for extending the previous paths to\ncompute the Viterbi probability at time tare",
    "metadata": {
      "source": "17",
      "chunk_id": 17,
      "token_count": 756,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\n17.4 \u2022 HMM P ART-OF-SPEECH TAGGING 13\nJJNNPNNPNNPMDMDMDMDVBVBJJJJJJNNNNRBRBRBRBDTDTDTDT\nNNPJanetwillbackthebillNNVBMDNNVBJJRB\nNNPDTNNVB\nFigure 17.11 A sketch of the lattice for Janet will back the bill , showing the possible tags\n(qi) for each word and highlighting the path corresponding to the correct tag sequence through\nthe hidden states. States (parts of speech) which have a zero probability of generating a\nparticular word according to the Bmatrix (such as the probability that a determiner DT will\nbe realized as Janet ) are greyed out.\nvt\u00001(i)theprevious Viterbi path probability from the previous time step\nai j thetransition probability from previous state qito current state qj\nbj(ot) thestate observation likelihood of the observation symbol otgiven\nthe current state j\n17.4.6 Working through an example\nLet\u2019s tag the sentence Janet will back the bill ; the goal is the correct series of tags\n(see also Fig. 17.11):\n(17.20) Janet/NNP will/MD back/VB the/DT bill/NN\nNNP MD VB JJ NN RB DT\n<s> 0.2767 0.0006 0.0031 0.0453 0.0449 0.0510 0.2026\nNNP 0.3777 0.0110 0.0009 0.0084 0.0584 0.0090 0.0025\nMD 0.0008 0.0002 0.7968 0.0005 0.0008 0.1698 0.0041\nVB 0.0322 0.0005 0.0050 0.0837 0.0615 0.0514 0.2231\nJJ 0.0366 0.0004 0.0001 0.0733 0.4509 0.0036 0.0036\nNN 0.0096 0.0176 0.0014 0.0086 0.1216 0.0177 0.0068\nRB 0.0068 0.0102 0.1011 0.1012 0.0120 0.0728 0.0479\nDT 0.1147 0.0021 0.0002 0.2157 0.4744 0.0102 0.0017\nFigure 17.12 TheAtransition probabilities P(tijti\u00001)computed from the WSJ corpus with-\nout smoothing. Rows are labeled with the conditioning event; thus P(V BjMD)is 0.7968.\n<s>is the start token.\nLet the HMM be de\ufb01ned by the two tables in Fig. 17.12 and Fig. 17.13. Fig-\nure 17.12 lists the ai jprobabilities for transitioning between the hidden states (part-\nof-speech tags). Figure 17.13 expresses the bi(ot)probabilities, the observation\nlikelihoods of words given tags. This table is (slightly simpli\ufb01ed) from counts in the\nWSJ corpus. So the word Janet only appears as an NNP, back has 4 possible parts",
    "metadata": {
      "source": "17",
      "chunk_id": 18,
      "token_count": 760,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n14 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\nJanet will back the bill\nNNP 0.000032 0 0 0.000048 0\nMD 0 0.308431 0 0 0\nVB 0 0.000028 0.000672 0 0.000028\nJJ 0 0 0.000340 0 0\nNN 0 0.000200 0.000223 0 0.002337\nRB 0 0 0.010446 0 0\nDT 0 0 0 0.506099 0\nFigure 17.13 Observation likelihoods Bcomputed from the WSJ corpus without smooth-\ning, simpli\ufb01ed slightly.\nof speech, and the word thecan appear as a determiner or as an NNP (in titles like\n\u201cSomewhere Over the Rainbow\u201d all words are tagged as NNP).\n\u03c0P(NNP|start) = .28* P(MD|MD)= 0*  P(MD|NNP).000009*.01  = .9e-8 v1(2)=.0006 x 0 = 0v1(1) = .28* .000032 = .000009\ntMDq2q1\no1Janetbillwillo2o3backVBJJv1(3)=.0031 x 0 = 0v1(4)= .045*0=0\no4  *  P(MD|VB) = 0 * P(MD|JJ)= 0P(VB|start) = .0031P(JJ |start) =.045\nbacktraceq3q4\ntheNNq5RBq6DTq7\nv2(2) =max * .308 =2.772e-8v2(5)=max * .0002 = .0000000001\nv2(3)=max * .000028 =     2.5e-13v3(6)=max * .0104v3(5)=max * .000223v3(4)=max * .00034v3(3)=max * .00067v1(5)v1(6)v1(7)\nv2(1)v2(4)v2(6)v2(7)\nbacktrace* P(RB|NN)* P(NN|NN)\nstartstartstartstartstarto5NNPP(MD|start) = .0006\nFigure 17.14 The \ufb01rst few entries in the individual state columns for the Viterbi algorithm. Each cell keeps\nthe probability of the best path so far and a pointer to the previous cell along that path. We have only \ufb01lled out\ncolumns 1 and 2; to avoid clutter most cells with value 0 are left empty. The rest is left as an exercise for the\nreader. After the cells are \ufb01lled in, backtracing from the endstate, we should be able to reconstruct the correct\nstate sequence NNP MD VB DT NN.\nFigure 17.14 shows a \ufb02eshed-out version of the sketch we saw in Fig. 17.11,\nthe Viterbi lattice for computing the best hidden state sequence for the observation\nsequence Janet will back the bill .\nThere are N=5 state columns. We begin in column 1 (for the word Janet ) by\nsetting the Viterbi value in each cell to the product of the ptransition probability (the\nstart probability for that state i, which we get from the <s>entry of Fig. 17.12), and",
    "metadata": {
      "source": "17",
      "chunk_id": 19,
      "token_count": 795,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15\n\n17.5 \u2022 C ONDITIONAL RANDOM FIELDS (CRF S)15\nthe observation likelihood of the word Janet given the tag for that cell. Most of the\ncells in the column are zero since the word Janet cannot be any of those tags. The\nreader should \ufb01nd this in Fig. 17.14.\nNext, each cell in the willcolumn gets updated. For each state, we compute the\nvalue viterbi [s;t]by taking the maximum over the extensions of all the paths from\nthe previous column that lead to the current cell according to Eq. 17.19. We have\nshown the values for the MD, VB, and NN cells. Each cell gets the max of the 7 val-\nues from the previous column, multiplied by the appropriate transition probability;\nas it happens in this case, most of them are zero from the previous column. The re-\nmaining value is multiplied by the relevant observation probability, and the (trivial)\nmax is taken. In this case the \ufb01nal value, 2.772e-8, comes from the NNP state at the\nprevious column. The reader should \ufb01ll in the rest of the lattice in Fig. 17.14 and\nbacktrace to see whether or not the Viterbi algorithm returns the gold state sequence\nNNP MD VB DT NN.\n17.5 Conditional Random Fields (CRFs)\nWhile the HMM is a useful and powerful model, it turns out that HMMs need a\nnumber of augmentations to achieve high accuracy. For example, in POS tagging\nas in other tasks, we often run into unknown words : proper names and acronymsunknown\nwords\nare created very often, and even new common nouns and verbs enter the language\nat a surprising rate. It would be great to have ways to add arbitrary features to\nhelp with this, perhaps based on capitalization or morphology (words starting with\ncapital letters are likely to be proper nouns, words ending with -edtend to be past\ntense (VBD or VBN), etc.) Or knowing the previous or following words might be a\nuseful feature (if the previous word is the, the current tag is unlikely to be a verb).\nAlthough we could try to hack the HMM to \ufb01nd ways to incorporate some of\nthese, in general it\u2019s hard for generative models like HMMs to add arbitrary features\ndirectly into the model in a clean way. We\u2019ve already seen a model for combining\narbitrary features in a principled way: log-linear models like the logistic regression\nmodel of Chapter 5! But logistic regression isn\u2019t a sequence model; it assigns a class\nto a single observation.\nLuckily, there is a discriminative sequence model based on log-linear models:\ntheconditional random \ufb01eld (CRF ). We\u2019ll describe here the linear chain CRF , CRF\nthe version of the CRF most commonly used for language processing, and the one\nwhose conditioning closely matches the HMM.\nAssuming we have a sequence of input words X=x1:::xnand want to compute\na sequence of output tags Y=y1:::yn. In an HMM to compute the best tag sequence\nthat maximizes P(YjX)we rely on Bayes\u2019 rule and the likelihood P(XjY):\n\u02c6Y=argmax\nYp(YjX)\n=argmax\nYp(XjY)p(Y)\n=argmax\nYY\nip(xijyi)Y\nip(yijyi\u00001) (17.21)\nIn a CRF, by contrast, we compute the posterior p(YjX)directly, training the CRF",
    "metadata": {
      "source": "17",
      "chunk_id": 20,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16\n\n16 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\nto discriminate among the possible tag sequences:\n\u02c6Y=argmax\nY2YP(YjX) (17.22)\nHowever, the CRF does not compute a probability for each tag at each time step. In-\nstead, at each time step the CRF computes log-linear functions over a set of relevant\nfeatures, and these local features are aggregated and normalized to produce a global\nprobability for the whole sequence.\nLet\u2019s introduce the CRF more formally, again using XandYas the input and\noutput sequences. A CRF is a log-linear model that assigns a probability to an entire\noutput (tag) sequence Y, out of all possible sequences Y, given the entire input\n(word) sequence X. We can think of a CRF as like a giant sequential version of\nthe multinomial logistic regression algorithm we saw for text categorization. Recall\nthat we introduced the feature function fin regular multinomial logistic regression\nfor text categorization as a function of a tuple: the input text xand a single class y\n(page ??). In a CRF, we\u2019re dealing with a sequence, so the function Fmaps an entire\ninput sequence Xand an entire output sequence Yto a feature vector. Let\u2019s assume\nwe have Kfeatures, with a weight wkfor each feature Fk:\np(YjX) =exp KX\nk=1wkFk(X;Y)!\nX\nY02Yexp KX\nk=1wkFk(X;Y0)! (17.23)\nIt\u2019s common to also describe the same equation by pulling out the denominator into\na function Z(X):\np(YjX) =1\nZ(X)exp KX\nk=1wkFk(X;Y)!\n(17.24)\nZ(X) =X\nY02Yexp KX\nk=1wkFk(X;Y0)!\n(17.25)\nWe\u2019ll call these Kfunctions Fk(X;Y)global features , since each one is a property\nof the entire input sequence Xand output sequence Y. We compute them by decom-\nposing into a sum of local features for each position iinY:\nFk(X;Y) =nX\ni=1fk(yi\u00001;yi;X;i) (17.26)\nEach of these local features fkin a linear-chain CRF is allowed to make use of the\ncurrent output token yi, the previous output token yi\u00001, the entire input string X(or\nany subpart of it), and the current position i. This constraint to only depend on\nthe current and previous output tokens yiandyi\u00001are what characterizes a linear\nchain CRF . As we will see, this limitation makes it possible to use versions of thelinear chain\nCRF\nef\ufb01cient Viterbi and Forward-Backwards algorithms from the HMM. A general CRF,\nby contrast, allows a feature to make use of any output token, and are thus necessary\nfor tasks in which the decision depend on distant output tokens, like yi\u00004. General\nCRFs require more complex inference, and are less commonly used for language\nprocessing.",
    "metadata": {
      "source": "17",
      "chunk_id": 21,
      "token_count": 683,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17",
    "metadata": {
      "source": "17",
      "chunk_id": 22,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "17.5 \u2022 C ONDITIONAL RANDOM FIELDS (CRF S)17\n17.5.1 Features in a CRF POS Tagger\nLet\u2019s look at some of these features in detail, since the reason to use a discriminative\nsequence model is that it\u2019s easier to incorporate a lot of features.2\nAgain, in a linear-chain CRF, each local feature fkat position ican depend on\nany information from: (yi\u00001;yi;X;i). So some legal features representing common\nsituations might be the following:\n1fxi=the;yi=DETg\n1fyi=PROPN, xi+1=Street ,yi\u00001=NUMg\n1fyi=VERB, yi\u00001=AUXg\nFor simplicity, we\u2019ll assume all CRF features take on the value 1 or 0. Above, we\nexplicitly use the notation 1fxgto mean \u201c1 if xis true, and 0 otherwise\u201d. From now\non, we\u2019ll leave off the 1when we de\ufb01ne features, but you can assume each feature\nhas it there implicitly.\nAlthough the idea of what features to use is done by the system designer by hand,\nthe speci\ufb01c features are automatically populated by using feature templates as wefeature\ntemplates\nbrie\ufb02y mentioned in Chapter 5. Here are some templates that only use information\nfrom (yi\u00001;yi;X;i):\nhyi;xii;hyi;yi\u00001i;hyi;xi\u00001;xi+2i\nThese templates automatically populate the set of features from every instance in\nthe training and test set. Thus for our example Janet/NNP will/MD back/VB the/DT\nbill/NN , when xiis the word back, the following features would be generated and\nhave the value 1 (we\u2019ve assigned them arbitrary feature numbers):\nf3743:yi= VB and xi= back\nf156:yi= VB and yi\u00001= MD\nf99732 :yi= VB and xi\u00001= will and xi+2= bill\nIt\u2019s also important to have features that help with unknown words. One of the\nmost important is word shape features, which represent the abstract letter pattern word shape\nof the word by mapping lower-case letters to \u2018x\u2019, upper-case to \u2018X\u2019, numbers to\n\u2019d\u2019, and retaining punctuation. Thus for example I.M.F. would map to X.X.X. and\nDC10-30 would map to XXdd-dd. A second class of shorter word shape features is\nalso used. In these features consecutive character types are removed, so words in all\ncaps map to X, words with initial-caps map to Xx, DC10-30 would be mapped to\nXd-d but I.M.F would still map to X.X.X. Pre\ufb01x and suf\ufb01x features are also useful.\nIn summary, here are some sample feature templates that help with unknown words:\nxicontains a particular pre\ufb01x (perhaps from all pre\ufb01xes of length \u00142)\nxicontains a particular suf\ufb01x (perhaps from all suf\ufb01xes of length \u00142)\nxi\u2019s word shape\nxi\u2019s short word shape\nFor example the word well-dressed might generate the following non-zero val-\nued feature values:\n2Because in HMMs all computation is based on the two probabilities P(tagjtag)andP(wordjtag), if\nwe want to include some source of knowledge into the tagging process, we must \ufb01nd a way to encode\nthe knowledge into one of these two probabilities. Each time we add a feature we have to do a lot of\ncomplicated conditioning which gets harder and harder as we have more and more such features.",
    "metadata": {
      "source": "17",
      "chunk_id": 23,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18\n\n18 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\npre\ufb01x( xi) =w\npre\ufb01x( xi) =we\nsuf\ufb01x( xi) =ed\nsuf\ufb01x( xi) =d\nword-shape( xi) =xxxx-xxxxxxx\nshort-word-shape( xi) =x-x\nThe known-word templates are computed for every word seen in the training\nset; the unknown word features can also be computed for all words in training, or\nonly on training words whose frequency is below some threshold. The result of the\nknown-word templates and word-signature features is a very large set of features.\nGenerally a feature cutoff is used in which features are thrown out if they have count\n<5 in the training set.\nRemember that in a CRF we don\u2019t learn weights for each of these local features\nfk. Instead, we \ufb01rst sum the values of each local feature (for example feature f3743)\nover the entire sentence, to create each global feature (for example F3743). It is those\nglobal features that will then be multiplied by weight w3743. Thus for training and\ninference there is always a \ufb01xed set of Kfeatures with Kweights, even though the\nlength of each sentence is different.\n17.5.2 Features for CRF Named Entity Recognizers\nA CRF for NER makes use of very similar features to a POS tagger, as shown in\nFigure 17.15.\nidentity of wi, identity of neighboring words\nembeddings for wi, embeddings for neighboring words\npart of speech of wi, part of speech of neighboring words\npresence of wiin agazetteer\nwicontains a particular pre\ufb01x (from all pre\ufb01xes of length \u00144)\nwicontains a particular suf\ufb01x (from all suf\ufb01xes of length \u00144)\nword shape of wi, word shape of neighboring words\nshort word shape of wi, short word shape of neighboring words\ngazetteer features\nFigure 17.15 Typical features for a feature-based NER system.\nOne feature that is especially useful for locations is a gazetteer , a list of place gazetteer\nnames, often providing millions of entries for locations with detailed geographical\nand political information.3This can be implemented as a binary feature indicating a\nphrase appears in the list. Other related resources like name-lists , for example from\nthe United States Census Bureau4, can be used, as can other entity dictionaries like\nlists of corporations or products, although they may not be as helpful as a gazetteer\n(Mikheev et al., 1999).\nThe sample named entity token L\u2019Occitane would generate the following non-\nzero valued feature values (assuming that L\u2019Occitane is neither in the gazetteer nor\nthe census).\n3www.geonames.org\n4www.census.gov",
    "metadata": {
      "source": "17",
      "chunk_id": 24,
      "token_count": 627,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\n17.5 \u2022 C ONDITIONAL RANDOM FIELDS (CRF S)19\npre\ufb01x( xi) =L suf\ufb01x( xi) =tane\npre\ufb01x( xi) =L' suf\ufb01x( xi) =ane\npre\ufb01x( xi) =L'O suf\ufb01x( xi) =ne\npre\ufb01x( xi) =L'Oc suf\ufb01x( xi) =e\nword-shape( xi) =X'Xxxxxxxx short-word-shape( xi) =X'Xx\nFigure 17.16 illustrates the result of adding part-of-speech tags and some shape\ninformation to our earlier example.\nWords POS Short shape Gazetteer BIO Label\nJane NNP Xx 0 B-PER\nVillanueva NNP Xx 1 I-PER\nof IN x 0 O\nUnited NNP Xx 0 B-ORG\nAirlines NNP Xx 0 I-ORG\nHolding NNP Xx 0 I-ORG\ndiscussed VBD x 0 O\nthe DT x 0 O\nChicago NNP Xx 1 B-LOC\nroute NN x 0 O\n. . . 0 O\nFigure 17.16 Some NER features for a sample sentence, assuming that Chicago and Vil-\nlanueva are listed as locations in a gazetteer. We assume features only take on the values 0 or\n1, so the \ufb01rst POS feature, for example, would be represented as 1fPOS=NNPg.\n17.5.3 Inference and Training for CRFs\nHow do we \ufb01nd the best tag sequence \u02c6Yfor a given input X? We start with Eq. 17.22:\n\u02c6Y=argmax\nY2YP(YjX)\n=argmax\nY2Y1\nZ(X)exp KX\nk=1wkFk(X;Y)!\n(17.27)\n=argmax\nY2Yexp KX\nk=1wknX\ni=1fk(yi\u00001;yi;X;i)!\n(17.28)\n=argmax\nY2YKX\nk=1wknX\ni=1fk(yi\u00001;yi;X;i) (17.29)\n=argmax\nY2YnX\ni=1KX\nk=1wkfk(yi\u00001;yi;X;i) (17.30)\nWe can ignore the exp function and the denominator Z(X), as we do above, because\nexp doesn\u2019t change the argmax, and the denominator Z(X)is constant for a given\nobservation sequence X.\nHow should we decode to \ufb01nd this optimal tag sequence \u02c6 y? Just as with HMMs,\nwe\u2019ll turn to the Viterbi algorithm, which works because, like the HMM, the linear-\nchain CRF depends at each timestep on only one previous output token yi\u00001.\nConcretely, this involves \ufb01lling an N\u0002Tarray with the appropriate values, main-\ntaining backpointers as we proceed. As with HMM Viterbi, when the table is \ufb01lled,\nwe simply follow pointers back from the maximum value in the \ufb01nal column to\nretrieve the desired set of labels.",
    "metadata": {
      "source": "17",
      "chunk_id": 25,
      "token_count": 715,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\n20 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\nThe requisite changes from HMM Viterbi have to do only with how we \ufb01ll each\ncell. Recall from Eq. 17.19 that the recursive step of the Viterbi equation computes\nthe Viterbi value of time tfor state jas\nvt(j) =Nmax\ni=1vt\u00001(i)ai jbj(ot); 1\u0014j\u0014N;1<t\u0014T (17.31)\nwhich is the HMM implementation of\nvt(j) =Nmax\ni=1vt\u00001(i)P(sjjsi)P(otjsj)1\u0014j\u0014N;1<t\u0014T (17.32)\nThe CRF requires only a slight change to this latter formula, replacing the aandb\nprior and likelihood probabilities with the CRF features:\nvt(j) =Nmax\ni=1\"\nvt\u00001(i)+KX\nk=1wkfk(yt\u00001;yt;X;t)1\u0014j\u0014N;1<t\u0014T#\n(17.33)\nLearning in CRFs relies on the same supervised learning algorithms we presented\nfor logistic regression. Given a sequence of observations, feature functions, and cor-\nresponding outputs, we use stochastic gradient descent to train the weights to maxi-\nmize the log-likelihood of the training corpus. The local nature of linear-chain CRFs\nmeans that the forward-backward algorithm introduced for HMMs in Appendix A\ncan be extended to a CRF version that will ef\ufb01ciently compute the necessary deriva-\ntives. As with logistic regression, L1 or L2 regularization is important.\n17.6 Evaluation of Named Entity Recognition\nPart-of-speech taggers are evaluated by the standard metric of accuracy . Named\nentity recognizers are evaluated by recall ,precision , and F1measure . Recall that\nrecall is the ratio of the number of correctly labeled responses to the total that should\nhave been labeled; precision is the ratio of the number of correctly labeled responses\nto the total labeled; and F-measure is the harmonic mean of the two.\nTo know if the difference between the F 1scores of two NER systems is a signif-\nicant difference, we use the paired bootstrap test, or the similar randomization test\n(Section ??).\nFor named entity tagging, the entity rather than the word is the unit of response.\nThus in the example in Fig. 17.16, the two entities Jane Villanueva andUnited Air-\nlines Holding and the non-entity discussed would each count as a single response.\nThe fact that named entity tagging has a segmentation component which is not\npresent in tasks like text categorization or part-of-speech tagging causes some prob-\nlems with evaluation. For example, a system that labeled Jane but not Jane Vil-\nlanueva as a person would cause two errors, a false positive for O and a false nega-\ntive for I-PER. In addition, using entities as the unit of response but words as the unit\nof training means that there is a mismatch between the training and test conditions.\n17.7 Further Details\nIn this section we summarize a few remaining details of the data and models for\npart-of-speech tagging and NER, beginning with data. Since the algorithms we have",
    "metadata": {
      "source": "17",
      "chunk_id": 26,
      "token_count": 708,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21",
    "metadata": {
      "source": "17",
      "chunk_id": 27,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "17.7 \u2022 F URTHER DETAILS 21\npresented are supervised, having labeled data is essential for training and testing. A\nwide variety of datasets exist for part-of-speech tagging and/or NER. The Universal\nDependencies (UD) dataset (de Marneffe et al., 2021) has POS tagged corpora in\nover a hundred languages, as do the Penn Treebanks in English, Chinese, and Arabic.\nOntoNotes has corpora labeled for named entities in English, Chinese, and Arabic\n(Hovy et al., 2006). Named entity tagged corpora are also available in particular\ndomains, such as for biomedical (Bada et al., 2012) and literary text (Bamman et al.,\n2019).\n17.7.1 Rule-based Methods\nWhile machine learned (neural or CRF) sequence models are the norm in academic\nresearch, commercial approaches to NER are often based on pragmatic combina-\ntions of lists and rules, with some smaller amount of supervised machine learning\n(Chiticariu et al., 2013). For example in the IBM System T architecture, a user\nspeci\ufb01es declarative constraints for tagging tasks in a formal query language that\nincludes regular expressions, dictionaries, semantic constraints, and other operators,\nwhich the system compiles into an ef\ufb01cient extractor (Chiticariu et al., 2018).\nOne common approach is to make repeated rule-based passes over a text, starting\nwith rules with very high precision but low recall, and, in subsequent stages, using\nmachine learning methods that take the output of the \ufb01rst pass into account (an\napproach \ufb01rst worked out for coreference (Lee et al., 2017)):\n1. First, use high-precision rules to tag unambiguous entity mentions.\n2. Then, search for substring matches of the previously detected names.\n3. Use application-speci\ufb01c name lists to \ufb01nd likely domain-speci\ufb01c mentions.\n4. Finally, apply supervised sequence labeling techniques that use tags from pre-\nvious stages as additional features.\nRule-based methods were also the earliest methods for part-of-speech tagging.\nRule-based taggers like the English Constraint Grammar system (Karlsson et al.\n1995, V outilainen 1999) use a two-stage formalism invented in the 1950s and 1960s:\n(1) a morphological analyzer with tens of thousands of word stem entries returns all\nparts of speech for a word, then (2) a large set of thousands of constraints are applied\nto the input sentence to rule out parts of speech inconsistent with the context.\n17.7.2 POS Tagging for Morphologically Rich Languages\nAugmentations to tagging algorithms become necessary when dealing with lan-\nguages with rich morphology like Czech, Hungarian and Turkish.\nThese productive word-formation processes result in a large vocabulary for these\nlanguages: a 250,000 word token corpus of Hungarian has more than twice as many\nword types as a similarly sized corpus of English (Oravecz and Dienes, 2002), while\na 10 million word token corpus of Turkish contains four times as many word types\nas a similarly sized English corpus (Hakkani-T \u00a8ur et al., 2002). Large vocabular-\nies mean many unknown words, and these unknown words cause signi\ufb01cant per-\nformance degradations in a wide variety of languages (including Czech, Slovene,\nEstonian, and Romanian) (Haji \u02c7c, 2000).\nHighly in\ufb02ectional languages also have much more information than English",
    "metadata": {
      "source": "17",
      "chunk_id": 28,
      "token_count": 764,
      "chapter_title": ""
    }
  },
  {
    "content": "1995, V outilainen 1999) use a two-stage formalism invented in the 1950s and 1960s:\n(1) a morphological analyzer with tens of thousands of word stem entries returns all\nparts of speech for a word, then (2) a large set of thousands of constraints are applied\nto the input sentence to rule out parts of speech inconsistent with the context.\n17.7.2 POS Tagging for Morphologically Rich Languages\nAugmentations to tagging algorithms become necessary when dealing with lan-\nguages with rich morphology like Czech, Hungarian and Turkish.\nThese productive word-formation processes result in a large vocabulary for these\nlanguages: a 250,000 word token corpus of Hungarian has more than twice as many\nword types as a similarly sized corpus of English (Oravecz and Dienes, 2002), while\na 10 million word token corpus of Turkish contains four times as many word types\nas a similarly sized English corpus (Hakkani-T \u00a8ur et al., 2002). Large vocabular-\nies mean many unknown words, and these unknown words cause signi\ufb01cant per-\nformance degradations in a wide variety of languages (including Czech, Slovene,\nEstonian, and Romanian) (Haji \u02c7c, 2000).\nHighly in\ufb02ectional languages also have much more information than English\ncoded in word morphology, like case (nominative, accusative, genitive) or gender\n(masculine, feminine). Because this information is important for tasks like pars-\ning and coreference resolution, part-of-speech taggers for morphologically rich lan-",
    "metadata": {
      "source": "17",
      "chunk_id": 29,
      "token_count": 348,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22\n\n22 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\nguages need to label words with case and gender information. Tagsets for morpho-\nlogically rich languages are therefore sequences of morphological tags rather than a\nsingle primitive tag. Here\u2019s a Turkish example, in which the word izinhas three pos-\nsible morphological/part-of-speech tags and meanings (Hakkani-T \u00a8ur et al., 2002):\n1. Yerdeki izintemizlenmesi gerek. iz + Noun+A3sg+Pnon+Gen\nThe trace on the \ufb02oor should be cleaned.\n2.\u00a8Uzerinde parmak izinkalmis \u00b8. iz + Noun+A3sg+P2sg+Nom\nYour \ufb01nger print is left on (it).\n3. Ic \u00b8eri girmek ic \u00b8in izinalman gerekiyor. izin + Noun+A3sg+Pnon+Nom\nYou need permission to enter.\nUsing a morphological parse sequence like Noun+A3sg+Pnon+Gen as the part-\nof-speech tag greatly increases the number of parts of speech, and so tagsets can\nbe 4 to 10 times larger than the 50\u2013100 tags we have seen for English. With such\nlarge tagsets, each word needs to be morphologically analyzed to generate the list\nof possible morphological tag sequences (part-of-speech tags) for the word. The\nrole of the tagger is then to disambiguate among these tags. This method also helps\nwith unknown words since morphological parsers can accept unknown stems and\nstill segment the af\ufb01xes properly.\n17.8 Summary\nThis chapter introduced parts of speech andnamed entities , and the tasks of part-\nof-speech tagging andnamed entity recognition :\n\u2022 Languages generally have a small set of closed class words that are highly\nfrequent, ambiguous, and act as function words , and open-class words like\nnouns ,verbs ,adjectives . Various part-of-speech tagsets exist, of between 40\nand 200 tags.\n\u2022Part-of-speech tagging is the process of assigning a part-of-speech label to\neach of a sequence of words.\n\u2022Named entities are words for proper nouns referring mainly to people, places,\nand organizations, but extended to many other types that aren\u2019t strictly entities\nor even proper nouns.\n\u2022 Two common approaches to sequence modeling are a generative approach,\nHMM tagging, and a discriminative approach, CRF tagging. We will see a\nneural approach in following chapters.\n\u2022 The probabilities in HMM taggers are estimated by maximum likelihood es-\ntimation on tag-labeled training corpora. The Viterbi algorithm is used for\ndecoding , \ufb01nding the most likely tag sequence\n\u2022Conditional Random Fields orCRF taggers train a log-linear model that can\nchoose the best tag sequence given an observation sequence, based on features\nthat condition on the output tag, the prior output tag, the entire input sequence,\nand the current timestep. They use the Viterbi algorithm for inference, to\nchoose the best sequence of tags, and a version of the Forward-Backward\nalgorithm (see Appendix A) for training,",
    "metadata": {
      "source": "17",
      "chunk_id": 30,
      "token_count": 697,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23",
    "metadata": {
      "source": "17",
      "chunk_id": 31,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "BIBLIOGRAPHICAL AND HISTORICAL NOTES 23\nBibliographical and Historical Notes\nWhat is probably the earliest part-of-speech tagger was part of the parser in Zellig\nHarris\u2019s Transformations and Discourse Analysis Project (TDAP), implemented be-\ntween June 1958 and July 1959 at the University of Pennsylvania (Harris, 1962),\nalthough earlier systems had used part-of-speech dictionaries. TDAP used 14 hand-\nwritten rules for part-of-speech disambiguation; the use of part-of-speech tag se-\nquences and the relative frequency of tags for a word pre\ufb01gures modern algorithms.\nThe parser was implemented essentially as a cascade of \ufb01nite-state transducers; see\nJoshi and Hopely (1999) and Karttunen (1999) for a reimplementation.\nThe Computational Grammar Coder (CGC) of Klein and Simmons (1963) had\nthree components: a lexicon, a morphological analyzer, and a context disambigua-\ntor. The small 1500-word lexicon listed only function words and other irregular\nwords. The morphological analyzer used in\ufb02ectional and derivational suf\ufb01xes to as-\nsign part-of-speech classes. These were run over words to produce candidate parts\nof speech which were then disambiguated by a set of 500 context rules by relying on\nsurrounding islands of unambiguous words. For example, one rule said that between\nan ARTICLE and a VERB, the only allowable sequences were ADJ-NOUN, NOUN-\nADVERB, or NOUN-NOUN. The TAGGIT tagger (Greene and Rubin, 1971) used\nthe same architecture as Klein and Simmons (1963), with a bigger dictionary and\nmore tags (87). TAGGIT was applied to the Brown corpus and, according to Francis\nand Ku \u02c7cera (1982, p. 9), accurately tagged 77% of the corpus; the remainder of the\nBrown corpus was then tagged by hand. All these early algorithms were based on\na two-stage architecture in which a dictionary was \ufb01rst used to assign each word a\nset of potential parts of speech, and then lists of handwritten disambiguation rules\nwinnowed the set down to a single part of speech per word.\nProbabilities were used in tagging by Stolz et al. (1965) and a complete proba-\nbilistic tagger with Viterbi decoding was sketched by Bahl and Mercer (1976). The\nLancaster-Oslo/Bergen (LOB) corpus, a British English equivalent of the Brown cor-\npus, was tagged in the early 1980\u2019s with the CLAWS tagger (Marshall 1983; Mar-\nshall 1987; Garside 1987), a probabilistic algorithm that approximated a simpli\ufb01ed\nHMM tagger. The algorithm used tag bigram probabilities, but instead of storing the\nword likelihood of each tag, the algorithm marked tags either as rare (P(tagjword)<\n:01)infrequent (P(tagjword)< :10) or normally frequent (P(tagjword)> :10).\nDeRose (1988) developed a quasi-HMM algorithm, including the use of dy-\nnamic programming, although computing P(tjw)P(w)instead of P(wjt)P(w). The\nsame year, the probabilistic PARTS tagger of Church 1988, 1989 was probably the\n\ufb01rst implemented HMM tagger, described correctly in Church (1989), although",
    "metadata": {
      "source": "17",
      "chunk_id": 32,
      "token_count": 769,
      "chapter_title": ""
    }
  },
  {
    "content": "winnowed the set down to a single part of speech per word.\nProbabilities were used in tagging by Stolz et al. (1965) and a complete proba-\nbilistic tagger with Viterbi decoding was sketched by Bahl and Mercer (1976). The\nLancaster-Oslo/Bergen (LOB) corpus, a British English equivalent of the Brown cor-\npus, was tagged in the early 1980\u2019s with the CLAWS tagger (Marshall 1983; Mar-\nshall 1987; Garside 1987), a probabilistic algorithm that approximated a simpli\ufb01ed\nHMM tagger. The algorithm used tag bigram probabilities, but instead of storing the\nword likelihood of each tag, the algorithm marked tags either as rare (P(tagjword)<\n:01)infrequent (P(tagjword)< :10) or normally frequent (P(tagjword)> :10).\nDeRose (1988) developed a quasi-HMM algorithm, including the use of dy-\nnamic programming, although computing P(tjw)P(w)instead of P(wjt)P(w). The\nsame year, the probabilistic PARTS tagger of Church 1988, 1989 was probably the\n\ufb01rst implemented HMM tagger, described correctly in Church (1989), although\nChurch (1988) also described the computation incorrectly as P(tjw)P(w)instead\nofP(wjt)P(w). Church (p.c.) explained that he had simpli\ufb01ed for pedagogical pur-\nposes because using the probability P(tjw)made the idea seem more understandable\nas \u201cstoring a lexicon in an almost standard form\u201d.\nLater taggers explicitly introduced the use of the hidden Markov model (Kupiec\n1992; Weischedel et al. 1993; Sch \u00a8utze and Singer 1994). Merialdo (1994) showed\nthat fully unsupervised EM didn\u2019t work well for the tagging task and that reliance\non hand-labeled data was important. Charniak et al. (1993) showed the importance\nof the most frequent tag baseline; the 92.3% number we give above was from Abney\net al. (1999). See Brants (2000) for HMM tagger implementation details, includ-\ning the extension to trigram contexts, and the use of sophisticated unknown word\nfeatures; its performance is still close to state of the art taggers.",
    "metadata": {
      "source": "17",
      "chunk_id": 33,
      "token_count": 537,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24",
    "metadata": {
      "source": "17",
      "chunk_id": 34,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "24 CHAPTER 17 \u2022 S EQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\nLog-linear models for POS tagging were introduced by Ratnaparkhi (1996),\nwho introduced a system called MXPOST which implemented a maximum entropy\nMarkov model (MEMM), a slightly simpler version of a CRF. Around the same\ntime, sequence labelers were applied to the task of named entity tagging, \ufb01rst with\nHMMs (Bikel et al., 1997) and MEMMs (McCallum et al., 2000), and then once\nCRFs were developed (Lafferty et al. 2001), they were also applied to NER (Mc-\nCallum and Li, 2003). A wide exploration of features followed (Zhou et al., 2005).\nNeural approaches to NER mainly follow from the pioneering results of Collobert\net al. (2011), who applied a CRF on top of a convolutional net. BiLSTMs with word\nand character-based embeddings as input followed shortly and became a standard\nneural algorithm for NER (Huang et al. 2015, Ma and Hovy 2016, Lample et al.\n2016) followed by the more recent use of Transformers and BERT.\nThe idea of using letter suf\ufb01xes for unknown words is quite old; the early Klein\nand Simmons (1963) system checked all \ufb01nal letter suf\ufb01xes of lengths 1-5. The\nunknown word features described on page 17 come mainly from Ratnaparkhi (1996),\nwith augmentations from Toutanova et al. (2003) and Manning (2011).\nState of the art POS taggers use neural algorithms, either bidirectional RNNs or\nTransformers like BERT; see Chapter 8 to Chapter 11. HMM (Brants 2000; Thede\nand Harper 1999) and CRF tagger accuracies are likely just a tad lower.\nManning (2011) investigates the remaining 2.7% of errors in a high-performing\ntagger (Toutanova et al., 2003). He suggests that a third or half of these remaining\nerrors are due to errors or inconsistencies in the training data, a third might be solv-\nable with richer linguistic models, and for the remainder the task is underspeci\ufb01ed\nor unclear.\nSupervised tagging relies heavily on in-domain training data hand-labeled by\nexperts. Ways to relax this assumption include unsupervised algorithms for cluster-\ning words into part-of-speech-like classes, summarized in Christodoulopoulos et al.\n(2010), and ways to combine labeled and unlabeled data, for example by co-training\n(Clark et al. 2003; S\u00f8gaard 2010).\nSee Householder (1995) for historical notes on parts of speech, and Sampson\n(1987) and Garside et al. (1997) on the provenance of the Brown and other tagsets.\nExercises\n17.1 Find one tagging error in each of the following sentences that are tagged with\nthe Penn Treebank tagset:\n1. I/PRP need/VBP a/DT \ufb02ight/NN from/IN Atlanta/NN\n2. Does/VBZ this/DT \ufb02ight/NN serve/VB dinner/NNS\n3. I/PRP have/VB a/DT friend/NN living/VBG in/IN Denver/NNP\n4. Can/VBP you/PRP list/VB the/DT nonstop/JJ afternoon/NN \ufb02ights/NNS",
    "metadata": {
      "source": "17",
      "chunk_id": 35,
      "token_count": 774,
      "chapter_title": ""
    }
  },
  {
    "content": "able with richer linguistic models, and for the remainder the task is underspeci\ufb01ed\nor unclear.\nSupervised tagging relies heavily on in-domain training data hand-labeled by\nexperts. Ways to relax this assumption include unsupervised algorithms for cluster-\ning words into part-of-speech-like classes, summarized in Christodoulopoulos et al.\n(2010), and ways to combine labeled and unlabeled data, for example by co-training\n(Clark et al. 2003; S\u00f8gaard 2010).\nSee Householder (1995) for historical notes on parts of speech, and Sampson\n(1987) and Garside et al. (1997) on the provenance of the Brown and other tagsets.\nExercises\n17.1 Find one tagging error in each of the following sentences that are tagged with\nthe Penn Treebank tagset:\n1. I/PRP need/VBP a/DT \ufb02ight/NN from/IN Atlanta/NN\n2. Does/VBZ this/DT \ufb02ight/NN serve/VB dinner/NNS\n3. I/PRP have/VB a/DT friend/NN living/VBG in/IN Denver/NNP\n4. Can/VBP you/PRP list/VB the/DT nonstop/JJ afternoon/NN \ufb02ights/NNS\n17.2 Use the Penn Treebank tagset to tag each word in the following sentences\nfrom Damon Runyon\u2019s short stories. You may ignore punctuation. Some of\nthese are quite dif\ufb01cult; do your best.\n1. It is a nice night.\n2. This crap game is over a garage in Fifty-second Street. . .\n3. . . . Nobody ever takes the newspapers she sells . . .\n4. He is a tall, skinny guy with a long, sad, mean-looking kisser, and a\nmournful voice.",
    "metadata": {
      "source": "17",
      "chunk_id": 36,
      "token_count": 400,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 25\n\nEXERCISES 25\n5. . . . I am sitting in Mindy\u2019s restaurant putting on the ge\ufb01llte \ufb01sh, which is\na dish I am very fond of, . . .\n6. When a guy and a doll get to taking peeks back and forth at each other,\nwhy there you are indeed.\n17.3 Now compare your tags from the previous exercise with one or two friend\u2019s\nanswers. On which words did you disagree the most? Why?\n17.4 Implement the \u201cmost likely tag\u201d baseline. Find a POS-tagged training set,\nand use it to compute for each word the tag that maximizes p(tjw). You will\nneed to implement a simple tokenizer to deal with sentence boundaries. Start\nby assuming that all unknown words are NN and compute your error rate on\nknown and unknown words. Now write at least \ufb01ve rules to do a better job of\ntagging unknown words, and show the difference in error rates.\n17.5 Build a bigram HMM tagger. You will need a part-of-speech-tagged corpus.\nFirst split the corpus into a training set and test set. From the labeled training\nset, train the transition and observation probabilities of the HMM tagger di-\nrectly on the hand-tagged data. Then implement the Viterbi algorithm so you\ncan decode a test sentence. Now run your algorithm on the test set. Report its\nerror rate and compare its performance to the most frequent tag baseline.\n17.6 Do an error analysis of your tagger. Build a confusion matrix and investigate\nthe most frequent errors. Propose some features for improving the perfor-\nmance of your tagger on these errors.\n17.7 Develop a set of regular expressions to recognize the character shape features\ndescribed on page 17.\n17.8 The BIO and other labeling schemes given in this chapter aren\u2019t the only\npossible one. For example, the Btag can be reserved only for those situations\nwhere an ambiguity exists between adjacent entities. Propose a new set of\nBIOtags for use with your NER system. Experiment with it and compare its\nperformance with the schemes presented in this chapter.\n17.9 Names of works of art (books, movies, video games, etc.) are quite different\nfrom the kinds of named entities we\u2019ve discussed in this chapter. Collect a\nlist of names of works of art from a particular category from a Web-based\nsource (e.g., gutenberg.org, amazon.com, imdb.com, etc.). Analyze your list\nand give examples of ways that the names in it are likely to be problematic for\nthe techniques described in this chapter.\n17.10 Develop an NER system speci\ufb01c to the category of names that you collected\nin the last exercise. Evaluate your system on a collection of text likely to\ncontain instances of these named entities.",
    "metadata": {
      "source": "17",
      "chunk_id": 37,
      "token_count": 606,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 26",
    "metadata": {
      "source": "17",
      "chunk_id": 38,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "26 Chapter 17 \u2022 Sequence Labeling for Parts of Speech and Named Entities\nAbney, S. P., R. E. Schapire, and Y . Singer. 1999. Boosting\napplied to tagging and PP attachment. EMNLP/VLC .\nBada, M., M. Eckert, D. Evans, K. Garcia, K. Shipley, D. Sit-\nnikov, W. A. Baumgartner, K. B. Cohen, K. Verspoor,\nJ. A. Blake, and L. E. Hunter. 2012. Concept annotation\nin the craft corpus. BMC bioinformatics , 13(1):161.\nBahl, L. R. and R. L. Mercer. 1976. Part of speech as-\nsignment by a statistical decision algorithm. Proceedings\nIEEE International Symposium on Information Theory .\nBamman, D., S. Popat, and S. Shen. 2019. An annotated\ndataset of literary entities. NAACL HLT .\nBikel, D. M., S. Miller, R. Schwartz, and R. Weischedel.\n1997. Nymble: A high-performance learning name-\n\ufb01nder. ANLP .\nBrants, T. 2000. TnT: A statistical part-of-speech tagger.\nANLP .\nBroschart, J. 1997. Why Tongan does it differently. Linguis-\ntic Typology , 1:123\u2013165.\nCharniak, E., C. Hendrickson, N. Jacobson, and\nM. Perkowitz. 1993. Equations for part-of-speech tag-\nging. AAAI .\nChiticariu, L., M. Danilevsky, Y . Li, F. Reiss, and H. Zhu.\n2018. SystemT: Declarative text understanding for enter-\nprise. NAACL HLT , volume 3.\nChiticariu, L., Y . Li, and F. R. Reiss. 2013. Rule-Based In-\nformation Extraction is Dead! Long Live Rule-Based In-\nformation Extraction Systems! EMNLP .\nChristodoulopoulos, C., S. Goldwater, and M. Steedman.\n2010. Two decades of unsupervised POS induction: How\nfar have we come? EMNLP .\nChurch, K. W. 1988. A stochastic parts program and noun\nphrase parser for unrestricted text. ANLP .\nChurch, K. W. 1989. A stochastic parts program and noun\nphrase parser for unrestricted text. ICASSP .\nClark, S., J. R. Curran, and M. Osborne. 2003. Bootstrapping\nPOS-taggers using unlabelled data. CoNLL .\nCollobert, R., J. Weston, L. Bottou, M. Karlen,\nK. Kavukcuoglu, and P. Kuksa. 2011. Natural language\nprocessing (almost) from scratch. JMLR , 12:2493\u20132537.\nDeRose, S. J. 1988. Grammatical category disambiguation\nby statistical optimization. Computational Linguistics ,\n14:31\u201339.\nEvans, N. 2000. Word classes in the world\u2019s languages. In\nG. Booij, C. Lehmann, and J. Mugdan, eds, Morphology:\nA Handbook on In\ufb02ection and Word Formation , 708\u2013732.\nMouton.",
    "metadata": {
      "source": "17",
      "chunk_id": 39,
      "token_count": 749,
      "chapter_title": ""
    }
  },
  {
    "content": "2010. Two decades of unsupervised POS induction: How\nfar have we come? EMNLP .\nChurch, K. W. 1988. A stochastic parts program and noun\nphrase parser for unrestricted text. ANLP .\nChurch, K. W. 1989. A stochastic parts program and noun\nphrase parser for unrestricted text. ICASSP .\nClark, S., J. R. Curran, and M. Osborne. 2003. Bootstrapping\nPOS-taggers using unlabelled data. CoNLL .\nCollobert, R., J. Weston, L. Bottou, M. Karlen,\nK. Kavukcuoglu, and P. Kuksa. 2011. Natural language\nprocessing (almost) from scratch. JMLR , 12:2493\u20132537.\nDeRose, S. J. 1988. Grammatical category disambiguation\nby statistical optimization. Computational Linguistics ,\n14:31\u201339.\nEvans, N. 2000. Word classes in the world\u2019s languages. In\nG. Booij, C. Lehmann, and J. Mugdan, eds, Morphology:\nA Handbook on In\ufb02ection and Word Formation , 708\u2013732.\nMouton.\nFrancis, W. N. and H. Ku \u02c7cera. 1982. Frequency Analysis of\nEnglish Usage . Houghton Mif\ufb02in, Boston.\nGarside, R. 1987. The CLAWS word-tagging system. In\nR. Garside, G. Leech, and G. Sampson, eds, The Compu-\ntational Analysis of English , 30\u201341. Longman.\nGarside, R., G. Leech, and A. McEnery. 1997. Corpus An-\nnotation . Longman.\nGil, D. 2000. Syntactic categories, cross-linguistic variation\nand universal grammar. In P. M. V ogel and B. Comrie,\neds, Approaches to the Typology of Word Classes , 173\u2013\n216. Mouton.Greene, B. B. and G. M. Rubin. 1971. Automatic grammati-\ncal tagging of English. Department of Linguistics, Brown\nUniversity, Providence, Rhode Island.\nHaji\u02c7c, J. 2000. Morphological tagging: Data vs. dictionaries.\nNAACL .\nHakkani-T \u00a8ur, D., K. O\ufb02azer, and G. T \u00a8ur. 2002. Sta-\ntistical morphological disambiguation for agglutinative\nlanguages. Journal of Computers and Humanities ,\n36(4):381\u2013410.\nHarris, Z. S. 1962. String Analysis of Sentence Structure .\nMouton, The Hague.\nHouseholder, F. W. 1995. Dionysius Thrax, the technai , and\nSextus Empiricus. In E. F. K. Koerner and R. E. Asher,\neds, Concise History of the Language Sciences , 99\u2013103.\nElsevier Science.\nHovy, E. H., M. P. Marcus, M. Palmer, L. A. Ramshaw,\nand R. Weischedel. 2006. OntoNotes: The 90% solution.\nHLT-NAACL .\nHuang, Z., W. Xu, and K. Yu. 2015. Bidirectional LSTM-\nCRF models for sequence tagging. arXiv preprint",
    "metadata": {
      "source": "17",
      "chunk_id": 40,
      "token_count": 762,
      "chapter_title": ""
    }
  },
  {
    "content": "cal tagging of English. Department of Linguistics, Brown\nUniversity, Providence, Rhode Island.\nHaji\u02c7c, J. 2000. Morphological tagging: Data vs. dictionaries.\nNAACL .\nHakkani-T \u00a8ur, D., K. O\ufb02azer, and G. T \u00a8ur. 2002. Sta-\ntistical morphological disambiguation for agglutinative\nlanguages. Journal of Computers and Humanities ,\n36(4):381\u2013410.\nHarris, Z. S. 1962. String Analysis of Sentence Structure .\nMouton, The Hague.\nHouseholder, F. W. 1995. Dionysius Thrax, the technai , and\nSextus Empiricus. In E. F. K. Koerner and R. E. Asher,\neds, Concise History of the Language Sciences , 99\u2013103.\nElsevier Science.\nHovy, E. H., M. P. Marcus, M. Palmer, L. A. Ramshaw,\nand R. Weischedel. 2006. OntoNotes: The 90% solution.\nHLT-NAACL .\nHuang, Z., W. Xu, and K. Yu. 2015. Bidirectional LSTM-\nCRF models for sequence tagging. arXiv preprint\narXiv:1508.01991 .\nJoshi, A. K. and P. Hopely. 1999. A parser from antiquity.\nIn A. Kornai, ed., Extended Finite State Models of Lan-\nguage , 6\u201315. Cambridge University Press.\nKarlsson, F., A. V outilainen, J. Heikkil \u00a8a, and A. Anttila, eds.\n1995. Constraint Grammar: A Language-Independent\nSystem for Parsing Unrestricted Text . Mouton de Gruyter.\nKarttunen, L. 1999. Comments on Joshi. In A. Kornai, ed.,\nExtended Finite State Models of Language , 16\u201318. Cam-\nbridge University Press.\nKlein, S. and R. F. Simmons. 1963. A computational ap-\nproach to grammatical coding of English words. Journal\nof the ACM , 10(3):334\u2013347.\nKupiec, J. 1992. Robust part-of-speech tagging using a hid-\nden Markov model. Computer Speech and Language ,\n6:225\u2013242.\nLafferty, J. D., A. McCallum, and F. C. N. Pereira. 2001.\nConditional random \ufb01elds: Probabilistic models for seg-\nmenting and labeling sequence data. ICML .\nLample, G., M. Ballesteros, S. Subramanian, K. Kawakami,\nand C. Dyer. 2016. Neural architectures for named entity\nrecognition. NAACL HLT .\nLee, H., M. Surdeanu, and D. Jurafsky. 2017. A scaffolding\napproach to coreference resolution integrating statistical\nand rule-based models. Natural Language Engineering ,\n23(5):733\u2013762.\nMa, X. and E. H. Hovy. 2016. End-to-end sequence labeling\nvia bi-directional LSTM-CNNs-CRF. ACL.\nManning, C. D. 2011. Part-of-speech tagging from 97% to\n100%: Is it time for some linguistics? CICLing 2011 .",
    "metadata": {
      "source": "17",
      "chunk_id": 41,
      "token_count": 755,
      "chapter_title": ""
    }
  },
  {
    "content": "of the ACM , 10(3):334\u2013347.\nKupiec, J. 1992. Robust part-of-speech tagging using a hid-\nden Markov model. Computer Speech and Language ,\n6:225\u2013242.\nLafferty, J. D., A. McCallum, and F. C. N. Pereira. 2001.\nConditional random \ufb01elds: Probabilistic models for seg-\nmenting and labeling sequence data. ICML .\nLample, G., M. Ballesteros, S. Subramanian, K. Kawakami,\nand C. Dyer. 2016. Neural architectures for named entity\nrecognition. NAACL HLT .\nLee, H., M. Surdeanu, and D. Jurafsky. 2017. A scaffolding\napproach to coreference resolution integrating statistical\nand rule-based models. Natural Language Engineering ,\n23(5):733\u2013762.\nMa, X. and E. H. Hovy. 2016. End-to-end sequence labeling\nvia bi-directional LSTM-CNNs-CRF. ACL.\nManning, C. D. 2011. Part-of-speech tagging from 97% to\n100%: Is it time for some linguistics? CICLing 2011 .\nMarcus, M. P., B. Santorini, and M. A. Marcinkiewicz. 1993.\nBuilding a large annotated corpus of English: The Penn\ntreebank. Computational Linguistics , 19(2):313\u2013330.\nde Marneffe, M.-C., C. D. Manning, J. Nivre, and D. Zeman.\n2021. Universal Dependencies. Computational Linguis-\ntics, 47(2):255\u2013308.\nMarshall, I. 1983. Choice of grammatical word-class with-\nout global syntactic analysis: Tagging words in the LOB\ncorpus. Computers and the Humanities , 17:139\u2013150.",
    "metadata": {
      "source": "17",
      "chunk_id": 42,
      "token_count": 424,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 27\n\nExercises 27\nMarshall, I. 1987. Tag selection using probabilistic meth-\nods. In R. Garside, G. Leech, and G. Sampson, eds, The\nComputational Analysis of English , 42\u201356. Longman.\nMcCallum, A., D. Freitag, and F. C. N. Pereira. 2000. Max-\nimum entropy Markov models for information extraction\nand segmentation. ICML .\nMcCallum, A. and W. Li. 2003. Early results for named\nentity recognition with conditional random \ufb01elds, feature\ninduction and web-enhanced lexicons. CoNLL .\nMerialdo, B. 1994. Tagging English text with a probabilistic\nmodel. Computational Linguistics , 20(2):155\u2013172.\nMikheev, A., M. Moens, and C. Grover. 1999. Named entity\nrecognition without gazetteers. EACL .\nOravecz, C. and P. Dienes. 2002. Ef\ufb01cient stochastic part-\nof-speech tagging for Hungarian. LREC .\nRamshaw, L. A. and M. P. Marcus. 1995. Text chunking\nusing transformation-based learning. Proceedings of the\n3rd Annual Workshop on Very Large Corpora .\nRatnaparkhi, A. 1996. A maximum entropy part-of-speech\ntagger. EMNLP .\nSampson, G. 1987. Alternative grammatical coding systems.\nIn R. Garside, G. Leech, and G. Sampson, eds, The Com-\nputational Analysis of English , 165\u2013183. Longman.\nSch\u00a8utze, H. and Y . Singer. 1994. Part-of-speech tagging us-\ning a variable memory Markov model. ACL.\nS\u00f8gaard, A. 2010. Simple semi-supervised training of part-\nof-speech taggers. ACL.\nStolz, W. S., P. H. Tannenbaum, and F. V . Carstensen. 1965.\nA stochastic approach to the grammatical coding of En-\nglish. CACM , 8(6):399\u2013405.\nThede, S. M. and M. P. Harper. 1999. A second-order hidden\nMarkov model for part-of-speech tagging. ACL.\nToutanova, K., D. Klein, C. D. Manning, and Y . Singer.\n2003. Feature-rich part-of-speech tagging with a cyclic\ndependency network. HLT-NAACL .\nV outilainen, A. 1999. Handcrafted rules. In H. van Halteren,\ned.,Syntactic Wordclass Tagging , 217\u2013246. Kluwer.\nWeischedel, R., M. Meteer, R. Schwartz, L. A. Ramshaw,\nand J. Palmucci. 1993. Coping with ambiguity and un-\nknown words through probabilistic models. Computa-\ntional Linguistics , 19(2):359\u2013382.\nWu, S. and M. Dredze. 2019. Beto, Bentz, Becas: The sur-\nprising cross-lingual effectiveness of BERT. EMNLP .\nZhou, G., J. Su, J. Zhang, and M. Zhang. 2005. Exploring\nvarious knowledge in relation extraction. ACL.",
    "metadata": {
      "source": "17",
      "chunk_id": 43,
      "token_count": 751,
      "chapter_title": ""
    }
  }
]