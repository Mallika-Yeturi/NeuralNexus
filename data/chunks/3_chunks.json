[
  {
    "content": "# 3\n\n## Page 1\n\nSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright \u00a92024. All\nrights reserved. Draft of January 12, 2025.\nCHAPTER\n3N-gram Language Models\n\u201cYou are uniformly charming!\u201d cried he, with a smile of associating and now\nand then I bowed and they perceived a chaise and four to wish for.\nRandom sentence generated from a Jane Austen trigram model\nPredicting is dif\ufb01cult\u2014especially about the future, as the old quip goes. But how\nabout predicting something that seems much easier, like the next word someone is\ngoing to say? What word, for example, is likely to follow\nThe water of Walden Pond is so beautifully ...\nYou might conclude that a likely word is blue , orgreen , orclear , but probably\nnotrefrigerator northis . In this chapter we formalize this intuition by intro-\nducing language models orLMs . A language model is a machine learning model language model\nLM that predicts upcoming words. More formally, a language model assigns a prob-\nability to each possible next word, or equivalently gives a probability distribution\nover possible next works. Language models can also assign a probability to an entire\nsentence. Thus an LM could tell us that the following sequence has a much higher\nprobability of appearing in a text:\nall of a sudden I notice three guys standing on the sidewalk\nthan does this same set of words in a different order:\non guys all I of notice sidewalk three a sudden standing the\nWhy would we want to predict upcoming words, or know the probability of a sen-\ntence? One reason is for generation: choosing contextually better words. For ex-\nample we can correct grammar or spelling errors like Their are two midterms ,\nin which There was mistyped as Their , orEverything has improve , in which\nimprove should have been improved . The phrase There are is more probable\nthanTheir are , and has improved thanhas improve , so a language model can\nhelp users select the more grammatical variant. Or for a speech system to recognize\nthat you said I will be back soonish and not I will be bassoon dish , it\nhelps to know that back soonish is a more probable sequence. Language models\ncan also help in augmentative and alternative communication (Trnka et al. 2007,\nKane et al. 2017). People can use AAC systems if they are physically unable to AAC\nspeak or sign but can instead use eye gaze or other movements to select words from\na menu. Word prediction can be used to suggest likely words for the menu.\nWord prediction is also central to NLP for another reason: large language mod-\nelsare built just by training them to predict words!! As we\u2019ll see in chapters 7-9,\nlarge language models learn an enormous amount about language solely from being\ntrained to predict upcoming words from neighboring words.\nIn this chapter we introduce the simplest kind of language model: the n-gram n-gram",
    "metadata": {
      "source": "3",
      "chunk_id": 0,
      "token_count": 634,
      "chapter_title": "3"
    }
  },
  {
    "content": "## Page 2",
    "metadata": {
      "source": "3",
      "chunk_id": 1,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "2CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\nlanguage model. An n-gram is a sequence of nwords: a 2-gram (which we\u2019ll call\nbigram ) is a two-word sequence of words like The water , orwater of , and a 3-\ngram (a trigram ) is a three-word sequence of words like The water of , orwater\nof Walden . But we also (in a bit of terminological ambiguity) use the word \u2018n-\ngram\u2019 to mean a probabilistic model that can estimate the probability of a word given\nthe n-1 previous words, and thereby also to assign probabilities to entire sequences.\nIn later chapters we will introduce the much more powerful neural large lan-\nguage models , based on the transformer architecture of Chapter 9. But because\nn-grams have a remarkably simple and clear formalization, we use them to intro-\nduce some major concepts of large language modeling, including training and test\nsets,perplexity ,sampling , and interpolation .\n3.1 N-Grams\nLet\u2019s begin with the task of computing P(wjh), the probability of a word wgiven\nsome history h. Suppose the history his \u201cThe water of Walden Pond is so\nbeautifully \u201d and we want to know the probability that the next word is blue :\nP(bluejThe water of Walden Pond is so beautifully ) (3.1)\nOne way to estimate this probability is directly from relative frequency counts: take a\nvery large corpus, count the number of times we see The water of Walden Pond\nis so beautifully , and count the number of times this is followed by blue . This\nwould be answering the question \u201cOut of the times we saw the history h, how many\ntimes was it followed by the word w\u201d, as follows:\nP(bluejThe water of Walden Pond is so beautifully ) =\nC(The water of Walden Pond is so beautifully blue )\nC(The water of Walden Pond is so beautifully )(3.2)\nIf we had a large enough corpus, we could compute these two counts and estimate\nthe probability from Eq. 3.2. But even the entire web isn\u2019t big enough to give us\ngood estimates for counts of entire sentences. This is because language is creative ;\nnew sentences are invented all the time, and we can\u2019t expect to get accurate counts\nfor such large objects as entire sentences. For this reason, we\u2019ll need more clever\nways to estimate the probability of a word wgiven a history h, or the probability of\nan entire word sequence W.\nLet\u2019s start with some notation. First, throughout this chapter we\u2019ll continue to\nrefer to words , although in practice we usually compute language models over to-\nkens like the BPE tokens of page ??. To represent the probability of a particular\nrandom variable Xitaking on the value \u201cthe\u201d, or P(Xi=\u201cthe\u201d), we will use the\nsimpli\ufb01cation P(the). We\u2019ll represent a sequence of nwords either as w1:::wnor\nw1:n. Thus the expression w1:n\u00001means the string w1;w2;:::;wn\u00001, but we\u2019ll also\nbe using the equivalent notation w<n, which can be read as \u201call the elements of w\nfrom w1up to and including wn\u00001\u201d. For the joint probability of each word in a se-\nquence having a particular value P(X1=w1;X2=w2;X3=w3;:::;Xn=wn)we\u2019ll\nuseP(w1;w2;:::;wn).\nNow, how can we compute probabilities of entire sequences like P(w1;w2;:::;wn)?\nOne thing we can do is decompose this probability using the chain rule of proba-",
    "metadata": {
      "source": "3",
      "chunk_id": 2,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 3\n\n3.1 \u2022 N-G RAMS 3\nbility :\nP(X1:::Xn) = P(X1)P(X2jX1)P(X3jX1:2):::P(XnjX1:n\u00001)\n=nY\nk=1P(XkjX1:k\u00001) (3.3)\nApplying the chain rule to words, we get\nP(w1:n) = P(w1)P(w2jw1)P(w3jw1:2):::P(wnjw1:n\u00001)\n=nY\nk=1P(wkjw1:k\u00001) (3.4)\nThe chain rule shows the link between computing the joint probability of a sequence\nand computing the conditional probability of a word given previous words. Equa-\ntion 3.4 suggests that we could estimate the joint probability of an entire sequence of\nwords by multiplying together a number of conditional probabilities. But using the\nchain rule doesn\u2019t really seem to help us! We don\u2019t know any way to compute the\nexact probability of a word given a long sequence of preceding words, P(wnjw1:n\u00001).\nAs we said above, we can\u2019t just estimate by counting the number of times every word\noccurs following every long string in some corpus, because language is creative and\nany particular context might have never occurred before!\n3.1.1 The Markov assumption\nThe intuition of the n-gram model is that instead of computing the probability of a\nword given its entire history, we can approximate the history by just the last few\nwords.\nThebigram model, for example, approximates the probability of a word given bigram\nall the previous words P(wnjw1:n\u00001)by using only the conditional probability given\nthe preceding word P(wnjwn\u00001). In other words, instead of computing the probabil-\nity\nP(bluejThe water of Walden Pond is so beautifully ) (3.5)\nwe approximate it with the probability\nP(bluejbeautifully ) (3.6)\nWhen we use a bigram model to predict the conditional probability of the next word,\nwe are thus making the following approximation:\nP(wnjw1:n\u00001)\u0019P(wnjwn\u00001) (3.7)\nThe assumption that the probability of a word depends only on the previous word is\ncalled a Markov assumption. Markov models are the class of probabilistic models Markov\nthat assume we can predict the probability of some future unit without looking too\nfar into the past. We can generalize the bigram (which looks one word into the past)\nto the trigram (which looks two words into the past) and thus to the n-gram (which n-gram\nlooks n\u00001 words into the past).\nLet\u2019s see a general equation for this n-gram approximation to the conditional\nprobability of the next word in a sequence. We\u2019ll use Nhere to mean the n-gram",
    "metadata": {
      "source": "3",
      "chunk_id": 3,
      "token_count": 635,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 4\n\n4CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\nsize, so N=2 means bigrams and N=3 means trigrams. Then we approximate the\nprobability of a word given its entire context as follows:\nP(wnjw1:n\u00001)\u0019P(wnjwn\u0000N+1:n\u00001) (3.8)\nGiven the bigram assumption for the probability of an individual word, we can com-\npute the probability of a complete word sequence by substituting Eq. 3.7 into Eq. 3.4:\nP(w1:n)\u0019nY\nk=1P(wkjwk\u00001) (3.9)\n3.1.2 How to estimate probabilities\nHow do we estimate these bigram or n-gram probabilities? An intuitive way to\nestimate probabilities is called maximum likelihood estimation orMLE . We getmaximum\nlikelihood\nestimationthe MLE estimate for the parameters of an n-gram model by getting counts from\na corpus, and normalizing the counts so that they lie between 0 and 1. For proba- normalize\nbilistic models, normalizing means dividing by some total count so that the resulting\nprobabilities fall between 0 and 1 and sum to 1.\nFor example, to compute a particular bigram probability of a word wngiven a\nprevious word wn\u00001, we\u2019ll compute the count of the bigram C(wn\u00001wn)and normal-\nize by the sum of all the bigrams that share the same \ufb01rst word wn\u00001:\nP(wnjwn\u00001) =C(wn\u00001wn)P\nwC(wn\u00001w)(3.10)\nWe can simplify this equation, since the sum of all bigram counts that start with\na given word wn\u00001must be equal to the unigram count for that word wn\u00001(the reader\nshould take a moment to be convinced of this):\nP(wnjwn\u00001) =C(wn\u00001wn)\nC(wn\u00001)(3.11)\nLet\u2019s work through an example using a mini-corpus of three sentences. We\u2019ll\n\ufb01rst need to augment each sentence with a special symbol <s>at the beginning\nof the sentence, to give us the bigram context of the \ufb01rst word. We\u2019ll also need a\nspecial end-symbol </s> .1\n<s> I am Sam </s>\n<s> Sam I am </s>\n<s> I do not like green eggs and ham </s>\nHere are the calculations for some of the bigram probabilities from this corpus\nP(I|<s> ) =2\n3=0:67 P(Sam|<s> ) =1\n3=0:33 P(am|I) =2\n3=0:67\nP(</s>|Sam ) =1\n2=0:5 P(Sam|am ) =1\n2=0:5 P(do|I) =1\n3=0:33\nFor the general case of MLE n-gram parameter estimation:\nP(wnjwn\u0000N+1:n\u00001) =C(wn\u0000N+1:n\u00001wn)\nC(wn\u0000N+1:n\u00001)(3.12)\n1We need the end-symbol to make the bigram grammar a true probability distribution. Without an end-\nsymbol, instead of the sentence probabilities of all sentences summing to one, the sentence probabilities\nfor all sentences of a given length would sum to one. This model would de\ufb01ne an in\ufb01nite set of probability\ndistributions, with one distribution per sentence length. See Exercise 3.5.",
    "metadata": {
      "source": "3",
      "chunk_id": 4,
      "token_count": 779,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 5",
    "metadata": {
      "source": "3",
      "chunk_id": 5,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "3.1 \u2022 N-G RAMS 5\nEquation 3.12 (like Eq. 3.11) estimates the n-gram probability by dividing the\nobserved frequency of a particular sequence by the observed frequency of a pre\ufb01x.\nThis ratio is called a relative frequency . We said above that this use of relativerelative\nfrequency\nfrequencies as a way to estimate probabilities is an example of maximum likelihood\nestimation or MLE. In MLE, the resulting parameter set maximizes the likelihood of\nthe training set Tgiven the model M(i.e., P(TjM)). For example, suppose the word\nChinese occurs 400 times in a corpus of a million words. What is the probability\nthat a random word selected from some other text of, say, a million words will be the\nword Chinese ? The MLE of its probability is400\n1000000or 0:0004. Now 0 :0004 is not\nthe best possible estimate of the probability of Chinese occurring in all situations; it\nmight turn out that in some other corpus or context Chinese is a very unlikely word.\nBut it is the probability that makes it most likely that Chinese will occur 400 times\nin a million-word corpus. We present ways to modify the MLE estimates slightly to\nget better probability estimates in Section 3.6.\nLet\u2019s move on to some examples from a real but tiny corpus, drawn from the\nnow-defunct Berkeley Restaurant Project, a dialogue system from the last century\nthat answered questions about a database of restaurants in Berkeley, California (Ju-\nrafsky et al., 1994). Here are some sample user queries (text-normalized, by lower\ncasing and with punctuation striped) (a sample of 9332 sentences is on the website):\ncan you tell me about any good cantonese restaurants close by\ntell me about chez panisse\ni\u2019m looking for a good place to eat breakfast\nwhen is caffe venezia open during the day\nFigure 3.1 shows the bigram counts from part of a bigram grammar from text-\nnormalized Berkeley Restaurant Project sentences. Note that the majority of the\nvalues are zero. In fact, we have chosen the sample words to cohere with each other;\na matrix selected from a random set of eight words would be even more sparse.\ni want to eat chinese food lunch spend\ni 5 827 0 9 0 0 0 2\nwant 2 0 608 1 6 6 5 1\nto 2 0 4 686 2 0 6 211\neat 0 0 2 0 16 2 42 0\nchinese 1 0 0 0 0 82 1 0\nfood 15 0 15 0 1 4 0 0\nlunch 2 0 0 0 0 1 0 0\nspend 1 0 1 0 0 0 0 0\nFigure 3.1 Bigram counts for eight of the words (out of V=1446) in the Berkeley Restau-\nrant Project corpus of 9332 sentences. Zero counts are in gray. Each cell shows the count of\nthe column label word following the row label word. Thus the cell in row iand column want\nmeans that want followed i827 times in the corpus.\nFigure 3.2 shows the bigram probabilities after normalization (dividing each cell\nin Fig. 3.1 by the appropriate unigram for its row, taken from the following set of\nunigram counts):\ni want to eat chinese food lunch spend\n2533 927 2417 746 158 1093 341 278",
    "metadata": {
      "source": "3",
      "chunk_id": 6,
      "token_count": 794,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 6",
    "metadata": {
      "source": "3",
      "chunk_id": 7,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "6CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\ni want to eat chinese food lunch spend\ni 0.002 0.33 0 0.0036 0 0 0 0.00079\nwant 0.0022 0 0.66 0.0011 0.0065 0.0065 0.0054 0.0011\nto 0.00083 0 0.0017 0.28 0.00083 0 0.0025 0.087\neat 0 0 0.0027 0 0.021 0.0027 0.056 0\nchinese 0.0063 0 0 0 0 0.52 0.0063 0\nfood 0.014 0 0.014 0 0.00092 0.0037 0 0\nlunch 0.0059 0 0 0 0 0.0029 0 0\nspend 0.0036 0 0.0036 0 0 0 0 0\nFigure 3.2 Bigram probabilities for eight words in the Berkeley Restaurant Project corpus\nof 9332 sentences. Zero probabilities are in gray.\nHere are a few other useful probabilities:\nP(i|<s> ) =0:25 P(english|want ) =0:0011\nP(food|english ) =0:5 P(</s>|food ) =0:68\nNow we can compute the probability of sentences like I want English food or\nI want Chinese food by simply multiplying the appropriate bigram probabilities to-\ngether, as follows:\nP(<s> i want english food </s> )\n=P(i|<s> )P(want|i )P(english|want )\nP(food|english )P(</s>|food )\n=0:25\u00020:33\u00020:0011\u00020:5\u00020:68\n=0:000031\nWe leave it as Exercise 3.2 to compute the probability of i want chinese food .\nWhat kinds of linguistic phenomena are captured in these bigram statistics?\nSome of the bigram probabilities above encode some facts that we think of as strictly\nsyntactic in nature, like the fact that what comes after eatis usually a noun or an\nadjective, or that what comes after tois usually a verb. Others might be a fact about\nthe personal assistant task, like the high probability of sentences beginning with\nthe words I. And some might even be cultural rather than linguistic, like the higher\nprobability that people are looking for Chinese versus English food.\n3.1.3 Dealing with scale in large n-gram models\nIn practice, language models can be very large, leading to practical issues.\nLog probabilities Language model probabilities are always stored and computed\nin log space as log probabilities . This is because probabilities are (by de\ufb01nition)log\nprobabilities\nless than or equal to 1, and so the more probabilities we multiply together, the\nsmaller the product becomes. Multiplying enough n-grams together would result\nin numerical under\ufb02ow. Adding in log space is equivalent to multiplying in linear\nspace, so we combine log probabilities by adding them. By adding log probabilities\ninstead of multiplying probabilities, we get results that are not as small. We do all\ncomputation and storage in log space, and just convert back into probabilities if we\nneed to report probabilities at the end by taking the exp of the logprob:",
    "metadata": {
      "source": "3",
      "chunk_id": 8,
      "token_count": 763,
      "chapter_title": ""
    }
  },
  {
    "content": "Some of the bigram probabilities above encode some facts that we think of as strictly\nsyntactic in nature, like the fact that what comes after eatis usually a noun or an\nadjective, or that what comes after tois usually a verb. Others might be a fact about\nthe personal assistant task, like the high probability of sentences beginning with\nthe words I. And some might even be cultural rather than linguistic, like the higher\nprobability that people are looking for Chinese versus English food.\n3.1.3 Dealing with scale in large n-gram models\nIn practice, language models can be very large, leading to practical issues.\nLog probabilities Language model probabilities are always stored and computed\nin log space as log probabilities . This is because probabilities are (by de\ufb01nition)log\nprobabilities\nless than or equal to 1, and so the more probabilities we multiply together, the\nsmaller the product becomes. Multiplying enough n-grams together would result\nin numerical under\ufb02ow. Adding in log space is equivalent to multiplying in linear\nspace, so we combine log probabilities by adding them. By adding log probabilities\ninstead of multiplying probabilities, we get results that are not as small. We do all\ncomputation and storage in log space, and just convert back into probabilities if we\nneed to report probabilities at the end by taking the exp of the logprob:\np1\u0002p2\u0002p3\u0002p4=exp(logp1+logp2+logp3+logp4) (3.13)\nIn practice throughout this book, we\u2019ll use log to mean natural log (ln) when the\nbase is not speci\ufb01ed.",
    "metadata": {
      "source": "3",
      "chunk_id": 9,
      "token_count": 353,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 7",
    "metadata": {
      "source": "3",
      "chunk_id": 10,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "3.2 \u2022 E VALUATING LANGUAGE MODELS : TRAINING AND TESTSETS 7\nLonger context Although for pedagogical purposes we have only described bi-\ngram models, when there is suf\ufb01cient training data we use trigram models, which trigram\ncondition on the previous two words, or 4-gram or5-gram models. For these larger 4-gram\n5-gram n-grams, we\u2019ll need to assume extra contexts to the left and right of the sentence end.\nFor example, to compute trigram probabilities at the very beginning of the sentence,\nwe use two pseudo-words for the \ufb01rst trigram (i.e., P(I|<s><s> ).\nSome large n-gram datasets have been created, like the million most frequent\nn-grams drawn from the Corpus of Contemporary American English (COCA), a\ncurated 1 billion word corpus of American English (Davies, 2020), Google\u2019s Web\n5-gram corpus from 1 trillion words of English web text (Franz and Brants, 2006),\nor the Google Books Ngrams corpora (800 billion tokens from Chinese, English,\nFrench, German, Hebrew, Italian, Russian, and Spanish) (Lin et al., 2012)).\nIt\u2019s even possible to use extremely long-range n-gram context. The in\ufb01ni-gram\n(\u00a5-gram) project (Liu et al., 2024) allows n-grams of any length. Their idea is to\navoid the expensive (in space and time) pre-computation of huge n-gram count ta-\nbles. Instead, n-gram probabilities with arbitrary n are computed quickly at inference\ntime by using an ef\ufb01cient representation called suf\ufb01x arrays. This allows computing\nof n-grams of every length for enormous corpora of 5 trillion tokens.\nEf\ufb01ciency considerations are important when building large n-gram language\nmodels. It is standard to quantize the probabilities using only 4-8 bits (instead of\n8-byte \ufb02oats), store the word strings on disk and represent them in memory only as\na 64-bit hash, and represent n-grams in special data structures like \u2018reverse tries\u2019.\nIt is also common to prune n-gram language models, for example by only keeping\nn-grams with counts greater than some threshold or using entropy to prune less-\nimportant n-grams (Stolcke, 1998). Ef\ufb01cient language model toolkits like KenLM\n(Hea\ufb01eld 2011, Hea\ufb01eld et al. 2013) use sorted arrays and use merge sorts to ef\ufb01-\nciently build the probability tables in a minimal number of passes through a large\ncorpus.\n3.2 Evaluating Language Models: Training and Test Sets\nThe best way to evaluate the performance of a language model is to embed it in\nan application and measure how much the application improves. Such end-to-end\nevaluation is called extrinsic evaluation . Extrinsic evaluation is the only way toextrinsic\nevaluation\nknow if a particular improvement in the language model (or any component) is really\ngoing to help the task at hand. Thus for evaluating n-gram language models that are\na component of some task like speech recognition or machine translation, we can\ncompare the performance of two candidate language models by running the speech\nrecognizer or machine translator twice, once with each language model, and seeing\nwhich gives the more accurate transcription.\nUnfortunately, running big NLP systems end-to-end is often very expensive. In-\nstead, it\u2019s helpful to have a metric that can be used to quickly evaluate potential",
    "metadata": {
      "source": "3",
      "chunk_id": 11,
      "token_count": 771,
      "chapter_title": ""
    }
  },
  {
    "content": "n-grams with counts greater than some threshold or using entropy to prune less-\nimportant n-grams (Stolcke, 1998). Ef\ufb01cient language model toolkits like KenLM\n(Hea\ufb01eld 2011, Hea\ufb01eld et al. 2013) use sorted arrays and use merge sorts to ef\ufb01-\nciently build the probability tables in a minimal number of passes through a large\ncorpus.\n3.2 Evaluating Language Models: Training and Test Sets\nThe best way to evaluate the performance of a language model is to embed it in\nan application and measure how much the application improves. Such end-to-end\nevaluation is called extrinsic evaluation . Extrinsic evaluation is the only way toextrinsic\nevaluation\nknow if a particular improvement in the language model (or any component) is really\ngoing to help the task at hand. Thus for evaluating n-gram language models that are\na component of some task like speech recognition or machine translation, we can\ncompare the performance of two candidate language models by running the speech\nrecognizer or machine translator twice, once with each language model, and seeing\nwhich gives the more accurate transcription.\nUnfortunately, running big NLP systems end-to-end is often very expensive. In-\nstead, it\u2019s helpful to have a metric that can be used to quickly evaluate potential\nimprovements in a language model. An intrinsic evaluation metric is one that mea-intrinsic\nevaluation\nsures the quality of a model independent of any application. In the next section we\u2019ll\nintroduce perplexity , which is the standard intrinsic metric for measuring language\nmodel performance, both for simple n-gram language models and for the more so-\nphisticated neural large language models of Chapter 9.\nIn order to evaluate any machine learning model, we need to have at least three\ndistinct data sets: the training set , the development set , and the test set . training set\ndevelopment\nset\ntest set",
    "metadata": {
      "source": "3",
      "chunk_id": 12,
      "token_count": 409,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 8",
    "metadata": {
      "source": "3",
      "chunk_id": 13,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "8CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\nThe training set is the data we use to learn the parameters of our model; for\nsimple n-gram language models it\u2019s the corpus from which we get the counts that\nwe normalize into the probabilities of the n-gram language model.\nThetest set is a different, held-out set of data, not overlapping with the training\nset, that we use to evaluate the model. We need a separate test set to give us an\nunbiased estimate of how well the model we trained can generalize when we apply\nit to some new unknown dataset. A machine learning model that perfectly captured\nthe training data, but performed terribly on any other data, wouldn\u2019t be much use\nwhen it comes time to apply it to any new data or problem! We thus measure the\nquality of an n-gram model by its performance on this unseen test set or test corpus.\nHow should we choose a training and test set? The test set should re\ufb02ect the\nlanguage we want to use the model for. If we\u2019re going to use our language model\nfor speech recognition of chemistry lectures, the test set should be text of chemistry\nlectures. If we\u2019re going to use it as part of a system for translating hotel booking re-\nquests from Chinese to English, the test set should be text of hotel booking requests.\nIf we want our language model to be general purpose, then the test set should be\ndrawn from a wide variety of texts. In such cases we might collect a lot of texts\nfrom different sources, and then divide it up into a training set and a test set. It\u2019s\nimportant to do the dividing carefully; if we\u2019re building a general purpose model,\nwe don\u2019t want the test set to consist of only text from one document, or one author,\nsince that wouldn\u2019t be a good measure of general performance.\nThus if we are given a corpus of text and want to compare the performance of\ntwo different n-gram models, we divide the data into training and test sets, and train\nthe parameters of both models on the training set. We can then compare how well\nthe two trained models \ufb01t the test set.\nBut what does it mean to \u201c\ufb01t the test set\u201d? The standard answer is simple:\nwhichever language model assigns a higher probability to the test set\u2014which\nmeans it more accurately predicts the test set\u2014is a better model. Given two proba-\nbilistic models, the better model is the one that better predicts the details of the test\ndata, and hence will assign a higher probability to the test data.\nSince our evaluation metric is based on test set probability, it\u2019s important not to\nlet the test sentences into the training set. Suppose we are trying to compute the\nprobability of a particular \u201ctest\u201d sentence. If our test sentence is part of the training\ncorpus, we will mistakenly assign it an arti\ufb01cially high probability when it occurs\nin the test set. We call this situation training on the test set . Training on the test\nset introduces a bias that makes the probabilities all look too high, and causes huge\ninaccuracies in perplexity , the probability-based metric we introduce below.\nEven if we don\u2019t train on the test set, if we test our language model on the\ntest set many times after making different changes, we might implicitly tune to its\ncharacteristics, by noticing which changes seem to make the model better. For this\nreason, we only want to run our model on the test set once, or a very few number of\ntimes, once we are sure our model is ready.\nFor this reason we normally instead have a third dataset called a developmentdevelopment\ntest",
    "metadata": {
      "source": "3",
      "chunk_id": 14,
      "token_count": 768,
      "chapter_title": ""
    }
  },
  {
    "content": "whichever language model assigns a higher probability to the test set\u2014which\nmeans it more accurately predicts the test set\u2014is a better model. Given two proba-\nbilistic models, the better model is the one that better predicts the details of the test\ndata, and hence will assign a higher probability to the test data.\nSince our evaluation metric is based on test set probability, it\u2019s important not to\nlet the test sentences into the training set. Suppose we are trying to compute the\nprobability of a particular \u201ctest\u201d sentence. If our test sentence is part of the training\ncorpus, we will mistakenly assign it an arti\ufb01cially high probability when it occurs\nin the test set. We call this situation training on the test set . Training on the test\nset introduces a bias that makes the probabilities all look too high, and causes huge\ninaccuracies in perplexity , the probability-based metric we introduce below.\nEven if we don\u2019t train on the test set, if we test our language model on the\ntest set many times after making different changes, we might implicitly tune to its\ncharacteristics, by noticing which changes seem to make the model better. For this\nreason, we only want to run our model on the test set once, or a very few number of\ntimes, once we are sure our model is ready.\nFor this reason we normally instead have a third dataset called a developmentdevelopment\ntest\ntest set or, devset . We do all our testing on this dataset until the very end, and then\nwe test on the test set once to see how good our model is.\nHow do we divide our data into training, development, and test sets? We want\nour test set to be as large as possible, since a small test set may be accidentally un-\nrepresentative, but we also want as much training data as possible. At the minimum,\nwe would want to pick the smallest test set that gives us enough statistical power\nto measure a statistically signi\ufb01cant difference between two potential models. It\u2019s",
    "metadata": {
      "source": "3",
      "chunk_id": 15,
      "token_count": 421,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 9\n\n3.3 \u2022 E VALUATING LANGUAGE MODELS : PERPLEXITY 9\nimportant that the devset be drawn from the same kind of text as the test set, since\nits goal is to measure how we would do on the test set.\n3.3 Evaluating Language Models: Perplexity\nWe said above that we evaluate language models based on which one assigns a\nhigher probability to the test set. A better model is better at predicting upcoming\nwords, and so it will be less surprised by (i.e., assign a higher probability to) each\nword when it occurs in the test set. Indeed, a perfect language model would correctly\nguess each next word in a corpus, assigning it a probability of 1, and all the other\nwords a probability of zero. So given a test corpus, a better language model will\nassign a higher probability to it than a worse language model.\nBut in fact, we do not use raw probability as our metric for evaluating language\nmodels. The reason is that the probability of a test set (or any sequence) depends\non the number of words or tokens in it; the probability of a test set gets smaller the\nlonger the text. We\u2019d prefer a metric that is per-word, normalized by length, so we\ncould compare across texts of different lengths. The metric we use is, a function of\nprobability called perplexity , is one of the most important metrics in NLP, used for\nevaluating large language models as well as n-gram models.\nTheperplexity (sometimes abbreviated as PP or PPL) of a language model on a perplexity\ntest set is the inverse probability of the test set (one over the probability of the test\nset), normalized by the number of words (or tokens). For this reason it\u2019s sometimes\ncalled the per-word or per-token perplexity. We normalize by the number of words\nNby taking the Nth root. For a test set W=w1w2:::wN,:\nperplexity (W) = P(w1w2:::wN)\u00001\nN (3.14)\n=Ns\n1\nP(w1w2:::wN)\nOr we can use the chain rule to expand the probability of W:\nperplexity (W) =NvuutNY\ni=11\nP(wijw1:::wi\u00001)(3.15)\nNote that because of the inverse in Eq. 3.15, the higher the probability of the word\nsequence, the lower the perplexity. Thus the the lower the perplexity of a model on\nthe data, the better the model . Minimizing perplexity is equivalent to maximizing\nthe test set probability according to the language model. Why does perplexity use\nthe inverse probability? It turns out the inverse arises from the original de\ufb01nition\nof perplexity from cross-entropy rate in information theory; for those interested, the\nexplanation is in the advanced Section 3.7. Meanwhile, we just have to remember\nthat perplexity has an inverse relationship with probability.\nThe details of computing the perplexity of a test set Wdepends on which lan-\nguage model we use. Here\u2019s the perplexity of Wwith a unigram language model\n(just the geometric mean of the inverse of the unigram probabilities):\nperplexity (W) =NvuutNY\ni=11\nP(wi)(3.16)",
    "metadata": {
      "source": "3",
      "chunk_id": 16,
      "token_count": 724,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 10\n\n10 CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\nThe perplexity of Wcomputed with a bigram language model is still a geometric\nmean, but now of the inverse of the bigram probabilities:\nperplexity (W) =NvuutNY\ni=11\nP(wijwi\u00001)(3.17)\nWhat we generally use for word sequence in Eq. 3.15 or Eq. 3.17 is the entire\nsequence of words in some test set. Since this sequence will cross many sentence\nboundaries, if our vocabulary includes a between-sentence token <EOS> or separate\nbegin- and end-sentence markers <s> and</s> then we can include them in the\nprobability computation. If we do, then we also include one token per sentence in\nthe total count of word tokens N.2\nWe mentioned above that perplexity is a function of both the text and the lan-\nguage model: given a text W, different language models will have different perplex-\nities. Because of this, perplexity can be used to compare different language models.\nFor example, here we trained unigram, bigram, and trigram grammars on 38 million\nwords from the Wall Street Journal newspaper. We then computed the perplexity of\neach of these models on a WSJ test set using Eq. 3.16 for unigrams, Eq. 3.17 for\nbigrams, and the corresponding equation for trigrams. The table below shows the\nperplexity of the 1.5 million word test set according to each of the language models.\nUnigram Bigram Trigram\nPerplexity 962 170 109\nAs we see above, the more information the n-gram gives us about the word\nsequence, the higher the probability the n-gram will assign to the string. A trigram\nmodel is less surprised than a unigram model because it has a better idea of what\nwords might come next, and so it assigns them a higher probability. And the higher\nthe probability, the lower the perplexity (since as Eq. 3.15 showed, perplexity is\nrelated inversely to the probability of the test sequence according to the model). So\na lower perplexity tells us that a language model is a better predictor of the test set.\nNote that in computing perplexities, the language model must be constructed\nwithout any knowledge of the test set, or else the perplexity will be arti\ufb01cially low.\nAnd the perplexity of two language models is only comparable if they use identical\nvocabularies.\nAn (intrinsic) improvement in perplexity does not guarantee an (extrinsic) im-\nprovement in the performance of a language processing task like speech recognition\nor machine translation. Nonetheless, because perplexity usually correlates with task\nimprovements, it is commonly used as a convenient evaluation metric. Still, when\npossible a model\u2019s improvement in perplexity should be con\ufb01rmed by an end-to-end\nevaluation on a real task.\n3.3.1 Perplexity as Weighted Average Branching Factor\nIt turns out that perplexity can also be thought of as the weighted average branch-\ning factor of a language. The branching factor of a language is the number of\npossible next words that can follow any word. For example consider a mini arti\ufb01cial\n2For example if we use both begin and end tokens, we would include the end-of-sentence marker </s>\nbut not the beginning-of-sentence marker <s>in our count of N; This is because the end-sentence token is\nfollowed directly by the begin-sentence token with probability almost 1, so we don\u2019t want the probability\nof that fake transition to in\ufb02uence our perplexity.",
    "metadata": {
      "source": "3",
      "chunk_id": 17,
      "token_count": 797,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 11\n\n3.4 \u2022 S AMPLING SENTENCES FROM A LANGUAGE MODEL 11\nlanguage that is deterministic (no probabilities), any word can follow any word, and\nwhose vocabulary consists of only three colors:\nL=fred;blue ;greeng (3.18)\nThe branching factor of this language is 3.\nNow let\u2019s make a probabilistic version of the same LM, let\u2019s call it A, where each\nword follows each other with equal probability1\n3(it was trained on a training set with\nequal counts for the 3 colors), and a test set T= \u201cred red red red blue \u201d.\nLet\u2019s \ufb01rst convince ourselves that if we compute the perplexity of this arti\ufb01cial\ndigit language on this test set (or any such test set) we indeed get 3. By Eq. 3.15, the\nperplexity of AonTis:\nperplexityA(T) = PA(red red red red blue )\u00001\n5\n= \u00121\n3\u00135!\u00001\n5\n=\u00121\n3\u0013\u00001\n=3 (3.19)\nBut now suppose redwas very likely in the training set a different LM B, and so B\nhas the following probabilities:\nP(red) =0:8P(green ) =0:1P(blue) =0:1 (3.20)\nWe should expect the perplexity of the same test set red red red red blue for\nlanguage model Bto be lower since most of the time the next color will be red, which\nis very predictable, i.e. has a high probability. So the probability of the test set will\nbe higher, and since perplexity is inversely related to probability, the perplexity will\nbe lower. Thus, although the branching factor is still 3, the perplexity or weighted\nbranching factor is smaller:\nperplexityB(T) = PB(red red red red blue )\u00001=5\n=0:04096\u00001\n5\n=0:527\u00001=1:89 (3.21)\n3.4 Sampling sentences from a language model\nOne important way to visualize what kind of knowledge a language model embodies\nis to sample from it. Sampling from a distribution means to choose random points sampling\naccording to their likelihood. Thus sampling from a language model\u2014which rep-\nresents a distribution over sentences\u2014means to generate some sentences, choosing\neach sentence according to its likelihood as de\ufb01ned by the model. Thus we are more\nlikely to generate sentences that the model thinks have a high probability and less\nlikely to generate sentences that the model thinks have a low probability.\nThis technique of visualizing a language model by sampling was \ufb01rst suggested\nvery early on by Shannon (1948) and Miller and Selfridge (1950). It\u2019s simplest to\nvisualize how this works for the unigram case. Imagine all the words of the English\nlanguage covering the number line between 0 and 1, each word covering an interval",
    "metadata": {
      "source": "3",
      "chunk_id": 18,
      "token_count": 635,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 12\n\n12 CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\n010.06the.060.03of0.02a0.02toin.09.11.13.15\u2026however(p=0.0003)polyphonicp=0.0000018\u20260.02.66.99\u2026\nFigure 3.3 A visualization of the sampling distribution for sampling sentences by repeat-\nedly sampling unigrams. The blue bar represents the relative frequency of each word (we\u2019ve\nordered them from most frequent to least frequent, but the choice of order is arbitrary). The\nnumber line shows the cumulative probabilities. If we choose a random number between 0\nand 1, it will fall in an interval corresponding to some word. The expectation for the random\nnumber to fall in the larger intervals of one of the frequent words ( the,of,a) is much higher\nthan in the smaller interval of one of the rare words ( polyphonic ).\nproportional to its frequency. Fig. 3.3 shows a visualization, using a unigram LM\ncomputed from the text of this book. We choose a random value between 0 and 1,\n\ufb01nd that point on the probability line, and print the word whose interval includes this\nchosen value. We continue choosing random numbers and generating words until\nwe randomly generate the sentence-\ufb01nal token </s> .\nWe can use the same technique to generate bigrams by \ufb01rst generating a ran-\ndom bigram that starts with <s>(according to its bigram probability). Let\u2019s say the\nsecond word of that bigram is w. We next choose a random bigram starting with w\n(again, drawn according to its bigram probability), and so on.\n3.5 Generalizing vs. over\ufb01tting the training set\nThe n-gram model, like many statistical models, is dependent on the training corpus.\nOne implication of this is that the probabilities often encode speci\ufb01c facts about a\ngiven training corpus. Another implication is that n-grams do a better and better job\nof modeling the training corpus as we increase the value of N.\nWe can use the sampling method from the prior section to visualize both of\nthese facts! To give an intuition for the increasing power of higher-order n-grams,\nFig. 3.4 shows random sentences generated from unigram, bigram, trigram, and 4-\ngram models trained on Shakespeare\u2019s works.\nThe longer the context, the more coherent the sentences. The unigram sen-\ntences show no coherent relation between words nor any sentence-\ufb01nal punctua-\ntion. The bigram sentences have some local word-to-word coherence (especially\nconsidering punctuation as words). The trigram sentences are beginning to look a\nlot like Shakespeare. Indeed, the 4-gram sentences look a little too much like Shake-\nspeare. The words It cannot be but so are directly from King John . This is because,\nnot to put the knock on Shakespeare, his oeuvre is not very large as corpora go\n(N=884;647;V=29;066), and our n-gram probability matrices are ridiculously\nsparse. There are V2=844;000;000 possible bigrams alone, and the number of\npossible 4-grams is V4=7\u00021017. Thus, once the generator has chosen the \ufb01rst\n3-gram ( It cannot be ), there are only seven possible next words for the 4th element\n(but,I,that,thus,this, and the period).\nTo get an idea of the dependence on the training set, let\u2019s look at LMs trained on a\ncompletely different corpus: the Wall Street Journal (WSJ) newspaper. Shakespeare",
    "metadata": {
      "source": "3",
      "chunk_id": 19,
      "token_count": 785,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 13\n\n3.5 \u2022 G ENERALIZING VS .OVERFITTING THE TRAINING SET 13\n1\u2013To him swallowed confess hear both. Which. Of save on trail for are ay device and\nrote life have\ngram \u2013Hill he late speaks; or! a more to leg less \ufb01rst you enter\n2\u2013Why dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live\nking. Follow.\ngram \u2013What means, sir. I confess she? then all sorts, he is trim, captain.\n3\u2013Fly, and will rid me these news of price. Therefore the sadness of parting, as they say,\n\u2019tis done.\ngram \u2013This shall forbid it should be branded, if renown made it empty.\n4\u2013King Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. A\ngreat banquet serv\u2019d in;\ngram \u2013It cannot be but so.\nFigure 3.4 Eight sentences randomly generated from four n-grams computed from Shakespeare\u2019s works. All\ncharacters were mapped to lower-case and punctuation marks were treated as words. Output is hand-corrected\nfor capitalization to improve readability.\nand the WSJ are both English, so we might have expected some overlap between our\nn-grams for the two genres. Fig. 3.5 shows sentences generated by unigram, bigram,\nand trigram grammars trained on 40 million words from WSJ.\n1Months the my and issue of year foreign new exchange\u2019s september\nwere recession exchange new endorsed a acquire to six executivesgram\n2Last December through the way to preserve the Hudson corporation N.\nB. E. C. Taylor would seem to complete the major central planners one\ngram point \ufb01ve percent of U. S. E. has already old M. X. corporation of living\non information such as more frequently \ufb01shing to keep her\n3They also point to ninety nine point six billion dollars from two hundred\nfour oh six three percent of the rates of interest stores as Mexico and\ngram Brazil on market conditions\nFigure 3.5 Three sentences randomly generated from three n-gram models computed from\n40 million words of the Wall Street Journal , lower-casing all characters and treating punctua-\ntion as words. Output was then hand-corrected for capitalization to improve readability.\nCompare these examples to the pseudo-Shakespeare in Fig. 3.4. While they both\nmodel \u201cEnglish-like sentences\u201d, there is no overlap in the generated sentences, and\nlittle overlap even in small phrases. Statistical models are pretty useless as predictors\nif the training sets and the test sets are as different as Shakespeare and the WSJ.\nHow should we deal with this problem when we build n-gram models? One step\nis to be sure to use a training corpus that has a similar genre to whatever task we are\ntrying to accomplish. To build a language model for translating legal documents,\nwe need a training corpus of legal documents. To build a language model for a\nquestion-answering system, we need a training corpus of questions.\nIt is equally important to get training data in the appropriate dialect orvariety ,\nespecially when processing social media posts or spoken transcripts. For exam-\nple some tweets will use features of African American English (AAE)\u2014 the name\nfor the many variations of language used in African American communities (King,\n2020). Such features can include words like \ufb01nna \u2014an auxiliary verb that marks\nimmediate future tense \u2014that don\u2019t occur in other varieties, or spellings like denfor\nthen, in tweets like this one (Blodgett and O\u2019Connor, 2017):",
    "metadata": {
      "source": "3",
      "chunk_id": 20,
      "token_count": 773,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 14\n\n14 CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\n(3.22) Bored af den my phone \ufb01nna die!!!\nwhile tweets from English-based languages like Nigerian Pidgin have markedly dif-\nferent vocabulary and n-gram patterns from American English (Jurgens et al., 2017):\n(3.23) @username R u a wizard or wat gan sef: in d mornin - u tweet, afternoon - u\ntweet, nyt gan u dey tweet. beta get ur IT placement wiv twitter\nIs it possible for the testset nonetheless to have a word we have never seen be-\nfore? What happens if the word Jurafsky never occurs in our training set, but pops up\nin the test set? The answer is that although words might be unseen, we normally run\nour NLP algorithms not on words but on subword tokens . With subword tokeniza-\ntion (like the BPE algorithm of Chapter 2) any word can be modeled as a sequence\nof known smaller subwords, if necessary by a sequence of tokens corresponding to\nindividual letters. So although for convenience we\u2019ve been referring to words in\nthis chapter, the language model vocabulary is normally the set of tokens rather than\nwords, and in this way the test set can never contain unseen tokens.\n3.6 Smoothing, Interpolation, and Backoff\nThere is a problem with using maximum likelihood estimates for probabilities: any\n\ufb01nite training corpus will be missing some perfectly acceptable English word se-\nquences. That is, cases where a particular n-gram never occurs in the training data\nbut appears in the test set. Perhaps our training corpus has the words ruby and\nslippers in it but just happens not to have the phrase ruby slippers .\nThese unseen sequences or zeros \u2014sequences that don\u2019t occur in the training set zeros\nbut do occur in the test set\u2014are a problem for two reasons. First, their presence\nmeans we are underestimating the probability of word sequences that might occur,\nwhich hurts the performance of any application we want to run on this data. Second,\nif the probability of any word in the test set is 0, the probability of the whole test\nset is 0. Perplexity is de\ufb01ned based on the inverse probability of the test set. Thus\nif some words in context have zero probability, we can\u2019t compute perplexity at all,\nsince we can\u2019t divide by 0!\nThe standard way to deal with putative \u201czero probability n-grams\u201d that should re-\nally have some non-zero probability is called smoothing ordiscounting . Smoothing smoothing\ndiscounting algorithms shave off a bit of probability mass from some more frequent events and\ngive it to unseen events. Here we\u2019ll introduce some simple smoothing algorithms:\nLaplace (add-one) smoothing ,stupid backoff , and n-gram interpolation .\n3.6.1 Laplace Smoothing\nThe simplest way to do smoothing is to add one to all the n-gram counts, before\nwe normalize them into probabilities. All the counts that used to be zero will now\nhave a count of 1, the counts of 1 will be 2, and so on. This algorithm is called\nLaplace smoothing . Laplace smoothing does not perform well enough to be usedLaplace\nsmoothing\nin modern n-gram models, but it usefully introduces many of the concepts that we\nsee in other smoothing algorithms, gives a useful baseline, and is also a practical\nsmoothing algorithm for other tasks like text classi\ufb01cation (Chapter 4).\nLet\u2019s start with the application of Laplace smoothing to unigram probabilities.\nRecall that the unsmoothed maximum likelihood estimate of the unigram probability",
    "metadata": {
      "source": "3",
      "chunk_id": 21,
      "token_count": 784,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 15",
    "metadata": {
      "source": "3",
      "chunk_id": 22,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "3.6 \u2022 S MOOTHING , INTERPOLATION ,AND BACKOFF 15\nof the word wiis its count cinormalized by the total number of word tokens N:\nP(wi) =ci\nN\nLaplace smoothing merely adds one to each count (hence its alternate name add-\nonesmoothing). Since there are Vwords in the vocabulary and each one was in- add-one\ncremented, we also need to adjust the denominator to take into account the extra V\nobservations. (What happens to our Pvalues if we don\u2019t increase the denominator?)\nPLaplace (wi) =ci+1\nN+V(3.24)\nNow that we have the intuition for the unigram case, let\u2019s smooth our Berkeley\nRestaurant Project bigrams. Figure 3.6 shows the add-one smoothed counts for the\nbigrams in Fig. 3.1.\ni want to eat chinese food lunch spend\ni 6 828 1 10 1 1 1 3\nwant 3 1 609 2 7 7 6 2\nto 3 1 5 687 3 1 7 212\neat 1 1 3 1 17 3 43 1\nchinese 2 1 1 1 1 83 2 1\nfood 16 1 16 1 2 5 1 1\nlunch 3 1 1 1 1 2 1 1\nspend 2 1 2 1 1 1 1 1\nFigure 3.6 Add-one smoothed bigram counts for eight of the words (out of V=1446) in\nthe Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero counts are in gray.\nFigure 3.7 shows the add-one smoothed probabilities for the bigrams in Fig. 3.2,\ncomputed by Eq. 3.26 below. Recall that normal bigram probabilities are computed\nby normalizing each row of counts by the unigram count:\nPMLE(wnjwn\u00001) =C(wn\u00001wn)\nC(wn\u00001)(3.25)\nFor add-one smoothed bigram counts, we need to augment the unigram count in the\ndenominator by the number of total word types in the vocabulary V. We can see\nwhy this is in the following equation, which makes it explicit that the unigram count\nin the denominator is really the sum over all the bigrams that start with wn\u00001. Since\nwe add one to each of these, and there are Vof them, we add a total of Vto the\ndenominator:\nPLaplace (wnjwn\u00001) =C(wn\u00001wn)+1P\nw(C(wn\u00001w)+1)=C(wn\u00001wn)+1\nC(wn\u00001)+V(3.26)\nThus, each of the unigram counts given on page 5 will need to be augmented by V=\n1446. The result, using Eq. 3.26, is the smoothed bigram probabilities in Fig. 3.7.\nOne useful visualization technique is to reconstruct an adjusted count matrix\nso we can see how much a smoothing algorithm has changed the original counts.\nThis adjusted count C\u0003is the count that, if divided by C(wn\u00001), would result in\nthe smoothed probability. This adjusted count is easier to compare directly with\nthe MLE counts. That is, the Laplace probability can equally be expressed as the\nadjusted count divided by the (non-smoothed) denominator from Eq. 3.25:",
    "metadata": {
      "source": "3",
      "chunk_id": 23,
      "token_count": 776,
      "chapter_title": ""
    }
  },
  {
    "content": "denominator by the number of total word types in the vocabulary V. We can see\nwhy this is in the following equation, which makes it explicit that the unigram count\nin the denominator is really the sum over all the bigrams that start with wn\u00001. Since\nwe add one to each of these, and there are Vof them, we add a total of Vto the\ndenominator:\nPLaplace (wnjwn\u00001) =C(wn\u00001wn)+1P\nw(C(wn\u00001w)+1)=C(wn\u00001wn)+1\nC(wn\u00001)+V(3.26)\nThus, each of the unigram counts given on page 5 will need to be augmented by V=\n1446. The result, using Eq. 3.26, is the smoothed bigram probabilities in Fig. 3.7.\nOne useful visualization technique is to reconstruct an adjusted count matrix\nso we can see how much a smoothing algorithm has changed the original counts.\nThis adjusted count C\u0003is the count that, if divided by C(wn\u00001), would result in\nthe smoothed probability. This adjusted count is easier to compare directly with\nthe MLE counts. That is, the Laplace probability can equally be expressed as the\nadjusted count divided by the (non-smoothed) denominator from Eq. 3.25:\nPLaplace (wnjwn\u00001) =C(wn\u00001wn)+1\nC(wn\u00001)+V=C\u0003(wn\u00001wn)\nC(wn\u00001)",
    "metadata": {
      "source": "3",
      "chunk_id": 24,
      "token_count": 329,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 16",
    "metadata": {
      "source": "3",
      "chunk_id": 25,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "16 CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\ni want to eat chinese food lunch spend\ni 0.0015 0.21 0.00025 0.0025 0.00025 0.00025 0.00025 0.00075\nwant 0.0013 0.00042 0.26 0.00084 0.0029 0.0029 0.0025 0.00084\nto 0.00078 0.00026 0.0013 0.18 0.00078 0.00026 0.0018 0.055\neat 0.00046 0.00046 0.0014 0.00046 0.0078 0.0014 0.02 0.00046\nchinese 0.0012 0.00062 0.00062 0.00062 0.00062 0.052 0.0012 0.00062\nfood 0.0063 0.00039 0.0063 0.00039 0.00079 0.002 0.00039 0.00039\nlunch 0.0017 0.00056 0.00056 0.00056 0.00056 0.0011 0.00056 0.00056\nspend 0.0012 0.00058 0.0012 0.00058 0.00058 0.00058 0.00058 0.00058\nFigure 3.7 Add-one smoothed bigram probabilities for eight of the words (out of V=1446) in the BeRP\ncorpus of 9332 sentences computed by Eq. 3.26. Previously-zero probabilities are in gray.\nRearranging terms, we can solve for C\u0003(wn\u00001wn):\nC\u0003(wn\u00001wn) =[C(wn\u00001wn)+1]\u0002C(wn\u00001)\nC(wn\u00001)+V(3.27)\nFigure 3.8 shows the reconstructed counts, computed by Eq. 3.27.\ni want to eat chinese food lunch spend\ni 3.8 527 0.64 6.4 0.64 0.64 0.64 1.9\nwant 1.2 0.39 238 0.78 2.7 2.7 2.3 0.78\nto 1.9 0.63 3.1 430 1.9 0.63 4.4 133\neat 0.34 0.34 1 0.34 5.8 1 15 0.34\nchinese 0.2 0.098 0.098 0.098 0.098 8.2 0.2 0.098\nfood 6.9 0.43 6.9 0.43 0.86 2.2 0.43 0.43\nlunch 0.57 0.19 0.19 0.19 0.19 0.38 0.19 0.19\nspend 0.32 0.16 0.32 0.16 0.16 0.16 0.16 0.16\nFigure 3.8 Add-one reconstituted counts for eight words (of V=1446) in the BeRP corpus",
    "metadata": {
      "source": "3",
      "chunk_id": 26,
      "token_count": 777,
      "chapter_title": ""
    }
  },
  {
    "content": "i want to eat chinese food lunch spend\ni 3.8 527 0.64 6.4 0.64 0.64 0.64 1.9\nwant 1.2 0.39 238 0.78 2.7 2.7 2.3 0.78\nto 1.9 0.63 3.1 430 1.9 0.63 4.4 133\neat 0.34 0.34 1 0.34 5.8 1 15 0.34\nchinese 0.2 0.098 0.098 0.098 0.098 8.2 0.2 0.098\nfood 6.9 0.43 6.9 0.43 0.86 2.2 0.43 0.43\nlunch 0.57 0.19 0.19 0.19 0.19 0.38 0.19 0.19\nspend 0.32 0.16 0.32 0.16 0.16 0.16 0.16 0.16\nFigure 3.8 Add-one reconstituted counts for eight words (of V=1446) in the BeRP corpus\nof 9332 sentences, computed by Eq. 3.27. Previously-zero counts are in gray.\nNote that add-one smoothing has made a very big change to the counts. Com-\nparing Fig. 3.8 to the original counts in Fig. 3.1, we can see that C(want to )changed\nfrom 608 to 238! We can see this in probability space as well: P(tojwant)decreases\nfrom 0.66 in the unsmoothed case to 0.26 in the smoothed case. Looking at the dis-\ncount d, de\ufb01ned as the ratio between new and old counts, shows us how strikingly\nthe counts for each pre\ufb01x word have been reduced; the discount for the bigram want\ntois 0.39, while the discount for Chinese food is 0.10, a factor of 10! The sharp\nchange occurs because too much probability mass is moved to all the zeros.\n3.6.2 Add-k smoothing\nOne alternative to add-one smoothing is to move a bit less of the probability mass\nfrom the seen to the unseen events. Instead of adding 1 to each count, we add a\nfractional count k(0.5? 0.01?). This algorithm is therefore called add-k smoothing . add-k\nP\u0003\nAdd-k(wnjwn\u00001) =C(wn\u00001wn)+k\nC(wn\u00001)+kV(3.28)\nAdd-k smoothing requires that we have a method for choosing k; this can be\ndone, for example, by optimizing on a devset . Although add-k is useful for some\ntasks (including text classi\ufb01cation), it turns out that it still doesn\u2019t work well for",
    "metadata": {
      "source": "3",
      "chunk_id": 27,
      "token_count": 670,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 17",
    "metadata": {
      "source": "3",
      "chunk_id": 28,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "3.6 \u2022 S MOOTHING , INTERPOLATION ,AND BACKOFF 17\nlanguage modeling, generating counts with poor variances and often inappropriate\ndiscounts (Gale and Church, 1994).\n3.6.3 Language Model Interpolation\nThere is an alternative source of knowledge we can draw on to solve the problem\nof zero frequency n-grams. If we are trying to compute P(wnjwn\u00002wn\u00001)but we\nhave no examples of a particular trigram wn\u00002wn\u00001wn, we can instead estimate its\nprobability by using the bigram probability P(wnjwn\u00001). Similarly, if we don\u2019t have\ncounts to compute P(wnjwn\u00001), we can look to the unigram P(wn). In other words,\nsometimes using less context can help us generalize more for contexts that the model\nhasn\u2019t learned much about.\nThe most common way to use this n-gram hierarchy is called interpolation : interpolation\ncomputing a new probability by interpolating (weighting and combining) the tri-\ngram, bigram, and unigram probabilities.3In simple linear interpolation, we com-\nbine different order n-grams by linearly interpolating them. Thus, we estimate the\ntrigram probability P(wnjwn\u00002wn\u00001)by mixing together the unigram, bigram, and\ntrigram probabilities, each weighted by a l:\n\u02c6P(wnjwn\u00002wn\u00001) = l1P(wn)\n+l2P(wnjwn\u00001)\n+l3P(wnjwn\u00002wn\u00001) (3.29)\nThe ls must sum to 1, making Eq. 3.29 equivalent to a weighted average. In a\nslightly more sophisticated version of linear interpolation, each lweight is com-\nputed by conditioning on the context. This way, if we have particularly accurate\ncounts for a particular bigram, we assume that the counts of the trigrams based on\nthis bigram will be more trustworthy, so we can make the ls for those trigrams\nhigher and thus give that trigram more weight in the interpolation. Equation 3.30\nshows the equation for interpolation with context-conditioned weights, where each\nlambda takes an argument that is the two prior word context:\n\u02c6P(wnjwn\u00002wn\u00001) = l1(wn\u00002:n\u00001)P(wn)\n+l2(wn\u00002:n\u00001)P(wnjwn\u00001)\n+l3(wn\u00002:n\u00001)P(wnjwn\u00002wn\u00001) (3.30)\nHow are these lvalues set? Both the simple interpolation and conditional interpo-\nlation ls are learned from a held-out corpus. A held-out corpus is an additional held-out\ntraining corpus, so-called because we hold it out from the training data, that we use\nto set these lvalues.4We do so by choosing the lvalues that maximize the likeli-\nhood of the held-out corpus. That is, we \ufb01x the n-gram probabilities and then search\nfor the lvalues that\u2014when plugged into Eq. 3.29\u2014give us the highest probability\nof the held-out set. There are various ways to \ufb01nd this optimal set of ls. One way\nis to use the EMalgorithm, an iterative learning algorithm that converges on locally\noptimal ls (Jelinek and Mercer, 1980).\n3We won\u2019t discuss the less-common alternative, called backoff , in which we use the trigram if the",
    "metadata": {
      "source": "3",
      "chunk_id": 29,
      "token_count": 754,
      "chapter_title": ""
    }
  },
  {
    "content": "lambda takes an argument that is the two prior word context:\n\u02c6P(wnjwn\u00002wn\u00001) = l1(wn\u00002:n\u00001)P(wn)\n+l2(wn\u00002:n\u00001)P(wnjwn\u00001)\n+l3(wn\u00002:n\u00001)P(wnjwn\u00002wn\u00001) (3.30)\nHow are these lvalues set? Both the simple interpolation and conditional interpo-\nlation ls are learned from a held-out corpus. A held-out corpus is an additional held-out\ntraining corpus, so-called because we hold it out from the training data, that we use\nto set these lvalues.4We do so by choosing the lvalues that maximize the likeli-\nhood of the held-out corpus. That is, we \ufb01x the n-gram probabilities and then search\nfor the lvalues that\u2014when plugged into Eq. 3.29\u2014give us the highest probability\nof the held-out set. There are various ways to \ufb01nd this optimal set of ls. One way\nis to use the EMalgorithm, an iterative learning algorithm that converges on locally\noptimal ls (Jelinek and Mercer, 1980).\n3We won\u2019t discuss the less-common alternative, called backoff , in which we use the trigram if the\nevidence is suf\ufb01cient for it, but if not we instead just use the bigram, otherwise the unigram. That is, we\nonly \u201cback off\u201d to a lower-order n-gram if we have zero evidence for a higher-order n-gram.\n4Held-out corpora are generally used to set hyperparameters , which are special parameters, unlike\nregular counts that are learned from the training data; we\u2019ll discuss hyperparameters in Chapter 7.",
    "metadata": {
      "source": "3",
      "chunk_id": 30,
      "token_count": 380,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 18",
    "metadata": {
      "source": "3",
      "chunk_id": 31,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "18 CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\n3.6.4 Stupid Backoff\nAn alternative to interpolation is backoff . In a backoff model, if the n-gram we need backoff\nhas zero counts, we approximate it by backing off to the (n-1)-gram. We continue\nbacking off until we reach a history that has some counts. For a backoff model to\ngive a correct probability distribution, we have to discount the higher-order n-grams discount\nto save some probability mass for the lower order n-grams. In practice, instead of\ndiscounting, it\u2019s common to use a much simpler non-discounted backoff algorithm\ncalled stupid backoff (Brants et al., 2007). stupid backoff\nStupid backoff gives up the idea of trying to make the language model a true\nprobability distribution. There is no discounting of the higher-order probabilities. If\na higher-order n-gram has a zero count, we simply backoff to a lower order n-gram,\nweighed by a \ufb01xed (context-independent) weight. This algorithm does not produce\na probability distribution, so we\u2019ll follow Brants et al. (2007) in referring to it as S:\nS(wijwi\u0000N+1:i\u00001) =8\n<\n:count (wi\u0000N+1:i)\ncount (wi\u0000N+1:i\u00001)if count (wi\u0000N+1:i)>0\nlS(wijwi\u0000N+2:i\u00001)otherwise(3.31)\nThe backoff terminates in the unigram, which has score S(w) =count (w)\nN. Brants et al.\n(2007) \ufb01nd that a value of 0.4 worked well for l.\n3.7 Advanced: Perplexity\u2019s Relation to Entropy\nWe introduced perplexity in Section 3.3 as a way to evaluate n-gram models on\na test set. A better n-gram model is one that assigns a higher probability to the\ntest data, and perplexity is a normalized version of the probability of the test set.\nThe perplexity measure actually arises from the information-theoretic concept of\ncross-entropy, which explains otherwise mysterious properties of perplexity (why\nthe inverse probability, for example?) and its relationship to entropy. Entropy is a Entropy\nmeasure of information. Given a random variable Xranging over whatever we are\npredicting (words, letters, parts of speech), the set of which we\u2019ll call c, and with a\nparticular probability function, call it p(x), the entropy of the random variable Xis:\nH(X) =\u0000X\nx2cp(x)log2p(x) (3.32)\nThe log can, in principle, be computed in any base. If we use log base 2, the\nresulting value of entropy will be measured in bits.\nOne intuitive way to think about entropy is as a lower bound on the number of\nbits it would take to encode a certain decision or piece of information in the optimal\ncoding scheme. Consider an example from the standard information theory textbook\nCover and Thomas (1991). Imagine that we want to place a bet on a horse race but\nit is too far to go all the way to Yonkers Racetrack, so we\u2019d like to send a short\nmessage to the bookie to tell him which of the eight horses to bet on. One way to\nencode this message is just to use the binary representation of the horse\u2019s number\nas the code; thus, horse 1 would be 001, horse 2 010, horse 3 011, and so on, with",
    "metadata": {
      "source": "3",
      "chunk_id": 32,
      "token_count": 778,
      "chapter_title": ""
    }
  },
  {
    "content": "measure of information. Given a random variable Xranging over whatever we are\npredicting (words, letters, parts of speech), the set of which we\u2019ll call c, and with a\nparticular probability function, call it p(x), the entropy of the random variable Xis:\nH(X) =\u0000X\nx2cp(x)log2p(x) (3.32)\nThe log can, in principle, be computed in any base. If we use log base 2, the\nresulting value of entropy will be measured in bits.\nOne intuitive way to think about entropy is as a lower bound on the number of\nbits it would take to encode a certain decision or piece of information in the optimal\ncoding scheme. Consider an example from the standard information theory textbook\nCover and Thomas (1991). Imagine that we want to place a bet on a horse race but\nit is too far to go all the way to Yonkers Racetrack, so we\u2019d like to send a short\nmessage to the bookie to tell him which of the eight horses to bet on. One way to\nencode this message is just to use the binary representation of the horse\u2019s number\nas the code; thus, horse 1 would be 001, horse 2 010, horse 3 011, and so on, with\nhorse 8 coded as 000. If we spend the whole day betting and each horse is coded\nwith 3 bits, on average we would be sending 3 bits per race.\nCan we do better? Suppose that the spread is the actual distribution of the bets\nplaced and that we represent it as the prior probability of each horse as follows:",
    "metadata": {
      "source": "3",
      "chunk_id": 33,
      "token_count": 349,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 19\n\n3.7 \u2022 A DVANCED : PERPLEXITY \u2019SRELATION TO ENTROPY 19\nHorse 11\n2Horse 51\n64\nHorse 21\n4Horse 61\n64\nHorse 31\n8Horse 71\n64\nHorse 41\n16Horse 81\n64\nThe entropy of the random variable Xthat ranges over horses gives us a lower\nbound on the number of bits and is\nH(X) =\u0000i=8X\ni=1p(i)log2p(i)\n=\u00001\n2log21\n2\u00001\n4log21\n4\u00001\n8log21\n8\u00001\n16log21\n16\u00004(1\n64log21\n64)\n=2 bits (3.33)\nA code that averages 2 bits per race can be built with short encodings for more\nprobable horses, and longer encodings for less probable horses. For example, we\ncould encode the most likely horse with the code 0, and the remaining horses as 10,\nthen 110,1110 ,111100 ,111101 ,111110 , and 111111 .\nWhat if the horses are equally likely? We saw above that if we used an equal-\nlength binary code for the horse numbers, each horse took 3 bits to code, so the\naverage was 3. Is the entropy the same? In this case each horse would have a\nprobability of1\n8. The entropy of the choice of horses is then\nH(X) =\u0000i=8X\ni=11\n8log21\n8=\u0000log21\n8=3 bits (3.34)\nUntil now we have been computing the entropy of a single variable. But most of\nwhat we will use entropy for involves sequences . For a grammar, for example, we\nwill be computing the entropy of some sequence of words W=fw1;w2;:::; wng.\nOne way to do this is to have a variable that ranges over sequences of words. For\nexample we can compute the entropy of a random variable that ranges over all se-\nquences of words of length nin some language Las follows:\nH(w1;w2;:::; wn) =\u0000X\nw1:n2Lp(w1:n)logp(w1:n) (3.35)\nWe could de\ufb01ne the entropy rate (we could also think of this as the per-word entropy rate\nentropy ) as the entropy of this sequence divided by the number of words:\n1\nnH(w1:n) =\u00001\nnX\nw1:n2Lp(w1:n)logp(w1:n) (3.36)\nBut to measure the true entropy of a language, we need to consider sequences of\nin\ufb01nite length. If we think of a language as a stochastic process Lthat produces a\nsequence of words, and allow Wto represent the sequence of words w1;:::; wn, then\nL\u2019s entropy rate H(L)is de\ufb01ned as\nH(L) = lim\nn!\u00a51\nnH(w1:n)\n=\u0000lim\nn!\u00a51\nnX\nW2Lp(w1:n)logp(w1:n) (3.37)",
    "metadata": {
      "source": "3",
      "chunk_id": 34,
      "token_count": 701,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 20\n\n20 CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\nThe Shannon-McMillan-Breiman theorem (Algoet and Cover 1988, Cover and Thomas\n1991) states that if the language is regular in certain ways (to be exact, if it is both\nstationary and ergodic),\nH(L) =lim\nn!\u00a5\u00001\nnlogp(w1:n) (3.38)\nThat is, we can take a single sequence that is long enough instead of summing over\nall possible sequences. The intuition of the Shannon-McMillan-Breiman theorem\nis that a long-enough sequence of words will contain in it many other shorter se-\nquences and that each of these shorter sequences will reoccur in the longer sequence\naccording to their probabilities.\nA stochastic process is said to be stationary if the probabilities it assigns to a Stationary\nsequence are invariant with respect to shifts in the time index. In other words, the\nprobability distribution for words at time tis the same as the probability distribution\nat time t+1. Markov models, and hence n-grams, are stationary. For example, in\na bigram, Piis dependent only on Pi\u00001. So if we shift our time index by x,Pi+xis\nstill dependent on Pi+x\u00001. But natural language is not stationary, since as we show\nin Appendix D, the probability of upcoming words can be dependent on events that\nwere arbitrarily distant and time dependent. Thus, our statistical models only give\nan approximation to the correct distributions and entropies of natural language.\nTo summarize, by making some incorrect but convenient simplifying assump-\ntions, we can compute the entropy of some stochastic process by taking a very long\nsample of the output and computing its average log probability.\nNow we are ready to introduce cross-entropy . The cross-entropy is useful when cross-entropy\nwe don\u2019t know the actual probability distribution pthat generated some data. It\nallows us to use some m, which is a model of p(i.e., an approximation to p). The\ncross-entropy of monpis de\ufb01ned by\nH(p;m) =lim\nn!\u00a5\u00001\nnX\nW2Lp(w1;:::; wn)logm(w1;:::; wn) (3.39)\nThat is, we draw sequences according to the probability distribution p, but sum the\nlog of their probabilities according to m.\nAgain, following the Shannon-McMillan-Breiman theorem, for a stationary er-\ngodic process:\nH(p;m) =lim\nn!\u00a5\u00001\nnlogm(w1w2:::wn) (3.40)\nThis means that, as for entropy, we can estimate the cross-entropy of a model m\non some distribution pby taking a single sequence that is long enough instead of\nsumming over all possible sequences.\nWhat makes the cross-entropy useful is that the cross-entropy H(p;m)is an up-\nper bound on the entropy H(p). For any model m:\nH(p)\u0014H(p;m) (3.41)\nThis means that we can use some simpli\ufb01ed model mto help estimate the true en-\ntropy of a sequence of symbols drawn according to probability p. The more accurate\nmis, the closer the cross-entropy H(p;m)will be to the true entropy H(p). Thus,\nthe difference between H(p;m)andH(p)is a measure of how accurate a model is.\nBetween two models m1andm2, the more accurate model will be the one with the",
    "metadata": {
      "source": "3",
      "chunk_id": 35,
      "token_count": 765,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 21\n\n3.8 \u2022 S UMMARY 21\nlower cross-entropy. (The cross-entropy can never be lower than the true entropy, so\na model cannot err by underestimating the true entropy.)\nWe are \ufb01nally ready to see the relation between perplexity and cross-entropy\nas we saw it in Eq. 3.40. Cross-entropy is de\ufb01ned in the limit as the length of the\nobserved word sequence goes to in\ufb01nity. We approximate this cross-entropy by\nrelying on a (suf\ufb01ciently long) sequence of \ufb01xed length. This approximation to the\ncross-entropy of a model M=P(wijwi\u0000N+1:i\u00001)on a sequence of words Wis\nH(W) =\u00001\nNlogP(w1w2:::wN) (3.42)\nTheperplexity of a model Pon a sequence of words Wis now formally de\ufb01ned as perplexity\n2 raised to the power of this cross-entropy:\nPerplexity (W) = 2H(W)\n=P(w1w2:::wN)\u00001\nN\n=Ns\n1\nP(w1w2:::wN)\n3.8 Summary\nThis chapter introduced language modeling via the n-gram model, a classic model\nthat allows us to introduce many of the basic concepts in language modeling.\n\u2022 Language models offer a way to assign a probability to a sentence or other\nsequence of words or tokens, and to predict a word or token from preceding\nwords or tokens.\n\u2022N-grams are perhaps the simplest kind of language model. They are Markov\nmodels that estimate words from a \ufb01xed window of previous words. N-gram\nmodels can be trained by counting in a training corpus and normalizing the\ncounts (the maximum likelihood estimate ).\n\u2022 N-gram language models can be evaluated on a test set using perplexity.\n\u2022 The perplexity of a test set according to a language model is a function of\nthe probability of the test set: the inverse test set probability according to the\nmodel, normalized by the length.\n\u2022Sampling from a language model means to generate some sentences, choos-\ning each sentence according to its likelihood as de\ufb01ned by the model.\n\u2022Smoothing algorithms provide a way to estimate probabilities for events that\nwere unseen in training. Commonly used smoothing algorithms for n-grams\ninclude add-1 smoothing, or rely on lower-order n-gram counts through inter-\npolation .\nBibliographical and Historical Notes\nThe underlying mathematics of the n-gram was \ufb01rst proposed by Markov (1913),\nwho used what are now called Markov chains (bigrams and trigrams) to predict\nwhether an upcoming letter in Pushkin\u2019s Eugene Onegin would be a vowel or a con-\nsonant. Markov classi\ufb01ed 20,000 letters as V or C and computed the bigram and",
    "metadata": {
      "source": "3",
      "chunk_id": 36,
      "token_count": 625,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 22",
    "metadata": {
      "source": "3",
      "chunk_id": 37,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "22 CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\ntrigram probability that a given letter would be a vowel given the previous one or\ntwo letters. Shannon (1948) applied n-grams to compute approximations to English\nword sequences. Based on Shannon\u2019s work, Markov models were commonly used in\nengineering, linguistic, and psychological work on modeling word sequences by the\n1950s. In a series of extremely in\ufb02uential papers starting with Chomsky (1956) and\nincluding Chomsky (1957) and Miller and Chomsky (1963), Noam Chomsky argued\nthat \u201c\ufb01nite-state Markov processes\u201d, while a possibly useful engineering heuristic,\nwere incapable of being a complete cognitive model of human grammatical knowl-\nedge. These arguments led many linguists and computational linguists to ignore\nwork in statistical modeling for decades.\nThe resurgence of n-gram language models came from Fred Jelinek and col-\nleagues at the IBM Thomas J. Watson Research Center, who were in\ufb02uenced by\nShannon, and James Baker at CMU, who was in\ufb02uenced by the prior, classi\ufb01ed\nwork of Leonard Baum and colleagues on these topics at labs like the US Institute\nfor Defense Analyses (IDA) after they were declassi\ufb01ed. Independently these two\nlabs successfully used n-grams in their speech recognition systems at the same time\n(Baker 1975b, Jelinek et al. 1975, Baker 1975a, Bahl et al. 1983, Jelinek 1990). The\nterms \u201clanguage model\u201d and \u201cperplexity\u201d were \ufb01rst used for this technology by the\nIBM group. Jelinek and his colleagues used the term language model in a pretty\nmodern way, to mean the entire set of linguistic in\ufb02uences on word sequence prob-\nabilities, including grammar, semantics, discourse, and even speaker characteristics,\nrather than just the particular n-gram model itself.\nAdd-one smoothing derives from Laplace\u2019s 1812 law of succession and was \ufb01rst\napplied as an engineering solution to the zero frequency problem by Jeffreys (1948)\nbased on an earlier Add-K suggestion by Johnson (1932). Problems with the add-\none algorithm are summarized in Gale and Church (1994).\nA wide variety of different language modeling and smoothing techniques were\nproposed in the 80s and 90s, including Good-Turing discounting\u2014\ufb01rst applied to the\nn-gram smoothing at IBM by Katz (N \u00b4adas 1984, Church and Gale 1991)\u2014 Witten-\nBell discounting (Witten and Bell, 1991), and varieties of class-based n-gram mod-class-based\nn-gram\nels that used information about word classes. Starting in the late 1990s, Chen and\nGoodman performed a number of carefully controlled experiments comparing dif-\nferent algorithms and parameters (Chen and Goodman 1999, Goodman 2006, inter\nalia). They showed the advantages of Modi\ufb01ed Interpolated Kneser-Ney , which\nbecame the standard baseline for n-gram language modeling around the turn of the\ncentury, especially because they showed that caches and class-based models pro-\nvided only minor additional improvement. SRILM (Stolcke, 2002) and KenLM\n(Hea\ufb01eld 2011, Hea\ufb01eld et al. 2013) are publicly available toolkits for building n-\ngram language models.\nLarge language models are based on neural networks rather than n-grams, en-",
    "metadata": {
      "source": "3",
      "chunk_id": 38,
      "token_count": 777,
      "chapter_title": ""
    }
  },
  {
    "content": "one algorithm are summarized in Gale and Church (1994).\nA wide variety of different language modeling and smoothing techniques were\nproposed in the 80s and 90s, including Good-Turing discounting\u2014\ufb01rst applied to the\nn-gram smoothing at IBM by Katz (N \u00b4adas 1984, Church and Gale 1991)\u2014 Witten-\nBell discounting (Witten and Bell, 1991), and varieties of class-based n-gram mod-class-based\nn-gram\nels that used information about word classes. Starting in the late 1990s, Chen and\nGoodman performed a number of carefully controlled experiments comparing dif-\nferent algorithms and parameters (Chen and Goodman 1999, Goodman 2006, inter\nalia). They showed the advantages of Modi\ufb01ed Interpolated Kneser-Ney , which\nbecame the standard baseline for n-gram language modeling around the turn of the\ncentury, especially because they showed that caches and class-based models pro-\nvided only minor additional improvement. SRILM (Stolcke, 2002) and KenLM\n(Hea\ufb01eld 2011, Hea\ufb01eld et al. 2013) are publicly available toolkits for building n-\ngram language models.\nLarge language models are based on neural networks rather than n-grams, en-\nabling them to solve the two major problems with n-grams: (1) the number of param-\neters increases exponentially as the n-gram order increases, and (2) n-grams have no\nway to generalize from training examples to test set examples unless they use iden-\ntical words. Neural language models instead project words into a continuous space\nin which words with similar contexts have similar representations. We\u2019ll introduce\ntransformer-based large language models in Chapter 9, along the way introducing\nfeedforward language models (Bengio et al. 2006, Schwenk 2007) in Chapter 7 and\nrecurrent language models (Mikolov, 2012) in Chapter 8.",
    "metadata": {
      "source": "3",
      "chunk_id": 39,
      "token_count": 435,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 23\n\nEXERCISES 23\nExercises\n3.1 Write out the equation for trigram probability estimation (modifying Eq. 3.11).\nNow write out all the non-zero trigram probabilities for the I am Sam corpus\non page 4.\n3.2 Calculate the probability of the sentence i want chinese food . Give two\nprobabilities, one using Fig. 3.2 and the \u2018useful probabilities\u2019 just below it on\npage 6, and another using the add-1 smoothed table in Fig. 3.7. Assume the\nadditional add-1 smoothed probabilities P(i|<s> ) =0:19 and P(</s>|food ) =\n0:40.\n3.3 Which of the two probabilities you computed in the previous exercise is higher,\nunsmoothed or smoothed? Explain why.\n3.4 We are given the following corpus, modi\ufb01ed from the one in the chapter:\n<s> I am Sam </s>\n<s> Sam I am </s>\n<s> I am Sam </s>\n<s> I do not like green eggs and Sam </s>\nUsing a bigram language model with add-one smoothing, what is P(Sam j\nam)? Include <s>and</s> in your counts just like any other token.\n3.5 Suppose we didn\u2019t use the end-symbol </s> . Train an unsmoothed bigram\ngrammar on the following training corpus without using the end-symbol </s> :\n<s> a b\n<s> b b\n<s> b a\n<s> a a\nDemonstrate that your bigram model does not assign a single probability dis-\ntribution across all sentence lengths by showing that the sum of the probability\nof the four possible 2 word sentences over the alphabet fa,bgis 1.0, and the\nsum of the probability of all possible 3 word sentences over the alphabet fa,bg\nis also 1.0.\n3.6 Suppose we train a trigram language model with add-one smoothing on a\ngiven corpus. The corpus contains V word types. Express a formula for esti-\nmating P(w3jw1,w2), where w3 is a word which follows the bigram (w1,w2),\nin terms of various n-gram counts and V . Use the notation c(w1,w2,w3) to\ndenote the number of times that trigram (w1,w2,w3) occurs in the corpus, and\nso on for bigrams and unigrams.\n3.7 We are given the following corpus, modi\ufb01ed from the one in the chapter:\n<s> I am Sam </s>\n<s> Sam I am </s>\n<s> I am Sam </s>\n<s> I do not like green eggs and Sam </s>\nIf we use linear interpolation smoothing between a maximum-likelihood bi-\ngram model and a maximum-likelihood unigram model with l1=1\n2andl2=\n1\n2, what is P(Samjam)? Include <s> and</s> in your counts just like any\nother token.\n3.8 Write a program to compute unsmoothed unigrams and bigrams.",
    "metadata": {
      "source": "3",
      "chunk_id": 40,
      "token_count": 668,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 24\n\n24 CHAPTER 3 \u2022 N- GRAM LANGUAGE MODELS\n3.9 Run your n-gram program on two different small corpora of your choice (you\nmight use email text or newsgroups). Now compare the statistics of the two\ncorpora. What are the differences in the most common unigrams between the\ntwo? How about interesting differences in bigrams?\n3.10 Add an option to your program to generate random sentences.\n3.11 Add an option to your program to compute the perplexity of a test set.\n3.12 You are given a training set of 100 numbers that consists of 91 zeros and 1\neach of the other digits 1-9. Now we see the following test set: 0 0 0 0 0 3 0 0\n0 0. What is the unigram perplexity?",
    "metadata": {
      "source": "3",
      "chunk_id": 41,
      "token_count": 185,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 25",
    "metadata": {
      "source": "3",
      "chunk_id": 42,
      "token_count": 4,
      "chapter_title": ""
    }
  },
  {
    "content": "Exercises 25\nAlgoet, P. H. and T. M. Cover. 1988. A sandwich proof of\nthe Shannon-McMillan-Breiman theorem. The Annals of\nProbability , 16(2):899\u2013909.\nBahl, L. R., F. Jelinek, and R. L. Mercer. 1983. A maxi-\nmum likelihood approach to continuous speech recogni-\ntion. IEEE Transactions on Pattern Analysis and Machine\nIntelligence , 5(2):179\u2013190.\nBaker, J. K. 1975a. The DRAGON system \u2013 An overview.\nIEEE Transactions on ASSP , ASSP-23(1):24\u201329.\nBaker, J. K. 1975b. Stochastic modeling for automatic\nspeech understanding. In D. R. Reddy, ed., Speech Recog-\nnition . Academic Press.\nBengio, Y ., H. Schwenk, J.-S. Sen \u00b4ecal, F. Morin, and J.-L.\nGauvain. 2006. Neural probabilistic language models. In\nInnovations in Machine Learning , 137\u2013186. Springer.\nBlodgett, S. L. and B. O\u2019Connor. 2017. Racial disparity in\nnatural language processing: A case study of social media\nAfrican-American English. FAT/ML Workshop, KDD .\nBrants, T., A. C. Popat, P. Xu, F. J. Och, and J. Dean.\n2007. Large language models in machine translation.\nEMNLP/CoNLL .\nChen, S. F. and J. Goodman. 1999. An empirical study of\nsmoothing techniques for language modeling. Computer\nSpeech and Language , 13:359\u2013394.\nChomsky, N. 1956. Three models for the description of\nlanguage. IRE Transactions on Information Theory ,\n2(3):113\u2013124.\nChomsky, N. 1957. Syntactic Structures . Mouton.\nChurch, K. W. and W. A. Gale. 1991. A comparison of the\nenhanced Good-Turing and deleted estimation methods\nfor estimating probabilities of English bigrams. Com-\nputer Speech and Language , 5:19\u201354.\nCover, T. M. and J. A. Thomas. 1991. Elements of Informa-\ntion Theory . Wiley.\nDavies, M. 2020. The Corpus of Contemporary Amer-\nican English (COCA): One billion words, 1990-2019.\nhttps://www.english-corpora.org/coca/ .\nFranz, A. and T. Brants. 2006. All our n-gram are\nbelong to you. https://research.google/blog/\nall-our-n-gram-are-belong-to-you/ .\nGale, W. A. and K. W. Church. 1994. What is wrong with\nadding one? In N. Oostdijk and P. de Haan, eds, Corpus-\nBased Research into Language , 189\u2013198. Rodopi.\nGoodman, J. 2006. A bit of progress in language model-\ning: Extended version. Technical Report MSR-TR-2001-\n72, Machine Learning and Applied Statistics Group, Mi-\ncrosoft Research, Redmond, WA.\nHea\ufb01eld, K. 2011. KenLM: Faster and smaller language\nmodel queries. Workshop on Statistical Machine Trans-\nlation .",
    "metadata": {
      "source": "3",
      "chunk_id": 43,
      "token_count": 757,
      "chapter_title": ""
    }
  },
  {
    "content": "for estimating probabilities of English bigrams. Com-\nputer Speech and Language , 5:19\u201354.\nCover, T. M. and J. A. Thomas. 1991. Elements of Informa-\ntion Theory . Wiley.\nDavies, M. 2020. The Corpus of Contemporary Amer-\nican English (COCA): One billion words, 1990-2019.\nhttps://www.english-corpora.org/coca/ .\nFranz, A. and T. Brants. 2006. All our n-gram are\nbelong to you. https://research.google/blog/\nall-our-n-gram-are-belong-to-you/ .\nGale, W. A. and K. W. Church. 1994. What is wrong with\nadding one? In N. Oostdijk and P. de Haan, eds, Corpus-\nBased Research into Language , 189\u2013198. Rodopi.\nGoodman, J. 2006. A bit of progress in language model-\ning: Extended version. Technical Report MSR-TR-2001-\n72, Machine Learning and Applied Statistics Group, Mi-\ncrosoft Research, Redmond, WA.\nHea\ufb01eld, K. 2011. KenLM: Faster and smaller language\nmodel queries. Workshop on Statistical Machine Trans-\nlation .\nHea\ufb01eld, K., I. Pouzyrevsky, J. H. Clark, and P. Koehn. 2013.\nScalable modi\ufb01ed Kneser-Ney language model estima-\ntion. ACL.\nJeffreys, H. 1948. Theory of Probability , 2nd edition. Claren-\ndon Press. Section 3.23.\nJelinek, F. 1990. Self-organized language modeling for\nspeech recognition. In A. Waibel and K.-F. Lee, eds,\nReadings in Speech Recognition , 450\u2013506. Morgan Kauf-\nmann. Originally distributed as IBM technical report in\n1985.Jelinek, F. and R. L. Mercer. 1980. Interpolated estimation\nof Markov source parameters from sparse data. In E. S.\nGelsema and L. N. Kanal, eds, Proceedings, Workshop\non Pattern Recognition in Practice , 381\u2013397. North Hol-\nland.\nJelinek, F., R. L. Mercer, and L. R. Bahl. 1975. Design of a\nlinguistic statistical decoder for the recognition of contin-\nuous speech. IEEE Transactions on Information Theory ,\nIT-21(3):250\u2013256.\nJohnson, W. E. 1932. Probability: deductive and inductive\nproblems (appendix to). Mind , 41(164):421\u2013423.\nJurafsky, D., C. Wooters, G. Tajchman, J. Segal, A. Stolcke,\nE. Fosler, and N. Morgan. 1994. The Berkeley restaurant\nproject. ICSLP .\nJurgens, D., Y . Tsvetkov, and D. Jurafsky. 2017. Incorpo-\nrating dialectal variability for socially equitable language\nidenti\ufb01cation. ACL.\nKane, S. K., M. R. Morris, A. Paradiso, and J. Campbell.\n2017. \u201cat times avuncular and cantankerous, with the\nre\ufb02exes of a mongoose\u201d: Understanding self-expression\nthrough augmentative and alternative communication de-\nvices. CSCW .",
    "metadata": {
      "source": "3",
      "chunk_id": 44,
      "token_count": 755,
      "chapter_title": ""
    }
  },
  {
    "content": "on Pattern Recognition in Practice , 381\u2013397. North Hol-\nland.\nJelinek, F., R. L. Mercer, and L. R. Bahl. 1975. Design of a\nlinguistic statistical decoder for the recognition of contin-\nuous speech. IEEE Transactions on Information Theory ,\nIT-21(3):250\u2013256.\nJohnson, W. E. 1932. Probability: deductive and inductive\nproblems (appendix to). Mind , 41(164):421\u2013423.\nJurafsky, D., C. Wooters, G. Tajchman, J. Segal, A. Stolcke,\nE. Fosler, and N. Morgan. 1994. The Berkeley restaurant\nproject. ICSLP .\nJurgens, D., Y . Tsvetkov, and D. Jurafsky. 2017. Incorpo-\nrating dialectal variability for socially equitable language\nidenti\ufb01cation. ACL.\nKane, S. K., M. R. Morris, A. Paradiso, and J. Campbell.\n2017. \u201cat times avuncular and cantankerous, with the\nre\ufb02exes of a mongoose\u201d: Understanding self-expression\nthrough augmentative and alternative communication de-\nvices. CSCW .\nKing, S. 2020. From African American Vernacular English\nto African American Language: Rethinking the study of\nrace and language in African Americans\u2019 speech. Annual\nReview of Linguistics , 6:285\u2013300.\nLin, Y ., J.-B. Michel, E. Aiden Lieberman, J. Orwant,\nW. Brockman, and S. Petrov. 2012. Syntactic annotations\nfor the Google books NGram corpus. ACL.\nLiu, J., S. Min, L. Zettlemoyer, Y . Choi, and H. Hajishirzi.\n2024. In\ufb01ni-gram: Scaling unbounded n-gram language\nmodels to a trillion tokens. ArXiv preprint.\nMarkov, A. A. 1913. Essai d\u2019une recherche statistique sur\nle texte du roman \u201cEugene Onegin\u201d illustrant la liaison\ndes epreuve en chain (\u2018Example of a statistical investiga-\ntion of the text of \u201cEugene Onegin\u201d illustrating the de-\npendence between samples in chain\u2019). Izvistia Impera-\ntorskoi Akademii Nauk (Bulletin de l\u2019Acad \u00b4emie Imp \u00b4eriale\ndes Sciences de St.-P \u00b4etersbourg) , 7:153\u2013162.\nMikolov, T. 2012. Statistical language models based on neu-\nral networks . Ph.D. thesis, Brno University of Technol-\nogy.\nMiller, G. A. and N. Chomsky. 1963. Finitary models of lan-\nguage users. In R. D. Luce, R. R. Bush, and E. Galanter,\neds, Handbook of Mathematical Psychology , volume II,\n419\u2013491. John Wiley.\nMiller, G. A. and J. A. Selfridge. 1950. Verbal context and\nthe recall of meaningful material. American Journal of\nPsychology , 63:176\u2013185.\nN\u00b4adas, A. 1984. Estimation of probabilities in the language\nmodel of the IBM speech recognition system. IEEE\nTransactions on ASSP , 32(4):859\u2013861.\nSchwenk, H. 2007. Continuous space language models.",
    "metadata": {
      "source": "3",
      "chunk_id": 45,
      "token_count": 763,
      "chapter_title": ""
    }
  },
  {
    "content": "tion of the text of \u201cEugene Onegin\u201d illustrating the de-\npendence between samples in chain\u2019). Izvistia Impera-\ntorskoi Akademii Nauk (Bulletin de l\u2019Acad \u00b4emie Imp \u00b4eriale\ndes Sciences de St.-P \u00b4etersbourg) , 7:153\u2013162.\nMikolov, T. 2012. Statistical language models based on neu-\nral networks . Ph.D. thesis, Brno University of Technol-\nogy.\nMiller, G. A. and N. Chomsky. 1963. Finitary models of lan-\nguage users. In R. D. Luce, R. R. Bush, and E. Galanter,\neds, Handbook of Mathematical Psychology , volume II,\n419\u2013491. John Wiley.\nMiller, G. A. and J. A. Selfridge. 1950. Verbal context and\nthe recall of meaningful material. American Journal of\nPsychology , 63:176\u2013185.\nN\u00b4adas, A. 1984. Estimation of probabilities in the language\nmodel of the IBM speech recognition system. IEEE\nTransactions on ASSP , 32(4):859\u2013861.\nSchwenk, H. 2007. Continuous space language models.\nComputer Speech & Language , 21(3):492\u2013518.\nShannon, C. E. 1948. A mathematical theory of commu-\nnication. Bell System Technical Journal , 27(3):379\u2013423.\nContinued in the following volume.\nStolcke, A. 1998. Entropy-based pruning of backoff lan-\nguage models. Proc. DARPA Broadcast News Transcrip-\ntion and Understanding Workshop .",
    "metadata": {
      "source": "3",
      "chunk_id": 46,
      "token_count": 369,
      "chapter_title": ""
    }
  },
  {
    "content": "## Page 26\n\n26 Chapter 3 \u2022 N-gram Language Models\nStolcke, A. 2002. SRILM \u2013 an extensible language modeling\ntoolkit. ICSLP .\nTrnka, K., D. Yarrington, J. McCaw, K. F. McCoy, and\nC. Pennington. 2007. The effects of word prediction on\ncommunication rate for AAC. NAACL-HLT .\nWitten, I. H. and T. C. Bell. 1991. The zero-frequency prob-\nlem: Estimating the probabilities of novel events in adap-\ntive text compression. IEEE Transactions on Information\nTheory , 37(4):1085\u20131094.",
    "metadata": {
      "source": "3",
      "chunk_id": 47,
      "token_count": 153,
      "chapter_title": ""
    }
  }
]