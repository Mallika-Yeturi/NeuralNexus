# LLM24aug

## Page 1

Large Language ModelsIntroduction to Large Language Models

## Page 2

Language models•Remember the simple n-gram language model•Assigns probabilities to sequences of words•Generate text by sampling possible next words•Is trained on counts computed from lots of text•Large language models are similar and different:•Assigns probabilities to sequences of words•Generate text by sampling possible next words•Are trained by learning to guess the next word

## Page 3

Large language models•Even through pretrained only to predict words•Learn a lot of useful language knowledge•Since training on a lot of text

## Page 4

Three architectures for large language modelsDecoders   Encoders     Encoder-decodersGPT, Claude,  BERT family,  Flan-T5, WhisperLlama    HuBERTMixtralPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.
32Decoders•Language models! What we’ve seen so far.•Nice to generate from; can’t condition on future wordsEncoders•Gets bidirectional context – can condition on future!•How do we train them to build strong representations?Encoder-Decoders•Good parts of decoders and encoders?•What’s the best way to pretrain them?Pretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.
32Decoders•Language models! What we’ve seen so far.•Nice to generate from; can’t condition on future wordsEncoders•Gets bidirectional context – can condition on future!•How do we train them to build strong representations?Encoder-Decoders•Good parts of decoders and encoders?•What’s the best way to pretrain them?Pretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.
32Decoders•Language models! What we’ve seen so far.•Nice to generate from; can’t condition on future wordsEncoders•Gets bidirectional context – can condition on future!•How do we train them to build strong representations?Encoder-Decoders•Good parts of decoders and encoders?•What’s the best way to pretrain them?

## Page 5

EncodersMany varieties!•Popular: Masked Language Models (MLMs)•BERT family•Trained by predicting words from surrounding words on both sides•Are usually finetuned (trained on supervised data) for classification tasks.Pretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.
32Decoders•Language models! What we’ve seen so far.•Nice to generate from; can’t condition on future wordsEncoders•Gets bidirectional context – can condition on future!•How do we train them to build strong representations?Encoder-Decoders•Good parts of decoders and encoders?•What’s the best way to pretrain them?

## Page 6

Encoder-Decoders•Trained to map from one sequence to another•Very popular for:•machine translation (map from one language to another)•speech recognition (map from acoustics to words)Pretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.
32Decoders•Language models! What we’ve seen so far.•Nice to generate from; can’t condition on future wordsEncoders•Gets bidirectional context – can condition on future!•How do we train them to build strong representations?Encoder-Decoders•Good parts of decoders and encoders?•What’s the best way to pretrain them?

## Page 7

Large Language ModelsIntroduction to Large Language Models

## Page 8

Large Language ModelsLarge Language Models: What tasks can they do?

## Page 9

Big ideaMany tasks can be turned into tasks of predicting words!

## Page 10

This lecture: decoder-only modelsAlso called:•Causal LLMs•Autoregressive LLMs•Left-to-right LLMs•Predict words left to rightPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.
32Decoders•Language models! What we’ve seen so far.•Nice to generate from; can’t condition on future wordsEncoders•Gets bidirectional context – can condition on future!•How do we train them to build strong representations?Encoder-Decoders•Good parts of decoders and encoders?•What’s the best way to pretrain them?

## Page 11

Conditional Generation: Generating text conditioned on previous text!
Preﬁx TextCompletion Text
EncoderTransformerBlocksSoftmax
longall
andthanksforallthe
the…UUUnencoder layerLanguage ModelingHeadlogits
So
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+…

## Page 12

Many practical NLP tasks can be cast as word prediction!Sentiment analysis: “I like Jackie Chan”1.We give the language model this string:The sentiment of the sentence "I like Jackie Chan" is: 2.And see what word it thinks comes next:10.1•LARGELANGUAGEMODELS WITHTRANSFORMERS3
Preﬁx TextCompletion Text
EncoderTransformerBlocksSoftmax
longall
andthanksforallthe
the…UUUnencoder layerLanguage ModelingHeadlogits
So
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+…
Figure 10.1Left-to-right (also called autoregressive) text completion with transformer-based large languagemodels. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.word “negative” to see which is higher:P(positive|The sentiment of the sentence ‘‘I like Jackie Chan" is:)P(negative|The sentiment of the sentence ‘‘I like Jackie Chan" is:)If the word “positive” is more probable, we say the sentiment of the sentence ispositive, otherwise we say the sentiment is negative.We can also cast more complex tasks as word prediction. Consider questionanswering, in which the system is given a question (for example a question witha simple factual answer) and must give a textual answer; we introduce this task indetail in Chapter 15. We can cast the task of question answering as word predictionby giving a language model a question and a token likeA:suggesting that an answershould come next:Q: Who wrote the book ‘‘The Origin of Species"? A:If we ask a language model to compute the probability distribution over possiblenext words given this preﬁx:P(w|Q: Who wrote the book ‘‘The Origin of Species"? A:)and look at which wordswhave high probabilities, we might expect to see thatCharlesis very likely, and then if we chooseCharlesand continue and askP(w|Q: Who wrote the book ‘‘The Origin of Species"? A: Charles)we might now see thatDarwinis the most probable token, and select it.Conditional generation can even be used to accomplish tasks that must generatelonger responses. Consider the task oftext summarization, which is to take a longtextsummarizationtext, such as a full-length article, and produce an effective shorter summary of it. Wecan cast summarization as language modeling by giving a large language model atext, and follow the text by a token liketl;dr; this token is short for something like

## Page 13

Framing lots of tasks as conditional generationQA: “Who wrote The Origin of Species”1.We give the language model this string:2.And see what word it thinks comes next:3.And iterate:20CHAPTER10•TRANSFORMERS ANDLARGELANGUAGEMODELS
Preﬁx TextCompletion Text
InputEmbeddingsTransformerBlocksSample from Softmax
Solongall
andthanksforallthe
the…linear layer
Figure 10.15Autoregressive text completion with transformer-based large language models.word “negative” to see which is higher:P(positive|The sentiment of the sentence “I like Jackie Chan” is:)P(negative|The sentiment of the sentence “I like Jackie Chan” is:)If the word “positive” is more probable, we say the sentiment of the sentence ispositive, otherwise we say the sentiment is negative.We can also cast more complex tasks as word prediction. Consider the taskof answering simple questions, a task we return to in Chapter 14. In this task thesystem is given some question and must give a textual answer. We can cast the taskof question answering as word prediction by giving a language model a question anda token likeA:suggesting that an answer should come next:Q: Who wrote the book ‘‘The Origin of Species"? A:If we ask a language model to computeP(w|Q: Who wrote the book “The Origin of Species”? A:)and look at which wordswhave high probabilities, we might expect to see thatCharlesis very likely, and then if we chooseCharlesand continue and askP(w|Q: Who wrote the book “The Origin of Species”? A: Charles)we might now see thatDarwinis the most probable word, and select it.Conditional generation can even be used to accomplish tasks that must generatelonger responses. Consider the task oftext summarization, which is to take a longtextsummarizationtext, such as a full-length article, and produce an effective shorter summary of it.We can cast summarization as language modeling by giving a large language modela text, and follow the text by a token liketl;dr; this token is short for somethinglike ‘too long; don’t read’ and in recent years people often use this token, especiallyin informal work emails, when they are going to give a short summary. We canthen do conditional generation: give the language model this preﬁx, and then ask10.1•LARGELANGUAGEMODELS WITHTRANSFORMERS3
Preﬁx TextCompletion Text
EncoderTransformerBlocksSoftmax
longall
andthanksforallthe
the…UUUnencoder layerLanguage ModelingHeadlogits
So
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+…
Figure 10.1Left-to-right (also called autoregressive) text completion with transformer-based large languagemodels. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.word “negative” to see which is higher:P(positive|The sentiment of the sentence ‘‘I like Jackie Chan" is:)P(negative|The sentiment of the sentence ‘‘I like Jackie Chan" is:)If the word “positive” is more probable, we say the sentiment of the sentence ispositive, otherwise we say the sentiment is negative.We can also cast more complex tasks as word prediction. Consider questionanswering, in which the system is given a question (for example a question witha simple factual answer) and must give a textual answer; we introduce this task indetail in Chapter 15. We can cast the task of question answering as word predictionby giving a language model a question and a token likeA:suggesting that an answershould come next:Q: Who wrote the book ‘‘The Origin of Species"? A:If we ask a language model to compute the probability distribution over possiblenext words given this preﬁx:P(w|Q: Who wrote the book ‘‘The Origin of Species"? A:)and look at which wordswhave high probabilities, we might expect to see thatCharlesis very likely, and then if we chooseCharlesand continue and askP(w|Q: Who wrote the book ‘‘The Origin of Species"? A: Charles)we might now see thatDarwinis the most probable token, and select it.Conditional generation can even be used to accomplish tasks that must generatelonger responses. Consider the task oftext summarization, which is to take a longtextsummarizationtext, such as a full-length article, and produce an effective shorter summary of it. Wecan cast summarization as language modeling by giving a large language model atext, and follow the text by a token liketl;dr; this token is short for something like10.1•LARGELANGUAGEMODELS WITHTRANSFORMERS3
Preﬁx TextCompletion Text
EncoderTransformerBlocksSoftmax
longall
andthanksforallthe
the…UUUnencoder layerLanguage ModelingHeadlogits
So
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+
Ei+…
Figure 10.1Left-to-right (also called autoregressive) text completion with transformer-based large languagemodels. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.word “negative” to see which is higher:P(positive|The sentiment of the sentence ‘‘I like Jackie Chan" is:)P(negative|The sentiment of the sentence ‘‘I like Jackie Chan" is:)If the word “positive” is more probable, we say the sentiment of the sentence ispositive, otherwise we say the sentiment is negative.We can also cast more complex tasks as word prediction. Consider questionanswering, in which the system is given a question (for example a question witha simple factual answer) and must give a textual answer; we introduce this task indetail in Chapter 15. We can cast the task of question answering as word predictionby giving a language model a question and a token likeA:suggesting that an answershould come next:Q: Who wrote the book ‘‘The Origin of Species"? A:If we ask a language model to compute the probability distribution over possiblenext words given this preﬁx:P(w|Q: Who wrote the book ‘‘The Origin of Species"? A:)and look at which wordswhave high probabilities, we might expect to see thatCharlesis very likely, and then if we chooseCharlesand continue and askP(w|Q: Who wrote the book ‘‘The Origin of Species"? A: Charles)we might now see thatDarwinis the most probable token, and select it.Conditional generation can even be used to accomplish tasks that must generatelonger responses. Consider the task oftext summarization, which is to take a longtextsummarizationtext, such as a full-length article, and produce an effective shorter summary of it. Wecan cast summarization as language modeling by giving a large language model atext, and follow the text by a token liketl;dr; this token is short for something like

## Page 14

Summarization4CHAPTER10•LARGELANGUAGEMODELS‘too long; didn’t read’ and in recent years people often use this token, especially ininformal work emails, when they are going to give a short summary. Since this tokenis sufﬁciently frequent in language model training data, language models have seenmany texts in which the token occurs before a summary, and hence will interpret thetoken as instructions to generate a summary. We can then do conditional generation:give the language model this preﬁx, and then have it generate the following words,one by one, and take the entire response as a summary. Fig.10.2shows an exampleof a text and a human-produced summary from a widely-used summarization corpusconsisting of CNN and Daily Mirror news articles.Original ArticleThe only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuffand offering it for sale online? People are actually buying it. For $89, self-styled entrepreneurKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enoughfor 10 to 15 snowballs, he says.But not if you live in New England or surrounding states. “We will not ship snow to any statesin the northeast!” says Waring’s website, ShipSnowYo.com. “We’re in the business of expungingsnow!”His website and social media accounts claim to have ﬁlled more than 133 orders for snow – morethan 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set arecord this winter for the snowiest month in its history. Most residents see the huge piles of snowchoking their yards and sidewalks as a nuisance, but Waring saw an opportunity.According to Boston.com, it all started a few weeks ago, when Waring and his wife were shov-eling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston. Hejoked about shipping the stuff to friends and family in warmer states, and an idea was born. [...]SummaryKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enoughfor 10 to 15 snowballs, he says. But not if you live in New England or surrounding states.Figure 10.2Excerpt from a sample article and its summary from the CNN/Daily Mail summarization corpus(Hermann et al.,2015), (Nallapati et al.,2016).If we take this full article and append the tokentl;dr, we can use this as the con-text to prime the generation process to produce a summary as illustrated in Fig.10.3.Again, what makes transformers able to succeed at this task (as compared, say, tothe primitive n-gram language model) is that attention can incorporate informationfrom the large context window, giving the model access to the original article as wellas to the newly generated text throughout the process.Which words do we generate at each step? One simple way to generate wordsis to always generate the most likely word given the context. Generating the mostlikely word given the context is calledgreedy decoding. A greedy algorithm is onegreedydecodingthat make a choice that is locally optimal, whether or not it will turn out to havebeen the best choice with hindsight. Thus in greedy decoding, at each time step ingeneration, the outputytis chosen by computing the probability for each possibleoutput (every word in the vocabulary) and then choosing the highest probabilityword (the argmax):ˆwt=argmaxw2VP(w|w<t)(10.1)In practice, however, we don’t use greedy decoding with large language models.A major problem with greedy decoding is that because the words it chooses are (bydeﬁnition) extremely predictable, the resulting text is generic and often quite repeti-tive. Indeed, greedy decoding is so predictable that it is deterministic; if the context4CHAPTER10•LARGELANGUAGEMODELS‘too long; didn’t read’ and in recent years people often use this token, especially ininformal work emails, when they are going to give a short summary. Since this tokenis sufﬁciently frequent in language model training data, language models have seenmany texts in which the token occurs before a summary, and hence will interpret thetoken as instructions to generate a summary. We can then do conditional generation:give the language model this preﬁx, and then have it generate the following words,one by one, and take the entire response as a summary. Fig.10.2shows an exampleof a text and a human-produced summary from a widely-used summarization corpusconsisting of CNN and Daily Mirror news articles.Original ArticleThe only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuffand offering it for sale online? People are actually buying it. For $89, self-styled entrepreneurKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enoughfor 10 to 15 snowballs, he says.But not if you live in New England or surrounding states. “We will not ship snow to any statesin the northeast!” says Waring’s website, ShipSnowYo.com. “We’re in the business of expungingsnow!”His website and social media accounts claim to have ﬁlled more than 133 orders for snow – morethan 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set arecord this winter for the snowiest month in its history. Most residents see the huge piles of snowchoking their yards and sidewalks as a nuisance, but Waring saw an opportunity.According to Boston.com, it all started a few weeks ago, when Waring and his wife were shov-eling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston. Hejoked about shipping the stuff to friends and family in warmer states, and an idea was born. [...]SummaryKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enoughfor 10 to 15 snowballs, he says. But not if you live in New England or surrounding states.Figure 10.2Excerpt from a sample article and its summary from the CNN/Daily Mail summarization corpus(Hermann et al.,2015), (Nallapati et al.,2016).If we take this full article and append the tokentl;dr, we can use this as the con-text to prime the generation process to produce a summary as illustrated in Fig.10.3.Again, what makes transformers able to succeed at this task (as compared, say, tothe primitive n-gram language model) is that attention can incorporate informationfrom the large context window, giving the model access to the original article as wellas to the newly generated text throughout the process.Which words do we generate at each step? One simple way to generate wordsis to always generate the most likely word given the context. Generating the mostlikely word given the context is calledgreedy decoding. A greedy algorithm is onegreedydecodingthat make a choice that is locally optimal, whether or not it will turn out to havebeen the best choice with hindsight. Thus in greedy decoding, at each time step ingeneration, the outputytis chosen by computing the probability for each possibleoutput (every word in the vocabulary) and then choosing the highest probabilityword (the argmax):ˆwt=argmaxw2VP(w|w<t)(10.1)In practice, however, we don’t use greedy decoding with large language models.A major problem with greedy decoding is that because the words it chooses are (bydeﬁnition) extremely predictable, the resulting text is generic and often quite repeti-tive. Indeed, greedy decoding is so predictable that it is deterministic; if the contextOriginal
Summary

## Page 15

LLMs for summarization (using  tl;dr)
Original StoryGenerated Summary
…ideaKyle
wasborn.KyleWaring
WaringonlyThe…will
DelimiterwillUUU
tl;drLM Head
E
E
E
E
E
E
E
E…

## Page 16

Large Language ModelsLarge Language Models: What tasks can they do?

## Page 17

Large Language ModelsSampling for LLM Generation

## Page 18

Decoding and SamplingThis task of choosing a word to generate based on the model’s probabilities is called decoding. The most common method for decoding in LLMs: sampling. Sampling from a model’s distribution over words:•choose random words according to their probability assigned by the model. After each token we’ll sample words to generate according to their probability conditioned on our previous choices, •A transformer language model will give the probability

## Page 19

Random sampling6CHAPTER10•LARGELANGUAGEMODELSas deﬁned by the model. Thus we are more likely to generate words that the modelthinks have a high probability in the context and less likely to generate words thatthe model thinks have a low probability.We saw back in Chapter 3 on page??how to generate text from a unigram lan-guage model , by repeatedly randomly sampling words according to their probabilityuntil we either reach a pre-determined length or select the end-of-sentence token. Togenerate text from a trained transformer language model we’ll just generalize thismodel a bit: at each step we’ll sample words according to their probabilitycondi-tioned on our previous choices, and we’ll use a transformer language model as theprobability model that tells us this probability.We can formalize this algorithm for generating a sequence of wordsW=w1,w2,...,wNuntil we hit the end-of-sequence token, usingx⇠p(x)to mean ‘choosexby sam-pling from the distributionp(x):i 1wi⇠p(w)whilewi!= EOSi i+1wi⇠p(wi|w<i)The algorithm above is calledrandom sampling, and it turns out random sam-randomsamplingpling doesn’t work well enough. The problem is that even though random samplingis mostly going to generate sensible, high-probable words, there are many odd, low-probability words in the tail of the distribution, and even though each one is low-probability, if you add up all the rare words, they constitute a large enough portionof the distribution that they get chosen often enough to result in generating weirdsentences. For this reason, instead of random sampling, we usually use samplingmethods that avoid generating the very unlikely words.The sampling methods we introduce below each have parameters that enabletrading off two important factors in generation:qualityanddiversity. Methodsthat emphasize the most probable words tend to produce generations that are ratedby people as more accurate, more coherent, and more factual, but also more boringand more repetitive. Methods that give a bit more weight to the middle-probabilitywords tend to be more creative and more diverse, but less factual and more likely tobe incoherent or otherwise low-quality.10.2.1 Top-ksamplingTop-k samplingis a simple generalization of greedy decoding. Instead of choosingtop-k samplingthe single most probable word to generate, we ﬁrst truncate the distribution to thetopkmost likely words, renormalize to produce a legitimate probability distribution,and then randomly sample from within thesekwords according to their renormalizedprobabilities. More formally:1.Choose in advance a number of wordsk2.For each word in the vocabularyV, use the language model to compute thelikelihood of this word given the contextp(wt|w<t)3.Sort the words by their likelihood, and throw away any word that is not one ofthe topkmost probable words.4.Renormalize the scores of thekwords to be a legitimate probability distribu-tion.

## Page 20

Random sampling doesn't work very wellEven though random sampling mostly generate sensible, high-probable words, There are many odd, low- probability words in the tail of the distribution Each one is low- probability but added up they constitute a large portion of the distribution So they get picked enough to generate weird sentences

## Page 21

Factors in word sampling: quality and diversityEmphasize high-probability words  + quality: more  accurate, coherent, and factual, - diversity: boring, repetitive. Emphasize middle-probability words + diversity: more creative, diverse, - quality: less factual, incoherent

## Page 22

Top-k sampling:1. Choose # of words k 2. For each word in the vocabulary V , use the language model to compute the likelihood of this word given the context p(wt |w<t ) 3. Sort the words by likelihood, keep only the top k most probable words. 4. Renormalize the scores of the k words to be a legitimate probability distribution. 5. Randomly sample a word from within these remaining k most-probable words according to its probability. 

## Page 23

Top-p sampling (= nucleus sampling)Problem with top-k:  k is fixed so may cover very different amounts of probability mass in different situationsIdea: Instead, keep the top p percent of the probability massGiven a distribution P(wt |w<t ), the top-p vocabulary V ( p) is the smallest set of words such that Holtzman et al., 2020 10.2•SAMPLING FORLLM GENERATION75.Randomly sample a word from within these remainingkmost-probable wordsaccording to its probability.Whenk=1, top-ksampling is identical to greedy decoding. Settingkto a largernumber than 1 leads us to sometimes select a word which is not necessarily the mostprobable, but is still probable enough, and whose choice results in generating morediverse but still high-enough-quality text.10.2.2 Nucleus or top-psamplingOne problem with top-ksampling is thatkis ﬁxed, but the shape of the probabilitydistribution over words differs in different contexts. If we setk=10, sometimesthe top 10 words will be very likely and include most of the probability mass, butother times the probability distribution will be ﬂatter and the top 10 words will onlyinclude a small part of the probability mass.An alternative, calledtop-p samplingornucleus sampling(Holtzman et al.,top-p sampling2020), is to keep not the topkwords, but the topppercent of the probability mass.The goal is the same; to truncate the distribution to remove the very unlikely words.But by measuring probability rather than the number of words, the hope is that themeasure will be more robust in very different contexts, dynamically increasing anddecreasing the pool of word candidates.Given a distributionP(wt|w<t), the top-pvocabularyV(p)is the smallest set ofwords such thatXw2V(p)P(w|w<t) p.(10.2)10.2.3 Temperature samplingIntemperature sampling, we don’t truncate the distribution, but instead reshapetemperaturesamplingit. The intuition for temperature sampling comes from thermodynamics, where asystem at a high temperature is very ﬂexible and can explore many possible states,while a system at a lower temperature is likely to explore a subset of lower energy(better) states. In low-temperature sampling, we smoothly increase the probabilityof the most probable words and decrease the probability of the rare words.We implement this intuition by simply dividing the logit by a temperature param-etertbefore we normalize it by passing it through the softmax. In low-temperaturesampling,t2(0,1]. Thus instead of computing the probability distribution over thevocabulary directly from the logit as in the following (repeated from (??)):y=softmax(u)(10.3)we instead ﬁrst divide the logits byt, computing the probability vectoryasy=softmax(u/t)(10.4)Why does this work? Whentis close to 1 the distribution doesn’t change much.But the lowertis, the larger the scores being passed to the softmax (dividing by asmaller fractiont1 results in making each score larger). Recall that one of theuseful properties of a softmax is that it tends to push high values toward 1 and lowvalues toward 0. Thus when larger numbers are passed to a softmax the result isa distribution with increased probabilities of the most high-probability words anddecreased probabilities of the low probability words, making the distribution moregreedy. Astapproaches 0 the probability of the most likely word approaches 1.

## Page 24

Temperature samplingReshape the distribution instead of truncating itIntuition from thermodynamics, •a system at high temperature is flexible and can explore many possible states,•a system at lower temperature is likely to explore a subset of lower energy (better) states. In low-temperature sampling,  (τ ≤ 1) we smoothly•increase the probability of the most probable words•decrease the probability of the rare words. 

## Page 25

Temperature samplingDivide the logit by a temperature parameter τ before passing it through the softmax.Instead ofWe do  10.2•SAMPLING FORLLM GENERATION75.Randomly sample a word from within these remainingkmost-probable wordsaccording to its probability.Whenk=1, top-ksampling is identical to greedy decoding. Settingkto a largernumber than 1 leads us to sometimes select a word which is not necessarily the mostprobable, but is still probable enough, and whose choice results in generating morediverse but still high-enough-quality text.10.2.2 Nucleus or top-psamplingOne problem with top-ksampling is thatkis ﬁxed, but the shape of the probabilitydistribution over words differs in different contexts. If we setk=10, sometimesthe top 10 words will be very likely and include most of the probability mass, butother times the probability distribution will be ﬂatter and the top 10 words will onlyinclude a small part of the probability mass.An alternative, calledtop-p samplingornucleus sampling(Holtzman et al.,top-p sampling2020), is to keep not the topkwords, but the topppercent of the probability mass.The goal is the same; to truncate the distribution to remove the very unlikely words.But by measuring probability rather than the number of words, the hope is that themeasure will be more robust in very different contexts, dynamically increasing anddecreasing the pool of word candidates.Given a distributionP(wt|w<t), the top-pvocabularyV(p)is the smallest set ofwords such thatXw2V(p)P(w|w<t) p.(10.2)10.2.3 Temperature samplingIntemperature sampling, we don’t truncate the distribution, but instead reshapetemperaturesamplingit. The intuition for temperature sampling comes from thermodynamics, where asystem at a high temperature is very ﬂexible and can explore many possible states,while a system at a lower temperature is likely to explore a subset of lower energy(better) states. In low-temperature sampling, we smoothly increase the probabilityof the most probable words and decrease the probability of the rare words.We implement this intuition by simply dividing the logit by a temperature param-etertbefore we normalize it by passing it through the softmax. In low-temperaturesampling,t2(0,1]. Thus instead of computing the probability distribution over thevocabulary directly from the logit as in the following (repeated from (??)):y=softmax(u)(10.3)we instead ﬁrst divide the logits byt, computing the probability vectoryasy=softmax(u/t)(10.4)Why does this work? Whentis close to 1 the distribution doesn’t change much.But the lowertis, the larger the scores being passed to the softmax (dividing by asmaller fractiont1 results in making each score larger). Recall that one of theuseful properties of a softmax is that it tends to push high values toward 1 and lowvalues toward 0. Thus when larger numbers are passed to a softmax the result isa distribution with increased probabilities of the most high-probability words anddecreased probabilities of the low probability words, making the distribution moregreedy. Astapproaches 0 the probability of the most likely word approaches 1.10.2•SAMPLING FORLLM GENERATION75.Randomly sample a word from within these remainingkmost-probable wordsaccording to its probability.Whenk=1, top-ksampling is identical to greedy decoding. Settingkto a largernumber than 1 leads us to sometimes select a word which is not necessarily the mostprobable, but is still probable enough, and whose choice results in generating morediverse but still high-enough-quality text.10.2.2 Nucleus or top-psamplingOne problem with top-ksampling is thatkis ﬁxed, but the shape of the probabilitydistribution over words differs in different contexts. If we setk=10, sometimesthe top 10 words will be very likely and include most of the probability mass, butother times the probability distribution will be ﬂatter and the top 10 words will onlyinclude a small part of the probability mass.An alternative, calledtop-p samplingornucleus sampling(Holtzman et al.,top-p sampling2020), is to keep not the topkwords, but the topppercent of the probability mass.The goal is the same; to truncate the distribution to remove the very unlikely words.But by measuring probability rather than the number of words, the hope is that themeasure will be more robust in very different contexts, dynamically increasing anddecreasing the pool of word candidates.Given a distributionP(wt|w<t), the top-pvocabularyV(p)is the smallest set ofwords such thatXw2V(p)P(w|w<t) p.(10.2)10.2.3 Temperature samplingIntemperature sampling, we don’t truncate the distribution, but instead reshapetemperaturesamplingit. The intuition for temperature sampling comes from thermodynamics, where asystem at a high temperature is very ﬂexible and can explore many possible states,while a system at a lower temperature is likely to explore a subset of lower energy(better) states. In low-temperature sampling, we smoothly increase the probabilityof the most probable words and decrease the probability of the rare words.We implement this intuition by simply dividing the logit by a temperature param-etertbefore we normalize it by passing it through the softmax. In low-temperaturesampling,t2(0,1]. Thus instead of computing the probability distribution over thevocabulary directly from the logit as in the following (repeated from (??)):y=softmax(u)(10.3)we instead ﬁrst divide the logits byt, computing the probability vectoryasy=softmax(u/t)(10.4)Why does this work? Whentis close to 1 the distribution doesn’t change much.But the lowertis, the larger the scores being passed to the softmax (dividing by asmaller fractiont1 results in making each score larger). Recall that one of theuseful properties of a softmax is that it tends to push high values toward 1 and lowvalues toward 0. Thus when larger numbers are passed to a softmax the result isa distribution with increased probabilities of the most high-probability words anddecreased probabilities of the low probability words, making the distribution moregreedy. Astapproaches 0 the probability of the most likely word approaches 1.

## Page 26

Temperature samplingWhy does this work?•When τ is close to 1 the distribution doesn’t change much. •The lower τ is, the larger the scores being passed to the softmax•Softmax pushes high values toward 1 and low values toward 0. •Large inputs pushes high-probability words higher and low probability word lower,  making the distribution more greedy. •As τ approaches 0, the probability of most likely word approaches 1 10.2•SAMPLING FORLLM GENERATION75.Randomly sample a word from within these remainingkmost-probable wordsaccording to its probability.Whenk=1, top-ksampling is identical to greedy decoding. Settingkto a largernumber than 1 leads us to sometimes select a word which is not necessarily the mostprobable, but is still probable enough, and whose choice results in generating morediverse but still high-enough-quality text.10.2.2 Nucleus or top-psamplingOne problem with top-ksampling is thatkis ﬁxed, but the shape of the probabilitydistribution over words differs in different contexts. If we setk=10, sometimesthe top 10 words will be very likely and include most of the probability mass, butother times the probability distribution will be ﬂatter and the top 10 words will onlyinclude a small part of the probability mass.An alternative, calledtop-p samplingornucleus sampling(Holtzman et al.,top-p sampling2020), is to keep not the topkwords, but the topppercent of the probability mass.The goal is the same; to truncate the distribution to remove the very unlikely words.But by measuring probability rather than the number of words, the hope is that themeasure will be more robust in very different contexts, dynamically increasing anddecreasing the pool of word candidates.Given a distributionP(wt|w<t), the top-pvocabularyV(p)is the smallest set ofwords such thatXw2V(p)P(w|w<t) p.(10.2)10.2.3 Temperature samplingIntemperature sampling, we don’t truncate the distribution, but instead reshapetemperaturesamplingit. The intuition for temperature sampling comes from thermodynamics, where asystem at a high temperature is very ﬂexible and can explore many possible states,while a system at a lower temperature is likely to explore a subset of lower energy(better) states. In low-temperature sampling, we smoothly increase the probabilityof the most probable words and decrease the probability of the rare words.We implement this intuition by simply dividing the logit by a temperature param-etertbefore we normalize it by passing it through the softmax. In low-temperaturesampling,t2(0,1]. Thus instead of computing the probability distribution over thevocabulary directly from the logit as in the following (repeated from (??)):y=softmax(u)(10.3)we instead ﬁrst divide the logits byt, computing the probability vectoryasy=softmax(u/t)(10.4)Why does this work? Whentis close to 1 the distribution doesn’t change much.But the lowertis, the larger the scores being passed to the softmax (dividing by asmaller fractiont1 results in making each score larger). Recall that one of theuseful properties of a softmax is that it tends to push high values toward 1 and lowvalues toward 0. Thus when larger numbers are passed to a softmax the result isa distribution with increased probabilities of the most high-probability words anddecreased probabilities of the low probability words, making the distribution moregreedy. Astapproaches 0 the probability of the most likely word approaches 1.0 ≤ τ ≤ 1 

## Page 27

Large Language ModelsSampling for LLM Generation

## Page 28

Large Language ModelsPretraining Large Language Models: Algorithm

## Page 29

PretrainingThe big idea that underlies all the amazing performance of language modelsFirst pretrain a transformer model on enormous amounts of textThen apply it to new tasks.

## Page 30

Self-supervised training algorithmWe just train them to predict the next word!1.Take a corpus of text 2.At each time step t i.ask the model to predict the next word ii.train the model using gradient descent to minimize the error in this prediction"Self-supervised" because it just uses the next word as the label!

## Page 31

Intuition of language model training: loss•Same loss function: cross-entropy loss•We want the model to assign a high probability to true word w•= want loss to be high if the model assigns too low a probability to w•CE Loss: The negative log probability that the model assigns to the true next word w•If the model assigns too low a probability to w•We move the model weights in the direction that assigns a higher probability to w

## Page 32

Cross-entropy loss for language modelingCE loss: difference between the correct probability distribution and the predicted distribution The correct distribution yt knows the next word, so is 1 for the actual next word and 0 for the others.So in this sum, all terms get multiplied by zero except one: the logp the model assigns to the correct next word, so: 8CHAPTER10•LARGELANGUAGEMODELSNote, by the way, that there can be other situations where we may want to dosomething quite different and ﬂatten the word probability distribution instead ofmaking it greedy. Temperature sampling can help with this situation too, in this casehigh-temperaturesampling, in which case we uset>1.10.3 Pretraining Large Language ModelsHow do we teach a transformer to be a language model? What is the algorithm andwhat data do we train on?10.3.1 Self-supervised training algorithmTo train a transformer as a language model, we use the sameself-supervision(orself-supervisionself-training) algorithm we saw in Section??: we take a corpus of text as trainingmaterial and at each time steptask the model to predict the next word. We callsuch a model self-supervised because we don’t have to add any special gold labelsto the data; the natural sequence of words is its own supervision! We simply train themodel to minimize the error in predicting the true next word in the training sequence,using cross-entropy as the loss function.Recall that the cross-entropy loss measures the difference between a predictedprobability distribution and the correct distribution.LCE= Xw2Vyt[w]logˆyt[w](10.5)In the case of language modeling, the correct distributionytcomes from knowing thenext word. This is represented as a one-hot vector corresponding to the vocabularywhere the entry for the actual next word is 1, and all the other entries are 0. Thus,the cross-entropy loss for language modeling is determined by the probability themodel assigns to the correct next word (all other words get multiplied by zero). Soat timetthe CE loss in (10.5) can be simpliﬁed as the negative log probability themodel assigns to the next word in the training sequence.LCE(ˆyt,yt)= logˆyt[wt+1](10.6)Thus at each word positiontof the input, the model takes as input the correct se-quence of tokensw1:t, and uses them to compute a probability distribution overpossible next words so as to compute the model’s loss for the next tokenwt+1. Thenwe move to the next word, we ignore what the model predicted for the next wordand instead use the correct sequence of tokensw1:t+1to estimate the probability oftokenwt+2. This idea that we always give the model the correct history sequence topredict the next word (rather than feeding the model its best case from the previoustime step) is calledteacher forcing.teacher forcingFig.10.4illustrates the general training approach. At each step, given all thepreceding words, the ﬁnal transformer layer produces an output distribution overthe entire vocabulary. During training, the probability assigned to the correct wordis used to calculate the cross-entropy loss for each item in the sequence. The lossfor a training sequence is the average cross-entropy loss over the entire sequence.The weights in the network are adjusted to minimize the average CE loss over thetraining sequence via gradient descent.8CHAPTER10•LARGELANGUAGEMODELSNote, by the way, that there can be other situations where we may want to dosomething quite different and ﬂatten the word probability distribution instead ofmaking it greedy. Temperature sampling can help with this situation too, in this casehigh-temperaturesampling, in which case we uset>1.10.3 Pretraining Large Language ModelsHow do we teach a transformer to be a language model? What is the algorithm andwhat data do we train on?10.3.1 Self-supervised training algorithmTo train a transformer as a language model, we use the sameself-supervision(orself-supervisionself-training) algorithm we saw in Section??: we take a corpus of text as trainingmaterial and at each time steptask the model to predict the next word. We callsuch a model self-supervised because we don’t have to add any special gold labelsto the data; the natural sequence of words is its own supervision! We simply train themodel to minimize the error in predicting the true next word in the training sequence,using cross-entropy as the loss function.Recall that the cross-entropy loss measures the difference between a predictedprobability distribution and the correct distribution.LCE= Xw2Vyt[w]logˆyt[w](10.5)In the case of language modeling, the correct distributionytcomes from knowing thenext word. This is represented as a one-hot vector corresponding to the vocabularywhere the entry for the actual next word is 1, and all the other entries are 0. Thus,the cross-entropy loss for language modeling is determined by the probability themodel assigns to the correct next word (all other words get multiplied by zero). Soat timetthe CE loss in (10.5) can be simpliﬁed as the negative log probability themodel assigns to the next word in the training sequence.LCE(ˆyt,yt)= logˆyt[wt+1](10.6)Thus at each word positiontof the input, the model takes as input the correct se-quence of tokensw1:t, and uses them to compute a probability distribution overpossible next words so as to compute the model’s loss for the next tokenwt+1. Thenwe move to the next word, we ignore what the model predicted for the next wordand instead use the correct sequence of tokensw1:t+1to estimate the probability oftokenwt+2. This idea that we always give the model the correct history sequence topredict the next word (rather than feeding the model its best case from the previoustime step) is calledteacher forcing.teacher forcingFig.10.4illustrates the general training approach. At each step, given all thepreceding words, the ﬁnal transformer layer produces an output distribution overthe entire vocabulary. During training, the probability assigned to the correct wordis used to calculate the cross-entropy loss for each item in the sequence. The lossfor a training sequence is the average cross-entropy loss over the entire sequence.The weights in the network are adjusted to minimize the average CE loss over thetraining sequence via gradient descent.

## Page 33

Teacher forcing•At each token position t, model sees correct tokens w1:t, •Computes  loss (–log probability) for the next token wt+1 •At next token position t+1 we ignore what model predicted for wt+1 •Instead we take the correct word wt+1, add it to context, move on

## Page 34

Training a transformer language model
longandthanksforNext tokenallLoss…=
<latexit sha1_base64="AovqpaL476UmJ1EU1xZPgDZ70tQ=">AAAB9nicbVDLSsNAFL2pr1pfURcu3AwWwY0lEakui25cVrAPaEqYTCbt0EkmzEzEEvIrbkTcKPgZ/oJ/Y9Jm09YDA4dzznDvPV7MmdKW9WtU1tY3Nreq27Wd3b39A/PwqKtEIgntEMGF7HtYUc4i2tFMc9qPJcWhx2nPm9wXfu+ZSsVE9KSnMR2GeBSxgBGsc8k1Ty4dLkZo6qZOiPVYhimO/CyruWbdalgzoFVil6QOJdqu+eP4giQhjTThWKmBbcV6mGKpGeE0qzmJojEmEzyi6WztDJ3nko8CIfMXaTRTF3I4VGoaenmy2E0te4X4nzdIdHA7TFkUJ5pGZD4oSDjSAhUdIJ9JSjSf5gQTyfINERljiYnOmypOt5cPXSXdq4bdbDQfr+utu7KEKpzCGVyADTfQggdoQwcIZPAGn/BlvBivxrvxMY9WjPLPMSzA+P4DPEiSHA==</latexit> logyand
StackedTransformerBlocksSolongandthanksfor……
…
U
Input tokensx1x2LanguageModelingHeadx3x4x5InputEncoding
E1+
E2+
E3+
E4+
E5+……………
U
U
U
U…logitslogitslogitslogitslogits…
<latexit sha1_base64="q3ZgXDyG7qtkT7t8hT47RdlwYG4=">AAAB+XicbVDLSsNAFJ3UV62vWHe6GVsEN5bERXUlBUVcVrAPaEqYTCft0MlMmJkIIQT8AT/CTRE3Cv6Ev+DfmLTdtPXAwOGcM9x7jxcyqrRl/RqFtfWNza3idmlnd2//wDwst5WIJCYtLJiQXQ8pwignLU01I91QEhR4jHS88W3ud56JVFTwJx2HpB+gIac+xUhnkmseXzhMDGHsJk6A9EgGiR4hPlZpWnLNqlWzpoCrxJ6TauP0tXw3qdw0XfPHGQgcBYRrzJBSPdsKdT9BUlPMSFpyIkVChMdoSJLp5ik8y6QB9IXMHtdwqi7kUKBUHHhZMl9PLXu5+J/Xi7R/3U8oDyNNOJ4N8iMGtYB5DXBAJcGaxRlBWNJsQ4hHSCKss7Ly0+3lQ1dJ+7Jm12v1x6yDezBDEZyACjgHNrgCDfAAmqAFMHgBE/AJvozEeDPejY9ZtGDM/xyBBRjff79pldo=</latexit> logythanks

## Page 35

Large Language ModelsPretraining Large Language Models: Algorithm

## Page 36

Large Language ModelsPretraining data for LLMs

## Page 37

LLMs are mainly trained on the webCommon crawl, snapshots of the entire web produced by the non- profit Common Crawl with billions of pagesColossal Clean Crawled Corpus (C4; Raffel et al. 2020), 156 billion tokens of English,  filtered What's in it? Mostly patent text documents, Wikipedia, and news sites 

## Page 38

The Pile: a pretraining corpus
Figure 1: Treemap of Pile components by effective size.troduce a new ﬁltered subset of Common Crawl,Pile-CC, with improved extraction quality.Through our analyses, we conﬁrm that the Pile issigniﬁcantly distinct from pure Common Crawldata. Additionally, our evaluations show that theexisting GPT-2 and GPT-3 models perform poorlyon many components of the Pile, and that modelstrained on the Pile signiﬁcantly outperform bothraw and ﬁltered Common Crawl models. To com-plement the performance evaluations, we also per-form an exploratory analysis of the text within thePile to provide a detailed picture of the data. Wehope that our extensive documentation of the con-struction and characteristics of the Pile will helpresearchers make informed decisions about poten-tial downstream applications.Finally, we make publicly available the preprocess-ing code for the constituent datasets of the Pile andthe code for constructing alternative versions2. Inthe interest of reproducibility, we also documentall processing performed on each dataset (and thePile as a whole) in as much detail as possible. Forfurther details about the processing of each dataset,see Section2and AppendixC.2https://github.com/EleutherAI/the-pile1.1 ContributionsThe core contributions of this paper are:1.The introduction of a825.18GiB english-language dataset for language modeling com-bining 22 diverse sources.2.The introduction of14new language model-ing datasets, which we expect to be of inde-pendent interest to researchers.3.Evaluations demonstrating signiﬁcant im-provements across many domains by GPT-2-sized models trained on this new dataset, com-pared to training on CC-100 and raw CommonCrawl.4.The investigation and documentation of thisdataset, which we hope will better inform re-searchers about how to use it as well as moti-vate them to undertake similar investigationsof their own data.2 The Pile DatasetsThe Pile is composed of 22 constituent sub-datasets,as shown in Table1. FollowingBrown et al.(2020),we increase the weights of higher quality compo-nents, with certain high-quality datasets such asWikipedia being seen up to 3 times (“epochs”) for2webacademicsbooks
dialog

## Page 39

Filtering for quality and safetyQuality is subjective•Many LLMs attempt to match Wikipedia, books, particular websites•Need to remove boilerplate, adult content•Deduplication at many levels (URLs, documents, even lines)Safety also subjective•Toxicity detection is important, although that has mixed results•Can mistakenly flag data written in dialects like African American English

## Page 40

What does a model learn from pretraining?•There are canines everywhere! One dog in the front room, and two dogs•It wasn't just big it was enormous•The author of "A Room of One's Own" is Virginia Woolf•The doctor told me that he•The square root of 4 is 2

## Page 41

Big ideaText contains enormous amounts of knowledgePretraining on lots of text with all that knowledge is what gives language models their ability to do so much

## Page 42

But there are problems with scraping from the webCopyright: much of the text in these datasets is copyrighted•Not clear if fair use doctrine in US allows for this use•This remains an open legal questionData consent•Website owners can indicate they don't want their site crawledPrivacy: •Websites can contain private IP addresses and phone numbers

## Page 43

Large Language ModelsPretraining data for LLMs

## Page 44

Large Language ModelsFinetuning

## Page 45

Finetuning for daptation to new domainsWhat happens if we need our LLM to work well on a domain it didn't see in pretraining?Perhaps some specific medical or legal domain?Or maybe a multilingual LM needs to see more data on some language that was rare in pretraining?

## Page 46

FinetuningFine-tuning Data
Pretraining DataPretraining
…
…
…Fine-tuning
…
…
…Pretrained LMFine-tuned LM

## Page 47

"Finetuning" means 4 different thingsWe'll discuss 1 here, and 3 in later lecturesIn all four cases, finetuning means:taking a pretrained model and further adapting some or all of its parameters to some new data

## Page 48

1. Finetuning as "continued pretraining" on new data•Further train all the parameters of model on new data•using the same method (word prediction) and loss function (cross-entropy loss) as for pretraining.•as if the new data were at the tail end of the pretraining data•Hence sometimes called continued pretraining

## Page 49

Large Language ModelsFinetuning

## Page 50

Large Language ModelsEvaluating Large Language Models

## Page 51

PerplexityJust as for n-gram grammars, we use perplexity to measure how well the LM predicts unseen textThe perplexity of a model θ on an unseen test set is the inverse probability that θ assigns to the test set, normalized by the test set length. For a test set of n tokens w1:n the perplexity is :12CHAPTER10•LARGELANGUAGEMODELSthe pretraining data, and so you’ll sometimes see this method calledcontinued pre-training.continuedpretrainingRetraining all the parameters of the model is very slow and expensive when thelanguage model is huge. So instead we canfreezesome of the parameters (i.e., leavefreezethem unchanged from their pretrained value) and train only a subset of parameterson the new data. In Section10.5.3we’ll describe this second variety of ﬁnetun-ing, calledparameter-efﬁcient ﬁnetuning, orPEFT. because we efﬁciently selectspeciﬁc parameters to update when ﬁnetuning, and leave the rest in their pretrainedvalues.In Chapter 11 we’ll introduce a third kind of ﬁnetuning, also parameter-efﬁcient.In this version, the goal is to use a language model as a kind of classiﬁer or labelerfor a speciﬁc task. For example we might train the model to be a sentiment classiﬁer.We do this by adding extra neural circuitry (an extrahead) after the top layer of themodel. This classiﬁcation head takes as input some of the top layer embeddings ofthe transformer and produces as output a classiﬁcation. In this method, most com-monly used with masked language models like BERT, we freeze the entire pretrainedmodel and only train the classiﬁcation head on some new data, usually labeled withsome class that we want to predict.Finally, in Chapter 12 we’ll introduce a fourth kind of ﬁnetuning, that is a cru-cial component of the largest language models:supervised ﬁnetuningorSFT. SFTis often used forinstruction ﬁnetuning, in which we want a pretrained languagemodel to learn to follow text instructions, for example to answer questions or followa command to write something. Here we create a dataset of prompts and desiredresponses (for example questions and their answers, or commands and their ful-ﬁllments), and we train the language model using the normal cross-entropy loss topredict each token in the instruction prompt iteratively, essentially training it to pro-duce the desired response from the command in the prompt. It’s called supervisedbecause unlike in pretraining, where we just take any data and predict the words init, we build the special ﬁnetuning dataset by hand, creating supervised responses toeach command.Often everything that happens after pretraining is lumped together aspost-training;we’ll discuss the various parts of post-training in Chapter 12 and Chapter 13.10.4 Evaluating Large Language ModelsPerplexityAs we ﬁrst saw in Chapter 3, one way to evaluate language models isto measure how well they predict unseen text. Intuitively, good models are those thatassign higher probabilities to unseen data (are less surprised when encountering thenew words).We instantiate this intuition by usingperplexityto measure the quality of aperplexitylanguage model. Recall from page??that the perplexity of a modelqon an unseentest set is the inverse probability thatqassigns to the test set, normalized by the testset length. For a test set ofntokensw1:n, the perplexity isPerplexityq(w1:n)=Pq(w1:n) 1n=ns1Pq(w1:n)(10.7)To visualize how perplexity can be computed as a function of the probabilities the

## Page 52

•Probability depends on size of test set•Probability gets smaller the longer the text•Better: a metric that is per-word, normalized by length•Perplexity is the inverse probability of the test set, normalized by the number of words(The inverse comes from the original definition of perplexity from cross-entropy rate in information theory)Probability range is  [0,1], perplexity range is [1,∞]Why perplexity instead of raw probability of the test set?

## Page 53

Perplexity•The higher the probability of the word sequence, the lower the perplexity.•Thus the lower the perplexity of a model on the data, the better the model. •Minimizing perplexity is the same as maximizing probabilityAlso: perplexity is sensitive to length/tokenization so best used when comparing LMs that use the same tokenizer.  

## Page 54

Many other factors that we evaluate, like:Size Big models take lots of GPUs and time to train, memory to storeEnergy usageCan measure kWh or kilograms of CO2 emitted FairnessBenchmarks measure gendered and racial stereotypes, or decreased performance for language from or about some groups. 

## Page 55

Large Language ModelsDealing with Scale

## Page 56

Scaling LawsLLM performance depends on•Model size: the number of parameters not counting embeddings•Dataset size: the amount of training data•Compute: Amount of compute (in FLOPS or etcCan improve a model by adding  parameters (more layers, wider contexts), more data, or training for more iterationsThe performance of a large language model (the loss) scales as a power-law with each of these three

## Page 57

Scaling LawsLoss L as a function of # parameters N, dataset size D, compute budget C (if other two are held constant)14CHAPTER10•LARGELANGUAGEMODELS10.5 Dealing with ScaleLarge language models are large. For example theLlama 3.1 405B Instructmodelfrom Meta has 405 billion parameters (126 layers, a model dimensionality of 16,384,128 attention heads) and was trained on 15.6 terabytes of text tokens (Llama Team,2024), using a vocabulary of 128K tokens. So there is a lot of research on un-derstanding how LLMs scale, and especially how to implement them given limitedresources. In the next few sections we discuss how to think about scale (the conceptofscaling laws), and important techniques for getting language models to workefﬁciently, such as theKV cacheand parameter-efﬁcient ﬁne tuning.10.5.1 Scaling lawsThe performance of large language models has shown to be mainly determined by3 factors: model size (the number of parameters not counting embeddings), datasetsize (the amount of training data), and the amount of compute used for training. Thatis, we can improve a model by adding parameters (adding more layers or havingwider contexts or both), by training on more data, or by training for more iterations.The relationships between these factors and performance are known asscalinglaws. Roughly speaking, the performance of a large language model (the loss) scalesscaling lawsas a power-law with each of these three properties of model training.For example,Kaplan et al.(2020) found the following three relationships forlossLas a function of the number of non-embedding parametersN, the dataset sizeD, and the compute budgetC, for models training with limited parameters, dataset,or compute budget, if in each case the other two properties are held constant:L(N)=✓NcN◆aN(10.9)L(D)=✓DcD◆aD(10.10)L(C)=✓CcC◆aC(10.11)The number of (non-embedding) parametersNcan be roughly computed as fol-lows (ignoring biases, and withdas the input and output dimensionality of themodel,dattnas the self-attention layer size, anddffthe size of the feedforward layer):N⇡2dnlayer(2dattn+dff)⇡12nlayerd2(10.12)(assumingdattn=dff/4=d)Thus GPT-3, withn=96 layers and dimensionalityd=12288, has 12⇥96⇥122882⇡175 billion parameters.The values ofNc,Dc,Cc,aN,aD, andaCdepend on the exact transformerarchitecture, tokenization, and vocabulary size, so rather than all the precise values,scaling laws focus on the relationship with loss.2Scaling laws can be useful in deciding how to train a model to a particular per-formance, for example by looking at early in the training curve, or performance with2For the initial experiment inKaplan et al.(2020) the precise values wereaN= 0.076,Nc= 8.8⇥1013(parameters),aD= 0.095,Dc= 5.4⇥1013(tokens),aC= 0.050,Cc= 3.1⇥108(petaﬂop-days).Scaling laws can be used early in training to predict what the loss would be if we were to add more data or increase model size. 

## Page 58

Number of non-embedding parameters N14CHAPTER10•LARGELANGUAGEMODELS10.5 Dealing with ScaleLarge language models are large. For example theLlama 3.1 405B Instructmodelfrom Meta has 405 billion parameters (126 layers, a model dimensionality of 16,384,128 attention heads) and was trained on 15.6 terabytes of text tokens (Llama Team,2024), using a vocabulary of 128K tokens. So there is a lot of research on un-derstanding how LLMs scale, and especially how to implement them given limitedresources. In the next few sections we discuss how to think about scale (the conceptofscaling laws), and important techniques for getting language models to workefﬁciently, such as theKV cacheand parameter-efﬁcient ﬁne tuning.10.5.1 Scaling lawsThe performance of large language models has shown to be mainly determined by3 factors: model size (the number of parameters not counting embeddings), datasetsize (the amount of training data), and the amount of compute used for training. Thatis, we can improve a model by adding parameters (adding more layers or havingwider contexts or both), by training on more data, or by training for more iterations.The relationships between these factors and performance are known asscalinglaws. Roughly speaking, the performance of a large language model (the loss) scalesscaling lawsas a power-law with each of these three properties of model training.For example,Kaplan et al.(2020) found the following three relationships forlossLas a function of the number of non-embedding parametersN, the dataset sizeD, and the compute budgetC, for models training with limited parameters, dataset,or compute budget, if in each case the other two properties are held constant:L(N)=✓NcN◆aN(10.9)L(D)=✓DcD◆aD(10.10)L(C)=✓CcC◆aC(10.11)The number of (non-embedding) parametersNcan be roughly computed as fol-lows (ignoring biases, and withdas the input and output dimensionality of themodel,dattnas the self-attention layer size, anddffthe size of the feedforward layer):N⇡2dnlayer(2dattn+dff)⇡12nlayerd2(10.12)(assumingdattn=dff/4=d)Thus GPT-3, withn=96 layers and dimensionalityd=12288, has 12⇥96⇥122882⇡175 billion parameters.The values ofNc,Dc,Cc,aN,aD, andaCdepend on the exact transformerarchitecture, tokenization, and vocabulary size, so rather than all the precise values,scaling laws focus on the relationship with loss.2Scaling laws can be useful in deciding how to train a model to a particular per-formance, for example by looking at early in the training curve, or performance with2For the initial experiment inKaplan et al.(2020) the precise values wereaN= 0.076,Nc= 8.8⇥1013(parameters),aD= 0.095,Dc= 5.4⇥1013(tokens),aC= 0.050,Cc= 3.1⇥108(petaﬂop-days).Thus GPT-3, with n = 96 layers and dimensionality d = 12288, has 12 × 96 × 122882 ≈ 175 billion parameters. 

## Page 59

KV CacheIn training, we can compute attention very efficiently in parallel:But not at inference! We generate the next tokens one at a time!For a new token x, need to multiply by WQ, WK, and WV to get query, key, valuesBut don't want to recompute the key and value vectors for all the prior tokens x<iInstead, store key and value vectors in memory in the KV cache, and then we can just grab them from the cache 10.5•DEALING WITHSCALE15smaller amounts of data, to predict what the loss would be if we were to add moredata or increase model size. Other aspects of scaling laws can also tell us how muchdata we need to add when scaling up a model.10.5.2 KV CacheWe saw in Fig.??and in Eq.??(repeated below) how the attention vector can bevery efﬁciently computed in parallel for training, via two matrix multiplications:A=softmax✓QK|pdk◆V(10.13)Unfortunately we can’t do quite the same efﬁcient computation in inference asin training. That’s because at inference time, we iteratively generate the next tokensone at a time. For a new token that we have just generated, call itxi, we need tocompute its query, key, and values by multiplying byWQ,WK, andWVrespec-tively. But it would be a waste of computation time to recompute the key and valuevectors for all thepriortokensx<i; at prior steps we already computed these keyand value vectors! So instead of recomputing these, whenever we compute the keyand value vectors we store them in memory in theKV cache, and then we can justKV cachegrab them from the cache when we need them. Fig.10.7modiﬁes Fig.??to showthe computation that takes place for a single new token, showing which values wecan take from the cache rather than recompute.
q4k1k2k4QKTQKTv1v2v3v4V
q4•k1q4•k2q4•k3q4•k4x==xa4A
1 x dkdk x N1 x NN x dv1 x dv
k3Figure 10.7Parts of the attention computation (extracted from Fig.??) showing, in black,the vectors that can be stored in the cache rather than recomputed when computing the atten-tion score for the 4th token.10.5.3 Parameter Efﬁcient Fine TuningAs we mentioned above, it’s very common to take a language model and give it moreinformation about a new domain byﬁnetuningit (continuing to train it to predictupcoming words) on some additional data.Fine-tuning can be very difﬁcult with very large language models, because thereare enormous numbers of parameters to train; each pass of batch gradient descenthas to backpropagate through many many huge layers. This makes ﬁnetuning hugelanguage models extremely expensive in processing power, in memory, and in time.For this reason, there are alternative methods that allow a model to be ﬁnetunedwithout changing all the parameters. Such methods are calledparameter-efﬁcientﬁne tuningor sometimesPEFT, because we efﬁciently select a subset of parametersparameter-efﬁcient ﬁnetuningPEFTto update when ﬁnetuning. For example we freeze some of the parameters (don’tchange them), and only update some particular subset of parameters.

## Page 60

KV Cache
q4k1k2k4QKTQKTv1v2v3v4V
q4•k1q4•k2q4•k3q4•k4x==xa4A
1 x dkdk x N1 x NN x dv1 x dv
k3q1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2•k2q4•k2q4•k3q4•k4q3•k2q3•k3−∞−∞−∞−∞−∞−∞q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3q1•k2q2•k3q1•k3q3•k4q2•k4q1•k4x=QKT maskedmask=q1•k1q2•k1q4•k1q3•k1q1•k1q1•k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X
N x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dvq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2•k2q4•k2q4•k3q4•k4q3•k2q3•k3−∞−∞−∞−∞−∞−∞q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3q1•k2q2•k3q1•k3q3•k4q2•k4q1•k4x=QKT maskedmask=q1•k1q2•k1q4•k1q3•k1q1•k1q1•k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X
N x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dvq1q2q3q4k1k2k3k4QKTQKTv1v2v3v4Vq2•k2q4•k2q4•k3q4•k4q3•k2q3•k3−∞−∞−∞−∞−∞−∞q1•k1q2•k1q2•k2q4•k1q4•k2q4•k3q4•k4q3•k1q3•k2q3•k3q1•k2q2•k3q1•k3q3•k4q2•k4q1•k4x=QKT maskedmask=q1•k1q2•k1q4•k1q3•k1q1•k1q1•k1=xa1a2a3a4AQuery Token 1Query Token 2Query Token 3Query Token 4QInput Token 1Input Token 2Input Token 3Input Token 4XxWQ=Value Token 1Value Token 2Value Token 3Value Token 4VxWV=Input Token 1Input Token 2Input Token 3Input Token 4XKey Token 1Key Token 2Key Token 3Key Token 4KxWK=Input Token 1Input Token 2Input Token 3Input Token 4X
N x dkdk x NN x NN x NN x dvN x dvd x dkd x dkd x dvN x dN x dkN x dN x dkN x dN x dv

## Page 61

Parameter-Efficient FinetuningAdapting to a new domain by continued pretraining (finetuning) is a problem with huge LLMs.•Enormous numbers of parameters to train •Each pass of batch gradient descent has to backpropagate through many many huge layers. •Expensive in processing power, in memory, and in time. Instead, parameter-efficient fine tuning (PEFT)•Efficiently select a subset of parameters to update when finetuning.•E.g., freeze some of the parameters (don’t change them), •And only update some a few parameters. 

## Page 62

LoRA (Low-Rank Adaptation)•Trransformers have many dense matrix multiply layers•Like WQ, WK, WV, WO layers in attention•Instead of updating these layers during finetuning, •Freeze these layers •Update a low-rank approximation with fewer parameters. 

## Page 63

LoRA•Consider a matrix W (shape [N × d])  that needs to be updated during finetuning via gradient descent. •Normally updates are ∆W  (shape [N × d])•In LoRA, we freeze W and update instead a low-rank decomposition of W:•A of shape [N×r], •B of shape [r×d], r is very small  (like 1 or 2)•That is, during  finetuning we update A and B instead of W. •Replace W + ∆W with W + BA. Forward pass: instead of     h = xW We do     h = xW + xAB 

## Page 64

LoRAhPretrained WeightsWdkrkABrxd11k
d×

## Page 65

Large Language ModelsDealing with Scale

## Page 66

Large Language ModelsHarms of Large Language Models

## Page 67

Hallucination

## Page 68

Copyright

## Page 69

Privacy

## Page 70

Toxicity and Abuse

## Page 71

Misinformation

## Page 72

Large Language ModelsHarms of Large Language Models

