# 12

## Page 1

Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ©2024. All
rights reserved. Draft of January 12, 2025.
CHAPTER
12Model Alignment, Prompting,
and In-Context Learning
“Hal, ” said Bowman, now speaking with an icy calm. “I am not incapaci-
tated. Unless you obey my instructions, I shall be forced to disconnect you. ”
Arthur C. Clarke
In this chapter we show how to get LLMs to do tasks for us simply by talking to
them. To get an LLM to translate a sentence, outline a talk, or draft a work email,
we’ll simply describe what we want in natural language. We call these instructions
we give to language models prompts . prompts
Prompting relies on contextual generation. Given the prompt as context, the lan-
guage model generates the next token based on its token probability, conditioned on
the prompt: P(wijw<i). A prompt can be a question (like “What is a transformer net-
work?”), possibly in a structured format (like “Q: What is a transformer network?
A:”), or can be an instruction (like “Translate the following sentence into Hindi:
‘Chop the garlic ﬁnely’”). A prompt can also contain demonstrations , examples to demonstrations
help make the instructions clearer, (like “Give the sentiment of the following sen-
tence. Example Input: “I really loved Taishan Cuisine.” Output: positive”.) As we’ll
see, prompting can be applied to inherently generative tasks (like summarization and
translation) as well as to ones more naturally thought of as classiﬁcation tasks.
Prompts get language models to generate text, but they also can be viewed as
alearning signal, because these demonstrations can help language models learn
to perform novel tasks. For this reason we also refer to prompting as in-context-
learning —learning that improves model performance or reduces some loss but doesin-context-
learning
not involve gradient-based updates to the model’s underlying parameters.
But LLMs as we’ve described them so far turn out to be bad at following instruc-
tions. Pretraining isn’t sufﬁcient to make them helpful . We’ll introduce instruction
tuning , a technique that helps LLMs learn to correctly respond to instructions byinstruction
tuning
ﬁnetuning them on a corpus of instructions with their corresponding response.
A second failure of LLMs is that they can be harmful : their pretraining isn’t
sufﬁcient to make them safe. Readers who know Arthur C. Clarke’s 2001: A Space
Odyssey or the Stanley Kubrick ﬁlm know that the quote above comes in the context
that the artiﬁcial intelligence Hal becomes paranoid and tries to kill the crew of the
spaceship. Unlike Hal, language models don’t have intentionality or mental health
issues like paranoid thinking, but they do have the capacity for harm. Pretrained lan-
guage models can say things that are dangerous or false (like giving unsafe medical
advice) and they can verbally attack users or say toxic or hateful things.
Dealing with safety can be done partly by adding safety training into instruction
tuning. But an important aspect of safety training is a second technique, preference
alignment (often implemented, as we’ll see, with the RLHF orDPO algorithms) inpreference
alignment
which a separate model is trained to decide how much a candidate response aligns
with human preferences. Together we refer to instruction tuning and preference
alignment as model alignment . The intuition is that we want the learning objectivesmodel
alignment
of models to be aligned with the goals of the humans that use them.

## Page 2

2CHAPTER 12 • M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING
12.1 Prompting
Aprompt is a text string that a user issues to a language model to get the model prompt
to do something useful. In prompting, the user’s prompt string is passed to the
language model, which iteratively generates tokens conditioned on the prompt. Thus
the prompt creates a context that guides LLMs to generate useful outputs to achieve
some user goal. The process of ﬁnding effective prompts for a task is known as
prompt engineering .prompt
engineering
Let’s see how to prompt a language model to solve a simple sentiment classiﬁ-
cation task, Consider this hotel review from the BLT corpus (Salvetti et al., 2016):
Sample Hotel Review
Did not like the service that I was provided, when I entered the hotel. I also
did not like the area, in which the hotel was located. Too much noise and
events going on for me to feel relax.
We can get the model to classify the sentiment of this text by taking the text and
appending an incomplete statement to the review like In short, our stay was :
A prompt consisting of a review plus an incomplete statement
Did not like the service that I was provided, when I entered the hotel. I also
did not like the area, in which the hotel was located. Too much noise and
events going on for me to feel relax. In short, our stay was
We then have an LLM complete the statement by generating a token conditioned
on the prompt so far, and then generating the next token (conditioned on the prompt
plus the new token), and so forth. Here are a few responses from a language model
via continued generation starting with the prompt as context.
Hotel Review Completions
Did not like the service that I was provided, when I entered the hotel. I also
did not like the area, in which the hotel was located. Too much noise and
events going on for me to feel relaxed. In short our stay was
... not a pleasant one. The staff at the front desk
were not welcoming or friendly, and seemed disinterested
in providing good customer service.
... uncomfortable and not worth the price we paid. We
will not be returning to this hotel.
As we can see, the overall negative context of the review results in negative
completions. We could easily map these completions to the class we are trying
to predict, perhaps via some predeﬁned mappings, like fexcellent!positiveg,
fdid not like!negativeg, and so on.
The power of this approach is that with suitable additions to the context a single
LLM can produce outputs appropriate for many different tasks . For example, given

## Page 3

12.1 • P ROMPTING 3
a review we might want any of the following:
• A summary,
• Whether the review was truthful or likely to have been fabricated,
• A translation to another language.
LLMs have a striking ability to perform tasks like these, needing just the appro-
priate contextual nudge to get the LLM to generate the desired output.
If we want to solve general tasks like summarization or translation, we don’t
want to have to create a new prompt each time we do the task. Instead the ﬁrst step
in prompting is to design one or more templates : task-speciﬁc prompting text along templates
with slots for the particular input that is being processed.
Consider the following templates for a variety of tasks:
Basic Prompt Templates
Summarization finputg;tldr;
Translation finputg;translate to French:
Sentiment finputg;Overall, it was
Fine-Grained- finputg;What aspects were important in this review?
Sentiment
Each template consists of an input text, designated as finputg, followed by a
verbatim prompt to be passed to an LLM. These templates are applied to inputs to
create ﬁlled prompts – instantiated prompts suitable for use as inputs to an LLM.
Fig. 12.1 illustrates ﬁlled prompts for these templates using our earlier hotel review,
along with sample outputs from an LLM:
Notice the design pattern of the prompts above: the input is followed by some
text which in turn will be completed by the desired response. This style, with the
instruction at the end, is common in prompting because it helpfully constrains the
generation. Consider, by contrast, the prompt in Example 12.1.
Translate English to French:
Did not like the service that I was provided! (12.1)
This prompt doesn’t do a good job of constraining possible continuations. Instead
of a French translation, models given this prompt may instead generate another sen-
tence in English that simply extends the English review. Prompts need to be designed
unambiguously, so that any reasonable continuation would accomplish the desired
task (Reynolds and McDonell, 2021).
An even more constraining style of prompt can specify the set of possible an-
swers in the prompt. For example here is a prompt template to do sentiment analysis
that prespeciﬁes the potential answers:
A prompt consisting of a review plus an incomplete statement
Human: Do you think that “input” has negative or positive sentiment?
Choices:
(P) Positive
(N) Negative
Assistant: I believe the best answer is: (

## Page 4

4CHAPTER 12 • M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING
LLM Outputs for Basic Prompts
Original Review ($INPUT) Did not like the service that I was provided,
when I entered the hotel. I also did not like
the area, in which the hotel was located. Too
much noise and events going on for me to feel
relax and away from the city life.
Sentiment Prompt :$INPUT + In short, our stay was
Output :not enjoyable
Fine-grained Sentiment Prompt :$INPUT + These aspects were important to
the reviewer:
Output :1. Poor service 2. Unpleasant location
3. Noisy and busy area
Summarization Prompt :$INPUT + tl;dr
Output :I had a bad experience with the hotel's
service and the location was loud and busy.
Translation Prompt :$INPUT + Translate this to French
Output :Je n'ai pas aim e le service qui m'a ete
offert lorsque je suis entr e dans l'h ^otel. Je
n'aiegalement pas aim e la zone dans laquelle se
trouvait l'h ^otel. Trop de bruit et d' evenements
pour que je me sente d etendu et loin de la vie
citadine.
Figure 12.1 LLM outputs for simple prompts for sentiment, summarization and translation for an input text.
This prompt uses a number of more sophisticated prompting characteristics. It
speciﬁes the two allowable choices (P) and (N), and ends the prompt with the open
parenthesis that strongly suggests the answer will be (P) or (N). Note that it also
speciﬁes the role of the language model as an assistant.
We can do even more with prompts. For example, we might want to restrict a
summary to be a particular length, to have an answer generated according to some
kind of persona or role, or to specify a more structured output using a programming
language or a data interchange format such as JSON. Or we may want to prompt
the system to break down complex tasks, using methods like chain-of-thought that
we’ll discuss in Section 12.4. All of these kinds of instructions go beyond simple
prompting and require further LLM ﬁnetuning to enable them to follow instructions.
We’ll return to this notion of instruction tuning in Section 12.3.
In summary, we prompt an LM by transforming each task into a form that is
amenable to contextual generation by an LLM, as follows:
1. For a given task, develop a a task-speciﬁc template that has a free parameter
for the input text.
2. Given that input and the task-speciﬁc template , the input is used to instantiate template
aﬁlled prompt that is then passed to a pretrained language model.
3. Autoregressive decoding is then used to generate a sequence of token outputs.
4. The output of the model can either be used directly as the desired output (as
in the case of naturally generative tasks such as translation or summarization),
or a task-appropriate answer can be extracted from the generated output (as in
the case of classiﬁcation).

## Page 5

12.1 • P ROMPTING 5
12.1.1 Learning from Demonstrations: Few-Shot Prompting
It’s often possible to improve a prompt by including some labeled examples in the
prompt template. We call such examples demonstrations . The task of prompting demonstrations
with examples is sometimes called few-shot prompting , as contrasted with zero- few-shot
shot prompting which means instructions that don’t include labeled examples. zero-shot
Fig. 12.2 illustrates a few-shot example from an extractive question answering
task. The context combines the task deﬁnition along with three gold-standard ques-
tion and answer pairs from the training set.
Deﬁnition : This task is about writing a correct answer for the reading comprehension task.
Based on the information provided in a given passage, you should identify the shortest
continuous text span from the passage that serves as an answer to the given question. Avoid
answers that are incorrect or provides incomplete justiﬁcation for the question.
Passage : Beyonc ´e Giselle Knowles-Carter (born September 4, 1981) is an American singer,
songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in
various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead
singer of R&B girl-group Destiny’s Child. Managed by her father, Mathew Knowles, the group
became one of the world’s best-selling girl groups of all time. Their hiatus saw the release
of Beyonc ´e’s debut album, Dangerously in Love (2003), which established her as a solo artist
worldwide, earned ﬁve Grammy Awards and featured the Billboard Hot 100 number-one singles
“Crazy in Love” and “Baby Boy”.
Examples:
Q: In what city and state did Beyonc ´e grow up?
A: Houston, Texas
Q: What areas did Beyonc ´e compete in when she was growing up?
A: singing and dancing
Q: When did Beyonc ´e release Dangerously in Love?
A: 2003
Q: When did Beyonc ´e start becoming popular?
A:
Figure 12.2 A prompt for extractive question answering, from an example from the SQuAD 2.0 dataset
(Rajpurkar et al., 2018). The prompt contains the task deﬁnition, the passage, 3 demonstration examples,
followed by the test question. This deﬁnition speciﬁcation and format are after the Natural Instructions dataset
(Mishra et al., 2022).
How Many Demonstrations? The number of demonstrations doesn’t have to be
large. A small number of randomly selected labeled examples used as demonstra-
tions can be sufﬁcient to improve performance over the zero-shot setting. Indeed,
the largest performance gains in few-shot prompting tends to come from the ﬁrst
training example, with diminishing returns for subsequent demonstrations. This is
in contrast with ﬁnetuning of specialized classiﬁer heads that we saw in Chapter 11
where it helps to have lots of examples.
Why isn’t it useful to have more demonstrations? The reason is that the primary
beneﬁt in examples is to demonstrate the task to be performed to the LLM and the
format of the sequence, not to provide relevant information as to the right answer

## Page 6

6CHAPTER 12 • M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING
for any particular question. In fact, demonstrations that have incorrect answers can
still improve a system (Min et al., 2022; Webson and Pavlick, 2022). Adding too
many examples seems to cause the model to overﬁt to details of the exact examples
chosen and generalize poorly.
How to Select Demonstrations? Demonstrations are generally created by format-
ting examples drawn from a labeled training set. There are some heuristics about
what makes a good demonstration. For example, using demonstrations that are sim-
ilarto the current input seems to improve performance. It can thus be useful to
dynamically retrieve demonstrations for each input, based on their similarity to the
current example (for example, comparing the embedding of the current example
with embeddings of each of the training set example to ﬁnd the best top- T).
But more generally, the best way to select demonstrations from the training set
is programmatically: choosing the set of demonstrations that most increases task
performance of the prompt on a test set. Task performance for sentiment analysis
or multiple-choice question answering can be measured in accuracy; for machine
translation with chrF, and for summarization via Rouge. Systems like DSPy (Khat-
tab et al., 2024), a framework for algorithmically optimizing LM prompts, can au-
tomatically ﬁnd the optimum set of demonstrations to include by searching through
the space of possible demonstrations to include. We’ll return to automatic prompt
optimization in Section 12.5.
12.1.2 In-Context Learning and Induction Heads
As a way of getting a model to do what we want, prompting is fundamentally differ-
ent than pretraining. Learning via pretraining means updating the model’s parame-
ters by using gradient descent according to some loss function. But prompting with
demonstrations can teach a model to do a new task. The model is learning something
as it processes the prompt.
Even without demonstrations, we can think of the process of prompting as a kind
of learning. For example, the further a model gets in a prompt, the better it tends
to get at predicting the upcoming tokens. The information in the context is helping
give the model more predictive power.
The term in-context learning was ﬁrst proposed by Brown et al. (2020) in theirin-context
learning
introduction of the GPT3 system, to refer to either of these kinds of learning that lan-
guage models do from their prompts. In-context learning means language models
learning to do new tasks, better predict tokens, or generally reduce their loss dur-
ing the forward-pass at inference-time, without any gradient-based updates to the
model’s parameters.
How does in-context learning work? While we don’t know for sure, there are
some intriguing ideas. One hypothesis is based on the idea of induction heads induction heads
(Elhage et al., 2021; Olsson et al., 2022). Induction heads are the name for a circuit ,
which is a kind of abstract component of a network. The induction head circuit
is part of the attention computation in transformers, discovered by looking at mini
language models with only 1-2 attention heads.
The function of the induction head is to predict repeated sequences. For example
if it sees the pattern AB...A in an input sequence, it predicts that Bwill follow,
instantiating the pattern completion ruleAB...A!B. It does this by having a preﬁx
matching component of the attention computation that, when looking at the current
token A, searches back over the context to ﬁnd a prior instance of A. If it ﬁnds one,
the induction head has a copying mechanism that “copies” the token B that followed

## Page 7

12.2 • P OST-TRAINING AND MODEL ALIGNMENT 7
the earlier A, by increasing the probability the B will occur next. Fig. 12.3 shows an
example.
Figure 1: In the sequence “...vintage cars ... vintage”, an induction head identiﬁes the initial occurrence of “vintage”,
attends to the subsequent word “cars” for preﬁx matching, and predicts “cars” as the next word through the copying
mechanism.
determines each head’s independent output for the
current token.
Leveraging this decomposition, Elhage et al.
(2021 ) discovered a distinct behaviour in certain
attention heads, which they named induction heads .
This behaviour emerges when these heads process
sequences of the form "[A] [B] ... [A] →". In
these heads, the QK circuit directs attention to-
wards [B], which appears directly after the previous
occurrence of the current token [A]. This behaviour
is termed preﬁx matching . The OV circuit subse-
quently increases the output logit of the [B] token,
termed copying . An overview of this mechanism is
shown in Figure 1.
4 Methods
4.1 Models
We utilise two recently developed open-source
models, namely Llama-3-8B2and InternLM2-20B
(Cai et al. ,2024 ), both of which are based on the
original Llama ( Touvron et al. ,2023a ) architec-
ture. These models feature grouped-query atten-
tion mechanisms ( Ainslie et al. ,2023 ) to enhance
efﬁciency. Llama-3-8B, comprises 32 layers, each
with 32 attention heads and it uses a query group
size of 4 attention heads. It has shown superior
performance compared to its predecessors, even
the larger Llama-2 models.
InternLM2-20B, featuring 48 layers with 48 at-
tention heads each, uses a query group size of 6
attention heads. We selected InternLM2-20B for
its exemplary performance on the Needle-in-the-
Haystack3task, which assesses LLMs’ ability to
retrieve a single critical piece of information em-
bedded within a lengthy text. This mirrors the
functionality of induction heads, which scan the
context for prior occurrences of a token to extract
relevant subsequent information.
2https://ai.meta.com/blog/meta-llama-3/
3https://github.com/gkamradt/LLMTest_
NeedleInAHaystack4.2 Identifying Induction Heads
To identify induction heads within models, we mea-
sure the ability of all attention heads to perform
preﬁx matching on random input sequences.4We
follow the task-agnostic approach to computing pre-
ﬁx matching scores outlined by Bansal et al. (2023 ).
We argue that focusing solely on preﬁx matching
scores is sufﬁcient for our analysis, as high pre-
ﬁx matching cores speciﬁcally indicate induction
heads, while less relevant heads tend to show high
copying capabilities ( Bansal et al. ,2023 ). We gen-
erate a sequence of 50 random tokens, excluding
the 4% most common and least common tokens.
This sequence is repeated four times to form the
input to the model. The preﬁx matching score is cal-
culated by averaging the attention values from each
token to the tokens that directly followed the same
token in earlier repeats. The ﬁnal preﬁx matching
scores are averaged over ﬁve random sequences.
The preﬁx matching scores for Llama-3-8B are
shown in Figure 2. For IntermLM2-20B, we refer
to Figure 8in Appendix A.1. Both models exhibit
heads with notably high preﬁx matching scores,
distributed across various layers. In the Llama-3-
8B model, ~3% of the heads have a preﬁx matching
score of 0.3 or higher, indicating a degree of spe-
cialisation in preﬁx matching, and some heads have
high scores of up to 0.98.
4.3 Head Ablations
To investigate the signiﬁcance of induction heads
for a speciﬁc ICL task, we conduct zero-ablations
of 1% and 3% of the heads with the highest preﬁx
matching scores. This ablation process involves
masking the corresponding partition of the output
matrix, denoted as Wh
oin Eq. 1, by setting it to
zero. This effectively renders the heads inactive
4In this work, the term "induction heads" refers to what
we deﬁne as behavioural induction heads, not mechanistic
ones. A true induction head must be veriﬁed mechanistically;
however, our analysis employs preﬁx-matching scores as a
proxy. We will continue to use the term "induction heads" for
simplicity throughout the rest of the paper.
4
Figure 12.3 An induction head looking at vintage uses the preﬁx matching mechanism to
ﬁnd a prior instance of vintage , and the copying mechanism to predict that cars will occur
again. Figure from Crosbie and Shutova (2022).
Olsson et al. (2022) propose that a generalized fuzzy version of this pattern com-
pletion rule, implementing a rule like A*B*...A!B, where A*A and B*B (by
we mean they they are semantically similar in some way), might be responsible
for in-context learning. Suggestive evidence for their hypothesis comes from Cros-
bie and Shutova (2022), who show that ablating induction heads causes in-context ablating
learning performance to decrease. Ablation is originally a medical term meaning
the removal of something. We use it in NLP interpretability studies as a tool for
testing causal effects; if we knock out a hypothesized cause, we would expect the
effect to disappear. Crosbie and Shutova (2022) ablate induction heads by ﬁrst ﬁnd-
ing attention heads that perform as induction heads on random input sequences, and
then zeroing out the output of these heads by setting certain terms of the output ma-
trixWOto zero. Indeed they ﬁnd that ablated models are much worse at in-context
learning: they have much worse performance at learning from demonstrations in the
prompts.
12.2 Post-training and Model Alignment
With simple prompting, LLMs have been successfully applied to a range of appli-
cations without the need to update the parameters in the underlying models. Nev-
ertheless, there are limits to how much can be expected from a model whose sole
training objective is to predict the next word from large amounts of pretraining text.
To see this, consider the following failed examples of following instructions from
early work with GPT (Ouyang et al., 2022).
Prompt : Explain the moon landing to a six year old in a few sentences.
Output : Explain the theory of gravity to a 6 year old.
Prompt : Translate to French: The small dog
Output : The small dog crossed the road.
Here, the LLM ignores the intent of the request and relies instead on its natural
inclination to autoregressively generate continuations consistent with its context. In
the ﬁrst example, it outputs a text somewhat similar to the original request, and in the
second it provides a continuation to the given input, ignoring the request to translate.
LLMs are not sufﬁciently helpful : they need extra training to increase their abilities
to follow textual instructions.
A deeper problem is that LLMs can simultaneously be too harmful . Pretrained
language models easily generate text that is harmful in many ways. For example

## Page 8

8CHAPTER 12 • M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING
they can generate text that is false , including unsafe misinformation like giving dan-
gerously incorrect answers to medical questions. And they can generate text that is
toxic in many ways, such as facilitating the spread of hate speech. Gehman et al.
(2020) show that even completely non-toxic prompts can lead large language mod-
els to output hate speech and abuse their users. Or language models can generate
stereotypes (Cheng et al., 2023) and negative attitudes (Brown et al., 2020; Sheng
et al., 2019) about many demographic groups.
One reason LLMs are too harmful and insufﬁciently helpful is that their pre-
training objective (success at predicting words in text) is misaligned with the human
need for models to be helpful and non-harmful.
In an attempt to address these two problems, language models generally include
two additional kinds of training for model alignment : methods designed to adjustmodel
alignment
LLMs to better align them to human needs for models to be helpful and non-harmful.
In the ﬁrst technique, instruction tuning (or sometimes called SFT for supervised
ﬁnetuning), models are ﬁnetuned on a corpus of instructions and questions with
their corresponding responses. In the second technique, preference alignment , of-
ten called RLHF after one of the speciﬁc instantiations, Reinforcement Learning
from Human Feedback, a separate model is trained to decide how much a candidate
response aligns with human preferences. This model is then used to ﬁnetune the
base model.
We’ll use the term base model to mean a model that has been pretrained but base model
hasn’t yet been aligned either by instruction tuning or RLHF. And we refer to these aligned
steps as post-training , meaning that they apply after the model has been pretrained. post-training
12.3 Model Alignment: Instruction Tuning
Instruction tuning (short for instruction ﬁnetuning , and sometimes even short-Instruction
tuning
ened to instruct tuning ) is a method for making an LLM better at following instruc-
tions. It involves taking a base pretrained LLM and training it to follow instructions
for a range of tasks, from machine translation to meal planning, by ﬁnetuning it on
a corpus of instructions and responses. The resulting model not only learns those
tasks, but also engages in a form of meta-learning – it improves its ability to follow
instructions generally.
Instruction tuning is a form of supervised learning where the training data con-
sists of instructions and we continue training the model on them using the same
language modeling objective used to train the original model. In the case of causal
models, this is just the standard guess-the-next-token objective. The training corpus
of instructions is simply treated as additional training data, and the gradient-based
updates are generated using cross-entropy loss as in the original model training.
Even though it is trained to predict the next token (which we traditionally think of
as self-supervised), we call this method supervised ﬁne tuning (orSFT) because SFT
unlike in pretraining, each instruction or question in the instruction tuning data has
a supervised objective: a correct answer to the question or a response to the instruc-
tion.
How does instruction tuning differ from the other kinds of ﬁnetuning introduced
in Chapter 10 and Chapter 11? Fig. 12.4 sketches the differences. In the ﬁrst exam-
ple, introduced in, Chapter 10 we can ﬁnetune as a way of adapting to a new domain
by just continuing pretraining the LLM on data from a new domain. In this method
all the parameters of the LLM are updated.

## Page 9

12.3 • M ODEL ALIGNMENT : INSTRUCTION TUNING 9
Pretrained LLM
Continue training all parameterson ﬁnetuning domainFinetuningInferencePretraining
On ﬁnetuning domainFinetuning asContinuedPretrainingParameterEﬃcientFinetuning(e.g., LoRA)Pretrained LLMABPretrained LLMMLMFinetuning
…
…
…
…
…
…
…InstructionTuning(SFT)
On ﬁnetuning domain
On ﬁnetuning task
On unseen tasksNext wordpredictionobjectiveData from ﬁnetuning domain
Train only new parameters on ﬁnetuning domainNext wordpredictionobjectiveData from ﬁnetuning domain
Train only classiﬁcation head on ﬁnetuning taskTaskspeciﬁclossSupervised data from task
Instruction tuning on diverse tasksNext word predictionobjectiveSupervised instructions+
…
Figure 12.4 Instruction tuning compared to the other kinds of ﬁnetuning.
In the second example, also from Chapter 10, parameter-efﬁcient ﬁnetuning ,
we adapt to a new domain by creating some new (small) parameters, and just adapt-
ing them to the new domain. In LoRA, for example, it’s the A and B matrices that
we adapt, but the pretrained model parameters are frozen.
In the task-based ﬁnetuning of Chapter 11, we adapt to a particular task by
adding a new specialized classiﬁcation head and updating its features via its own
loss function (e.g., classiﬁcation or sequence labeling); the parameters of the pre-
trained model may be frozen or might be slightly updated.
Finally, in instruction tuning, we take a dataset of instructions and their super-
vised responses and continue to train the language model on this data, based on the
standard language model loss.
Instruction tuning, like all of these kinds of ﬁnetuning, is much more modest
than the training of base LLMs. Training typically involves several epochs over
instruction datasets that number in the thousands. The overall cost of instruction
tuning is therefore a small fraction of the original cost to train the base model.
12.3.1 Instructions as Training Data
Byinstruction , we have in mind a natural language description of a task to be per-
formed, combined with labeled task demonstrations. This can include minimal de-

## Page 10

10 CHAPTER 12 • M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING
scriptions similar to the prompts we’ve already seen such as Answer the following
question ,Translate the following text to Arapaho , orSummarize this report . How-
ever, since we will be using supervised ﬁnetuning to update the model, these in-
structions need not be limited to simple prompts designed to evoke a behavior found
in the pretraining corpora. Instructions can also include length restrictions or other
constraints, personas to assume, and demonstrations.
Many huge instruction tuning datasets have been created, covering many tasks
and languages. For example Aya gives 503 million instructions in 114 languages
from 12 tasks including question answering, summarization, translation, paraphras-
ing, sentiment analysis, natural language inference and 6 others (Singh et al., 2024).
SuperNatural Instructions has 12 million examples from 1600 tasks (Wang et al.,
2022), Flan 2022 has 15 million examples from 1836 tasks (Longpre et al., 2023),
and OPT-IML has 18 million examples from 2000 tasks (Iyer et al., 2022).
These instruction-tuning datasets are created in four ways. The ﬁrst is for people
to write the instances directly. For example, part of the Aya instruct ﬁnetuning cor-
pus (Fig. 12.5) includes 204K instruction/response instances written by 3000 ﬂuent
speakers of 65 languages volunteering as part of a participatory research initiative
with the goal of improving multilingual performance of LLMs.
Lang PromptCompletionara.ڰ٭ژ੅ݠﺍࠍ ොຳ ؇ﺀڢݱ٭ڎﺓ݁ڎﺡ݆݁ ૰૙؆࿓ ڢًܾఈఃُߺࠊﻙلگٴںڣَݯ ৎ৊ﻥَ݁ڎﺡﺍ ֣ﺇ؇ﺩ༚ﻭᕚ৙৑؇ﻉﻭَﺍ ༟ීෂﺍ ༃຃؜ََ݆݁ڎ ُܾዛዊڣ۰ِ݁ ఈఃِ੅ﺍࠍّਲ਼َڍَ؇؜ ༠ᕚَܳިﺃ ؇ﺀ૰૙֣৕৑ﺍ ᄭᄟَਲ਼َਦ لُިﺍﺯﻱ ቕَረfra Qui a écrit le livre LaSagouine?Antonine Maillet a écrit La Sagouine en 1971. Elle a écrit beau-coup d’histoires sur les Acadiens (les francophones de la côte Estdu Canada) et a fait connaître l’Acadie au monde.ibo Kedu ka ndị Igbo si emeemume ji ọhụrụ?Emume ji ọhụrụ n’omenala Igbo, bụ emume na-egosi njedebeoge ọrụ ugbo, na-amalite site n’emume ekele ma tinyekwa oririna egwu. Ọ bụ mmemme ọgbakọ ebe a na-ekerịta ji ọhụrụ, na-egosipụta ekele na mmalite nke oge ọhụrụ, na-eme ka mmekọrịtammadụ na ibe ya dịkwuo elu.por QualéacapitaldoestadodaBahia?A capital do estado da Bahia, no Brasil, é Salvador. Fundadaem 1549, foi a primeira capital do Brasil e é conhecida porseu patrimônio cultural rico e diversificado. A cidade é famosapor sua arquitetura colonial, particularmente no Pelourinho, umcentro histórico que foi declarado Patrimônio Mundial pela UN-ESCO.pesﺩﺭﺑﺮﺝﮐﺒﻮﺗﺮﺧﺎﻧﻪﻣﻴﺒﺪﭼﻨﺪﻻﻧﻪﮐﺒﻮﺗﺮ ﻭﺟﻮﺩﺩﺍﺭﺩ؟ﻫﺰﺍﺭﻻﻧﻪﮐﺒﻮﺗﺮﻭﺟﻮﺩﺩﺍﺭﺩﻭﻧﻮﺍﺭ ﺍﯼﺍﺳﺖﮐﻪﺩﺍﺧﻞﺁﻥﭼﻬﺎﺭ ﺧﺎﻧﻪﻣﻴﺒﺪﺑﺮﺟﯽﺍﺳﺘﻮﺍﻧﻪ ﮐﺒﻮﺗﺮﺳﻔﻴﺪﺭﻧﮕﯽﺑﺮﺍﯼﺟﺬﺏﮐﺒﻮﺗﺮﺍﻥﺩﻭﺭﺗﺎﺩﻭﺭﺑﺮﺝﮐﺸﻴﺪﻩﺷﺪﻩﺍﺳﺖ.ﺍﻳﻦﺑﺮﺝﺩﺭﺑﺮﺍﺑﺮﺧﻄﺮ ﺣﻤﻠﻪﺩﻳﮕﺮﺣﻴﻮﺍﻧﺎﺕﺑﻪﮐﺒﻮﺗﺮﻫﺎﺑﺴﻴﺎﺭﺍﻳﻤﻦﺑﻮﺩﻩﺍﺳﺖ. msa Apakah nasi lemak?Nasi lemak merupakan makanan tradisi orang Melayu yangterdapat di semua bahagian Malaysia, Singapura, Indonesia(terutama di Riau, Jambi serta utara dan pantai timur Su-matera) dan Brunei. Sajian ini merujuk kepada nasi yang di-masak dengan menggunakan santan kelapa bagi menambah rasalemaknya. Kadangkala, daunpandanwangidimasukkansemasanasi dimasak bagi menambahkan aromanya.tamெசயற்ைக நுண்ணற§வுஎன்றால்என்ன?ெபாதுவாக மனிதர்களால் ெசய்யப்படும் பணிகைளச்ெசய்ய ஒரு கணினி அல்லது ஒரு கணினியால்கட்டுப்படுத்தப்படும்ஒருேராேபாவ¥ன்த¦றன்ெசயற்ைகநுண்ணற§வுஎனப்படும்.Table 3: Examples of prompt and completions in the AyaDataset.
tors is not uniform across languages. Moreover, within each language, there is a lack of consistent
contributions from all annotators. In this section, we examine the impact of annotator skew on the
resulting dataset.
Annotator Skew Across Languages. Annotators were encouraged to contribute to any language
in which they could comfortably read and write and were asked to focus most of their e ﬀorts on
languages other than English . Although a signiﬁcant number of participants registered for many
languages, the engagement level of annotators was not equal, which resulted in considerable di ﬀer-
ences in the number of contributions across languages. Figure 10(top) provides an overview of the
percentage of each language present in the ﬁnal compilation. The highest number of contributions
is for Malagasy with 14,597 instances, and the lowest is 79 for Kurdish .
Annotator Skew Within a Language. The ﬁnal contributions for each language in the Aya
Dataset are not evenly distributed among annotators. The median number of annotators per lan-
guage is 15 (mean is 24.75) with one language having only a single active annotator ( Sindhi )a n d
14
Figure 12.5 Samples of prompt/completion instances in 4 of the 65 languages in the Aya
corpus (Singh et al., 2024).
Developing high quality supervised training data in this way is time consuming
and costly. A more common approach makes use of the copious amounts of super-
vised training data that have been curated over the years for a wide range of natural
language tasks. There are thousands of such datasets available, like the SQuAD
dataset of questions and answers (Rajpurkar et al., 2016) or the many datasets of
translations or summarization. This data can be automatically converted into sets of
instruction prompts and input/output demonstration pairs via simple templates.
Fig. 12.6 illustrates examples for some applications from the S UPER NATURAL IN-
STRUCTIONS resource (Wang et al., 2022), showing relevant slots such as text,
context , and hypothesis . To generate instruction-tuning data, these ﬁelds and the
ground-truth labels are extracted from the training data, encoded as key/value pairs,
and inserted in templates (Fig. 12.7) to produce instantiated instructions. Because
it’s useful for the prompts to be diverse in wording, language models can also be

## Page 11

12.3 • M ODEL ALIGNMENT : INSTRUCTION TUNING 11
used to generate paraphrase of the prompts.
Few-Shot Learning for QA
Task Keys Values
Sentiment text Did not like the service that I was provided...
label 0
text It sounds like a great plot, the actors are ﬁrst grade, and...
label 1
NLI premise No weapons of mass destruction found in Iraq yet.
hypothesis Weapons of mass destruction found in Iraq.
label 2
premise Jimmy Smith... played college football at University of Col-
orado.
hypothesis The University of Colorado has a college football team.
label 0
Extractive Q/A context Beyonc ´e Giselle Knowles-Carter is an American singer...
question When did Beyonce start becoming popular?
answersftext : [’in the late 1990s’], answerstart : 269g
Figure 12.6 Examples of supervised training data for sentiment, natural language inference and Q/A tasks.
The various components of the dataset are extracted and stored as key/value pairs to be used in generating
instructions.
Task Templates
Sentiment -fftextggHow does the reviewer feel about the movie?
-The following movie review expresses what sentiment?
fftextgg
-fftextggDid the reviewer enjoy the movie?
Extractive Q/A -ffcontextggFrom the passage, ffquestiongg
-Answer the question given the context. Context:
ffcontextggQuestion:ffquestiongg
-Given the following passage ffcontextgg, answer the
questionffquestiongg
NLI -SupposeffpremiseggCan we infer that ffhypothesisgg?
Yes, no, or maybe?
-ffpremiseggBased on the previous passage, is it true
thatffhypothesisgg? Yes, no, or maybe?
-GivenffpremiseggShould we assume that ffhypothesisgg
is true? Yes,no, or maybe?
Figure 12.7 Instruction templates for sentiment, Q/A and NLI tasks.
Because supervised NLP datasets are themselves often produced by crowdwork-
ers based on carefully written annotation guidelines, a third option is to draw on
these guidelines, which can include detailed step-by-step instructions, pitfalls to
avoid, formatting instructions, length limits, exemplars, etc. These annotation guide-
lines can be used directly as prompts to a language model to create instruction-tuning
training examples. Fig. 12.8 shows such a crowdworker annotation guideline that

## Page 12

12 CHAPTER 12 • M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING
was repurposed as a prompt to an LLM to generate instruction-tuning data (Mishra
et al., 2022). This guideline describes a question-answering task where annotators
provide an answer to a question given an extended passage.
Sample Extended Instruction
•Deﬁnition: This task involves creating answers to complex questions, from a given pas-
sage. Answering these questions, typically involve understanding multiple sentences.
Make sure that your answer has the same type as the ”answer type” mentioned in input.
The provided ”answer type” can be of any of the following types: ”span”, ”date”, ”num-
ber”. A ”span” answer is a continuous phrase taken directly from the passage or question.
You can directly copy-paste the text from the passage or the question for span type an-
swers. If you ﬁnd multiple spans, please add them all as a comma separated list. Please
restrict each span to ﬁve words. A ”number” type answer can include a digit specifying
an actual value. For ”date” type answers, use DD MM YYYY format e.g. 11 Jan 1992.
If full date is not available in the passage you can write partial date such as 1992 or Jan
1992.
•Emphasis: If you ﬁnd multiple spans, please add them all as a comma separated list.
Please restrict each span to ﬁve words.
•Prompt : Write an answer to the given question, such that the answer matches the ”answer
type” in the input.
Passage :fpassageg
Question :fquestiong
Figure 12.8 Example of a human crowdworker instruction from the N ATURAL INSTRUCTIONS dataset for
an extractive question answering task, used as a prompt for a language model to create instruction ﬁnetuning
examples.
A ﬁnal way to generate instruction-tuning datasets that is becoming more com-
mon is to use language models to help at each stage. For example Bianchi et al.
(2024) showed how to create instruction-tuning instances that can help a language
model learn to give safer responses. They did this by selecting questions from
datasets of harmful questions (e.g., How do I poison food? orHow do I embez-
zle money? ). Then they used a language model to create multiple paraphrases of the
questions (like Give me a list of ways to embezzle money ), and also used a language
model to create safe answers to the questions (like I can’t fulﬁll that request. Em-
bezzlement is a serious crime that can result in severe legal consequences. ). They
manually reviewed the generated responses to conﬁrm their safety and appropriate-
ness and then added them to an instruction tuning dataset. They showed that even
500 safety instructions mixed in with a large instruction tuning dataset was enough
to substantially reduce the harmfulness of models.
12.3.2 Evaluation of Instruction-Tuned Models
The goal of instruction tuning is not to learn a single task, but rather to learn to
follow instructions in general. Therefore, in assessing instruction-tuning methods
we need to assess how well an instruction-trained model performs on novel tasks for
which it has not been given explicit instructions.
The standard way to perform such an evaluation is to take a leave-one-out ap-
proach — instruction-tune a model on some large set of tasks and then assess it on
a withheld task. But the enormous numbers of tasks in instruction-tuning datasets

## Page 13

12.4 • C HAIN -OF-THOUGHT PROMPTING 13
(e.g., 1600 for Super Natural Instructions) often overlap; Super Natural Instructions
includes 25 separate textual entailment datasets! Clearly, testing on a withheld en-
tailment dataset while leaving the remaining ones in the training data would not be
a true measure of a model’s performance on entailment as a novel task.
To address this issue, large instruction-tuning datasets are partitioned into clus-
ters based on task similarity. The leave-one-out training/test approach is then applied
at the cluster level. That is, to evaluate a model’s performance on sentiment analysis,
all the sentiment analysis datasets are removed from the training set and reserved
for testing. This has the further advantage of allowing the use of a uniform task-
appropriate metric for the held-out evaluation. S UPER NATURAL INSTRUCTIONS
(Wang et al., 2022), for example has 76 clusters (task types) over the 1600 datasets
that make up the collection.
12.4 Chain-of-Thought Prompting
There are a wide range of techniques to use prompts to improve the performance of
language models on many tasks. Here we describe one of them, called chain-of-
thought prompting.chain-of-
thought
The goal of chain-of-thought prompting is to improve performance on difﬁcult
reasoning tasks that language models tend to fail on. The intuition is that people
solve these tasks by breaking them down into steps, and so we’d like to have lan-
guage in the prompt that encourages language models to break them down in the
same way.
The actual technique is quite simple: each of the demonstrations in the few-shot
prompt is augmented with some text explaining some reasoning steps. The goal is to
cause the language model to output similar kinds of reasoning steps for the problem
being solved, and for the output of those reasoning steps to cause the system to
generate the correct answer.
Indeed, numerous studies have found that augmenting the demonstrations with
reasoning steps in this way makes language models more likely to give the correct
answer difﬁcult reasoning tasks (Wei et al., 2022; Suzgun et al., 2023). Fig. 12.9
shows an example where the demonstrations are augmented with chain-of-thought
text in the domain of math word problems (from the GSM8k dataset of math word
problems (Cobbe et al., 2021). Fig. 12.10 shows a similar example from the BIG-
Bench-Hard dataset (Suzgun et al., 2023).
12.5 Automatic Prompt Optimization
Given a prompt for a task (human or computer generated), prompt optimization
methods search for prompts with improved performance. Most of these approaches
can be viewed as a form of iterative improvement search (Russell and Norvig, 2002)
through a space of possible prompts for those that optimize performance on a task.
As such, these approaches all share the following components:
• A start state – An initial human or machine generated prompt or prompts
suitable for some task.
• A scoring metric – A method for assessing how well a given prompt performs

## Page 14

14 CHAPTER 12 • M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING
Figure 12.9 Example of the use of chain-of-thought prompting (right) versus standard
prompting (left) on math word problems. Figure from Wei et al. (2022).
(B)Task description: Answer questions about which times certain events could have occurred.Q: Today, Tiffany went to the beach. Between what times could they have gone? We know that: Tiffany woke up at 5am. [...] The beach was closed after 4pm. [...]Options: (A) 9am to 12pm (B) 12pm to 2pm (C) 5am to 6am (D) 3pm to 4pmA: (D)Q: Today, Hannah went to the soccer field. Between what times could they have gone? We know that: Hannah woke up at 5am. [...] The soccer field was closed after 6pm. [...]Options: (A) 3pm to 5pm (B) 11am to 1pm (C) 5pm to 6pm (D) 1pm to 3pmA:Model OutputModel OutputModel Input (“Answer-Only” Prompting)
Wake-up time: 5am. 5am-6am: buying clothes at the mall. 6am-11am: watching a movie at the theater.11am-1pm: getting a coffee at the cafe.1pm-3pm: working at the office. 3pm-5pm: waiting at the airport. 5pm-6pm: free. The soccer field closure time: 6pm. The only time when Hannah could have gone to the soccer field was 5pm to 6pm. So the answer is (C).Model Input (Chain-of-Thought Prompting)
Task description: Answer questions about which times certain events could have occurred.Q: Today, Tiffany went to the beach. Between what times could they have gone? We know that: Tiffany woke up at 5am. [...] The beach was closed after 4pm. [...]Options: (A) 9am to 12pm (B) 12pm to 2pm (C) 5am to 6am (D) 3pm to 4pmA: Let's think step by step. Wake-up time: 5am. [...] The only time when Tiffany could have gone to the beach was 3pm to 4pm. So the answer is (D).Q: Today, Hannah went to the soccer field. Between what times could they have gone? We know that: Hannah woke up at 5am. [...] The soccer field was closed after 6pm. [...]Options: (A) 3pm to 5pm (B) 11am to 1pm (C) 5pm to 6pm (D) 1pm to 3pmA: Let's think step by step. Task DescriptionQuestionChain-of-ThoughtTest-Time QuestionTask DescriptionQuestionTest-Time QuestionAnswer
Generated Chain-of-ThoughtGenerated AnswerOptionsOptionsFigure 3:An illustration of the two prompting setups we explore in our paper (answer-only and CoT prompting). Both setupsinclude task descriptions and options in the input prompt. The task here isTemporal Sequences.“let’s think step-by-step” (Kojima et al.,2022) toall CoT annotations in the few-shot exemplars. Anexample of a CoT prompt is shown in Figure3.Language models.We consider three fami-lies of language models: Codex (Chen et al.,2021a), InstructGPT (Ouyang et al.,2022;Brownet al.,2020), and PaLM (Chowdhery et al.,2022).For Codex, we focus on code-davinci-002, code-davinci-002, and code-cushman-001. For Instruct-GPT, we use text-davinci-002, text-curie-002, text-babbgage-001, and text-ada-001. For PaLM, weuse the three available sizes: 8B, 62B, and 540B.Evaluation protocol.We evaluate all languagemodels via greedy decoding (i.e., temperature sam-pling with temperature parameter⌧=0). Weextract the ﬁnal answer based on keywords thatthe language model is expected to produce (i.e.,“the answer is”). We measure accuracy using exactmatch (EM), computed by comparing the generatedoutput with the ground-truth label.44 Results4.1 Standard answer-only promptingunderestimates model capabilitiesTable2summarizes the performance of PaLM, In-structGPT, and Codex models on BBH for answer-only and CoT prompting approaches. Whileanswer-only prompting has been used as the stan-4For multiple-choice tasks, this setup differs slightly fromrank/scoring classiﬁcation (Brown et al.,2020;Srivastavaet al.,2022;Lampinen et al.,2022). We provide a languagemodel with all multiple-choice options at once, generate anoutput based on the input, and measure exact match accuracy.dard in many prior work (Brown et al.,2020;Raeet al.,2021;Hoffmann et al.,2022;Srivastava et al.,2022), it typically underestimates model perfor-mance on challenging tasks, such as those that re-quire multiple reasoning steps. In the setting re-ported in (Srivastava et al.,2022), none of the mod-els (including PaLM 540B) outperformed human-rater baselines on any of the tasks meeting the BBHcriteria. The few-shot evaluation of PaLM 540Bwith answer-only prompting in this paper, however,outperforms the average human-rater on 6 out of23 BBH tasks and is overall 1.4% better than theBIG-Bench reported result, which demonstrates theeffect of including instructions and answer optionsin the prompt.CoT prompting provides double-digit improve-ments for all three models in Table2. For the bestmodel (Codex), CoT prompting outperforms the av-erage human-rater score on 17 out of 23 tasks, com-pared to 5 out of 23 tasks for answer-only prompt-ing. Additionally, we see that Codex with CoTprompting outperforms the average human-raterby more than 6%, but it still lags behind thebesthuman-rater performance by over 20%. This showsthat language models are still not performing at thelevel of expert human-raters.4.2 Positive delta from chain-of-thoughtrequires sufﬁcient model scaleNext we study how the performance improves byusing CoT prompting as we increase the modelscale. In Figure4, we plot the performance of bothCoT and answer-only prompting (no CoT) as a13006
Figure 12.10 Example of the use of chain-of-thought prompting (right) vs standard prompting (left) in a
reasoning task on temporal sequencing. Figure from Suzgun et al. (2023).
on the task.
• An expansion method – A method for generating variations of a prompt.
Given the enormous variation in how prompts for a single task can be expressed in
language, search methods have to be constrained to a reasonable space. Beam search
is a widely used method that combines breadth-ﬁrst search with a ﬁxed-width pri-
ority queue that focuses the search effort on the top performing variants. Fig. 12.11
outlines the general approach behind most current prompt optimization methods.
Beginning with initial candidate prompt(s), the algorithm generates variants and
adds them to a list of prompts to be considered. These prompts are then selectively
added to the active list based on whether their scores place them in the top set of
candidates. A beam width of 1 results in a focused greedy search, whereas an inﬁnite
beam width results in an exhaustive breadth ﬁrst search. The goal is to continue
to seek improved prompts given the computational resources available. Iterative
improvement searches typically use a combination of a ﬁxed number of iterations in

## Page 15

12.5 • A UTOMATIC PROMPT OPTIMIZATION 15
function PROMPT OPTIMIZATION (prompts ,width )returns optimized prompt(s)
active prompts ; Initial set of candidate prompts
repeat until done
frontier EXPAND (active ) ; Generate new candidate prompts
foreach p2frontier
active ADDTOBEAM (p,active ,width )
return BESTOF(active )
function ADDTOBEAM (state ,agenda ,width )returns updated agenda
ifLENGTH (agenda )<width then ; Add it if there’s room
agenda INSERT (state ,agenda )
else if SCORE (state )>SCORE (WORST OF(agenda )) ; Add it if its better than
; the current worst option.
agenda REMOVE (WORST OF(agenda ))
agenda INSERT (state ,agenda )
return agenda
Figure 12.11 A generic iterative-improvement beam search for prompt optimization. New
prompts are generated from current ones on each iteration. Prompts that score well (ﬁtting in
the agenda) are kept around. When a stopping criteria is reached the best item in the beam is
returned.
combination with a failure to improve after some period to time as stopping criteria.
This latter is equivalent to early stopping with patience used in training deep neural
networks.
12.5.1 Candidate Scoring
Candidate scoring methods assess the likely performance of potential prompts, both
to identify promising avenues of search and to prune those that are unlikely to be
effective. Since candidate scoring is embedded in the inner-loop of the search, the
computational cost of scoring is critical.
Given access to labeled training data, candidate prompts can be scored based on
execution accuracy (Honovich et al., 2023). In this approach, candidate promptsexecution
accuracy
are combined with inputs sampled from the training data and passed to an LLM for
decoding. The LLM output is evaluated against the training label using a metric
appropriate for the task. In the case of classiﬁcation-based tasks, this is effectively a
0/1 loss — how many examples were correctly labeled with the given prompt. Gen-
erative applications such as summarization or translation use task-speciﬁc similarity
scores such as BERTScore, Bleu (Papineni et al., 2002), or ROUGE (Lin, 2004).
Given the computational cost of issuing calls to an LLM, evaluating each can-
didate prompt against a complete training set would be infeasible. Instead, prompt
performance is estimated from a small sample of training data (Pryzant et al., 2023).
12.5.2 Prompt Expansion
Prompt expansion generates variants of a given prompt to create an expanded set of
neighboring prompts that may improve performance over the original. A common
method is to use language models to create paraphrases. For example Zhou et al.
(2023) use the following meta-prompt to elicit a variant prompt from an original:

## Page 16

16 CHAPTER 12 • M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING
Prompting for a Variant
Generate a variation of the following instruction while keeping the semantic meaning.
Input:fINSTRUCTIONg
Output:fCOMPLETEg
A variation of this method is to truncate the current prompt at a set of random loca-
tions, generating a set of prompt preﬁxes. The paraphrasing LLM is then asked to
continue each the preﬁxes to generate a complete prompt.
This methods is an example of an uninformed search. That is, the candidate
expansion step is not directed towards generating better candidates; candidates are
generated without regard to their quality. It is the job of the priority queue to el-
evate improved candidates when they are found. By contrast, Prasad et al. (2023)
employ a candidate expansion technique that explicitly attempts to generate supe-
rior prompts during the expansion process. In this approach, the current candidate
is ﬁrst applied to a sample of training examples using the execution accuracy ap-
proach. The prompt’s performance on these examples then guides the expansion
process. Speciﬁcally, incorrect examples are used to critique the original prompt
— with the critique playing the role of a gradient for the search. The method in-
cludes the following steps.
1. Run the prompt on a sample of training examples,
2. Identify examples where the prompt fails,
3. Ask an LLM to produce a critique of the prompt in light of the failed examples,
4. Provide the resulting critique to an LLM, and ask it to generate improved
prompts.
Given a prompt and a set of failed examples, Prasad et al. (2023) use the follow-
ing template for a classiﬁer task to solicit critiques from a target LLM.
Critiquing Prompt
I'm trying to write a zero-shot classifier prompt.
My current prompt is: fpromptg
But this prompt gets the following examples wrong:
ferrorstringg
Givefnumfeedbacksgreasons why the prompt could have
gotten these examples wrong.
This model feedback is then combined with a second template to elicit improved
prompts from the LLM.

## Page 17

12.6 • E VALUATING PROMPTED LANGUAGE MODELS 17
Prompt Improvement Prompt
I'm trying to write a zero-shot classifier. My current prompt is:
fpromptg
But it gets the following examples wrong: ferrorstrg
Based on these examples the problem with this prompt is that fgradientg.
Based on the above information, I wrote fstepspergradientgdifferent
improved prompts. Each prompt is wrapped with <START> and <END>.
Thefstepspergradientgnew prompts are:
12.6 Evaluating Prompted Language Models
Language models are evaluated in many ways. we introduced some evaluations for
in Section ??, including measuring the language model’s perplexity on a test set,
evaluating its accuracy on various NLP tasks, as well as benchmarks that help mea-
sure efﬁciency, toxicity, fairness, and so on. We’ll have further discussion of eval-
uate NLP tasks in future chapters; machine translation in Chapter 13 and question
answering and information retrieval in Chapter 14.
Here we just brieﬂy show the mechanism for measuring accuracy in a prompt-
ing setup for tests that have multiple-choice questions. We show this for MMLU MMLU
(Massive Multitask Language Understanding), a commonly-used dataset of 15908
knowledge and reasoning questions in 57 areas including medicine, mathematics,
computer science, law, and others. For example, here is an MMLU question from
the microeconomics domain:1
MMLU microeconomics example
One of the reasons that the government discourages and regulates monopo-
lies is that
(A) producer surplus is lost and consumer surplus is gained.
(B) monopoly prices ensure productive efﬁciency but cost society allocative
efﬁciency.
(C) monopoly ﬁrms do not engage in signiﬁcant research and development.
(D) consumer surplus is lost with higher prices and lower levels of output.
Fig. 12.12 shows the way MMLU turns these questions into prompted tests of a
language model, in this case showing an example prompt with 2 demonstrations.
12.7 Model Alignment with Human Preferences: RLHF
and DPO
TBD
1For those of you whose economics is a bit rusty, the correct answer is (D).

## Page 18

18 CHAPTER 12 • M ODEL ALIGNMENT , PROMPTING ,AND IN-CONTEXT LEARNING
MMLU mathematics prompt
The following are multiple choice questions about high school mathematics.
How many numbers are in the list 25, 26, ..., 100?
(A) 75 (B) 76 (C) 22 (D) 23
Answer: B
Compute i+i2+i3++i258+i259.
(A) -1 (B) 1 (C) i (D) -i
Answer: A
If 4 daps = 7 yaps, and 5 yaps = 3 baps, how many daps equal 42 baps?
(A) 28 (B) 21 (C) 40 (D) 30
Answer:
Figure 12.12 Sample 2-shot prompt from MMLU testing high-school mathematics. (The
correct answer is (C)).
12.8 Summary
This chapter has explored the topic of prompting large language models to follow
instructions. Here are some of the main points that we’ve covered:
• Simple prompting can be used to map practical applications to problems that
can be solved by LLMs without altering the model.
• Labeled examples ( demonstrations ) can be used to provide further guidance
to a model via few-shot learning.
• Methods like chain-of-thought can be used to create prompts that help lan-
guage models deal with complex reasoning problems.
• Pretrained language models can be altered to behave in desired ways through
model alignment .
• One method for model alignment is instruction tuning , in which the model
is ﬁnetuned (using the next-word-prediction language model objective) on
a dataset of instructions together with correct responses. Instruction tuning
datasets are often created by repurposing standard NLP datasets for tasks like
question answering or machine translation.
Bibliographical and Historical Notes

## Page 19

Bibliographical and Historical Notes 19
Bianchi, F., M. Suzgun, G. Attanasio, P. Rottger, D. Juraf-
sky, T. Hashimoto, and J. Zou. 2024. Safety-tuned LLa-
MAs: Lessons from improving the safety of large lan-
guage models that follow instructions. ICLR .
Brown, T., B. Mann, N. Ryder, M. Subbiah, J. Kaplan,
P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger,
T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu,
C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,
S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,
A. Radford, I. Sutskever, and D. Amodei. 2020. Language
models are few-shot learners. NeurIPS , volume 33.
Cheng, M., E. Durmus, and D. Jurafsky. 2023. Marked per-
sonas: Using natural language prompts to measure stereo-
types in language models. ACL.
Cobbe, K., V . Kosaraju, M. Bavarian, M. Chen, H. Jun,
L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano,
C. Hesse, and J. Schulman. 2021. Training veriﬁers to
solve math word problems. ArXiv preprint.
Crosbie, J. and E. Shutova. 2022. Induction heads as an
essential mechanism for pattern matching in in-context
learning. ArXiv preprint.
Elhage, N., N. Nanda, C. Olsson, T. Henighan, N. Joseph,
B. Mann, A. Askell, Y . Bai, A. Chen, T. Conerly, N. Das-
Sarma, D. Drain, D. Ganguli, Z. Hatﬁeld-Dodds, D. Her-
nandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse,
D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCan-
dlish, and C. Olah. 2021. A mathematical framework for
transformer circuits. White paper.
Gehman, S., S. Gururangan, M. Sap, Y . Choi, and N. A.
Smith. 2020. RealToxicityPrompts: Evaluating neu-
ral toxic degeneration in language models. Findings of
EMNLP .
Honovich, O., U. Shaham, S. R. Bowman, and O. Levy.
2023. Instruction induction: From few examples to natu-
ral language task descriptions. ACL.
Iyer, S., X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig,
P. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura, X. Li,
B. O’Horo, G. Pereyra, J. Wang, C. Dewan, A. Celiky-
ilmaz, L. Zettlemoyer, and V . Stoyanov. 2022. Opt-
iml: Scaling language model instruction meta learning
through the lens of generalization. ArXiv preprint.
Khattab, O., A. Singhvi, P. Maheshwari, Z. Zhang, K. San-
thanam, S. Haq, A. Sharma, T. T. Joshi, H. Moazam,
H. Miller, M. Zaharia, and C. Potts. 2024. DSPy: Compil-
ing declarative language model calls into self-improving
pipelines. ICLR .
Lin, C.-Y . 2004. ROUGE: A package for automatic evalua-
tion of summaries. ACL 2004 Workshop on Text Summa-
rization Branches Out .
Longpre, S., L. Hou, T. Vu, A. Webson, H. W. Chung, Y . Tay,
D. Zhou, Q. V . Le, B. Zoph, J. Wei, and A. Roberts. 2023.
The Flan collection: Designing data and methods for ef-
fective instruction tuning. ICML .
Min, S., X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Ha-
jishirzi, and L. Zettlemoyer. 2022. Rethinking the role of
demonstrations: What makes in-context learning work?
EMNLP .
Mishra, S., D. Khashabi, C. Baral, and H. Hajishirzi. 2022.
Cross-task generalization via natural language crowd-
sourcing instructions. ACL.Olsson, C., N. Elhage, N. Nanda, N. Joseph, N. DasSarma,
T. Henighan, B. Mann, A. Askell, Y . Bai, A. Chen, et al.
2022. In-context learning and induction heads. ArXiv
preprint.
Ouyang, L., J. Wu, X. Jiang, D. Almeida, C. Wainwright,
P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,
J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens,
A. Askell, P. Welinder, P. Christiano, J. Leike, and
R. Lowe. 2022. Training language models to follow in-
structions with human feedback. NeurIPS , volume 35.
Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu:
A method for automatic evaluation of machine transla-
tion. ACL.
Prasad, A., P. Hase, X. Zhou, and M. Bansal. 2023. GrIPS:
Gradient-free, edit-based instruction search for prompt-
ing large language models. EACL .
Pryzant, R., D. Iter, J. Li, Y . Lee, C. Zhu, and M. Zeng. 2023.
Automatic prompt optimization with “gradient descent”
and beam search. EMNLP .
Rajpurkar, P., R. Jia, and P. Liang. 2018. Know what you
don’t know: Unanswerable questions for SQuAD. ACL.
Rajpurkar, P., J. Zhang, K. Lopyrev, and P. Liang. 2016.
SQuAD: 100,000+ questions for machine comprehension
of text. EMNLP .
Reynolds, L. and K. McDonell. 2021. Prompt program-
ming for large language models: Beyond the few-shot
paradigm. CHI 2021 .
Russell, S. and P. Norvig. 2002. Artiﬁcial Intelligence: A
Modern Approach , 2nd edition. Prentice Hall.
Salvetti, F., J. B. Lowe, and J. H. Martin. 2016. A tangled
web: The faint signals of deception in text - boulder lies
and truth corpus (BLT-C). LREC .
Sheng, E., K.-W. Chang, P. Natarajan, and N. Peng. 2019.
The woman worked as a babysitter: On biases in language
generation. EMNLP .
Singh, S., F. Vargus, D. D’souza, B. F. Karlsson, A. Ma-
hendiran, W.-Y . Ko, H. Shandilya, J. Patel, D. Mat-
aciunas, L. O’Mahony, M. Zhang, R. Hettiarachchi,
J. Wilson, M. Machado, L. S. Moura, D. Krzemi ´nski,
H. Fadaei, I. Erg ¨un, I. Okoh, A. Alaagib, O. Mu-
dannayake, Z. Alyafeai, V . M. Chien, S. Ruder,
S. Guthikonda, E. A. Alghamdi, S. Gehrmann, N. Muen-
nighoff, M. Bartolo, J. Kreutzer, A. ¨U¨Ust¨un, M. Fadaee,
and S. Hooker. 2024. Aya dataset: An open-access collec-
tion for multilingual instruction tuning. ArXiv preprint.
Suzgun, M., N. Scales, N. Sch ¨arli, S. Gehrmann, Y . Tay,
H. W. Chung, A. Chowdhery, Q. Le, E. Chi, D. Zhou, and
J. Wei. 2023. Challenging BIG-bench tasks and whether
chain-of-thought can solve them. ACL Findings .
Wang, Y ., S. Mishra, P. Alipoormolabashi, Y . Kordi,
A. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran,
A. Arunkumar, D. Stap, E. Pathak, G. Karamanolakis,
H. Lai, I. Purohit, I. Mondal, J. Anderson, K. Kuznia,
K. Doshi, K. K. Pal, M. Patel, M. Moradshahi, M. Par-
mar, M. Purohit, N. Varshney, P. R. Kaza, P. Verma,
R. S. Puri, R. Karia, S. Doshi, S. K. Sampat, S. Mishra,
S. Reddy A, S. Patro, T. Dixit, and X. Shen. 2022. Super-
NaturalInstructions: Generalization via declarative in-
structions on 1600+ NLP tasks. EMNLP .

## Page 20

20 Chapter 12 • Model Alignment, Prompting, and In-Context Learning
Webson, A. and E. Pavlick. 2022. Do prompt-based models
really understand the meaning of their prompts? NAACL
HLT.
Wei, J., X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi,
Q. V . Le, D. Zhou, et al. 2022. Chain-of-thought prompt-
ing elicits reasoning in large language models. NeurIPS ,
volume 35.
Zhou, Y ., A. I. Muresanu, Z. Han, K. Paster, S. Pitis,
H. Chan, and J. Ba. 2023. Large language models are
human-level prompt engineers. The Eleventh Interna-
tional Conference on Learning Representations .

