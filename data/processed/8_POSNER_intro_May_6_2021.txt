# 8_POSNER_intro_May_6_2021

## Page 1

Sequence Labeling for Part of Speech and Named EntitiesPart of Speech Tagging

## Page 2

Parts of SpeechFrom the earliest linguistic traditions (Yaskaand Panini 5thC. BCE, Aristotle 4thC. BCE), the idea that words can be classified into grammatical categories•part of speech, word classes, POS, POS tags8 parts of speech attributed to Dionysius Thraxof Alexandria (c. 1stC. BCE): •noun, verb, pronoun, preposition, adverb, conjunction, participle, article •These categories are relevant for NLP today.

## Page 3

Two classes of words: Open vs. ClosedClosed class words•Relatively fixed membership•Usually functionwords: short, frequent words with grammatical function•determiners: a, an, the•pronouns: she, he, I•prepositions: on, under, over, near, by, …Open class words•Usually contentwords: Nouns, Verbs, Adjectives, Adverbs•Plus interjections: oh, ouch, uh-huh, yes, hello•New nouns and verbs like iPhone or to fax

## Page 4

Open class ("content") words
Closed class ("function")Nouns
VerbsProperCommonAuxiliaryMainAdjectivesAdverbsPrepositionsParticlesDeterminersConjunctionsPronouns… more… moreJanetItalycat,  catsmangoeatwentcanhadold   green   tastyslowly yesterday
to withoff   upthe someand orthey itsNumbers122,312oneInterjectionsOw  hello

## Page 5

Part-of-Speech TaggingAssigning a part-of-speech to each word in a text. Words often have more than one POS. book:•VERB: (Bookthat flight) •NOUN: (Hand me that book).

## Page 6

Part-of-Speech Tagging8.2•PART-OF-SPEECHTAGGING5
willNOUNAUXVERBDETNOUNJanetbackthebillPart of Speech Taggerx1x2x3x4x5y1y2y3y4y5
Figure 8.3The task of part-of-speech tagging: mapping from input wordsx1,x2,. . . ,xntooutput POS tagsy1,y2,. . . ,yn.thought thatyour ﬂight was earlier). The goal of POS-tagging is toresolvetheseambiguityresolutionambiguities, choosing the proper tag for the context.Theaccuracyof part-of-speech tagging algorithms (the percentage of test setaccuracytags that match human gold labels) is extremely high. One study found accuraciesover 97% across 15 languages from the Universal Dependency (UD) treebank(Wuand Dredze, 2019). Accuracies on various English treebanks are also 97% (no matterthe algorithm; HMMs, CRFs, BERT perform similarly). This 97% number is alsoabout the human performance on this task, at least for English(Manning, 2011).Types: WSJ BrownUnambiguous(1 tag) 44,432 (86%) 45,799 (85%)Ambiguous(2+ tags) 7,025 (14%) 8,050 (15%)Tokens:Unambiguous(1 tag) 577,421 (45%) 384,349 (33%)Ambiguous(2+ tags) 711,780 (55%) 786,646 (67%)Figure 8.4Tag ambiguity in the Brown and WSJ corpora (Treebank-3 45-tag tagset).We’ll introduce algorithms for the task in the next few sections, but ﬁrst let’sexplore the task. Exactly how hard is it? Fig.8.4shows that most word types(85-86%) are unambiguous (Janetis always NNP,hesitantlyis always RB). But theambiguous words, though accounting for only 14-15% of the vocabulary, are verycommon, and 55-67% of word tokens in running text are ambiguous. Particularlyambiguous common words includethat,back,down,putandset; here are someexamples of the 6 different parts of speech for the wordback:earnings growth took aback/JJseata small building in theback/NNa clear majority of senatorsback/VBPthe billDave began toback/VBtoward the doorenable the country to buyback/RPdebtI was twenty-oneback/RBthenNonetheless, many words are easy to disambiguate, because their different tagsaren’t equally likely. For example,acan be a determiner or the lettera, but thedeterminer sense is much more likely.This idea suggests a usefulbaseline: given an ambiguous word, choose the tagwhich ismost frequentin the training corpus. This is a key concept:Most Frequent Class Baseline:Always compare a classiﬁer against a baseline atleast as good as the most frequent class baseline (assigning each token to the classit occurred in most often in the training set).Map from sequence x1,…,xnof words to y1,…,ynof POS tags 

## Page 7

"Universal Dependencies" Tagset2CHAPTER8•SEQUENCELABELING FORPARTS OFSPEECH ANDNAMEDENTITIES8.1 (Mostly) English Word ClassesUntil now we have been using part-of-speech terms likenounandverbrather freely.In this section we give more complete deﬁnitions. While word classes do havesemantic tendencies—adjectives, for example, often describepropertiesand nounspeople— parts of speech are deﬁned instead based on their grammatical relationshipwith neighboring words or the morphological properties about their afﬁxes.Tag Description ExampleOpen ClassADJAdjective: noun modiﬁers describing propertiesred,young,awesomeADVAdverb: verb modiﬁers of time, place, mannervery,slowly,home,yesterdayNOUNwords for persons, places, things, etc.algorithm,cat,mango,beautyVERBwords for actions and processesdraw,provide,goPROPNProper noun: name of a person, organization, place, etc..Regina,IBM,ColoradoINTJInterjection: exclamation, greeting, yes/no response, etc.oh,um,yes,helloClosed Class WordsADPAdposition (Preposition/Postposition): marks a noun’sspacial, temporal, or other relationin, on, by underAUXAuxiliary: helping verb marking tense, aspect, mood, etc.,can, may, should, areCCONJCoordinating Conjunction: joins two phrases/clausesand,or,butDETDeterminer: marks noun phrase propertiesa, an, the, thisNUMNumeralone, two, ﬁrst, secondPARTParticle: a preposition-like form used together with a verbup, down, on, off, in, out, at, byPRONPronoun: a shorthand for referring to an entity or eventshe, who, I, othersSCONJSubordinating Conjunction: joins a main clause with asubordinate clause such as a sentential complementthat,whichOtherPUNCTPunctuation˙, , ()SYMSymbols like $ or emoji $, %XOther asdf, qwfgFigure 8.1The 17 parts of speech in the Universal Dependencies tagset(Nivre et al., 2016a). Features canbe added to make ﬁner-grained distinctions (with properties like number, case, deﬁniteness, and so on).Parts of speech fall into two broad categories:closed classandopen class.closed classopen classClosed classes are those with relatively ﬁxed membership, such as prepositions—new prepositions are rarely coined. By contrast, nouns and verbs are open classes—new nouns and verbs likeiPhoneorto faxare continually being created or borrowed.Closed class words are generallyfunction wordslikeof,it,and, oryou, which tendfunction wordto be very short, occur frequently, and often have structuring uses in grammar.Four major open classes occur in the languages of the world:nouns(includingproper nouns),verbs,adjectives, andadverbs, as well as the smaller open class ofinterjections. English has all ﬁve, although not every language does.Nounsare words for people, places, or things, but include others as well.Com-nounmon nounsinclude concrete terms likecatandmango, abstractions likealgorithmcommon nounandbeauty, and verb-like terms likepacingas inHis pacing to and fro became quiteannoying. Nouns in English can occur with determiners (a goat, its bandwidth)take possessives (IBM’s annual revenue), and may occur in the plural (goats, abaci).Many languages, including English, divide common nouns intocount nounsandcount nounmass nouns. Count nouns can occur in the singular and plural (goat/goats, rela-mass nountionship/relationships) and can be counted (one goat, two goats). Mass nouns areused when something is conceptualized as a homogeneous group. Sosnow, salt, andcommunismare not counted (i.e.,*two snowsor*two communisms).Proper nouns,proper nounlikeRegina,Colorado, andIBM, are names of speciﬁc persons or entities.Nivreet al. 2016

## Page 8

Sample "Tagged" English sentencesThere/PROwere/VERB70/NUMchildren/NOUNthere/ADV./PUNCPreliminary/ADJfindings/NOUNwere/AUXreported/VERBin/ADPtoday/NOUN’s/PARTNew/PROPNEngland/PROPNJournal/PROPNof/ADPMedicine/PROPN

## Page 9

Why Part of Speech Tagging?◦Can be useful for other NLP tasks◦Parsing: POS tagging can improve syntactic parsing◦MT: reordering of adjectives and nouns (say from Spanish to English)◦Sentiment or affective tasks: may want to distinguish adjectives or other POS◦Text-to-speech (how do we pronounce “lead”or "object"?)◦Or linguistic or language-analytic computational tasks◦Need to control for POS when studying linguistic change like creation of new words, or meaning shift◦Or control for POS in measuring meaning similarity or difference

## Page 10

How difficult is POS tagging in English?Roughly 15% of word types are ambiguous•Hence 85% of word types are unambiguous•Janetis always PROPN, hesitantlyis always ADV But those 15% tend to be very common. So ~60% of word tokens are ambiguousE.g., backearnings growth took a back/ADJ seata small building in the back/NOUNa clear majority of senators back/VERB the bill enable the country to buy back/PART debtI was twenty-one back/ADV then 

## Page 11

POS tagging performance in EnglishHow many tags are correct?  (Tag accuracy)◦About 97%◦Hasn't changed in the last 10+ years◦HMMs, CRFs, BERT perform similarly .◦Human accuracy about the sameBut baseline is 92%!◦Baseline is performance of stupidest possible method◦"Most frequent class baseline" is an important baseline for many tasks◦Tag every word with its most frequent tag◦(and tag unknown words as nouns)◦Partly easy because◦Many words are unambiguous

## Page 12

Sources of information for POS taggingJanet willback the billAUX/NOUN/VERB?           NOUN/VERB?Prior probabilities of word/tag•"will" is usually an AUXIdentity of neighboring words•"the" means the next word is probably not a verbMorphology and wordshape:◦Prefixesunable: un-®ADJ◦Suffixesimportantly: -ly®ADJ◦CapitalizationJanet: CAP®PROPN

## Page 13

Standard algorithms for POS taggingSupervised Machine Learning Algorithms:•Hidden Markov Models•Conditional Random Fields (CRF)/ Maximum Entropy Markov Models (MEMM)•Neural sequence models (RNNs or Transformers)•Large Language Models (like BERT), finetunedAll required a hand-labeled training set, all about equal performance (97% on English)All make use of information sources we discussed•Via human created features: HMMs and CRFs•Via representation learning:  Neural LMs

## Page 14

Sequence Labeling for Part of Speech and Named EntitiesPart of Speech Tagging

## Page 15

Sequence Labeling for Part of Speech and Named EntitiesNamed Entity Recognition (NER)

## Page 16

Named Entities◦Named entity, in its core usage, means anything that can be referred to with a proper name. Most common 4 tags:◦PER(Person): “Marie Curie”◦LOC(Location): “New York City” ◦ORG(Organization): “Stanford University”◦GPE(Geo-Political Entity): "Boulder, Colorado"◦Often multi-word phrases◦But the term is also extended to things that aren't entities:◦dates, times, prices

## Page 17

Named Entity taggingThe task of named entity recognition (NER):•find spans of text that constitute proper names•tag the type of the entity. 

## Page 18

NER output6CHAPTER8•SEQUENCELABELING FORPARTS OFSPEECH ANDNAMEDENTITIESThe most-frequent-tag baseline has an accuracy of about 92%1. The baselinethus differs from the state-of-the-art and human ceiling (97%) by only 5%.8.3 Named Entities and Named Entity TaggingPart of speech tagging can tell us that words likeJanet,Stanford University, andColoradoare all proper nouns; being a proper noun is a grammatical property ofthese words. But viewed from a semantic perspective, these proper nouns refer todifferent kinds of entities: Janet is a person, Stanford University is an organization,..and Colorado is a location.Anamed entityis, roughly speaking, anything that can be referred to with anamed entityproper name: a person, a location, an organization. The task ofnamed entity recog-nition(NER) is to ﬁnd spans of text that constitute proper names and tag the type ofnamed entityrecognitionNERthe entity. Four entity tags are most common:PER(person),LOC(location),ORG(organization), orGPE(geo-political entity). However, the termnamed entityiscommonly extended to include things that aren’t entities per se, including dates,times, and other kinds of temporal expressions, and even numerical expressions likeprices. Here’s an example of the output of an NER tagger:Citing high fuel prices,[ORGUnited Airlines]said[TIMEFriday]ithas increased fares by[MONEY$6]per round trip on ﬂights to somecities also served by lower-cost carriers.[ORGAmerican Airlines],aunit of[ORGAMR Corp.], immediately matched the move, spokesman[PERTim Wagner]said.[ORGUnited], a unit of[ORGUAL Corp.],said the increase took effect[TIMEThursday]and applies to mostroutes where it competes against discount carriers, such as[LOCChicago]to[LOCDallas]and[LOCDenver]to[LOCSan Francisco].The text contains 13 mentions of named entities including 5 organizations, 4 loca-tions, 2 times, 1 person, and 1 mention of money. Figure8.5shows typical genericnamed entity types. Many applications will also need to use speciﬁc entity types likeproteins, genes, commercial products, or works of art.Type Tag Sample Categories Example sentencesPeoplePERpeople, charactersTuringis a giant of computer science.OrganizationORGcompanies, sports teams TheIPCCwarned about the cyclone.LocationLOCregions, mountains, seasMt. Sanitasis inSunshine Canyon.Geo-Political EntityGPEcountries, statesPalo Altois raising the fees for parking.Figure 8.5A list of generic named entity types with the kinds of entities they refer to.Named entity tagging is a useful ﬁrst step in lots of natural language understand-ing tasks. In sentiment analysis we might want to know a consumer’s sentimenttoward a particular entity. Entities are a useful ﬁrst stage in question answering,or for linking text to information in structured knowledge sources like Wikipedia.And named entity tagging is also central to natural language understanding tasksof building semantic representations, like extracting events and the relationship be-tween participants.Unlike part-of-speech tagging, where there is no segmentation problem sinceeach word gets one tag, the task of named entity recognition is to ﬁnd and label1In English, on the WSJ corpus, tested on sections 22-24.

## Page 19

Why NER?Sentiment analysis: consumer’s sentiment toward a particular company or person?Question Answering: answer questions about an entity?Information Extraction: Extracting facts about entities from text.

## Page 20

Why NER is hard1)Segmentation•In POS tagging, no segmentation problem since each word gets one tag.•In NER we have to find and segment the entities!2)Type ambiguity8.3•NAMEDENTITIES ANDNAMEDENTITYTAGGING7spansof text, and is difﬁcult partly because of the ambiguity of segmentation; weneed to decide what’s an entity and what isn’t, and where the boundaries are. Indeed,most words in a text will not be named entities. Another difﬁculty is caused by typeambiguity. The mentionJFKcan refer to a person, the airport in New York, or anynumber of schools, bridges, and streets around the United States. Some examples ofthis kind of cross-type confusion are given in Figure8.6.[PERWashington] was born into slavery on the farm of James Burroughs.[ORGWashington] went up 2 games to 1 in the four-game series.Blair arrived in [LOCWashington] for what may well be his last state visit.In June, [GPEWashington] passed a primary seatbelt law.Figure 8.6Examples of type ambiguities in the use of the nameWashington.The standard approach to sequence labeling for a span-recognition problem likeNER isBIOtagging(Ramshaw and Marcus, 1995). This is a method that allows usto treat NER like a word-by-word sequence labeling task, via tags that capture boththe boundary and the named entity type. Consider the following sentence:[PERJane Villanueva]of[ORGUnited], a unit of[ORGUnited AirlinesHolding], said the fare applies to the[LOCChicago]route.Figure8.7shows the same excerpt represented withBIOtagging, as well asBIOvariants calledIOtagging andBIOEStagging. In BIO tagging we label any tokenthatbeginsa span of interest with the labelB, tokens that occurinsidea span aretagged with anI, and any tokens outside of any span of interest are labeledO. Whilethere is only oneOtag, we’ll have distinctBandItags for each named entity class.The number of tags is thus 2n+1 tags, wherenis the number of entity types. BIOtagging can represent exactly the same information as the bracketed notation, but hasthe advantage that we can represent the task in the same simple sequence modelingway as part-of-speech tagging: assigning a single labelyito each input wordxi:WordsIO LabelBIO LabelBIOES LabelJaneI-PERB-PERB-PERVillanuevaI-PERI-PERE-PERofOOOUnitedI-ORGB-ORGB-ORGAirlinesI-ORGI-ORGI-ORGHoldingI-ORGI-ORGE-ORGdiscussedOOOtheOOOChicagoI-LOCB-LOCS-LOCrouteOOO.OOOFigure 8.7NER as a sequence model, showing IO, BIO, and BIOES taggings.We’ve also shown two variant tagging schemes: IO tagging, which loses someinformation by eliminating the B tag, and BIOES tagging, which adds an end tagEfor the end of a span, and a span tagSfor a span consisting of only one word.A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label eachtoken in a text with tags that indicate the presence (or absence) of particular kindsof named entities.

## Page 21

BIO TaggingHow can we turn this structured problem into a sequence problem like POS tagging, with one label per word?[PER Jane Villanueva] of [ORG United] , a unit of [ORG United Airlines Holding] , said the fare applies to the [LOC Chicago ] route. 

## Page 22

BIO Tagging[PER Jane Villanueva] of [ORG United] , a unit of [ORG United Airlines Holding] , said the fare applies to the [LOC Chicago ] route. 8.3•NAMEDENTITIES ANDNAMEDENTITYTAGGING7spansof text, and is difﬁcult partly because of the ambiguity of segmentation; weneed to decide what’s an entity and what isn’t, and where the boundaries are. Indeed,most words in a text will not be named entities. Another difﬁculty is caused by typeambiguity. The mentionJFKcan refer to a person, the airport in New York, or anynumber of schools, bridges, and streets around the United States. Some examples ofthis kind of cross-type confusion are given in Figure8.6.[PERWashington] was born into slavery on the farm of James Burroughs.[ORGWashington] went up 2 games to 1 in the four-game series.Blair arrived in [LOCWashington] for what may well be his last state visit.In June, [GPEWashington] passed a primary seatbelt law.Figure 8.6Examples of type ambiguities in the use of the nameWashington.The standard approach to sequence labeling for a span-recognition problem likeNER isBIOtagging(Ramshaw and Marcus, 1995). This is a method that allows usto treat NER like a word-by-word sequence labeling task, via tags that capture boththe boundary and the named entity type. Consider the following sentence:[PERJane Villanueva]of[ORGUnited], a unit of[ORGUnited AirlinesHolding], said the fare applies to the[LOCChicago]route.Figure8.7shows the same excerpt represented withBIOtagging, as well asBIOvariants calledIOtagging andBIOEStagging. In BIO tagging we label any tokenthatbeginsa span of interest with the labelB, tokens that occurinsidea span aretagged with anI, and any tokens outside of any span of interest are labeledO. Whilethere is only oneOtag, we’ll have distinctBandItags for each named entity class.The number of tags is thus 2n+1 tags, wherenis the number of entity types. BIOtagging can represent exactly the same information as the bracketed notation, but hasthe advantage that we can represent the task in the same simple sequence modelingway as part-of-speech tagging: assigning a single labelyito each input wordxi:WordsIO LabelBIO LabelBIOES LabelJaneI-PERB-PERB-PERVillanuevaI-PERI-PERE-PERofOOOUnitedI-ORGB-ORGB-ORGAirlinesI-ORGI-ORGI-ORGHoldingI-ORGI-ORGE-ORGdiscussedOOOtheOOOChicagoI-LOCB-LOCS-LOCrouteOOO.OOOFigure 8.7NER as a sequence model, showing IO, BIO, and BIOES taggings.We’ve also shown two variant tagging schemes: IO tagging, which loses someinformation by eliminating the B tag, and BIOES tagging, which adds an end tagEfor the end of a span, and a span tagSfor a span consisting of only one word.A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label eachtoken in a text with tags that indicate the presence (or absence) of particular kindsof named entities.8.3•NAMEDENTITIES ANDNAMEDENTITYTAGGING7spansof text, and is difﬁcult partly because of the ambiguity of segmentation; weneed to decide what’s an entity and what isn’t, and where the boundaries are. Indeed,most words in a text will not be named entities. Another difﬁculty is caused by typeambiguity. The mentionJFKcan refer to a person, the airport in New York, or anynumber of schools, bridges, and streets around the United States. Some examples ofthis kind of cross-type confusion are given in Figure8.6.[PERWashington] was born into slavery on the farm of James Burroughs.[ORGWashington] went up 2 games to 1 in the four-game series.Blair arrived in [LOCWashington] for what may well be his last state visit.In June, [GPEWashington] passed a primary seatbelt law.Figure 8.6Examples of type ambiguities in the use of the nameWashington.The standard approach to sequence labeling for a span-recognition problem likeNER isBIOtagging(Ramshaw and Marcus, 1995). This is a method that allows usto treat NER like a word-by-word sequence labeling task, via tags that capture boththe boundary and the named entity type. Consider the following sentence:[PERJane Villanueva]of[ORGUnited], a unit of[ORGUnited AirlinesHolding], said the fare applies to the[LOCChicago]route.Figure8.7shows the same excerpt represented withBIOtagging, as well asBIOvariants calledIOtagging andBIOEStagging. In BIO tagging we label any tokenthatbeginsa span of interest with the labelB, tokens that occurinsidea span aretagged with anI, and any tokens outside of any span of interest are labeledO. Whilethere is only oneOtag, we’ll have distinctBandItags for each named entity class.The number of tags is thus 2n+1 tags, wherenis the number of entity types. BIOtagging can represent exactly the same information as the bracketed notation, but hasthe advantage that we can represent the task in the same simple sequence modelingway as part-of-speech tagging: assigning a single labelyito each input wordxi:WordsIO LabelBIO LabelBIOES LabelJaneI-PERB-PERB-PERVillanuevaI-PERI-PERE-PERofOOOUnitedI-ORGB-ORGB-ORGAirlinesI-ORGI-ORGI-ORGHoldingI-ORGI-ORGE-ORGdiscussedOOOtheOOOChicagoI-LOCB-LOCS-LOCrouteOOO.OOOFigure 8.7NER as a sequence model, showing IO, BIO, and BIOES taggings.We’ve also shown two variant tagging schemes: IO tagging, which loses someinformation by eliminating the B tag, and BIOES tagging, which adds an end tagEfor the end of a span, and a span tagSfor a span consisting of only one word.A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label eachtoken in a text with tags that indicate the presence (or absence) of particular kindsof named entities.Now we have one tag per token!!!

## Page 23

BIO TaggingB: token that begins a spanI: tokens inside a spanO: tokens outside of any span# of tags (where n is #entity types):1 O tag, nB tags, nI tagstotal of 2n+18.3•NAMEDENTITIES ANDNAMEDENTITYTAGGING7spansof text, and is difﬁcult partly because of the ambiguity of segmentation; weneed to decide what’s an entity and what isn’t, and where the boundaries are. Indeed,most words in a text will not be named entities. Another difﬁculty is caused by typeambiguity. The mentionJFKcan refer to a person, the airport in New York, or anynumber of schools, bridges, and streets around the United States. Some examples ofthis kind of cross-type confusion are given in Figure8.6.[PERWashington] was born into slavery on the farm of James Burroughs.[ORGWashington] went up 2 games to 1 in the four-game series.Blair arrived in [LOCWashington] for what may well be his last state visit.In June, [GPEWashington] passed a primary seatbelt law.Figure 8.6Examples of type ambiguities in the use of the nameWashington.The standard approach to sequence labeling for a span-recognition problem likeNER isBIOtagging(Ramshaw and Marcus, 1995). This is a method that allows usto treat NER like a word-by-word sequence labeling task, via tags that capture boththe boundary and the named entity type. Consider the following sentence:[PERJane Villanueva]of[ORGUnited], a unit of[ORGUnited AirlinesHolding], said the fare applies to the[LOCChicago]route.Figure8.7shows the same excerpt represented withBIOtagging, as well asBIOvariants calledIOtagging andBIOEStagging. In BIO tagging we label any tokenthatbeginsa span of interest with the labelB, tokens that occurinsidea span aretagged with anI, and any tokens outside of any span of interest are labeledO. Whilethere is only oneOtag, we’ll have distinctBandItags for each named entity class.The number of tags is thus 2n+1 tags, wherenis the number of entity types. BIOtagging can represent exactly the same information as the bracketed notation, but hasthe advantage that we can represent the task in the same simple sequence modelingway as part-of-speech tagging: assigning a single labelyito each input wordxi:WordsIO LabelBIO LabelBIOES LabelJaneI-PERB-PERB-PERVillanuevaI-PERI-PERE-PERofOOOUnitedI-ORGB-ORGB-ORGAirlinesI-ORGI-ORGI-ORGHoldingI-ORGI-ORGE-ORGdiscussedOOOtheOOOChicagoI-LOCB-LOCS-LOCrouteOOO.OOOFigure 8.7NER as a sequence model, showing IO, BIO, and BIOES taggings.We’ve also shown two variant tagging schemes: IO tagging, which loses someinformation by eliminating the B tag, and BIOES tagging, which adds an end tagEfor the end of a span, and a span tagSfor a span consisting of only one word.A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label eachtoken in a text with tags that indicate the presence (or absence) of particular kindsof named entities.8.3•NAMEDENTITIES ANDNAMEDENTITYTAGGING7spansof text, and is difﬁcult partly because of the ambiguity of segmentation; weneed to decide what’s an entity and what isn’t, and where the boundaries are. Indeed,most words in a text will not be named entities. Another difﬁculty is caused by typeambiguity. The mentionJFKcan refer to a person, the airport in New York, or anynumber of schools, bridges, and streets around the United States. Some examples ofthis kind of cross-type confusion are given in Figure8.6.[PERWashington] was born into slavery on the farm of James Burroughs.[ORGWashington] went up 2 games to 1 in the four-game series.Blair arrived in [LOCWashington] for what may well be his last state visit.In June, [GPEWashington] passed a primary seatbelt law.Figure 8.6Examples of type ambiguities in the use of the nameWashington.The standard approach to sequence labeling for a span-recognition problem likeNER isBIOtagging(Ramshaw and Marcus, 1995). This is a method that allows usto treat NER like a word-by-word sequence labeling task, via tags that capture boththe boundary and the named entity type. Consider the following sentence:[PERJane Villanueva]of[ORGUnited], a unit of[ORGUnited AirlinesHolding], said the fare applies to the[LOCChicago]route.Figure8.7shows the same excerpt represented withBIOtagging, as well asBIOvariants calledIOtagging andBIOEStagging. In BIO tagging we label any tokenthatbeginsa span of interest with the labelB, tokens that occurinsidea span aretagged with anI, and any tokens outside of any span of interest are labeledO. Whilethere is only oneOtag, we’ll have distinctBandItags for each named entity class.The number of tags is thus 2n+1 tags, wherenis the number of entity types. BIOtagging can represent exactly the same information as the bracketed notation, but hasthe advantage that we can represent the task in the same simple sequence modelingway as part-of-speech tagging: assigning a single labelyito each input wordxi:WordsIO LabelBIO LabelBIOES LabelJaneI-PERB-PERB-PERVillanuevaI-PERI-PERE-PERofOOOUnitedI-ORGB-ORGB-ORGAirlinesI-ORGI-ORGI-ORGHoldingI-ORGI-ORGE-ORGdiscussedOOOtheOOOChicagoI-LOCB-LOCS-LOCrouteOOO.OOOFigure 8.7NER as a sequence model, showing IO, BIO, and BIOES taggings.We’ve also shown two variant tagging schemes: IO tagging, which loses someinformation by eliminating the B tag, and BIOES tagging, which adds an end tagEfor the end of a span, and a span tagSfor a span consisting of only one word.A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label eachtoken in a text with tags that indicate the presence (or absence) of particular kindsof named entities.

## Page 24

BIO Tagging variants: IO and BIOES[PER Jane Villanueva] of [ORG United] , a unit of [ORG United Airlines Holding] , said the fare applies to the [LOC Chicago ] route. 8.3•NAMEDENTITIES ANDNAMEDENTITYTAGGING7spansof text, and is difﬁcult partly because of the ambiguity of segmentation; weneed to decide what’s an entity and what isn’t, and where the boundaries are. Indeed,most words in a text will not be named entities. Another difﬁculty is caused by typeambiguity. The mentionJFKcan refer to a person, the airport in New York, or anynumber of schools, bridges, and streets around the United States. Some examples ofthis kind of cross-type confusion are given in Figure8.6.[PERWashington] was born into slavery on the farm of James Burroughs.[ORGWashington] went up 2 games to 1 in the four-game series.Blair arrived in [LOCWashington] for what may well be his last state visit.In June, [GPEWashington] passed a primary seatbelt law.Figure 8.6Examples of type ambiguities in the use of the nameWashington.The standard approach to sequence labeling for a span-recognition problem likeNER isBIOtagging(Ramshaw and Marcus, 1995). This is a method that allows usto treat NER like a word-by-word sequence labeling task, via tags that capture boththe boundary and the named entity type. Consider the following sentence:[PERJane Villanueva]of[ORGUnited], a unit of[ORGUnited AirlinesHolding], said the fare applies to the[LOCChicago]route.Figure8.7shows the same excerpt represented withBIOtagging, as well asBIOvariants calledIOtagging andBIOEStagging. In BIO tagging we label any tokenthatbeginsa span of interest with the labelB, tokens that occurinsidea span aretagged with anI, and any tokens outside of any span of interest are labeledO. Whilethere is only oneOtag, we’ll have distinctBandItags for each named entity class.The number of tags is thus 2n+1 tags, wherenis the number of entity types. BIOtagging can represent exactly the same information as the bracketed notation, but hasthe advantage that we can represent the task in the same simple sequence modelingway as part-of-speech tagging: assigning a single labelyito each input wordxi:WordsIO LabelBIO LabelBIOES LabelJaneI-PERB-PERB-PERVillanuevaI-PERI-PERE-PERofOOOUnitedI-ORGB-ORGB-ORGAirlinesI-ORGI-ORGI-ORGHoldingI-ORGI-ORGE-ORGdiscussedOOOtheOOOChicagoI-LOCB-LOCS-LOCrouteOOO.OOOFigure 8.7NER as a sequence model, showing IO, BIO, and BIOES taggings.We’ve also shown two variant tagging schemes: IO tagging, which loses someinformation by eliminating the B tag, and BIOES tagging, which adds an end tagEfor the end of a span, and a span tagSfor a span consisting of only one word.A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label eachtoken in a text with tags that indicate the presence (or absence) of particular kindsof named entities.

## Page 25

Standard algorithms for NERSupervised Machine Learning given a human-labeled training set of text annotated with tags•Hidden Markov Models•Conditional Random Fields (CRF)/ Maximum Entropy Markov Models (MEMM)•Neural sequence models (RNNs or Transformers)•Large Language Models (like BERT), finetuned

## Page 26

Sequence Labeling for Part of Speech and Named EntitiesNamed Entity Recognition (NER)

