# H

## Page 1

Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ©2024. All
rights reserved. Draft of January 12, 2025.
CHAPTER
HPhonetics
The characters that make up the texts we’ve been discussing in this book aren’t just
random symbols. They are also an amazing scientiﬁc invention: a theoretical model
of the elements that make up human speech.
The earliest writing systems we know of (Sumerian, Chinese, Mayan) were
mainly logographic : one symbol representing a whole word. But from the ear-
liest stages we can ﬁnd, some symbols were also used to represent the sounds
that made up words. The cuneiform sign to the right pro-
nounced baand meaning “ration” in Sumerian could also
function purely as the sound /ba/. The earliest Chinese char-
acters we have, carved into bones for divination, similarly
contain phonetic elements. Purely sound-based writing systems, whether syllabic
(like Japanese hiragana ), alphabetic (like the Roman alphabet), or consonantal (like
Semitic writing systems), trace back to these early logo-syllabic systems, often as
two cultures came together. Thus, the Arabic, Aramaic, Hebrew, Greek, and Roman
systems all derive from a West Semitic script that is presumed to have been modiﬁed
by Western Semitic mercenaries from a cursive form of Egyptian hieroglyphs. The
Japanese syllabaries were modiﬁed from a cursive form of Chinese phonetic charac-
ters, which themselves were used in Chinese to phonetically represent the Sanskrit
in the Buddhist scriptures that came to China in the Tang dynasty.
This implicit idea that the spoken word is composed of smaller units of speech
underlies algorithms for both speech recognition (transcribing waveforms into text)
andtext-to-speech (converting text into waveforms). In this chapter we give a com-
putational perspective on phonetics , the study of the speech sounds used in the phonetics
languages of the world, how they are produced in the human vocal tract, how they
are realized acoustically, and how they can be digitized and processed.
H.1 Speech Sounds and Phonetic Transcription
A letter like ‘p’ or ‘a’ is already a useful model of the sounds of human speech,
and indeed we’ll see in Chapter 16 how to map between letters and waveforms.
Nonetheless, it is helpful to represent sounds slightly more abstractly. We’ll repre-
sent the pronunciation of a word as a string of phones , which are speech sounds, phone
each represented with symbols adapted from the Roman alphabet.
The standard phonetic representation for transcribing the world’s languages is
theInternational Phonetic Alphabet (IPA), an evolving standard ﬁrst developed in IPA
1888, But in this chapter we’ll instead represent phones with the ARPAbet (Shoup,
1980), a simple phonetic alphabet (Fig. H.1) that conveniently uses ASCII symbols
to represent an American-English subset of the IPA.
Many of the IPA and ARPAbet symbols are equivalent to familiar Roman let-
ters. So, for example, the ARPAbet phone [p]represents the consonant sound at the

## Page 2

2APPENDIX H • P HONETICS
ARPAbet IPA ARPAbet ARPAbet IPA ARPAbet
Symbol Symbol Word Transcription Symbol Symbol Word Transcription
[p] [p] p arsley [p aa r s l iy] [iy] [i] lily [l ih l iy]
[t] [t] t ea [t iy] [ih] [I] lily [l ih l iy]
[k] [k] c ook [k uh k] [ey] [eI] daisy [d ey z iy]
[b] [b] b ay [b ey] [eh] [E] pen [p eh n]
[d] [d] d ill [d ih l] [ae] [æ] aster [ae s t axr]
[g] [g] g arlic [g aa r l ix k] [aa] [A] poppy [p aa p iy]
[m] [m] m int [m ih n t] [ao] [O] orchid [ao r k ix d]
[n] [n] n utmeg [n ah t m eh g] [uh] [U] wood [w uh d]
[ng] [ N] baking [b ey k ix ng] [ow] [oU] lotus [l ow dx ax s]
[f] [f] f lour [f l aw axr] [uw] [u] tulip [t uw l ix p]
[v] [v] clov e [k l ow v] [ah] [2] butter [b ah dx axr]
[th] [ T] th ick [th ih k] [er] [Ç] bird [b er d]
[dh] [ D] th ose [dh ow z] [ay] [aI] iris [ay r ix s]
[s] [s] s oup [s uw p] [aw] [aU] ﬂower [f l aw axr]
[z] [z] eggs [eh g z] [oy] [oI] soil [s oy l]
[sh] [ S] squash [s k w aa sh] [ax] [@] pita [p iy t ax]
[zh] [ Z] ambros ia [ae m b r ow zh ax]
[ch] [t S] ch erry [ch eh r iy]
[jh] [d Z] j ar [jh aa r]
[l] [l] l icorice [l ih k axr ix sh]
[w] [w] kiw i [k iy w iy]
[r] [r] r ice [r ay s]
[y] [j] y ellow [y eh l ow]
[h] [h] h oney [h ah n iy]
Figure H.1 ARPAbet and IPA symbols for English consonants (left) and vowels (right).
beginning of platypus ,puma , and plantain , the middle of leopard , or the end of an-
telope . In general, however, the mapping between the letters of English orthography
and phones is relatively opaque ; a single letter can represent very different sounds
in different contexts. The English letter ccorresponds to phone [k] in cougar [k uw
g axr], but phone [s] in cell[s eh l]. Besides appearing as candk, the phone [k] can
appear as part of x(fox[f aa k s]), as ck(jackal [jh ae k el]) and as cc(raccoon [r ae
k uw n]). Many other languages, for example, Spanish, are much more transparent
in their sound-orthography mapping than English.
H.2 Articulatory Phonetics
Articulatory phonetics is the study of how these phones are produced as the variousarticulatory
phonetics
organs in the mouth, throat, and nose modify the airﬂow from the lungs.
The Vocal Organs
Figure H.2 shows the organs of speech. Sound is produced by the rapid movement
of air. Humans produce most sounds in spoken languages by expelling air from the
lungs through the windpipe (technically, the trachea ) and then out the mouth or
nose. As it passes through the trachea, the air passes through the larynx , commonly
known as the Adam’s apple or voice box. The larynx contains two small folds of

## Page 3

H.2 • A RTICULATORY PHONETICS 3
Figure H.2 The vocal organs, shown in side view. (Figure from OpenStax University
Physics, CC BY 4.0)
muscle, the vocal folds (often referred to non-technically as the vocal cords ), which
can be moved together or apart. The space between these two folds is called the
glottis . If the folds are close together (but not tightly closed), they will vibrate as air glottis
passes through them; if they are far apart, they won’t vibrate. Sounds made with the
vocal folds together and vibrating are called voiced ; sounds made without this vocal voiced sound
cord vibration are called unvoiced orvoiceless . V oiced sounds include [b], [d], [g], unvoiced sound
[v], [z], and all the English vowels, among others. Unvoiced sounds include [p], [t],
[k], [f], [s], and others.
The area above the trachea is called the vocal tract ; it consists of the oral tract
and the nasal tract . After the air leaves the trachea, it can exit the body through the
mouth or the nose. Most sounds are made by air passing through the mouth. Sounds
made by air passing through the nose are called nasal sounds ; nasal sounds (like nasal
English [m], [n], and [ng]) use both the oral and nasal tracts as resonating cavities.
Phones are divided into two main classes: consonants andvowels . Both kinds consonant
vowel of sounds are formed by the motion of air through the mouth, throat or nose. Con-
sonants are made by restriction or blocking of the airﬂow in some way, and can be
voiced or unvoiced. V owels have less obstruction, are usually voiced, and are gen-
erally louder and longer-lasting than consonants. The technical use of these terms is
much like the common usage; [p], [b], [t], [d], [k], [g], [f], [v], [s], [z], [r], [l], etc.,
are consonants; [aa], [ae], [ao], [ih], [aw], [ow], [uw], etc., are vowels. Semivow-
els(such as [y] and [w]) have some of the properties of both; they are voiced like
vowels, but they are short and less syllabic like consonants.

## Page 4

4APPENDIX H • P HONETICS
Consonants: Place of Articulation
Because consonants are made by restricting airﬂow, we can group them into classes
by their point of maximum restriction, their place of articulation (Fig. H.3).place of
articulation
(nasal tract)dentalbilabialglottalpalatalvelaralveolar
Figure H.3 Major English places of articulation.
Labial: Consonants whose main restriction is formed by the two lips coming to- labial
gether have a bilabial place of articulation. In English these include [p] as
inpossum , [b] as in bear, and [m] as in marmot . The English labiodental
consonants [v] and [f] are made by pressing the bottom lip against the upper
row of teeth and letting the air ﬂow through the space in the upper teeth.
Dental: Sounds that are made by placing the tongue against the teeth are dentals. dental
The main dentals in English are the [th] of thingand the [dh] of though , which
are made by placing the tongue behind the teeth with the tip slightly between
the teeth.
Alveolar: The alveolar ridge is the portion of the roof of the mouth just behind the alveolar
upper teeth. Most speakers of American English make the phones [s], [z], [t],
and [d] by placing the tip of the tongue against the alveolar ridge. The word
coronal is often used to refer to both dental and alveolar.
Palatal: The roof of the mouth (the palate ) rises sharply from the back of the palatal
palate alveolar ridge. The palato-alveolar sounds [sh] ( shrimp ), [ch] ( china), [zh]
(Asian ), and [jh] ( jar) are made with the blade of the tongue against the rising
back of the alveolar ridge. The palatal sound [y] of yakis made by placing the
front of the tongue up close to the palate.
Velar: Thevelum , or soft palate, is a movable muscular ﬂap at the very back of the velar
roof of the mouth. The sounds [k] ( cuckoo), [g] ( goose), and [N](kingﬁsher )
are made by pressing the back of the tongue up against the velum.
Glottal: The glottal stop [q] is made by closing the glottis (by bringing the vocal glottal
folds together).
Consonants: Manner of Articulation
Consonants are also distinguished by how the restriction in airﬂow is made, for ex-
ample, by a complete stoppage of air or by a partial blockage. This feature is called
themanner of articulation of a consonant. The combination of place and mannermanner of
articulation
of articulation is usually sufﬁcient to uniquely identify a consonant. Following are
the major manners of articulation for English consonants:
Astop is a consonant in which airﬂow is completely blocked for a short time. stop
This blockage is followed by an explosive sound as the air is released. The period
of blockage is called the closure , and the explosion is called the release . English

## Page 5

H.2 • A RTICULATORY PHONETICS 5
has voiced stops like [b], [d], and [g] as well as unvoiced stops like [p], [t], and [k].
Stops are also called plosives .
Thenasal sounds [n], [m], and [ng] are made by lowering the velum and allow- nasal
ing air to pass into the nasal cavity.
Infricatives , airﬂow is constricted but not cut off completely. The turbulent fricatives
airﬂow that results from the constriction produces a characteristic “hissing” sound.
The English labiodental fricatives [f] and [v] are produced by pressing the lower
lip against the upper teeth, allowing a restricted airﬂow between the upper teeth.
The dental fricatives [th] and [dh] allow air to ﬂow around the tongue between the
teeth. The alveolar fricatives [s] and [z] are produced with the tongue against the
alveolar ridge, forcing air over the edge of the teeth. In the palato-alveolar fricatives
[sh] and [zh], the tongue is at the back of the alveolar ridge, forcing air through a
groove formed in the tongue. The higher-pitched fricatives (in English [s], [z], [sh]
and [zh]) are called sibilants . Stops that are followed immediately by fricatives are sibilants
called affricates ; these include English [ch] ( chicken ) and [jh] ( giraffe ).
Inapproximants , the two articulators are close together but not close enough to approximant
cause turbulent airﬂow. In English [y] ( yellow ), the tongue moves close to the roof
of the mouth but not close enough to cause the turbulence that would characterize a
fricative. In English [w] ( wood), the back of the tongue comes close to the velum.
American [r] can be formed in at least two ways; with just the tip of the tongue
extended and close to the palate or with the whole tongue bunched up near the palate.
[l] is formed with the tip of the tongue up against the alveolar ridge or the teeth, with
one or both sides of the tongue lowered to allow air to ﬂow over it. [l] is called a
lateral sound because of the drop in the sides of the tongue.
Ataporﬂap[dx] is a quick motion of the tongue against the alveolar ridge. The tap
consonant in the middle of the word lotus ([l ow dx ax s]) is a tap in most dialects of
American English; speakers of many U.K. dialects would use a [t] instead.
Vowels
Like consonants, vowels can be characterized by the position of the articulators as
they are made. The three most relevant parameters for vowels are what is called
vowel height , which correlates roughly with the height of the highest part of the
tongue, vowel frontness orbackness , indicating whether this high point is toward
the front or back of the oral tract and whether the shape of the lips is rounded or
not. Figure H.4 shows the position of the tongue for different vowels.
boot [uw]closedvelumbat [ae]palatebeet [iy]tongue
Figure H.4 Tongue positions for English high front [iy], low front [ae]and high back [uw].
In the vowel [iy], for example, the highest point of the tongue is toward the
front of the mouth. In the vowel [uw], by contrast, the high-point of the tongue is
located toward the back of the mouth. V owels in which the tongue is raised toward
the front are called front vowels ; those in which the tongue is raised toward the Front vowel

## Page 6

6APPENDIX H • P HONETICS
back are called back vowels . Note that while both [ih] and [eh] are front vowels, back vowel
the tongue is higher for [ih] than for [eh]. V owels in which the highest point of the
tongue is comparatively high are called high vowels ; vowels with mid or low values high vowel
of maximum tongue height are called mid vowels orlow vowels , respectively.
front back
lowhigh
iy
ih
eh
aeuw
uh
ax
ahao
aay uw
eyowoy
ayaw
Figure H.5 The schematic “vowel space” for English vowels.
Figure H.5 shows a schematic characterization of the height of different vowels.
It is schematic because the abstract property height correlates only roughly with ac-
tual tongue positions; it is, in fact, a more accurate reﬂection of acoustic facts. Note
that the chart has two kinds of vowels: those in which tongue height is represented
as a point and those in which it is represented as a path. A vowel in which the tongue
position changes markedly during the production of the vowel is a diphthong . En- diphthong
glish is particularly rich in diphthongs.
The second important articulatory dimension for vowels is the shape of the lips.
Certain vowels are pronounced with the lips rounded (the same lip shape used for
whistling). These rounded vowels include [uw], [ao], and [ow]. rounded vowel
Syllables
Consonants and vowels combine to make a syllable . A syllable is a vowel-like (or syllable
sonorant ) sound together with some of the surrounding consonants that are most
closely associated with it. The word doghas one syllable, [d aa g] (in our dialect);
the word catnip has two syllables, [k ae t] and [n ih p]. We call the vowel at the
core of a syllable the nucleus . Initial consonants, if any, are called the onset . Onsets nucleus
onset with more than one consonant (as in strike [s t r ay k]), are called complex onsets .
Thecoda is the optional consonant or sequence of consonants following the nucleus. coda
Thus [d] is the onset of dog, and [g] is the coda. The rime , orrhyme , is the nucleus rime
plus coda. Figure H.6 shows some sample syllable structures.
The task of automatically breaking up a word into syllables is called syllabiﬁca-
tion. Syllable structure is also closely related to the phonotactics of a language. The syllabiﬁcation
term phonotactics means the constraints on which phones can follow each other in phonotactics
a language. For example, English has strong constraints on what kinds of conso-
nants can appear together in an onset; the sequence [zdr], for example, cannot be a
legal English syllable onset. Phonotactics can be represented by a language model
or ﬁnite-state model of phone sequences.

## Page 7

H.3 • P ROSODY 7
s
Rime
Coda
mNucleus
aeOnset
hs
Rime
Coda
nNucleus
iyOnset
rgs
Rime
Coda
zgNucleus
eh
Figure H.6 Syllable structure of ham,green ,eggs.s=syllable.
H.3 Prosody
Prosody is the study of the intonational and rhythmic aspects of language, and in prosody
particular the use of F0,energy , and duration to convey pragmatic, affective, or
conversation-interactional meanings.1We’ll introduce these acoustic quantities in
detail in the next section when we turn to acoustic phonetics, but brieﬂy we can
think of energy as the acoustic quality that we perceive as loudness, and F0 as the
frequency of the sound that is produced, the acoustic quality that we hear as the
pitch of an utterance. Prosody can be used to mark discourse structure , like the
difference between statements and questions, or the way that a conversation is struc-
tured. Prosody is used to mark the saliency of a particular word or phrase. Prosody
is heavily used for paralinguistic functions like conveying affective meanings like
happiness, surprise, or anger. And prosody plays an important role in managing
turn-taking in conversation.
H.3.1 Prosodic Prominence: Accent, Stress and Schwa
In a natural utterance of American English, some words sound more prominent than prominence
others, and certain syllables in these words are also more prominent than others.
What we mean by prominence is that these words or syllables are perceptually more
salient to the listener. Speakers make a word or syllable more salient in English
by saying it louder, saying it slower (so it has a longer duration), or by varying F0
during the word, making it higher or more variable.
Accent We represent prominence via a linguistic marker called pitch accent . Words pitch accent
or syllables that are prominent are said to bear (be associated with) a pitch accent.
Thus this utterance might be pronounced by accenting the underlined words:
(H.1) I’m a little surprised to hear it characterized as happy .
Lexical Stress The syllables that bear pitch accent are called accented syllables.
Not every syllable of a word can be accented: pitch accent has to be realized on the
syllable that has lexical stress . Lexical stress is a property of the word’s pronuncia- lexical stress
tion in dictionaries; the syllable that has lexical stress is the one that will be louder
or longer if the word is accented. For example, the word surprised is stressed on its
second syllable, not its ﬁrst. (Try stressing the other syllable by saying SURprised;
hopefully that sounds wrong to you). Thus, if the word surprised receives a pitch
accent in a sentence, it is the second syllable that will be stronger. The following ex-
1The word is used in a different but related way in poetry, to mean the study of verse metrical structure.

## Page 8

8APPENDIX H • P HONETICS
ample shows underlined accented words with the stressed syllable bearing the accent
(the louder, longer syllable) in boldface:
(H.2) I’m a little sur prised to hear it char acterized ashappy.
Stress is marked in dictionaries. The CMU dictionary (CMU, 1993), for ex-
ample, marks vowels with 0 (unstressed) or 1 (stressed) as in entries for counter :
[K AW1 N T ER0], or table : [T EY1 B AH0 L]. Difference in lexical stress can
affect word meaning; the noun content is pronounced [K AA1 N T EH0 N T], while
the adjective is pronounced [K AA0 N T EH1 N T].
Reduced Vowels and Schwa Unstressed vowels can be weakened even further to
reduced vowels , the most common of which is schwa ([ax]), as in the second vowel reduced vowel
schwa ofparakeet : [p ae r ax k iy t]. In a reduced vowel the articulatory gesture isn’t as
complete as for a full vowel. Not all unstressed vowels are reduced; any vowel, and
diphthongs in particular, can retain its full quality even in unstressed position. For
example, the vowel [iy] can appear in stressed position as in the word eat[iy t] or in
unstressed position as in the word carry [k ae r iy].
In summary, there is a continuum of prosodic prominence , for which it is often prominence
useful to represent levels like accented, stressed, full vowel, and reduced vowel.
H.3.2 Prosodic Structure
Spoken sentences have prosodic structure: some words seem to group naturally to-
gether, while some words seem to have a noticeable break or disjuncture between
them. Prosodic structure is often described in terms of prosodic phrasing , mean-prosodic
phrasing
ing that an utterance has a prosodic phrase structure in a similar way to it having
a syntactic phrase structure. For example, the sentence I wanted to go to London,
but could only get tickets for France seems to have two main intonation phrases ,intonation
phrase
their boundary occurring at the comma. Furthermore, in the ﬁrst phrase, there seems
to be another set of lesser prosodic phrase boundaries (often called intermediate
phrase s) that split up the words as I wantedjto gojto London . These kinds ofintermediate
phrase
intonation phrases are often correlated with syntactic structure constituents (Price
et al. 1991, Bennett and Elfner 2019).
Automatically predicting prosodic boundaries can be important for tasks like
TTS. Modern approaches use sequence models that take either raw text or text an-
notated with features like parse trees as input, and make a break/no-break decision
at each word boundary. They can be trained on data labeled for prosodic structure
like the Boston University Radio News Corpus (Ostendorf et al., 1995).
H.3.3 Tune
Two utterances with the same prominence and phrasing patterns can still differ
prosodically by having different tunes . The tune of an utterance is the rise and tune
fall of its F0 over time. A very obvious example of tune is the difference between
statements and yes-no questions in English. The same words can be said with a ﬁnal
F0 rise to indicate a yes-no question (called a question rise ): question rise
You    know    what    Imean ?
or a ﬁnal drop in F0 (called a ﬁnal fall ) to indicate a declarative intonation: ﬁnal fall

## Page 9

H.4 • A COUSTIC PHONETICS AND SIGNALS 9
You    know        what         Imean .
Languages make wide use of tune to express meaning (Xu, 2005). In English,
for example, besides this well-known rise for yes-no questions, a phrase containing
a list of nouns separated by commas often has a short rise called a continuation
riseafter each noun. Other examples include the characteristic English contours forcontinuation
rise
expressing contradiction and expressing surprise .
Linking Prominence and Tune
Pitch accents come in different varieties that are related to tune; high pitched accents,
for example, have different functions than low pitched accents. There are many
typologies of accent classes in different languages. One such typology is part of the
ToBI (Tone and Break Indices) theory of intonation (Silverman et al. 1992). Each ToBI
word in ToBI can be associated with one of ﬁve types of pitch accents shown in
in Fig. H.7. Each utterance in ToBI consists of a sequence of intonational phrases,
each of which ends in one of four boundary tones shown in Fig. H.7, representing boundary tone
the utterance ﬁnal aspects of tune. There are version of ToBI for many languages.
Pitch Accents Boundary Tones
H* peak accent L-L% “ﬁnal fall”: “declarative contour” of American
English
L* low accent L-H% continuation rise
L*+H scooped accent H-H% “question rise”: cantonical yes-no question
contour
L+H* rising peak accent H-L% ﬁnal level plateau
H+!H* step down
Figure H.7 The accent and boundary tones labels from the ToBI transcription system for
American English intonation (Beckman and Ayers 1997, Beckman and Hirschberg 1994).
H.4 Acoustic Phonetics and Signals
We begin with a very brief introduction to the acoustic waveform and its digitization
and frequency analysis; the interested reader is encouraged to consult the references
at the end of the chapter.
H.4.1 Waves
Acoustic analysis is based on the sine and cosine functions. Figure H.8 shows a plot
of a sine wave, in particular the function
y=Asin(2pft) (H.3)
where we have set the amplitude A to 1 and the frequency fto 10 cycles per second.
Recall from basic mathematics that two important characteristics of a wave are
itsfrequency andamplitude . The frequency is the number of times a second that a frequency
amplitude wave repeats itself, that is, the number of cycles . We usually measure frequency in
cycles per second . The signal in Fig. H.8 repeats itself 5 times in .5 seconds, hence
10 cycles per second. Cycles per second are usually called hertz (shortened to Hz), Hertz

## Page 10

10 APPENDIX H • P HONETICS
Time (s)0 0.5–1.01.0
0
0 0.1 0.2 0.3 0.4 0.5
Figure H.8 A sine wave with a frequency of 10 Hz and an amplitude of 1.
so the frequency in Fig. H.8 would be described as 10 Hz. The amplitude Aof a
sine wave is the maximum value on the Y axis. The period Tof the wave is the time period
it takes for one cycle to complete, deﬁned as
T=1
f(H.4)
Each cycle in Fig. H.8 lasts a tenth of a second; hence T=:1 seconds.
H.4.2 Speech Sound Waves
Let’s turn from hypothetical waves to sound waves. The input to a speech recog-
nizer, like the input to the human ear, is a complex series of changes in air pressure.
These changes in air pressure obviously originate with the speaker and are caused
by the speciﬁc way that air passes through the glottis and out the oral or nasal cav-
ities. We represent sound waves by plotting the change in air pressure over time.
One metaphor which sometimes helps in understanding these graphs is that of a ver-
tical plate blocking the air pressure waves (perhaps in a microphone in front of a
speaker’s mouth, or the eardrum in a hearer’s ear). The graph measures the amount
ofcompression orrarefaction (uncompression) of the air molecules at this plate.
Figure H.9 shows a short segment of a waveform taken from the Switchboard corpus
of telephone speech of the vowel [iy] from someone saying “she just had a baby”.
Time (s)0 0.03875–0.016970.02283
0
Figure H.9 A waveform of the vowel [iy] from an utterance shown later in Fig. H.13 on page 14. The y-axis
shows the level of air pressure above and below normal atmospheric pressure. The x-axis shows time. Notice
that the wave repeats regularly.
The ﬁrst step in digitizing a sound wave like Fig. H.9 is to convert the analog
representations (ﬁrst air pressure and then analog electric signals in a microphone)
into a digital signal. This analog-to-digital conversion has two steps: sampling and sampling
quantization . To sample a signal, we measure its amplitude at a particular time; the
sampling rate is the number of samples taken per second. To accurately measure a
wave, we must have at least two samples in each cycle: one measuring the positive

## Page 11

H.4 • A COUSTIC PHONETICS AND SIGNALS 11
part of the wave and one measuring the negative part. More than two samples per
cycle increases the amplitude accuracy, but fewer than two samples causes the fre-
quency of the wave to be completely missed. Thus, the maximum frequency wave
that can be measured is one whose frequency is half the sample rate (since every
cycle needs two samples). This maximum frequency for a given sampling rate is
called the Nyquist frequency . Most information in human speech is in frequenciesNyquist
frequency
below 10,000 Hz; thus, a 20,000 Hz sampling rate would be necessary for com-
plete accuracy. But telephone speech is ﬁltered by the switching network, and only
frequencies less than 4,000 Hz are transmitted by telephones. Thus, an 8,000 Hz
sampling rate is sufﬁcient for telephone-bandwidth speech like the Switchboard
corpus, while 16,000 Hz sampling is often used for microphone speech.
Even an 8,000 Hz sampling rate requires 8000 amplitude measurements for each
second of speech, so it is important to store amplitude measurements efﬁciently.
They are usually stored as integers, either 8 bit (values from -128–127) or 16 bit
(values from -32768–32767). This process of representing real-valued numbers as
integers is called quantization because the difference between two integers acts as quantization
a minimum granularity (a quantum size) and all values that are closer together than
this quantum size are represented identically.
Once data is quantized, it is stored in various formats. One parameter of these
formats is the sample rate and sample size discussed above; telephone speech is
often sampled at 8 kHz and stored as 8-bit samples, and microphone data is often
sampled at 16 kHz and stored as 16-bit samples. Another parameter is the number of
channels . For stereo data or for two-party conversations, we can store both channels channel
in the same ﬁle or we can store them in separate ﬁles. A ﬁnal parameter is individual
sample storage—linearly or compressed. One common compression format used for
telephone speech is m-law (often written u-law but still pronounced mu-law). The
intuition of log compression algorithms like m-law is that human hearing is more
sensitive at small intensities than large ones; the log represents small values with
more faithfulness at the expense of more error on large values. The linear (unlogged)
values are generally referred to as linear PCM values (PCM stands for pulse code PCM
modulation, but never mind that). Here’s the equation for compressing a linear PCM
sample value xto 8-bit m-law, (where m=255 for 8 bits):
F(x) =sgn(x)log(1+mjxj)
log(1+m) 1x1 (H.5)
There are a number of standard ﬁle formats for storing the resulting digitized wave-
ﬁle, such as Microsoft’s .wav and Apple’s AIFF all of which have special headers;
simple headerless “raw” ﬁles are also used. For example, the .wav format is a subset
of Microsoft’s RIFF format for multimedia ﬁles; RIFF is a general format that can
represent a series of nested chunks of data and control information. Figure H.10
shows a simple .wav ﬁle with a single data chunk together with its format chunk.
Figure H.10 Microsoft waveﬁle header format, assuming simple ﬁle with one chunk. Fol-
lowing this 44-byte header would be the data chunk.

## Page 12

12 APPENDIX H • P HONETICS
H.4.3 Frequency and Amplitude; Pitch and Loudness
Sound waves, like all waves, can be described in terms of frequency, amplitude, and
the other characteristics that we introduced earlier for pure sine waves. In sound
waves, these are not quite as simple to measure as they were for sine waves. Let’s
consider frequency. Note in Fig. H.9 that although not exactly a sine, the wave is
nonetheless periodic, repeating 10 times in the 38.75 milliseconds (.03875 seconds)
captured in the ﬁgure. Thus, the frequency of this segment of the wave is 10/.03875
or 258 Hz.
Where does this periodic 258 Hz wave come from? It comes from the speed of
vibration of the vocal folds; since the waveform in Fig. H.9 is from the vowel [iy], it
is voiced. Recall that voicing is caused by regular openings and closing of the vocal
folds. When the vocal folds are open, air is pushing up through the lungs, creating
a region of high pressure. When the folds are closed, there is no pressure from the
lungs. Thus, when the vocal folds are vibrating, we expect to see regular peaks in
amplitude of the kind we see in Fig. H.9, each major peak corresponding to an open-
ing of the vocal folds. The frequency of the vocal fold vibration, or the frequency
of the complex wave, is called the fundamental frequency of the waveform, oftenfundamental
frequency
abbreviated F0. We can plot F0 over time in a pitch track . Figure H.11 shows the F0
pitch track pitch track of a short question, “Three o’clock?” represented below the waveform.
Note the rise in F0 at the end of the question.
three o’clock
Time (s)0 0.5443750 Hz500 Hz
Figure H.11 Pitch track of the question “Three o’clock?”, shown below the waveﬁle. Note
the rise in F0 at the end of the question. Note the lack of pitch trace during the very quiet part
(the “o’” of “o’clock”; automatic pitch tracking is based on counting the pulses in the voiced
regions, and doesn’t work if there is no voicing (or insufﬁcient sound).
The vertical axis in Fig. H.9 measures the amount of air pressure variation; pres-
sure is force per unit area, measured in Pascals (Pa). A high value on the vertical
axis (a high amplitude) indicates that there is more air pressure at that point in time,
a zero value means there is normal (atmospheric) air pressure, and a negative value
means there is lower than normal air pressure (rarefaction).
In addition to this value of the amplitude at any point in time, we also often
need to know the average amplitude over some time range, to give us some idea
of how great the average displacement of air pressure is. But we can’t just take
the average of the amplitude values over a range; the positive and negative values
would (mostly) cancel out, leaving us with a number close to zero. Instead, we
generally use the RMS (root-mean-square) amplitude, which squares each number

## Page 13

H.4 • A COUSTIC PHONETICS AND SIGNALS 13
before averaging (making it positive), and then takes the square root at the end.
RMS amplitudeN
i=1=vuut1
NNX
i=1x2
i (H.6)
Thepower of the signal is related to the square of the amplitude. If the number power
of samples of a sound is N, the power is
Power =1
NNX
i=1x2
i (H.7)
Rather than power, we more often refer to the intensity of the sound, which intensity
normalizes the power to the human auditory threshold and is measured in dB. If P0
is the auditory threshold pressure = 2 10 5Pa, then intensity is deﬁned as follows:
Intensity =10log101
NP0NX
i=1x2
i (H.8)
Figure H.12 shows an intensity plot for the sentence “Is it a long movie?” from
the CallHome corpus, again shown below the waveform plot.
is it a long movie?
Time (s)0 1.1675
Figure H.12 Intensity plot for the sentence “Is it a long movie?”. Note the intensity peaks
at each vowel and the especially high peak for the word long.
Two important perceptual properties, pitch andloudness , are related to fre-
quency and intensity. The pitch of a sound is the mental sensation, or perceptual pitch
correlate, of fundamental frequency; in general, if a sound has a higher fundamen-
tal frequency we perceive it as having a higher pitch. We say “in general” because
the relationship is not linear, since human hearing has different acuities for different
frequencies. Roughly speaking, human pitch perception is most accurate between
100 Hz and 1000 Hz and in this range pitch correlates linearly with frequency. Hu-
man hearing represents frequencies above 1000 Hz less accurately, and above this
range, pitch correlates logarithmically with frequency. Logarithmic representation
means that the differences between high frequencies are compressed and hence not
as accurately perceived. There are various psychoacoustic models of pitch percep-
tion scales. One common model is the melscale (Stevens et al. 1937, Stevens and Mel

## Page 14

14 APPENDIX H • P HONETICS
V olkmann 1940). A mel is a unit of pitch deﬁned such that pairs of sounds which
are perceptually equidistant in pitch are separated by an equal number of mels. The
mel frequency mcan be computed from the raw acoustic frequency as follows:
m=1127ln (1+f
700) (H.9)
As we’ll see in Chapter 16, the mel scale plays an important role in speech
recognition.
Theloudness of a sound is the perceptual correlate of the power . So sounds with
higher amplitudes are perceived as louder, but again the relationship is not linear.
First of all, as we mentioned above when we deﬁned m-law compression, humans
have greater resolution in the low-power range; the ear is more sensitive to small
power differences. Second, it turns out that there is a complex relationship between
power, frequency, and perceived loudness; sounds in certain frequency ranges are
perceived as being louder than those in other frequency ranges.
Various algorithms exist for automatically extracting F0. In a slight abuse of ter-
minology, these are called pitch extraction algorithms. The autocorrelation method pitch extraction
of pitch extraction, for example, correlates the signal with itself at various offsets.
The offset that gives the highest correlation gives the period of the signal. There
are various publicly available pitch extraction toolkits; for example, an augmented
autocorrelation pitch tracker is provided with Praat (Boersma and Weenink, 2005).
H.4.4 Interpretation of Phones from a Waveform
Much can be learned from a visual inspection of a waveform. For example, vowels
are pretty easy to spot. Recall that vowels are voiced; another property of vowels is
that they tend to be long and are relatively loud (as we can see in the intensity plot in
Fig. H.12). Length in time manifests itself directly on the x-axis, and loudness is re-
lated to (the square of) amplitude on the y-axis. We saw in the previous section that
voicing is realized by regular peaks in amplitude of the kind we saw in Fig. H.9, each
major peak corresponding to an opening of the vocal folds. Figure H.13 shows the
waveform of the short sentence “she just had a baby”. We have labeled this wave-
form with word and phone labels. Notice that each of the six vowels in Fig. H.13,
[iy], [ax], [ae], [ax], [ey], [iy], all have regular amplitude peaks indicating voicing.
she just had a baby
sh iy j ax s h ae dx ax b ey b iy
Time (s)0 1.059
Figure H.13 A waveform of the sentence “She just had a baby” from the Switchboard corpus (conversation
4325). The speaker is female, was 20 years old in 1991, which is approximately when the recording was made,
and speaks the South Midlands dialect of American English.
For a stop consonant, which consists of a closure followed by a release, we can
often see a period of silence or near silence followed by a slight burst of amplitude.
We can see this for both of the [b]’s in baby in Fig. H.13.

## Page 15

H.4 • A COUSTIC PHONETICS AND SIGNALS 15
Another phone that is often quite recognizable in a waveform is a fricative. Re-
call that fricatives, especially very strident fricatives like [sh], are made when a
narrow channel for airﬂow causes noisy, turbulent air. The resulting hissy sounds
have a noisy, irregular waveform. This can be seen somewhat in Fig. H.13; it’s even
clearer in Fig. H.14, where we’ve magniﬁed just the ﬁrst word she.
she
sh iy
Time (s)0 0.257
Figure H.14 A more detailed view of the ﬁrst word “she” extracted from the waveﬁle in Fig. H.13. Notice
the difference between the random noise of the fricative [sh] and the regular voicing of the vowel [iy].
H.4.5 Spectra and the Frequency Domain
While some broad phonetic features (such as energy, pitch, and the presence of voic-
ing, stop closures, or fricatives) can be interpreted directly from the waveform, most
computational applications such as speech recognition (as well as human auditory
processing) are based on a different representation of the sound in terms of its com-
ponent frequencies. The insight of Fourier analysis is that every complex wave can
be represented as a sum of many sine waves of different frequencies. Consider the
waveform in Fig. H.15. This waveform was created (in Praat) by summing two sine
waveforms, one of frequency 10 Hz and one of frequency 100 Hz.
Time (s)0 0.5–11
0
Figure H.15 A waveform that is the sum of two sine waveforms, one of frequency 10
Hz (note ﬁve repetitions in the half-second window) and one of frequency 100 Hz, both of
amplitude 1.
We can represent these two component frequencies with a spectrum . The spec- spectrum
trum of a signal is a representation of each of its frequency components and their
amplitudes. Figure H.16 shows the spectrum of Fig. H.15. Frequency in Hz is on
the x-axis and amplitude on the y-axis. Note the two spikes in the ﬁgure, one at
10 Hz and one at 100 Hz. Thus, the spectrum is an alternative representation of
the original waveform, and we use the spectrum as a tool to study the component
frequencies of a sound wave at a particular time point.

## Page 16

16 APPENDIX H • P HONETICS
Frequency (Hz)1 10 100 2 20 200 5 50Sound pressure level (dB /Hz)
406080
Figure H.16 The spectrum of the waveform in Fig. H.15.
Let’s look now at the frequency components of a speech waveform. Figure H.17
shows part of the waveform for the vowel [ae] of the word had, cut out from the
sentence shown in Fig. H.13.
Time (s)0 0.04275–0.055540.04968
0
Figure H.17 The waveform of part of the vowel [ae] from the word hadcut out from the
waveform shown in Fig. H.13.
Note that there is a complex wave that repeats about ten times in the ﬁgure; but
there is also a smaller repeated wave that repeats four times for every larger pattern
(notice the four small peaks inside each repeated wave). The complex wave has a
frequency of about 234 Hz (we can ﬁgure this out since it repeats roughly 10 times
in .0427 seconds, and 10 cycles/.0427 seconds = 234 Hz).
The smaller wave then should have a frequency of roughly four times the fre-
quency of the larger wave, or roughly 936 Hz. Then, if you look carefully, you can
see two little waves on the peak of many of the 936 Hz waves. The frequency of this
tiniest wave must be roughly twice that of the 936 Hz wave, hence 1872 Hz.
Figure H.18 shows a smoothed spectrum for the waveform in Fig. H.17, com-
puted with a discrete Fourier transform (DFT).
Frequency (Hz)0 4000Sound pressure level (dB /Hz)
–20020
0 2000 4000 0 1000 2000 3000 4000
Figure H.18 A spectrum for the vowel [ae] from the word hadin the waveform of She just
had a baby in Fig. H.13.

## Page 17

H.4 • A COUSTIC PHONETICS AND SIGNALS 17
The x-axis of a spectrum shows frequency, and the y-axis shows some mea-
sure of the magnitude of each frequency component (in decibels (dB), a logarithmic
measure of amplitude that we saw earlier). Thus, Fig. H.18 shows signiﬁcant fre-
quency components at around 930 Hz, 1860 Hz, and 3020 Hz, along with many
other lower-magnitude frequency components. These ﬁrst two components are just
what we noticed in the time domain by looking at the wave in Fig. H.17!
Why is a spectrum useful? It turns out that these spectral peaks that are easily
visible in a spectrum are characteristic of different phones; phones have characteris-
tic spectral “signatures”. Just as chemical elements give off different wavelengths of
light when they burn, allowing us to detect elements in stars by looking at the spec-
trum of the light, we can detect the characteristic signature of the different phones
by looking at the spectrum of a waveform. This use of spectral information is essen-
tial to both human and machine speech recognition. In human audition, the function
of the cochlea , orinner ear , is to compute a spectrum of the incoming waveform. cochlea
Similarly, the acoustic features used in speech recognition are spectral representa-
tions.
Let’s look at the spectrum of different vowels. Since some vowels change over
time, we’ll use a different kind of plot called a spectrogram . While a spectrum
shows the frequency components of a wave at one point in time, a spectrogram is a spectrogram
way of envisioning how the different frequencies that make up a waveform change
over time. The x-axis shows time, as it did for the waveform, but the y-axis now
shows frequencies in hertz. The darkness of a point on a spectrogram corresponds
to the amplitude of the frequency component. Very dark points have high amplitude,
light points have low amplitude. Thus, the spectrogram is a useful way of visualizing
the three dimensions (time x frequency x amplitude).
Figure H.19 shows spectrograms of three American English vowels, [ih], [ae],
and [ah]. Note that each vowel has a set of dark bars at various frequency bands,
slightly different bands for each vowel. Each of these represents the same kind of
spectral peak that we saw in Fig. H.17.
Time (s)0 2.8139705000Frequency (Hz)
Figure H.19 Spectrograms for three American English vowels, [ih], [ae], and [uh]
Each dark bar (or spectral peak) is called a formant . As we discuss below, a formant
formant is a frequency band that is particularly ampliﬁed by the vocal tract. Since
different vowels are produced with the vocal tract in different positions, they will
produce different kinds of ampliﬁcations or resonances. Let’s look at the ﬁrst two
formants, called F1 and F2. Note that F1, the dark bar closest to the bottom, is in a
different position for the three vowels; it’s low for [ih] (centered at about 470 Hz)
and somewhat higher for [ae] and [ah] (somewhere around 800 Hz). By contrast,
F2, the second dark bar from the bottom, is highest for [ih], in the middle for [ae],
and lowest for [ah].
We can see the same formants in running speech, although the reduction and

## Page 18

18 APPENDIX H • P HONETICS
coarticulation processes make them somewhat harder to see. Figure H.20 shows the
spectrogram of “she just had a baby”, whose waveform was shown in Fig. H.13. F1
and F2 (and also F3) are pretty clear for the [ax] of just, the [ae] of had, and the [ey]
ofbaby .
she just had a baby
sh iy j ax s h ae dxax b ey b iy
Time (s)0 1.059
Figure H.20 A spectrogram of the sentence “she just had a baby” whose waveform was shown in Fig. H.13.
We can think of a spectrogram as a collection of spectra (time slices), like Fig. H.18 placed end to end.
What speciﬁc clues can spectral representations give for phone identiﬁcation?
First, since different vowels have their formants at characteristic places, the spectrum
can distinguish vowels from each other. We’ve seen that [ae] in the sample waveform
had formants at 930 Hz, 1860 Hz, and 3020 Hz. Consider the vowel [iy] at the
beginning of the utterance in Fig. H.13. The spectrum for this vowel is shown in
Fig. H.21. The ﬁrst formant of [iy] is 540 Hz, much lower than the ﬁrst formant for
[ae], and the second formant (2581 Hz) is much higher than the second formant for
[ae]. If you look carefully, you can see these formants as dark bars in Fig. H.20 just
around 0.5 seconds.
−1001020304050607080
0 1000 2000 3000
Figure H.21 A smoothed (LPC) spectrum for the vowel [iy] at the start of She just had a
baby . Note that the ﬁrst formant (540 Hz) is much lower than the ﬁrst formant for [ae] shown
in Fig. H.18, and the second formant (2581 Hz) is much higher than the second formant for
[ae].
The location of the ﬁrst two formants (called F1 and F2) plays a large role in de-
termining vowel identity, although the formants still differ from speaker to speaker.
Higher formants tend to be caused more by general characteristics of a speaker’s
vocal tract rather than by individual vowels. Formants also can be used to identify
the nasal phones [n], [m], and [ng] and the liquids [l] and [r].

## Page 19

H.4 • A COUSTIC PHONETICS AND SIGNALS 19
H.4.6 The Source-Filter Model
Why do different vowels have different spectral signatures? As we brieﬂy mentioned
above, the formants are caused by the resonant cavities of the mouth. The source-
ﬁlter model is a way of explaining the acoustics of a sound by modeling how thesource-ﬁlter
model
pulses produced by the glottis (the source ) are shaped by the vocal tract (the ﬁlter ).
Let’s see how this works. Whenever we have a wave such as the vibration in air
caused by the glottal pulse, the wave also has harmonics . A harmonic is another harmonic
wave whose frequency is a multiple of the fundamental wave. Thus, for example, a
115 Hz glottal fold vibration leads to harmonics (other waves) of 230 Hz, 345 Hz,
460 Hz, and so on on. In general, each of these waves will be weaker, that is, will
have much less amplitude than the wave at the fundamental frequency.
It turns out, however, that the vocal tract acts as a kind of ﬁlter or ampliﬁer;
indeed any cavity, such as a tube, causes waves of certain frequencies to be ampliﬁed
and others to be damped. This ampliﬁcation process is caused by the shape of the
cavity; a given shape will cause sounds of a certain frequency to resonate and hence
be ampliﬁed. Thus, by changing the shape of the cavity, we can cause different
frequencies to be ampliﬁed.
When we produce particular vowels, we are essentially changing the shape of
the vocal tract cavity by placing the tongue and the other articulators in particular
positions. The result is that different vowels cause different harmonics to be ampli-
ﬁed. So a wave of the same fundamental frequency passed through different vocal
tract positions will result in different harmonics being ampliﬁed.
We can see the result of this ampliﬁcation by looking at the relationship between
the shape of the vocal tract and the corresponding spectrum. Figure H.22 shows
the vocal tract position for three vowels and a typical resulting spectrum. The for-
mants are places in the spectrum where the vocal tract happens to amplify particular
harmonic frequencies.
Frequency (Hz)04000Sound pressure level (dB/Hz)
020
2682416F1F2[iy]  (tea)
Frequency (Hz)04000Sound pressure level (dB/Hz)
020
9031695F1 F2[ae] (cat) 
Frequency (Hz)04000Sound pressure level (dB/Hz)
–200
295817F1 F2[uw]  (moo)
[ae] (cat) [uw]  (moo)[iy]  (tea)
Figure H.22 Visualizing the vocal tract position as a ﬁlter: the tongue positions for three English vowels and
the resulting smoothed spectra showing F1 and F2.

## Page 20

20 APPENDIX H • P HONETICS
H.5 Phonetic Resources
A wide variety of phonetic resources can be drawn on for computational work. On-
linepronunciation dictionaries give phonetic transcriptions for words. The LDCpronunciation
dictionary
distributes pronunciation lexicons for Egyptian Arabic, Dutch, English, German,
Japanese, Korean, Mandarin, and Spanish. For English, the CELEX dictionary
(Baayen et al., 1995) has pronunciations for 160,595 wordforms, with syllabiﬁca-
tion, stress, and morphological and part-of-speech information. The open-source
CMU Pronouncing Dictionary (CMU, 1993) has pronunciations for about 134,000
wordforms, while the ﬁne-grained 110,000 word UNISYN dictionary (Fitt, 2002),
freely available for research purposes, gives syllabiﬁcations, stress, and also pronun-
ciations for dozens of dialects of English.
Another useful resource is a phonetically annotated corpus , in which a col-
lection of waveforms is hand-labeled with the corresponding string of phones. The
TIMIT corpus (NIST, 1990), originally a joint project between Texas Instruments
(TI), MIT, and SRI, is a corpus of 6300 read sentences, with 10 sentences each from
630 speakers. The 6300 sentences were drawn from a set of 2342 sentences, some
selected to have particular dialect shibboleths, others to maximize phonetic diphone
coverage. Each sentence in the corpus was phonetically hand-labeled, the sequence
of phones was automatically aligned with the sentence waveﬁle, and then the au-
tomatic phone boundaries were manually hand-corrected (Seneff and Zue, 1988).
The result is a time-aligned transcription : a transcription in which each phone istime-aligned
transcription
associated with a start and end time in the waveform, like the example in Fig. H.23.
she had your dark suit ingreasy wash water all year
sh iy hv ae dcl jh axr dcl d aa r kcl s ux q engcl g r iy s ix w aa sh q w aa dx axr q aa l y ix axr
Figure H.23 Phonetic transcription from the TIMIT corpus, using special ARPAbet features for narrow tran-
scription, such as the palatalization of [d] in had, unreleased ﬁnal stop in dark, glottalization of ﬁnal [t] in suit
to [q], and ﬂap of [t] in water . The TIMIT corpus also includes time-alignments (not shown).
The Switchboard Transcription Project phonetically annotated corpus consists
of 3.5 hours of sentences extracted from the Switchboard corpus (Greenberg et al.,
1996), together with transcriptions time-aligned at the syllable level. Figure H.24
shows an example .
0.470 0.640 0.720 0.900 0.953 1.279 1.410 1.630
dh er k aa n ax v ih m b ix t w iy n r ay n aw
Figure H.24 Phonetic transcription of the Switchboard phrase they’re kind of in between
right now . Note vowel reduction in they’re andof, coda deletion in kind andright , and re-
syllabiﬁcation (the [v] of ofattaches as the onset of in). Time is given in number of seconds
from the beginning of sentence to the start of each syllable.
The Buckeye corpus (Pitt et al. 2007, Pitt et al. 2005) is a phonetically tran-
scribed corpus of spontaneous American speech, containing about 300,000 words
from 40 talkers. Phonetically transcribed corpora are also available for other lan-
guages, including the Kiel corpus of German and Mandarin corpora transcribed by
the Chinese Academy of Social Sciences (Li et al., 2000).
In addition to resources like dictionaries and corpora, there are many useful pho-
netic software tools. Many of the ﬁgures in this book were generated by the Praat
package (Boersma and Weenink, 2005), which includes pitch, spectral, and formant
analysis, as well as a scripting language.

## Page 21

H.6 • S UMMARY 21
H.6 Summary
This chapter has introduced many of the important concepts of phonetics and com-
putational phonetics.
• We can represent the pronunciation of words in terms of units called phones .
The standard system for representing phones is the International Phonetic
Alphabet orIPA. The most common computational system for transcription
of English is the ARPAbet , which conveniently uses ASCII symbols.
• Phones can be described by how they are produced articulatorily by the vocal
organs; consonants are deﬁned in terms of their place andmanner of articu-
lation and voicing ; vowels by their height ,backness , and roundness .
• Speech sounds can also be described acoustically . Sound waves can be de-
scribed in terms of frequency ,amplitude , or their perceptual correlates, pitch
andloudness .
• The spectrum of a sound describes its different frequency components. While
some phonetic properties are recognizable from the waveform, both humans
and machines rely on spectral analysis for phone detection.
• A spectrogram is a plot of a spectrum over time. V owels are described by
characteristic harmonics called formants .
Bibliographical and Historical Notes
The major insights of articulatory phonetics date to the linguists of 800–150 B.C.
India. They invented the concepts of place and manner of articulation, worked out
the glottal mechanism of voicing, and understood the concept of assimilation. Eu-
ropean science did not catch up with the Indian phoneticians until over 2000 years
later, in the late 19th century. The Greeks did have some rudimentary phonetic
knowledge; by the time of Plato’s Theaetetus andCratylus , for example, they distin-
guished vowels from consonants, and stop consonants from continuants. The Stoics
developed the idea of the syllable and were aware of phonotactic constraints on pos-
sible words. An unknown Icelandic scholar of the 12th century exploited the concept
of the phoneme and proposed a phonemic writing system for Icelandic, including
diacritics for length and nasality. But his text remained unpublished until 1818 and
even then was largely unknown outside Scandinavia (Robins, 1967). The modern
era of phonetics is usually said to have begun with Sweet, who proposed what is
essentially the phoneme in his Handbook of Phonetics 1877. He also devised an al-
phabet for transcription and distinguished between broad andnarrow transcription,
proposing many ideas that were eventually incorporated into the IPA. Sweet was
considered the best practicing phonetician of his time; he made the ﬁrst scientiﬁc
recordings of languages for phonetic purposes and advanced the state of the art of
articulatory description. He was also infamously difﬁcult to get along with, a trait
that is well captured in Henry Higgins, the stage character that George Bernard Shaw
modeled after him. The phoneme was ﬁrst named by the Polish scholar Baudouin
de Courtenay, who published his theories in 1894.
Introductory phonetics textbooks include Ladefoged (1993) and Clark and Yal-
lop (1995). Wells (1982) is the deﬁnitive three-volume source on dialects of English.
Many of the classic insights in acoustic phonetics had been developed by the
late 1950s or early 1960s; just a few highlights include techniques like the sound

## Page 22

22 APPENDIX H • P HONETICS
spectrograph (Koenig et al., 1946), theoretical insights like the working out of the
source-ﬁlter theory and other issues in the mapping between articulation and acous-
tics ((Fant, 1960), Stevens et al. 1953, Stevens and House 1955, Heinz and Stevens
1961, Stevens and House 1961) the F1xF2 space of vowel formants (Peterson and
Barney, 1952), the understanding of the phonetic nature of stress and the use of
duration and intensity as cues (Fry, 1955), and a basic understanding of issues in
phone perception (Miller and Nicely 1955,Liberman et al. 1952). Lehiste (1967) is
a collection of classic papers on acoustic phonetics. Many of the seminal papers of
Gunnar Fant have been collected in Fant (2004).
Excellent textbooks on acoustic phonetics include Johnson (2003) and Lade-
foged (1996). Coleman (2005) includes an introduction to computational process-
ing of acoustics and speech from a linguistic perspective. Stevens (1998) lays out
an inﬂuential theory of speech sound production. There are a number of software
packages for acoustic phonetic analysis. Probably the most widely used one is Praat
(Boersma and Weenink, 2005).
Exercises
H.1 Find the mistakes in the ARPAbet transcriptions of the following words:
a.“three” [dh r i] d.“study” [s t uh d i] g.“slight” [s l iy t]
b.“sing” [s ih n g] e.“though” [th ow]
c.“eyes” [ay s] f.“planning” [p pl aa n ih ng]
H.2 Ira Gershwin’s lyric for Let’s Call the Whole Thing Off talks about two pro-
nunciations (each) of the words “tomato”, “potato”, and “either”. Transcribe
into the ARPAbet both pronunciations of each of these three words.
H.3 Transcribe the following words in the ARPAbet:
1. dark
2. suit
3. greasy
4. wash
5. water
H.4 Take a waveﬁle of your choice. Some examples are on the textbook website.
Download the Praat software, and use it to transcribe the waveﬁles at the word
level and into ARPAbet phones, using Praat to help you play pieces of each
waveﬁle and to look at the waveﬁle and the spectrogram.
H.5 Record yourself saying ﬁve of the English vowels: [aa], [eh], [ae], [iy], [uw].
Find F1 and F2 for each of your vowels.

## Page 23

Exercises 23
Baayen, R. H., R. Piepenbrock, and L. Gulikers. 1995.
The CELEX Lexical Database (Release 2) [CD-ROM] .
Linguistic Data Consortium, University of Pennsylvania
[Distributor].
Beckman, M. E. and G. M. Ayers. 1997. Guidelines
for ToBI labelling. Unpublished manuscript, Ohio
State University, http://www.ling.ohio-state.
edu/research/phonetics/E_ToBI/ .
Beckman, M. E. and J. Hirschberg. 1994. The ToBI annota-
tion conventions. Manuscript, Ohio State University.
Bennett, R. and E. Elfner. 2019. The syntax–prosody inter-
face. Annual Review of Linguistics , 5:151–171.
Boersma, P. and D. Weenink. 2005. Praat: doing phonetics
by computer (version 4.3.14). [Computer program]. Re-
trieved May 26, 2005, from http://www.praat.org/ .
Clark, J. and C. Yallop. 1995. An Introduction to Phonetics
and Phonology , 2nd edition. Blackwell.
CMU. 1993. The Carnegie Mellon Pronouncing Dictionary
v0.1. Carnegie Mellon University.
Coleman, J. 2005. Introducing Speech and Language Pro-
cessing . Cambridge University Press.
Fant, G. M. 1960. Acoustic Theory of Speech Production .
Mouton.
Fant, G. M. 2004. Speech Acoustics and Phonetics . Kluwer.
Fitt, S. 2002. Unisyn lexicon. http://www.cstr.ed.ac.
uk/projects/unisyn/ .
Fry, D. B. 1955. Duration and intensity as physical correlates
of linguistic stress. JASA , 27:765–768.
Greenberg, S., D. Ellis, and J. Hollenback. 1996. Insights
into spoken language gleaned from phonetic transcription
of the Switchboard corpus. ICSLP .
Heinz, J. M. and K. N. Stevens. 1961. On the properties of
voiceless fricative consonants. JASA , 33:589–596.
Johnson, K. 2003. Acoustic and Auditory Phonetics , 2nd
edition. Blackwell.
Koenig, W., H. K. Dunn, and L. Y . Lacy. 1946. The sound
spectrograph. JASA , 18:19–49.
Ladefoged, P. 1993. A Course in Phonetics . Harcourt Brace
Jovanovich. (3rd ed.).
Ladefoged, P. 1996. Elements of Acoustic Phonetics , 2nd
edition. University of Chicago.
Lehiste, I., ed. 1967. Readings in Acoustic Phonetics . MIT
Press.
Li, A., F. Zheng, W. Byrne, P. Fung, T. Kamm, L. Yi,
Z. Song, U. Ruhi, V . Venkataramani, and X. Chen. 2000.
CASS: A phonetically transcribed corpus of Mandarin
spontaneous speech. ICSLP .
Liberman, A. M., P. C. Delattre, and F. S. Cooper. 1952. The
role of selected stimulus variables in the perception of the
unvoiced stop consonants. American Journal of Psychol-
ogy, 65:497–516.
Miller, G. A. and P. E. Nicely. 1955. An analysis of percep-
tual confusions among some English consonants. JASA ,
27:338–352.
NIST. 1990. TIMIT Acoustic-Phonetic Continuous Speech
Corpus. National Institute of Standards and Technology
Speech Disc 1-1.1. NIST Order No. PB91-505065.Ostendorf, M., P. Price, and S. Shattuck-Hufnagel. 1995. The
Boston University Radio News Corpus. Technical Report
ECS-95-001, Boston University.
Peterson, G. E. and H. L. Barney. 1952. Control methods
used in a study of the vowels. JASA , 24:175–184.
Pitt, M. A., L. Dilley, K. Johnson, S. Kiesling, W. D. Ray-
mond, E. Hume, and E. Fosler-Lussier. 2007. Buckeye
corpus of conversational speech (2nd release). Depart-
ment of Psychology, Ohio State University (Distributor).
Pitt, M. A., K. Johnson, E. Hume, S. Kiesling, and W. D.
Raymond. 2005. The buckeye corpus of conversational
speech: Labeling conventions and a test of transcriber re-
liability. Speech Communication , 45:90–95.
Price, P. J., M. Ostendorf, S. Shattuck-Hufnagel, and
C. Fong. 1991. The use of prosody in syntactic disam-
biguation. JASA , 90(6).
Robins, R. H. 1967. A Short History of Linguistics . Indiana
University Press, Bloomington.
Seneff, S. and V . W. Zue. 1988. Transcription and align-
ment of the TIMIT database. Proceedings of the Second
Symposium on Advanced Man-Machine Interface through
Spoken Language .
Shoup, J. E. 1980. Phonological aspects of speech recogni-
tion. In W. A. Lea, ed., Trends in Speech Recognition ,
125–138. Prentice Hall.
Silverman, K., M. E. Beckman, J. F. Pitrelli, M. Ostendorf,
C. W. Wightman, P. J. Price, J. B. Pierrehumbert, and
J. Hirschberg. 1992. ToBI: A standard for labelling En-
glish prosody. ICSLP .
Stevens, K. N. 1998. Acoustic Phonetics . MIT Press.
Stevens, K. N. and A. S. House. 1955. Development of
a quantitative description of vowel articulation. JASA ,
27:484–493.
Stevens, K. N. and A. S. House. 1961. An acoustical theory
of vowel production and some of its implications. Journal
of Speech and Hearing Research , 4:303–320.
Stevens, K. N., S. Kasowski, and G. M. Fant. 1953. An elec-
trical analog of the vocal tract. JASA , 25(4):734–742.
Stevens, S. S. and J. V olkmann. 1940. The relation of pitch
to frequency: A revised scale. The American Journal of
Psychology , 53(3):329–353.
Stevens, S. S., J. V olkmann, and E. B. Newman. 1937. A
scale for the measurement of the psychological magni-
tude pitch. JASA , 8:185–190.
Sweet, H. 1877. A Handbook of Phonetics . Clarendon Press.
Wells, J. C. 1982. Accents of English . Cambridge University
Press.
Xu, Y . 2005. Speech melody as articulatorily implemented
communicative functions. Speech communication , 46(3-
4):220–251.

