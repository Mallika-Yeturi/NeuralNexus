# 5_LR_Apr_7_2021

## Page 1

Logistic RegressionBackground: Generative and Discriminative Classifiers

## Page 2

Logistic RegressionImportant analytic tool in natural and social sciencesBaseline supervised machine learning tool for classificationIs also the foundation of neural networks

## Page 3

Generative and Discriminative ClassifiersNaive Bayes is a generativeclassifierby contrast:Logistic regression is a discriminativeclassifier

## Page 4

Generative and Discriminative Classifiers
Suppose we're distinguishing cat from dog images
imagenetimagenet

## Page 5

Generative Classifier:
â€¢Build a model of what's in a cat imageâ€¢Knows about whiskers, ears, eyesâ€¢Assigns a probability to any image: â€¢how cat-y is this image?Also build a model for dog imagesNow given a new image:Run both models and see which one fits better 

## Page 6

Discriminative Classifier
Just try to distinguish dogs from catsOh look, dogs have collars!Let's ignore everything else

## Page 7

Finding the correct class c from a document d inGenerative vs Discriminative ClassifiersNaive BayesLogistic Regression
72CHAPTER5â€¢LOGISTICREGRESSIONMore formally, recall that the naive Bayes assigns a classcto a documentdnotby directly computingP(c|d)but by computing a likelihood and a priorË†c=argmaxc2Clikelihoodz}|{P(d|c)priorz}|{P(c)(5.1)Agenerative modellike naive Bayes makes use of thislikelihoodterm, whichgenerativemodelexpresses how to generate the features of a documentif we knew it was of class c.By contrast adiscriminative modelin this text categorization scenario attemptsdiscriminativemodeltodirectlycomputeP(c|d). Perhaps it will learn to assign high weight to documentfeatures that directly improve its ability todiscriminatebetween possible classes,even if it couldnâ€™t generate an example of one of the classes.Components of a probabilistic machine learning classiï¬er:Like naive Bayes,logistic regression is a probabilistic classiï¬er that makes use of supervised machinelearning. Machine learning classiï¬ers require a training corpus ofMobservationsinput/output pairs(x(i),y(i)). (Weâ€™ll use superscripts in parentheses to refer to indi-vidual instances in the training setâ€”for sentiment classiï¬cation each instance mightbe an individual document to be classiï¬ed). A machine learning system for classiï¬-cation then has four components:1.Afeature representationof the input. For each input observationx(i), thiswill be a vector of features[x1,x2,. . . ,xn]. We will generally refer to featureifor inputx(j)asx(j)i, sometimes simpliï¬ed asxi, but we will also see thenotationfi,fi(x), or, for multiclass classiï¬cation,fi(c,x).2.A classiï¬cation function that computes Ë†y, the estimated class, viap(y|x). Inthe next section we will introduce thesigmoidandsoftmaxtools for classiï¬-cation.3.An objective function for learning, usually involving minimizing error ontraining examples. We will introduce thecross-entropy loss function4.An algorithm for optimizing the objective function. We introduce thestochas-tic gradient descentalgorithm.Logistic regression has two phases:training:we train the system (speciï¬cally the weightswandb) using stochasticgradient descent and the cross-entropy loss.test:Given a test examplexwe computep(y|x)and return the higher probabilitylabely=1 ory=0.5.1 Classiï¬cation: the sigmoidThe goal of binary logistic regression is to train a classiï¬er that can make a binarydecision about the class of a new input observation. Here we introduce thesigmoidclassiï¬er that will help us make this decision.Consider a single input observationx, which we will represent by a vector offeatures[x1,x2,. . . ,xn](weâ€™ll show sample features in the next subsection). The clas-siï¬er outputycan be 1 (meaning the observation is a member of the class) or 0(the observation is not a member of the class). We want to know the probabilityP(y=1|x)that this observation is a member of the class. So perhaps the decision2CHAPTER5â€¢LOGISTICREGRESSIONMore formally, recall that the naive Bayes assigns a classcto a documentdnotby directly computingP(c|d)but by computing a likelihood and a priorË†c=argmaxc2Clikelihoodz}|{P(d|c)priorz}|{P(c)(5.1)Agenerative modellike naive Bayes makes use of thislikelihoodterm, whichgenerativemodelexpresses how to generate the features of a documentif we knew it was of class c.By contrast adiscriminative modelin this text categorization scenario attemptsdiscriminativemodeltodirectlycomputeP(c|d). Perhaps it will learn to assign high weight to documentfeatures that directly improve its ability todiscriminatebetween possible classes,even if it couldnâ€™t generate an example of one of the classes.Components of a probabilistic machine learning classiï¬er:Like naive Bayes,logistic regression is a probabilistic classiï¬er that makes use of supervised machinelearning. Machine learning classiï¬ers require a training corpus ofMobservationsinput/output pairs(x(i),y(i)). (Weâ€™ll use superscripts in parentheses to refer to indi-vidual instances in the training setâ€”for sentiment classiï¬cation each instance mightbe an individual document to be classiï¬ed). A machine learning system for classiï¬-cation then has four components:1.Afeature representationof the input. For each input observationx(i), thiswill be a vector of features[x1,x2,. . . ,xn]. We will generally refer to featureifor inputx(j)asx(j)i, sometimes simpliï¬ed asxi, but we will also see thenotationfi,fi(x), or, for multiclass classiï¬cation,fi(c,x).2.A classiï¬cation function that computes Ë†y, the estimated class, viap(y|x). Inthe next section we will introduce thesigmoidandsoftmaxtools for classiï¬-cation.3.An objective function for learning, usually involving minimizing error ontraining examples. We will introduce thecross-entropy loss function4.An algorithm for optimizing the objective function. We introduce thestochas-tic gradient descentalgorithm.Logistic regression has two phases:training:we train the system (speciï¬cally the weightswandb) using stochasticgradient descent and the cross-entropy loss.test:Given a test examplexwe computep(y|x)and return the higher probabilitylabely=1 ory=0.5.1 Classiï¬cation: the sigmoidThe goal of binary logistic regression is to train a classiï¬er that can make a binarydecision about the class of a new input observation. Here we introduce thesigmoidclassiï¬er that will help us make this decision.Consider a single input observationx, which we will represent by a vector offeatures[x1,x2,. . . ,xn](weâ€™ll show sample features in the next subsection). The clas-siï¬er outputycan be 1 (meaning the observation is a member of the class) or 0(the observation is not a member of the class). We want to know the probabilityP(y=1|x)that this observation is a member of the class. So perhaps the decisionP(c|d)posterior

## Page 8

Components of a probabilistic machine learning classifier1.A feature representation of the input. For each input observation x(i), a vector of features [x1, x2, ... , xn]. Feature j for input x(i) is xj, more completely  xj(i), or sometimes fj(x).2.A classification function that computes !ğ‘¦, the estimated class, via p(y|x), like the sigmoidor softmaxfunctions.3.An objective function for learning, like cross-entropy loss. 4.An algorithm for optimizing the objective function: stochastic gradient descent. Given m input/output pairs (x(i),y(i)):

## Page 9

The two phases of logistic regression Training: we learn weights w and busing stochastic gradient descentand cross-entropy loss. Test: Given a test example x we compute p(y|x) using learned weights wand b, and return whichever label (y = 1 or y = 0) is higher probability

## Page 10

Logistic RegressionBackground: Generative and Discriminative Classifiers

## Page 11

Logistic RegressionClassification in Logistic Regression

## Page 12

Classification ReminderPositive/negative sentiment  Spam/not spamAuthorship attribution  (Hamilton or Madison?)
Alexander Hamilton

## Page 13

Text Classification: definitionInput:â—¦a document xâ—¦a fixed set of classes  C ={c1, c2,â€¦, cJ}Output: a predicted class !ğ‘¦ÃC

## Page 14

Binary Classification in Logistic RegressionGiven a series of input/output pairs:â—¦(x(i), y(i))For each observation x(i)â—¦We represent x(i)by a feature vector [x1, x2,â€¦, xn]â—¦We compute an output: a predicted class !ğ‘¦(i)Ã{0,1}

## Page 15

Features in logistic regressionâ€¢For feature xi, weight witells is how important is xiâ€¢xi="review contains â€˜awesomeâ€™":      wi=  +10â€¢xj="review contains â€˜abysmalâ€™":      wj= -10â€¢xk=â€œreview contains â€˜mediocreâ€™":   wk= -2

## Page 16

Logistic Regression for one observation xInput observation: vector  x = [x1, x2,â€¦, xn]Weights: one per feature: W= [w1, w2,â€¦, wn]â—¦Sometimes we call the weights Î¸= [Î¸1, Î¸2,â€¦, Î¸n]Output: a predicted class !ğ‘¦Ã{0,1}(multinomial logistic regression: !ğ‘¦Ã{0, 1, 2, 3, 4})

## Page 17

How to do classificationFor each feature xi, weight witells us importance of xiâ—¦(Plus we'll have a bias b)We'll sum up all the weighted features and the biasIf this sum is high, we say y=1; if low, then y=05.1â€¢CLASSIFICATION:THE SIGMOID3is â€œpositive sentimentâ€ versus â€œnegative sentimentâ€, the features represent countsof words in a document, andP(y=1|x)is the probability that the document haspositive sentiment, while andP(y=0|x)is the probability that the document hasnegative sentiment.Logistic regression solves this task by learning, from a training set, a vector ofweightsand abias term. Each weightwiis a real number, and is associated with oneof the input featuresxi. The weightwirepresents how important that input feature isto the classiï¬cation decision, and can be positive (meaning the feature is associatedwith the class) or negative (meaning the feature is not associated with the class).Thus we might expect in a sentiment task the wordawesometo have a high positiveweight, andabysmalto have a very negative weight. Thebias term, also called thebias termintercept, is another real number thatâ€™s added to the weighted inputs.interceptTo make a decision on a test instanceâ€” after weâ€™ve learned the weights intrainingâ€” the classiï¬er ï¬rst multiplies eachxiby its weightwi, sums up the weightedfeatures, and adds the bias termb. The resulting single numberzexpresses theweighted sum of the evidence for the class.z= nXi=1wixi!+b(5.2)In the rest of the book weâ€™ll represent such sums using thedot productnotation fromdot productlinear algebra. The dot product of two vectorsaandb, written asaÂ·bis the sum ofthe products of the corresponding elements of each vector. Thus the following is anequivalent formation to Eq.5.2:z=wÂ·x+b(5.3)But note that nothing in Eq.5.3forceszto be a legal probability, that is, to liebetween 0 and 1. In fact, since weights are real-valued, the output might even benegative;zranges from â€¢toâ€¢.
Figure 5.1The sigmoid functiony=11+e ztakes a real value and maps it to the range[0,1].Because it is nearly linear around 0 but has a sharp slope toward the ends, it tends to squashoutlier values toward 0 or 1.To create a probability, weâ€™ll passzthrough thesigmoidfunction,s(z). Thesigmoidsigmoid function (named because it looks like ans) is also called thelogistic func-tion, and gives logistic regression its name. The sigmoid has the following equation,logisticfunctionshown graphically in Fig.5.1:y=s(z)=11+e z(5.4)5.1â€¢CLASSIFICATION:THE SIGMOID3is â€œpositive sentimentâ€ versus â€œnegative sentimentâ€, the features represent countsof words in a document, andP(y=1|x)is the probability that the document haspositive sentiment, while andP(y=0|x)is the probability that the document hasnegative sentiment.Logistic regression solves this task by learning, from a training set, a vector ofweightsand abias term. Each weightwiis a real number, and is associated with oneof the input featuresxi. The weightwirepresents how important that input feature isto the classiï¬cation decision, and can be positive (meaning the feature is associatedwith the class) or negative (meaning the feature is not associated with the class).Thus we might expect in a sentiment task the wordawesometo have a high positiveweight, andabysmalto have a very negative weight. Thebias term, also called thebias termintercept, is another real number thatâ€™s added to the weighted inputs.interceptTo make a decision on a test instanceâ€” after weâ€™ve learned the weights intrainingâ€” the classiï¬er ï¬rst multiplies eachxiby its weightwi, sums up the weightedfeatures, and adds the bias termb. The resulting single numberzexpresses theweighted sum of the evidence for the class.z= nXi=1wixi!+b(5.2)In the rest of the book weâ€™ll represent such sums using thedot productnotation fromdot productlinear algebra. The dot product of two vectorsaandb, written asaÂ·bis the sum ofthe products of the corresponding elements of each vector. Thus the following is anequivalent formation to Eq.5.2:z=wÂ·x+b(5.3)But note that nothing in Eq.5.3forceszto be a legal probability, that is, to liebetween 0 and 1. In fact, since weights are real-valued, the output might even benegative;zranges from â€¢toâ€¢.
Figure 5.1The sigmoid functiony=11+e ztakes a real value and maps it to the range[0,1].Because it is nearly linear around 0 but has a sharp slope toward the ends, it tends to squashoutlier values toward 0 or 1.To create a probability, weâ€™ll passzthrough thesigmoidfunction,s(z). Thesigmoidsigmoid function (named because it looks like ans) is also called thelogistic func-tion, and gives logistic regression its name. The sigmoid has the following equation,logisticfunctionshown graphically in Fig.5.1:y=s(z)=11+e z(5.4)

## Page 18

But we want a probabilistic classifierWe need to formalize â€œsum is highâ€.Weâ€™d like a principled classifier that gives us a probability, just like Naive Bayes didWe want a model that can tell us:p(y=1|x;Î¸)p(y=0|x; Î¸)

## Page 19

The problem:  z isn't a probability, it's just a number!Solution: use a function of z that goes from 0 to 15.1â€¢CLASSIFICATION:THE SIGMOID3is â€œpositive sentimentâ€ versus â€œnegative sentimentâ€, the features represent countsof words in a document, andP(y=1|x)is the probability that the document haspositive sentiment, while andP(y=0|x)is the probability that the document hasnegative sentiment.Logistic regression solves this task by learning, from a training set, a vector ofweightsand abias term. Each weightwiis a real number, and is associated with oneof the input featuresxi. The weightwirepresents how important that input feature isto the classiï¬cation decision, and can be positive (meaning the feature is associatedwith the class) or negative (meaning the feature is not associated with the class).Thus we might expect in a sentiment task the wordawesometo have a high positiveweight, andabysmalto have a very negative weight. Thebias term, also called thebias termintercept, is another real number thatâ€™s added to the weighted inputs.interceptTo make a decision on a test instanceâ€” after weâ€™ve learned the weights intrainingâ€” the classiï¬er ï¬rst multiplies eachxiby its weightwi, sums up the weightedfeatures, and adds the bias termb. The resulting single numberzexpresses theweighted sum of the evidence for the class.z= nXi=1wixi!+b(5.2)In the rest of the book weâ€™ll represent such sums using thedot productnotation fromdot productlinear algebra. The dot product of two vectorsaandb, written asaÂ·bis the sum ofthe products of the corresponding elements of each vector. Thus the following is anequivalent formation to Eq.5.2:z=wÂ·x+b(5.3)But note that nothing in Eq.5.3forceszto be a legal probability, that is, to liebetween 0 and 1. In fact, since weights are real-valued, the output might even benegative;zranges from â€¢toâ€¢.
Figure 5.1The sigmoid functiony=11+e ztakes a real value and maps it to the range[0,1].Because it is nearly linear around 0 but has a sharp slope toward the ends, it tends to squashoutlier values toward 0 or 1.To create a probability, weâ€™ll passzthrough thesigmoidfunction,s(z). Thesigmoidsigmoid function (named because it looks like ans) is also called thelogistic func-tion, and gives logistic regression its name. The sigmoid has the following equation,logisticfunctionshown graphically in Fig.5.1:y=s(z)=11+e z(5.4)5.1â€¢CLASSIFICATION:THE SIGMOID3sentimentâ€ versus â€œnegative sentimentâ€, the features represent counts of words in adocument,P(y=1|x)is the probability that the document has positive sentiment,andP(y=0|x)is the probability that the document has negative sentiment.Logistic regression solves this task by learning, from a training set, a vector ofweightsand abias term. Each weightwiis a real number, and is associated with oneof the input featuresxi. The weightwirepresents how important that input featureis to the classiï¬cation decision, and can be positive (providing evidence that the in-stance being classiï¬ed belongs in the positive class) or negative (providing evidencethat the instance being classiï¬ed belongs in the negative class). Thus we mightexpect in a sentiment task the wordawesometo have a high positive weight, andabysmalto have a very negative weight. Thebias term, also called theintercept, isbias terminterceptanother real number thatâ€™s added to the weighted inputs.To make a decision on a test instanceâ€” after weâ€™ve learned the weights intrainingâ€” the classiï¬er ï¬rst multiplies eachxiby its weightwi, sums up the weightedfeatures, and adds the bias termb. The resulting single numberzexpresses theweighted sum of the evidence for the class.z= nXi=1wixi!+b(5.2)In the rest of the book weâ€™ll represent such sums using thedot productnotation fromdot productlinear algebra. The dot product of two vectorsaandb, written asaÂ·bis the sum ofthe products of the corresponding elements of each vector. Thus the following is anequivalent formation to Eq.5.2:z=wÂ·x+b(5.3)But note that nothing in Eq.5.3forceszto be a legal probability, that is, to liebetween 0 and 1. In fact, since weights are real-valued, the output might even benegative;zranges from â€¢toâ€¢.
Figure 5.1The sigmoid functiony=11+e ztakes a real value and maps it to the range[0,1].It is nearly linear around 0 but outlier values get squashed toward 0 or 1.To create a probability, weâ€™ll passzthrough thesigmoidfunction,s(z). Thesigmoidsigmoid function (named because it looks like ans) is also called thelogistic func-tion, and gives logistic regression its name. The sigmoid has the following equation,logisticfunctionshown graphically in Fig.5.1:y=s(z)=11+e z=11+exp( z)(5.4)(For the rest of the book, weâ€™ll use the notation exp(x)to meanex.) The sigmoidhas a number of advantages; it takes a real-valued number and maps it into the range

## Page 20

The very useful sigmoid or logistic function
205.1â€¢CLASSIFICATION:THE SIGMOID3is â€œpositive sentimentâ€ versus â€œnegative sentimentâ€, the features represent countsof words in a document, andP(y=1|x)is the probability that the document haspositive sentiment, while andP(y=0|x)is the probability that the document hasnegative sentiment.Logistic regression solves this task by learning, from a training set, a vector ofweightsand abias term. Each weightwiis a real number, and is associated with oneof the input featuresxi. The weightwirepresents how important that input feature isto the classiï¬cation decision, and can be positive (meaning the feature is associatedwith the class) or negative (meaning the feature is not associated with the class).Thus we might expect in a sentiment task the wordawesometo have a high positiveweight, andabysmalto have a very negative weight. Thebias term, also called thebias termintercept, is another real number thatâ€™s added to the weighted inputs.interceptTo make a decision on a test instanceâ€” after weâ€™ve learned the weights intrainingâ€” the classiï¬er ï¬rst multiplies eachxiby its weightwi, sums up the weightedfeatures, and adds the bias termb. The resulting single numberzexpresses theweighted sum of the evidence for the class.z= nXi=1wixi!+b(5.2)In the rest of the book weâ€™ll represent such sums using thedot productnotation fromdot productlinear algebra. The dot product of two vectorsaandb, written asaÂ·bis the sum ofthe products of the corresponding elements of each vector. Thus the following is anequivalent formation to Eq.5.2:z=wÂ·x+b(5.3)But note that nothing in Eq.5.3forceszto be a legal probability, that is, to liebetween 0 and 1. In fact, since weights are real-valued, the output might even benegative;zranges from â€¢toâ€¢.
Figure 5.1The sigmoid functiony=11+e ztakes a real value and maps it to the range[0,1].Because it is nearly linear around 0 but has a sharp slope toward the ends, it tends to squashoutlier values toward 0 or 1.To create a probability, weâ€™ll passzthrough thesigmoidfunction,s(z). Thesigmoidsigmoid function (named because it looks like ans) is also called thelogistic func-tion, and gives logistic regression its name. The sigmoid has the following equation,logisticfunctionshown graphically in Fig.5.1:y=s(z)=11+e z(5.4)5.1â€¢CLASSIFICATION:THE SIGMOID3is â€œpositive sentimentâ€ versus â€œnegative sentimentâ€, the features represent countsof words in a document, andP(y=1|x)is the probability that the document haspositive sentiment, while andP(y=0|x)is the probability that the document hasnegative sentiment.Logistic regression solves this task by learning, from a training set, a vector ofweightsand abias term. Each weightwiis a real number, and is associated with oneof the input featuresxi. The weightwirepresents how important that input feature isto the classiï¬cation decision, and can be positive (meaning the feature is associatedwith the class) or negative (meaning the feature is not associated with the class).Thus we might expect in a sentiment task the wordawesometo have a high positiveweight, andabysmalto have a very negative weight. Thebias term, also called thebias termintercept, is another real number thatâ€™s added to the weighted inputs.interceptTo make a decision on a test instanceâ€” after weâ€™ve learned the weights intrainingâ€” the classiï¬er ï¬rst multiplies eachxiby its weightwi, sums up the weightedfeatures, and adds the bias termb. The resulting single numberzexpresses theweighted sum of the evidence for the class.z= nXi=1wixi!+b(5.2)In the rest of the book weâ€™ll represent such sums using thedot productnotation fromdot productlinear algebra. The dot product of two vectorsaandb, written asaÂ·bis the sum ofthe products of the corresponding elements of each vector. Thus the following is anequivalent formation to Eq.5.2:z=wÂ·x+b(5.3)But note that nothing in Eq.5.3forceszto be a legal probability, that is, to liebetween 0 and 1. In fact, since weights are real-valued, the output might even benegative;zranges from â€¢toâ€¢.
Figure 5.1The sigmoid functiony=11+e ztakes a real value and maps it to the range[0,1].Because it is nearly linear around 0 but has a sharp slope toward the ends, it tends to squashoutlier values toward 0 or 1.To create a probability, weâ€™ll passzthrough thesigmoidfunction,s(z). Thesigmoidsigmoid function (named because it looks like ans) is also called thelogistic func-tion, and gives logistic regression its name. The sigmoid has the following equation,logisticfunctionshown graphically in Fig.5.1:y=s(z)=11+e z(5.4)

## Page 21

Idea of logistic regressionWeâ€™ll compute wâˆ™x+bAnd then weâ€™ll pass it through the sigmoid function:Ïƒ(wâˆ™x+b)And we'll just treat it as a probability

## Page 22

Making probabilities with sigmoids4CHAPTER5â€¢LOGISTICREGRESSION[0,1], which is just what we want for a probability. Because it is nearly linear around0 but ï¬‚attens toward the ends, it tends to squash outlier values toward 0 or 1. Anditâ€™s differentiable, which as weâ€™ll see in Section5.8will be handy for learning.Weâ€™re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(wÂ·x+b)=11+exp( (wÂ·x+b))P(y=0)=1 s(wÂ·x+b)=1 11+exp( (wÂ·x+b))=exp( (wÂ·x+b))1+exp( (wÂ·x+b))(5.5)The sigmoid function has the property1 s(x)=s( x)(5.6)so we could also have expressedP(y=0)ass( (wÂ·x+b)).Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundaryË†y=â‡¢1 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classiï¬cationLetâ€™s have an example. Suppose we are doing binary sentiment classiï¬cation onmovie review text, and we would like to know whether to assign the sentiment class+or to a review documentdoc. Weâ€™ll represent each input observation by the 6featuresx1...x6of the input shown in the following table; Fig.5.2shows the featuresin a sample mini test document.Var Deï¬nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3â‡¢1 if â€œnoâ€2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5â‡¢1 if â€œ!â€2doc0 otherwise0x6log(word count of doc)ln(66)=4.19Letâ€™s assume for the moment that weâ€™ve already learned a real-valued weight foreach of these features, and that the 6 weights corresponding to the 6 features are[2.5, 5.0, 1.2,0.5,2.0,0.7], whileb= 0.1. (Weâ€™ll discuss in the next section how4CHAPTER5â€¢LOGISTICREGRESSION[0,1], which is just what we want for a probability. Because it is nearly linear around0 but ï¬‚attens toward the ends, it tends to squash outlier values toward 0 or 1. Anditâ€™s differentiable, which as weâ€™ll see in Section5.8will be handy for learning.Weâ€™re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(wÂ·x+b)=11+exp( (wÂ·x+b))P(y=0)=1 s(wÂ·x+b)=1 11+exp( (wÂ·x+b))=exp( (wÂ·x+b))1+exp( (wÂ·x+b))(5.5)The sigmoid function has the property1 s(x)=s( x)(5.6)so we could also have expressedP(y=0)ass( (wÂ·x+b)).Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundaryË†y=â‡¢1 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classiï¬cationLetâ€™s have an example. Suppose we are doing binary sentiment classiï¬cation onmovie review text, and we would like to know whether to assign the sentiment class+or to a review documentdoc. Weâ€™ll represent each input observation by the 6featuresx1...x6of the input shown in the following table; Fig.5.2shows the featuresin a sample mini test document.Var Deï¬nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3â‡¢1 if â€œnoâ€2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5â‡¢1 if â€œ!â€2doc0 otherwise0x6log(word count of doc)ln(66)=4.19Letâ€™s assume for the moment that weâ€™ve already learned a real-valued weight foreach of these features, and that the 6 weights corresponding to the 6 features are[2.5, 5.0, 1.2,0.5,2.0,0.7], whileb= 0.1. (Weâ€™ll discuss in the next section how

## Page 23

By the way:4CHAPTER5â€¢LOGISTICREGRESSION[0,1], which is just what we want for a probability. Because it is nearly linear around0 but ï¬‚attens toward the ends, it tends to squash outlier values toward 0 or 1. Anditâ€™s differentiable, which as weâ€™ll see in Section5.8will be handy for learning.Weâ€™re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(wÂ·x+b)=11+exp( (wÂ·x+b))P(y=0)=1 s(wÂ·x+b)=1 11+exp( (wÂ·x+b))=exp( (wÂ·x+b))1+exp( (wÂ·x+b))(5.5)The sigmoid function has the property1 s(x)=s( x)(5.6)so we could also have expressedP(y=0)ass( (wÂ·x+b)).Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundaryË†y=â‡¢1 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classiï¬cationLetâ€™s have an example. Suppose we are doing binary sentiment classiï¬cation onmovie review text, and we would like to know whether to assign the sentiment class+or to a review documentdoc. Weâ€™ll represent each input observation by the 6featuresx1...x6of the input shown in the following table; Fig.5.2shows the featuresin a sample mini test document.Var Deï¬nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3â‡¢1 if â€œnoâ€2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5â‡¢1 if â€œ!â€2doc0 otherwise0x6log(word count of doc)ln(66)=4.19Letâ€™s assume for the moment that weâ€™ve already learned a real-valued weight foreach of these features, and that the 6 weights corresponding to the 6 features are[2.5, 5.0, 1.2,0.5,2.0,0.7], whileb= 0.1. (Weâ€™ll discuss in the next section how4CHAPTER5â€¢LOGISTICREGRESSION[0,1], which is just what we want for a probability. Because it is nearly linear around0 but ï¬‚attens toward the ends, it tends to squash outlier values toward 0 or 1. Anditâ€™s differentiable, which as weâ€™ll see in Section5.8will be handy for learning.Weâ€™re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(wÂ·x+b)=11+exp( (wÂ·x+b))P(y=0)=1 s(wÂ·x+b)=1 11+exp( (wÂ·x+b))=exp( (wÂ·x+b))1+exp( (wÂ·x+b))(5.5)The sigmoid function has the property1 s(x)=s( x)(5.6)so we could also have expressedP(y=0)ass( (wÂ·x+b)).Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundaryË†y=â‡¢1 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classiï¬cationLetâ€™s have an example. Suppose we are doing binary sentiment classiï¬cation onmovie review text, and we would like to know whether to assign the sentiment class+or to a review documentdoc. Weâ€™ll represent each input observation by the 6featuresx1...x6of the input shown in the following table; Fig.5.2shows the featuresin a sample mini test document.Var Deï¬nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3â‡¢1 if â€œnoâ€2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5â‡¢1 if â€œ!â€2doc0 otherwise0x6log(word count of doc)ln(66)=4.19Letâ€™s assume for the moment that weâ€™ve already learned a real-valued weight foreach of these features, and that the 6 weights corresponding to the 6 features are[2.5, 5.0, 1.2,0.5,2.0,0.7], whileb= 0.1. (Weâ€™ll discuss in the next section how=Because4CHAPTER5â€¢LOGISTICREGRESSION[0,1], which is just what we want for a probability. Because it is nearly linear around0 but ï¬‚attens toward the ends, it tends to squash outlier values toward 0 or 1. Anditâ€™s differentiable, which as weâ€™ll see in Section5.8will be handy for learning.Weâ€™re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(wÂ·x+b)=11+exp( (wÂ·x+b))P(y=0)=1 s(wÂ·x+b)=1 11+exp( (wÂ·x+b))=exp( (wÂ·x+b))1+exp( (wÂ·x+b))(5.5)The sigmoid function has the property1 s(x)=s( x)(5.6)so we could also have expressedP(y=0)ass( (wÂ·x+b)).Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundaryË†y=â‡¢1 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classiï¬cationLetâ€™s have an example. Suppose we are doing binary sentiment classiï¬cation onmovie review text, and we would like to know whether to assign the sentiment class+or to a review documentdoc. Weâ€™ll represent each input observation by the 6featuresx1...x6of the input shown in the following table; Fig.5.2shows the featuresin a sample mini test document.Var Deï¬nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3â‡¢1 if â€œnoâ€2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5â‡¢1 if â€œ!â€2doc0 otherwise0x6log(word count of doc)ln(66)=4.19Letâ€™s assume for the moment that weâ€™ve already learned a real-valued weight foreach of these features, and that the 6 weights corresponding to the 6 features are[2.5, 5.0, 1.2,0.5,2.0,0.7], whileb= 0.1. (Weâ€™ll discuss in the next section how

## Page 24

Turning a probability into a classifier4CHAPTER5â€¢LOGISTICREGRESSIONThe sigmoid has a number of advantages; it take a real-valued number and mapsit into the range[0,1], which is just what we want for a probability. Because it isnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outliervalues toward 0 or 1. And itâ€™s differentiable, which as weâ€™ll see in Section5.8willbe handy for learning.Weâ€™re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(wÂ·x+b)=11+e (wÂ·x+b)P(y=0)=1 s(wÂ·x+b)=1 11+e (wÂ·x+b)=e (wÂ·x+b)1+e (wÂ·x+b)(5.5)Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundaryË†y=â‡¢1 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classiï¬cationLetâ€™s have an example. Suppose we are doing binary sentiment classiï¬cation onmovie review text, and we would like to know whether to assign the sentiment class+or to a review documentdoc. Weâ€™ll represent each input observation by thefollowing 6 featuresx1...x6of the input; Fig.5.2shows the features in a sample minitest document.Var Deï¬nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3â‡¢1 if â€œnoâ€2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5â‡¢1 if â€œ!â€2doc0 otherwise0x6log(word count of doc)ln(64)=4.15Letâ€™s assume for the moment that weâ€™ve already learned a real-valued weightfor each of these features, and that the 6 weights corresponding to the 6 featuresare[2.5, 5.0, 1.2,0.5,2.0,0.7], whileb= 0.1. (Weâ€™ll discuss in the next sectionhow the weights are learned.) The weightw1, for example indicates how important0.5 here is called the decision boundary

## Page 25

The probabilistic classifier5.1â€¢CLASSIFICATION:THE SIGMOID3is â€œpositive sentimentâ€ versus â€œnegative sentimentâ€, the features represent countsof words in a document, andP(y=1|x)is the probability that the document haspositive sentiment, while andP(y=0|x)is the probability that the document hasnegative sentiment.Logistic regression solves this task by learning, from a training set, a vector ofweightsand abias term. Each weightwiis a real number, and is associated with oneof the input featuresxi. The weightwirepresents how important that input feature isto the classiï¬cation decision, and can be positive (meaning the feature is associatedwith the class) or negative (meaning the feature is not associated with the class).Thus we might expect in a sentiment task the wordawesometo have a high positiveweight, andabysmalto have a very negative weight. Thebias term, also called thebias termintercept, is another real number thatâ€™s added to the weighted inputs.interceptTo make a decision on a test instanceâ€” after weâ€™ve learned the weights intrainingâ€” the classiï¬er ï¬rst multiplies eachxiby its weightwi, sums up the weightedfeatures, and adds the bias termb. The resulting single numberzexpresses theweighted sum of the evidence for the class.z= nXi=1wixi!+b(5.2)In the rest of the book weâ€™ll represent such sums using thedot productnotation fromdot productlinear algebra. The dot product of two vectorsaandb, written asaÂ·bis the sum ofthe products of the corresponding elements of each vector. Thus the following is anequivalent formation to Eq.5.2:z=wÂ·x+b(5.3)But note that nothing in Eq.5.3forceszto be a legal probability, that is, to liebetween 0 and 1. In fact, since weights are real-valued, the output might even benegative;zranges from â€¢toâ€¢.
Figure 5.1The sigmoid functiony=11+e ztakes a real value and maps it to the range[0,1].Because it is nearly linear around 0 but has a sharp slope toward the ends, it tends to squashoutlier values toward 0 or 1.To create a probability, weâ€™ll passzthrough thesigmoidfunction,s(z). Thesigmoidsigmoid function (named because it looks like ans) is also called thelogistic func-tion, and gives logistic regression its name. The sigmoid has the following equation,logisticfunctionshown graphically in Fig.5.1:y=s(z)=11+e z(5.4)wx+ b4CHAPTER5â€¢LOGISTICREGRESSIONThe sigmoid has a number of advantages; it take a real-valued number and mapsit into the range[0,1], which is just what we want for a probability. Because it isnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outliervalues toward 0 or 1. And itâ€™s differentiable, which as weâ€™ll see in Section5.8willbe handy for learning.Weâ€™re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(wÂ·x+b)=11+e (wÂ·x+b)P(y=0)=1 s(wÂ·x+b)=1 11+e (wÂ·x+b)=e (wÂ·x+b)1+e (wÂ·x+b)(5.5)Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundaryË†y=â‡¢1 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classiï¬cationLetâ€™s have an example. Suppose we are doing binary sentiment classiï¬cation onmovie review text, and we would like to know whether to assign the sentiment class+or to a review documentdoc. Weâ€™ll represent each input observation by thefollowing 6 featuresx1...x6of the input; Fig.5.2shows the features in a sample minitest document.Var Deï¬nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3â‡¢1 if â€œnoâ€2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5â‡¢1 if â€œ!â€2doc0 otherwise0x6log(word count of doc)ln(64)=4.15Letâ€™s assume for the moment that weâ€™ve already learned a real-valued weightfor each of these features, and that the 6 weights corresponding to the 6 featuresare[2.5, 5.0, 1.2,0.5,2.0,0.7], whileb= 0.1. (Weâ€™ll discuss in the next sectionhow the weights are learned.) The weightw1, for example indicates how importantP(y=1)

## Page 26

Turning a probability into a classifier4CHAPTER5â€¢LOGISTICREGRESSIONThe sigmoid has a number of advantages; it take a real-valued number and mapsit into the range[0,1], which is just what we want for a probability. Because it isnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outliervalues toward 0 or 1. And itâ€™s differentiable, which as weâ€™ll see in Section5.8willbe handy for learning.Weâ€™re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(wÂ·x+b)=11+e (wÂ·x+b)P(y=0)=1 s(wÂ·x+b)=1 11+e (wÂ·x+b)=e (wÂ·x+b)1+e (wÂ·x+b)(5.5)Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundaryË†y=â‡¢1 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classiï¬cationLetâ€™s have an example. Suppose we are doing binary sentiment classiï¬cation onmovie review text, and we would like to know whether to assign the sentiment class+or to a review documentdoc. Weâ€™ll represent each input observation by thefollowing 6 featuresx1...x6of the input; Fig.5.2shows the features in a sample minitest document.Var Deï¬nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3â‡¢1 if â€œnoâ€2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5â‡¢1 if â€œ!â€2doc0 otherwise0x6log(word count of doc)ln(64)=4.15Letâ€™s assume for the moment that weâ€™ve already learned a real-valued weightfor each of these features, and that the 6 weights corresponding to the 6 featuresare[2.5, 5.0, 1.2,0.5,2.0,0.7], whileb= 0.1. (Weâ€™ll discuss in the next sectionhow the weights are learned.) The weightw1, for example indicates how importantif wâˆ™x+b> 0if wâˆ™x+bâ‰¤ 0

## Page 27

Logistic RegressionClassification in Logistic Regression

## Page 28

Logistic RegressionLogistic Regression: a text example on sentiment classification

## Page 29

Sentiment example: does y=1 or y=0?It's hokey . There are virtually no surprises , and the writing is second-rate .         So why was it so enjoyable ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing . It sucked me in , and it'll do the same to you .
29

## Page 30

305.1â€¢CLASSIFICATION:THE SIGMOID5 It's hokey . There are virtually no surprises , and the writing is second-rate . So why was it so enjoyable  ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing .  It sucked me in , and it'll do the same to you  .x1=3x6=4.19x3=1x4=3x5=0x2=2
Figure 5.2A sample mini test document showing the extracted features in the vectorx.Given these 6 features and the input reviewx,P(+|x)andP( |x)can be com-puted using Eq.5.5:p(+|x)=P(Y=1|x)=s(wÂ·x+b)=s([2.5, 5.0, 1.2,0.5,2.0,0.7]Â·[3,2,1,3,0,4.19]+0.1)=s(.833)=0.70(5.6)p( |x)=P(Y=0|x)=1 s(wÂ·x+b)=0.30Logistic regression is commonly applied to all sorts of NLP tasks, and any propertyof the input can be a feature. Consider the task ofperiod disambiguation: decidingif a period is the end of a sentence or part of a word, by classifying each periodinto one of two classes EOS (end-of-sentence) and not-EOS. We might use featureslikex1below expressing that the current word is lower case and the class is EOS(perhaps with a positive weight), or that the current word is in our abbreviationsdictionary (â€œProf.â€) and the class is EOS (perhaps with a negative weight). A featurecan also express a quite complex combination of properties. For example a periodfollowing an upper case word is likely to be an EOS, but if the word itself isSt.andthe previous word is capitalized, then the period is likely part of a shortening of thewordstreet.x1=â‡¢1 if â€œCase(wi)=Lowerâ€0 otherwisex2=â‡¢1 if â€œwi2AcronymDictâ€0 otherwisex3=â‡¢1 if â€œwi=St. &Case(wi 1)=Capâ€0 otherwiseDesigning features:Features are generally designed by examining the trainingset with an eye to linguistic intuitions and the linguistic literature on the domain. Acareful error analysis on the training set or devset of an early version of a systemoften provides insights into features.For some tasks it is especially helpful to build complex features that are combi-nations of more primitive features. We saw such a feature for period disambiguationabove, where a period on the wordSt.was less likely to be the end of the sentenceif the previous word was capitalized. For logistic regression and naive Bayes thesecombination features orfeature interactionshave to be designed by hand.featureinteractions4CHAPTER5â€¢LOGISTICREGRESSIONnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outliervalues toward 0 or 1. And itâ€™s differentiable, which as weâ€™ll see in Section5.8willbe handy for learning.Weâ€™re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(wÂ·x+b)=11+e (wÂ·x+b)P(y=0)=1 s(wÂ·x+b)=1 11+e (wÂ·x+b)=e (wÂ·x+b)1+e (wÂ·x+b)(5.5)Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if the probabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecision boundary:decisionboundaryË†y=â‡¢1 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classiï¬cationLetâ€™s have an example. Suppose we are doing binary sentiment classiï¬cation onmovie review text, and we would like to know whether to assign the sentiment class+or to a review documentdoc. Weâ€™ll represent each input observation by the 6featuresx1...x6of the input shown in the following table; Fig.5.2shows the featuresin a sample mini test document.Var Deï¬nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3â‡¢1 if â€œnoâ€2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5â‡¢1 if â€œ!â€2doc0 otherwise0x6log(word count of doc)ln(66)=4.19Letâ€™s assume for the moment that weâ€™ve already learned a real-valued weight foreach of these features, and that the 6 weights corresponding to the 6 features are[2.5, 5.0, 1.2,0.5,2.0,0.7], whileb= 0.1. (Weâ€™ll discuss in the next section howthe weights are learned.) The weightw1, for example indicates how important afeature the number of positive lexicon words (great,nice,enjoyable, etc.) is toa positive sentiment decision, whilew2tells us the importance of negative lexiconwords. Note thatw1=2.5 is positive, whilew2= 5.0, meaning that negative wordsare negatively associated with a positive sentiment decision, and are about twice asimportant as positive words.

## Page 31

Classifying sentiment for input x
314CHAPTER5â€¢LOGISTICREGRESSIONnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outliervalues toward 0 or 1. And itâ€™s differentiable, which as weâ€™ll see in Section5.8willbe handy for learning.Weâ€™re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(wÂ·x+b)=11+e (wÂ·x+b)P(y=0)=1 s(wÂ·x+b)=1 11+e (wÂ·x+b)=e (wÂ·x+b)1+e (wÂ·x+b)(5.5)Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if the probabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecision boundary:decisionboundaryË†y=â‡¢1 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classiï¬cationLetâ€™s have an example. Suppose we are doing binary sentiment classiï¬cation onmovie review text, and we would like to know whether to assign the sentiment class+or to a review documentdoc. Weâ€™ll represent each input observation by the 6featuresx1...x6of the input shown in the following table; Fig.5.2shows the featuresin a sample mini test document.Var Deï¬nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3â‡¢1 if â€œnoâ€2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5â‡¢1 if â€œ!â€2doc0 otherwise0x6log(word count of doc)ln(66)=4.19Letâ€™s assume for the moment that weâ€™ve already learned a real-valued weight foreach of these features, and that the 6 weights corresponding to the 6 features are[2.5, 5.0, 1.2,0.5,2.0,0.7], whileb= 0.1. (Weâ€™ll discuss in the next section howthe weights are learned.) The weightw1, for example indicates how important afeature the number of positive lexicon words (great,nice,enjoyable, etc.) is toa positive sentiment decision, whilew2tells us the importance of negative lexiconwords. Note thatw1=2.5 is positive, whilew2= 5.0, meaning that negative wordsare negatively associated with a positive sentiment decision, and are about twice asimportant as positive words.4CHAPTER5â€¢LOGISTICREGRESSIONThe sigmoid has a number of advantages; it take a real-valued number and mapsit into the range[0,1], which is just what we want for a probability. Because it isnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outliervalues toward 0 or 1. And itâ€™s differentiable, which as weâ€™ll see in Section5.8willbe handy for learning.Weâ€™re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(wÂ·x+b)=11+e (wÂ·x+b)P(y=0)=1 s(wÂ·x+b)=1 11+e (wÂ·x+b)=e (wÂ·x+b)1+e (wÂ·x+b)(5.5)Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundaryË†y=â‡¢1 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classiï¬cationLetâ€™s have an example. Suppose we are doing binary sentiment classiï¬cation onmovie review text, and we would like to know whether to assign the sentiment class+or to a review documentdoc. Weâ€™ll represent each input observation by thefollowing 6 featuresx1...x6of the input; Fig.5.2shows the features in a sample minitest document.Var Deï¬nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3â‡¢1 if â€œnoâ€2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5â‡¢1 if â€œ!â€2doc0 otherwise0x6log(word count of doc)ln(64)=4.15Letâ€™s assume for the moment that weâ€™ve already learned a real-valued weightfor each of these features, and that the 6 weights corresponding to the 6 featuresare[2.5, 5.0, 1.2,0.5,2.0,0.7], whileb= 0.1. (Weâ€™ll discuss in the next sectionhow the weights are learned.) The weightw1, for example indicates how importantSuppose w =b = 0.1

## Page 32

Classifying sentiment for input x5.1â€¢CLASSIFICATION:THE SIGMOID5 It's hokey . There are virtually no surprises , and the writing is second-rate . So why was it so enjoyable  ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing .  It sucked me in , and it'll do the same to you  .x1=3x6=4.19x3=1x4=3x5=0x2=2
Figure 5.2A sample mini test document showing the extracted features in the vectorx.Given these 6 features and the input reviewx,P(+|x)andP( |x)can be com-puted using Eq.5.5:p(+|x)=P(Y=1|x)=s(wÂ·x+b)=s([2.5, 5.0, 1.2,0.5,2.0,0.7]Â·[3,2,1,3,0,4.19]+0.1)=s(.833)=0.70(5.6)p( |x)=P(Y=0|x)=1 s(wÂ·x+b)=0.30Logistic regression is commonly applied to all sorts of NLP tasks, and any propertyof the input can be a feature. Consider the task ofperiod disambiguation: decidingif a period is the end of a sentence or part of a word, by classifying each periodinto one of two classes EOS (end-of-sentence) and not-EOS. We might use featureslikex1below expressing that the current word is lower case and the class is EOS(perhaps with a positive weight), or that the current word is in our abbreviationsdictionary (â€œProf.â€) and the class is EOS (perhaps with a negative weight). A featurecan also express a quite complex combination of properties. For example a periodfollowing an upper case word is likely to be an EOS, but if the word itself isSt.andthe previous word is capitalized, then the period is likely part of a shortening of thewordstreet.x1=â‡¢1 if â€œCase(wi)=Lowerâ€0 otherwisex2=â‡¢1 if â€œwi2AcronymDictâ€0 otherwisex3=â‡¢1 if â€œwi=St. &Case(wi 1)=Capâ€0 otherwiseDesigning features:Features are generally designed by examining the trainingset with an eye to linguistic intuitions and the linguistic literature on the domain. Acareful error analysis on the training set or devset of an early version of a systemoften provides insights into features.For some tasks it is especially helpful to build complex features that are combi-nations of more primitive features. We saw such a feature for period disambiguationabove, where a period on the wordSt.was less likely to be the end of the sentenceif the previous word was capitalized. For logistic regression and naive Bayes thesecombination features orfeature interactionshave to be designed by hand.featureinteractions325.1â€¢CLASSIFICATION:THE SIGMOID5 It's hokey . There are virtually no surprises , and the writing is second-rate . So why was it so enjoyable  ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing .  It sucked me in , and it'll do the same to you  .x1=3x6=4.19x3=1x4=3x5=0x2=2
Figure 5.2A sample mini test document showing the extracted features in the vectorx.Given these 6 features and the input reviewx,P(+|x)andP( |x)can be com-puted using Eq.5.5:p(+|x)=P(Y=1|x)=s(wÂ·x+b)=s([2.5, 5.0, 1.2,0.5,2.0,0.7]Â·[3,2,1,3,0,4.19]+0.1)=s(.833)=0.70(5.6)p( |x)=P(Y=0|x)=1 s(wÂ·x+b)=0.30Logistic regression is commonly applied to all sorts of NLP tasks, and any propertyof the input can be a feature. Consider the task ofperiod disambiguation: decidingif a period is the end of a sentence or part of a word, by classifying each periodinto one of two classes EOS (end-of-sentence) and not-EOS. We might use featureslikex1below expressing that the current word is lower case and the class is EOS(perhaps with a positive weight), or that the current word is in our abbreviationsdictionary (â€œProf.â€) and the class is EOS (perhaps with a negative weight). A featurecan also express a quite complex combination of properties. For example a periodfollowing an upper case word is likely to be an EOS, but if the word itself isSt.andthe previous word is capitalized, then the period is likely part of a shortening of thewordstreet.x1=â‡¢1 if â€œCase(wi)=Lowerâ€0 otherwisex2=â‡¢1 if â€œwi2AcronymDictâ€0 otherwisex3=â‡¢1 if â€œwi=St. &Case(wi 1)=Capâ€0 otherwiseDesigning features:Features are generally designed by examining the trainingset with an eye to linguistic intuitions and the linguistic literature on the domain. Acareful error analysis on the training set or devset of an early version of a systemoften provides insights into features.For some tasks it is especially helpful to build complex features that are combi-nations of more primitive features. We saw such a feature for period disambiguationabove, where a period on the wordSt.was less likely to be the end of the sentenceif the previous word was capitalized. For logistic regression and naive Bayes thesecombination features orfeature interactionshave to be designed by hand.featureinteractions

## Page 33

We can build features for logistic regression for any classification task: period disambiguation5.1â€¢CLASSIFICATION:THE SIGMOID5 It's hokey. There are virtually no surprises , and the writing is second-rate . So why was it so enjoyable? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing .  It sucked me in , and it'll do the same to you .x1=3x6=4.15x3=1x4=3x5=0x2=2
Figure 5.2A sample mini test document showing the extracted features in the vectorx.a feature the number of positive lexicon words (great,nice,enjoyable, etc.) is toa positive sentiment decision, whilew2tells us the importance of negative lexiconwords. Note thatw1=2.5 is positive, whilew2= 5.0, meaning that negative wordsare negatively associated with a positive sentiment decision, and are about twice asimportant as positive words.Given these 6 features and the input reviewx,P(+|x)andP( |x)can be com-puted using Eq.5.5:p(+|x)=P(Y=1|x)=s(wÂ·x+b)=s([2.5, 5.0, 1.2,0.5,2.0,0.7]Â·[3,2,1,3,0,4.15]+0.1)=s(1.805)=0.86p( |x)=P(Y=0|x)=1 s(wÂ·x+b)=0.14Logistic regression is commonly applied to all sorts of NLP tasks, and any prop-erty of the input can be a feature. Consider the task ofperiod disambiguation:deciding if a period is the end of a sentence or part of a word, by classifying eachperiod into one of two classes EOS (end-of-sentence) and not-EOS. We might usefeatures likex1below expressing that the current word is lower case and the classis EOS (perhaps with a positive weight), or that the current word is in our abbrevia-tions dictionary (â€œProf.â€) and the class is EOS (perhaps with a negative weight). Afeature can also express a quite complex combination of properties. For example aperiod following a upper cased word is a likely to be an EOS, but if the word itself isSt.and the previous word is capitalized, then the period is likely part of a shorteningof the wordstreet.x1=â‡¢1 if â€œCase(wi)=Lowerâ€0 otherwisex2=â‡¢1 if â€œwi2AcronymDictâ€0 otherwisex3=â‡¢1 if â€œwi=St. &Case(wi 1)=Capâ€0 otherwiseDesigning features:Features are generally designed by examining the trainingset with an eye to linguistic intuitions and the linguistic literature on the domain. Acareful error analysis on the training or dev set. of an early version of a system oftenprovides insights into features.33This ends in a period.The house at 465 Main St. is new.End of sentenceNot end

## Page 34

Classification in (binary)logistic regression: summaryGiven:â—¦a set of classes:  (+ sentiment,-sentiment)â—¦a vector xof features [x1, x2, â€¦, xn]â—¦x1= count( "awesome")â—¦x2 = log(number of words in review)â—¦A vector wof weights  [w1, w2, â€¦, wn]â—¦wifor each feature fi4CHAPTER5â€¢LOGISTICREGRESSIONThe sigmoid has a number of advantages; it take a real-valued number and mapsit into the range[0,1], which is just what we want for a probability. Because it isnearly linear around 0 but has a sharp slope toward the ends, it tends to squash outliervalues toward 0 or 1. And itâ€™s differentiable, which as weâ€™ll see in Section5.8willbe handy for learning.Weâ€™re almost there. If we apply the sigmoid to the sum of the weighted features,we get a number between 0 and 1. To make it a probability, we just need to makesure that the two cases,p(y=1)andp(y=0), sum to 1. We can do this as follows:P(y=1)=s(wÂ·x+b)=11+e (wÂ·x+b)P(y=0)=1 s(wÂ·x+b)=1 11+e (wÂ·x+b)=e (wÂ·x+b)1+e (wÂ·x+b)(5.5)Now we have an algorithm that given an instancexcomputes the probabilityP(y=1|x). How do we make a decision? For a test instancex, we say yes if theprobabilityP(y=1|x)is more than .5, and no otherwise. We call .5 thedecisionboundary:decisionboundaryË†y=â‡¢1 ifP(y=1|x)>0.50 otherwise5.1.1 Example: sentiment classiï¬cationLetâ€™s have an example. Suppose we are doing binary sentiment classiï¬cation onmovie review text, and we would like to know whether to assign the sentiment class+or to a review documentdoc. Weâ€™ll represent each input observation by thefollowing 6 featuresx1...x6of the input; Fig.5.2shows the features in a sample minitest document.Var Deï¬nition Value in Fig.5.2x1count(positive lexicon)2doc)3x2count(negative lexicon)2doc)2x3â‡¢1 if â€œnoâ€2doc0 otherwise1x4count(1st and 2nd pronouns2doc)3x5â‡¢1 if â€œ!â€2doc0 otherwise0x6log(word count of doc)ln(64)=4.15Letâ€™s assume for the moment that weâ€™ve already learned a real-valued weightfor each of these features, and that the 6 weights corresponding to the 6 featuresare[2.5, 5.0, 1.2,0.5,2.0,0.7], whileb= 0.1. (Weâ€™ll discuss in the next sectionhow the weights are learned.) The weightw1, for example indicates how important

## Page 35

Logistic RegressionLogistic Regression: a text example on sentiment classification

## Page 36

Logistic RegressionLearning: Cross-Entropy Loss

## Page 37

Wait, where did the Wâ€™s come from?Supervised classification: â€¢We know the correct label y(either 0 or 1) for each x. â€¢But what the system produces is an estimate, !ğ‘¦We want to set w and bto minimize the distancebetween our estimate !ğ‘¦(i)and the true y(i). â€¢We need a distance estimator: a loss function or a cost functionâ€¢We need an optimization algorithm to update wand bto minimize the loss.37

## Page 38

Learning componentsA loss function:â—¦cross-entropy lossAn optimization algorithm:â—¦stochastic gradient descent

## Page 39

The distance between !ğ‘¦and yWe want to know how far is the classifier output:!ğ‘¦= Ïƒ(wÂ·x+b)from the true output:y        [= either 0 or 1]We'll call this difference:L(!ğ‘¦,y) = how much !ğ‘¦differs from the true y 

## Page 40

Intuition of negative log likelihood loss= cross-entropy lossA case of conditional maximum likelihood estimation We choose the parameters w,bthat maximizeâ€¢the log probability â€¢of the true y labels in the training data â€¢given the observations x

## Page 41

Deriving cross-entropy loss for a single observation xGoal: maximize probability of the correct label p(y|x) Since there are only 2 discrete outcomes (0 or 1) we can express the probability p(y|x) from our classifier (the thing we want to maximize) asnoting:if y=1, this simplifies to !ğ‘¦if y=0, this simplifies to 1-!ğ‘¦5.3â€¢THE CROSS-ENTROPY LOSS FUNCTION7thecross-entropy loss.The second thing we need is an optimization algorithm for iteratively updatingthe weights so as to minimize this loss function. The standard algorithm for this isgradient descent; weâ€™ll introduce thestochastic gradient descentalgorithm in thefollowing section.5.3 The cross-entropy loss functionWe need a loss function that expresses, for an observationx, how close the classiï¬eroutput ( Ë†y=s(wÂ·x+b)) is to the correct output (y, which is 0 or 1). Weâ€™ll call this:L(Ë†y,y)=How much Ë†ydiffers from the truey(5.8)We do this via a loss function that prefers the correct class labels of the train-ing examples to bemore likely. This is calledconditional maximum likelihoodestimation: we choose the parametersw,bthatmaximize the log probability ofthe trueylabels in the training datagiven the observationsx. The resulting lossfunction is the negative log likelihood loss, generally called thecross-entropy loss.cross-entropylossLetâ€™s derive this loss function, applied to a single observationx. Weâ€™d like tolearn weights that maximize the probability of the correct labelp(y|x). Since thereare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we canexpress the probabilityp(y|x)that our classiï¬er produces for one observation asthe following (keeping in mind that if y=1, Eq.5.9simpliï¬es to Ë†y; if y=0, Eq.5.9simpliï¬es to 1 Ë†y):p(y|x)=Ë†yy(1 Ë†y)1 y(5.9)Now we take the log of both sides. This will turn out to be handy mathematically,and doesnâ€™t hurt us; whatever values maximize a probability will also maximize thelog of the probability:logp(y|x)=logâ‡¥Ë†yy(1 Ë†y)1 yâ‡¤=ylog Ë†y+(1 y)log(1 Ë†y)(5.10)Eq.5.10describes a log likelihood that should be maximized. In order to turn thisinto loss function (something that we need to minimize), weâ€™ll just ï¬‚ip the sign onEq.5.10. The result is the cross-entropy lossLCE:LCE(Ë†y,y)= logp(y|x)= [ylog Ë†y+(1 y)log(1 Ë†y)](5.11)Finally, we can plug in the deï¬nition of Ë†y=s(wÂ·x+b):LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))](5.12)Letâ€™s see if this loss function does the right thing for our example from Fig.5.2.W ewant the loss to be smaller if the modelâ€™s estimate is close to correct, and bigger ifthe model is confused. So ï¬rst letâ€™s suppose the correct gold label for the sentimentexample in Fig.5.2is positive, i.e.,y=1. In this case our model is doing well, sincefrom Eq.5.7it indeed gave the example a higher probability of being positive (.69)than negative (.31). If we plugs(wÂ·x+b)=.69 andy=1 into Eq.5.12, the right

## Page 42

Deriving cross-entropy loss for a single observation xNow take the log of both sides (mathematically handy)Whatever values maximize log p(y|x) will also maximize p(y|x)5.3â€¢THE CROSS-ENTROPY LOSS FUNCTION7thecross-entropy loss.The second thing we need is an optimization algorithm for iteratively updatingthe weights so as to minimize this loss function. The standard algorithm for this isgradient descent; weâ€™ll introduce thestochastic gradient descentalgorithm in thefollowing section.5.3 The cross-entropy loss functionWe need a loss function that expresses, for an observationx, how close the classiï¬eroutput ( Ë†y=s(wÂ·x+b)) is to the correct output (y, which is 0 or 1). Weâ€™ll call this:L(Ë†y,y)=How much Ë†ydiffers from the truey(5.8)We do this via a loss function that prefers the correct class labels of the train-ing examples to bemore likely. This is calledconditional maximum likelihoodestimation: we choose the parametersw,bthatmaximize the log probability ofthe trueylabels in the training datagiven the observationsx. The resulting lossfunction is the negative log likelihood loss, generally called thecross-entropy loss.cross-entropylossLetâ€™s derive this loss function, applied to a single observationx. Weâ€™d like tolearn weights that maximize the probability of the correct labelp(y|x). Since thereare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we canexpress the probabilityp(y|x)that our classiï¬er produces for one observation asthe following (keeping in mind that if y=1, Eq.5.9simpliï¬es to Ë†y; if y=0, Eq.5.9simpliï¬es to 1 Ë†y):p(y|x)=Ë†yy(1 Ë†y)1 y(5.9)Now we take the log of both sides. This will turn out to be handy mathematically,and doesnâ€™t hurt us; whatever values maximize a probability will also maximize thelog of the probability:logp(y|x)=logâ‡¥Ë†yy(1 Ë†y)1 yâ‡¤=ylog Ë†y+(1 y)log(1 Ë†y)(5.10)Eq.5.10describes a log likelihood that should be maximized. In order to turn thisinto loss function (something that we need to minimize), weâ€™ll just ï¬‚ip the sign onEq.5.10. The result is the cross-entropy lossLCE:LCE(Ë†y,y)= logp(y|x)= [ylog Ë†y+(1 y)log(1 Ë†y)](5.11)Finally, we can plug in the deï¬nition of Ë†y=s(wÂ·x+b):LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))](5.12)Letâ€™s see if this loss function does the right thing for our example from Fig.5.2.W ewant the loss to be smaller if the modelâ€™s estimate is close to correct, and bigger ifthe model is confused. So ï¬rst letâ€™s suppose the correct gold label for the sentimentexample in Fig.5.2is positive, i.e.,y=1. In this case our model is doing well, sincefrom Eq.5.7it indeed gave the example a higher probability of being positive (.69)than negative (.31). If we plugs(wÂ·x+b)=.69 andy=1 into Eq.5.12, the right5.3â€¢THE CROSS-ENTROPY LOSS FUNCTION7thecross-entropy loss.The second thing we need is an optimization algorithm for iteratively updatingthe weights so as to minimize this loss function. The standard algorithm for this isgradient descent; weâ€™ll introduce thestochastic gradient descentalgorithm in thefollowing section.5.3 The cross-entropy loss functionWe need a loss function that expresses, for an observationx, how close the classiï¬eroutput ( Ë†y=s(wÂ·x+b)) is to the correct output (y, which is 0 or 1). Weâ€™ll call this:L(Ë†y,y)=How much Ë†ydiffers from the truey(5.8)We do this via a loss function that prefers the correct class labels of the train-ing examples to bemore likely. This is calledconditional maximum likelihoodestimation: we choose the parametersw,bthatmaximize the log probability ofthe trueylabels in the training datagiven the observationsx. The resulting lossfunction is the negative log likelihood loss, generally called thecross-entropy loss.cross-entropylossLetâ€™s derive this loss function, applied to a single observationx. Weâ€™d like tolearn weights that maximize the probability of the correct labelp(y|x). Since thereare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we canexpress the probabilityp(y|x)that our classiï¬er produces for one observation asthe following (keeping in mind that if y=1, Eq.5.9simpliï¬es to Ë†y; if y=0, Eq.5.9simpliï¬es to 1 Ë†y):p(y|x)=Ë†yy(1 Ë†y)1 y(5.9)Now we take the log of both sides. This will turn out to be handy mathematically,and doesnâ€™t hurt us; whatever values maximize a probability will also maximize thelog of the probability:logp(y|x)=logâ‡¥Ë†yy(1 Ë†y)1 yâ‡¤=ylog Ë†y+(1 y)log(1 Ë†y)(5.10)Eq.5.10describes a log likelihood that should be maximized. In order to turn thisinto loss function (something that we need to minimize), weâ€™ll just ï¬‚ip the sign onEq.5.10. The result is the cross-entropy lossLCE:LCE(Ë†y,y)= logp(y|x)= [ylog Ë†y+(1 y)log(1 Ë†y)](5.11)Finally, we can plug in the deï¬nition of Ë†y=s(wÂ·x+b):LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))](5.12)Letâ€™s see if this loss function does the right thing for our example from Fig.5.2.W ewant the loss to be smaller if the modelâ€™s estimate is close to correct, and bigger ifthe model is confused. So ï¬rst letâ€™s suppose the correct gold label for the sentimentexample in Fig.5.2is positive, i.e.,y=1. In this case our model is doing well, sincefrom Eq.5.7it indeed gave the example a higher probability of being positive (.69)than negative (.31). If we plugs(wÂ·x+b)=.69 andy=1 into Eq.5.12, the rightGoal: maximize probability of the correct label p(y|x) Maximize:Maximize:

## Page 43

Deriving cross-entropy loss for a single observation xNow flip sign to turn this into a loss: something to minimizeCross-entropy loss (because is formula for cross-entropy(y,!ğ‘¦))Or, plugging in definition of !ğ‘¦:5.3â€¢THE CROSS-ENTROPY LOSS FUNCTION7thecross-entropy loss.The second thing we need is an optimization algorithm for iteratively updatingthe weights so as to minimize this loss function. The standard algorithm for this isgradient descent; weâ€™ll introduce thestochastic gradient descentalgorithm in thefollowing section.5.3 The cross-entropy loss functionWe need a loss function that expresses, for an observationx, how close the classiï¬eroutput ( Ë†y=s(wÂ·x+b)) is to the correct output (y, which is 0 or 1). Weâ€™ll call this:L(Ë†y,y)=How much Ë†ydiffers from the truey(5.8)We do this via a loss function that prefers the correct class labels of the train-ing examples to bemore likely. This is calledconditional maximum likelihoodestimation: we choose the parametersw,bthatmaximize the log probability ofthe trueylabels in the training datagiven the observationsx. The resulting lossfunction is the negative log likelihood loss, generally called thecross-entropy loss.cross-entropylossLetâ€™s derive this loss function, applied to a single observationx. Weâ€™d like tolearn weights that maximize the probability of the correct labelp(y|x). Since thereare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we canexpress the probabilityp(y|x)that our classiï¬er produces for one observation asthe following (keeping in mind that if y=1, Eq.5.9simpliï¬es to Ë†y; if y=0, Eq.5.9simpliï¬es to 1 Ë†y):p(y|x)=Ë†yy(1 Ë†y)1 y(5.9)Now we take the log of both sides. This will turn out to be handy mathematically,and doesnâ€™t hurt us; whatever values maximize a probability will also maximize thelog of the probability:logp(y|x)=logâ‡¥Ë†yy(1 Ë†y)1 yâ‡¤=ylog Ë†y+(1 y)log(1 Ë†y)(5.10)Eq.5.10describes a log likelihood that should be maximized. In order to turn thisinto loss function (something that we need to minimize), weâ€™ll just ï¬‚ip the sign onEq.5.10. The result is the cross-entropy lossLCE:LCE(Ë†y,y)= logp(y|x)= [ylog Ë†y+(1 y)log(1 Ë†y)](5.11)Finally, we can plug in the deï¬nition of Ë†y=s(wÂ·x+b):LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))](5.12)Letâ€™s see if this loss function does the right thing for our example from Fig.5.2.W ewant the loss to be smaller if the modelâ€™s estimate is close to correct, and bigger ifthe model is confused. So ï¬rst letâ€™s suppose the correct gold label for the sentimentexample in Fig.5.2is positive, i.e.,y=1. In this case our model is doing well, sincefrom Eq.5.7it indeed gave the example a higher probability of being positive (.69)than negative (.31). If we plugs(wÂ·x+b)=.69 andy=1 into Eq.5.12, the rightGoal: maximize probability of the correct label p(y|x) Maximize:Minimize:5.3â€¢THE CROSS-ENTROPY LOSS FUNCTION7thecross-entropy loss.The second thing we need is an optimization algorithm for iteratively updatingthe weights so as to minimize this loss function. The standard algorithm for this isgradient descent; weâ€™ll introduce thestochastic gradient descentalgorithm in thefollowing section.5.3 The cross-entropy loss functionWe need a loss function that expresses, for an observationx, how close the classiï¬eroutput ( Ë†y=s(wÂ·x+b)) is to the correct output (y, which is 0 or 1). Weâ€™ll call this:L(Ë†y,y)=How much Ë†ydiffers from the truey(5.8)We do this via a loss function that prefers the correct class labels of the train-ing examples to bemore likely. This is calledconditional maximum likelihoodestimation: we choose the parametersw,bthatmaximize the log probability ofthe trueylabels in the training datagiven the observationsx. The resulting lossfunction is the negative log likelihood loss, generally called thecross-entropy loss.cross-entropylossLetâ€™s derive this loss function, applied to a single observationx. Weâ€™d like tolearn weights that maximize the probability of the correct labelp(y|x). Since thereare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we canexpress the probabilityp(y|x)that our classiï¬er produces for one observation asthe following (keeping in mind that if y=1, Eq.5.9simpliï¬es to Ë†y; if y=0, Eq.5.9simpliï¬es to 1 Ë†y):p(y|x)=Ë†yy(1 Ë†y)1 y(5.9)Now we take the log of both sides. This will turn out to be handy mathematically,and doesnâ€™t hurt us; whatever values maximize a probability will also maximize thelog of the probability:logp(y|x)=logâ‡¥Ë†yy(1 Ë†y)1 yâ‡¤=ylog Ë†y+(1 y)log(1 Ë†y)(5.10)Eq.5.10describes a log likelihood that should be maximized. In order to turn thisinto loss function (something that we need to minimize), weâ€™ll just ï¬‚ip the sign onEq.5.10. The result is the cross-entropy lossLCE:LCE(Ë†y,y)= logp(y|x)= [ylog Ë†y+(1 y)log(1 Ë†y)](5.11)Finally, we can plug in the deï¬nition of Ë†y=s(wÂ·x+b):LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))](5.12)Letâ€™s see if this loss function does the right thing for our example from Fig.5.2.W ewant the loss to be smaller if the modelâ€™s estimate is close to correct, and bigger ifthe model is confused. So ï¬rst letâ€™s suppose the correct gold label for the sentimentexample in Fig.5.2is positive, i.e.,y=1. In this case our model is doing well, sincefrom Eq.5.7it indeed gave the example a higher probability of being positive (.69)than negative (.31). If we plugs(wÂ·x+b)=.69 andy=1 into Eq.5.12, the right5.3â€¢THE CROSS-ENTROPY LOSS FUNCTION7thecross-entropy loss.The second thing we need is an optimization algorithm for iteratively updatingthe weights so as to minimize this loss function. The standard algorithm for this isgradient descent; weâ€™ll introduce thestochastic gradient descentalgorithm in thefollowing section.5.3 The cross-entropy loss functionWe need a loss function that expresses, for an observationx, how close the classiï¬eroutput ( Ë†y=s(wÂ·x+b)) is to the correct output (y, which is 0 or 1). Weâ€™ll call this:L(Ë†y,y)=How much Ë†ydiffers from the truey(5.8)We do this via a loss function that prefers the correct class labels of the train-ing examples to bemore likely. This is calledconditional maximum likelihoodestimation: we choose the parametersw,bthatmaximize the log probability ofthe trueylabels in the training datagiven the observationsx. The resulting lossfunction is the negative log likelihood loss, generally called thecross-entropy loss.cross-entropylossLetâ€™s derive this loss function, applied to a single observationx. Weâ€™d like tolearn weights that maximize the probability of the correct labelp(y|x). Since thereare only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we canexpress the probabilityp(y|x)that our classiï¬er produces for one observation asthe following (keeping in mind that if y=1, Eq.5.9simpliï¬es to Ë†y; if y=0, Eq.5.9simpliï¬es to 1 Ë†y):p(y|x)=Ë†yy(1 Ë†y)1 y(5.9)Now we take the log of both sides. This will turn out to be handy mathematically,and doesnâ€™t hurt us; whatever values maximize a probability will also maximize thelog of the probability:logp(y|x)=logâ‡¥Ë†yy(1 Ë†y)1 yâ‡¤=ylog Ë†y+(1 y)log(1 Ë†y)(5.10)Eq.5.10describes a log likelihood that should be maximized. In order to turn thisinto loss function (something that we need to minimize), weâ€™ll just ï¬‚ip the sign onEq.5.10. The result is the cross-entropy lossLCE:LCE(Ë†y,y)= logp(y|x)= [ylog Ë†y+(1 y)log(1 Ë†y)](5.11)Finally, we can plug in the deï¬nition of Ë†y=s(wÂ·x+b):LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))](5.12)Letâ€™s see if this loss function does the right thing for our example from Fig.5.2.W ewant the loss to be smaller if the modelâ€™s estimate is close to correct, and bigger ifthe model is confused. So ï¬rst letâ€™s suppose the correct gold label for the sentimentexample in Fig.5.2is positive, i.e.,y=1. In this case our model is doing well, sincefrom Eq.5.7it indeed gave the example a higher probability of being positive (.69)than negative (.31). If we plugs(wÂ·x+b)=.69 andy=1 into Eq.5.12, the right

## Page 44

Let's see if this works for our sentiment exampleWe want loss to be:â€¢smaller if the model estimate is close to correctâ€¢bigger if model is confusedLet's first suppose the true label of this is y=1 (positive)It's hokey . There are virtually no surprises , and the writing is second-rate .         So why was it so enjoyable ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing . It sucked me in , and it'll do the same to you .

## Page 45

Let's see if this works for our sentiment exampleTrue value is y=1.  How well is our model doing?Pretty well!  What's the loss?5.1â€¢CLASSIFICATION:THE SIGMOID5 It's hokey . There are virtually no surprises , and the writing is second-rate . So why was it so enjoyable  ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing .  It sucked me in , and it'll do the same to you  .x1=3x6=4.19x3=1x4=3x5=0x2=2
Figure 5.2A sample mini test document showing the extracted features in the vectorx.Given these 6 features and the input reviewx,P(+|x)andP( |x)can be com-puted using Eq.5.5:p(+|x)=P(Y=1|x)=s(wÂ·x+b)=s([2.5, 5.0, 1.2,0.5,2.0,0.7]Â·[3,2,1,3,0,4.19]+0.1)=s(.833)=0.70(5.6)p( |x)=P(Y=0|x)=1 s(wÂ·x+b)=0.30Logistic regression is commonly applied to all sorts of NLP tasks, and any propertyof the input can be a feature. Consider the task ofperiod disambiguation: decidingif a period is the end of a sentence or part of a word, by classifying each periodinto one of two classes EOS (end-of-sentence) and not-EOS. We might use featureslikex1below expressing that the current word is lower case and the class is EOS(perhaps with a positive weight), or that the current word is in our abbreviationsdictionary (â€œProf.â€) and the class is EOS (perhaps with a negative weight). A featurecan also express a quite complex combination of properties. For example a periodfollowing an upper case word is likely to be an EOS, but if the word itself isSt.andthe previous word is capitalized, then the period is likely part of a shortening of thewordstreet.x1=â‡¢1 if â€œCase(wi)=Lowerâ€0 otherwisex2=â‡¢1 if â€œwi2AcronymDictâ€0 otherwisex3=â‡¢1 if â€œwi=St. &Case(wi 1)=Capâ€0 otherwiseDesigning features:Features are generally designed by examining the trainingset with an eye to linguistic intuitions and the linguistic literature on the domain. Acareful error analysis on the training set or devset of an early version of a systemoften provides insights into features.For some tasks it is especially helpful to build complex features that are combi-nations of more primitive features. We saw such a feature for period disambiguationabove, where a period on the wordSt.was less likely to be the end of the sentenceif the previous word was capitalized. For logistic regression and naive Bayes thesecombination features orfeature interactionshave to be designed by hand.featureinteractions8CHAPTER5â€¢LOGISTICREGRESSIONside of the equation drops out, leading to the following loss (weâ€™ll use log to meannatural log when the base is not speciï¬ed):LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))]= [logs(wÂ·x+b)]= log(.70)=.36By contrast, letâ€™s pretend instead that the example in Fig.5.2was actually negative,i.e.,y=0 (perhaps the reviewer went on to say â€œBut bottom line, the movie isterrible! I beg you not to see it!â€). In this case our model is confused and weâ€™d wantthe loss to be higher. Now if we plugy=0 and 1 s(wÂ·x+b)=.31 from Eq.5.7into Eq.5.12, the left side of the equation drops out:LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))]= [log(1 s(wÂ·x+b))]= log(.30)=1.2Sure enough, the loss for the ï¬rst classiï¬er (.37) is less than the loss for the secondclassiï¬er (1.17).Why does minimizing this negative log probability do what we want? A per-fect classiï¬er would assign probability 1 to the correct outcome (y=1 or y=0) andprobability 0 to the incorrect outcome. That means the higher Ë†y(the closer it isto 1), the better the classiï¬er; the lower Ë†yis (the closer it is to 0), the worse theclassiï¬er. The negative log of this probability is a convenient loss metric since itgoes from 0 (negative log of 1, no loss) to inï¬nity (negative log of 0, inï¬nite loss).This loss function also ensures that as the probability of the correct answer is max-imized, the probability of the incorrect answer is minimized; since the two sum toone, any increase in the probability of the correct answer is coming at the expenseof the incorrect answer. Itâ€™s called the cross-entropy loss, because Eq.5.10is alsothe formula for thecross-entropybetween the true probability distributionyand ourestimated distribution Ë†y.Now we know what we want to minimize; in the next section, weâ€™ll see how toï¬nd the minimum.5.4 Gradient DescentOur goal with gradient descent is to ï¬nd the optimal weights: minimize the lossfunction weâ€™ve deï¬ned for the model. In Eq.5.13below, weâ€™ll explicitly representthe fact that the loss functionLis parameterized by the weights, which weâ€™ll referto in machine learning in general asq(in the case of logistic regressionq=w,b).So the goal is to ï¬nd the set of weights which minimizes the loss function, averagedover all examples:Ë†q=argminq1mmXi=1LCE(f(x(i);q),y(i))(5.13)

## Page 46

Let's see if this works for our sentiment exampleSuppose true value instead  was y=0.  What's the loss?8CHAPTER5â€¢LOGISTICREGRESSIONside of the equation drops out, leading to the following loss (weâ€™ll use log to meannatural log when the base is not speciï¬ed):LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))]= [logs(wÂ·x+b)]= log(.70)=.36By contrast, letâ€™s pretend instead that the example in Fig.5.2was actually negative,i.e.,y=0 (perhaps the reviewer went on to say â€œBut bottom line, the movie isterrible! I beg you not to see it!â€). In this case our model is confused and weâ€™d wantthe loss to be higher. Now if we plugy=0 and 1 s(wÂ·x+b)=.31 from Eq.5.7into Eq.5.12, the left side of the equation drops out:LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))]= [log(1 s(wÂ·x+b))]= log(.30)=1.2Sure enough, the loss for the ï¬rst classiï¬er (.37) is less than the loss for the secondclassiï¬er (1.17).Why does minimizing this negative log probability do what we want? A per-fect classiï¬er would assign probability 1 to the correct outcome (y=1 or y=0) andprobability 0 to the incorrect outcome. That means the higher Ë†y(the closer it isto 1), the better the classiï¬er; the lower Ë†yis (the closer it is to 0), the worse theclassiï¬er. The negative log of this probability is a convenient loss metric since itgoes from 0 (negative log of 1, no loss) to inï¬nity (negative log of 0, inï¬nite loss).This loss function also ensures that as the probability of the correct answer is max-imized, the probability of the incorrect answer is minimized; since the two sum toone, any increase in the probability of the correct answer is coming at the expenseof the incorrect answer. Itâ€™s called the cross-entropy loss, because Eq.5.10is alsothe formula for thecross-entropybetween the true probability distributionyand ourestimated distribution Ë†y.Now we know what we want to minimize; in the next section, weâ€™ll see how toï¬nd the minimum.5.4 Gradient DescentOur goal with gradient descent is to ï¬nd the optimal weights: minimize the lossfunction weâ€™ve deï¬ned for the model. In Eq.5.13below, weâ€™ll explicitly representthe fact that the loss functionLis parameterized by the weights, which weâ€™ll referto in machine learning in general asq(in the case of logistic regressionq=w,b).So the goal is to ï¬nd the set of weights which minimizes the loss function, averagedover all examples:Ë†q=argminq1mmXi=1LCE(f(x(i);q),y(i))(5.13)5.1â€¢CLASSIFICATION:THE SIGMOID5 It's hokey . There are virtually no surprises , and the writing is second-rate . So why was it so enjoyable  ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing .  It sucked me in , and it'll do the same to you  .x1=3x6=4.19x3=1x4=3x5=0x2=2
Figure 5.2A sample mini test document showing the extracted features in the vectorx.Given these 6 features and the input reviewx,P(+|x)andP( |x)can be com-puted using Eq.5.5:p(+|x)=P(Y=1|x)=s(wÂ·x+b)=s([2.5, 5.0, 1.2,0.5,2.0,0.7]Â·[3,2,1,3,0,4.19]+0.1)=s(.833)=0.70(5.6)p( |x)=P(Y=0|x)=1 s(wÂ·x+b)=0.30Logistic regression is commonly applied to all sorts of NLP tasks, and any propertyof the input can be a feature. Consider the task ofperiod disambiguation: decidingif a period is the end of a sentence or part of a word, by classifying each periodinto one of two classes EOS (end-of-sentence) and not-EOS. We might use featureslikex1below expressing that the current word is lower case and the class is EOS(perhaps with a positive weight), or that the current word is in our abbreviationsdictionary (â€œProf.â€) and the class is EOS (perhaps with a negative weight). A featurecan also express a quite complex combination of properties. For example a periodfollowing an upper case word is likely to be an EOS, but if the word itself isSt.andthe previous word is capitalized, then the period is likely part of a shortening of thewordstreet.x1=â‡¢1 if â€œCase(wi)=Lowerâ€0 otherwisex2=â‡¢1 if â€œwi2AcronymDictâ€0 otherwisex3=â‡¢1 if â€œwi=St. &Case(wi 1)=Capâ€0 otherwiseDesigning features:Features are generally designed by examining the trainingset with an eye to linguistic intuitions and the linguistic literature on the domain. Acareful error analysis on the training set or devset of an early version of a systemoften provides insights into features.For some tasks it is especially helpful to build complex features that are combi-nations of more primitive features. We saw such a feature for period disambiguationabove, where a period on the wordSt.was less likely to be the end of the sentenceif the previous word was capitalized. For logistic regression and naive Bayes thesecombination features orfeature interactionshave to be designed by hand.featureinteractions

## Page 47

Let's see if this works for our sentiment exampleThe loss when model was right (if true y=1) Is lower than the loss when model was wrong (if true y=0):Sure enough, loss was bigger when model was wrong!8CHAPTER5â€¢LOGISTICREGRESSIONside of the equation drops out, leading to the following loss (weâ€™ll use log to meannatural log when the base is not speciï¬ed):LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))]= [logs(wÂ·x+b)]= log(.70)=.36By contrast, letâ€™s pretend instead that the example in Fig.5.2was actually negative,i.e.,y=0 (perhaps the reviewer went on to say â€œBut bottom line, the movie isterrible! I beg you not to see it!â€). In this case our model is confused and weâ€™d wantthe loss to be higher. Now if we plugy=0 and 1 s(wÂ·x+b)=.31 from Eq.5.7into Eq.5.12, the left side of the equation drops out:LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))]= [log(1 s(wÂ·x+b))]= log(.30)=1.2Sure enough, the loss for the ï¬rst classiï¬er (.37) is less than the loss for the secondclassiï¬er (1.17).Why does minimizing this negative log probability do what we want? A per-fect classiï¬er would assign probability 1 to the correct outcome (y=1 or y=0) andprobability 0 to the incorrect outcome. That means the higher Ë†y(the closer it isto 1), the better the classiï¬er; the lower Ë†yis (the closer it is to 0), the worse theclassiï¬er. The negative log of this probability is a convenient loss metric since itgoes from 0 (negative log of 1, no loss) to inï¬nity (negative log of 0, inï¬nite loss).This loss function also ensures that as the probability of the correct answer is max-imized, the probability of the incorrect answer is minimized; since the two sum toone, any increase in the probability of the correct answer is coming at the expenseof the incorrect answer. Itâ€™s called the cross-entropy loss, because Eq.5.10is alsothe formula for thecross-entropybetween the true probability distributionyand ourestimated distribution Ë†y.Now we know what we want to minimize; in the next section, weâ€™ll see how toï¬nd the minimum.5.4 Gradient DescentOur goal with gradient descent is to ï¬nd the optimal weights: minimize the lossfunction weâ€™ve deï¬ned for the model. In Eq.5.13below, weâ€™ll explicitly representthe fact that the loss functionLis parameterized by the weights, which weâ€™ll referto in machine learning in general asq(in the case of logistic regressionq=w,b).So the goal is to ï¬nd the set of weights which minimizes the loss function, averagedover all examples:Ë†q=argminq1mmXi=1LCE(f(x(i);q),y(i))(5.13)8CHAPTER5â€¢LOGISTICREGRESSIONside of the equation drops out, leading to the following loss (weâ€™ll use log to meannatural log when the base is not speciï¬ed):LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))]= [logs(wÂ·x+b)]= log(.70)=.36By contrast, letâ€™s pretend instead that the example in Fig.5.2was actually negative,i.e.,y=0 (perhaps the reviewer went on to say â€œBut bottom line, the movie isterrible! I beg you not to see it!â€). In this case our model is confused and weâ€™d wantthe loss to be higher. Now if we plugy=0 and 1 s(wÂ·x+b)=.31 from Eq.5.7into Eq.5.12, the left side of the equation drops out:LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))]= [log(1 s(wÂ·x+b))]= log(.30)=1.2Sure enough, the loss for the ï¬rst classiï¬er (.37) is less than the loss for the secondclassiï¬er (1.17).Why does minimizing this negative log probability do what we want? A per-fect classiï¬er would assign probability 1 to the correct outcome (y=1 or y=0) andprobability 0 to the incorrect outcome. That means the higher Ë†y(the closer it isto 1), the better the classiï¬er; the lower Ë†yis (the closer it is to 0), the worse theclassiï¬er. The negative log of this probability is a convenient loss metric since itgoes from 0 (negative log of 1, no loss) to inï¬nity (negative log of 0, inï¬nite loss).This loss function also ensures that as the probability of the correct answer is max-imized, the probability of the incorrect answer is minimized; since the two sum toone, any increase in the probability of the correct answer is coming at the expenseof the incorrect answer. Itâ€™s called the cross-entropy loss, because Eq.5.10is alsothe formula for thecross-entropybetween the true probability distributionyand ourestimated distribution Ë†y.Now we know what we want to minimize; in the next section, weâ€™ll see how toï¬nd the minimum.5.4 Gradient DescentOur goal with gradient descent is to ï¬nd the optimal weights: minimize the lossfunction weâ€™ve deï¬ned for the model. In Eq.5.13below, weâ€™ll explicitly representthe fact that the loss functionLis parameterized by the weights, which weâ€™ll referto in machine learning in general asq(in the case of logistic regressionq=w,b).So the goal is to ï¬nd the set of weights which minimizes the loss function, averagedover all examples:Ë†q=argminq1mmXi=1LCE(f(x(i);q),y(i))(5.13)

## Page 48

Logistic RegressionCross-Entropy Loss

## Page 49

Logistic RegressionStochastic Gradient Descent

## Page 50

Our goal: minimize the lossLet's make explicit that the loss function is parameterized by weights ğ›³=(w,b)â€¢And weâ€™ll represent !ğ‘¦as f (x; Î¸ ) to make the dependence on Î¸ more obviousWe want the weights that minimize the loss, averaged over all examples:8CHAPTER5â€¢LOGISTICREGRESSIONside of the equation drops out, leading to the following loss (weâ€™ll use log to meannatural log when the base is not speciï¬ed):LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))]= [logs(wÂ·x+b)]= log(.70)=.36By contrast, letâ€™s pretend instead that the example in Fig.5.2was actually negative,i.e.,y=0 (perhaps the reviewer went on to say â€œBut bottom line, the movie isterrible! I beg you not to see it!â€). In this case our model is confused and weâ€™d wantthe loss to be higher. Now if we plugy=0 and 1 s(wÂ·x+b)=.31 from Eq.5.7into Eq.5.12, the left side of the equation drops out:LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))]= [log(1 s(wÂ·x+b))]= log(.30)=1.2Sure enough, the loss for the ï¬rst classiï¬er (.37) is less than the loss for the secondclassiï¬er (1.17).Why does minimizing this negative log probability do what we want? A per-fect classiï¬er would assign probability 1 to the correct outcome (y=1 or y=0) andprobability 0 to the incorrect outcome. That means the higher Ë†y(the closer it isto 1), the better the classiï¬er; the lower Ë†yis (the closer it is to 0), the worse theclassiï¬er. The negative log of this probability is a convenient loss metric since itgoes from 0 (negative log of 1, no loss) to inï¬nity (negative log of 0, inï¬nite loss).This loss function also ensures that as the probability of the correct answer is max-imized, the probability of the incorrect answer is minimized; since the two sum toone, any increase in the probability of the correct answer is coming at the expenseof the incorrect answer. Itâ€™s called the cross-entropy loss, because Eq.5.10is alsothe formula for thecross-entropybetween the true probability distributionyand ourestimated distribution Ë†y.Now we know what we want to minimize; in the next section, weâ€™ll see how toï¬nd the minimum.5.4 Gradient DescentOur goal with gradient descent is to ï¬nd the optimal weights: minimize the lossfunction weâ€™ve deï¬ned for the model. In Eq.5.13below, weâ€™ll explicitly representthe fact that the loss functionLis parameterized by the weights, which weâ€™ll referto in machine learning in general asq(in the case of logistic regressionq=w,b).So the goal is to ï¬nd the set of weights which minimizes the loss function, averagedover all examples:Ë†q=argminq1mmXi=1LCE(f(x(i);q),y(i))(5.13)

## Page 51

Intuition of gradient descentHow do I get to the bottom of this river canyon?
xLook around me 360âˆ˜Find the direction of steepest slope downGo that way

## Page 52

Our goal: minimize the lossFor logistic regression, loss function is convexâ€¢A convex function has just one minimumâ€¢Gradient descent starting from any point is guaranteed to find the minimumâ€¢(Loss for neural networks is non-convex)

## Page 53

Let's first visualize for a single scalar w
wLoss
0w1wmin(goal)Should we move right or left from here?Q: Given current w, should we make it bigger or smaller?A: Move w in the reverse direction from the slope of the function 

## Page 54

Let's first visualize for a single scalar w
wLoss
0w1wminslope of loss at w1 is negative(goal)Q: Given current w, should we make it bigger or smaller?A: Move w in the reverse direction from the slope of the function 
So we'll move positive

## Page 55

Let's first visualize for a single scalar w
wLoss
0w1wminslope of loss at w1 is negative(goal)one stepof gradientdescentQ: Given current w, should we make it bigger or smaller?A: Move w in the reverse direction from the slope of the function 
So we'll move positive

## Page 56

GradientsThe gradientof a function of many variables is a vector pointing in the direction of the greatest increase in a function. Gradient Descent: Find the gradient of the loss function at the current point and move in the oppositedirection. 

## Page 57

How much do we move in that direction ?â€¢The value of the gradient (slope in our example)  !!"ğ¿(ğ‘“ğ‘¥;ğ‘¤,ğ‘¦)weighted by a learning rate Î· â€¢Higher learning rate means move wfaster10CHAPTER5â€¢LOGISTICREGRESSIONexample):wt+1=wt hddwL(f(x;w),y)(5.14)Now letâ€™s extend the intuition from a function of one scalar variablewto manyvariables, because we donâ€™t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If weâ€™re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.
Cost(w,b)
wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially weâ€™reasking: â€œHow much would a small change in that variablewiinï¬‚uence the total lossfunctionL?â€In each dimensionwi, we express the slope as a partial derivativeâˆ‚âˆ‚wiof the lossfunction. The gradient is then deï¬ned as a vector of these partials. Weâ€™ll represent Ë†yasf(x;q)to make the dependence onqmore obvious:â€”qL(f(x;q),y)) =266664âˆ‚âˆ‚w1L(f(x;q),y)âˆ‚âˆ‚w2L(f(x;q),y)...âˆ‚âˆ‚wnL(f(x;q),y)377775(5.15)The ï¬nal equation for updatingqbased on the gradient is thusqt+1=qt hâ€”L(f(x;q),y)(5.16)

## Page 58

Now let's consider N dimensionsWe want to know where in the N-dimensional space (of the N parameters that make up Î¸ ) we should move. The gradient is just such a vector; it expresses the directional components of the sharpest slope along each of the N dimensions. 

## Page 59

Imagine 2 dimensions, w and bVisualizing the gradient vector at the red pointIt has two dimensions shown in the x-y plane10CHAPTER5â€¢LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt hddwf(x;w)(5.14)Now letâ€™s extend the intuition from a function of one scalar variablewto manyvariables, because we donâ€™t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If weâ€™re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.
Cost(w,b)
wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially weâ€™reasking: â€œHow much would a small change in that variablewiinï¬‚uence the total lossfunctionL?â€In each dimensionwi, we express the slope as a partial derivativeâˆ‚âˆ‚wiof the lossfunction. The gradient is then deï¬ned as a vector of these partials. Weâ€™ll represent Ë†yasf(x;q)to make the dependence onqmore obvious:â€”qL(f(x;q),y)) =266664âˆ‚âˆ‚w1L(f(x;q),y)âˆ‚âˆ‚w2L(f(x;q),y)...âˆ‚âˆ‚wnL(f(x;q),y)377775(5.15)The ï¬nal equation for updatingqbased on the gradient is thusqt+1=qt hâ€”L(f(x;q),y)(5.16)

## Page 60

Real gradientsAre much longer; lots and lots of weightsFor each dimension withe gradient component itells us the slope with respect to that variable. â—¦â€œHow much would a small change in wiinfluence the total loss function L?â€ â—¦We express the slope as a partial derivative âˆ‚ of the loss âˆ‚wiThe gradient is then defined as a vector of these partials. 

## Page 61

The gradient10CHAPTER5â€¢LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt hddwf(x;w)(5.14)Now letâ€™s extend the intuition from a function of one scalar variablewto manyvariables, because we donâ€™t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If weâ€™re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.
Cost(w,b)
wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially weâ€™reasking: â€œHow much would a small change in that variablewiinï¬‚uence the total lossfunctionL?â€In each dimensionwi, we express the slope as a partial derivativeâˆ‚âˆ‚wiof the lossfunction. The gradient is then deï¬ned as a vector of these partials. Weâ€™ll represent Ë†yasf(x;q)to make the dependence onqmore obvious:â€”qL(f(x;q),y)) =266664âˆ‚âˆ‚w1L(f(x;q),y)âˆ‚âˆ‚w2L(f(x;q),y)...âˆ‚âˆ‚wnL(f(x;q),y)377775(5.15)The ï¬nal equation for updatingqbased on the gradient is thusqt+1=qt hâ€”L(f(x;q),y)(5.16)Weâ€™ll represent !ğ‘¦as f (x; Î¸ ) to make the dependence on Î¸ more obvious: The final equation for updating Î¸ based on the gradient is thus 10CHAPTER5â€¢LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt hddwf(x;w)(5.14)Now letâ€™s extend the intuition from a function of one scalar variablewto manyvariables, because we donâ€™t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If weâ€™re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.
Cost(w,b)
wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially weâ€™reasking: â€œHow much would a small change in that variablewiinï¬‚uence the total lossfunctionL?â€In each dimensionwi, we express the slope as a partial derivativeâˆ‚âˆ‚wiof the lossfunction. The gradient is then deï¬ned as a vector of these partials. Weâ€™ll represent Ë†yasf(x;q)to make the dependence onqmore obvious:â€”qL(f(x;q),y)) =266664âˆ‚âˆ‚w1L(f(x;q),y)âˆ‚âˆ‚w2L(f(x;q),y)...âˆ‚âˆ‚wnL(f(x;q),y)377775(5.15)The ï¬nal equation for updatingqbased on the gradient is thusqt+1=qt hâ€”L(f(x;q),y)(5.16)

## Page 62

What are these partial derivatives for logistic regression?The loss function5.4â€¢GRADIENTDESCENT115.4.1 The Gradient for Logistic RegressionIn order to updateq, we need a deï¬nition for the gradientâ€”L(f(x;q),y). Recall thatfor logistic regression, the cross-entropy loss function is:LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))](5.17)It turns out that the derivative of this function for one observation vectorxisEq.5.18(the interested reader can see Section5.8for the derivation of this equation):âˆ‚LCE(Ë†y,y)âˆ‚wj=[s(wÂ·x+b) y]xj(5.18)Note in Eq.5.18that the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated Ë†y=s(wÂ·x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute Ë†y(i)=f(x(i);q)# What is our estimated output Ë†y?Compute the lossL(Ë†y(i),y(i))# How far off is Ë†y(i))from the true outputy(i)?2.g â€”qL(f(x(i);q),y(i))# How should we moveqto maximize loss?3.q q hg# Go the other way insteadreturnqFigure 5.5The stochastic gradient descent algorithm. Step 1 (computing the loss) is usedto report how well we are doing on the current tuple. The algorithm can terminate when itconverges (or when the gradient norm<âœ), or when progress halts (for example when theloss starts going up on a held-out set).The learning ratehis ahyperparameterthat must be adjusted. If itâ€™s too high,hyperparameterthe learner will take steps that are too large, overshooting the minimum of the lossfunction. If itâ€™s too low, the learner will take steps that are too small, and take toolong to get to the minimum. It is common to start with a higher learning rate and thenslowly decrease it, so that it is a function of the iterationkof training; the notationhkcan be used to mean the value of the learning rate at iterationk.Weâ€™ll discuss hyperparameters in more detail in Chapter 7, but brieï¬‚y they area special kind of parameter for any machine learning model. Unlike regular param-eters of a model (weights likewandb), which are learned by the algorithm fromthe training set, hyperparameters are special parameters chosen by the algorithmdesigner that affect how the algorithm works.5.4â€¢GRADIENTDESCENT115.4.1 The Gradient for Logistic RegressionIn order to updateq, we need a deï¬nition for the gradientâ€”L(f(x;q),y). Recall thatfor logistic regression, the cross-entropy loss function is:LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))](5.17)It turns out that the derivative of this function for one observation vectorxisEq.5.18(the interested reader can see Section5.8for the derivation of this equation):âˆ‚LCE(Ë†y,y)âˆ‚wj=[s(wÂ·x+b) y]xj(5.18)Note in Eq.5.18that the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated Ë†y=s(wÂ·x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute Ë†y(i)=f(x(i);q)# What is our estimated output Ë†y?Compute the lossL(Ë†y(i),y(i))# How far off is Ë†y(i))from the true outputy(i)?2.g â€”qL(f(x(i);q),y(i))# How should we moveqto maximize loss?3.q q hg# Go the other way insteadreturnqFigure 5.5The stochastic gradient descent algorithm. Step 1 (computing the loss) is usedto report how well we are doing on the current tuple. The algorithm can terminate when itconverges (or when the gradient norm<âœ), or when progress halts (for example when theloss starts going up on a held-out set).The learning ratehis ahyperparameterthat must be adjusted. If itâ€™s too high,hyperparameterthe learner will take steps that are too large, overshooting the minimum of the lossfunction. If itâ€™s too low, the learner will take steps that are too small, and take toolong to get to the minimum. It is common to start with a higher learning rate and thenslowly decrease it, so that it is a function of the iterationkof training; the notationhkcan be used to mean the value of the learning rate at iterationk.Weâ€™ll discuss hyperparameters in more detail in Chapter 7, but brieï¬‚y they area special kind of parameter for any machine learning model. Unlike regular param-eters of a model (weights likewandb), which are learned by the algorithm fromthe training set, hyperparameters are special parameters chosen by the algorithmdesigner that affect how the algorithm works.The elegant derivative of this function(see textbook 5.8 for derivation)

## Page 63

5.4â€¢GRADIENTDESCENT115.4.1 The Gradient for Logistic RegressionIn order to updateq, we need a deï¬nition for the gradientâ€”L(f(x;q),y). Recall thatfor logistic regression, the cross-entropy loss function is:LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))](5.17)It turns out that the derivative of this function for one observation vectorxisEq.5.18(the interested reader can see Section5.8for the derivation of this equation):âˆ‚LCE(Ë†y,y)âˆ‚wj=[s(wÂ·x+b) y]xj(5.18)Note in Eq.5.18that the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated Ë†y=s(wÂ·x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute Ë†y(i)=f(x(i);q)# What is our estimated output Ë†y?Compute the lossL(Ë†y(i),y(i))# How far off is Ë†y(i))from the true outputy(i)?2.g â€”qL(f(x(i);q),y(i))# How should we moveqto maximize loss?3.q q hg# Go the other way insteadreturnqFigure 5.5The stochastic gradient descent algorithm. Step 1 (computing the loss) is usedto report how well we are doing on the current tuple. The algorithm can terminate when itconverges (or when the gradient norm<âœ), or when progress halts (for example when theloss starts going up on a held-out set).The learning ratehis ahyperparameterthat must be adjusted. If itâ€™s too high,hyperparameterthe learner will take steps that are too large, overshooting the minimum of the lossfunction. If itâ€™s too low, the learner will take steps that are too small, and take toolong to get to the minimum. It is common to start with a higher learning rate and thenslowly decrease it, so that it is a function of the iterationkof training; the notationhkcan be used to mean the value of the learning rate at iterationk.Weâ€™ll discuss hyperparameters in more detail in Chapter 7, but brieï¬‚y they area special kind of parameter for any machine learning model. Unlike regular param-eters of a model (weights likewandb), which are learned by the algorithm fromthe training set, hyperparameters are special parameters chosen by the algorithmdesigner that affect how the algorithm works.

## Page 64

HyperparametersThe learning rate Î·is a hyperparameterâ—¦too high: the learner will take big steps and overshootâ—¦too low: the learner will take too longHyperparameters:â€¢Briefly, a special kind of parameter for an ML modelâ€¢Instead of being learned by algorithm from supervision (like regular parameters), they are chosen by algorithm designer.

## Page 65

Logistic RegressionStochastic Gradient Descent

## Page 66

Logistic RegressionStochastic Gradient Descent: An example and more details

## Page 67

Working through an exampleOne step of gradient descentA mini-sentiment example, where the true y=1 (positive)Two features:x1= 3    (count of positive lexicon words) x2= 2    (count of negative lexicon words) Assume 3 parameters (2 weights and 1 bias) in Î˜0are zero:w1= w2= b  = 0 Î· = 0.1 

## Page 68

Example of gradient descentUpdate step for update Î¸ is:whereGradient vector has 3 dimensions:10CHAPTER5â€¢LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt hddwf(x;w)(5.14)Now letâ€™s extend the intuition from a function of one scalar variablewto manyvariables, because we donâ€™t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If weâ€™re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.
Cost(w,b)
wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially weâ€™reasking: â€œHow much would a small change in that variablewiinï¬‚uence the total lossfunctionL?â€In each dimensionwi, we express the slope as a partial derivativeâˆ‚âˆ‚wiof the lossfunction. The gradient is then deï¬ned as a vector of these partials. Weâ€™ll represent Ë†yasf(x;q)to make the dependence onqmore obvious:â€”qL(f(x;q),y)) =266664âˆ‚âˆ‚w1L(f(x;q),y)âˆ‚âˆ‚w2L(f(x;q),y)...âˆ‚âˆ‚wnL(f(x;q),y)377775(5.15)The ï¬nal equation for updatingqbased on the gradient is thusqt+1=qt hâ€”L(f(x;q),y)(5.16)12CHAPTER5â€¢LOGISTICREGRESSION5.4.3 Working through an exampleLetâ€™s walk though a single step of the gradient descent algorithm. Weâ€™ll use a sim-pliï¬ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Letâ€™s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt hâ€”qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the ï¬rst gradient as follows:â€”w,b=264âˆ‚LCE(Ë†y,y)âˆ‚w1âˆ‚LCE(Ë†y,y)âˆ‚w2âˆ‚LCE(Ë†y,y)âˆ‚b375=24(s(wÂ·x+b) y)x1(s(wÂ·x+b) y)x2s(wÂ·x+b) y35=24(s(0) 1)x1(s(0) 1)x2s(0) 135=24 0.5x1 0.5x2 0.535=24 1.5 1.0 0.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35 h24 1.5 1.0 0.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so itâ€™s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,w1= w2= b  = 0;    x1= 3;   x2= 2   5.4â€¢GRADIENTDESCENT115.4.1 The Gradient for Logistic RegressionIn order to updateq, we need a deï¬nition for the gradientâ€”L(f(x;q),y). Recall thatfor logistic regression, the cross-entropy loss function is:LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))](5.17)It turns out that the derivative of this function for one observation vectorxisEq.5.18(the interested reader can see Section5.8for the derivation of this equation):âˆ‚LCE(Ë†y,y)âˆ‚wj=[s(wÂ·x+b) y]xj(5.18)Note in Eq.5.18that the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated Ë†y=s(wÂ·x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute Ë†y(i)=f(x(i);q)# What is our estimated output Ë†y?Compute the lossL(Ë†y(i),y(i))# How far off is Ë†y(i))from the true outputy(i)?2.g â€”qL(f(x(i);q),y(i))# How should we moveqto maximize loss?3.q q hg# Go the other way insteadreturnqFigure 5.5The stochastic gradient descent algorithm. Step 1 (computing the loss) is usedto report how well we are doing on the current tuple. The algorithm can terminate when itconverges (or when the gradient norm<âœ), or when progress halts (for example when theloss starts going up on a held-out set).The learning ratehis ahyperparameterthat must be adjusted. If itâ€™s too high,hyperparameterthe learner will take steps that are too large, overshooting the minimum of the lossfunction. If itâ€™s too low, the learner will take steps that are too small, and take toolong to get to the minimum. It is common to start with a higher learning rate and thenslowly decrease it, so that it is a function of the iterationkof training; the notationhkcan be used to mean the value of the learning rate at iterationk.Weâ€™ll discuss hyperparameters in more detail in Chapter 7, but brieï¬‚y they area special kind of parameter for any machine learning model. Unlike regular param-eters of a model (weights likewandb), which are learned by the algorithm fromthe training set, hyperparameters are special parameters chosen by the algorithmdesigner that affect how the algorithm works.

## Page 69

Example of gradient descentUpdate step for update Î¸ is:whereGradient vector has 3 dimensions:10CHAPTER5â€¢LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt hddwf(x;w)(5.14)Now letâ€™s extend the intuition from a function of one scalar variablewto manyvariables, because we donâ€™t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If weâ€™re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.
Cost(w,b)
wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially weâ€™reasking: â€œHow much would a small change in that variablewiinï¬‚uence the total lossfunctionL?â€In each dimensionwi, we express the slope as a partial derivativeâˆ‚âˆ‚wiof the lossfunction. The gradient is then deï¬ned as a vector of these partials. Weâ€™ll represent Ë†yasf(x;q)to make the dependence onqmore obvious:â€”qL(f(x;q),y)) =266664âˆ‚âˆ‚w1L(f(x;q),y)âˆ‚âˆ‚w2L(f(x;q),y)...âˆ‚âˆ‚wnL(f(x;q),y)377775(5.15)The ï¬nal equation for updatingqbased on the gradient is thusqt+1=qt hâ€”L(f(x;q),y)(5.16)12CHAPTER5â€¢LOGISTICREGRESSION5.4.3 Working through an exampleLetâ€™s walk though a single step of the gradient descent algorithm. Weâ€™ll use a sim-pliï¬ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Letâ€™s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt hâ€”qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the ï¬rst gradient as follows:â€”w,b=264âˆ‚LCE(Ë†y,y)âˆ‚w1âˆ‚LCE(Ë†y,y)âˆ‚w2âˆ‚LCE(Ë†y,y)âˆ‚b375=24(s(wÂ·x+b) y)x1(s(wÂ·x+b) y)x2s(wÂ·x+b) y35=24(s(0) 1)x1(s(0) 1)x2s(0) 135=24 0.5x1 0.5x2 0.535=24 1.5 1.0 0.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35 h24 1.5 1.0 0.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so itâ€™s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,w1= w2= b  = 0;    x1= 3;   x2= 2   5.4â€¢GRADIENTDESCENT115.4.1 The Gradient for Logistic RegressionIn order to updateq, we need a deï¬nition for the gradientâ€”L(f(x;q),y). Recall thatfor logistic regression, the cross-entropy loss function is:LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))](5.17)It turns out that the derivative of this function for one observation vectorxisEq.5.18(the interested reader can see Section5.8for the derivation of this equation):âˆ‚LCE(Ë†y,y)âˆ‚wj=[s(wÂ·x+b) y]xj(5.18)Note in Eq.5.18that the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated Ë†y=s(wÂ·x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute Ë†y(i)=f(x(i);q)# What is our estimated output Ë†y?Compute the lossL(Ë†y(i),y(i))# How far off is Ë†y(i))from the true outputy(i)?2.g â€”qL(f(x(i);q),y(i))# How should we moveqto maximize loss?3.q q hg# Go the other way insteadreturnqFigure 5.5The stochastic gradient descent algorithm. Step 1 (computing the loss) is usedto report how well we are doing on the current tuple. The algorithm can terminate when itconverges (or when the gradient norm<âœ), or when progress halts (for example when theloss starts going up on a held-out set).The learning ratehis ahyperparameterthat must be adjusted. If itâ€™s too high,hyperparameterthe learner will take steps that are too large, overshooting the minimum of the lossfunction. If itâ€™s too low, the learner will take steps that are too small, and take toolong to get to the minimum. It is common to start with a higher learning rate and thenslowly decrease it, so that it is a function of the iterationkof training; the notationhkcan be used to mean the value of the learning rate at iterationk.Weâ€™ll discuss hyperparameters in more detail in Chapter 7, but brieï¬‚y they area special kind of parameter for any machine learning model. Unlike regular param-eters of a model (weights likewandb), which are learned by the algorithm fromthe training set, hyperparameters are special parameters chosen by the algorithmdesigner that affect how the algorithm works.

## Page 70

Example of gradient descentUpdate step for update Î¸ is:whereGradient vector has 3 dimensions:10CHAPTER5â€¢LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt hddwf(x;w)(5.14)Now letâ€™s extend the intuition from a function of one scalar variablewto manyvariables, because we donâ€™t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If weâ€™re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.
Cost(w,b)
wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially weâ€™reasking: â€œHow much would a small change in that variablewiinï¬‚uence the total lossfunctionL?â€In each dimensionwi, we express the slope as a partial derivativeâˆ‚âˆ‚wiof the lossfunction. The gradient is then deï¬ned as a vector of these partials. Weâ€™ll represent Ë†yasf(x;q)to make the dependence onqmore obvious:â€”qL(f(x;q),y)) =266664âˆ‚âˆ‚w1L(f(x;q),y)âˆ‚âˆ‚w2L(f(x;q),y)...âˆ‚âˆ‚wnL(f(x;q),y)377775(5.15)The ï¬nal equation for updatingqbased on the gradient is thusqt+1=qt hâ€”L(f(x;q),y)(5.16)12CHAPTER5â€¢LOGISTICREGRESSION5.4.3 Working through an exampleLetâ€™s walk though a single step of the gradient descent algorithm. Weâ€™ll use a sim-pliï¬ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Letâ€™s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt hâ€”qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the ï¬rst gradient as follows:â€”w,b=264âˆ‚LCE(Ë†y,y)âˆ‚w1âˆ‚LCE(Ë†y,y)âˆ‚w2âˆ‚LCE(Ë†y,y)âˆ‚b375=24(s(wÂ·x+b) y)x1(s(wÂ·x+b) y)x2s(wÂ·x+b) y35=24(s(0) 1)x1(s(0) 1)x2s(0) 135=24 0.5x1 0.5x2 0.535=24 1.5 1.0 0.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35 h24 1.5 1.0 0.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so itâ€™s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,w1= w2= b  = 0;    x1= 3;   x2= 2   5.4â€¢GRADIENTDESCENT115.4.1 The Gradient for Logistic RegressionIn order to updateq, we need a deï¬nition for the gradientâ€”L(f(x;q),y). Recall thatfor logistic regression, the cross-entropy loss function is:LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))](5.17)It turns out that the derivative of this function for one observation vectorxisEq.5.18(the interested reader can see Section5.8for the derivation of this equation):âˆ‚LCE(Ë†y,y)âˆ‚wj=[s(wÂ·x+b) y]xj(5.18)Note in Eq.5.18that the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated Ë†y=s(wÂ·x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute Ë†y(i)=f(x(i);q)# What is our estimated output Ë†y?Compute the lossL(Ë†y(i),y(i))# How far off is Ë†y(i))from the true outputy(i)?2.g â€”qL(f(x(i);q),y(i))# How should we moveqto maximize loss?3.q q hg# Go the other way insteadreturnqFigure 5.5The stochastic gradient descent algorithm. Step 1 (computing the loss) is usedto report how well we are doing on the current tuple. The algorithm can terminate when itconverges (or when the gradient norm<âœ), or when progress halts (for example when theloss starts going up on a held-out set).The learning ratehis ahyperparameterthat must be adjusted. If itâ€™s too high,hyperparameterthe learner will take steps that are too large, overshooting the minimum of the lossfunction. If itâ€™s too low, the learner will take steps that are too small, and take toolong to get to the minimum. It is common to start with a higher learning rate and thenslowly decrease it, so that it is a function of the iterationkof training; the notationhkcan be used to mean the value of the learning rate at iterationk.Weâ€™ll discuss hyperparameters in more detail in Chapter 7, but brieï¬‚y they area special kind of parameter for any machine learning model. Unlike regular param-eters of a model (weights likewandb), which are learned by the algorithm fromthe training set, hyperparameters are special parameters chosen by the algorithmdesigner that affect how the algorithm works.

## Page 71

Example of gradient descentUpdate step for update Î¸ is:whereGradient vector has 3 dimensions:10CHAPTER5â€¢LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt hddwf(x;w)(5.14)Now letâ€™s extend the intuition from a function of one scalar variablewto manyvariables, because we donâ€™t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If weâ€™re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.
Cost(w,b)
wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially weâ€™reasking: â€œHow much would a small change in that variablewiinï¬‚uence the total lossfunctionL?â€In each dimensionwi, we express the slope as a partial derivativeâˆ‚âˆ‚wiof the lossfunction. The gradient is then deï¬ned as a vector of these partials. Weâ€™ll represent Ë†yasf(x;q)to make the dependence onqmore obvious:â€”qL(f(x;q),y)) =266664âˆ‚âˆ‚w1L(f(x;q),y)âˆ‚âˆ‚w2L(f(x;q),y)...âˆ‚âˆ‚wnL(f(x;q),y)377775(5.15)The ï¬nal equation for updatingqbased on the gradient is thusqt+1=qt hâ€”L(f(x;q),y)(5.16)12CHAPTER5â€¢LOGISTICREGRESSION5.4.3 Working through an exampleLetâ€™s walk though a single step of the gradient descent algorithm. Weâ€™ll use a sim-pliï¬ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Letâ€™s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt hâ€”qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the ï¬rst gradient as follows:â€”w,b=264âˆ‚LCE(Ë†y,y)âˆ‚w1âˆ‚LCE(Ë†y,y)âˆ‚w2âˆ‚LCE(Ë†y,y)âˆ‚b375=24(s(wÂ·x+b) y)x1(s(wÂ·x+b) y)x2s(wÂ·x+b) y35=24(s(0) 1)x1(s(0) 1)x2s(0) 135=24 0.5x1 0.5x2 0.535=24 1.5 1.0 0.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35 h24 1.5 1.0 0.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so itâ€™s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,w1= w2= b  = 0;    x1= 3;   x2= 2   5.4â€¢GRADIENTDESCENT115.4.1 The Gradient for Logistic RegressionIn order to updateq, we need a deï¬nition for the gradientâ€”L(f(x;q),y). Recall thatfor logistic regression, the cross-entropy loss function is:LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))](5.17)It turns out that the derivative of this function for one observation vectorxisEq.5.18(the interested reader can see Section5.8for the derivation of this equation):âˆ‚LCE(Ë†y,y)âˆ‚wj=[s(wÂ·x+b) y]xj(5.18)Note in Eq.5.18that the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated Ë†y=s(wÂ·x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute Ë†y(i)=f(x(i);q)# What is our estimated output Ë†y?Compute the lossL(Ë†y(i),y(i))# How far off is Ë†y(i))from the true outputy(i)?2.g â€”qL(f(x(i);q),y(i))# How should we moveqto maximize loss?3.q q hg# Go the other way insteadreturnqFigure 5.5The stochastic gradient descent algorithm. Step 1 (computing the loss) is usedto report how well we are doing on the current tuple. The algorithm can terminate when itconverges (or when the gradient norm<âœ), or when progress halts (for example when theloss starts going up on a held-out set).The learning ratehis ahyperparameterthat must be adjusted. If itâ€™s too high,hyperparameterthe learner will take steps that are too large, overshooting the minimum of the lossfunction. If itâ€™s too low, the learner will take steps that are too small, and take toolong to get to the minimum. It is common to start with a higher learning rate and thenslowly decrease it, so that it is a function of the iterationkof training; the notationhkcan be used to mean the value of the learning rate at iterationk.Weâ€™ll discuss hyperparameters in more detail in Chapter 7, but brieï¬‚y they area special kind of parameter for any machine learning model. Unlike regular param-eters of a model (weights likewandb), which are learned by the algorithm fromthe training set, hyperparameters are special parameters chosen by the algorithmdesigner that affect how the algorithm works.

## Page 72

Example of gradient descentUpdate step for update Î¸ is:whereGradient vector has 3 dimensions:10CHAPTER5â€¢LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt hddwf(x;w)(5.14)Now letâ€™s extend the intuition from a function of one scalar variablewto manyvariables, because we donâ€™t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If weâ€™re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.
Cost(w,b)
wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially weâ€™reasking: â€œHow much would a small change in that variablewiinï¬‚uence the total lossfunctionL?â€In each dimensionwi, we express the slope as a partial derivativeâˆ‚âˆ‚wiof the lossfunction. The gradient is then deï¬ned as a vector of these partials. Weâ€™ll represent Ë†yasf(x;q)to make the dependence onqmore obvious:â€”qL(f(x;q),y)) =266664âˆ‚âˆ‚w1L(f(x;q),y)âˆ‚âˆ‚w2L(f(x;q),y)...âˆ‚âˆ‚wnL(f(x;q),y)377775(5.15)The ï¬nal equation for updatingqbased on the gradient is thusqt+1=qt hâ€”L(f(x;q),y)(5.16)12CHAPTER5â€¢LOGISTICREGRESSION5.4.3 Working through an exampleLetâ€™s walk though a single step of the gradient descent algorithm. Weâ€™ll use a sim-pliï¬ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Letâ€™s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt hâ€”qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the ï¬rst gradient as follows:â€”w,b=264âˆ‚LCE(Ë†y,y)âˆ‚w1âˆ‚LCE(Ë†y,y)âˆ‚w2âˆ‚LCE(Ë†y,y)âˆ‚b375=24(s(wÂ·x+b) y)x1(s(wÂ·x+b) y)x2s(wÂ·x+b) y35=24(s(0) 1)x1(s(0) 1)x2s(0) 135=24 0.5x1 0.5x2 0.535=24 1.5 1.0 0.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35 h24 1.5 1.0 0.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so itâ€™s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,w1= w2= b  = 0;    x1= 3;   x2= 2   5.4â€¢GRADIENTDESCENT115.4.1 The Gradient for Logistic RegressionIn order to updateq, we need a deï¬nition for the gradientâ€”L(f(x;q),y). Recall thatfor logistic regression, the cross-entropy loss function is:LCE(Ë†y,y)= [ylogs(wÂ·x+b)+(1 y)log(1 s(wÂ·x+b))](5.17)It turns out that the derivative of this function for one observation vectorxisEq.5.18(the interested reader can see Section5.8for the derivation of this equation):âˆ‚LCE(Ë†y,y)âˆ‚wj=[s(wÂ·x+b) y]xj(5.18)Note in Eq.5.18that the gradient with respect to a single weightwjrepresents avery intuitive value: the difference between the trueyand our estimated Ë†y=s(wÂ·x+b)for that observation, multiplied by the corresponding input valuexj.5.4.2 The Stochastic Gradient Descent AlgorithmStochastic gradient descent is an online algorithm that minimizes the loss functionby computing its gradient after each training example, and nudgingqin the rightdirection (the opposite direction of the gradient). Fig.5.5shows the algorithm.functionSTOCHASTICGRADIENTDESCENT(L(),f(),x,y)returnsq# where: L is the loss function# f is a function parameterized byq# x is the set of training inputsx(1),x(2),. .. ,x(m)# y is the set of training outputs (labels)y(1),y(2),. .. ,y(m)q 0repeattil done # see captionFor each training tuple(x(i),y(i))(in random order)1. Optional (for reporting): # How are we doing on this tuple?Compute Ë†y(i)=f(x(i);q)# What is our estimated output Ë†y?Compute the lossL(Ë†y(i),y(i))# How far off is Ë†y(i))from the true outputy(i)?2.g â€”qL(f(x(i);q),y(i))# How should we moveqto maximize loss?3.q q hg# Go the other way insteadreturnqFigure 5.5The stochastic gradient descent algorithm. Step 1 (computing the loss) is usedto report how well we are doing on the current tuple. The algorithm can terminate when itconverges (or when the gradient norm<âœ), or when progress halts (for example when theloss starts going up on a held-out set).The learning ratehis ahyperparameterthat must be adjusted. If itâ€™s too high,hyperparameterthe learner will take steps that are too large, overshooting the minimum of the lossfunction. If itâ€™s too low, the learner will take steps that are too small, and take toolong to get to the minimum. It is common to start with a higher learning rate and thenslowly decrease it, so that it is a function of the iterationkof training; the notationhkcan be used to mean the value of the learning rate at iterationk.Weâ€™ll discuss hyperparameters in more detail in Chapter 7, but brieï¬‚y they area special kind of parameter for any machine learning model. Unlike regular param-eters of a model (weights likewandb), which are learned by the algorithm fromthe training set, hyperparameters are special parameters chosen by the algorithmdesigner that affect how the algorithm works.

## Page 73

Example of gradient descent10CHAPTER5â€¢LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt hddwf(x;w)(5.14)Now letâ€™s extend the intuition from a function of one scalar variablewto manyvariables, because we donâ€™t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If weâ€™re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.
Cost(w,b)
wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially weâ€™reasking: â€œHow much would a small change in that variablewiinï¬‚uence the total lossfunctionL?â€In each dimensionwi, we express the slope as a partial derivativeâˆ‚âˆ‚wiof the lossfunction. The gradient is then deï¬ned as a vector of these partials. Weâ€™ll represent Ë†yasf(x;q)to make the dependence onqmore obvious:â€”qL(f(x;q),y)) =266664âˆ‚âˆ‚w1L(f(x;q),y)âˆ‚âˆ‚w2L(f(x;q),y)...âˆ‚âˆ‚wnL(f(x;q),y)377775(5.15)The ï¬nal equation for updatingqbased on the gradient is thusqt+1=qt hâ€”L(f(x;q),y)(5.16)12CHAPTER5â€¢LOGISTICREGRESSION5.4.3 Working through an exampleLetâ€™s walk though a single step of the gradient descent algorithm. Weâ€™ll use a sim-pliï¬ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Letâ€™s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt hâ€”qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the ï¬rst gradient as follows:â€”w,b=264âˆ‚LCE(Ë†y,y)âˆ‚w1âˆ‚LCE(Ë†y,y)âˆ‚w2âˆ‚LCE(Ë†y,y)âˆ‚b375=24(s(wÂ·x+b) y)x1(s(wÂ·x+b) y)x2s(wÂ·x+b) y35=24(s(0) 1)x1(s(0) 1)x2s(0) 135=24 0.5x1 0.5x2 0.535=24 1.5 1.0 0.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35 h24 1.5 1.0 0.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so itâ€™s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,Î· = 0.1; 12CHAPTER5â€¢LOGISTICREGRESSION5.4.3 Working through an exampleLetâ€™s walk though a single step of the gradient descent algorithm. Weâ€™ll use a sim-pliï¬ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Letâ€™s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt hâ€”qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the ï¬rst gradient as follows:â€”w,b=264âˆ‚LCE(Ë†y,y)âˆ‚w1âˆ‚LCE(Ë†y,y)âˆ‚w2âˆ‚LCE(Ë†y,y)âˆ‚b375=24(s(wÂ·x+b) y)x1(s(wÂ·x+b) y)x2s(wÂ·x+b) y35=24(s(0) 1)x1(s(0) 1)x2s(0) 135=24 0.5x1 0.5x2 0.535=24 1.5 1.0 0.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35 h24 1.5 1.0 0.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so itâ€™s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,Now that we have a gradient, we compute the new parameter vector Î¸1by moving Î¸0in the opposite direction from the gradient: 

## Page 74

Example of gradient descent10CHAPTER5â€¢LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt hddwf(x;w)(5.14)Now letâ€™s extend the intuition from a function of one scalar variablewto manyvariables, because we donâ€™t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If weâ€™re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.
Cost(w,b)
wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially weâ€™reasking: â€œHow much would a small change in that variablewiinï¬‚uence the total lossfunctionL?â€In each dimensionwi, we express the slope as a partial derivativeâˆ‚âˆ‚wiof the lossfunction. The gradient is then deï¬ned as a vector of these partials. Weâ€™ll represent Ë†yasf(x;q)to make the dependence onqmore obvious:â€”qL(f(x;q),y)) =266664âˆ‚âˆ‚w1L(f(x;q),y)âˆ‚âˆ‚w2L(f(x;q),y)...âˆ‚âˆ‚wnL(f(x;q),y)377775(5.15)The ï¬nal equation for updatingqbased on the gradient is thusqt+1=qt hâ€”L(f(x;q),y)(5.16)12CHAPTER5â€¢LOGISTICREGRESSION5.4.3 Working through an exampleLetâ€™s walk though a single step of the gradient descent algorithm. Weâ€™ll use a sim-pliï¬ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Letâ€™s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt hâ€”qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the ï¬rst gradient as follows:â€”w,b=264âˆ‚LCE(Ë†y,y)âˆ‚w1âˆ‚LCE(Ë†y,y)âˆ‚w2âˆ‚LCE(Ë†y,y)âˆ‚b375=24(s(wÂ·x+b) y)x1(s(wÂ·x+b) y)x2s(wÂ·x+b) y35=24(s(0) 1)x1(s(0) 1)x2s(0) 135=24 0.5x1 0.5x2 0.535=24 1.5 1.0 0.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35 h24 1.5 1.0 0.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so itâ€™s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,Î· = 0.1; 12CHAPTER5â€¢LOGISTICREGRESSION5.4.3 Working through an exampleLetâ€™s walk though a single step of the gradient descent algorithm. Weâ€™ll use a sim-pliï¬ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Letâ€™s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt hâ€”qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the ï¬rst gradient as follows:â€”w,b=264âˆ‚LCE(Ë†y,y)âˆ‚w1âˆ‚LCE(Ë†y,y)âˆ‚w2âˆ‚LCE(Ë†y,y)âˆ‚b375=24(s(wÂ·x+b) y)x1(s(wÂ·x+b) y)x2s(wÂ·x+b) y35=24(s(0) 1)x1(s(0) 1)x2s(0) 135=24 0.5x1 0.5x2 0.535=24 1.5 1.0 0.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35 h24 1.5 1.0 0.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so itâ€™s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,Now that we have a gradient, we compute the new parameter vector Î¸1by moving Î¸0in the opposite direction from the gradient: 

## Page 75

Example of gradient descent10CHAPTER5â€¢LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt hddwf(x;w)(5.14)Now letâ€™s extend the intuition from a function of one scalar variablewto manyvariables, because we donâ€™t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If weâ€™re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.
Cost(w,b)
wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially weâ€™reasking: â€œHow much would a small change in that variablewiinï¬‚uence the total lossfunctionL?â€In each dimensionwi, we express the slope as a partial derivativeâˆ‚âˆ‚wiof the lossfunction. The gradient is then deï¬ned as a vector of these partials. Weâ€™ll represent Ë†yasf(x;q)to make the dependence onqmore obvious:â€”qL(f(x;q),y)) =266664âˆ‚âˆ‚w1L(f(x;q),y)âˆ‚âˆ‚w2L(f(x;q),y)...âˆ‚âˆ‚wnL(f(x;q),y)377775(5.15)The ï¬nal equation for updatingqbased on the gradient is thusqt+1=qt hâ€”L(f(x;q),y)(5.16)12CHAPTER5â€¢LOGISTICREGRESSION5.4.3 Working through an exampleLetâ€™s walk though a single step of the gradient descent algorithm. Weâ€™ll use a sim-pliï¬ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Letâ€™s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt hâ€”qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the ï¬rst gradient as follows:â€”w,b=264âˆ‚LCE(Ë†y,y)âˆ‚w1âˆ‚LCE(Ë†y,y)âˆ‚w2âˆ‚LCE(Ë†y,y)âˆ‚b375=24(s(wÂ·x+b) y)x1(s(wÂ·x+b) y)x2s(wÂ·x+b) y35=24(s(0) 1)x1(s(0) 1)x2s(0) 135=24 0.5x1 0.5x2 0.535=24 1.5 1.0 0.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35 h24 1.5 1.0 0.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so itâ€™s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,Î· = 0.1; 12CHAPTER5â€¢LOGISTICREGRESSION5.4.3 Working through an exampleLetâ€™s walk though a single step of the gradient descent algorithm. Weâ€™ll use a sim-pliï¬ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Letâ€™s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt hâ€”qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the ï¬rst gradient as follows:â€”w,b=264âˆ‚LCE(Ë†y,y)âˆ‚w1âˆ‚LCE(Ë†y,y)âˆ‚w2âˆ‚LCE(Ë†y,y)âˆ‚b375=24(s(wÂ·x+b) y)x1(s(wÂ·x+b) y)x2s(wÂ·x+b) y35=24(s(0) 1)x1(s(0) 1)x2s(0) 135=24 0.5x1 0.5x2 0.535=24 1.5 1.0 0.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35 h24 1.5 1.0 0.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so itâ€™s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,Now that we have a gradient, we compute the new parameter vector Î¸1by moving Î¸0in the opposite direction from the gradient: 

## Page 76

Example of gradient descent10CHAPTER5â€¢LOGISTICREGRESSIONlearning rate times the gradient (or the slope, in our single-variable example):wt+1=wt hddwf(x;w)(5.14)Now letâ€™s extend the intuition from a function of one scalar variablewto manyvariables, because we donâ€™t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If weâ€™re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.
Cost(w,b)
wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially weâ€™reasking: â€œHow much would a small change in that variablewiinï¬‚uence the total lossfunctionL?â€In each dimensionwi, we express the slope as a partial derivativeâˆ‚âˆ‚wiof the lossfunction. The gradient is then deï¬ned as a vector of these partials. Weâ€™ll represent Ë†yasf(x;q)to make the dependence onqmore obvious:â€”qL(f(x;q),y)) =266664âˆ‚âˆ‚w1L(f(x;q),y)âˆ‚âˆ‚w2L(f(x;q),y)...âˆ‚âˆ‚wnL(f(x;q),y)377775(5.15)The ï¬nal equation for updatingqbased on the gradient is thusqt+1=qt hâ€”L(f(x;q),y)(5.16)12CHAPTER5â€¢LOGISTICREGRESSION5.4.3 Working through an exampleLetâ€™s walk though a single step of the gradient descent algorithm. Weâ€™ll use a sim-pliï¬ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Letâ€™s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt hâ€”qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the ï¬rst gradient as follows:â€”w,b=264âˆ‚LCE(Ë†y,y)âˆ‚w1âˆ‚LCE(Ë†y,y)âˆ‚w2âˆ‚LCE(Ë†y,y)âˆ‚b375=24(s(wÂ·x+b) y)x1(s(wÂ·x+b) y)x2s(wÂ·x+b) y35=24(s(0) 1)x1(s(0) 1)x2s(0) 135=24 0.5x1 0.5x2 0.535=24 1.5 1.0 0.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35 h24 1.5 1.0 0.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so itâ€™s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,Î· = 0.1; 12CHAPTER5â€¢LOGISTICREGRESSION5.4.3 Working through an exampleLetâ€™s walk though a single step of the gradient descent algorithm. Weâ€™ll use a sim-pliï¬ed version of the example in Fig.5.2as it sees a single observationx, whosecorrect value isy=1 (this is a positive review), and with only two features:x1=3 (count of positive lexicon words)x2=2 (count of negative lexicon words)Letâ€™s assume the initial weights and bias inq0are all set to 0, and the initial learningratehis 0.1:w1=w2=b=0h=0.1The single update step requires that we compute the gradient, multiplied by thelearning rateqt+1=qt hâ€”qL(f(x(i);q),y(i))In our mini example there are three parameters, so the gradient vector has 3 dimen-sions, forw1,w2, andb. We can compute the ï¬rst gradient as follows:â€”w,b=264âˆ‚LCE(Ë†y,y)âˆ‚w1âˆ‚LCE(Ë†y,y)âˆ‚w2âˆ‚LCE(Ë†y,y)âˆ‚b375=24(s(wÂ·x+b) y)x1(s(wÂ·x+b) y)x2s(wÂ·x+b) y35=24(s(0) 1)x1(s(0) 1)x2s(0) 135=24 0.5x1 0.5x2 0.535=24 1.5 1.0 0.535Now that we have a gradient, we compute the new parameter vectorq1by movingq0in the opposite direction from the gradient:q1=24w1w2b35 h24 1.5 1.0 0.535=24.15.1.0535So after one step of gradient descent, the weights have shifted to be:w1=.15,w2=.1, andb=.05.Note that this observationxhappened to be a positive example. We would expectthat after seeing more negative examples with high counts of negative words, thatthe weightw2would shift to have a negative value.5.4.4 Mini-batch trainingStochastic gradient descent is called stochastic because it chooses a single randomexample at a time, moving the weights so as to improve performance on that singleexample. That can result in very choppy movements, so itâ€™s common to compute thegradient over batches of training instances rather than a single instance.For example inbatch trainingwe compute the gradient over the entire dataset.batch trainingBy seeing so many examples, batch training offers a superb estimate of which di-rection to move the weights, at the cost of spending a lot of time processing everysingle example in the training set to compute this perfect direction.A compromise ismini-batchtraining: we train on a group ofmexamples (per-mini-batchhaps 512, or 1024) that is less than the whole dataset. (Ifmis the size of the dataset,Now that we have a gradient, we compute the new parameter vector Î¸1by moving Î¸0in the opposite direction from the gradient: 
Note that enough negative examples would eventually make w2negative

## Page 77

Mini-batch trainingStochastic gradient descent chooses a single random example at a time.That can result in choppy movementsMore common to compute gradient over batches of training instances.Batch training: entire datasetMini-batch training: mexamples (512, or 1024)

## Page 78

Logistic RegressionStochastic Gradient Descent: An example and more details

## Page 79

Logistic RegressionRegularization

## Page 80

OverfittingA model that perfectly match the training data has a problem.It will also overfitto the data, modeling noise â—¦A random word that perfectly predicts y(it happens to only occur in one class) will get a very high weight. â—¦Failing to generalize to a test set without this word. A good model should be able to generalize

## Page 81

OverfittingThis movie drew me in, and it'll do the same to you.81X1 = "this"X2 = "movieX3 = "hated"I can't tell you how much I hated this movie. It sucked.X5 = "the same to you"X7 = "tell you how much"X4 = "drew me in"+-Useful or harmless features4gram features that just "memorize" training set and might cause problems

## Page 82

Overfitting4-gram model on tiny data will just memorize the dataâ—¦100% accuracy on the training setBut it will be surprised by the novel 4-grams in the test dataâ—¦Low accuracy on test setModels that are too powerful can overfitthe dataâ—¦Fitting the details of the training data so exactly that the model doesn't generalize well to the test setâ—¦How to avoid overfitting?â—¦Regularization in logistic regression â—¦Dropout in neural networks82

## Page 83

RegularizationA solution for overfittingAdd a regularization term R(Î¸) to the loss function (for now written as maximizing logprobrather than minimizing loss) Idea: choose an R(Î¸)that penalizes large weightsâ—¦fitting the data well with lots of big weights not as good as fitting the data a little less well, with small weights14CHAPTER5â€¢LOGISTICREGRESSIONdata to the unseen test set, but a model that overï¬ts will have poor generalization.To avoid overï¬tting, a newregularizationtermR(q)is added to the objectiveregularizationfunction in Eq.5.13, resulting in the following objective for a batch ofmexam-ples (slightly rewritten from Eq.5.13to be maximizing log probability rather thanminimizing loss, and removing the1mterm which doesnâ€™t affect the argmax):Ë†q=argmaxqmXi=1logP(y(i)|x(i)) aR(q)(5.22)The new regularization termR(q)is used to penalize large weights. Thus a settingof the weights that matches the training data perfectlyâ€” but uses many weights withhigh values to do soâ€”will be penalized more than a setting that matches the data alittle less well, but does so using smaller weights. There are two common ways tocompute this regularization termR(q).L2 regularizationis a quadratic function ofL2regularizationthe weight values, named because it uses the (square of the) L2 norm of the weightvalues. The L2 norm,||q||2, is the same as theEuclidean distanceof the vectorqfrom the origin. Ifqconsists ofnweights, then:R(q)=||q||22=nXj=1q2j(5.23)The L2 regularized objective function becomes:Ë†q=argmaxq"mXi=1logP(y(i)|x(i))# anXj=1q2j(5.24)L1 regularizationis a linear function of the weight values, named after the L1 normL1regularization||W||1, the sum of the absolute values of the weights, orManhattan distance(theManhattan distance is the distance youâ€™d have to walk between two points in a citywith a street grid like New York):R(q)=||q||1=nXi=1|qi|(5.25)The L1 regularized objective function becomes:Ë†q=argmaxq"mX1=ilogP(y(i)|x(i))# anXj=1|qj|(5.26)These kinds of regularization come from statistics, where L1 regularization is calledlasso regression(Tibshirani, 1996)and L2 regularization is calledridge regression,lassoridgeand both are commonly used in language processing. L2 regularization is easier tooptimize because of its simple derivative (the derivative ofq2is just 2q), whileL1 regularization is more complex (the derivative of|q|is non-continuous at zero).But where L2 prefers weight vectors with many small weights, L1 prefers sparsesolutions with some larger weights but many more weights set to zero. Thus L1regularization leads to much sparser weight vectors, that is, far fewer features.Both L1 and L2 regularization have Bayesian interpretations as constraints onthe prior of how weights should look. L1 regularization can be viewed as a Laplaceprior on the weights. L2 regularization corresponds to assuming that weights are

## Page 84

L2 Regularization (= ridge regression)The sum of the squares of the weightsThe name is because this is the (square of the)        L2 norm||Î¸||2, = Euclidean distance of Î¸ to the origin.L2 regularized objective function:14CHAPTER5â€¢LOGISTICREGRESSIONdata to the unseen test set, but a model that overï¬ts will have poor generalization.To avoid overï¬tting, a newregularizationtermR(q)is added to the objectiveregularizationfunction in Eq.5.13, resulting in the following objective for a batch ofmexam-ples (slightly rewritten from Eq.5.13to be maximizing log probability rather thanminimizing loss, and removing the1mterm which doesnâ€™t affect the argmax):Ë†q=argmaxqmXi=1logP(y(i)|x(i)) aR(q)(5.22)The new regularization termR(q)is used to penalize large weights. Thus a settingof the weights that matches the training data perfectlyâ€” but uses many weights withhigh values to do soâ€”will be penalized more than a setting that matches the data alittle less well, but does so using smaller weights. There are two common ways tocompute this regularization termR(q).L2 regularizationis a quadratic function ofL2regularizationthe weight values, named because it uses the (square of the) L2 norm of the weightvalues. The L2 norm,||q||2, is the same as theEuclidean distanceof the vectorqfrom the origin. Ifqconsists ofnweights, then:R(q)=||q||22=nXj=1q2j(5.23)The L2 regularized objective function becomes:Ë†q=argmaxq"mXi=1logP(y(i)|x(i))# anXj=1q2j(5.24)L1 regularizationis a linear function of the weight values, named after the L1 normL1regularization||W||1, the sum of the absolute values of the weights, orManhattan distance(theManhattan distance is the distance youâ€™d have to walk between two points in a citywith a street grid like New York):R(q)=||q||1=nXi=1|qi|(5.25)The L1 regularized objective function becomes:Ë†q=argmaxq"mX1=ilogP(y(i)|x(i))# anXj=1|qj|(5.26)These kinds of regularization come from statistics, where L1 regularization is calledlasso regression(Tibshirani, 1996)and L2 regularization is calledridge regression,lassoridgeand both are commonly used in language processing. L2 regularization is easier tooptimize because of its simple derivative (the derivative ofq2is just 2q), whileL1 regularization is more complex (the derivative of|q|is non-continuous at zero).But where L2 prefers weight vectors with many small weights, L1 prefers sparsesolutions with some larger weights but many more weights set to zero. Thus L1regularization leads to much sparser weight vectors, that is, far fewer features.Both L1 and L2 regularization have Bayesian interpretations as constraints onthe prior of how weights should look. L1 regularization can be viewed as a Laplaceprior on the weights. L2 regularization corresponds to assuming that weights are14CHAPTER5â€¢LOGISTICREGRESSIONdata to the unseen test set, but a model that overï¬ts will have poor generalization.To avoid overï¬tting, a newregularizationtermR(q)is added to the objectiveregularizationfunction in Eq.5.13, resulting in the following objective for a batch ofmexam-ples (slightly rewritten from Eq.5.13to be maximizing log probability rather thanminimizing loss, and removing the1mterm which doesnâ€™t affect the argmax):Ë†q=argmaxqmXi=1logP(y(i)|x(i)) aR(q)(5.22)The new regularization termR(q)is used to penalize large weights. Thus a settingof the weights that matches the training data perfectlyâ€” but uses many weights withhigh values to do soâ€”will be penalized more than a setting that matches the data alittle less well, but does so using smaller weights. There are two common ways tocompute this regularization termR(q).L2 regularizationis a quadratic function ofL2regularizationthe weight values, named because it uses the (square of the) L2 norm of the weightvalues. The L2 norm,||q||2, is the same as theEuclidean distanceof the vectorqfrom the origin. Ifqconsists ofnweights, then:R(q)=||q||22=nXj=1q2j(5.23)The L2 regularized objective function becomes:Ë†q=argmaxq"mXi=1logP(y(i)|x(i))# anXj=1q2j(5.24)L1 regularizationis a linear function of the weight values, named after the L1 normL1regularization||W||1, the sum of the absolute values of the weights, orManhattan distance(theManhattan distance is the distance youâ€™d have to walk between two points in a citywith a street grid like New York):R(q)=||q||1=nXi=1|qi|(5.25)The L1 regularized objective function becomes:Ë†q=argmaxq"mX1=ilogP(y(i)|x(i))# anXj=1|qj|(5.26)These kinds of regularization come from statistics, where L1 regularization is calledlasso regression(Tibshirani, 1996)and L2 regularization is calledridge regression,lassoridgeand both are commonly used in language processing. L2 regularization is easier tooptimize because of its simple derivative (the derivative ofq2is just 2q), whileL1 regularization is more complex (the derivative of|q|is non-continuous at zero).But where L2 prefers weight vectors with many small weights, L1 prefers sparsesolutions with some larger weights but many more weights set to zero. Thus L1regularization leads to much sparser weight vectors, that is, far fewer features.Both L1 and L2 regularization have Bayesian interpretations as constraints onthe prior of how weights should look. L1 regularization can be viewed as a Laplaceprior on the weights. L2 regularization corresponds to assuming that weights are

## Page 85

L1 Regularization (= lasso regression)The sum of the (absolute value of the) weightsNamed after the L1 norm ||W||1, = sum of the absolute values of the weights, = Manhattan distanceL1 regularized objective function:14CHAPTER5â€¢LOGISTICREGRESSIONdata to the unseen test set, but a model that overï¬ts will have poor generalization.To avoid overï¬tting, a newregularizationtermR(q)is added to the objectiveregularizationfunction in Eq.5.13, resulting in the following objective for a batch ofmexam-ples (slightly rewritten from Eq.5.13to be maximizing log probability rather thanminimizing loss, and removing the1mterm which doesnâ€™t affect the argmax):Ë†q=argmaxqmXi=1logP(y(i)|x(i)) aR(q)(5.22)The new regularization termR(q)is used to penalize large weights. Thus a settingof the weights that matches the training data perfectlyâ€” but uses many weights withhigh values to do soâ€”will be penalized more than a setting that matches the data alittle less well, but does so using smaller weights. There are two common ways tocompute this regularization termR(q).L2 regularizationis a quadratic function ofL2regularizationthe weight values, named because it uses the (square of the) L2 norm of the weightvalues. The L2 norm,||q||2, is the same as theEuclidean distanceof the vectorqfrom the origin. Ifqconsists ofnweights, then:R(q)=||q||22=nXj=1q2j(5.23)The L2 regularized objective function becomes:Ë†q=argmaxq"mXi=1logP(y(i)|x(i))# anXj=1q2j(5.24)L1 regularizationis a linear function of the weight values, named after the L1 normL1regularization||W||1, the sum of the absolute values of the weights, orManhattan distance(theManhattan distance is the distance youâ€™d have to walk between two points in a citywith a street grid like New York):R(q)=||q||1=nXi=1|qi|(5.25)The L1 regularized objective function becomes:Ë†q=argmaxq"mX1=ilogP(y(i)|x(i))# anXj=1|qj|(5.26)These kinds of regularization come from statistics, where L1 regularization is calledlasso regression(Tibshirani, 1996)and L2 regularization is calledridge regression,lassoridgeand both are commonly used in language processing. L2 regularization is easier tooptimize because of its simple derivative (the derivative ofq2is just 2q), whileL1 regularization is more complex (the derivative of|q|is non-continuous at zero).But where L2 prefers weight vectors with many small weights, L1 prefers sparsesolutions with some larger weights but many more weights set to zero. Thus L1regularization leads to much sparser weight vectors, that is, far fewer features.Both L1 and L2 regularization have Bayesian interpretations as constraints onthe prior of how weights should look. L1 regularization can be viewed as a Laplaceprior on the weights. L2 regularization corresponds to assuming that weights are14CHAPTER5â€¢LOGISTICREGRESSIONdata to the unseen test set, but a model that overï¬ts will have poor generalization.To avoid overï¬tting, a newregularizationtermR(q)is added to the objectiveregularizationfunction in Eq.5.13, resulting in the following objective for a batch ofmexam-ples (slightly rewritten from Eq.5.13to be maximizing log probability rather thanminimizing loss, and removing the1mterm which doesnâ€™t affect the argmax):Ë†q=argmaxqmXi=1logP(y(i)|x(i)) aR(q)(5.22)The new regularization termR(q)is used to penalize large weights. Thus a settingof the weights that matches the training data perfectlyâ€” but uses many weights withhigh values to do soâ€”will be penalized more than a setting that matches the data alittle less well, but does so using smaller weights. There are two common ways tocompute this regularization termR(q).L2 regularizationis a quadratic function ofL2regularizationthe weight values, named because it uses the (square of the) L2 norm of the weightvalues. The L2 norm,||q||2, is the same as theEuclidean distanceof the vectorqfrom the origin. Ifqconsists ofnweights, then:R(q)=||q||22=nXj=1q2j(5.23)The L2 regularized objective function becomes:Ë†q=argmaxq"mXi=1logP(y(i)|x(i))# anXj=1q2j(5.24)L1 regularizationis a linear function of the weight values, named after the L1 normL1regularization||W||1, the sum of the absolute values of the weights, orManhattan distance(theManhattan distance is the distance youâ€™d have to walk between two points in a citywith a street grid like New York):R(q)=||q||1=nXi=1|qi|(5.25)The L1 regularized objective function becomes:Ë†q=argmaxq"mX1=ilogP(y(i)|x(i))# anXj=1|qj|(5.26)These kinds of regularization come from statistics, where L1 regularization is calledlasso regression(Tibshirani, 1996)and L2 regularization is calledridge regression,lassoridgeand both are commonly used in language processing. L2 regularization is easier tooptimize because of its simple derivative (the derivative ofq2is just 2q), whileL1 regularization is more complex (the derivative of|q|is non-continuous at zero).But where L2 prefers weight vectors with many small weights, L1 prefers sparsesolutions with some larger weights but many more weights set to zero. Thus L1regularization leads to much sparser weight vectors, that is, far fewer features.Both L1 and L2 regularization have Bayesian interpretations as constraints onthe prior of how weights should look. L1 regularization can be viewed as a Laplaceprior on the weights. L2 regularization corresponds to assuming that weights are

## Page 86

Logistic RegressionRegularization

## Page 87

Logistic RegressionMultinomial Logistic Regression

## Page 88

Multinomial Logistic RegressionOften we need more than 2 classesâ—¦Positive/negative/neutralâ—¦Parts of speech (noun, verb, adjective, adverb, preposition, etc.)â—¦Classify emergency SMSs into different actionable classesIf >2 classes we use multinomial logistic regression= Softmaxregression= Multinomial logit= (defunct names : Maximum entropy modeling or MaxEntSo "logistic regression" will just mean binary (2 output classes)88

## Page 89

Multinomial Logistic RegressionThe probability of everything must still sum to 1P(positive|doc) + P(negative|doc) + P(neutral|doc) = 1Need a generalization of the sigmoid called the softmaxâ—¦Takes a vector z = [z1, z2, ..., zk] of k arbitrary values â—¦Outputs a probability distributionâ—¦each value in the range [0,1]â—¦all the values summing to 189

## Page 90

The softmaxfunctionTurns a vector z = [z1, z2, ... , zk]of k arbitrary values into probabilities 
905.6â€¢MULTINOMIAL LOGISTIC REGRESSION15distributed according to a Gaussian distribution with meanÂµ=0. In a Gaussianor normal distribution, the further away a value is from the mean, the lower itsprobability (scaled by the variances). By using a Gaussian prior on the weights, weare saying that weights prefer to have the value 0. A Gaussian for a weightqjis1q2ps2jexp  (qj Âµj)22s2j!(5.27)If we multiply each weight by a Gaussian prior on the weight, we are thus maximiz-ing the following constraint:Ë†q=argmaxqMYi=1P(y(i)|x(i))â‡¥nYj=11q2ps2jexp  (qj Âµj)22s2j!(5.28)which in log space, withÂµ=0, and assuming 2s2=1, corresponds toË†q=argmaxqmXi=1logP(y(i)|x(i)) anXj=1q2j(5.29)which is in the same form as Eq.5.24.5.6 Multinomial logistic regressionSometimes we need more than two classes. Perhaps we might want to do 3-waysentiment classiï¬cation (positive, negative, or neutral). Or we could be assigningsome of the labels we will introduce in Chapter 8, like the part of speech of a word(choosing from 10, 30, or even 50 different parts of speech), or the named entitytype of a phrase (choosing from tags like person, location, organization).In such cases we usemultinomial logistic regression, also calledsoftmax re-multinomiallogisticregressiongression(or, historically, themaxent classiï¬er). In multinomial logistic regressionthe targetyis a variable that ranges over more than two classes; we want to knowthe probability ofybeing in each potential classc2C,p(y=c|x).The multinomial logistic classiï¬er uses a generalization of the sigmoid, calledthesoftmaxfunction, to compute the probabilityp(y=c|x). The softmax functionsoftmaxtakes a vectorz=[z1,z2,. . . ,zk]ofkarbitrary values and maps them to a probabilitydistribution, with each value in the range (0,1), and all the values summing to 1.Like the sigmoid, it is an exponential function.For a vectorzof dimensionalityk, the softmax is deï¬ned as:softmax(zi)=exp(zi)Pkj=1exp(zj)1ï£¿iï£¿k(5.30)The softmax of an input vectorz=[z1,z2,. . . ,zk]is thus a vector itself:softmax(z)="exp(z1)Pki=1exp(zi),exp(z2)Pki=1exp(zi),. . . ,exp(zk)Pki=1exp(zi)#(5.31)5.6â€¢MULTINOMIAL LOGISTIC REGRESSION15distributed according to a Gaussian distribution with meanÂµ=0. In a Gaussianor normal distribution, the further away a value is from the mean, the lower itsprobability (scaled by the variances). By using a Gaussian prior on the weights, weare saying that weights prefer to have the value 0. A Gaussian for a weightqjis1q2ps2jexp  (qj Âµj)22s2j!(5.27)If we multiply each weight by a Gaussian prior on the weight, we are thus maximiz-ing the following constraint:Ë†q=argmaxqMYi=1P(y(i)|x(i))â‡¥nYj=11q2ps2jexp  (qj Âµj)22s2j!(5.28)which in log space, withÂµ=0, and assuming 2s2=1, corresponds toË†q=argmaxqmXi=1logP(y(i)|x(i)) anXj=1q2j(5.29)which is in the same form as Eq.5.24.5.6 Multinomial logistic regressionSometimes we need more than two classes. Perhaps we might want to do 3-waysentiment classiï¬cation (positive, negative, or neutral). Or we could be assigningsome of the labels we will introduce in Chapter 8, like the part of speech of a word(choosing from 10, 30, or even 50 different parts of speech), or the named entitytype of a phrase (choosing from tags like person, location, organization).In such cases we usemultinomial logistic regression, also calledsoftmax re-multinomiallogisticregressiongression(or, historically, themaxent classiï¬er). In multinomial logistic regressionthe targetyis a variable that ranges over more than two classes; we want to knowthe probability ofybeing in each potential classc2C,p(y=c|x).The multinomial logistic classiï¬er uses a generalization of the sigmoid, calledthesoftmaxfunction, to compute the probabilityp(y=c|x). The softmax functionsoftmaxtakes a vectorz=[z1,z2,. . . ,zk]ofkarbitrary values and maps them to a probabilitydistribution, with each value in the range (0,1), and all the values summing to 1.Like the sigmoid, it is an exponential function.For a vectorzof dimensionalityk, the softmax is deï¬ned as:softmax(zi)=exp(zi)Pkj=1exp(zj)1ï£¿iï£¿k(5.30)The softmax of an input vectorz=[z1,z2,. . . ,zk]is thus a vector itself:softmax(z)="exp(z1)Pki=1exp(zi),exp(z2)Pki=1exp(zi),. . . ,exp(zk)Pki=1exp(zi)#(5.31)5.6â€¢MULTINOMIAL LOGISTIC REGRESSION15For a vectorzof dimensionalityk, the softmax is deï¬ned as:softmax(zi)=eziPkj=1ezj1ï£¿iï£¿k(5.32)The softmax of an input vectorz=[z1,z2,. . . ,zk]is thus a vector itself:softmax(z)="ez1Pki=1ezi,ez2Pki=1ezi,. . . ,ezkPki=1ezi#(5.33)The denominatorPki=1eziis used to normalize all the values into probabilities.Thus for example given a vector:z=[0.6,1.1, 1.5,1.2,3.2, 1.1]the result softmax(z) is[0.055,0.090,0.0067,0.10,0.74,0.010]Again like the sigmoid, the input to the softmax will be the dot product betweena weight vectorwand an input vectorx(plus a bias). But now weâ€™ll need separateweight vectors (and bias) for each of theKclasses.p(y=c|x)=ewcÂ·x+bckXj=1ewjÂ·x+bj(5.34)Like the sigmoid, the softmax has the property of squashing values toward 0 or1. thus if one of the inputs is larger than the others, will tend to push its probabilitytoward 1, and suppress the probabilities of the smaller inputs.5.6.1 Features in Multinomial Logistic RegressionFor multiclass classiï¬cation the input features need to be a function of both theobservationxand the candidate output classc. Thus instead of the notationxi,fiorfi(x), when weâ€™re discussing features we will use the notationfi(c,x), meaningfeatureifor a particular classcfor a given observationx.In binary classiï¬cation, a positive weight on a feature pointed toward y=1 anda negative weight toward y=0... but in multiclass a feature could be evidence for oragainst an individual class.Letâ€™s look at some sample features for a few NLP tasks to help understand thisperhaps unintuitive use of features that are functions of both the observationxandthe classc,Suppose we are doing text classiï¬cation, and instead of binary classiï¬cation ourtask is to assign one of the 3 classes+, , or 0 (neutral) to a document. Now afeature related to exclamation marks might have a negative weight for 0 documents,and a positive weight for+or documents:

## Page 91

The softmaxfunctionâ—¦Turns  a vector z = [z1,z2,...,zk]of k arbitrary values into probabilities 
915.6â€¢MULTINOMIAL LOGISTIC REGRESSION15For a vectorzof dimensionalityk, the softmax is deï¬ned as:softmax(zi)=eziPkj=1ezj1ï£¿iï£¿k(5.32)The softmax of an input vectorz=[z1,z2,. . . ,zk]is thus a vector itself:softmax(z)="ez1Pki=1ezi,ez2Pki=1ezi,. . . ,ezkPki=1ezi#(5.33)The denominatorPki=1eziis used to normalize all the values into probabilities.Thus for example given a vector:z=[0.6,1.1, 1.5,1.2,3.2, 1.1]the result softmax(z) is[0.055,0.090,0.0067,0.10,0.74,0.010]Again like the sigmoid, the input to the softmax will be the dot product betweena weight vectorwand an input vectorx(plus a bias). But now weâ€™ll need separateweight vectors (and bias) for each of theKclasses.p(y=c|x)=ewcÂ·x+bckXj=1ewjÂ·x+bj(5.34)Like the sigmoid, the softmax has the property of squashing values toward 0 or1. thus if one of the inputs is larger than the others, will tend to push its probabilitytoward 1, and suppress the probabilities of the smaller inputs.5.6.1 Features in Multinomial Logistic RegressionFor multiclass classiï¬cation the input features need to be a function of both theobservationxand the candidate output classc. Thus instead of the notationxi,fiorfi(x), when weâ€™re discussing features we will use the notationfi(c,x), meaningfeatureifor a particular classcfor a given observationx.In binary classiï¬cation, a positive weight on a feature pointed toward y=1 anda negative weight toward y=0... but in multiclass a feature could be evidence for oragainst an individual class.Letâ€™s look at some sample features for a few NLP tasks to help understand thisperhaps unintuitive use of features that are functions of both the observationxandthe classc,Suppose we are doing text classiï¬cation, and instead of binary classiï¬cation ourtask is to assign one of the 3 classes+, , or 0 (neutral) to a document. Now afeature related to exclamation marks might have a negative weight for 0 documents,and a positive weight for+or documents:5.6â€¢MULTINOMIAL LOGISTIC REGRESSION15For a vectorzof dimensionalityk, the softmax is deï¬ned as:softmax(zi)=eziPkj=1ezj1ï£¿iï£¿k(5.32)The softmax of an input vectorz=[z1,z2,. . . ,zk]is thus a vector itself:softmax(z)="ez1Pki=1ezi,ez2Pki=1ezi,. . . ,ezkPki=1ezi#(5.33)The denominatorPki=1eziis used to normalize all the values into probabilities.Thus for example given a vector:z=[0.6,1.1, 1.5,1.2,3.2, 1.1]the result softmax(z) is[0.055,0.090,0.0067,0.10,0.74,0.010]Again like the sigmoid, the input to the softmax will be the dot product betweena weight vectorwand an input vectorx(plus a bias). But now weâ€™ll need separateweight vectors (and bias) for each of theKclasses.p(y=c|x)=ewcÂ·x+bckXj=1ewjÂ·x+bj(5.34)Like the sigmoid, the softmax has the property of squashing values toward 0 or1. thus if one of the inputs is larger than the others, will tend to push its probabilitytoward 1, and suppress the probabilities of the smaller inputs.5.6.1 Features in Multinomial Logistic RegressionFor multiclass classiï¬cation the input features need to be a function of both theobservationxand the candidate output classc. Thus instead of the notationxi,fiorfi(x), when weâ€™re discussing features we will use the notationfi(c,x), meaningfeatureifor a particular classcfor a given observationx.In binary classiï¬cation, a positive weight on a feature pointed toward y=1 anda negative weight toward y=0... but in multiclass a feature could be evidence for oragainst an individual class.Letâ€™s look at some sample features for a few NLP tasks to help understand thisperhaps unintuitive use of features that are functions of both the observationxandthe classc,Suppose we are doing text classiï¬cation, and instead of binary classiï¬cation ourtask is to assign one of the 3 classes+, , or 0 (neutral) to a document. Now afeature related to exclamation marks might have a negative weight for 0 documents,and a positive weight for+or documents:5.6â€¢MULTINOMIAL LOGISTIC REGRESSION15distributed according to a Gaussian distribution with meanÂµ=0. In a Gaussianor normal distribution, the further away a value is from the mean, the lower itsprobability (scaled by the variances). By using a Gaussian prior on the weights, weare saying that weights prefer to have the value 0. A Gaussian for a weightqjis1q2ps2jexp  (qj Âµj)22s2j!(5.27)If we multiply each weight by a Gaussian prior on the weight, we are thus maximiz-ing the following constraint:Ë†q=argmaxqMYi=1P(y(i)|x(i))â‡¥nYj=11q2ps2jexp  (qj Âµj)22s2j!(5.28)which in log space, withÂµ=0, and assuming 2s2=1, corresponds toË†q=argmaxqmXi=1logP(y(i)|x(i)) anXj=1q2j(5.29)which is in the same form as Eq.5.24.5.6 Multinomial logistic regressionSometimes we need more than two classes. Perhaps we might want to do 3-waysentiment classiï¬cation (positive, negative, or neutral). Or we could be assigningsome of the labels we will introduce in Chapter 8, like the part of speech of a word(choosing from 10, 30, or even 50 different parts of speech), or the named entitytype of a phrase (choosing from tags like person, location, organization).In such cases we usemultinomial logistic regression, also calledsoftmax re-multinomiallogisticregressiongression(or, historically, themaxent classiï¬er). In multinomial logistic regressionthe targetyis a variable that ranges over more than two classes; we want to knowthe probability ofybeing in each potential classc2C,p(y=c|x).The multinomial logistic classiï¬er uses a generalization of the sigmoid, calledthesoftmaxfunction, to compute the probabilityp(y=c|x). The softmax functionsoftmaxtakes a vectorz=[z1,z2,. . . ,zk]ofkarbitrary values and maps them to a probabilitydistribution, with each value in the range (0,1), and all the values summing to 1.Like the sigmoid, it is an exponential function.For a vectorzof dimensionalityk, the softmax is deï¬ned as:softmax(zi)=exp(zi)Pkj=1exp(zj)1ï£¿iï£¿k(5.30)The softmax of an input vectorz=[z1,z2,. . . ,zk]is thus a vector itself:softmax(z)="exp(z1)Pki=1exp(zi),exp(z2)Pki=1exp(zi),. . . ,exp(zk)Pki=1exp(zi)#(5.31)

## Page 92

Softmaxin multinomial logistic regression16CHAPTER5â€¢LOGISTICREGRESSIONThe denominatorPki=1exp(zi)is used to normalize all the values into probabil-ities. Thus for example given a vector:z=[0.6,1.1, 1.5,1.2,3.2, 1.1]the resulting (rounded) softmax(z) is[0.055,0.090,0.006,0.099,0.74,0.010]Again like the sigmoid, the input to the softmax will be the dot product betweena weight vectorwand an input vectorx(plus a bias). But now weâ€™ll need separateweight vectors (and bias) for each of theKclasses.p(y=c|x)=exp(wcÂ·x+bc)kXj=1exp(wjÂ·x+bj)(5.32)Like the sigmoid, the softmax has the property of squashing values toward 0 or 1.Thus if one of the inputs is larger than the others, it will tend to push its probabilitytoward 1, and suppress the probabilities of the smaller inputs.5.6.1 Features in Multinomial Logistic RegressionFeatures in multinomial logistic regression function similarly to binary logistic re-gression, with one difference that weâ€™ll need separate weight vectors (and biases) foreach of theKclasses. Recall our binary exclamation point featurex5from page79:x5=â‡¢1 if â€œ!â€2doc0 otherwiseIn binary classiï¬cation a positive weightw5on a feature inï¬‚uences the classiï¬ertowardy=1 (positive sentiment) and a negative weight inï¬‚uences it towardy=0(negative sentiment) with the absolute value indicating how important the featureis. For multinominal logistic regression, by contrast, with separate weights for eachclass, a feature can be evidence for or against each individual class.In 3-way multiclass sentiment classiï¬cation, for example, we must assign eachdocument one of the 3 classes+, , or 0 (neutral). Now a feature related to excla-mation marks might have a negative weight for 0 documents, and a positive weightfor+or documents:Feature Deï¬nitionw5,+w5, w5,0f5(x)â‡¢1 if â€œ!â€2doc0 otherwise3.53.1 5.35.6.2 Learning in Multinomial Logistic RegressionThe loss function for multinomial logistic regression generalizes the loss functionfor binary logistic regression from 2 toKclasses. Recall that that the cross-entropyloss for binary logistic regression (repeated from Eq.5.11) is:LCE(Ë†y,y)= logp(y|x)= [ylog Ë†y+(1 y)log(1 Ë†y)](5.33)92Input is still the dot product between weight vector w and input vector xBut now weâ€™ll need separate weight vectors for each of the K classes. 

## Page 93

Features in binary versus multinomial logistic regressionBinary: positive weight Ã y=1  neg weight Ã y=0Multinominal: separate weights for each class:
9316CHAPTER5â€¢LOGISTICREGRESSIONThe denominatorPki=1exp(zi)is used to normalize all the values into probabil-ities. Thus for example given a vector:z=[0.6,1.1, 1.5,1.2,3.2, 1.1]the resulting (rounded) softmax(z) is[0.055,0.090,0.006,0.099,0.74,0.010]Again like the sigmoid, the input to the softmax will be the dot product betweena weight vectorwand an input vectorx(plus a bias). But now weâ€™ll need separateweight vectors (and bias) for each of theKclasses.p(y=c|x)=exp(wcÂ·x+bc)kXj=1exp(wjÂ·x+bj)(5.32)Like the sigmoid, the softmax has the property of squashing values toward 0 or 1.Thus if one of the inputs is larger than the others, it will tend to push its probabilitytoward 1, and suppress the probabilities of the smaller inputs.5.6.1 Features in Multinomial Logistic RegressionFeatures in multinomial logistic regression function similarly to binary logistic re-gression, with one difference that weâ€™ll need separate weight vectors (and biases) foreach of theKclasses. Recall our binary exclamation point featurex5from page79:x5=â‡¢1 if â€œ!â€2doc0 otherwiseIn binary classiï¬cation a positive weightw5on a feature inï¬‚uences the classiï¬ertowardy=1 (positive sentiment) and a negative weight inï¬‚uences it towardy=0(negative sentiment) with the absolute value indicating how important the featureis. For multinominal logistic regression, by contrast, with separate weights for eachclass, a feature can be evidence for or against each individual class.In 3-way multiclass sentiment classiï¬cation, for example, we must assign eachdocument one of the 3 classes+, , or 0 (neutral). Now a feature related to excla-mation marks might have a negative weight for 0 documents, and a positive weightfor+or documents:Feature Deï¬nitionw5,+w5, w5,0f5(x)â‡¢1 if â€œ!â€2doc0 otherwise3.53.1 5.35.6.2 Learning in Multinomial Logistic RegressionThe loss function for multinomial logistic regression generalizes the loss functionfor binary logistic regression from 2 toKclasses. Recall that that the cross-entropyloss for binary logistic regression (repeated from Eq.5.11) is:LCE(Ë†y,y)= logp(y|x)= [ylog Ë†y+(1 y)log(1 Ë†y)](5.33)w5= 3.016CHAPTER5â€¢LOGISTICREGRESSIONThe denominatorPki=1exp(zi)is used to normalize all the values into probabil-ities. Thus for example given a vector:z=[0.6,1.1, 1.5,1.2,3.2, 1.1]the resulting (rounded) softmax(z) is[0.055,0.090,0.006,0.099,0.74,0.010]Again like the sigmoid, the input to the softmax will be the dot product betweena weight vectorwand an input vectorx(plus a bias). But now weâ€™ll need separateweight vectors (and bias) for each of theKclasses.p(y=c|x)=exp(wcÂ·x+bc)kXj=1exp(wjÂ·x+bj)(5.32)Like the sigmoid, the softmax has the property of squashing values toward 0 or 1.Thus if one of the inputs is larger than the others, it will tend to push its probabilitytoward 1, and suppress the probabilities of the smaller inputs.5.6.1 Features in Multinomial Logistic RegressionFeatures in multinomial logistic regression function similarly to binary logistic re-gression, with one difference that weâ€™ll need separate weight vectors (and biases) foreach of theKclasses. Recall our binary exclamation point featurex5from page79:x5=â‡¢1 if â€œ!â€2doc0 otherwiseIn binary classiï¬cation a positive weightw5on a feature inï¬‚uences the classiï¬ertowardy=1 (positive sentiment) and a negative weight inï¬‚uences it towardy=0(negative sentiment) with the absolute value indicating how important the featureis. For multinominal logistic regression, by contrast, with separate weights for eachclass, a feature can be evidence for or against each individual class.In 3-way multiclass sentiment classiï¬cation, for example, we must assign eachdocument one of the 3 classes+, , or 0 (neutral). Now a feature related to excla-mation marks might have a negative weight for 0 documents, and a positive weightfor+or documents:Feature Deï¬nitionw5,+w5, w5,0f5(x)â‡¢1 if â€œ!â€2doc0 otherwise3.53.1 5.35.6.2 Learning in Multinomial Logistic RegressionThe loss function for multinomial logistic regression generalizes the loss functionfor binary logistic regression from 2 toKclasses. Recall that that the cross-entropyloss for binary logistic regression (repeated from Eq.5.11) is:LCE(Ë†y,y)= logp(y|x)= [ylog Ë†y+(1 y)log(1 Ë†y)](5.33)

## Page 94

Logistic RegressionMultinomial Logistic Regression

