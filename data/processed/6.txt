# 6

## Page 1

Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬©2024. All
rights reserved. Draft of January 12, 2025.
CHAPTER
6Vector Semantics and
Embeddings
ËçÉËÄÖÊâÄ‰ª•Âú®È±ºÔºåÂæóÈ±ºËÄåÂøòËçÉNets are for Ô¨Åsh;
Once you get the Ô¨Åsh, you can forget the net.
Ë®ÄËÄÖÊâÄ‰ª•Âú®ÊÑèÔºåÂæóÊÑèËÄåÂøòË®ÄWords are for meaning;
Once you get the meaning, you can forget the words
Â∫ÑÂ≠ê(Zhuangzi), Chapter 26
The asphalt that Los Angeles is famous for occurs mainly on its freeways. But
in the middle of the city is another patch of asphalt, the La Brea tar pits, and this
asphalt preserves millions of fossil bones from the last of the Ice Ages of the Pleis-
tocene Epoch. One of these fossils is the Smilodon , or saber-toothed tiger, instantly
recognizable by its long canines. Five million years ago or so, a completely different
saber-tooth tiger called Thylacosmilus lived
in Argentina and other parts of South Amer-
ica. Thylacosmilus was a marsupial whereas
Smilodon was a placental mammal, but Thy-
lacosmilus had the same long upper canines
and, like Smilodon, had a protective bone
Ô¨Çange on the lower jaw. The similarity of
these two mammals is one of many examples
of parallel or convergent evolution, in which particular contexts or environments
lead to the evolution of very similar structures in different species (Gould, 1980).
The role of context is also important in the similarity of a less biological kind
of organism: the word. Words that occur in similar contexts tend to have similar
meanings . This link between similarity in how words are distributed and similarity
in what they mean is called the distributional hypothesis . The hypothesis wasdistributional
hypothesis
Ô¨Årst formulated in the 1950s by linguists like Joos (1950), Harris (1954), and Firth
(1957), who noticed that words which are synonyms (like oculist andeye-doctor )
tended to occur in the same environment (e.g., near words like eyeorexamined )
with the amount of meaning difference between two words ‚Äúcorresponding roughly
to the amount of difference in their environments‚Äù (Harris, 1954, p. 157).
In this chapter we introduce vector semantics , which instantiates this linguisticvector
semantics
hypothesis by learning representations of the meaning of words, called embeddings , embeddings
directly from their distributions in texts. These representations are used in every nat-
ural language processing application that makes use of meaning, and the static em-
beddings we introduce here underlie the more powerful dynamic or contextualized
embeddings likeBERT that we will see in Chapter 11.
These word representations are also the Ô¨Årst example in this book of repre-
sentation learning , automatically learning useful representations of the input text.representation
learning
Finding such self-supervised ways to learn representations of the input, instead of
creating representations by hand via feature engineering , is an important focus of
NLP research (Bengio et al., 2013).

## Page 2

2CHAPTER 6 ‚Ä¢ V ECTOR SEMANTICS AND EMBEDDINGS
6.1 Lexical Semantics
Let‚Äôs begin by introducing some basic principles of word meaning. How should
we represent the meaning of a word? In the n-gram models of Chapter 3, and in
classical NLP applications, our only representation of a word is as a string of letters,
or an index in a vocabulary list. This representation is not that different from a
tradition in philosophy, perhaps you‚Äôve seen it in introductory logic classes, in which
the meaning of words is represented by just spelling the word with small capital
letters; representing the meaning of ‚Äúdog‚Äù as DOG, and ‚Äúcat‚Äù as CAT, or by using an
apostrophe ( DOG ‚Äô).
Representing the meaning of a word by capitalizing it is a pretty unsatisfactory
model. You might have seen a version of a joke due originally to semanticist Barbara
Partee (Carlson, 1977):
Q: What‚Äôs the meaning of life?
A:LIFE ‚Äô
Surely we can do better than this! After all, we‚Äôll want a model of word meaning
to do all sorts of things for us. It should tell us that some words have similar mean-
ings ( catis similar to dog), others are antonyms ( cold is the opposite of hot), some
have positive connotations ( happy ) while others have negative connotations ( sad). It
should represent the fact that the meanings of buy,sell, and payoffer differing per-
spectives on the same underlying purchasing event. (If I buy something from you,
you‚Äôve probably sold it to me, and I likely paid you.) More generally, a model of
word meaning should allow us to draw inferences to address meaning-related tasks
like question-answering or dialogue.
In this section we summarize some of these desiderata, drawing on results in the
linguistic study of word meaning, which is called lexical semantics ; we‚Äôll return tolexical
semantics
and expand on this list in Appendix G and Chapter 21.
Lemmas and Senses Let‚Äôs start by looking at how one word (we‚Äôll choose mouse )
might be deÔ¨Åned in a dictionary (simpliÔ¨Åed from the online dictionary WordNet):
mouse (N)
1. any of numerous small rodents...
2. a hand-operated device that controls a cursor...
Here the form mouse is the lemma , also called the citation form . The form lemma
citation form mouse would also be the lemma for the word mice ; dictionaries don‚Äôt have separate
deÔ¨Ånitions for inÔ¨Çected forms like mice . Similarly sing is the lemma for sing,sang ,
sung . In many languages the inÔ¨Ånitive form is used as the lemma for the verb, so
Spanish dormir ‚Äúto sleep‚Äù is the lemma for duermes ‚Äúyou sleep‚Äù. The speciÔ¨Åc forms
sung orcarpets orsing orduermes are called wordforms . wordform
As the example above shows, each lemma can have multiple meanings; the
lemma mouse can refer to the rodent or the cursor control device. We call each
of these aspects of the meaning of mouse aword sense . The fact that lemmas can
bepolysemous (have multiple senses) can make interpretation difÔ¨Åcult (is someone
who types ‚Äúmouse info‚Äù into a search engine looking for a pet or a tool?). Chap-
ter 11 and Appendix G will discuss the problem of polysemy, and introduce word
sense disambiguation , the task of determining which sense of a word is being used
in a particular context.
Synonymy One important component of word meaning is the relationship be-
tween word senses. For example when one word has a sense whose meaning is

## Page 3

6.1 ‚Ä¢ L EXICAL SEMANTICS 3
identical to a sense of another word, or nearly identical, we say the two senses of
those two words are synonyms . Synonyms include such pairs as synonym
couch/sofa vomit/throw up Ô¨Ålbert/hazelnut car/automobile
A more formal deÔ¨Ånition of synonymy (between words rather than senses) is that
two words are synonymous if they are substitutable for one another in any sentence
without changing the truth conditions of the sentence, the situations in which the
sentence would be true.
While substitutions between some pairs of words like car/automobile orwa-
ter/H2Oare truth preserving, the words are still not identical in meaning. Indeed,
probably no two words are absolutely identical in meaning. One of the fundamental
tenets of semantics, called the principle of contrast (Girard 1718, Br ¬¥eal 1897, Clarkprinciple of
contrast
1987), states that a difference in linguistic form is always associated with some dif-
ference in meaning. For example, the word H2Ois used in scientiÔ¨Åc contexts and
would be inappropriate in a hiking guide‚Äî water would be more appropriate‚Äî and
this genre difference is part of the meaning of the word. In practice, the word syn-
onym is therefore used to describe a relationship of approximate or rough synonymy.
Word Similarity While words don‚Äôt have many synonyms, most words do have
lots of similar words. Catis not a synonym of dog, but cats anddogs are certainly
similar words. In moving from synonymy to similarity, it will be useful to shift from
talking about relations between word senses (like synonymy) to relations between
words (like similarity). Dealing with words avoids having to commit to a particular
representation of word senses, which will turn out to simplify our task.
The notion of word similarity is very useful in larger semantic tasks. Knowing similarity
how similar two words are can help in computing how similar the meaning of two
phrases or sentences are, a very important component of tasks like question answer-
ing, paraphrasing, and summarization. One way of getting values for word similarity
is to ask humans to judge how similar one word is to another. A number of datasets
have resulted from such experiments. For example the SimLex-999 dataset (Hill
et al., 2015) gives values on a scale from 0 to 10, like the examples below, which
range from near-synonyms ( vanish ,disappear ) to pairs that scarcely seem to have
anything in common ( hole,agreement ):
vanish disappear 9.8
belief impression 5.95
muscle bone 3.65
modest Ô¨Çexible 0.98
hole agreement 0.3
Word Relatedness The meaning of two words can be related in ways other than
similarity. One such class of connections is called word relatedness (Budanitsky relatedness
and Hirst, 2006), also traditionally called word association in psychology. association
Consider the meanings of the words coffee andcup. Coffee is not similar to cup;
they share practically no features (coffee is a plant or a beverage, while a cup is a
manufactured object with a particular shape). But coffee and cup are clearly related;
they are associated by co-participating in an everyday event (the event of drinking
coffee out of a cup). Similarly scalpel andsurgeon are not similar but are related
eventively (a surgeon tends to make use of a scalpel).
One common kind of relatedness between words is if they belong to the same
semantic Ô¨Åeld . A semantic Ô¨Åeld is a set of words which cover a particular semantic semantic Ô¨Åeld
domain and bear structured relations with each other. For example, words might be

## Page 4

4CHAPTER 6 ‚Ä¢ V ECTOR SEMANTICS AND EMBEDDINGS
related by being in the semantic Ô¨Åeld of hospitals ( surgeon ,scalpel ,nurse ,anes-
thetic ,hospital ), restaurants ( waiter ,menu ,plate ,food,chef), or houses ( door ,roof,
kitchen ,family ,bed). Semantic Ô¨Åelds are also related to topic models , like Latent topic models
Dirichlet Allocation ,LDA , which apply unsupervised learning on large sets of texts
to induce sets of associated words from text. Semantic Ô¨Åelds and topic models are
very useful tools for discovering topical structure in documents.
In Appendix G we‚Äôll introduce more relations between senses like hypernymy
orIS-A ,antonymy (opposites) and meronymy (part-whole relations).
Semantic Frames and Roles Closely related to semantic Ô¨Åelds is the idea of a
semantic frame . A semantic frame is a set of words that denote perspectives or semantic frame
participants in a particular type of event. A commercial transaction, for example,
is a kind of event in which one entity trades money to another entity in return for
some good or service, after which the good changes hands or perhaps the service is
performed. This event can be encoded lexically by using verbs like buy(the event
from the perspective of the buyer), sell(from the perspective of the seller), pay
(focusing on the monetary aspect), or nouns like buyer . Frames have semantic roles
(like buyer ,seller ,goods ,money ), and words in a sentence can take on these roles.
Knowing that buyandsellhave this relation makes it possible for a system to
know that a sentence like Sam bought the book from Ling could be paraphrased as
Ling sold the book to Sam , and that Sam has the role of the buyer in the frame and
Ling the seller . Being able to recognize such paraphrases is important for question
answering, and can help in shifting perspective for machine translation.
Connotation Finally, words have affective meanings orconnotations . The word connotations
connotation has different meanings in different Ô¨Åelds, but here we use it to mean the
aspects of a word‚Äôs meaning that are related to a writer or reader‚Äôs emotions, senti-
ment, opinions, or evaluations. For example some words have positive connotations
(wonderful ) while others have negative connotations ( dreary ). Even words whose
meanings are similar in other ways can vary in connotation; consider the difference
in connotations between fake,knockoff ,forgery , on the one hand, and copy ,replica ,
reproduction on the other, or innocent (positive connotation) and naive (negative
connotation). Some words describe positive evaluation ( great ,love) and others neg-
ative evaluation ( terrible ,hate). Positive or negative evaluation language is called
sentiment , as we saw in Chapter 4, and word sentiment plays a role in important sentiment
tasks like sentiment analysis, stance detection, and applications of NLP to the lan-
guage of politics and consumer reviews.
Early work on affective meaning (Osgood et al., 1957) found that words varied
along three important dimensions of affective meaning:
valence: the pleasantness of the stimulus
arousal: the intensity of emotion provoked by the stimulus
dominance: the degree of control exerted by the stimulus
Thus words like happy orsatisÔ¨Åed are high on valence, while unhappy oran-
noyed are low on valence. Excited is high on arousal, while calm is low on arousal.
Controlling is high on dominance, while awed orinÔ¨Çuenced are low on dominance.
Each word is thus represented by three numbers, corresponding to its value on each
of the three dimensions:

## Page 5

6.2 ‚Ä¢ V ECTOR SEMANTICS 5
Valence Arousal Dominance
courageous 8.05 5.5 7.38
music 7.67 5.57 6.5
heartbreak 2.45 5.65 3.58
cub 6.71 3.95 4.24
Osgood et al. (1957) noticed that in using these 3 numbers to represent the
meaning of a word, the model was representing each word as a point in a three-
dimensional space, a vector whose three dimensions corresponded to the word‚Äôs
rating on the three scales. This revolutionary idea that word meaning could be rep-
resented as a point in space (e.g., that part of the meaning of heartbreak can be
represented as the point [2:45;5:65;3:58]) was the Ô¨Årst expression of the vector se-
mantics models that we introduce next.
6.2 Vector Semantics
Vector semantics is the standard way to represent word meaning in NLP, helpingvector
semantics
us model many of the aspects of word meaning we saw in the previous section. The
roots of the model lie in the 1950s when two big ideas converged: Osgood‚Äôs 1957
idea mentioned above to use a point in three-dimensional space to represent the
connotation of a word, and the proposal by linguists like Joos (1950), Harris (1954),
and Firth (1957) to deÔ¨Åne the meaning of a word by its distribution in language
use, meaning its neighboring words or grammatical environments. Their idea was
that two words that occur in very similar distributions (whose neighboring words are
similar) have similar meanings.
For example, suppose you didn‚Äôt know the meaning of the word ongchoi (a re-
cent borrowing from Cantonese) but you see it in the following contexts:
(6.1) Ongchoi is delicious sauteed with garlic.
(6.2) Ongchoi is superb over rice.
(6.3) ...ongchoi leaves with salty sauces...
And suppose that you had seen many of these context words in other contexts:
(6.4) ...spinach sauteed with garlic over rice...
(6.5) ...chard stems and leaves are delicious...
(6.6) ...collard greens and other salty leafy greens
The fact that ongchoi occurs with words like riceandgarlic anddelicious and
salty , as do words like spinach ,chard , and collard greens might suggest that ongchoi
is a leafy green similar to these other leafy greens.1We can do the same thing
computationally by just counting words in the context of ongchoi .
The idea of vector semantics is to represent a word as a point in a multidimen-
sional semantic space that is derived (in ways we‚Äôll see) from the distributions of
word neighbors. Vectors for representing words are called embeddings (although embeddings
the term is sometimes more strictly applied only to dense vectors like word2vec
(Section 6.8), rather than sparse tf-idf or PPMI vectors (Section 6.3-Section 6.6)).
The word ‚Äúembedding‚Äù derives from its mathematical sense as a mapping from one
space or structure to another, although the meaning has shifted; see the end of the
chapter.
1It‚Äôs in fact Ipomoea aquatica , a relative of morning glory sometimes called water spinach in English.

## Page 6

6CHAPTER 6 ‚Ä¢ V ECTOR SEMANTICS AND EMBEDDINGS
goodnicebadworstnot good
wonderfulamazingterriÔ¨Åcdislikeworsevery goodincredibly goodfantasticincredibly badnowyouithatwithbyto‚Äôsareisathan
Figure 6.1 A two-dimensional (t-SNE) projection of embeddings for some words and
phrases, showing that words with similar meanings are nearby in space. The original 60-
dimensional embeddings were trained for sentiment analysis. SimpliÔ¨Åed from Li et al. (2015)
with colors added for explanation.
Fig. 6.1 shows a visualization of embeddings learned for sentiment analysis,
showing the location of selected words projected down from 60-dimensional space
into a two dimensional space. Notice the distinct regions containing positive words,
negative words, and neutral function words.
The Ô¨Åne-grained model of word similarity of vector semantics offers enormous
power to NLP applications. NLP applications like the sentiment classiÔ¨Åers of Chap-
ter 4 or Chapter 5 depend on the same words appearing in the training and test sets.
But by representing words as embeddings, a classiÔ¨Åer can assign sentiment as long
as it sees some words with similar meanings . And as we‚Äôll see, vector semantic
models can be learned automatically from text without supervision.
In this chapter we‚Äôll introduce the two most commonly used models. In the tf-idf
model, an important baseline, the meaning of a word is deÔ¨Åned by a simple function
of the counts of nearby words. We will see that this method results in very long
vectors that are sparse , i.e. mostly zeros (since most words simply never occur in
the context of others). We‚Äôll introduce the word2vec model family for construct-
ing short, dense vectors that have useful semantic properties. We‚Äôll also introduce
thecosine , the standard way to use embeddings to compute semantic similarity , be-
tween two words, two sentences, or two documents, an important tool in practical
applications like question answering, summarization, or automatic essay grading.
6.3 Words and Vectors
‚ÄúThe most important attributes of a vector in 3-space are fLocation, Location, Location g‚Äù
Randall Munroe, https://xkcd.com/2358/
Vector or distributional models of meaning are generally based on a co-occurrence
matrix , a way of representing how often words co-occur. We‚Äôll look at two popular
matrices: the term-document matrix and the term-term matrix.
6.3.1 Vectors and documents
In aterm-document matrix , each row represents a word in the vocabulary and eachterm-document
matrix
column represents a document from some collection of documents. Fig. 6.2 shows a
small selection from a term-document matrix showing the occurrence of four words
in four plays by Shakespeare. Each cell in this matrix represents the number of times

## Page 7

6.3 ‚Ä¢ W ORDS AND VECTORS 7
a particular word (deÔ¨Åned by the row) occurs in a particular document (deÔ¨Åned by
the column). Thus foolappeared 58 times in Twelfth Night .
As You Like It Twelfth Night Julius Caesar Henry V
battle 1 0 7 13
good 114 80 62 89
fool 36 58 1 4
wit 20 15 2 3
Figure 6.2 The term-document matrix for four words in four Shakespeare plays. Each cell
contains the number of times the (row) word occurs in the (column) document.
The term-document matrix of Fig. 6.2 was Ô¨Årst deÔ¨Åned as part of the vector
space model of information retrieval (Salton, 1971). In this model, a document isvector space
model
represented as a count vector, a column in Fig. 6.3.
To review some basic linear algebra, a vector is, at heart, just a list or array of vector
numbers. So As You Like It is represented as the list [1,114,36,20] (the Ô¨Årst column
vector in Fig. 6.3) and Julius Caesar is represented as the list [7,62,1,2] (the third
column vector). A vector space is a collection of vectors, and is characterized by vector space
itsdimension . Vectors in a 3-dimensional vector space have an element for each dimension
dimension of the space. We will loosely refer to a vector in a 4-dimensional space
as a 4-dimensional vector, with one element along each dimension. In the example
in Fig. 6.3, we‚Äôve chosen to make the document vectors of dimension 4, just so they
Ô¨Åt on the page; in real term-document matrices, the document vectors would have
dimensionalityjVj, the vocabulary size.
The ordering of the numbers in a vector space indicates the different dimensions
on which documents vary. The Ô¨Årst dimension for both these vectors corresponds to
the number of times the word battle occurs, and we can compare each dimension,
noting for example that the vectors for As You Like It andTwelfth Night have similar
values (1 and 0, respectively) for the Ô¨Årst dimension.
As You Like It Twelfth Night Julius Caesar Henry V
battle 1 0 7 13
good 114 80 62 89
fool 36 58 1 4
wit 20 15 2 3
Figure 6.3 The term-document matrix for four words in four Shakespeare plays. The red
boxes show that each document is represented as a column vector of length four.
We can think of the vector for a document as a point in jVj-dimensional space;
thus the documents in Fig. 6.3 are points in 4-dimensional space. Since 4-dimensional
spaces are hard to visualize, Fig. 6.4 shows a visualization in two dimensions; we‚Äôve
arbitrarily chosen the dimensions corresponding to the words battle andfool.
Term-document matrices were originally deÔ¨Åned as a means of Ô¨Ånding similar
documents for the task of document information retrieval . Two documents that are
similar will tend to have similar words, and if two documents have similar words
their column vectors will tend to be similar. The vectors for the comedies As You
Like It [1,114,36,20] and Twelfth Night [0,80,58,15] look a lot more like each other
(more fools and wit than battles) than they look like Julius Caesar [7,62,1,2] or
Henry V [13,89,4,3]. This is clear with the raw numbers; in the Ô¨Årst dimension
(battle) the comedies have low numbers and the others have high numbers, and we
can see it visually in Fig. 6.4; we‚Äôll see very shortly how to quantify this intuition
more formally.

## Page 8

8CHAPTER 6 ‚Ä¢ V ECTOR SEMANTICS AND EMBEDDINGS
51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540
354045505560
Figure 6.4 A spatial visualization of the document vectors for the four Shakespeare play
documents, showing just two of the dimensions, corresponding to the words battle andfool.
The comedies have high values for the fooldimension and low values for the battle dimension.
A real term-document matrix, of course, wouldn‚Äôt just have 4 rows and columns,
let alone 2. More generally, the term-document matrix has jVjrows (one for each
word type in the vocabulary) and Dcolumns (one for each document in the collec-
tion); as we‚Äôll see, vocabulary sizes are generally in the tens of thousands, and the
number of documents can be enormous (think about all the pages on the web).
Information retrieval (IR) is the task of Ô¨Ånding the document dfrom the Dinformation
retrieval
documents in some collection that best matches a query q. For IR we‚Äôll therefore also
represent a query by a vector, also of length jVj, and we‚Äôll need a way to compare
two vectors to Ô¨Ånd how similar they are. (Doing IR will also require efÔ¨Åcient ways
to store and manipulate these vectors by making use of the convenient fact that these
vectors are sparse, i.e., mostly zeros).
Later in the chapter we‚Äôll introduce some of the components of this vector com-
parison process: the tf-idf term weighting, and the cosine similarity metric.
6.3.2 Words as vectors: document dimensions
We‚Äôve seen that documents can be represented as vectors in a vector space. But
vector semantics can also be used to represent the meaning of words . We do this
by associating each word with a word vector‚Äî a row vector rather than a column row vector
vector, hence with different dimensions, as shown in Fig. 6.5. The four dimensions
of the vector for fool, [36,58,1,4], correspond to the four Shakespeare plays. Word
counts in the same four dimensions are used to form the vectors for the other 3
words: wit, [20,15,2,3]; battle , [1,0,7,13]; and good [114,80,62,89].
As You Like It Twelfth Night Julius Caesar Henry V
battle 1 0 7 13
good 114 80 62 89
fool 36 58 1 4
wit 20 15 2 3
Figure 6.5 The term-document matrix for four words in four Shakespeare plays. The red
boxes show that each word is represented as a row vector of length four.
For documents, we saw that similar documents had similar vectors, because sim-
ilar documents tend to have similar words. This same principle applies to words:
similar words have similar vectors because they tend to occur in similar documents.
The term-document matrix thus lets us represent the meaning of a word by the doc-
uments it tends to occur in.

## Page 9

6.3 ‚Ä¢ W ORDS AND VECTORS 9
6.3.3 Words as vectors: word dimensions
An alternative to using the term-document matrix to represent words as vectors of
document counts, is to use the term-term matrix , also called the word-word ma-
trixor the term-context matrix , in which the columns are labeled by words ratherword-word
matrix
than documents. This matrix is thus of dimensionality jVjjVjand each cell records
the number of times the row (target) word and the column (context) word co-occur
in some context in some training corpus. The context could be the document, in
which case the cell represents the number of times the two words appear in the same
document. It is most common, however, to use smaller contexts, generally a win-
dow around the word, for example of 4 words to the left and 4 words to the right,
in which case the cell represents the number of times (in some training corpus) the
column word occurs in such a 4 word window around the row word. Here are four
examples of words in their windows:
is traditionally followed by cherry pie, a traditional dessert
often mixed, such as strawberry rhubarb pie. Apple pie
computer peripherals and personal digital assistants. These devices usually
a computer. This includes information available on the internet
If we then take every occurrence of each word (say strawberry ) and count the
context words around it, we get a word-word co-occurrence matrix. Fig. 6.6 shows a
simpliÔ¨Åed subset of the word-word co-occurrence matrix for these four words com-
puted from the Wikipedia corpus (Davies, 2015).
aardvark ... computer data result pie sugar ...
cherry 0 ... 2 8 9 442 25 ...
strawberry 0 ... 0 0 1 60 19 ...
digital 0 ... 1670 1683 85 5 4 ...
information 0 ... 3325 3982 378 5 13 ...
Figure 6.6 Co-occurrence vectors for four words in the Wikipedia corpus, showing six of
the dimensions (hand-picked for pedagogical purposes). The vector for digital is outlined in
red. Note that a real vector would have vastly more dimensions and thus be much sparser.
Note in Fig. 6.6 that the two words cherry andstrawberry are more similar to
each other (both pieandsugar tend to occur in their window) than they are to other
words like digital ; conversely, digital andinformation are more similar to each other
than, say, to strawberry . Fig. 6.7 shows a spatial visualization.
100020003000400010002000digital [1683,1670]computer datainformation [3982,3325] 30004000
Figure 6.7 A spatial visualization of word vectors for digital andinformation , showing just
two of the dimensions, corresponding to the words data andcomputer .
Note thatjVj, the dimensionality of the vector, is generally the size of the vo-
cabulary, often between 10,000 and 50,000 words (using the most frequent words

## Page 10

10 CHAPTER 6 ‚Ä¢ V ECTOR SEMANTICS AND EMBEDDINGS
in the training corpus; keeping words after about the most frequent 50,000 or so is
generally not helpful). Since most of these numbers are zero these are sparse vector
representations; there are efÔ¨Åcient algorithms for storing and computing with sparse
matrices.
Now that we have some intuitions, let‚Äôs move on to examine the details of com-
puting word similarity. Afterwards we‚Äôll discuss methods for weighting cells.
6.4 Cosine for measuring similarity
To measure similarity between two target words vandw, we need a metric that
takes two vectors (of the same dimensionality, either both with words as dimensions,
hence of lengthjVj, or both with documents as dimensions, of length jDj) and gives
a measure of their similarity. By far the most common similarity metric is the cosine
of the angle between the vectors.
The cosine‚Äîlike most measures for vector similarity used in NLP‚Äîis based on
thedot product operator from linear algebra, also called the inner product : dot product
inner product
dot product (v;w) =vw=NX
i=1viwi=v1w1+v2w2+:::+vNwN (6.7)
The dot product acts as a similarity metric because it will tend to be high just when
the two vectors have large values in the same dimensions. Alternatively, vectors that
have zeros in different dimensions‚Äîorthogonal vectors‚Äîwill have a dot product of
0, representing their strong dissimilarity.
This raw dot product, however, has a problem as a similarity metric: it favors
long vectors. The vector length is deÔ¨Åned as vector length
jvj=vuutNX
i=1v2
i(6.8)
The dot product is higher if a vector is longer, with higher values in each dimension.
More frequent words have longer vectors, since they tend to co-occur with more
words and have higher co-occurrence values with each of them. The raw dot product
thus will be higher for frequent words. But this is a problem; we‚Äôd like a similarity
metric that tells us how similar two words are regardless of their frequency.
We modify the dot product to normalize for the vector length by dividing the
dot product by the lengths of each of the two vectors. This normalized dot product
turns out to be the same as the cosine of the angle between the two vectors, following
from the deÔ¨Ånition of the dot product between two vectors aandb:
ab=jajjbjcosq
ab
jajjbj=cosq (6.9)
Thecosine similarity metric between two vectors vandwthus can be computed as: cosine

## Page 11

6.5 ‚Ä¢ TF-IDF: W EIGHING TERMS IN THE VECTOR 11
cosine (v;w) =vw
jvjjwj=NX
i=1viwi
vuutNX
i=1v2
ivuutNX
i=1w2
i(6.10)
For some applications we pre-normalize each vector, by dividing it by its length,
creating a unit vector of length 1. Thus we could compute a unit vector from aby unit vector
dividing it byjaj. For unit vectors, the dot product is the same as the cosine.
The cosine value ranges from 1 for vectors pointing in the same direction, through
0 for orthogonal vectors, to -1 for vectors pointing in opposite directions. But since
raw frequency values are non-negative, the cosine for these vectors ranges from 0‚Äì1.
Let‚Äôs see how the cosine computes which of the words cherry ordigital is closer
in meaning to information , just using raw counts from the following shortened table:
pie data computer
cherry 442 8 2
digital 5 1683 1670
information 5 3982 3325
cos(cherry ;information ) =4425+83982+23325p
4422+82+22p
52+39822+33252=:018
cos(digital ;information ) =55+16833982+16703325p
52+16832+16702p
52+39822+33252=:996
The model decides that information is way closer to digital than it is to cherry , a
result that seems sensible. Fig. 6.8 shows a visualization.
50010001500200025003000500digitalcherryinformationDimension 1: ‚Äòpie‚Äô
Dimension 2: ‚Äòcomputer‚Äô
Figure 6.8 A (rough) graphical demonstration of cosine similarity, showing vectors for
three words ( cherry ,digital , and information ) in the two dimensional space deÔ¨Åned by counts
of the words computer andpienearby. The Ô¨Ågure doesn‚Äôt show the cosine, but it highlights the
angles; note that the angle between digital andinformation is smaller than the angle between
cherry andinformation . When two vectors are more similar, the cosine is larger but the angle
is smaller; the cosine has its maximum (1) when the angle between two vectors is smallest
(0); the cosine of all other angles is less than 1.
6.5 TF-IDF: Weighing terms in the vector
The co-occurrence matrices above represent each cell by frequencies, either of words
with documents (Fig. 6.5), or words with other words (Fig. 6.6). But raw frequency

## Page 12

12 CHAPTER 6 ‚Ä¢ V ECTOR SEMANTICS AND EMBEDDINGS
is not the best measure of association between words. Raw frequency is very skewed
and not very discriminative. If we want to know what kinds of contexts are shared
bycherry andstrawberry but not by digital andinformation , we‚Äôre not going to get
good discrimination from words like the,it, orthey, which occur frequently with
all sorts of words and aren‚Äôt informative about any particular word. We saw this
also in Fig. 6.3 for the Shakespeare corpus; the dimension for the word good is not
very discriminative between plays; good is simply a frequent word and has roughly
equivalent high frequencies in each of the plays.
It‚Äôs a bit of a paradox. Words that occur nearby frequently (maybe pienearby
cherry ) are more important than words that only appear once or twice. Yet words
that are too frequent‚Äîubiquitous, like theorgood ‚Äî are unimportant. How can we
balance these two conÔ¨Çicting constraints?
There are two common solutions to this problem: in this section we‚Äôll describe
thetf-idf weighting, usually used when the dimensions are documents. In the next
section we introduce the PPMI algorithm (usually used when the dimensions are
words).
Thetf-idf weighting (the ‚Äò-‚Äô here is a hyphen, not a minus sign) is the product
of two terms, each term capturing one of these two intuitions:
The Ô¨Årst is the term frequency (Luhn, 1957): the frequency of the word tin the term frequency
document d. We can just use the raw count as the term frequency:
tft;d=count (t;d) (6.11)
More commonly we squash the raw frequency a bit, by using the log 10of the fre-
quency instead. The intuition is that a word appearing 100 times in a document
doesn‚Äôt make that word 100 times more likely to be relevant to the meaning of the
document. We also need to do something special with counts of 0, since we can‚Äôt
take the log of 0.2
tft;d=(
1+log10count (t;d) if count (t;d)>0
0 otherwise(6.12)
If we use log weighting, terms which occur 0 times in a document would have tf =0,
1 times in a document tf =1+log10(1) =1+0=1, 10 times in a document tf =
1+log10(10) =2, 100 times tf =1+log10(100) =3, 1000 times tf =4, and so on.
The second factor in tf-idf is used to give a higher weight to words that occur
only in a few documents. Terms that are limited to a few documents are useful
for discriminating those documents from the rest of the collection; terms that occur
frequently across the entire collection aren‚Äôt as helpful. The document frequencydocument
frequency
dftof a term tis the number of documents it occurs in. Document frequency is
not the same as the collection frequency of a term, which is the total number of
times the word appears in the whole collection in any document. Consider in the
collection of Shakespeare‚Äôs 37 plays the two words Romeo andaction . The words
have identical collection frequencies (they both occur 113 times in all the plays) but
very different document frequencies, since Romeo only occurs in a single play. If
our goal is to Ô¨Ånd documents about the romantic tribulations of Romeo, the word
Romeo should be highly weighted, but not action :
Collection Frequency Document Frequency
Romeo 113 1
action 113 31
2We can also use this alternative formulation, which we have used in earlier editions: tf t;d=
log10(count (t;d)+1)

## Page 13

6.5 ‚Ä¢ TF-IDF: W EIGHING TERMS IN THE VECTOR 13
We emphasize discriminative words like Romeo via the inverse document fre-
quency oridfterm weight (Sparck Jones, 1972). The idf is deÔ¨Åned using the frac- idf
tionN=dft, where Nis the total number of documents in the collection, and df tis
the number of documents in which term toccurs. The fewer documents in which a
term occurs, the higher this weight. The lowest weight of 1 is assigned to terms that
occur in all the documents. It‚Äôs usually clear what counts as a document: in Shake-
speare we would use a play; when processing a collection of encyclopedia articles
like Wikipedia, the document is a Wikipedia page; in processing newspaper articles,
the document is a single article. Occasionally your corpus might not have appropri-
ate document divisions and you might need to break up the corpus into documents
yourself for the purposes of computing idf.
Because of the large number of documents in many collections, this measure
too is usually squashed with a log function. The resulting deÔ¨Ånition for inverse
document frequency (idf) is thus
idft=log10N
dft
(6.13)
Here are some idf values for some words in the Shakespeare corpus, (along with
the document frequency df values on which they are based) ranging from extremely
informative words which occur in only one play like Romeo , to those that occur in a
few like salad orFalstaff , to those which are very common like foolor so common
as to be completely non-discriminative since they occur in all 37 plays like good or
sweet .3
Word df idf
Romeo 1 1.57
salad 2 1.27
Falstaff 4 0.967
forest 12 0.489
battle 21 0.246
wit 34 0.037
fool 36 0.012
good 37 0
sweet 37 0
The tf-idf weighted value wt;dfor word tin document dthus combines term tf-idf
frequency tf t;d(deÔ¨Åned either by Eq. 6.11 or by Eq. 6.12) with idf from Eq. 6.13:
wt;d=tft;didft (6.14)
Fig. 6.9 applies tf-idf weighting to the Shakespeare term-document matrix in Fig. 6.2,
using the tf equation Eq. 6.12. Note that the tf-idf values for the dimension corre-
sponding to the word good have now all become 0; since this word appears in every
document, the tf-idf weighting leads it to be ignored. Similarly, the word fool, which
appears in 36 out of the 37 plays, has a much lower weight.
The tf-idf weighting is the way for weighting co-occurrence matrices in infor-
mation retrieval, but also plays a role in many other aspects of natural language
processing. It‚Äôs also a great baseline, the simple thing to try Ô¨Årst. We‚Äôll look at other
weightings like PPMI (Positive Pointwise Mutual Information) in Section 6.6.
3Sweet was one of Shakespeare‚Äôs favorite adjectives, a fact probably related to the increased use of
sugar in European recipes around the turn of the 16th century (Jurafsky, 2014, p. 175).

## Page 14

14 CHAPTER 6 ‚Ä¢ V ECTOR SEMANTICS AND EMBEDDINGS
As You Like It Twelfth Night Julius Caesar Henry V
battle 0.246 0 0.454 0.520
good 0 0 0 0
fool 0.030 0.033 0.0012 0.0019
wit 0.085 0.081 0.048 0.054
Figure 6.9 A portion of the tf-idf weighted term-document matrix for four words in Shake-
speare plays, showing a selection of 4 plays, using counts from Fig. 6.2. For example the
0:085 value for witinAs You Like It is the product of tf =1+log10(20) =2:301 and idf =:037.
Note that the idf weighting has eliminated the importance of the ubiquitous word good and
vastly reduced the impact of the almost-ubiquitous word fool.
6.6 Pointwise Mutual Information (PMI)
An alternative weighting function to tf-idf, PPMI (positive pointwise mutual infor-
mation), is used for term-term-matrices, when the vector dimensions correspond to
words rather than documents. PPMI draws on the intuition that the best way to weigh
the association between two words is to ask how much more the two words co-occur
in our corpus than we would have a priori expected them to appear by chance.
Pointwise mutual information (Fano, 1961)4is one of the most important con-pointwise
mutual
informationcepts in NLP. It is a measure of how often two events xandyoccur, compared with
what we would expect if they were independent:
I(x;y) =log2P(x;y)
P(x)P(y)(6.16)
The pointwise mutual information between a target word wand a context word
c(Church and Hanks 1989, Church and Hanks 1990) is then deÔ¨Åned as:
PMI(w;c) =log2P(w;c)
P(w)P(c)(6.17)
The numerator tells us how often we observed the two words together (assuming
we compute probability by using the MLE). The denominator tells us how often
we would expect the two words to co-occur assuming they each occurred indepen-
dently; recall that the probability of two independent events both occurring is just
the product of the probabilities of the two events. Thus, the ratio gives us an esti-
mate of how much more the two words co-occur than we expect by chance. PMI is
a useful tool whenever we need to Ô¨Ånd words that are strongly associated.
PMI values range from negative to positive inÔ¨Ånity. But negative PMI values
(which imply things are co-occurring less often than we would expect by chance)
tend to be unreliable unless our corpora are enormous. To distinguish whether
two words whose individual probability is each 10 6occur together less often than
chance, we would need to be certain that the probability of the two occurring to-
gether is signiÔ¨Åcantly less than 10 12, and this kind of granularity would require an
enormous corpus. Furthermore it‚Äôs not clear whether it‚Äôs even possible to evaluate
such scores of ‚Äòunrelatedness‚Äô with human judgments. For this reason it is more
4PMI is based on the mutual information between two random variables XandY, deÔ¨Åned as:
I(X;Y) =X
xX
yP(x;y)log2P(x;y)
P(x)P(y)(6.15)
In a confusion of terminology, Fano used the phrase mutual information to refer to what we now call
pointwise mutual information and the phrase expectation of the mutual information for what we now call
mutual information

## Page 15

6.6 ‚Ä¢ P OINTWISE MUTUAL INFORMATION (PMI) 15
common to use Positive PMI (called PPMI ) which replaces all negative PMI values PPMI
with zero (Church and Hanks 1989, Dagan et al. 1993, Niwa and Nitta 1994)5:
PPMI (w;c) =max(log2P(w;c)
P(w)P(c);0) (6.18)
More formally, let‚Äôs assume we have a co-occurrence matrix F with W rows (words)
and C columns (contexts), where fi jgives the number of times word wioccurs with
context cj. This can be turned into a PPMI matrix where PPMI i jgives the PPMI
value of word wiwith context cj(which we can also express as PPMI( wi;cj) or
PPMI( w=i;c=j)) as follows:
pi j=fi jPW
i=1PC
j=1fi j;pi=PC
j=1fi jPW
i=1PC
j=1fi j;pj=PW
i=1fi jPW
i=1PC
j=1fi j(6.19)
PPMI i j=max(log2pi j
pipj;0) (6.20)
Let‚Äôs see some PPMI calculations. We‚Äôll use Fig. 6.10, which repeats Fig. 6.6 plus
all the count marginals, and let‚Äôs pretend for ease of calculation that these are the
only words/contexts that matter.
computer data result pie sugar count(w)
cherry 2 8 9 442 25 486
strawberry 0 0 1 60 19 80
digital 1670 1683 85 5 4 3447
information 3325 3982 378 5 13 7703
count(context) 4997 5673 473 512 61 11716
Figure 6.10 Co-occurrence counts for four words in 5 contexts in the Wikipedia corpus,
together with the marginals, pretending for the purpose of this calculation that no other
words/contexts matter.
Thus for example we could compute PPMI(information,data), assuming we pre-
tended that Fig. 6.6 encompassed all the relevant word contexts/dimensions, as fol-
lows:
P(w=information, c=data ) =3982
11716=:3399
P(w=information ) =7703
11716=:6575
P(c=data ) =5673
11716=:4842
PPMI (information,data ) = log2(:3399 =(:6575:4842)) = :0944
Fig. 6.11 shows the joint probabilities computed from the counts in Fig. 6.10, and
Fig. 6.12 shows the PPMI values. Not surprisingly, cherry andstrawberry are highly
associated with both pieandsugar , and data is mildly associated with information .
PMI has the problem of being biased toward infrequent events; very rare words
tend to have very high PMI values. One way to reduce this bias toward low frequency
5Positive PMI also cleanly solves the problem of what to do with zero counts, using 0 to replace the
 ¬•from log (0).

## Page 16

16 CHAPTER 6 ‚Ä¢ V ECTOR SEMANTICS AND EMBEDDINGS
p(w,context) p(w)
computer data result pie sugar p(w)
cherry 0.0002 0.0007 0.0008 0.0377 0.0021 0.0415
strawberry 0.0000 0.0000 0.0001 0.0051 0.0016 0.0068
digital 0.1425 0.1436 0.0073 0.0004 0.0003 0.2942
information 0.2838 0.3399 0.0323 0.0004 0.0011 0.6575
p(context) 0.4265 0.4842 0.0404 0.0437 0.0052
Figure 6.11 Replacing the counts in Fig. 6.6 with joint probabilities, showing the marginals
in the right column and the bottom row.
computer data result pie sugar
cherry 0 0 0 4.38 3.30
strawberry 0 0 0 4.10 5.51
digital 0.18 0.01 0 0 0
information 0.02 0.09 0.28 0 0
Figure 6.12 The PPMI matrix showing the association between words and context words,
computed from the counts in Fig. 6.11. Note that most of the 0 PPMI values are ones that had
a negative PMI; for example PMI( cherry,computer ) = -6.7, meaning that cherry andcomputer
co-occur on Wikipedia less often than we would expect by chance, and with PPMI we replace
negative values by zero.
events is to slightly change the computation for P(c), using a different function Pa(c)
that raises the probability of the context word to the power of a:
PPMI a(w;c) =max(log2P(w;c)
P(w)Pa(c);0) (6.21)
Pa(c) =count (c)a
P
ccount (c)a(6.22)
Levy et al. (2015) found that a setting of a=0:75 improved performance of
embeddings on a wide range of tasks (drawing on a similar weighting used for skip-
grams described below in Eq. 6.32). This works because raising the count to a=
0:75 increases the probability assigned to rare contexts, and hence lowers their PMI
(Pa(c)>P(c)when cis rare).
Another possible solution is Laplace smoothing: Before computing PMI, a small
constant k(values of 0.1-3 are common) is added to each of the counts, shrinking
(discounting) all the non-zero values. The larger the k, the more the non-zero counts
are discounted.
6.7 Applications of the tf-idf or PPMI vector models
In summary, the vector semantics model we‚Äôve described so far represents a target
word as a vector with dimensions corresponding either to the documents in a large
collection (the term-document matrix) or to the counts of words in some neighboring
window (the term-term matrix). The values in each dimension are counts, weighted
by tf-idf (for term-document matrices) or PPMI (for term-term matrices), and the
vectors are sparse (since most values are zero).
The model computes the similarity between two words xandyby taking the
cosine of their tf-idf or PPMI vectors; high cosine, high similarity. This entire model

## Page 17

6.8 ‚Ä¢ W ORD2VEC 17
is sometimes referred to as the tf-idf model or the PPMI model, after the weighting
function.
The tf-idf model of meaning is often used for document functions like deciding
if two documents are similar. We represent a document by taking the vectors of
all the words in the document, and computing the centroid of all those vectors. centroid
The centroid is the multidimensional version of the mean; the centroid of a set of
vectors is a single vector that has the minimum sum of squared distances to each of
the vectors in the set. Given kword vectors w1;w2;:::;wk, the centroid document
vector dis:document
vector
d=w1+w2+:::+wk
k(6.23)
Given two documents, we can then compute their document vectors d1andd2, and
estimate the similarity between the two documents by cos (d1;d2). Document sim-
ilarity is also useful for all sorts of applications; information retrieval, plagiarism
detection, news recommender systems, and even for digital humanities tasks like
comparing different versions of a text to see which are similar to each other.
Either the PPMI model or the tf-idf model can be used to compute word simi-
larity, for tasks like Ô¨Ånding word paraphrases, tracking changes in word meaning, or
automatically discovering meanings of words in different corpora. For example, we
can Ô¨Ånd the 10 most similar words to any target word wby computing the cosines
between wand each of the V 1 other words, sorting, and looking at the top 10.
6.8 Word2vec
In the previous sections we saw how to represent a word as a sparse, long vector with
dimensions corresponding to words in the vocabulary or documents in a collection.
We now introduce a more powerful word representation: embeddings , short dense
vectors. Unlike the vectors we‚Äôve seen so far, embeddings are short , with number
of dimensions dranging from 50-1000, rather than the much larger vocabulary size
jVjor number of documents Dwe‚Äôve seen. These ddimensions don‚Äôt have a clear
interpretation. And the vectors are dense : instead of vector entries being sparse,
mostly-zero counts or functions of counts, the values will be real-valued numbers
that can be negative.
It turns out that dense vectors work better in every NLP task than sparse vectors.
While we don‚Äôt completely understand all the reasons for this, we have some intu-
itions. Representing words as 300-dimensional dense vectors requires our classiÔ¨Åers
to learn far fewer weights than if we represented words as 50,000-dimensional vec-
tors, and the smaller parameter space possibly helps with generalization and avoid-
ing overÔ¨Åtting. Dense vectors may also do a better job of capturing synonymy.
For example, in a sparse vector representation, dimensions for synonyms like car
andautomobile dimension are distinct and unrelated; sparse vectors may thus fail
to capture the similarity between a word with caras a neighbor and a word with
automobile as a neighbor.
In this section we introduce one method for computing embeddings: skip-gram skip-gram
with negative sampling , sometimes called SGNS . The skip-gram algorithm is one SGNS
of two algorithms in a software package called word2vec , and so sometimes the word2vec
algorithm is loosely referred to as word2vec (Mikolov et al. 2013a, Mikolov et al.
2013b). The word2vec methods are fast, efÔ¨Åcient to train, and easily available on-

## Page 18

18 CHAPTER 6 ‚Ä¢ V ECTOR SEMANTICS AND EMBEDDINGS
line with code and pretrained embeddings. Word2vec embeddings are static em-
beddings , meaning that the method learns one Ô¨Åxed embedding for each word in thestatic
embeddings
vocabulary. In Chapter 11 we‚Äôll introduce methods for learning dynamic contextual
embeddings like the popular family of BERT representations, in which the vector
for each word is different in different contexts.
The intuition of word2vec is that instead of counting how often each word woc-
curs near, say, apricot , we‚Äôll instead train a classiÔ¨Åer on a binary prediction task: ‚ÄúIs
word wlikely to show up near apricot ?‚Äù We don‚Äôt actually care about this prediction
task; instead we‚Äôll take the learned classiÔ¨Åer weights as the word embeddings.
The revolutionary intuition here is that we can just use running text as implicitly
supervised training data for such a classiÔ¨Åer; a word cthat occurs near the target
word apricot acts as gold ‚Äòcorrect answer‚Äô to the question ‚ÄúIs word clikely to show
up near apricot ?‚Äù This method, often called self-supervision , avoids the need for self-supervision
any sort of hand-labeled supervision signal. This idea was Ô¨Årst proposed in the task
of neural language modeling, when Bengio et al. (2003) and Collobert et al. (2011)
showed that a neural language model (a neural network that learned to predict the
next word from prior words) could just use the next word in running text as its
supervision signal, and could be used to learn an embedding representation for each
word as part of doing this prediction task.
We‚Äôll see how to do neural networks in the next chapter, but word2vec is a
much simpler model than the neural network language model, in two ways. First,
word2vec simpliÔ¨Åes the task (making it binary classiÔ¨Åcation instead of word pre-
diction). Second, word2vec simpliÔ¨Åes the architecture (training a logistic regression
classiÔ¨Åer instead of a multi-layer neural network with hidden layers that demand
more sophisticated training algorithms). The intuition of skip-gram is:
1. Treat the target word and a neighboring context word as positive examples.
2. Randomly sample other words in the lexicon to get negative samples.
3. Use logistic regression to train a classiÔ¨Åer to distinguish those two cases.
4. Use the learned weights as the embeddings.
6.8.1 The classiÔ¨Åer
Let‚Äôs start by thinking about the classiÔ¨Åcation task, and then turn to how to train.
Imagine a sentence like the following, with a target word apricot , and assume we‚Äôre
using a window of 2 context words:
... lemon, a [tablespoon of apricot jam, a] pinch ...
c1 c2 w c3 c4
Our goal is to train a classiÔ¨Åer such that, given a tuple (w;c)of a target word
wpaired with a candidate context word c(for example ( apricot ,jam), or perhaps
(apricot ,aardvark )) it will return the probability that cis a real context word (true
forjam, false for aardvark ):
P(+jw;c) (6.24)
The probability that word cis not a real context word for wis just 1 minus
Eq. 6.24:
P( jw;c) =1 P(+jw;c) (6.25)
How does the classiÔ¨Åer compute the probability P? The intuition of the skip-
gram model is to base this probability on embedding similarity: a word is likely to

## Page 19

6.8 ‚Ä¢ W ORD2VEC 19
occur near the target if its embedding vector is similar to the target embedding. To
compute similarity between these dense embeddings, we rely on the intuition that
two vectors are similar if they have a high dot product (after all, cosine is just a
normalized dot product). In other words:
Similarity (w;c)cw (6.26)
The dot product cwis not a probability, it‚Äôs just a number ranging from  ¬•to¬•
(since the elements in word2vec embeddings can be negative, the dot product can be
negative). To turn the dot product into a probability, we‚Äôll use the logistic orsigmoid
function s(x), the fundamental core of logistic regression:
s(x) =1
1+exp( x)(6.27)
We model the probability that word cis a real context word for target word was:
P(+jw;c) = s(cw) =1
1+exp( cw)(6.28)
The sigmoid function returns a number between 0 and 1, but to make it a probability
we‚Äôll also need the total probability of the two possible events ( cis a context word,
andcisn‚Äôt a context word) to sum to 1. We thus estimate the probability that word c
is not a real context word for was:
P( jw;c) = 1 P(+jw;c)
=s( cw) =1
1+exp(cw)(6.29)
Equation 6.28 gives us the probability for one word, but there are many context
words in the window. Skip-gram makes the simplifying assumption that all context
words are independent, allowing us to just multiply their probabilities:
P(+jw;c1:L) =LY
i=1s(ciw) (6.30)
logP(+jw;c1:L) =LX
i=1logs(ciw) (6.31)
In summary, skip-gram trains a probabilistic classiÔ¨Åer that, given a test target word
wand its context window of Lwords c1:L, assigns a probability based on how similar
this context window is to the target word. The probability is based on applying the
logistic (sigmoid) function to the dot product of the embeddings of the target word
with each context word. To compute this probability, we just need embeddings for
each target word and context word in the vocabulary.
Fig. 6.13 shows the intuition of the parameters we‚Äôll need. Skip-gram actually
stores two embeddings for each word, one for the word as a target, and one for the
word considered as context. Thus the parameters we need to learn are two matrices
WandC, each containing an embedding for every one of the jVjwords in the
vocabulary V.6Let‚Äôs now turn to learning these embeddings (which is the real goal
of training this classiÔ¨Åer in the Ô¨Årst place).
6In principle the target matrix and the context matrix could use different vocabularies, but we‚Äôll simplify
by assuming one shared vocabulary V.

## Page 20

20 CHAPTER 6 ‚Ä¢ V ECTOR SEMANTICS AND EMBEDDINGS
1WCaardvark
zebrazebraaardvarkapricotapricot|V||V|+12V& =target wordscontext & noisewords‚Ä¶
‚Ä¶1..d‚Ä¶
‚Ä¶
Figure 6.13 The embeddings learned by the skipgram model. The algorithm stores two
embeddings for each word, the target embedding (sometimes called the input embedding)
and the context embedding (sometimes called the output embedding). The parameter qthat
the algorithm learns is thus a matrix of 2 jVjvectors, each of dimension d, formed by concate-
nating two matrices, the target embeddings Wand the context+noise embeddings C.
6.8.2 Learning skip-gram embeddings
The learning algorithm for skip-gram embeddings takes as input a corpus of text,
and a chosen vocabulary size N. It begins by assigning a random embedding vector
for each of the N vocabulary words, and then proceeds to iteratively shift the em-
bedding of each word wto be more like the embeddings of words that occur nearby
in texts, and less like the embeddings of words that don‚Äôt occur nearby. Let‚Äôs start
by considering a single piece of training data:
... lemon, a [tablespoon of apricot jam, a] pinch ...
c1 c2 w c3 c4
This example has a target word w(apricot), and 4 context words in the L=2
window, resulting in 4 positive training instances (on the left below):
positive examples +
w c pos
apricot tablespoon
apricot of
apricot jam
apricot anegative examples -
w c neg w c neg
apricot aardvark apricot seven
apricot my apricot forever
apricot where apricot dear
apricot coaxial apricot if
For training a binary classiÔ¨Åer we also need negative examples. In fact skip-
gram with negative sampling (SGNS) uses more negative examples than positive
examples (with the ratio between them set by a parameter k). So for each of these
(w;cpos)training instances we‚Äôll create knegative samples, each consisting of the
target wplus a ‚Äònoise word‚Äô cneg. A noise word is a random word from the lexicon,
constrained not to be the target word w. The right above shows the setting where
k=2, so we‚Äôll have 2 negative examples in the negative training set  for each
positive example w;cpos.
The noise words are chosen according to their weighted unigram frequency
pa(w), where ais a weight. If we were sampling according to unweighted fre-
quency p(w), it would mean that with unigram probability p(‚Äúthe‚Äù)we would choose
the word theas a noise word, with unigram probability p(‚Äúaardvark ‚Äù)we would
choose aardvark , and so on. But in practice it is common to set a=0:75, i.e. use

## Page 21

6.8 ‚Ä¢ W ORD2VEC 21
the weighting p3
4(w):
Pa(w) =count (w)a
P
w0count (w0)a(6.32)
Setting a=:75 gives better performance because it gives rare noise words slightly
higher probability: for rare words, Pa(w)>P(w). To illustrate this intuition, it
might help to work out the probabilities for an example with a=:75 and two events,
P(a) =0:99 and P(b) =0:01:
Pa(a) =:99:75
:99:75+:01:75=0:97
Pa(b) =:01:75
:99:75+:01:75=0:03 (6.33)
Thus using a=:75 increases the probability of the rare event bfrom 0.01 to 0.03.
Given the set of positive and negative training instances, and an initial set of
embeddings, the goal of the learning algorithm is to adjust those embeddings to
‚Ä¢ Maximize the similarity of the target word, context word pairs (w;cpos)drawn
from the positive examples
‚Ä¢ Minimize the similarity of the (w;cneg)pairs from the negative examples.
If we consider one word/context pair (w;cpos)with its knoise words cneg1:::cnegk,
we can express these two goals as the following loss function Lto be minimized
(hence the ); here the Ô¨Årst term expresses that we want the classiÔ¨Åer to assign the
real context word cposa high probability of being a neighbor, and the second term
expresses that we want to assign each of the noise words cnegia high probability of
being a non-neighbor, all multiplied because we assume independence:
L= log"
P(+jw;cpos)kY
i=1P( jw;cnegi)#
= "
logP(+jw;cpos)+kX
i=1logP( jw;cnegi)#
= "
logP(+jw;cpos)+kX
i=1log 
1 P(+jw;cnegi)#
= "
logs(cposw)+kX
i=1logs( cnegiw)#
(6.34)
That is, we want to maximize the dot product of the word with the actual context
words, and minimize the dot products of the word with the knegative sampled non-
neighbor words.
We minimize this loss function using stochastic gradient descent. Fig. 6.14
shows the intuition of one step of learning.
To get the gradient, we need to take the derivative of Eq. 6.34 with respect to
the different embeddings. It turns out the derivatives are the following (we leave the

## Page 22

22 CHAPTER 6 ‚Ä¢ V ECTOR SEMANTICS AND EMBEDDINGS
WCmove apricot and jam closer,increasing cpos z waardvark
move apricot and matrix apartdecreasing cneg1 z w‚Äú‚Ä¶apricot jam‚Ä¶‚Äùw
zebrazebraaardvarkjamapricot
cposmatrixTolstoymove apricot and Tolstoy apartdecreasing cneg2 z w!cneg1cneg2k=2
Figure 6.14 Intuition of one step of gradient descent. The skip-gram model tries to shift
embeddings so the target embeddings (here for apricot ) are closer to (have a higher dot prod-
uct with) context embeddings for nearby words (here jam) and further from (lower dot product
with) context embeddings for noise words that don‚Äôt occur nearby (here Tolstoy andmatrix ).
proof as an exercise at the end of the chapter):
¬∂L
¬∂cpos= [s(cposw) 1]w (6.35)
¬∂L
¬∂cneg= [s(cnegw)]w (6.36)
¬∂L
¬∂w= [s(cposw) 1]cpos+kX
i=1[s(cnegiw)]cnegi(6.37)
The update equations going from time step ttot+1 in stochastic gradient descent
are thus:
ct+1
pos=ct
pos h[s(ct
poswt) 1]wt(6.38)
ct+1
neg=ct
neg h[s(ct
negwt)]wt(6.39)
wt+1=wt h"
[s(ct
poswt) 1]ct
pos+kX
i=1[s(ct
negiwt)]ct
negi#
(6.40)
Just as in logistic regression, then, the learning algorithm starts with randomly ini-
tialized WandCmatrices, and then walks through the training corpus using gradient
descent to move WandCso as to minimize the loss in Eq. 6.34 by making the up-
dates in (Eq. 6.38)-(Eq. 6.40).
Recall that the skip-gram model learns twoseparate embeddings for each word i:
thetarget embedding wiand the context embedding ci, stored in two matrices, thetarget
embeddingcontext
embedding target matrix Wand the context matrix C. It‚Äôs common to just add them together,
representing word iwith the vector wi+ci. Alternatively we can throw away the C
matrix and just represent each word iby the vector wi.
As with the simple count-based methods like tf-idf, the context window size L
affects the performance of skip-gram embeddings, and experiments often tune the
parameter Lon a devset.

## Page 23

6.9 ‚Ä¢ V ISUALIZING EMBEDDINGS 23
6.8.3 Other kinds of static embeddings
There are many kinds of static embeddings. An extension of word2vec, fasttext fasttext
(Bojanowski et al., 2017), addresses a problem with word2vec as we have presented
it so far: it has no good way to deal with unknown words ‚Äîwords that appear in
a test corpus but were unseen in the training corpus. A related problem is word
sparsity, such as in languages with rich morphology, where some of the many forms
for each noun and verb may only occur rarely. Fasttext deals with these problems
by using subword models, representing each word as itself plus a bag of constituent
n-grams, with special boundary symbols <and>added to each word. For example,
with n=3 the word where would be represented by the sequence <where> plus the
character n-grams:
<wh, whe, her, ere, re>
Then a skipgram embedding is learned for each constituent n-gram, and the word
where is represented by the sum of all of the embeddings of its constituent n-grams.
Unknown words can then be presented only by the sum of the constituent n-grams.
A fasttext open-source library, including pretrained embeddings for 157 languages,
is available at https://fasttext.cc .
Another very widely used static embedding model is GloVe (Pennington et al.,
2014), short for Global Vectors, because the model is based on capturing global
corpus statistics. GloVe is based on ratios of probabilities from the word-word co-
occurrence matrix, combining the intuitions of count-based models like PPMI while
also capturing the linear structures used by methods like word2vec.
It turns out that dense embeddings like word2vec actually have an elegant math-
ematical relationship with sparse embeddings like PPMI, in which word2vec can
be seen as implicitly optimizing a function of a PPMI matrix (Levy and Goldberg,
2014c).
6.9 Visualizing Embeddings
‚ÄúI see well in many dimensions as long as the dimensions are around two.‚Äù
The late economist Martin Shubik
Visualizing embeddings is an important goal in helping understand, apply, and
improve these models of word meaning. But how can we visualize a (for example)
100-dimensional vector?
Rohde, Gonnerman, Plaut Modeling Word Meaning Using Lexical Co-Occurrence
HEADHANDFACE
DOGAMERICA
CATEYEEUROPE
FOOTCHINAFRANCE
CHICAGOARM
FINGER
NOSELEGRUSSIA
MOUSEAFRICA
ATLANTAEARSHOULDERASIA
COW
BULLPUPPYLIONHAWAII
MONTREALTOKYOTOEMOSCOW
TOOTH
NASHVILLEBRAZILWRIST
KITTENANKLE
TURTLE
OYSTER
Figure 8: Multidimensional scaling for three noun classes.WRIST
ANKLE
SHOULDER
ARM
LEG
HAND
FOOT
HEAD
NOSE
FINGER
TOE
FACE
EAR
EYE
TOOTH
DOG
CAT
PUPPY
KITTEN
COW
MOUSE
TURTLE
OYSTER
LION
BULL
CHICAGO
ATLANTA
MONTREAL
NASHVILLE
TOKYOCHINA
RUSSIA
AFRICA
ASIA
EUROPE
AMERICA
BRAZIL
MOSCOW
FRANCEHAWAIIFigure 9: Hierarchical clustering for three noun classes using distances based on vector correlations.
20
The simplest way to visualize the meaning of a word
wembedded in a space is to list the most similar words to
wby sorting the vectors for all words in the vocabulary by
their cosine with the vector for w. For example the 7 closest
words to frogusing a particular embeddings computed with
the GloVe algorithm are: frogs ,toad,litoria ,leptodactyli-
dae,rana,lizard , and eleutherodactylus (Pennington et al.,
2014).
Yet another visualization method is to use a clustering
algorithm to show a hierarchical representation of which
words are similar to others in the embedding space. The
uncaptioned Ô¨Ågure on the left uses hierarchical clustering
of some embedding vectors for nouns as a visualization

## Page 24

24 CHAPTER 6 ‚Ä¢ V ECTOR SEMANTICS AND EMBEDDINGS
method (Rohde et al., 2006).
Probably the most common visualization method, how-
ever, is to project the 100 dimensions of a word down into 2
dimensions. Fig. 6.1 showed one such visualization, as does
Fig. 6.16, using a projection method called t-SNE (van der
Maaten and Hinton, 2008).
6.10 Semantic properties of embeddings
In this section we brieÔ¨Çy summarize some of the semantic properties of embeddings
that have been studied.
Different types of similarity or association: One parameter of vector semantic
models that is relevant to both sparse PPMI vectors and dense word2vec vectors is
the size of the context window used to collect counts. This is generally between 1
and 10 words on each side of the target word (for a total context of 2-20 words).
The choice depends on the goals of the representation. Shorter context windows
tend to lead to representations that are a bit more syntactic, since the information is
coming from immediately nearby words. When the vectors are computed from short
context windows, the most similar words to a target word wtend to be semantically
similar words with the same parts of speech. When vectors are computed from long
context windows, the highest cosine words to a target word wtend to be words that
are topically related but not similar.
For example Levy and Goldberg (2014a) showed that using skip-gram with a
window of2, the most similar words to the word Hogwarts (from the Harry Potter
series) were names of other Ô¨Åctional schools: Sunnydale (from Buffy the Vampire
Slayer ) orEvernight (from a vampire series). With a window of 5, the most similar
words to Hogwarts were other words topically related to the Harry Potter series:
Dumbledore ,Malfoy , and half-blood .
It‚Äôs also often useful to distinguish two kinds of similarity or association between
words (Sch ¬®utze and Pedersen, 1993). Two words have Ô¨Årst-order co-occurrenceÔ¨Årst-order
co-occurrence
(sometimes called syntagmatic association ) if they are typically nearby each other.
Thus wrote is a Ô¨Årst-order associate of book orpoem . Two words have second-order
co-occurrence (sometimes called paradigmatic association ) if they have similarsecond-order
co-occurrence
neighbors. Thus wrote is a second-order associate of words like said orremarked .
Analogy/Relational Similarity: Another semantic property of embeddings is their
ability to capture relational meanings. In an important early vector space model of
cognition, Rumelhart and Abrahamson (1973) proposed the parallelogram modelparallelogram
model
for solving simple analogy problems of the form a is to b as a* is to what? . In
such problems, a system is given a problem like apple:tree::grape:? , i.e., apple is
to tree as grape is to , and must Ô¨Åll in the word vine. In the parallelogram
model, illustrated in Fig. 6.15, the vector from the word apple to the word tree(=#   tree #       apple) is added to the vector for grape (#        grape); the nearest word to that point
is returned.
In early work with sparse embeddings, scholars showed that sparse vector mod-
els of meaning could solve such analogy problems (Turney and Littman, 2005),
but the parallelogram method received more modern attention because of its suc-
cess with word2vec or GloVe vectors (Mikolov et al. 2013c, Levy and Goldberg
2014b, Pennington et al. 2014). For example, the result of the expression#     king 

## Page 25

6.10 ‚Ä¢ S EMANTIC PROPERTIES OF EMBEDDINGS 25
treeapplegrapevine
Figure 6.15 The parallelogram model for analogy problems (Rumelhart and Abrahamson,
1973): the location of#     vine can be found by subtracting#       apple from#   tree and adding#       grape.
#     man+#            woman is a vector close to#         queen. Similarly,#      Paris #           France +#     Italy results
in a vector that is close to#         Rome. The embedding model thus seems to be extract-
ing representations of relations like MALE -FEMALE , or CAPITAL -CITY -OF, or even
COMPARATIVE /SUPERLATIVE , as shown in Fig. 6.16 from GloVe.
(a) (b)
Figure 6.16 Relational properties of the GloVe vector space, shown by projecting vectors onto two dimen-
sions. (a)#     king #     man+#            woman is close to#        queen. (b) offsets seem to capture comparative and superlative
morphology (Pennington et al., 2014).
For a a:b::a:bproblem, meaning the algorithm is given vectors a,b, and
aand must Ô¨Ånd b, the parallelogram method is thus:
ÀÜb=argmin
xdistance (x;b a+a) (6.41)
with some distance function, such as Euclidean distance.
There are some caveats. For example, the closest value returned by the paral-
lelogram algorithm in word2vec or GloVe embedding spaces is usually not in fact
b* but one of the 3 input words or their morphological variants (i.e., cherry:red ::
potato:x returns potato orpotatoes instead of brown ), so these must be explicitly
excluded. Furthermore while embedding spaces perform well if the task involves
frequent words, small distances, and certain relations (like relating countries with
their capitals or verbs/nouns with their inÔ¨Çected forms), the parallelogram method
with embeddings doesn‚Äôt work as well for other relations (Linzen 2016, Gladkova
et al. 2016, Schluter 2018, Ethayarajh et al. 2019a), and indeed Peterson et al. (2020)
argue that the parallelogram method is in general too simple to model the human
cognitive process of forming analogies of this kind.

## Page 26

26 CHAPTER 6 ‚Ä¢ V ECTOR SEMANTICS AND EMBEDDINGS
6.10.1 Embeddings and Historical Semantics
Embeddings can also be a useful tool for studying how meaning changes over time,
by computing multiple embedding spaces, each from texts written in a particular
time period. For example Fig. 6.17 shows a visualization of changes in meaning in
English words over the last two centuries, computed by building separate embedding
spaces for each decade from historical corpora like Google n-grams (Lin et al., 2012)
and the Corpus of Historical American English (Davies, 2012).
CHAPTER 5. DYNAMIC SOCIAL REPRESENTATIONS OF WORD MEANING 79
Figure 5.1: Two-dimensional visualization of semantic change in English using SGNS
vectors (see Section 5.8 for the visualization algorithm). A,T h ew o r d gay shifted
from meaning ‚Äúcheerful‚Äù or ‚Äúfrolicsome‚Äù to referring to homosexuality. A,I nt h ee a r l y
20th century broadcast referred to ‚Äúcasting out seeds‚Äù; with the rise of television and
radio its meaning shifted to ‚Äútransmitting signals‚Äù. C,Awful underwent a process of
pejoration, as it shifted from meaning ‚Äúfull of awe‚Äù to meaning ‚Äúterrible or appalling‚Äù
[212].
that adverbials (e.g., actually )h a v eag e n e r a lt e n d e n c yt ou n d e r g os u b j e c t i Ô¨Å c a t i o n
where they shift from objective statements about the world (e.g., ‚ÄúSorry, the car is
actually broken‚Äù) to subjective statements (e.g., ‚ÄúI can‚Äôt believe he actually did that‚Äù,
indicating surprise/disbelief).
5.2.2 Computational linguistic studies
There are also a number of recent works analyzing semantic change using computational
methods. [ 200] use latent semantic analysis to analyze how word meanings broaden
and narrow over time. [ 113]u s er a wc o - o c c u r r e n c ev e c t o r st op e r f o r man u m b e ro f
historical case-studies on semantic change, and [ 252] perform a similar set of small-
scale case-studies using temporal topic models. [ 87]c o n s t r u c tp o i n t - w i s em u t u a l
information-based embeddings and found that semantic changes uncovered by their
method had reasonable agreement with human judgments. [ 129]a n d[ 119]u s e‚Äú n e u r a l ‚Äù
word-embedding methods to detect linguistic change points. Finally, [ 257]a n a l y z e
historical co-occurrences to test whether synonyms tend to change in similar ways.
Figure 6.17 A t-SNE visualization of the semantic change of 3 words in English using
word2vec vectors. The modern sense of each word, and the grey context words, are com-
puted from the most recent (modern) time-point embedding space. Earlier points are com-
puted from earlier historical embedding spaces. The visualizations show the changes in the
word gayfrom meanings related to ‚Äúcheerful‚Äù or ‚Äúfrolicsome‚Äù to referring to homosexuality,
the development of the modern ‚Äútransmission‚Äù sense of broadcast from its original sense of
sowing seeds, and the pejoration of the word awful as it shifted from meaning ‚Äúfull of awe‚Äù
to meaning ‚Äúterrible or appalling‚Äù (Hamilton et al., 2016).
6.11 Bias and Embeddings
In addition to their ability to learn word meaning from text, embeddings, alas,
also reproduce the implicit biases and stereotypes that were latent in the text. As
the prior section just showed, embeddings can roughly model relational similar-
ity: ‚Äòqueen‚Äô as the closest word to ‚Äòking‚Äô - ‚Äòman‚Äô + ‚Äòwoman‚Äô implies the analogy
man:woman::king:queen . But these same embedding analogies also exhibit gender
stereotypes. For example Bolukbasi et al. (2016) Ô¨Ånd that the closest occupation
to ‚Äòcomputer programmer‚Äô - ‚Äòman‚Äô + ‚Äòwoman‚Äô in word2vec embeddings trained on
news text is ‚Äòhomemaker‚Äô, and that the embeddings similarly suggest the analogy
‚Äòfather‚Äô is to ‚Äòdoctor‚Äô as ‚Äòmother‚Äô is to ‚Äònurse‚Äô. This could result in what Crawford
(2017) and Blodgett et al. (2020) call an allocational harm , when a system allo-allocational
harm
cates resources (jobs or credit) unfairly to different groups. For example algorithms
that use embeddings as part of a search for hiring potential programmers or doctors
might thus incorrectly downweight documents with women‚Äôs names.
It turns out that embeddings don‚Äôt just reÔ¨Çect the statistics of their input, but also
amplify bias; gendered terms become more gendered in embedding space than theybias
ampliÔ¨Åcation
were in the input text statistics (Zhao et al. 2017, Ethayarajh et al. 2019b, Jia et al.
2020), and biases are more exaggerated than in actual labor employment statistics
(Garg et al., 2018).
Embeddings also encode the implicit associations that are a property of human
reasoning. The Implicit Association Test (Greenwald et al., 1998) measures peo-

## Page 27

6.12 ‚Ä¢ E VALUATING VECTOR MODELS 27
ple‚Äôs associations between concepts (like ‚ÄòÔ¨Çowers‚Äô or ‚Äòinsects‚Äô) and attributes (like
‚Äòpleasantness‚Äô and ‚Äòunpleasantness‚Äô) by measuring differences in the latency with
which they label words in the various categories.7Using such methods, people
in the United States have been shown to associate African-American names with
unpleasant words (more than European-American names), male names more with
mathematics and female names with the arts, and old people‚Äôs names with unpleas-
ant words (Greenwald et al. 1998, Nosek et al. 2002a, Nosek et al. 2002b). Caliskan
et al. (2017) replicated all these Ô¨Åndings of implicit associations using GloVe vectors
and cosine similarity instead of human latencies. For example African-American
names like ‚ÄòLeroy‚Äô and ‚ÄòShaniqua‚Äô had a higher GloVe cosine with unpleasant words
while European-American names (‚ÄòBrad‚Äô, ‚ÄòGreg‚Äô, ‚ÄòCourtney‚Äô) had a higher cosine
with pleasant words. These problems with embeddings are an example of a repre-
sentational harm (Crawford 2017, Blodgett et al. 2020), which is a harm caused byrepresentational
harm
a system demeaning or even ignoring some social groups. Any embedding-aware al-
gorithm that made use of word sentiment could thus exacerbate bias against African
Americans.
Recent research focuses on ways to try to remove these kinds of biases, for
example by developing a transformation of the embedding space that removes gen-
der stereotypes but preserves deÔ¨Ånitional gender (Bolukbasi et al. 2016, Zhao et al.
2017) or changing the training procedure (Zhao et al., 2018). However, although
these sorts of debiasing may reduce bias in embeddings, they do not eliminate it debiasing
(Gonen and Goldberg, 2019), and this remains an open problem.
Historical embeddings are also being used to measure biases in the past. Garg
et al. (2018) used embeddings from historical texts to measure the association be-
tween embeddings for occupations and embeddings for names of various ethnici-
ties or genders (for example the relative cosine similarity of women‚Äôs names versus
men‚Äôs to occupation words like ‚Äòlibrarian‚Äô or ‚Äòcarpenter‚Äô) across the 20th century.
They found that the cosines correlate with the empirical historical percentages of
women or ethnic groups in those occupations. Historical embeddings also repli-
cated old surveys of ethnic stereotypes; the tendency of experimental participants in
1933 to associate adjectives like ‚Äòindustrious‚Äô or ‚Äòsuperstitious‚Äô with, e.g., Chinese
ethnicity, correlates with the cosine between Chinese last names and those adjectives
using embeddings trained on 1930s text. They also were able to document historical
gender biases, such as the fact that embeddings for adjectives related to competence
(‚Äòsmart‚Äô, ‚Äòwise‚Äô, ‚Äòthoughtful‚Äô, ‚Äòresourceful‚Äô) had a higher cosine with male than fe-
male words, and showed that this bias has been slowly decreasing since 1960. We
return in later chapters to this question about the role of bias in natural language
processing.
6.12 Evaluating Vector Models
The most important evaluation metric for vector models is extrinsic evaluation on
tasks, i.e., using vectors in an NLP task and seeing whether this improves perfor-
mance over some other model.
7Roughly speaking, if humans associate ‚ÄòÔ¨Çowers‚Äô with ‚Äòpleasantness‚Äô and ‚Äòinsects‚Äô with ‚Äòunpleasant-
ness‚Äô, when they are instructed to push a green button for ‚ÄòÔ¨Çowers‚Äô (daisy, iris, lilac) and ‚Äòpleasant words‚Äô
(love, laughter, pleasure) and a red button for ‚Äòinsects‚Äô (Ô¨Çea, spider, mosquito) and ‚Äòunpleasant words‚Äô
(abuse, hatred, ugly) they are faster than in an incongruous condition where they push a red button for
‚ÄòÔ¨Çowers‚Äô and ‚Äòunpleasant words‚Äô and a green button for ‚Äòinsects‚Äô and ‚Äòpleasant words‚Äô.

## Page 28

28 CHAPTER 6 ‚Ä¢ V ECTOR SEMANTICS AND EMBEDDINGS
Nonetheless it is useful to have intrinsic evaluations. The most common metric
is to test their performance on similarity , computing the correlation between an
algorithm‚Äôs word similarity scores and word similarity ratings assigned by humans.
WordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0
to 10 for 353 noun pairs; for example ( plane ,car) had an average score of 5.77.
SimLex-999 (Hill et al., 2015) is a more complex dataset that quantiÔ¨Åes similarity
(cup, mug ) rather than relatedness ( cup, coffee ), and includes concrete and abstract
adjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions, each
consisting of a target word with 4 additional word choices; the task is to choose
which is the correct synonym, as in the example: Levied is closest in meaning to:
imposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these
datasets present words without context.
Slightly more realistic are intrinsic similarity tasks that include context. The
Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) and the
Word-in-Context (WiC) dataset (Pilehvar and Camacho-Collados, 2019) offer richer
evaluation scenarios. SCWS gives human judgments on 2,003 pairs of words in
their sentential context, while WiC gives target words in two sentential contexts that
are either in the same or different senses; see Appendix G. The semantic textual
similarity task (Agirre et al. 2012, Agirre et al. 2015) evaluates the performance of
sentence-level similarity algorithms, consisting of a set of pairs of sentences, each
pair with human-labeled similarity scores.
Another task used for evaluation is the analogy task, discussed on page 24, where
the system has to solve problems of the form a is to b as a* is to b* , given a, b, anda*
and having to Ô¨Ånd b*(Turney and Littman, 2005). A number of sets of tuples have
been created for this task (Mikolov et al. 2013a, Mikolov et al. 2013c, Gladkova
et al. 2016), covering morphology ( city:cities::child:children ), lexicographic rela-
tions ( leg:table::spout:teapot ) and encyclopedia relations ( Beijing:China::Dublin:Ireland ),
some drawing from the SemEval-2012 Task 2 dataset of 79 different relations (Jur-
gens et al., 2012).
All embedding algorithms suffer from inherent variability. For example because
of randomness in the initialization and the random negative sampling, algorithms
like word2vec may produce different results even from the same dataset, and in-
dividual documents in a collection may strongly impact the resulting embeddings
(Tian et al. 2016, Hellrich and Hahn 2016, Antoniak and Mimno 2018). When em-
beddings are used to study word associations in particular corpora, therefore, it is
best practice to train multiple embeddings with bootstrap sampling over documents
and average the results (Antoniak and Mimno, 2018).
6.13 Summary
‚Ä¢ In vector semantics, a word is modeled as a vector‚Äîa point in high-dimensional
space, also called an embedding . In this chapter we focus on static embed-
dings , where each word is mapped to a Ô¨Åxed embedding.
‚Ä¢ Vector semantic models fall into two classes: sparse anddense . In sparse
models each dimension corresponds to a word in the vocabulary Vand cells
are functions of co-occurrence counts . The term-document matrix has a
row for each word ( term ) in the vocabulary and a column for each document.
Theword-context orterm-term matrix has a row for each (target) word in

## Page 29

BIBLIOGRAPHICAL AND HISTORICAL NOTES 29
the vocabulary and a column for each context term in the vocabulary. Two
sparse weightings are common: the tf-idf weighting which weights each cell
by its term frequency andinverse document frequency , and PPMI (point-
wise positive mutual information), which is most common for word-context
matrices.
‚Ä¢ Dense vector models have dimensionality 50‚Äì1000. Word2vec algorithms
likeskip-gram are a popular way to compute dense embeddings. Skip-gram
trains a logistic regression classiÔ¨Åer to compute the probability that two words
are ‚Äòlikely to occur nearby in text‚Äô. This probability is computed from the dot
product between the embeddings for the two words.
‚Ä¢ Skip-gram uses stochastic gradient descent to train the classiÔ¨Åer, by learning
embeddings that have a high dot product with embeddings of words that occur
nearby and a low dot product with noise words.
‚Ä¢ Other important embedding algorithms include GloVe , a method based on
ratios of word co-occurrence probabilities.
‚Ä¢ Whether using sparse or dense vectors, word and document similarities are
computed by some function of the dot product between vectors. The cosine
of two vectors‚Äîa normalized dot product‚Äîis the most popular such metric.
Bibliographical and Historical Notes
The idea of vector semantics arose out of research in the 1950s in three distinct
Ô¨Åelds: linguistics, psychology, and computer science, each of which contributed a
fundamental aspect of the model.
The idea that meaning is related to the distribution of words in context was
widespread in linguistic theory of the 1950s, among distributionalists like Zellig
Harris, Martin Joos, and J. R. Firth, and semioticians like Thomas Sebeok. As Joos
(1950) put it,
the linguist‚Äôs ‚Äúmeaning‚Äù of a morpheme. . . is by deÔ¨Ånition the set of conditional
probabilities of its occurrence in context with all other morphemes.
The idea that the meaning of a word might be modeled as a point in a multi-
dimensional semantic space came from psychologists like Charles E. Osgood, who
had been studying how people responded to the meaning of words by assigning val-
ues along scales like happy/sad orhard/soft . Osgood et al. (1957) proposed that the
meaning of a word in general could be modeled as a point in a multidimensional
Euclidean space, and that the similarity of meaning between two words could be
modeled as the distance between these points in the space.
A Ô¨Ånal intellectual source in the 1950s and early 1960s was the Ô¨Åeld then called
mechanical indexing , now known as information retrieval . In what became knownmechanical
indexing
as the vector space model for information retrieval (Salton 1971, Sparck Jones
1986), researchers demonstrated new ways to deÔ¨Åne the meaning of words in terms
of vectors (Switzer, 1965), and reÔ¨Åned methods for word similarity based on mea-
sures of statistical association between words like mutual information (Giuliano,
1965) and idf (Sparck Jones, 1972), and showed that the meaning of documents
could be represented in the same vector spaces used for words. Around the same
time, (Cordier, 1965) showed that factor analysis of word association probabilities
could be used to form dense vector representations of words.

## Page 30

30 CHAPTER 6 ‚Ä¢ V ECTOR SEMANTICS AND EMBEDDINGS
Some of the philosophical underpinning of the distributional way of thinking
came from the late writings of the philosopher Wittgenstein, who was skeptical of
the possibility of building a completely formal theory of meaning deÔ¨Ånitions for
each word. Wittgenstein suggested instead that ‚Äúthe meaning of a word is its use in
the language‚Äù (Wittgenstein, 1953, PI 43). That is, instead of using some logical lan-
guage to deÔ¨Åne each word, or drawing on denotations or truth values, Wittgenstein‚Äôs
idea is that we should deÔ¨Åne a word by how it is used by people in speaking and un-
derstanding in their day-to-day interactions, thus preÔ¨Åguring the movement toward
embodied and experiential models in linguistics and NLP (Glenberg and Robertson
2000, Lake and Murphy 2021, Bisk et al. 2020, Bender and Koller 2020).
More distantly related is the idea of deÔ¨Åning words by a vector of discrete fea-
tures, which has roots at least as far back as Descartes and Leibniz (Wierzbicka 1992,
Wierzbicka 1996). By the middle of the 20th century, beginning with the work of
Hjelmslev (Hjelmslev, 1969) (originally 1943) and Ô¨Çeshed out in early models of
generative grammar (Katz and Fodor, 1963), the idea arose of representing mean-
ing with semantic features , symbols that represent some sort of primitive meaning.semantic
feature
For example words like hen,rooster , orchick , have something in common (they all
describe chickens) and something different (their age and sex), representable as:
hen +female, +chicken, +adult
rooster -female, +chicken, +adult
chick +chicken, -adult
The dimensions used by vector models of meaning to deÔ¨Åne words, however, are
only abstractly related to this idea of a small Ô¨Åxed number of hand-built dimensions.
Nonetheless, there has been some attempt to show that certain dimensions of em-
bedding models do contribute some speciÔ¨Åc compositional aspect of meaning like
these early semantic features.
The use of dense vectors to model word meaning, and indeed the term embed-
ding , grew out of the latent semantic indexing (LSI) model (Deerwester et al.,
1988) recast as LSA (latent semantic analysis ) (Deerwester et al., 1990). In LSA
singular value decomposition ‚ÄîSVD ‚Äî is applied to a term-document matrix (each SVD
cell weighted by log frequency and normalized by entropy), and then the Ô¨Årst 300
dimensions are used as the LSA embedding. Singular Value Decomposition (SVD)
is a method for Ô¨Ånding the most important dimensions of a data set, those dimen-
sions along which the data varies the most. LSA was then quickly widely applied:
as a cognitive model Landauer and Dumais (1997), and for tasks like spell checking
(Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and Ju-
rafsky 1998, Bellegarda 2000), morphology induction (Schone and Jurafsky 2000,
Schone and Jurafsky 2001b), multiword expressions (MWEs) (Schone and Jurafsky,
2001a), and essay grading (Rehder et al., 1998). Related models were simultane-
ously developed and applied to word sense disambiguation by Sch ¬®utze (1992). LSA
also led to the earliest use of embeddings to represent words in a probabilistic clas-
siÔ¨Åer, in the logistic regression document router of Sch ¬®utze et al. (1995). The idea of
SVD on the term-term matrix (rather than the term-document matrix) as a model of
meaning for NLP was proposed soon after LSA by Sch ¬®utze (1992). Sch ¬®utze applied
the low-rank (97-dimensional) embeddings produced by SVD to the task of word
sense disambiguation, analyzed the resulting semantic space, and also suggested
possible techniques like dropping high-order dimensions. See Sch ¬®utze (1997).
A number of alternative matrix models followed on from the early SVD work,
including Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Latent
Dirichlet Allocation (LDA) (Blei et al., 2003), and Non-negative Matrix Factoriza-

## Page 31

EXERCISES 31
tion (NMF) (Lee and Seung, 1999).
The LSA community seems to have Ô¨Årst used the word ‚Äúembedding‚Äù in Landauer
et al. (1997), in a variant of its mathematical meaning as a mapping from one space
or mathematical structure to another. In LSA, the word embedding seems to have
described the mapping from the space of sparse count vectors to the latent space of
SVD dense vectors. Although the word thus originally meant the mapping from one
space to another, it has metonymically shifted to mean the resulting dense vector in
the latent space, and it is in this sense that we currently use the word.
By the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that
neural language models could also be used to develop embeddings as part of the task
of word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and
Collobert et al. (2011) then demonstrated that embeddings could be used to represent
word meanings for a number of NLP tasks. Turian et al. (2010) compared the value
of different kinds of embeddings for different NLP tasks. Mikolov et al. (2011)
showed that recurrent neural nets could be used as language models. The idea of
simplifying the hidden layer of these neural net language models to create the skip-
gram (and also CBOW) algorithms was proposed by Mikolov et al. (2013a). The
negative sampling training algorithm was proposed in Mikolov et al. (2013b). There
are numerous surveys of static embeddings and their parameterizations (Bullinaria
and Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark
2014, Levy et al. 2015).
See Manning et al. (2008) and Chapter 14 for a deeper understanding of the role
of vectors in information retrieval, including how to compare queries with docu-
ments, more details on tf-idf, and issues of scaling to very large datasets. See Kim
(2019) for a clear and comprehensive tutorial on word2vec. Cruse (2004) is a useful
introductory linguistic text on lexical semantics.
Exercises

## Page 32

32 Chapter 6 ‚Ä¢ Vector Semantics and Embeddings
Agirre, E., C. Banea, C. Cardie, D. Cer, M. Diab,
A. Gonzalez-Agirre, W. Guo, I. Lopez-Gazpio, M. Mar-
itxalar, R. Mihalcea, G. Rigau, L. Uria, and J. Wiebe.
2015. SemEval-2015 task 2: Semantic textual similarity,
English, Spanish and pilot on interpretability. SemEval-
15.
Agirre, E., M. Diab, D. Cer, and A. Gonzalez-Agirre. 2012.
SemEval-2012 task 6: A pilot on semantic textual simi-
larity. SemEval-12 .
Antoniak, M. and D. Mimno. 2018. Evaluating the stability
of embedding-based word similarities. TACL , 6:107‚Äì119.
Bellegarda, J. R. 1997. A latent semantic analysis framework
for large-span language modeling. EUROSPEECH .
Bellegarda, J. R. 2000. Exploiting latent semantic informa-
tion in statistical language modeling. Proceedings of the
IEEE , 89(8):1279‚Äì1296.
Bender, E. M. and A. Koller. 2020. Climbing towards NLU:
On meaning, form, and understanding in the age of data.
ACL.
Bengio, Y ., A. Courville, and P. Vincent. 2013. Represen-
tation learning: A review and new perspectives. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence , 35(8):1798‚Äì1828.
Bengio, Y ., R. Ducharme, P. Vincent, and C. Jauvin. 2003.
A neural probabilistic language model. JMLR , 3:1137‚Äì
1155.
Bengio, Y ., H. Schwenk, J.-S. Sen ¬¥ecal, F. Morin, and J.-L.
Gauvain. 2006. Neural probabilistic language models. In
Innovations in Machine Learning , 137‚Äì186. Springer.
Bisk, Y ., A. Holtzman, J. Thomason, J. Andreas, Y . Bengio,
J. Chai, M. Lapata, A. Lazaridou, J. May, A. Nisnevich,
N. Pinto, and J. Turian. 2020. Experience grounds lan-
guage. EMNLP .
Blei, D. M., A. Y . Ng, and M. I. Jordan. 2003. Latent Dirich-
let allocation. JMLR , 3(5):993‚Äì1022.
Blodgett, S. L., S. Barocas, H. Daum ¬¥e III, and H. Wallach.
2020. Language (technology) is power: A critical survey
of ‚Äúbias‚Äù in NLP. ACL.
Bojanowski, P., E. Grave, A. Joulin, and T. Mikolov. 2017.
Enriching word vectors with subword information. TACL ,
5:135‚Äì146.
Bolukbasi, T., K.-W. Chang, J. Zou, V . Saligrama, and A. T.
Kalai. 2016. Man is to computer programmer as woman
is to homemaker? Debiasing word embeddings. NeurIPS .
Br¬¥eal, M. 1897. Essai de S ¬¥emantique: Science des signiÔ¨Åca-
tions . Hachette.
Budanitsky, A. and G. Hirst. 2006. Evaluating WordNet-
based measures of lexical semantic relatedness. Compu-
tational Linguistics , 32(1):13‚Äì47.
Bullinaria, J. A. and J. P. Levy. 2007. Extracting seman-
tic representations from word co-occurrence statistics:
A computational study. Behavior research methods ,
39(3):510‚Äì526.
Bullinaria, J. A. and J. P. Levy. 2012. Extracting semantic
representations from word co-occurrence statistics: stop-
lists, stemming, and SVD. Behavior research methods ,
44(3):890‚Äì907.
Caliskan, A., J. J. Bryson, and A. Narayanan. 2017. Seman-
tics derived automatically from language corpora contain
human-like biases. Science , 356(6334):183‚Äì186.Carlson, G. N. 1977. Reference to kinds in English . Ph.D.
thesis, University of Massachusetts, Amherst. Forward.
Church, K. W. and P. Hanks. 1989. Word association norms,
mutual information, and lexicography. ACL.
Church, K. W. and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics , 16(1):22‚Äì29.
Clark, E. 1987. The principle of contrast: A constraint on
language acquisition. In B. MacWhinney, ed., Mecha-
nisms of language acquisition , 1‚Äì33. LEA.
Coccaro, N. and D. Jurafsky. 1998. Towards better integra-
tion of semantic predictors in statistical language model-
ing. ICSLP .
Collobert, R. and J. Weston. 2007. Fast semantic extraction
using a novel neural network architecture. ACL.
Collobert, R. and J. Weston. 2008. A uniÔ¨Åed architecture for
natural language processing: Deep neural networks with
multitask learning. ICML .
Collobert, R., J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural language
processing (almost) from scratch. JMLR , 12:2493‚Äì2537.
Cordier, B. 1965. Factor-analysis of correspondences. COL-
ING 1965 .
Crawford, K. 2017. The trouble with bias. Keynote at
NeurIPS.
Cruse, D. A. 2004. Meaning in Language: an Introduction
to Semantics and Pragmatics . Oxford University Press.
Second edition.
Dagan, I., S. Marcus, and S. Markovitch. 1993. Contextual
word similarity and estimation from sparse data. ACL.
Davies, M. 2012. Expanding horizons in historical lin-
guistics with the 400-million word Corpus of Historical
American English. Corpora , 7(2):121‚Äì157.
Davies, M. 2015. The Wikipedia Corpus: 4.6 million arti-
cles, 1.9 billion words. Adapted from Wikipedia. https:
//www.english-corpora.org/wiki/ .
Deerwester, S. C., S. T. Dumais, G. W. Furnas, R. A. Harsh-
man, T. K. Landauer, K. E. Lochbaum, and L. Streeter.
1988. Computer information retrieval using latent seman-
tic structure: US Patent 4,839,853.
Deerwester, S. C., S. T. Dumais, T. K. Landauer, G. W. Fur-
nas, and R. A. Harshman. 1990. Indexing by latent se-
mantics analysis. JASIS , 41(6):391‚Äì407.
Ethayarajh, K., D. Duvenaud, and G. Hirst. 2019a. Towards
understanding linear word analogies. ACL.
Ethayarajh, K., D. Duvenaud, and G. Hirst. 2019b. Under-
standing undesirable word embedding associations. ACL.
Fano, R. M. 1961. Transmission of Information: A Statistical
Theory of Communications . MIT Press.
Finkelstein, L., E. Gabrilovich, Y . Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing
search in context: The concept revisited. ACM Trans-
actions on Information Systems , 20(1):116‚Äî-131.
Firth, J. R. 1957. A synopsis of linguistic theory 1930‚Äì
1955. In Studies in Linguistic Analysis . Philological So-
ciety. Reprinted in Palmer, F. (ed.) 1968. Selected Papers
of J. R. Firth. Longman, Harlow.

## Page 33

Exercises 33
Garg, N., L. Schiebinger, D. Jurafsky, and J. Zou. 2018.
Word embeddings quantify 100 years of gender and eth-
nic stereotypes. Proceedings of the National Academy of
Sciences , 115(16):E3635‚ÄìE3644.
Girard, G. 1718. La justesse de la langue franc ¬∏oise: ou les
diff¬¥erentes signiÔ¨Åcations des mots qui passent pour syn-
onimes . Laurent d‚ÄôHoury, Paris.
Giuliano, V . E. 1965. The interpretation of word
associations. Statistical Association Methods For
Mechanized Documentation. Symposium Proceed-
ings. Washington, D.C., USA, March 17, 1964 .
https://nvlpubs.nist.gov/nistpubs/Legacy/
MP/nbsmiscellaneouspub269.pdf .
Gladkova, A., A. Drozd, and S. Matsuoka. 2016. Analogy-
based detection of morphological and semantic relations
with word embeddings: what works and what doesn‚Äôt.
NAACL Student Research Workshop .
Glenberg, A. M. and D. A. Robertson. 2000. Symbol ground-
ing and meaning: A comparison of high-dimensional and
embodied theories of meaning. Journal of memory and
language , 43(3):379‚Äì401.
Gonen, H. and Y . Goldberg. 2019. Lipstick on a pig: Debi-
asing methods cover up systematic gender biases in word
embeddings but do not remove them. NAACL HLT .
Gould, S. J. 1980. The Panda‚Äôs Thumb . Penguin Group.
Greenwald, A. G., D. E. McGhee, and J. L. K. Schwartz.
1998. Measuring individual differences in implicit cogni-
tion: the implicit association test. Journal of personality
and social psychology , 74(6):1464‚Äì1480.
Hamilton, W. L., J. Leskovec, and D. Jurafsky. 2016. Di-
achronic word embeddings reveal statistical laws of se-
mantic change. ACL.
Harris, Z. S. 1954. Distributional structure. Word , 10:146‚Äì
162.
Hellrich, J. and U. Hahn. 2016. Bad company‚Äî
Neighborhoods in neural embedding spaces considered
harmful. COLING .
Hill, F., R. Reichart, and A. Korhonen. 2015. Simlex-999:
Evaluating semantic models with (genuine) similarity es-
timation. Computational Linguistics , 41(4):665‚Äì695.
Hjelmslev, L. 1969. Prologomena to a Theory of Language .
University of Wisconsin Press. Translated by Francis J.
WhitÔ¨Åeld; original Danish edition 1943.
Hofmann, T. 1999. Probabilistic latent semantic indexing.
SIGIR-99 .
Huang, E. H., R. Socher, C. D. Manning, and A. Y . Ng. 2012.
Improving word representations via global context and
multiple word prototypes. ACL.
Jia, S., T. Meng, J. Zhao, and K.-W. Chang. 2020. Mitigat-
ing gender bias ampliÔ¨Åcation in distribution by posterior
regularization. ACL.
Jones, M. P. and J. H. Martin. 1997. Contextual spelling cor-
rection using latent semantic analysis. ANLP .
Joos, M. 1950. Description of language design. JASA ,
22:701‚Äì708.
Jurafsky, D. 2014. The Language of Food . W. W. Norton,
New York.Jurgens, D., S. M. Mohammad, P. Turney, and K. Holyoak.
2012. SemEval-2012 task 2: Measuring degrees of rela-
tional similarity. *SEM 2012 .
Katz, J. J. and J. A. Fodor. 1963. The structure of a semantic
theory. Language , 39:170‚Äì210.
Kiela, D. and S. Clark. 2014. A systematic study of semantic
vector space model parameters. EACL 2nd Workshop on
Continuous Vector Space Models and their Composition-
ality (CVSC) .
Kim, E. 2019. Optimize computational efÔ¨Åciency
of skip-gram with negative sampling. https://
aegis4048.github.io/optimize_computational_
efficiency_of_skip-gram_with_negative_
sampling .
Lake, B. M. and G. L. Murphy. 2021. Word meaning in
minds and machines. Psychological Review . In press.
Landauer, T. K. and S. T. Dumais. 1997. A solution to Plato‚Äôs
problem: The Latent Semantic Analysis theory of acqui-
sition, induction, and representation of knowledge. Psy-
chological Review , 104:211‚Äì240.
Landauer, T. K., D. Laham, B. Rehder, and M. E. Schreiner.
1997. How well can passage meaning be derived with-
out using word order? A comparison of Latent Semantic
Analysis and humans. COGSCI .
Lapesa, G. and S. Evert. 2014. A large scale evaluation of
distributional semantic models: Parameters, interactions
and model selection. TACL , 2:531‚Äì545.
Lee, D. D. and H. S. Seung. 1999. Learning the parts of
objects by non-negative matrix factorization. Nature ,
401(6755):788‚Äì791.
Levy, O. and Y . Goldberg. 2014a. Dependency-based word
embeddings. ACL.
Levy, O. and Y . Goldberg. 2014b. Linguistic regularities in
sparse and explicit word representations. CoNLL .
Levy, O. and Y . Goldberg. 2014c. Neural word embedding
as implicit matrix factorization. NeurIPS .
Levy, O., Y . Goldberg, and I. Dagan. 2015. Improving dis-
tributional similarity with lessons learned from word em-
beddings. TACL , 3:211‚Äì225.
Li, J., X. Chen, E. H. Hovy, and D. Jurafsky. 2015. Visual-
izing and understanding neural models in NLP. NAACL
HLT.
Lin, Y ., J.-B. Michel, E. Lieberman Aiden, J. Orwant,
W. Brockman, and S. Petrov. 2012. Syntactic annotations
for the Google Books NGram corpus. ACL.
Linzen, T. 2016. Issues in evaluating semantic spaces us-
ing word analogies. 1st Workshop on Evaluating Vector-
Space Representations for NLP .
Luhn, H. P. 1957. A statistical approach to the mechanized
encoding and searching of literary information. IBM
Journal of Research and Development , 1(4):309‚Äì317.
Manning, C. D., P. Raghavan, and H. Sch ¬®utze. 2008. Intro-
duction to Information Retrieval . Cambridge.
Mikolov, T., K. Chen, G. S. Corrado, and J. Dean. 2013a. Ef-
Ô¨Åcient estimation of word representations in vector space.
ICLR 2013 .
Mikolov, T., S. Kombrink, L. Burget, J. H. ÀáCernock `y, and
S. Khudanpur. 2011. Extensions of recurrent neural net-
work language model. ICASSP .

## Page 34

34 Chapter 6 ‚Ä¢ Vector Semantics and Embeddings
Mikolov, T., I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. 2013b. Distributed representations of words and
phrases and their compositionality. NeurIPS .
Mikolov, T., W.-t. Yih, and G. Zweig. 2013c. Linguis-
tic regularities in continuous space word representations.
NAACL HLT .
Niwa, Y . and Y . Nitta. 1994. Co-occurrence vectors from
corpora vs. distance vectors from dictionaries. COLING .
Nosek, B. A., M. R. Banaji, and A. G. Greenwald. 2002a.
Harvesting implicit group attitudes and beliefs from a
demonstration web site. Group Dynamics: Theory, Re-
search, and Practice , 6(1):101.
Nosek, B. A., M. R. Banaji, and A. G. Greenwald. 2002b.
Math=male, me=female, therefore math 6=me.Journal of
personality and social psychology , 83(1):44.
Osgood, C. E., G. J. Suci, and P. H. Tannenbaum. 1957. The
Measurement of Meaning . University of Illinois Press.
Pennington, J., R. Socher, and C. D. Manning. 2014. GloVe:
Global vectors for word representation. EMNLP .
Peterson, J. C., D. Chen, and T. L. GrifÔ¨Åths. 2020. Parallelo-
grams revisited: Exploring the limitations of vector space
models for simple analogies. Cognition , 205.
Pilehvar, M. T. and J. Camacho-Collados. 2019. WiC: the
word-in-context dataset for evaluating context-sensitive
meaning representations. NAACL HLT .
Rehder, B., M. E. Schreiner, M. B. W. Wolfe, D. Laham,
T. K. Landauer, and W. Kintsch. 1998. Using Latent
Semantic Analysis to assess knowledge: Some technical
considerations. Discourse Processes , 25(2-3):337‚Äì354.
Rohde, D. L. T., L. M. Gonnerman, and D. C. Plaut. 2006.
An improved model of semantic similarity based on lexi-
cal co-occurrence. CACM , 8:627‚Äì633.
Rumelhart, D. E. and A. A. Abrahamson. 1973. A model for
analogical reasoning. Cognitive Psychology , 5(1):1‚Äì28.
Salton, G. 1971. The SMART Retrieval System: Experiments
in Automatic Document Processing . Prentice Hall.
Schluter, N. 2018. The word analogy testing caveat. NAACL
HLT.
Schone, P. and D. Jurafsky. 2000. Knowlege-free induction
of morphology using latent semantic analysis. CoNLL .
Schone, P. and D. Jurafsky. 2001a. Is knowledge-free in-
duction of multiword unit dictionary headwords a solved
problem? EMNLP .
Schone, P. and D. Jurafsky. 2001b. Knowledge-free induc-
tion of inÔ¨Çectional morphologies. NAACL .
Sch¬®utze, H. 1992. Dimensions of meaning. Proceedings of
Supercomputing ‚Äô92 . IEEE Press.
Sch¬®utze, H. 1997. Ambiguity Resolution in Language Learn-
ing ‚Äì Computational and Cognitive Models . CSLI, Stan-
ford, CA.
Sch¬®utze, H., D. A. Hull, and J. Pedersen. 1995. A compar-
ison of classiÔ¨Åers and document representations for the
routing problem. SIGIR-95 .
Sch¬®utze, H. and J. Pedersen. 1993. A vector model for syn-
tagmatic and paradigmatic relatedness. 9th Annual Con-
ference of the UW Centre for the New OED and Text Re-
search .Sparck Jones, K. 1972. A statistical interpretation of term
speciÔ¨Åcity and its application in retrieval. Journal of Doc-
umentation , 28(1):11‚Äì21.
Sparck Jones, K. 1986. Synonymy and Semantic ClassiÔ¨Åca-
tion. Edinburgh University Press, Edinburgh. Republica-
tion of 1964 PhD Thesis.
Switzer, P. 1965. Vector images in document retrieval.
Statistical Association Methods For Mechanized Docu-
mentation. Symposium Proceedings. Washington, D.C.,
USA, March 17, 1964 .https://nvlpubs.nist.gov/
nistpubs/Legacy/MP/nbsmiscellaneouspub269.
pdf.
Tian, Y ., V . Kulkarni, B. Perozzi, and S. Skiena. 2016. On
the convergent properties of word embedding methods.
ArXiv preprint arXiv:1605.03956.
Turian, J., L. Ratinov, and Y . Bengio. 2010. Word represen-
tations: a simple and general method for semi-supervised
learning. ACL.
Turney, P. D. and M. L. Littman. 2005. Corpus-based learn-
ing of analogies and semantic relations. Machine Learn-
ing, 60(1-3):251‚Äì278.
van der Maaten, L. and G. E. Hinton. 2008. Visualizing high-
dimensional data using t-SNE. JMLR , 9:2579‚Äì2605.
Wierzbicka, A. 1992. Semantics, Culture, and Cognition:
University Human Concepts in Culture-SpeciÔ¨Åc ConÔ¨Ågu-
rations . Oxford University Press.
Wierzbicka, A. 1996. Semantics: Primes and Universals .
Oxford University Press.
Wittgenstein, L. 1953. Philosophical Investigations. (Trans-
lated by Anscombe, G.E.M.) . Blackwell.
Zhao, J., T. Wang, M. Yatskar, V . Ordonez, and K.-
W. Chang. 2017. Men also like shopping: Reducing
gender bias ampliÔ¨Åcation using corpus-level constraints.
EMNLP .
Zhao, J., Y . Zhou, Z. Li, W. Wang, and K.-W. Chang. 2018.
Learning gender-neutral word embeddings. EMNLP .

