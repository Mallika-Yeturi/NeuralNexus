# vectorsemantics2024

## Page 1

Vector Semantics & EmbeddingsWord Meaning

## Page 2

What do words mean?N-gram or text classification methods we've seen so far◦Words are just strings (or indices wiin a vocabulary list)◦That's not very satisfactory!Introductory logic classes:◦The meaning of "dog" is DOG;  cat is CAT∀x DOG(x) ⟶MAMMAL(x)Old linguistics joke by Barbara Partee in 1967:◦Q: What's the meaning of life?◦A: LIFEThat seems hardly better!

## Page 3

DesiderataWhat should a theory of word meaning do for us?Let's look at some desiderataFrom lexical semantics, the linguistic study of word meaning

## Page 4

mouse (N)1. any of numerous small rodents...2. a hand-operated device that controls a cursor... Lemmas and sensessenselemma
A sense or “concept” is the meaning component of a wordLemmas can be polysemous (have multiple senses)Modified from the online thesaurus WordNet

## Page 5

Relations between senses: SynonymySynonyms have the same meaning in some or all contexts.◦filbert / hazelnut◦couch / sofa◦big / large◦automobile / car◦vomit / throw up◦water / H20

## Page 6

Relations between senses: SynonymyNote that there are probably no examples of perfect synonymy.◦Even if many aspects of meaning are identical◦Still may differ based on politeness, slang, register, genre, etc.

## Page 7

Relation: Synonymy?water/H20"H20" in a surfing guide?big/largemy big sister != my large sister

## Page 8

The Linguistic Principle of ContrastDifference in form àdifference in meaning

## Page 9

AbbéGabriel Girard 1718
 [I do not believe that there is a synonymous word in any language]
""Re: "exact" synonyms
Thanks to Mark Aronoff!

## Page 10

Relation: SimilarityWords with similar meanings.  Not synonyms, but sharing some element of meaningcar, bicyclecow, horse

## Page 11

Ask humans how similar 2 words areword1word2similarityvanishdisappear9.8 behaveobey7.3 beliefimpression 5.95 musclebone 3.65 modestflexible0.98 holeagreement0.3 SimLex-999 dataset (Hill et al., 2015) 

## Page 12

Relation: Word relatednessAlso called "word association"Words can be related in any way, perhaps via a semantic frame or field◦coffee, tea:    similar◦coffee, cup:   related, not similar

## Page 13

Semantic fieldWords that ◦cover a particular semantic domain ◦bear structured relations with each other. hospitalssurgeon, scalpel, nurse, anaesthetic, hospitalrestaurantswaiter, menu, plate, food, menu,chefhousesdoor, roof, kitchen, family, bed

## Page 14

Relation: AntonymySenses that are opposites with respect to only one feature of meaningOtherwise, they are very similar!dark/light   short/longfast/slowrise/fallhot/coldup/downin/outMore formally: antonyms can◦define a binary opposition or be at opposite ends of a scale◦long/short, fast/slow◦Be reversives:◦rise/fall, up/down

## Page 15

Connotation (sentiment)•Words have affectivemeanings•Positive connotations (happy) •Negative connotations (sad)•Connotations can be subtle:•Positive connotation: copy, replica, reproduction •Negative connotation: fake, knockoff, forgery•Evaluation (sentiment!)•Positive evaluation (great, love) •Negative evaluation (terrible, hate)

## Page 16

ConnotationWords seem to vary along 3 affective dimensions:◦valence: the pleasantness of the stimulus◦arousal: the intensity of emotion provoked by the stimulus◦dominance: the degree of control exerted by the stimulusOsgood et al. (1957)
WordScoreWordScoreValencelove1.000toxic0.008happy1.000nightmare0.005Arousalelated0.960mellow0.069frenzy0.965napping0.046Dominancepowerful0.991weak0.045leadership0.983empty0.081Values from NRC VAD Lexicon  (Mohammad 2018)

## Page 17

So farConceptsor word senses◦Have a complex many-to-many association with words(homonymy, multiple senses)Have relations with each other◦Synonymy◦Antonymy◦Similarity◦Relatedness◦Connotation

## Page 18

Vector Semantics & EmbeddingsWord Meaning

## Page 19

Vector Semantics & EmbeddingsVector Semantics

## Page 20

Computational models of word meaningCan we build a theory of how to represent word meaning, that accounts for at least some of the desiderata?We'll introduce vector semanticsThe standard model in language processing!Handles many of our goals!

## Page 21

Ludwig WittgensteinPI #43: "The meaning of a word is its use in the language"

## Page 22

Let's define words by their usagesOne way to define "usage": words are defined by their environments (the words around them)ZelligHarris (1954): If A and B have almost identical environments we say that they are synonyms.

## Page 23

What does recent English borrowing ongchoimean?Suppose you see these sentences:•Ong choiis delicious sautéedwithgarlic. •Ong choiis superb over rice•Ong choileaveswith salty saucesAnd you've also seen these:•…spinach sautéedwithgarlicover rice•Chard stems and leavesare delicious•Collard greens and other saltyleafy greensConclusion:◦Ongchoiis a leafy green like spinach, chard, or collard greens◦We could conclude this based on words like "leaves" and "delicious" and "sauteed" 

## Page 24

Ongchoi: Ipomoea aquatica "Water Spinach"
Yamaguchi, Wikimedia Commons, public domain!"#kangkongrau muống…

## Page 25

Idea 1: Defining meaning by linguistic distributionLet's define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. 

## Page 26

Idea 2: Meaning as a point in space (Osgood et al. 1957)3 affective dimensions for a word◦valence: pleasantness ◦arousal: intensity of emotion ◦dominance: the degree of control exerted◦Hence the connotation of a word is a vector in 3-spaceWordScoreWordScoreValencelove1.000toxic0.008happy1.000nightmare0.005Arousalelated0.960mellow0.069frenzy0.965napping0.046Dominancepowerful0.991weak0.045leadership0.983empty0.081NRC VAD Lexicon  (Mohammad 2018)

## Page 27

Idea 1: Defining meaning by linguistic distributionIdea 2: Meaning as a point in multidimensional space

## Page 28

6CHAPTER6•VECTORSEMANTICS ANDEMBEDDINGS
goodnicebadworstnot good
wonderfulamazingterriﬁcdislikeworsevery goodincredibly goodfantasticincredibly badnowyouithatwithbyto’sareisathanFigure 6.1A two-dimensional (t-SNE) projection of embeddings for some words andphrases, showing that words with similar meanings are nearby in space. The original 60-dimensional embeddings were trained for sentiment analysis. Simpliﬁed fromLi et al. (2015)with colors added for explanation.The ﬁne-grained model of word similarity of vector semantics offers enormouspower to NLP applications. NLP applications like the sentiment classiﬁers of Chap-ter 4 or Chapter 5 depend on the same words appearing in the training and test sets.But by representing words as embeddings, classiﬁers can assign sentiment as long asit sees some words withsimilar meanings. And as we’ll see, vector semantic modelscan be learned automatically from text without supervision.In this chapter we’ll introduce the two most commonly used models. In thetf-idfmodel, an important baseline, the meaning of a word is deﬁned by a simple functionof the counts of nearby words. We will see that this method results in very longvectors that aresparse, i.e. mostly zeros (since most words simply never occur inthe context of others). We’ll introduce theword2vecmodel family for construct-ing short,densevectors that have useful semantic properties. We’ll also introducethecosine, the standard way to use embeddings to computesemantic similarity, be-tween two words, two sentences, or two documents, an important tool in practicalapplications like question answering, summarization, or automatic essay grading.6.3 Words and Vectors“The most important attributes of a vector in 3-space are{Location, Location, Location}”Randall Munroe,https://xkcd.com/2358/Vector or distributional models of meaning are generally based on aco-occurrencematrix, a way of representing how often words co-occur. We’ll look at two popularmatrices: the term-document matrix and the term-term matrix.6.3.1 Vectors and documentsIn aterm-document matrix, each row represents a word in the vocabulary and eachterm-documentmatrixcolumn represents a document from some collection of documents. Fig.6.2shows asmall selection from a term-document matrix showing the occurrence of four wordsin four plays by Shakespeare. Each cell in this matrix represents the number of timesa particular word (deﬁned by the row) occurs in a particular document (deﬁned bythe column). Thusfoolappeared 58 times inTwelfth Night.The term-document matrix of Fig.6.2was ﬁrst deﬁned as part of thevectorspace modelof information retrieval(Salton, 1971). In this model, a document isvector spacemodelDefining meaning as a point in space based on distributionEach word = a vector   (not just "good" or "w45")Similar words are "nearby in semantic space"We build this space automatically by seeing which words are nearby in text

## Page 29

We define meaning of a word as a vectorCalled an "embedding" because it's embedded into a space (see textbook)The standard way to represent meaning in NLPEvery modern NLP algorithm uses embeddings as the representation of word meaningFine-grained model of meaning for similarity 

## Page 30

Intuition: why vectors?Consider sentiment analysis:◦With words,  a feature is a word identity◦Feature 5: 'The previous word was "terrible"'◦requires exactsamewordto be in training and test◦With embeddings: ◦Feature is a word vector◦'The previous word was vector [35,22,17…]◦Now in the test set we might see a similar vector [34,21,14]◦We can generalize to similar but unseenwords!!! 

## Page 31

We'll discuss 2 kinds of embeddingstf-idf◦Information Retrieval workhorse!◦A common baseline model◦Sparsevectors◦Words are represented by (a simple function of) the counts of nearby wordsWord2vec◦Densevectors◦Representation is created by training a classifier to predictwhether a word is likely to appear nearby◦Later we'll discuss extensions called  contextual embeddings

## Page 32

From now on:Computing with meaning representationsinstead of string representationsSpeech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright©2020. Allrights reserved. Draft of January 13, 2021.CHAPTER6Vector Semantics andEmbeddingsC⇧@Â(| ó| ÿCNets are for ﬁsh;Once you get the ﬁsh, you can forget the net. ⇧@Â(✏ ó✏ ÿ Words are for meaning;Once you get the meaning, you can forget the wordsÑP(Zhuangzi), Chapter 26The asphalt that Los Angeles is famous for occurs mainly on its freeways. Butin the middle of the city is another patch of asphalt, the La Brea tar pits, and thisasphalt preserves millions of fossil bones from the last of the Ice Ages of the Pleis-tocene Epoch. One of these fossils is theSmilodon, or saber-toothed tiger, instantlyrecognizable by its long canines. Five million years ago or so, a completely different
sabre-tooth tiger calledThylacosmiluslivedin Argentina and other parts of South Amer-ica. Thylacosmilus was a marsupial whereasSmilodon was a placental mammal, but Thy-lacosmilus had the same long upper caninesand, like Smilodon, had a protective boneﬂange on the lower jaw. The similarity ofthese two mammals is one of many examplesof parallel or convergent evolution, in which particular contexts or environmentslead to the evolution of very similar structures in different species(Gould, 1980).The role of context is also important in the similarity of a less biological kindof organism: the word. Words that occur insimilar contextstend to havesimilarmeanings. This link between similarity in how words are distributed and similarityin what they mean is called thedistributional hypothesis. The hypothesis wasdistributionalhypothesisﬁrst formulated in the 1950s by linguists likeJoos (1950),Harris (1954), andFirth(1957), who noticed that words which are synonyms (likeoculistandeye-doctor)tended to occur in the same environment (e.g., near words likeeyeorexamined)with the amount of meaning difference between two words “corresponding roughlyto the amount of difference in their environments”(Harris, 1954, 157).In this chapter we introducevector semantics, which instantiates this linguisticvectorsemanticshypothesis by learning representations of the meaning of words, calledembeddings,embeddingsdirectly from their distributions in texts. These representations are used in every nat-ural language processing application that makes use of meaning, and thestatic em-beddingswe introduce here underlie the more powerful dynamic orcontextualizedembeddingslikeBERTthat we will see in Chapter 10.These word representations are also the ﬁrst example in this book ofrepre-sentation learning, automatically learning useful representations of the input text.representationlearningFinding suchself-supervisedways to learn representations of the input, instead ofcreating representations by hand viafeature engineering, is an important focus ofNLP research(Bengio et al., 2013).

## Page 33

Vector Semantics & EmbeddingsVector Semantics

## Page 34

Vector Semantics & EmbeddingsWords and Vectors

## Page 35

Term-document matrix6.3•WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like Itis represented as the list [1,114,36,20] (the ﬁrstcolumnvectorin Fig.6.3) andJulius Caesaris represented as the list [7,62,1,2] (the thirdcolumn vector). Avector spaceis a collection of vectors, characterized by theirvector spacedimension. In the example in Fig.6.3, the document vectors are of dimension 4,dimensionjust so they ﬁt on the page; in real term-document matrices, the vectors representingeach document would have dimensionality|V|, the vocabulary size.The ordering of the numbers in a vector space indicates different meaningful di-mensions on which documents vary. Thus the ﬁrst dimension for both these vectorscorresponds to the number of times the wordbattleoccurs, and we can compareeach dimension, noting for example that the vectors forAs You Like ItandTwelfthNighthave similar values (1 and 0, respectively) for the ﬁrst dimension.As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.3The term-document matrix for four words in four Shakespeare plays. The redboxes show that each document is represented as a column vector of length four.We can think of the vector for a document as a point in|V|-dimensional space;thus the documents in Fig.6.3are points in 4-dimensional space. Since 4-dimensionalspaces are hard to visualize, Fig.6.4shows a visualization in two dimensions; we’vearbitrarily chosen the dimensions corresponding to the wordsbattleandfool.
51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540
354045505560Figure 6.4A spatial visualization of the document vectors for the four Shakespeare playdocuments, showing just two of the dimensions, corresponding to the wordsbattleandfool.The comedies have high values for thefooldimension and low values for thebattledimension.Term-document matrices were originally deﬁned as a means of ﬁnding similardocuments for the task of documentinformation retrieval. Two documents that are6.3•WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like Itis represented as the list [1,114,36,20] (the ﬁrstcolumnvectorin Fig.6.3) andJulius Caesaris represented as the list [7,62,1,2] (the thirdcolumn vector). Avector spaceis a collection of vectors, characterized by theirvector spacedimension. In the example in Fig.6.3, the document vectors are of dimension 4,dimensionjust so they ﬁt on the page; in real term-document matrices, the vectors representingeach document would have dimensionality|V|, the vocabulary size.The ordering of the numbers in a vector space indicates different meaningful di-mensions on which documents vary. Thus the ﬁrst dimension for both these vectorscorresponds to the number of times the wordbattleoccurs, and we can compareeach dimension, noting for example that the vectors forAs You Like ItandTwelfthNighthave similar values (1 and 0, respectively) for the ﬁrst dimension.As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.3The term-document matrix for four words in four Shakespeare plays. The redboxes show that each document is represented as a column vector of length four.We can think of the vector for a document as a point in|V|-dimensional space;thus the documents in Fig.6.3are points in 4-dimensional space. Since 4-dimensionalspaces are hard to visualize, Fig.6.4shows a visualization in two dimensions; we’vearbitrarily chosen the dimensions corresponding to the wordsbattleandfool.
51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540
354045505560Figure 6.4A spatial visualization of the document vectors for the four Shakespeare playdocuments, showing just two of the dimensions, corresponding to the wordsbattleandfool.The comedies have high values for thefooldimension and low values for thebattledimension.Term-document matrices were originally deﬁned as a means of ﬁnding similardocuments for the task of documentinformation retrieval. Two documents that areEach document is represented by a vector of words

## Page 36

Visualizing document vectors6.3•WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like Itis represented as the list [1,114,36,20] (the ﬁrstcolumnvectorin Fig.6.3) andJulius Caesaris represented as the list [7,62,1,2] (the thirdcolumn vector). Avector spaceis a collection of vectors, characterized by theirvector spacedimension. In the example in Fig.6.3, the document vectors are of dimension 4,dimensionjust so they ﬁt on the page; in real term-document matrices, the vectors representingeach document would have dimensionality|V|, the vocabulary size.The ordering of the numbers in a vector space indicates different meaningful di-mensions on which documents vary. Thus the ﬁrst dimension for both these vectorscorresponds to the number of times the wordbattleoccurs, and we can compareeach dimension, noting for example that the vectors forAs You Like ItandTwelfthNighthave similar values (1 and 0, respectively) for the ﬁrst dimension.As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.3The term-document matrix for four words in four Shakespeare plays. The redboxes show that each document is represented as a column vector of length four.We can think of the vector for a document as a point in|V|-dimensional space;thus the documents in Fig.6.3are points in 4-dimensional space. Since 4-dimensionalspaces are hard to visualize, Fig.6.4shows a visualization in two dimensions; we’vearbitrarily chosen the dimensions corresponding to the wordsbattleandfool.
51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540
354045505560Figure 6.4A spatial visualization of the document vectors for the four Shakespeare playdocuments, showing just two of the dimensions, corresponding to the wordsbattleandfool.The comedies have high values for thefooldimension and low values for thebattledimension.Term-document matrices were originally deﬁned as a means of ﬁnding similardocuments for the task of documentinformation retrieval. Two documents that are

## Page 37

Vectors are the basis of information retrieval6.3•WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like Itis represented as the list [1,114,36,20] (the ﬁrstcolumnvectorin Fig.6.3) andJulius Caesaris represented as the list [7,62,1,2] (the thirdcolumn vector). Avector spaceis a collection of vectors, characterized by theirvector spacedimension. In the example in Fig.6.3, the document vectors are of dimension 4,dimensionjust so they ﬁt on the page; in real term-document matrices, the vectors representingeach document would have dimensionality|V|, the vocabulary size.The ordering of the numbers in a vector space indicates different meaningful di-mensions on which documents vary. Thus the ﬁrst dimension for both these vectorscorresponds to the number of times the wordbattleoccurs, and we can compareeach dimension, noting for example that the vectors forAs You Like ItandTwelfthNighthave similar values (1 and 0, respectively) for the ﬁrst dimension.As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.3The term-document matrix for four words in four Shakespeare plays. The redboxes show that each document is represented as a column vector of length four.We can think of the vector for a document as a point in|V|-dimensional space;thus the documents in Fig.6.3are points in 4-dimensional space. Since 4-dimensionalspaces are hard to visualize, Fig.6.4shows a visualization in two dimensions; we’vearbitrarily chosen the dimensions corresponding to the wordsbattleandfool.
51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540
354045505560Figure 6.4A spatial visualization of the document vectors for the four Shakespeare playdocuments, showing just two of the dimensions, corresponding to the wordsbattleandfool.The comedies have high values for thefooldimension and low values for thebattledimension.Term-document matrices were originally deﬁned as a means of ﬁnding similardocuments for the task of documentinformation retrieval. Two documents that areVectors are similar for the two comediesBut comedies are different than the other two  Comedies have more fools and wit and fewer battles.

## Page 38

Idea for word meaning: Words can be vectors too!!!6.3•WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like Itis represented as the list [1,114,36,20] (the ﬁrstcolumnvectorin Fig.6.3) andJulius Caesaris represented as the list [7,62,1,2] (the thirdcolumn vector). Avector spaceis a collection of vectors, characterized by theirvector spacedimension. In the example in Fig.6.3, the document vectors are of dimension 4,dimensionjust so they ﬁt on the page; in real term-document matrices, the vectors representingeach document would have dimensionality|V|, the vocabulary size.The ordering of the numbers in a vector space indicates different meaningful di-mensions on which documents vary. Thus the ﬁrst dimension for both these vectorscorresponds to the number of times the wordbattleoccurs, and we can compareeach dimension, noting for example that the vectors forAs You Like ItandTwelfthNighthave similar values (1 and 0, respectively) for the ﬁrst dimension.As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.3The term-document matrix for four words in four Shakespeare plays. The redboxes show that each document is represented as a column vector of length four.We can think of the vector for a document as a point in|V|-dimensional space;thus the documents in Fig.6.3are points in 4-dimensional space. Since 4-dimensionalspaces are hard to visualize, Fig.6.4shows a visualization in two dimensions; we’vearbitrarily chosen the dimensions corresponding to the wordsbattleandfool.
51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540
354045505560Figure 6.4A spatial visualization of the document vectors for the four Shakespeare playdocuments, showing just two of the dimensions, corresponding to the wordsbattleandfool.The comedies have high values for thefooldimension and low values for thebattledimension.Term-document matrices were originally deﬁned as a means of ﬁnding similardocuments for the task of documentinformation retrieval. Two documents that arebattle is "the kind of word that occurs in Julius Caesar and Henry V"fool is "the kind of word that occurs  in comedies, especially Twelfth Night"8CHAPTER6•VECTORSEMANTICS ANDEMBEDDINGSsimilar will tend to have similar words, and if two documents have similar wordstheir column vectors will tend to be similar. The vectors for the comediesAs YouLike It[1,114,36,20] andTwelfth Night[0,80,58,15] look a lot more like each other(more fools and wit than battles) than they look likeJulius Caesar[7,62,1,2] orHenry V[13,89,4,3]. This is clear with the raw numbers; in the ﬁrst dimension(battle) the comedies have low numbers and the others have high numbers, and wecan see it visually in Fig.6.4; we’ll see very shortly how to quantify this intuitionmore formally.A real term-document matrix, of course, wouldn’t just have 4 rows and columns,let alone 2. More generally, the term-document matrix has|V|rows (one for eachword type in the vocabulary) andDcolumns (one for each document in the collec-tion); as we’ll see, vocabulary sizes are generally in the tens of thousands, and thenumber of documents can be enormous (think about all the pages on the web).Information retrieval(IR) is the task of ﬁnding the documentdfrom theDinformationretrievaldocuments in some collection that best matches a queryq. For IR we’ll therefore alsorepresent a query by a vector, also of length|V|, and we’ll need a way to comparetwo vectors to ﬁnd how similar they are. (Doing IR will also require efﬁcient waysto store and manipulate these vectors by making use of the convenient fact that thesevectors are sparse, i.e., mostly zeros).Later in the chapter we’ll introduce some of the components of this vector com-parison process: the tf-idf term weighting, and the cosine similarity metric.6.3.2 Words as vectors: document dimensionsWe’ve seen that documents can be represented as vectors in a vector space. Butvector semantics can also be used to represent the meaning ofwords. We do thisby associating each word with a word vector— arow vectorrather than a columnrow vectorvector, hence with different dimensions, as shown in Fig.6.5. The four dimensionsof the vector forfool, [36,58,1,4], correspond to the four Shakespeare plays. Wordcounts in the same four dimensions are used to form the vectors for the other 3words:wit, [20,15,2,3];battle, [1,0,7,13]; andgood[114,80,62,89].As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.5The term-document matrix for four words in four Shakespeare plays. The redboxes show that each word is represented as a row vector of length four.For documents, we saw that similar documents had similar vectors, because sim-ilar documents tend to have similar words. This same principle applies to words:similar words have similar vectors because they tend to occur in similar documents.The term-document matrix thus lets us represent the meaning of a word by the doc-uments it tends to occur in.6.3.3 Words as vectors: word dimensionsAn alternative to using the term-document matrix to represent words as vectors ofdocument counts, is to use theterm-term matrix, also called theword-word ma-trixor theterm-context matrix, in which the columns are labeled by words ratherword-wordmatrixthan documents. This matrix is thus of dimensionality|V|⇥|V|and each cell records

## Page 39

More common: word-word matrix(or "term-context matrix")Two wordsare similar in meaning if their context vectors are similar6.3•WORDS ANDVECTORS9Information retrieval(IR) is the task of ﬁnding the documentdfrom theDinformationretrievaldocuments in some collection that best matches a queryq. For IR we’ll therefore alsorepresent a query by a vector, also of length|V|, and we’ll need a way to comparetwo vectors to ﬁnd how similar they are. (Doing IR will also require efﬁcient waysto store and manipulate these vectors by making use of the convenient fact that thesevectors are sparse, i.e., mostly zeros).Later in the chapter we’ll introduce some of the components of this vector com-parison process: the tf-idf term weighting, and the cosine similarity metric.6.3.2 Words as vectorsWe’ve seen that documents can be represented as vectors in a vector space. Butvector semantics can also be used to represent the meaning ofwords, by associatingeach word with a vector.The word vector is now arow vectorrather than a column vector, and hence therow vectordimensions of the vector are different. The four dimensions of the vector forfool,[36,58,1,4], correspond to the four Shakespeare plays. The same four dimensionsare used to form the vectors for the other 3 words:wit, [20,15,2,3];battle, [1,0,7,13];andgood[114,80,62,89]. Each entry in the vector thus represents the counts of theword’s occurrence in the document corresponding to that dimension.For documents, we saw that similar documents had similar vectors, because sim-ilar documents tend to have similar words. This same principle applies to words:similar words have similar vectors because they tend to occur in similar documents.The term-document matrix thus lets us represent the meaning of a word by the doc-uments it tends to occur in.However, it is most common to use a different kind of context for the dimensionsof a word’s vector representation. Rather than the term-document matrix we use theterm-term matrix, more commonly called theword-word matrixor theterm-word-wordmatrixcontext matrix, in which the columns are labeled by words rather than documents.This matrix is thus of dimensionality|V|⇥|V|and each cell records the number oftimes the row (target) word and the column (context) word co-occur in some contextin some training corpus. The context could be the document, in which case the cellrepresents the number of times the two words appear in the same document. It ismost common, however, to use smaller contexts, generally a window around theword, for example of 4 words to the left and 4 words to the right, in which casethe cell represents the number of times (in some training corpus) the column wordoccurs in such a±4 word window around the row word. For example here is oneexample each of some words in their windows:is traditionally followed bycherrypie, a traditional dessertoften mixed, such asstrawberryrhubarb pie. Apple piecomputer peripherals and personaldigitalassistants. These devices usuallya computer. This includesinformationavailable on the internetIf we then take every occurrence of each word (saystrawberry) and count the con-text words around it, we get a word-word co-occurrence matrix. Fig.6.5shows asimpliﬁed subset of the word-word co-occurrence matrix for these four words com-puted from the Wikipedia corpus(Davies, 2015).Note in Fig.6.5that the two wordscherryandstrawberryare more similar toeach other (bothpieandsugartend to occur in their window) than they are to otherwords likedigital; conversely,digitalandinformationare more similar to each otherthan, say, tostrawberry. Fig.6.6shows a spatial visualization.6.3•WORDS ANDVECTORS9the number of times the row (target) word and the column (context) word co-occurin some context in some training corpus. The context could be the document, inwhich case the cell represents the number of times the two words appear in the samedocument. It is most common, however, to use smaller contexts, generally a win-dow around the word, for example of 4 words to the left and 4 words to the right,in which case the cell represents the number of times (in some training corpus) thecolumn word occurs in such a±4 word window around the row word. For examplehere is one example each of some words in their windows:is traditionally followed bycherrypie, a traditional dessertoften mixed, such asstrawberryrhubarb pie. Apple piecomputer peripherals and personaldigitalassistants. These devices usuallya computer. This includesinformationavailable on the internetIf we then take every occurrence of each word (saystrawberry) and count thecontext words around it, we get a word-word co-occurrence matrix. Fig.6.6shows asimpliﬁed subset of the word-word co-occurrence matrix for these four words com-puted from the Wikipedia corpus(Davies, 2015).aardvark...computer data result pie sugar...cherry0 ... 2 8 9 442 25 ...strawberry0 ... 0 0 1 60 19 ...digital0 ... 1670 1683 85 5 4 ...information0 ... 3325 3982 378 5 13 ...Figure 6.6Co-occurrence vectors for four words in the Wikipedia corpus, showing six ofthe dimensions (hand-picked for pedagogical purposes). The vector fordigitalis outlined inred. Note that a real vector would have vastly more dimensions and thus be much sparser.Note in Fig.6.6that the two wordscherryandstrawberryare more similar toeach other (bothpieandsugartend to occur in their window) than they are to otherwords likedigital; conversely,digitalandinformationare more similar to each otherthan, say, tostrawberry. Fig.6.7shows a spatial visualization.
100020003000400010002000digital [1683,1670]computer datainformation [3982,3325] 30004000
Figure 6.7A spatial visualization of word vectors fordigitalandinformation, showing justtwo of the dimensions, corresponding to the wordsdataandcomputer.Note that|V|, the length of the vector, is generally the size of the vocabulary, of-ten between 10,000 and 50,000 words (using the most frequent words in the trainingcorpus; keeping words after about the most frequent 50,000 or so is generally nothelpful). Since most of these numbers are zero these aresparsevector representa-tions; there are efﬁcient algorithms for storing and computing with sparse matrices.Now that we have some intuitions, let’s move on to examine the details of com-puting word similarity. Afterwards we’ll discuss methods for weighting cells.

## Page 40

10CHAPTER6•VECTORSEMANTICS ANDEMBEDDINGSaardvark...computer data result pie sugar...cherry0 ... 2 8 9 442 25strawberry0 ... 0 0 1 60 19digital0 ... 1670 1683 85 5 4information0 ... 3325 3982 378 5 13Figure 6.5Co-occurrence vectors for four words in the Wikipedia corpus, showing six ofthe dimensions (hand-picked for pedagogical purposes). The vector fordigitalis outlined inred. Note that a real vector would have vastly more dimensions and thus be much sparser.
100020003000400010002000digital [1683,1670]computer datainformation [3982,3325] 30004000
Figure 6.6A spatial visualization of word vectors fordigitalandinformation, showing justtwo of the dimensions, corresponding to the wordsdataandcomputer.Note that|V|, the length of the vector, is generally the size of the vocabulary,usually between 10,000 and 50,000 words (using the most frequent words in thetraining corpus; keeping words after about the most frequent 50,000 or so is gener-ally not helpful). But of course since most of these numbers are zero these aresparsevector representations, and there are efﬁcient algorithms for storing and computingwith sparse matrices.Now that we have some intuitions, let’s move on to examine the details of com-puting word similarity. Afterwards we’ll discuss the tf-idf method of weightingcells.6.4 Cosine for measuring similarityTo deﬁne similarity between two target wordsvandw, we need a measure for takingtwo such vectors and giving a measure of vector similarity. By far the most commonsimilarity metric is thecosineof the angle between the vectors.The cosine—like most measures for vector similarity used in NLP—is based onthedot productoperator from linear algebra, also called theinner product:dot productinner productdot product(v,w)=v·w=NXi=1viwi=v1w1+v2w2+...+vNwN(6.7)As we will see, most metrics for similarity between vectors are based on the dotproduct. The dot product acts as a similarity metric because it will tend to be highjust when the two vectors have large values in the same dimensions. Alternatively,vectors that have zeros in different dimensions—orthogonal vectors—will have adot product of 0, representing their strong dissimilarity.

## Page 41

Vector Semantics & EmbeddingsWords and Vectors

## Page 42

Vector Semantics & EmbeddingsCosine for computing word similarity

## Page 43

Computing word similarity: Dot product and cosineThe dot product between two vectors is a scalar:The dot product tends to be high when the two vectors have large values in the same dimensionsDot product can thus be a useful similarity metric between vectors10CHAPTER6•VECTORSEMANTICS ANDEMBEDDINGS6.4 Cosine for measuring similarityTo measure similarity between two target wordsvandw, we need a metric thattakes two vectors (of the same dimensionality, either both with words as dimensions,hence of length|V|, or both with documents as dimensions as documents, of length|D|) and gives a measure of their similarity. By far the most common similaritymetric is thecosineof the angle between the vectors.The cosine—like most measures for vector similarity used in NLP—is based onthedot productoperator from linear algebra, also called theinner product:dot productinner productdot product(v,w)=v·w=NXi=1viwi=v1w1+v2w2+...+vNwN(6.7)As we will see, most metrics for similarity between vectors are based on the dotproduct. The dot product acts as a similarity metric because it will tend to be highjust when the two vectors have large values in the same dimensions. Alternatively,vectors that have zeros in different dimensions—orthogonal vectors—will have adot product of 0, representing their strong dissimilarity.This raw dot product, however, has a problem as a similarity metric: it favorslongvectors. Thevector lengthis deﬁned asvector length|v|=vuutNXi=1v2i(6.8)The dot product is higher if a vector is longer, with higher values in each dimension.More frequent words have longer vectors, since they tend to co-occur with morewords and have higher co-occurrence values with each of them. The raw dot productthus will be higher for frequent words. But this is a problem; we’d like a similaritymetric that tells us how similar two words are regardless of their frequency.We modify the dot product to normalize for the vector length by dividing thedot product by the lengths of each of the two vectors. Thisnormalized dot productturns out to be the same as the cosine of the angle between the two vectors, followingfrom the deﬁnition of the dot product between two vectorsaandb:a·b=|a||b|cosqa·b|a||b|=cosq(6.9)Thecosinesimilarity metric between two vectorsvandwthus can be computed as:cosinecosine(v,w)=v·w|v||w|=NXi=1viwivuutNXi=1v2ivuutNXi=1w2i(6.10)For some applications we pre-normalize each vector, by dividing it by its length,creating aunit vectorof length 1. Thus we could compute a unit vector fromabyunit vectordividing it by|a|. For unit vectors, the dot product is the same as the cosine.

## Page 44

Problem with raw dot-productDot product favors long vectorsDot product is higher if a vector is longer (has higher values in many dimension)Vector length:Frequent words (of, the, you) have long vectors (since they occur many times with other words).So dot product overly favors frequent words10CHAPTER6•VECTORSEMANTICS ANDEMBEDDINGS6.4 Cosine for measuring similarityTo measure similarity between two target wordsvandw, we need a metric thattakes two vectors (of the same dimensionality, either both with words as dimensions,hence of length|V|, or both with documents as dimensions as documents, of length|D|) and gives a measure of their similarity. By far the most common similaritymetric is thecosineof the angle between the vectors.The cosine—like most measures for vector similarity used in NLP—is based onthedot productoperator from linear algebra, also called theinner product:dot productinner productdot product(v,w)=v·w=NXi=1viwi=v1w1+v2w2+...+vNwN(6.7)As we will see, most metrics for similarity between vectors are based on the dotproduct. The dot product acts as a similarity metric because it will tend to be highjust when the two vectors have large values in the same dimensions. Alternatively,vectors that have zeros in different dimensions—orthogonal vectors—will have adot product of 0, representing their strong dissimilarity.This raw dot product, however, has a problem as a similarity metric: it favorslongvectors. Thevector lengthis deﬁned asvector length|v|=vuutNXi=1v2i(6.8)The dot product is higher if a vector is longer, with higher values in each dimension.More frequent words have longer vectors, since they tend to co-occur with morewords and have higher co-occurrence values with each of them. The raw dot productthus will be higher for frequent words. But this is a problem; we’d like a similaritymetric that tells us how similar two words are regardless of their frequency.We modify the dot product to normalize for the vector length by dividing thedot product by the lengths of each of the two vectors. Thisnormalized dot productturns out to be the same as the cosine of the angle between the two vectors, followingfrom the deﬁnition of the dot product between two vectorsaandb:a·b=|a||b|cosqa·b|a||b|=cosq(6.9)Thecosinesimilarity metric between two vectorsvandwthus can be computed as:cosinecosine(v,w)=v·w|v||w|=NXi=1viwivuutNXi=1v2ivuutNXi=1w2i(6.10)For some applications we pre-normalize each vector, by dividing it by its length,creating aunit vectorof length 1. Thus we could compute a unit vector fromabyunit vectordividing it by|a|. For unit vectors, the dot product is the same as the cosine.

## Page 45

Alternative: cosine for computing word similarity12CHAPTER6•VECTORSEMANTICS~a·~b=|~a||~b|cosq~a·~b|~a||~b|=cosq(6.9)Thecosinesimilarity metric between two vectors~vand~wthus can be computedcosineas:cosine(~v,~w)=~v·~w|~v||~w|=NXi=1viwivuutNXi=1v2ivuutNXi=1w2i(6.10)For some applications we pre-normalize each vector, by dividing it by its length,creating aunit vectorof length 1. Thus we could compute a unit vector from~abyunit vectordividing it by|~a|. For unit vectors, the dot product is the same as the cosine.The cosine value ranges from 1 for vectors pointing in the same direction, through0 for vectors that are orthogonal, to -1 for vectors pointing in opposite directions.But raw frequency values are non-negative, so the cosine for these vectors rangesfrom 0–1.Let’s see how the cosine computes which of the wordsapricotordigitalis closerin meaning toinformation, just using raw counts from the following simpliﬁed table:large data computerapricot20 0digital01 2information16 1cos(apricot,information)=2+0+0p4+0+0p1+36+1=22p38=.16cos(digital,information)=0+6+2p0+1+4p1+36+1=8p38p5=.58(6.11)The model decides thatinformationis closer todigitalthan it is toapricot,aresult that seems sensible. Fig.6.7shows a visualization.6.5 TF-IDF: Weighing terms in the vectorThe co-occurrence matrix in Fig.6.5represented each cell by the raw frequency ofthe co-occurrence of two words.It turns out, however, that simple frequency isn’t the best measure of associationbetween words. One problem is that raw frequency is very skewed and not verydiscriminative. If we want to know what kinds of contexts are shared byapricotandpineapplebut not bydigitalandinformation, we’re not going to get good discrimi-nation from words likethe,it, orthey, which occur frequently with all sorts of wordsand aren’t informative about any particular word.It’s a bit of a paradox. Word that occur nearby frequently (maybesugarappearsoften in our corpus nearapricot) are more important than words that only appear10CHAPTER6•VECTORSEMANTICS ANDEMBEDDINGS6.4 Cosine for measuring similarityTo measure similarity between two target wordsvandw, we need a metric thattakes two vectors (of the same dimensionality, either both with words as dimensions,hence of length|V|, or both with documents as dimensions as documents, of length|D|) and gives a measure of their similarity. By far the most common similaritymetric is thecosineof the angle between the vectors.The cosine—like most measures for vector similarity used in NLP—is based onthedot productoperator from linear algebra, also called theinner product:dot productinner productdot product(v,w)=v·w=NXi=1viwi=v1w1+v2w2+...+vNwN(6.7)As we will see, most metrics for similarity between vectors are based on the dotproduct. The dot product acts as a similarity metric because it will tend to be highjust when the two vectors have large values in the same dimensions. Alternatively,vectors that have zeros in different dimensions—orthogonal vectors—will have adot product of 0, representing their strong dissimilarity.This raw dot product, however, has a problem as a similarity metric: it favorslongvectors. Thevector lengthis deﬁned asvector length|v|=vuutNXi=1v2i(6.8)The dot product is higher if a vector is longer, with higher values in each dimension.More frequent words have longer vectors, since they tend to co-occur with morewords and have higher co-occurrence values with each of them. The raw dot productthus will be higher for frequent words. But this is a problem; we’d like a similaritymetric that tells us how similar two words are regardless of their frequency.We modify the dot product to normalize for the vector length by dividing thedot product by the lengths of each of the two vectors. Thisnormalized dot productturns out to be the same as the cosine of the angle between the two vectors, followingfrom the deﬁnition of the dot product between two vectorsaandb:a·b=|a||b|cosqa·b|a||b|=cosq(6.9)Thecosinesimilarity metric between two vectorsvandwthus can be computed as:cosinecosine(v,w)=v·w|v||w|=NXi=1viwivuutNXi=1v2ivuutNXi=1w2i(6.10)For some applications we pre-normalize each vector, by dividing it by its length,creating aunit vectorof length 1. Thus we could compute a unit vector fromabyunit vectordividing it by|a|. For unit vectors, the dot product is the same as the cosine.Based on the definition of the dot product between two vectors a and b 

## Page 46

Cosine as a similarity metric-1: vectors point in opposite directions +1:  vectors point in same directions0: vectors are orthogonalBut since raw frequency values are non-negative, the cosine for term-term matrix vectors ranges from 0–1 46

## Page 47

Cosine examplespiedatacomputercherry44282digital516831670information539823325
47cos(v,w)=v•wvw=vv•ww=viwii=1N∑vi2i=1N∑wi2i=1N∑6.4•COSINE FOR MEASURING SIMILARITY11This raw dot product, however, has a problem as a similarity metric: it favorslongvectors. Thevector lengthis deﬁned asvector length|v|=vuutNXi=1v2i(6.8)The dot product is higher if a vector is longer, with higher values in each dimension.More frequent words have longer vectors, since they tend to co-occur with morewords and have higher co-occurrence values with each of them. The raw dot productthus will be higher for frequent words. But this is a problem; we’d like a similaritymetric that tells us how similar two words are regardless of their frequency.The simplest way to modify the dot product to normalize for the vector length isto divide the dot product by the lengths of each of the two vectors. Thisnormalizeddot productturns out to be the same as the cosine of the angle between the twovectors, following from the deﬁnition of the dot product between two vectorsaandb:a·b=|a||b|cosqa·b|a||b|=cosq(6.9)Thecosinesimilarity metric between two vectorsvandwthus can be computed as:cosinecosine(v,w)=v·w|v||w|=NXi=1viwivuutNXi=1v2ivuutNXi=1w2i(6.10)For some applications we pre-normalize each vector, by dividing it by its length,creating aunit vectorof length 1. Thus we could compute a unit vector fromabyunit vectordividing it by|a|. For unit vectors, the dot product is the same as the cosine.The cosine value ranges from 1 for vectors pointing in the same direction, through0 for vectors that are orthogonal, to -1 for vectors pointing in opposite directions.But raw frequency values are non-negative, so the cosine for these vectors rangesfrom 0–1.Let’s see how the cosine computes which of the wordscherryordigitalis closerin meaning toinformation, just using raw counts from the following shortened table:pie data computercherry442 8 2digital5 1683 1670information5 3982 3325cos(cherry,information)=442⇤5+8⇤3982+2⇤3325p4422+82+22p52+39822+33252=.017cos(digital,information)=5⇤5+1683⇤3982+1670⇤3325p52+16832+16702p52+39822+33252=.996The model decides thatinformationis way closer todigitalthan it is tocherry,aresult that seems sensible. Fig.6.7shows a visualization.6.4•COSINE FOR MEASURING SIMILARITY11This raw dot product, however, has a problem as a similarity metric: it favorslongvectors. Thevector lengthis deﬁned asvector length|v|=vuutNXi=1v2i(6.8)The dot product is higher if a vector is longer, with higher values in each dimension.More frequent words have longer vectors, since they tend to co-occur with morewords and have higher co-occurrence values with each of them. The raw dot productthus will be higher for frequent words. But this is a problem; we’d like a similaritymetric that tells us how similar two words are regardless of their frequency.The simplest way to modify the dot product to normalize for the vector length isto divide the dot product by the lengths of each of the two vectors. Thisnormalizeddot productturns out to be the same as the cosine of the angle between the twovectors, following from the deﬁnition of the dot product between two vectorsaandb:a·b=|a||b|cosqa·b|a||b|=cosq(6.9)Thecosinesimilarity metric between two vectorsvandwthus can be computed as:cosinecosine(v,w)=v·w|v||w|=NXi=1viwivuutNXi=1v2ivuutNXi=1w2i(6.10)For some applications we pre-normalize each vector, by dividing it by its length,creating aunit vectorof length 1. Thus we could compute a unit vector fromabyunit vectordividing it by|a|. For unit vectors, the dot product is the same as the cosine.The cosine value ranges from 1 for vectors pointing in the same direction, through0 for vectors that are orthogonal, to -1 for vectors pointing in opposite directions.But raw frequency values are non-negative, so the cosine for these vectors rangesfrom 0–1.Let’s see how the cosine computes which of the wordscherryordigitalis closerin meaning toinformation, just using raw counts from the following shortened table:pie data computercherry442 8 2digital5 1683 1670information5 3982 3325cos(cherry,information)=442⇤5+8⇤3982+2⇤3325p4422+82+22p52+39822+33252=.017cos(digital,information)=5⇤5+1683⇤3982+1670⇤3325p52+16832+16702p52+39822+33252=.996The model decides thatinformationis way closer todigitalthan it is tocherry,aresult that seems sensible. Fig.6.7shows a visualization.6.4•COSINE FOR MEASURING SIMILARITY11This raw dot product, however, has a problem as a similarity metric: it favorslongvectors. Thevector lengthis deﬁned asvector length|v|=vuutNXi=1v2i(6.8)The dot product is higher if a vector is longer, with higher values in each dimension.More frequent words have longer vectors, since they tend to co-occur with morewords and have higher co-occurrence values with each of them. The raw dot productthus will be higher for frequent words. But this is a problem; we’d like a similaritymetric that tells us how similar two words are regardless of their frequency.The simplest way to modify the dot product to normalize for the vector length isto divide the dot product by the lengths of each of the two vectors. Thisnormalizeddot productturns out to be the same as the cosine of the angle between the twovectors, following from the deﬁnition of the dot product between two vectorsaandb:a·b=|a||b|cosqa·b|a||b|=cosq(6.9)Thecosinesimilarity metric between two vectorsvandwthus can be computed as:cosinecosine(v,w)=v·w|v||w|=NXi=1viwivuutNXi=1v2ivuutNXi=1w2i(6.10)For some applications we pre-normalize each vector, by dividing it by its length,creating aunit vectorof length 1. Thus we could compute a unit vector fromabyunit vectordividing it by|a|. For unit vectors, the dot product is the same as the cosine.The cosine value ranges from 1 for vectors pointing in the same direction, through0 for vectors that are orthogonal, to -1 for vectors pointing in opposite directions.But raw frequency values are non-negative, so the cosine for these vectors rangesfrom 0–1.Let’s see how the cosine computes which of the wordscherryordigitalis closerin meaning toinformation, just using raw counts from the following shortened table:pie data computercherry442 8 2digital5 1683 1670information5 3982 3325cos(cherry,information)=442⇤5+8⇤3982+2⇤3325p4422+82+22p52+39822+33252=.017cos(digital,information)=5⇤5+1683⇤3982+1670⇤3325p52+16832+16702p52+39822+33252=.996The model decides thatinformationis way closer todigitalthan it is tocherry,aresult that seems sensible. Fig.6.7shows a visualization.6.4•COSINE FOR MEASURING SIMILARITY11This raw dot product, however, has a problem as a similarity metric: it favorslongvectors. Thevector lengthis deﬁned asvector length|v|=vuutNXi=1v2i(6.8)The dot product is higher if a vector is longer, with higher values in each dimension.More frequent words have longer vectors, since they tend to co-occur with morewords and have higher co-occurrence values with each of them. The raw dot productthus will be higher for frequent words. But this is a problem; we’d like a similaritymetric that tells us how similar two words are regardless of their frequency.The simplest way to modify the dot product to normalize for the vector length isto divide the dot product by the lengths of each of the two vectors. Thisnormalizeddot productturns out to be the same as the cosine of the angle between the twovectors, following from the deﬁnition of the dot product between two vectorsaandb:a·b=|a||b|cosqa·b|a||b|=cosq(6.9)Thecosinesimilarity metric between two vectorsvandwthus can be computed as:cosinecosine(v,w)=v·w|v||w|=NXi=1viwivuutNXi=1v2ivuutNXi=1w2i(6.10)For some applications we pre-normalize each vector, by dividing it by its length,creating aunit vectorof length 1. Thus we could compute a unit vector fromabyunit vectordividing it by|a|. For unit vectors, the dot product is the same as the cosine.The cosine value ranges from 1 for vectors pointing in the same direction, through0 for vectors that are orthogonal, to -1 for vectors pointing in opposite directions.But raw frequency values are non-negative, so the cosine for these vectors rangesfrom 0–1.Let’s see how the cosine computes which of the wordscherryordigitalis closerin meaning toinformation, just using raw counts from the following shortened table:pie data computercherry442 8 2digital5 1683 1670information5 3982 3325cos(cherry,information)=442⇤5+8⇤3982+2⇤3325p4422+82+22p52+39822+33252=.017cos(digital,information)=5⇤5+1683⇤3982+1670⇤3325p52+16832+16702p52+39822+33252=.996The model decides thatinformationis way closer todigitalthan it is tocherry,aresult that seems sensible. Fig.6.7shows a visualization.

## Page 48

Visualizing cosines (well, angles)12CHAPTER6•VECTORSEMANTICS ANDEMBEDDINGS
50010001500200025003000500digitalcherryinformationDimension 1: ‘pie’
Dimension 2: ‘computer’Figure 6.7A (rough) graphical demonstration of cosine similarity, showing vectors forthree words (cherry,digital, andinformation) in the two dimensional space deﬁned by countsof the wordscomputerandpienearby. Note that the angle betweendigitalandinformationissmaller than the angle betweencherryandinformation. When two vectors are more similar,the cosine is larger but the angle is smaller; the cosine has its maximum (1) when the anglebetween two vectors is smallest (0 ); the cosine of all other angles is less than 1.6.5 TF-IDF: Weighing terms in the vectorThe co-occurrence matrix in Fig.6.5represented each cell by the raw frequency ofthe co-occurrence of two words.It turns out, however, that simple frequency isn’t the best measure of associationbetween words. One problem is that raw frequency is very skewed and not verydiscriminative. If we want to know what kinds of contexts are shared bycherryandstrawberrybut not bydigitalandinformation, we’re not going to get good discrimi-nation from words likethe,it, orthey, which occur frequently with all sorts of wordsand aren’t informative about any particular word. We saw this also in Fig.6.3forthe Shakespeare corpus; the dimension for the wordgoodis not very discrimina-tive between plays;goodis simply a frequent word and has roughly equivalent highfrequencies in each of the plays.It’s a bit of a paradox. Words that occur nearby frequently (maybepienearbycherry) are more important than words that only appear once or twice. Yet wordsthat are too frequent—ubiquitous, liketheorgood— are unimportant. How can webalance these two conﬂicting constraints?Thetf-idf algorithm(the ‘-’ here is a hyphen, not a minus sign) is the productof two terms, each term capturing one of these two intuitions:The ﬁrst is theterm frequency(Luhn, 1957): the frequency of the wordtin theterm frequencydocumentd. We can just use the raw count as the term frequency:tft,d=count(t,d)(6.11)Alternatively we can squash the raw frequency a bit, by using the log10of the fre-quency instead. The intuition is that a word appearing 100 times in a documentdoesn’t make that word 100 times more likely to be relevant to the meaning of thedocument. Because we can’t take the log of 0, we normally add 1 to the count:3tft,d=log10(count(t,d)+1)(6.12)If we use log weighting, terms which occur 10 times in a document would have atf=2, 100 times in a document tf=3, 1000 times tf=4, and so on.3Or we can use this alternative: tft,d=⇢1+log10count(t,d)if count(t,d)>00 otherwise

## Page 49

Vector Semantics & EmbeddingsCosine for computing word similarity

## Page 50

Vector Semantics & EmbeddingsTF-IDF

## Page 51

But raw frequency is a bad representation•The co-occurrence matrices we have seen represent each cell by word frequencies.•Frequency is clearly useful; if sugarappears a lot near apricot, that's useful information.•But overly frequent words like the, it,or theyare not very informative about the context•It's a paradox! How can we balance these two conflicting constraints? 

## Page 52

Two common solutions for word weightingtf-idf:     tf-idfvalue for word t in document d:PMI: (Pointwise mutual information)◦PMI𝒘𝟏,𝒘𝟐=𝒍𝒐𝒈𝒑(𝒘𝟏,𝒘𝟐)𝒑𝒘𝟏𝒑(𝒘𝟐)14CHAPTER6•VECTORSEMANTICSCollection FrequencyDocument FrequencyRomeo1131action11331We assign importance to these more discriminative words likeRomeoviatheinverse document frequencyoridfterm weight(Sparck Jones, 1972).idfThe idf is deﬁned using the fractionN/dft, whereNis the total number ofdocuments in the collection, and dftis the number of documents in whichtermtoccurs. The fewer documents in which a term occurs, the higher thisweight. The lowest weight of 1 is assigned to terms that occur in all thedocuments. It’s usually clear what counts as a document: in Shakespearewe would use a play; when processing a collection of encyclopedia articleslike Wikipedia, the document is a Wikipedia page; in processing newspaperarticles, the document is a single article. Occasionally your corpus mightnot have appropriate document divisions and you might need to break up thecorpus into documents yourself for the purposes of computing idf.Because of the large number of documents in many collections, this mea-sure is usually squashed with a log function. The resulting deﬁnition for in-verse document frequency (idf) is thusidft=log10✓Ndft◆(6.12)Here are some idf values for some words in the Shakespeare corpus, rangingfrom extremely informative words which occur in only one play likeRomeo, tothose that occur in a few likesaladorFalstaff, to those which are very common likefoolor so common as to be completely non-discriminative since they occur in all 37plays likegoodorsweet.3WorddfidfRomeo11.57salad21.27Falstaff40.967forest120.489battle210.074fool360.012good370sweet370Thetf-idfweighting of the value for wordtin documentd,wt,dthus combinestf-idfterm frequency with idf:wt,d=tft,d⇥idft(6.13)Fig.6.8applies tf-idf weighting to the Shakespeare term-document matrix in Fig.6.2.Note that the tf-idf values for the dimension corresponding to the wordgoodhavenow all become 0; since this word appears in every document, the tf-idf algorithmleads it to be ignored in any comparison of the plays. Similarly, the wordfool, whichappears in 36 out of the 37 plays, has a much lower weight.The tf-idf weighting is by far the dominant way of weighting co-occurrence ma-trices in information retrieval, but also plays a role in many other aspects of natural3Sweetwas one of Shakespeare’s favorite adjectives, a fact probably related to the increased use ofsugar in European recipes around the turn of the 16th century(Jurafsky, 2014, p. 175).Words like "the" or "it" have very low idfSee if words like "good" appear more often with "great" than we would expect by chance

## Page 53

Term frequency (tf) in the tf-idfalgorithmWe could imagine using raw count:tft,d= count(t,d)But instead of using raw count, we usually squash a bit:12CHAPTER6•VECTORSEMANTICS ANDEMBEDDINGSis not the best measure of association between words. Raw frequency is very skewedand not very discriminative. If we want to know what kinds of contexts are sharedbycherryandstrawberrybut not bydigitalandinformation, we’re not going to getgood discrimination from words likethe,it, orthey, which occur frequently withall sorts of words and aren’t informative about any particular word. We saw thisalso in Fig.6.3for the Shakespeare corpus; the dimension for the wordgoodis notvery discriminative between plays;goodis simply a frequent word and has roughlyequivalent high frequencies in each of the plays.It’s a bit of a paradox. Words that occur nearby frequently (maybepienearbycherry) are more important than words that only appear once or twice. Yet wordsthat are too frequent—ubiquitous, liketheorgood— are unimportant. How can webalance these two conﬂicting constraints?There are two common solutions to this problem: in this section we’ll describethetf-idfweighting, usually used when the dimensions are documents. In the nextwe introduce thePPMIalgorithm (usually used when the dimensions are words).Thetf-idf weighting(the ‘-’ here is a hyphen, not a minus sign) is the productof two terms, each term capturing one of these two intuitions:The ﬁrst is theterm frequency(Luhn,1957): the frequency of the wordtin theterm frequencydocumentd. We can just use the raw count as the term frequency:tft,d=count(t,d)(6.11)More commonly we squash the raw frequency a bit, by using the log10of the fre-quency instead. The intuition is that a word appearing 100 times in a documentdoesn’t make that word 100 times more likely to be relevant to the meaning of thedocument. We also need to do something special with counts of 0, since we can’ttake the log of 0.2tft,d=(1+log10count(t,d)if count(t,d)>00 otherwise(6.12)If we use log weighting, terms which occur 0 times in a document would have tf=0,1 times in a document tf=1+log10(1)=1+0=1, 10 times in a document tf=1+log10(10)=2, 100 times tf=1+log10(100)=3, 1000 times tf=4, and so on.The second factor in tf-idf is used to give a higher weight to words that occuronly in a few documents. Terms that are limited to a few documents are usefulfor discriminating those documents from the rest of the collection; terms that occurfrequently across the entire collection aren’t as helpful. Thedocument frequencydocumentfrequencydftof a termtis the number of documents it occurs in. Document frequency isnot the same as thecollection frequencyof a term, which is the total number oftimes the word appears in the whole collection in any document. Consider in thecollection of Shakespeare’s 37 plays the two wordsRomeoandaction. The wordshave identical collection frequencies (they both occur 113 times in all the plays) butvery different document frequencies, since Romeo only occurs in a single play. Ifour goal is to ﬁnd documents about the romantic tribulations of Romeo, the wordRomeoshould be highly weighted, but notaction:Collection FrequencyDocument FrequencyRomeo1131action113312We can also use this alternative formulation, which we have used in earlier editions: tft,d=log10(count(t,d)+1)

## Page 54

Document frequency (df)dftis the number of documents toccurs in.(note this is not collection frequency: total count across all documents)"Romeo" is very distinctive for one Shakespeare play:12CHAPTER6•VECTORSEMANTICS ANDEMBEDDINGSthat are too frequent—ubiquitous, liketheorgood— are unimportant. How can webalance these two conﬂicting constraints?There are two common solutions to this problem: in this section we’ll describethetf-idfalgorithm, usually used when the dimensions are documents. In the nextwe introduce thePPMIalgorithm (usually used when the dimensions are words).Thetf-idf algorithm(the ‘-’ here is a hyphen, not a minus sign) is the productof two terms, each term capturing one of these two intuitions:The ﬁrst is theterm frequency(Luhn, 1957): the frequency of the wordtin theterm frequencydocumentd. We can just use the raw count as the term frequency:tft,d=count(t,d)(6.11)More commonly we squash the raw frequency a bit, by using the log10of the fre-quency instead. The intuition is that a word appearing 100 times in a documentdoesn’t make that word 100 times more likely to be relevant to the meaning of thedocument. Because we can’t take the log of 0, we normally add 1 to the count:2tft,d=log10(count(t,d)+1)(6.12)If we use log weighting, terms which occur 0 times in a document would havetf=log10(1)=0, 10 times in a document tf=log10(11)=1.4, 100 times tf=log10(101)=2.004, 1000 times tf=3.00044, and so on.The second factor in tf-idf is used to give a higher weight to words that occuronly in a few documents. Terms that are limited to a few documents are usefulfor discriminating those documents from the rest of the collection; terms that occurfrequently across the entire collection aren’t as helpful. Thedocument frequencydocumentfrequencydftof a termtis the number of documents it occurs in. Document frequency isnot the same as thecollection frequencyof a term, which is the total number oftimes the word appears in the whole collection in any document. Consider in thecollection of Shakespeare’s 37 plays the two wordsRomeoandaction. The wordshave identical collection frequencies (they both occur 113 times in all the plays) butvery different document frequencies, since Romeo only occurs in a single play. Ifour goal is to ﬁnd documents about the romantic tribulations of Romeo, the wordRomeoshould be highly weighted, but notaction:Collection FrequencyDocument FrequencyRomeo1131action11331We emphasize discriminative words likeRomeovia theinverse document fre-quencyoridfterm weight(Sparck Jones, 1972). The idf is deﬁned using the frac-idftionN/dft, whereNis the total number of documents in the collection, and dftisthe number of documents in which termtoccurs. The fewer documents in which aterm occurs, the higher this weight. The lowest weight of 1 is assigned to terms thatoccur in all the documents. It’s usually clear what counts as a document: in Shake-speare we would use a play; when processing a collection of encyclopedia articleslike Wikipedia, the document is a Wikipedia page; in processing newspaper articles,the document is a single article. Occasionally your corpus might not have appropri-ate document divisions and you might need to break up the corpus into documentsyourself for the purposes of computing idf.2Or we can use this alternative: tft,d=⇢1+log10count(t,d)if count(t,d)>00 otherwise

## Page 55

Inverse document frequency (idf)6.5•TF-IDF: WEIGHING TERMS IN THE VECTOR13Because of the large number of documents in many collections, this measuretoo is usually squashed with a log function. The resulting deﬁnition for inversedocument frequency (idf) is thusidft=log10✓Ndft◆(6.13)Here are some idf values for some words in the Shakespeare corpus, ranging fromextremely informative words which occur in only one play likeRomeo, to those thatoccur in a few likesaladorFalstaff, to those which are very common likefoolor socommon as to be completely non-discriminative since they occur in all 37 plays likegoodorsweet.3WorddfidfRomeo11.57salad21.27Falstaff40.967forest120.489battle210.246wit340.037fool360.012good370sweet370Thetf-idfweighted valuewt,dfor wordtin documentdthus combines termtf-idffrequency tft,d(deﬁned either by Eq.6.11or by Eq.6.12) with idf from Eq.6.13:wt,d=tft,d⇥idft(6.14)Fig.6.9applies tf-idf weighting to the Shakespeare term-document matrix in Fig.6.2,using the tf equation Eq.6.12. Note that the tf-idf values for the dimension corre-sponding to the wordgoodhave now all become 0; since this word appears in everydocument, the tf-idf algorithm leads it to be ignored. Similarly, the wordfool, whichappears in 36 out of the 37 plays, has a much lower weight.As You Like It Twelfth Night Julius Caesar Henry Vbattle0.074 0 0.22 0.28good00 0 0fool0.019 0.021 0.0036 0.0083wit0.049 0.044 0.018 0.022Figure 6.9A tf-idf weighted term-document matrix for four words in four Shakespeareplays, using the counts in Fig.6.2. For example the 0.049 value forwitinAs You Like Itisthe product of tf=log10(20+1)=1.322 and idf=.037. Note that the idf weighting haseliminated the importance of the ubiquitous wordgoodand vastly reduced the impact of thealmost-ubiquitous wordfool.The tf-idf weighting is the way for weighting co-occurrence matrices in infor-mation retrieval, but also plays a role in many other aspects of natural languageprocessing. It’s also a great baseline, the simple thing to try ﬁrst. We’ll look at otherweightings like PPMI (Positive Pointwise Mutual Information) in Section6.6.3Sweetwas one of Shakespeare’s favorite adjectives, a fact probably related to the increased use ofsugar in European recipes around the turn of the 16th century(Jurafsky, 2014, p. 175).6.5•TF-IDF: WEIGHING TERMS IN THE VECTOR13Because of the large number of documents in many collections, this measuretoo is usually squashed with a log function. The resulting deﬁnition for inversedocument frequency (idf) is thusidft=log10✓Ndft◆(6.13)Here are some idf values for some words in the Shakespeare corpus, ranging fromextremely informative words which occur in only one play likeRomeo, to those thatoccur in a few likesaladorFalstaff, to those which are very common likefoolor socommon as to be completely non-discriminative since they occur in all 37 plays likegoodorsweet.3WorddfidfRomeo11.57salad21.27Falstaff40.967forest120.489battle210.246wit340.037fool360.012good370sweet370Thetf-idfweighted valuewt,dfor wordtin documentdthus combines termtf-idffrequency tft,d(deﬁned either by Eq.6.11or by Eq.6.12) with idf from Eq.6.13:wt,d=tft,d⇥idft(6.14)Fig.6.9applies tf-idf weighting to the Shakespeare term-document matrix in Fig.6.2,using the tf equation Eq.6.12. Note that the tf-idf values for the dimension corre-sponding to the wordgoodhave now all become 0; since this word appears in everydocument, the tf-idf algorithm leads it to be ignored. Similarly, the wordfool, whichappears in 36 out of the 37 plays, has a much lower weight.As You Like It Twelfth Night Julius Caesar Henry Vbattle0.074 0 0.22 0.28good00 0 0fool0.019 0.021 0.0036 0.0083wit0.049 0.044 0.018 0.022Figure 6.9A tf-idf weighted term-document matrix for four words in four Shakespeareplays, using the counts in Fig.6.2. For example the 0.049 value forwitinAs You Like Itisthe product of tf=log10(20+1)=1.322 and idf=.037. Note that the idf weighting haseliminated the importance of the ubiquitous wordgoodand vastly reduced the impact of thealmost-ubiquitous wordfool.The tf-idf weighting is the way for weighting co-occurrence matrices in infor-mation retrieval, but also plays a role in many other aspects of natural languageprocessing. It’s also a great baseline, the simple thing to try ﬁrst. We’ll look at otherweightings like PPMI (Positive Pointwise Mutual Information) in Section6.6.3Sweetwas one of Shakespeare’s favorite adjectives, a fact probably related to the increased use ofsugar in European recipes around the turn of the 16th century(Jurafsky, 2014, p. 175).N is the total number of documents in the collection

## Page 56

What is a document?Could be a play or a Wikipedia articleBut for the purposes of tf-idf, documents can be anything; we often call each paragraph a document!

## Page 57

Final tf-idfweighted value for a wordRaw counts:tf-idf:6.5•TF-IDF: WEIGHING TERMS IN THE VECTOR13Because of the large number of documents in many collections, this measuretoo is usually squashed with a log function. The resulting deﬁnition for inversedocument frequency (idf) is thusidft=log10✓Ndft◆(6.13)Here are some idf values for some words in the Shakespeare corpus, ranging fromextremely informative words which occur in only one play likeRomeo, to those thatoccur in a few likesaladorFalstaff, to those which are very common likefoolor socommon as to be completely non-discriminative since they occur in all 37 plays likegoodorsweet.3WorddfidfRomeo11.57salad21.27Falstaff40.967forest120.489battle210.246wit340.037fool360.012good370sweet370Thetf-idfweighted valuewt,dfor wordtin documentdthus combines termtf-idffrequency tft,d(deﬁned either by Eq.6.11or by Eq.6.12) with idf from Eq.6.13:wt,d=tft,d⇥idft(6.14)Fig.6.9applies tf-idf weighting to the Shakespeare term-document matrix in Fig.6.2,using the tf equation Eq.6.12. Note that the tf-idf values for the dimension corre-sponding to the wordgoodhave now all become 0; since this word appears in everydocument, the tf-idf algorithm leads it to be ignored. Similarly, the wordfool, whichappears in 36 out of the 37 plays, has a much lower weight.As You Like It Twelfth Night Julius Caesar Henry Vbattle0.074 0 0.22 0.28good00 0 0fool0.019 0.021 0.0036 0.0083wit0.049 0.044 0.018 0.022Figure 6.9A tf-idf weighted term-document matrix for four words in four Shakespeareplays, using the counts in Fig.6.2. For example the 0.049 value forwitinAs You Like Itisthe product of tf=log10(20+1)=1.322 and idf=.037. Note that the idf weighting haseliminated the importance of the ubiquitous wordgoodand vastly reduced the impact of thealmost-ubiquitous wordfool.The tf-idf weighting is the way for weighting co-occurrence matrices in infor-mation retrieval, but also plays a role in many other aspects of natural languageprocessing. It’s also a great baseline, the simple thing to try ﬁrst. We’ll look at otherweightings like PPMI (Positive Pointwise Mutual Information) in Section6.6.3Sweetwas one of Shakespeare’s favorite adjectives, a fact probably related to the increased use ofsugar in European recipes around the turn of the 16th century(Jurafsky, 2014, p. 175).6.3•WORDS ANDVECTORS7As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.2The term-document matrix for four words in four Shakespeare plays. Each cellcontains the number of times the (row) word occurs in the (column) document.represented as a count vector, a column in Fig.6.3.To review some basic linear algebra, avectoris, at heart, just a list or array ofvectornumbers. SoAs You Like Itis represented as the list [1,114,36,20] (the ﬁrstcolumnvectorin Fig.6.3) andJulius Caesaris represented as the list [7,62,1,2] (the thirdcolumn vector). Avector spaceis a collection of vectors, characterized by theirvector spacedimension. In the example in Fig.6.3, the document vectors are of dimension 4,dimensionjust so they ﬁt on the page; in real term-document matrices, the vectors representingeach document would have dimensionality|V|, the vocabulary size.The ordering of the numbers in a vector space indicates different meaningful di-mensions on which documents vary. Thus the ﬁrst dimension for both these vectorscorresponds to the number of times the wordbattleoccurs, and we can compareeach dimension, noting for example that the vectors forAs You Like ItandTwelfthNighthave similar values (1 and 0, respectively) for the ﬁrst dimension.As You Like It Twelfth Night Julius Caesar Henry Vbattle1 0 7 13good114 80 62 89fool36 58 1 4wit20 15 2 3Figure 6.3The term-document matrix for four words in four Shakespeare plays. The redboxes show that each document is represented as a column vector of length four.We can think of the vector for a document as a point in|V|-dimensional space;thus the documents in Fig.6.3are points in 4-dimensional space. Since 4-dimensionalspaces are hard to visualize, Fig.6.4shows a visualization in two dimensions; we’vearbitrarily chosen the dimensions corresponding to the wordsbattleandfool.
51015202530510Henry V [4,13]As You Like It [36,1]Julius Caesar [1,7]battle foolTwelfth Night [58,0]1540
354045505560Figure 6.4A spatial visualization of the document vectors for the four Shakespeare playdocuments, showing just two of the dimensions, corresponding to the wordsbattleandfool.The comedies have high values for thefooldimension and low values for thebattledimension.Term-document matrices were originally deﬁned as a means of ﬁnding similardocuments for the task of documentinformation retrieval. Two documents that are14CHAPTER6•VECTORSEMANTICS ANDEMBEDDINGSAs You Like It Twelfth Night Julius Caesar Henry Vbattle0.246 0 0.454 0.520good00 0 0fool0.030 0.033 0.0012 0.0019wit0.085 0.081 0.048 0.054Figure 6.9A tf-idf weighted term-document matrix for four words in four Shakespeareplays, using the counts in Fig.6.2. For example the 0.085 value forwitinAs You Like Itisthe product of tf=1+log10(20)=2.301 and idf=.037. Note that the idf weighting haseliminated the importance of the ubiquitous wordgoodand vastly reduced the impact of thealmost-ubiquitous wordfool.6.6 Pointwise Mutual Information (PMI)An alternative weighting function to tf-idf, PPMI (positive pointwise mutual infor-mation), is used for term-term-matrices, when the vector dimensions correspond towords rather than documents. PPMI draws on the intuition that the best way to weighthe association between two words is to ask how muchmorethe two words co-occurin our corpus than we would have a priori expected them to appear by chance.Pointwise mutual information(Fano,1961)4is one of the most important con-pointwisemutualinformationcepts in NLP. It is a measure of how often two eventsxandyoccur, compared withwhat we would expect if they were independent:I(x,y)=log2P(x,y)P(x)P(y)(6.16)The pointwise mutual information between a target wordwand a context wordc(Church and Hanks1989,Church and Hanks1990) is then deﬁned as:PMI(w,c)=log2P(w,c)P(w)P(c)(6.17)The numerator tells us how often we observed the two words together (assumingwe compute probability by using the MLE). The denominator tells us how oftenwe wouldexpectthe two words to co-occur assuming they each occurred indepen-dently; recall that the probability of two independent events both occurring is justthe product of the probabilities of the two events. Thus, the ratio gives us an esti-mate of how much more the two words co-occur than we expect by chance. PMI isa useful tool whenever we need to ﬁnd words that are strongly associated.PMI values range from negative to positive inﬁnity. But negative PMI values(which imply things are co-occurringless oftenthan we would expect by chance)tend to be unreliable unless our corpora are enormous. To distinguish whethertwo words whose individual probability is each 10 6occur together less often thanchance, we would need to be certain that the probability of the two occurring to-gether is signiﬁcantly less than 10 12, and this kind of granularity would require anenormous corpus. Furthermore it’s not clear whether it’s even possible to evaluatesuch scores of ‘unrelatedness’ with human judgments. For this reason it is more4PMI is based on themutual informationbetween two random variablesXandY, deﬁned as:I(X,Y)=XxXyP(x,y)log2P(x,y)P(x)P(y)(6.15)In a confusion of terminology, Fano used the phrasemutual informationto refer to what we now callpointwise mutual informationand the phraseexpectation of the mutual informationfor what we now callmutual information

## Page 58

Vector Semantics & EmbeddingsTF-IDF

## Page 59

Vector Semantics & EmbeddingsWord2vec

## Page 60

Sparse versus dense vectorstf-idf(or PMI) vectors are◦long(length |V|= 20,000 to 50,000)◦sparse (most elements are zero)Alternative: learn vectors which are◦short(length 50-1000)◦dense(most elements are non-zero)

## Page 61

Sparse versus dense vectorsWhy dense vectors?◦Short vectors may be easier to use as featuresin machine learning (fewer weights to tune)◦Dense vectors may generalizebetter than explicit counts◦Dense vectors may do better at capturing synonymy:◦carand automobileare synonyms; but are distinct dimensions◦a word with caras a neighbor and a word with automobileas a neighbor should be similar, but aren't◦In practice, they work better61

## Page 62

Common methods for getting short dense vectors“Neural Language Model”-inspired models◦Word2vec (skipgram, CBOW), GloVeSingular Value Decomposition (SVD)◦A special case of this is called LSA –Latent Semantic AnalysisAlternative to these "static embeddings":•Contextual Embeddings (ELMo, BERT)•Compute distinct embeddings for a word in its context•Separate embeddings for each token of a word

## Page 63

Simple static embeddings you can download!Word2vec (Mikolovet al)https://code.google.com/archive/p/word2vec/GloVe(Pennington, Socher, Manning)http://nlp.stanford.edu/projects/glove/

## Page 64

Word2vecPopular embedding methodVery fast to trainCode available on the webIdea: predictrather than countWord2vec provides various options. We'll do:skip-gram with negative sampling (SGNS)

## Page 65

Word2vecInstead of countinghow often each word woccurs near "apricot"◦Train a classifier on a binary predictiontask:◦Is w likely to show up near "apricot"?We don’t actually care about this task◦But we'll take the learned classifier weights as the word embeddingsBig idea:  self-supervision: ◦A word c that occurs near apricot in the corpus cats as the gold "correct answer" for supervised learning◦No need for human labels◦Bengioet al. (2003); Collobertet al. (2011) 

## Page 66

Approach: predict if candidate word cis a "neighbor"1.Treat the target word tand a neighboring context word cas positive examples.2.Randomly sample other words in the lexicon to get negative examples3.Use logistic regression to train a classifier to distinguish those two cases4.Use the learned weights as the embeddings

## Page 67

Skip-Gram Training DataAssume a +/-2 word window, given training sentence:…lemon, a [tablespoon of  apricot  jam,   a]  pinch…c1                   c2 c3      c4                                 [target]

## Page 68

Skip-Gram Classifier(assuming a +/-2 word window)…lemon, a [tablespoon of  apricot  jam,   a]  pinch…c1                   c2 [target]c3      c4Goal: train a classifier that is given a candidate (word, context) pair(apricot, jam)(apricot, aardvark)…And assigns each pair a probability:P(+|w, c) P(−|w, c) = 1 − P(+|w, c) 

## Page 69

Similarity is computed from dot productRemember: two vectors are similar if they have a high dot product◦Cosine is just a normalized dot productSo:◦Similarity(w,c)  ∝w· cWe’ll need to normalize to get a probability ◦(cosine isn't a probability either)69

## Page 70

Turning dot products into probabilitiesSim(w,c) ≈ w· cTo turn this into a probability We'll use the sigmoid from logistic regression:6.8•WORD2VEC19We model the probability that wordcis a real context word for target wordwas:P(+|w,c)=s(c·w)=11+exp( c·w)(6.28)The sigmoid function returns a number between 0 and 1, but to make it a probabilitywe’ll also need the total probability of the two possible events (cis a context word,andcisn’t a context word) to sum to 1. We thus estimate the probability that wordcis not a real context word forwas:P( |w,c)=1 P(+|w,c)=s( c·w)=11+exp(c·w)(6.29)Equation6.28gives us the probability for one word, but there are many contextwords in the window. Skip-gram makes the simplifying assumption that all contextwords are independent, allowing us to just multiply their probabilities:P(+|w,c1:L)=LYi=1s( ci·w)(6.30)logP(+|w,c1:L)=LXi=1logs( ci·w)(6.31)In summary, skip-gram trains a probabilistic classiﬁer that, given a test target wordwand its context window ofLwordsc1:L, assigns a probability based on how similarthis context window is to the target word. The probability is based on applying thelogistic (sigmoid) function to the dot product of the embeddings of the target wordwith each context word. To compute this probability, we just need embeddings foreach target word and context word in the vocabulary.1WCaardvark
zebrazebraaardvarkapricotapricot|V||V|+12V& =target wordscontext & noisewords…
…1..d…
…Figure 6.13The embeddings learned by the skipgram model. The algorithm stores twoembeddings for each word, the target embedding (sometimes called the input embedding)and the context embedding (sometimes called the output embedding). The parameterqthatthe algorithm learns is thus a matrix of 2|V|vectors, each of dimensiond, formed by concate-nating two matrices, the target embeddingsWand the context+noise embeddingsC.Fig.6.13shows the intuition of the parameters we’ll need. Skip-gram actuallystores two embeddings for each word, one for the word as a target, and one for the6.8•WORD2VEC19We model the probability that wordcis a real context word for target wordwas:P(+|w,c)=s(c·w)=11+exp( c·w)(6.28)The sigmoid function returns a number between 0 and 1, but to make it a probabilitywe’ll also need the total probability of the two possible events (cis a context word,andcisn’t a context word) to sum to 1. We thus estimate the probability that wordcis not a real context word forwas:P( |w,c)=1 P(+|w,c)=s( c·w)=11+exp(c·w)(6.29)Equation6.28gives us the probability for one word, but there are many contextwords in the window. Skip-gram makes the simplifying assumption that all contextwords are independent, allowing us to just multiply their probabilities:P(+|w,c1:L)=LYi=1s( ci·w)(6.30)logP(+|w,c1:L)=LXi=1logs( ci·w)(6.31)In summary, skip-gram trains a probabilistic classiﬁer that, given a test target wordwand its context window ofLwordsc1:L, assigns a probability based on how similarthis context window is to the target word. The probability is based on applying thelogistic (sigmoid) function to the dot product of the embeddings of the target wordwith each context word. To compute this probability, we just need embeddings foreach target word and context word in the vocabulary.1WCaardvark
zebrazebraaardvarkapricotapricot|V||V|+12V& =target wordscontext & noisewords…
…1..d…
…Figure 6.13The embeddings learned by the skipgram model. The algorithm stores twoembeddings for each word, the target embedding (sometimes called the input embedding)and the context embedding (sometimes called the output embedding). The parameterqthatthe algorithm learns is thus a matrix of 2|V|vectors, each of dimensiond, formed by concate-nating two matrices, the target embeddingsWand the context+noise embeddingsC.Fig.6.13shows the intuition of the parameters we’ll need. Skip-gram actuallystores two embeddings for each word, one for the word as a target, and one for the

## Page 71

How Skip-Gram Classifier computes P(+|w, c) This is for one context word, but we have lots of context words.We'll assume independence and just multiply them:6.8•WORD2VEC19We model the probability that wordcis a real context word for target wordwas:P(+|w,c)=s(c·w)=11+exp( c·w)(6.28)The sigmoid function returns a number between 0 and 1, but to make it a probabilitywe’ll also need the total probability of the two possible events (cis a context word,andcisn’t a context word) to sum to 1. We thus estimate the probability that wordcis not a real context word forwas:P( |w,c)=1 P(+|w,c)=s( c·w)=11+exp(c·w)(6.29)Equation6.28gives us the probability for one word, but there are many contextwords in the window. Skip-gram makes the simplifying assumption that all contextwords are independent, allowing us to just multiply their probabilities:P(+|w,c1:L)=LYi=1s( ci·w)(6.30)logP(+|w,c1:L)=LXi=1logs( ci·w)(6.31)In summary, skip-gram trains a probabilistic classiﬁer that, given a test target wordwand its context window ofLwordsc1:L, assigns a probability based on how similarthis context window is to the target word. The probability is based on applying thelogistic (sigmoid) function to the dot product of the embeddings of the target wordwith each context word. To compute this probability, we just need embeddings foreach target word and context word in the vocabulary.1WCaardvark
zebrazebraaardvarkapricotapricot|V||V|+12V& =target wordscontext & noisewords…
…1..d…
…Figure 6.13The embeddings learned by the skipgram model. The algorithm stores twoembeddings for each word, the target embedding (sometimes called the input embedding)and the context embedding (sometimes called the output embedding). The parameterqthatthe algorithm learns is thus a matrix of 2|V|vectors, each of dimensiond, formed by concate-nating two matrices, the target embeddingsWand the context+noise embeddingsC.Fig.6.13shows the intuition of the parameters we’ll need. Skip-gram actuallystores two embeddings for each word, one for the word as a target, and one for the6.8•WORD2VEC19We model the probability that wordcis a real context word for target wordwas:P(+|w,c)=s(c·w)=11+exp( c·w)(6.28)The sigmoid function returns a number between 0 and 1, but to make it a probabilitywe’ll also need the total probability of the two possible events (cis a context word,andcisn’t a context word) to sum to 1. We thus estimate the probability that wordcis not a real context word forwas:P( |w,c)=1 P(+|w,c)=s( c·w)=11+exp(c·w)(6.29)Equation6.28gives us the probability for one word, but there are many contextwords in the window. Skip-gram makes the simplifying assumption that all contextwords are independent, allowing us to just multiply their probabilities:P(+|w,c1:L)=LYi=1s(ci·w)(6.30)logP(+|w,c1:L)=LXi=1logs(ci·w)(6.31)In summary, skip-gram trains a probabilistic classiﬁer that, given a test target wordwand its context window ofLwordsc1:L, assigns a probability based on how similarthis context window is to the target word. The probability is based on applying thelogistic (sigmoid) function to the dot product of the embeddings of the target wordwith each context word. To compute this probability, we just need embeddings foreach target word and context word in the vocabulary.1WCaardvark
zebrazebraaardvarkapricotapricot|V||V|+12V& =target wordscontext & noisewords…
…1..d…
…Figure 6.13The embeddings learned by the skipgram model. The algorithm stores twoembeddings for each word, the target embedding (sometimes called the input embedding)and the context embedding (sometimes called the output embedding). The parameterqthatthe algorithm learns is thus a matrix of 2|V|vectors, each of dimensiond, formed by concate-nating two matrices, the target embeddingsWand the context+noise embeddingsC.Fig.6.13shows the intuition of the parameters we’ll need. Skip-gram actuallystores two embeddings for each word, one for the word as a target, and one for the

## Page 72

Skip-gram classifier: summaryA probabilistic classifier, given •a test target word w •its context window of L words c1:LEstimates probability that w occurs in this window based on similarity of w (embeddings) to c1:L(embeddings).To compute this, we just need embeddings for all the words.

## Page 73

These embeddings we'll need: a set for w, a set for c1WCaardvark
zebrazebraaardvarkapricotapricot|V||V|+12V& =target wordscontext & noisewords…
…1..d…
…

## Page 74

Vector Semantics & EmbeddingsWord2vec

## Page 75

Vector Semantics & EmbeddingsWord2vec: Learning the embeddings

## Page 76

Skip-Gram Training data…lemon, a [tablespoon of  apricot  jam,   a]  pinch…c1                   c2 [target]c3      c4
7620CHAPTER6•VECTORSEMANTICS ANDEMBEDDINGSthis context window is to the target word. The probability is based on applying thelogistic (sigmoid) function to the dot product of the embeddings of the target wordwith each context word. We could thus compute this probability if only we hadembeddings for each target word and context word in the vocabulary. Let’s now turnto learning these embeddings (which is the real goal of training this classiﬁer in theﬁrst place).6.8.2 Learning skip-gram embeddingsWord2vec learns embeddings by starting with an initial set of embedding vectorsand then iteratively shifting the embedding of each wordwto be more like the em-beddings of words that occur nearby in texts, and less like the embeddings of wordsthat don’t occur nearby. Let’s start by considering a single piece of training data:... lemon, a [tablespoon of apricot jam, a] pinch ...c1 c2 t c3 c4This example has a target wordt(apricot), and 4 context words in theL=±2window, resulting in 4 positive training instances (on the left below):positive examples +tcapricot tablespoonapricot ofapricot jamapricot anegative examples -tc tcapricot aardvark apricot sevenapricot my apricot foreverapricot where apricot dearapricot coaxial apricot ifFor training a binary classiﬁer we also need negative examples. In fact skip-gram uses more negative examples than positive examples (with the ratio betweenthem set by a parameterk). So for each of these(t,c)training instances we’ll createknegative samples, each consisting of the targettplus a ‘noise word’. A noise wordis a random word from the lexicon, constrained not to be the target wordt. Theright above shows the setting wherek=2, so we’ll have 2 negative examples in thenegative training set for each positive examplet,c.The noise words are chosen according to their weighted unigram frequencypa(w), whereais a weight. If we were sampling according to unweighted fre-quencyp(w), it would mean that with unigram probabilityp(“the”)we would choosethe wordtheas a noise word, with unigram probabilityp(“aardvark”)we wouldchooseaardvark, and so on. But in practice it is common to seta=.75, i.e. use theweightingp34(w):Pa(w)=count(w)aPw0count(w0)a(6.32)Settinga=.75 gives better performance because it gives rare noise words slightlyhigher probability: for rare words,Pa(w)>P(w). To visualize this intuition, itmight help to work out the probabilities for an example with two events,P(a)=.99andP(b)=.01:Pa(a)=.99.75.99.75+.01.75=.97Pa(b)=.01.75.99.75+.01.75=.03(6.33)

## Page 77

Skip-Gram Training data…lemon, a [tablespoon of  apricot  jam,   a]  pinch…c1                   c2 [target]c3      c4
77For each positive example we'll grab k negative examples, sampling by frequency20CHAPTER6•VECTORSEMANTICS ANDEMBEDDINGSthis context window is to the target word. The probability is based on applying thelogistic (sigmoid) function to the dot product of the embeddings of the target wordwith each context word. We could thus compute this probability if only we hadembeddings for each target word and context word in the vocabulary. Let’s now turnto learning these embeddings (which is the real goal of training this classiﬁer in theﬁrst place).6.8.2 Learning skip-gram embeddingsWord2vec learns embeddings by starting with an initial set of embedding vectorsand then iteratively shifting the embedding of each wordwto be more like the em-beddings of words that occur nearby in texts, and less like the embeddings of wordsthat don’t occur nearby. Let’s start by considering a single piece of training data:... lemon, a [tablespoon of apricot jam, a] pinch ...c1 c2 t c3 c4This example has a target wordt(apricot), and 4 context words in theL=±2window, resulting in 4 positive training instances (on the left below):positive examples +tcapricot tablespoonapricot ofapricot jamapricot anegative examples -tc tcapricot aardvark apricot sevenapricot my apricot foreverapricot where apricot dearapricot coaxial apricot ifFor training a binary classiﬁer we also need negative examples. In fact skip-gram uses more negative examples than positive examples (with the ratio betweenthem set by a parameterk). So for each of these(t,c)training instances we’ll createknegative samples, each consisting of the targettplus a ‘noise word’. A noise wordis a random word from the lexicon, constrained not to be the target wordt. Theright above shows the setting wherek=2, so we’ll have 2 negative examples in thenegative training set for each positive examplet,c.The noise words are chosen according to their weighted unigram frequencypa(w), whereais a weight. If we were sampling according to unweighted fre-quencyp(w), it would mean that with unigram probabilityp(“the”)we would choosethe wordtheas a noise word, with unigram probabilityp(“aardvark”)we wouldchooseaardvark, and so on. But in practice it is common to seta=.75, i.e. use theweightingp34(w):Pa(w)=count(w)aPw0count(w0)a(6.32)Settinga=.75 gives better performance because it gives rare noise words slightlyhigher probability: for rare words,Pa(w)>P(w). To visualize this intuition, itmight help to work out the probabilities for an example with two events,P(a)=.99andP(b)=.01:Pa(a)=.99.75.99.75+.01.75=.97Pa(b)=.01.75.99.75+.01.75=.03(6.33)

## Page 78

Skip-Gram Training data…lemon, a [tablespoon of  apricot  jam,   a]  pinch…c1                   c2 [target]c3      c4
7820CHAPTER6•VECTORSEMANTICS ANDEMBEDDINGSthis context window is to the target word. The probability is based on applying thelogistic (sigmoid) function to the dot product of the embeddings of the target wordwith each context word. We could thus compute this probability if only we hadembeddings for each target word and context word in the vocabulary. Let’s now turnto learning these embeddings (which is the real goal of training this classiﬁer in theﬁrst place).6.8.2 Learning skip-gram embeddingsWord2vec learns embeddings by starting with an initial set of embedding vectorsand then iteratively shifting the embedding of each wordwto be more like the em-beddings of words that occur nearby in texts, and less like the embeddings of wordsthat don’t occur nearby. Let’s start by considering a single piece of training data:... lemon, a [tablespoon of apricot jam, a] pinch ...c1 c2 t c3 c4This example has a target wordt(apricot), and 4 context words in theL=±2window, resulting in 4 positive training instances (on the left below):positive examples +tcapricot tablespoonapricot ofapricot jamapricot anegative examples -tc tcapricot aardvark apricot sevenapricot my apricot foreverapricot where apricot dearapricot coaxial apricot ifFor training a binary classiﬁer we also need negative examples. In fact skip-gram uses more negative examples than positive examples (with the ratio betweenthem set by a parameterk). So for each of these(t,c)training instances we’ll createknegative samples, each consisting of the targettplus a ‘noise word’. A noise wordis a random word from the lexicon, constrained not to be the target wordt. Theright above shows the setting wherek=2, so we’ll have 2 negative examples in thenegative training set for each positive examplet,c.The noise words are chosen according to their weighted unigram frequencypa(w), whereais a weight. If we were sampling according to unweighted fre-quencyp(w), it would mean that with unigram probabilityp(“the”)we would choosethe wordtheas a noise word, with unigram probabilityp(“aardvark”)we wouldchooseaardvark, and so on. But in practice it is common to seta=.75, i.e. use theweightingp34(w):Pa(w)=count(w)aPw0count(w0)a(6.32)Settinga=.75 gives better performance because it gives rare noise words slightlyhigher probability: for rare words,Pa(w)>P(w). To visualize this intuition, itmight help to work out the probabilities for an example with two events,P(a)=.99andP(b)=.01:Pa(a)=.99.75.99.75+.01.75=.97Pa(b)=.01.75.99.75+.01.75=.03(6.33)20CHAPTER6•VECTORSEMANTICS ANDEMBEDDINGSthis context window is to the target word. The probability is based on applying thelogistic (sigmoid) function to the dot product of the embeddings of the target wordwith each context word. We could thus compute this probability if only we hadembeddings for each target word and context word in the vocabulary. Let’s now turnto learning these embeddings (which is the real goal of training this classiﬁer in theﬁrst place).6.8.2 Learning skip-gram embeddingsWord2vec learns embeddings by starting with an initial set of embedding vectorsand then iteratively shifting the embedding of each wordwto be more like the em-beddings of words that occur nearby in texts, and less like the embeddings of wordsthat don’t occur nearby. Let’s start by considering a single piece of training data:... lemon, a [tablespoon of apricot jam, a] pinch ...c1 c2 t c3 c4This example has a target wordt(apricot), and 4 context words in theL=±2window, resulting in 4 positive training instances (on the left below):positive examples +tcapricot tablespoonapricot ofapricot jamapricot anegative examples -tc tcapricot aardvark apricot sevenapricot my apricot foreverapricot where apricot dearapricot coaxial apricot ifFor training a binary classiﬁer we also need negative examples. In fact skip-gram uses more negative examples than positive examples (with the ratio betweenthem set by a parameterk). So for each of these(t,c)training instances we’ll createknegative samples, each consisting of the targettplus a ‘noise word’. A noise wordis a random word from the lexicon, constrained not to be the target wordt. Theright above shows the setting wherek=2, so we’ll have 2 negative examples in thenegative training set for each positive examplet,c.The noise words are chosen according to their weighted unigram frequencypa(w), whereais a weight. If we were sampling according to unweighted fre-quencyp(w), it would mean that with unigram probabilityp(“the”)we would choosethe wordtheas a noise word, with unigram probabilityp(“aardvark”)we wouldchooseaardvark, and so on. But in practice it is common to seta=.75, i.e. use theweightingp34(w):Pa(w)=count(w)aPw0count(w0)a(6.32)Settinga=.75 gives better performance because it gives rare noise words slightlyhigher probability: for rare words,Pa(w)>P(w). To visualize this intuition, itmight help to work out the probabilities for an example with two events,P(a)=.99andP(b)=.01:Pa(a)=.99.75.99.75+.01.75=.97Pa(b)=.01.75.99.75+.01.75=.03(6.33)

## Page 79

Word2vec: how to learn vectorsGiven the set of positive and negative training instances, and an initial set of embedding vectors The goal of learning is to adjust those word vectors such that we:◦Maximizethe similarity of the target word, context wordpairs (w , cpos) drawn from the positive data◦Minimizethe similarity of the (w , cneg) pairs drawn from the negative data. 2/3/2479

## Page 80

Loss function for one w with cpos, cneg1...cnegkMaximize the similarity of the target with the actual context words, and minimize the similarity of the target with the k negative sampled non-neighbor words. 6.8•WORD2VEC21Given the set of positive and negative training instances, and an initial set of embed-dings, the goal of the learning algorithm is to adjust those embeddings to•Maximize the similarity of the target word, context word pairs(w,cpos)drawnfrom the positive examples•Minimize the similarity of the(w,cneg)pairs from the negative examples.If we consider one word/context pair(w,cpos)with itsknoise wordscneg1...cnegk,we can express these two goals as the following loss functionLto be minimized(hence the ); here the ﬁrst term expresses that we want the classiﬁer to assign thereal context wordcposa high probability of being a neighbor, and the second termexpresses that we want to assign each of the noise wordscnegia high probability ofbeing a non-neighbor, all multiplied because we assume independence:LCE= log"P(+|w,cpos)kYi=1P( |w,cnegi)#= "logP(+|w,cpos)+kXi=1logP( |w,cnegi)#= "logP(+|w,cpos)+kXi=1log 1 P(+|w,cnegi) #= "logs(cpos·w)+kXi=1logs( cnegi·w)#(6.34)That is, we want to maximize the dot product of the word with the actual contextwords, and minimize the dot products of the word with theknegative sampled non-neighbor words.We minimize this loss function using stochastic gradient descent. Fig.6.14shows the intuition of one step of learning.WCmove apricot and jam closer,increasing cpos z waardvark
move apricot and matrix apartdecreasing cneg1 z w“…apricot jam…”w
zebrazebraaardvarkjamapricot
cposmatrixTolstoymove apricot and Tolstoy apartdecreasing cneg2 z w!cneg1cneg2k=2Figure 6.14Intuition of one step of gradient descent. The skip-gram model tries to shiftembeddings so the target embeddings (here forapricot) are closer to (have a higher dot prod-uct with) context embeddings for nearby words (herejam) and further from (lower dot productwith) context embeddings for noise words that don’t occur nearby (hereTolstoyandmatrix).To get the gradient, we need to take the derivative of Eq.6.34with respect tothe different embeddings. It turns out the derivatives are the following (we leave the

## Page 81

Learning the classifierHow to learn?◦Stochastic gradient descent!We’ll adjust the word weights to◦make the positive pairs more likely ◦and the negative pairs less likely, ◦over the entire training set.

## Page 82

Intuition of one step of gradient descentWCmove apricot and jam closer,increasing cpos z waardvark
move apricot and matrix apartdecreasing cneg1 z w“…apricot jam…”w
zebrazebraaardvarkjamapricot
cposmatrixTolstoymove apricot and Tolstoy apartdecreasing cneg2 z w!cneg1cneg2k=2

## Page 83

Reminder: gradient descent•At each step•Direction: We move in the reverse direction from the gradient of the loss function•Magnitude: we move the value of this gradient !!"𝐿(𝑓𝑥;𝑤,𝑦)weighted by a learning rate η •Higher learning rate means move wfaster10CHAPTER5•LOGISTICREGRESSIONexample):wt+1=wt hddwL(f(x;w),y)(5.14)Now let’s extend the intuition from a function of one scalar variablewto manyvariables, because we don’t just want to move left or right, we want to know wherein the N-dimensional space (of theNparameters that make upq) we should move.Thegradientis just such a vector; it expresses the directional components of thesharpest slope along each of thoseNdimensions. If we’re just imagining two weightdimensions (say for one weightwand one biasb), the gradient might be a vector withtwo orthogonal components, each of which tells us how much the ground slopes inthewdimension and in thebdimension. Fig.5.4shows a visualization of the valueof a 2-dimensional gradient vector taken at the red point.
Cost(w,b)
wbFigure 5.4Visualization of the gradient vector at the red point in two dimensionswandb,showing the gradient as a red arrow in the x-y plane.In an actual logistic regression, the parameter vectorwis much longer than 1 or2, since the input feature vectorxcan be quite long, and we need a weightwiforeachxi. For each dimension/variablewiinw(plus the biasb), the gradient will havea component that tells us the slope with respect to that variable. Essentially we’reasking: “How much would a small change in that variablewiinﬂuence the total lossfunctionL?”In each dimensionwi, we express the slope as a partial derivative∂∂wiof the lossfunction. The gradient is then deﬁned as a vector of these partials. We’ll represent ˆyasf(x;q)to make the dependence onqmore obvious:—qL(f(x;q),y)) =266664∂∂w1L(f(x;q),y)∂∂w2L(f(x;q),y)...∂∂wnL(f(x;q),y)377775(5.15)The ﬁnal equation for updatingqbased on the gradient is thusqt+1=qt h—L(f(x;q),y)(5.16)

## Page 84

The derivatives of the loss function22CHAPTER6•VECTORSEMANTICS ANDEMBEDDINGSproof as an exercise at the end of the chapter):∂LCE∂cpos=[s(cpos·w) 1]w(6.35)∂LCE∂cneg=[s(cneg·w)]w(6.36)∂LCE∂w=[s(cpos·w) 1]cpos+kXi=1[s(cnegi·w)]cnegi(6.37)The update equations going from time stepttot+1 in stochastic gradient descentare thus:ct+1pos=ctpos h[s(ctpos·w) 1]w(6.38)ct+1neg=ctneg h[s(ctneg·w)]w(6.39)wt+1=wt h[s(cpos·wt) 1]cpos+kXi=1[s(cnegi·wt)]cnegi(6.40)Just as in logistic regression, then, the learning algorithm starts with randomly ini-tializedWandCmatrices, and then walks through the training corpus using gradientdescent to moveWandCso as to maximize the objective in Eq.6.34by making theupdates in (Eq.6.39)-(Eq.6.40).Recall that the skip-gram model learnstwoseparate embeddings for each wordi:thetarget embeddingwiand thecontext embeddingci, stored in two matrices, thetargetembeddingcontextembeddingtarget matrixWand thecontext matrixC. It’s common to just add them together,representing wordiwith the vectorwi+ci. Alternatively we can throw away theCmatrix and just represent each wordiby the vectorwi.As with the simple count-based methods like tf-idf, the context window sizeLaffects the performance of skip-gram embeddings, and experiments often tune theparameterLon a devset.6.8.3 Other kinds of static embeddingsThere are many kinds of static embeddings. An extension of word2vec,fasttextfasttext(Bojanowski et al., 2017), deals with unknown words and sparsity in languages withrich morphology, by using subword models. Each word in fasttext is represented asitself plus a bag of constituent n-grams, with special boundary symbols<and>added to each word. For example, withn=3 the wordwherewould be representedby the sequence<where>plus the character n-grams:<wh, whe, her, ere, re>Then a skipgram embedding is learned for each constituent n-gram, and the wordwhereis represented by the sum of all of the embeddings of its constituent n-grams.A fasttext open-source library, including pretrained embeddings for 157 languages,is available athttps://fasttext.cc.The most widely used static embedding model besides word2vec is GloVe(Pen-nington et al., 2014), short for Global Vectors, because the model is based on cap-turing global corpus statistics. GloVe is based on ratios of probabilities from theword-word co-occurrence matrix, combining the intuitions of count-based modelslike PPMI while also capturing the linear structures used by methods like word2vec.6.8•WORD2VEC21Given the set of positive and negative training instances, and an initial set of embed-dings, the goal of the learning algorithm is to adjust those embeddings to•Maximize the similarity of the target word, context word pairs(w,cpos)drawnfrom the positive examples•Minimize the similarity of the(w,cneg)pairs from the negative examples.If we consider one word/context pair(w,cpos)with itsknoise wordscneg1...cnegk,we can express these two goals as the following loss functionLto be minimized(hence the ); here the ﬁrst term expresses that we want the classiﬁer to assign thereal context wordcposa high probability of being a neighbor, and the second termexpresses that we want to assign each of the noise wordscnegia high probability ofbeing a non-neighbor, all multiplied because we assume independence:LCE= log"P(+|w,cpos)kYi=1P( |w,cnegi)#= "logP(+|w,cpos)+kXi=1logP( |w,cnegi)#= "logP(+|w,cpos)+kXi=1log 1 P(+|w,cnegi) #= "logs(cpos·w)+kXi=1logs( cnegi·w)#(6.34)That is, we want to maximize the dot product of the word with the actual contextwords, and minimize the dot products of the word with theknegative sampled non-neighbor words.We minimize this loss function using stochastic gradient descent. Fig.6.14shows the intuition of one step of learning.WCmove apricot and jam closer,increasing cpos z waardvark
move apricot and matrix apartdecreasing cneg1 z w“…apricot jam…”w
zebrazebraaardvarkjamapricot
cposmatrixTolstoymove apricot and Tolstoy apartdecreasing cneg2 z w!cneg1cneg2k=2Figure 6.14Intuition of one step of gradient descent. The skip-gram model tries to shiftembeddings so the target embeddings (here forapricot) are closer to (have a higher dot prod-uct with) context embeddings for nearby words (herejam) and further from (lower dot productwith) context embeddings for noise words that don’t occur nearby (hereTolstoyandmatrix).To get the gradient, we need to take the derivative of Eq.6.34with respect tothe different embeddings. It turns out the derivatives are the following (we leave the6.8•WORD2VEC21Given the set of positive and negative training instances, and an initial set of embed-dings, the goal of the learning algorithm is to adjust those embeddings to•Maximize the similarity of the target word, context word pairs(w,cpos)drawnfrom the positive examples•Minimize the similarity of the(w,cneg)pairs from the negative examples.If we consider one word/context pair(w,cpos)with itsknoise wordscneg1...cnegk,we can express these two goals as the following loss functionLto be minimized(hence the ); here the ﬁrst term expresses that we want the classiﬁer to assign thereal context wordcposa high probability of being a neighbor, and the second termexpresses that we want to assign each of the noise wordscnegia high probability ofbeing a non-neighbor, all multiplied because we assume independence:LCE= log"P(+|w,cpos)kYi=1P( |w,cnegi)#= "logP(+|w,cpos)+kXi=1logP( |w,cnegi)#= "logP(+|w,cpos)+kXi=1log 1 P(+|w,cnegi) #= "logs(cpos·w)+kXi=1logs( cnegi·w)#(6.34)That is, we want to maximize the dot product of the word with the actual contextwords, and minimize the dot products of the word with theknegative sampled non-neighbor words.We minimize this loss function using stochastic gradient descent. Fig.6.14shows the intuition of one step of learning.WCmove apricot and jam closer,increasing cpos z waardvark
move apricot and matrix apartdecreasing cneg1 z w“…apricot jam…”w
zebrazebraaardvarkjamapricotcposmatrixTolstoymove apricot and Tolstoy apartdecreasing cneg2 z w!cneg1cneg2k=2Figure 6.14Intuition of one step of gradient descent. The skip-gram model tries to shiftembeddings so the target embeddings (here forapricot) are closer to (have a higher dot prod-uct with) context embeddings for nearby words (herejam) and further from (lower dot productwith) context embeddings for noise words that don’t occur nearby (hereTolstoyandmatrix).To get the gradient, we need to take the derivative of Eq.6.34with respect tothe different embeddings. It turns out the derivatives are the following (we leave the

## Page 85

Update equation in SGD22CHAPTER6•VECTORSEMANTICS ANDEMBEDDINGSproof as an exercise at the end of the chapter):∂LCE∂cpos=[s(cpos·w) 1]w(6.35)∂LCE∂cneg=[s(cneg·w)]w(6.36)∂LCE∂w=[s(cpos·w) 1]cpos+kXi=1[s(cnegi·w)]cnegi(6.37)The update equations going from time stepttot+1 in stochastic gradient descentare thus:ct+1pos=ctpos h[s(ctpos·wt) 1]wt(6.38)ct+1neg=ctneg h[s(ctneg·wt)]wt(6.39)wt+1=wt h"[s(cpos·wt) 1]cpos+kXi=1[s(cnegi·wt)]cnegi#(6.40)Just as in logistic regression, then, the learning algorithm starts with randomly ini-tializedWandCmatrices, and then walks through the training corpus using gradientdescent to moveWandCso as to maximize the objective in Eq.6.34by making theupdates in (Eq.6.39)-(Eq.6.40).Recall that the skip-gram model learnstwoseparate embeddings for each wordi:thetarget embeddingwiand thecontext embeddingci, stored in two matrices, thetargetembeddingcontextembeddingtarget matrixWand thecontext matrixC. It’s common to just add them together,representing wordiwith the vectorwi+ci. Alternatively we can throw away theCmatrix and just represent each wordiby the vectorwi.As with the simple count-based methods like tf-idf, the context window sizeLaffects the performance of skip-gram embeddings, and experiments often tune theparameterLon a devset.6.8.3 Other kinds of static embeddingsThere are many kinds of static embeddings. An extension of word2vec,fasttextfasttext(Bojanowski et al., 2017), deals with unknown words and sparsity in languages withrich morphology, by using subword models. Each word in fasttext is represented asitself plus a bag of constituent n-grams, with special boundary symbols<and>added to each word. For example, withn=3 the wordwherewould be representedby the sequence<where>plus the character n-grams:<wh, whe, her, ere, re>Then a skipgram embedding is learned for each constituent n-gram, and the wordwhereis represented by the sum of all of the embeddings of its constituent n-grams.A fasttext open-source library, including pretrained embeddings for 157 languages,is available athttps://fasttext.cc.The most widely used static embedding model besides word2vec is GloVe(Pen-nington et al., 2014), short for Global Vectors, because the model is based on cap-turing global corpus statistics. GloVe is based on ratios of probabilities from theword-word co-occurrence matrix, combining the intuitions of count-based modelslike PPMI while also capturing the linear structures used by methods like word2vec.Start with randomly initialized C and W matrices, then incrementally do updates

## Page 86

Two sets of embeddingsSGNS learns two sets of embeddingsTarget embeddings matrix WContext embedding matrix C It's common to just add them together, representing word ias the vector  wi+ ci

## Page 87

Summary: How to learn word2vec (skip-gram) embeddingsStart with V random d-dimensional vectors as initial embeddingsTrain a classifier based on embedding similarity◦Take a corpus and take pairs of words that co-occur as positive examples◦Take pairs of words that don't co-occur as negative examples◦Train the classifier to distinguish these by slowly adjusting all the embeddingsto improve the classifier performance◦Throw away the classifier code and keep the embeddings.

## Page 88

Vector Semantics & EmbeddingsWord2vec: Learning the embeddings

## Page 89

Vector Semantics & EmbeddingsProperties of Embeddings

## Page 90

The kinds of neighbors depend on window sizeSmall windows (C= +/-2): nearest words are syntactically similar words in same taxonomy◦Hogwartsnearest neighbors are other fictional schools◦Sunnydale, Evernight, BlandingsLarge windows (C= +/-5):  nearest words are related words in same semantic field◦Hogwartsnearest neighbors are Harry Potter world:◦Dumbledore, half-blood,  Malfoy

## Page 91

24CHAPTER6•VECTORSEMANTICS ANDEMBEDDINGSFor exampleLevy and Goldberg (2014a)showed that using skip-gram with awindow of±2, the most similar words to the wordHogwarts(from theHarry Potterseries) were names of other ﬁctional schools:Sunnydale(fromBuffy the VampireSlayer) orEvernight(from a vampire series). With a window of±5, the most similarwords toHogwartswere other words topically related to theHarry Potterseries:Dumbledore,Malfoy, andhalf-blood.It’s also often useful to distinguish two kinds of similarity or association betweenwords(Sch¨utze and Pedersen, 1993). Two words haveﬁrst-order co-occurrenceﬁrst-orderco-occurrence(sometimes calledsyntagmatic association) if they are typically nearby each other.Thuswroteis a ﬁrst-order associate ofbookorpoem. Two words havesecond-orderco-occurrence(sometimes calledparadigmatic association) if they have similarsecond-orderco-occurrenceneighbors. Thuswroteis a second-order associate of words likesaidorremarked.Analogy/Relational Similarity:Another semantic property of embeddings is theirability to capture relational meanings. In an important early vector space model ofcognition,Rumelhart and Abrahamson (1973)proposed theparallelogram modelparallelogrammodelfor solving simple analogy problems of the forma is to b as a* is to what?. In suchproblems, a system given a problem likeapple:tree::grape:?, i.e.,apple is to tree asgrape is to, and must ﬁll in the wordvine. In the parallelogram model, illus-trated in Fig.6.15, the vector from the wordappleto the wordtree(=#       »apple #   »tree)is added to the vector forgrape(#        »grape); the nearest word to that point is returned.treeapplegrapevineFigure 6.15The parallelogram model for analogy problems(Rumelhart and Abrahamson,1973): the location of#     »vine can be found by subtracting#   »tree from#       »apple and adding#       »grape.In early work with sparse embeddings, scholars showed that sparse vector mod-els of meaning could solve such analogy problems(Turney and Littman, 2005), butthe parallelogram method received more modern attention because of its successwith word2vec or GloVe vectors (Mikolov et al. 2013b,Levy and Goldberg 2014b,Pennington et al. 2014). For example, the result of the expression (#     »king) #     »man+#            »woman is a vector close to#         »queen. Similarly,#      »Paris #           »France+#     »Italy) results in avector that is close to#         »Rome. The embedding model thus seems to be extracting rep-resentations of relations likeMALE-FEMALE, orCAPITAL-CITY-OF, or evenCOM-PARATIVE/SUPERLATIVE, as shown in Fig.6.16from GloVe.For aa:b::a*:b*problem, meaning the algorithm is givena, b,anda*and mustﬁndb*, the parallelogram method is thus:ˆb⇤=argmaxxdistance(x,a⇤ a+b)(6.41)with the distance function deﬁned either as cosine or as Euclidean distance.There are some caveats. For example, the closest value returned by the paral-lelogram algorithm in word2vec or GloVe embedding spaces is usually not in factb* but one of the 3 input words or their morphological variants (i.e.,cherry:red ::Analogical relationsThe classic parallelogram model of analogical reasoning (Rumelhartand Abrahamson 1973)To solve: "apple is to tree as grape is to  _____"Add tree –apple  to grape to get vine

## Page 92

Analogical relations via parallelogramThe parallelogram method can solve analogies with both sparse and dense embeddings (Turney and Littman 2005, Mikolovet al. 2013b)king –man + woman is close to queenParis –France + Italy is close to RomeFor a problem a:a*::b:b*, the parallelogram method is:24CHAPTER6•VECTORSEMANTICS ANDEMBEDDINGSFor exampleLevy and Goldberg (2014a)showed that using skip-gram with awindow of±2, the most similar words to the wordHogwarts(from theHarry Potterseries) were names of other ﬁctional schools:Sunnydale(fromBuffy the VampireSlayer) orEvernight(from a vampire series). With a window of±5, the most similarwords toHogwartswere other words topically related to theHarry Potterseries:Dumbledore,Malfoy, andhalf-blood.It’s also often useful to distinguish two kinds of similarity or association betweenwords(Sch¨utze and Pedersen, 1993). Two words haveﬁrst-order co-occurrenceﬁrst-orderco-occurrence(sometimes calledsyntagmatic association) if they are typically nearby each other.Thuswroteis a ﬁrst-order associate ofbookorpoem. Two words havesecond-orderco-occurrence(sometimes calledparadigmatic association) if they have similarsecond-orderco-occurrenceneighbors. Thuswroteis a second-order associate of words likesaidorremarked.Analogy/Relational Similarity:Another semantic property of embeddings is theirability to capture relational meanings. In an important early vector space model ofcognition,Rumelhart and Abrahamson (1973)proposed theparallelogram modelparallelogrammodelfor solving simple analogy problems of the forma is to b as a* is to what?. In suchproblems, a system given a problem likeapple:tree::grape:?, i.e.,apple is to tree asgrape is to, and must ﬁll in the wordvine. In the parallelogram model, illus-trated in Fig.6.15, the vector from the wordappleto the wordtree(=#       »apple #   »tree)is added to the vector forgrape(#        »grape); the nearest word to that point is returned.treeapplegrapevineFigure 6.15The parallelogram model for analogy problems(Rumelhart and Abrahamson,1973): the location of#     »vine can be found by subtracting#   »tree from#       »apple and adding#       »grape.In early work with sparse embeddings, scholars showed that sparse vector mod-els of meaning could solve such analogy problems(Turney and Littman, 2005), butthe parallelogram method received more modern attention because of its successwith word2vec or GloVe vectors (Mikolov et al. 2013b,Levy and Goldberg 2014b,Pennington et al. 2014). For example, the result of the expression (#     »king) #     »man+#            »woman is a vector close to#         »queen. Similarly,#      »Paris #           »France+#     »Italy) results in avector that is close to#         »Rome. The embedding model thus seems to be extracting rep-resentations of relations likeMALE-FEMALE, orCAPITAL-CITY-OF, or evenCOM-PARATIVE/SUPERLATIVE, as shown in Fig.6.16from GloVe.For aa:b::a*:b*problem, meaning the algorithm is givena, b,anda*and mustﬁndb*, the parallelogram method is thus:ˆb⇤=argmaxxdistance(x,a⇤ a+b)(6.41)with the distance function deﬁned either as cosine or as Euclidean distance.There are some caveats. For example, the closest value returned by the paral-lelogram algorithm in word2vec or GloVe embedding spaces is usually not in factb* but one of the 3 input words or their morphological variants (i.e.,cherry:red ::

## Page 93

Structure in GloVE Embedding space

## Page 94

Caveats with the parallelogram methodIt only seems to work for frequent words, small distances and certain relations (relating countries to capitals, or parts of speech), but not others. (Linzen2016, Gladkovaet al. 2016, Ethayarajhet al. 2019a) Understanding analogy is an open area of research (Peterson et al. 2020)

## Page 95

Train embeddings on different decades of historical text to see meanings shift~30 million books, 1850-1990, Google Books dataEmbeddings as a window onto historical semantics
William L. Hamilton, Jure Leskovec, and Dan Jurafsky. 2016. Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change. Proceedings of ACL.

## Page 96

Embeddings reflect cultural bias!Ask “Paris : France :: Tokyo : x” ◦x = JapanAsk “father : doctor :: mother : x” ◦x = nurseAsk “man : computer programmer :: woman : x” ◦x = homemakerBolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. "Man is to computer programmer as woman is to homemaker? debiasing word embeddings." InNeurIPS, pp. 4349-4357. 2016.
Algorithms that use embeddings as part of e.g., hiring searches for programmers, might lead to bias in hiring

## Page 97

Historical embedding as a tool to study cultural biases•Compute a gender or ethnic bias for each adjective: e.g., how much closer the adjective is to "woman" synonyms than "man" synonyms, or names of particular ethnicities•Embeddings for competence adjective (smart, wise, brilliant, resourceful, thoughtful, logical) are biased toward men, a bias slowly decreasing 1960-1990•Embeddings for dehumanizing adjectives (barbaric, monstrous, bizarre)  were biased toward Asians in the 1930s, bias decreasing over the 20th century.•These match the results of old surveys done in the 1930sGarg, N., Schiebinger, L., Jurafsky, D., and Zou, J. (2018). Word embeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences 115(16), E3635–E3644.

## Page 98

Vector Semantics & EmbeddingsProperties of Embeddings

